COMPUTER NETWORKS
Prof. Dr. Sujoy Ghosh
Department of Computer Science and Engineering
IIT Kharagpur
Lecture -16
Error Control
Good day.
In the last lecture, we have seen some functionalities of the data link layer namely framing etc. And we have been talking off and on about the error control. Error control means error detection correction, etc. We will be talking about error detection correction in this lecture. We will be talking about error control now. 
 
      (Refer slide time: 01:14 - 03:35)
Now data errors, when data is transmitted over a cable or a channel there is always a chance that some of the bits will be changed or corrupted due to noise, signal distortion or attenuation etc. So many things may happen; for example, suppose you have a wireless channel and suddenly there is a burst of noise, so some of the data will get garbled. Similarly the data may have become very attenuated due to some loose contact somewhere or something and the one that was sent was not received or maybe it was received as a 0 or something. So, whenever you are sending some data or something, there is some communication going on over some transmission line, you always have to assume that data may not reach the other side in a perfect condition and error control has to deal with how to handle that. So if errors do occur then some of the transmitted bits will either change from 0 to 1 or from 1 to 0. Now what do you mean by error? Some bits were transmitted from 1 and were changed to 0 or from 0 they was changed to 1. So this is the only kind of error that may occur and if you knew where the position of that error is, we can correct it. Because the correct one will be the reverse of the faulty one. So if it is 0, then the correct one is 1 and if it is 1 then the correct one is 0. The whole trick is to find out whether any errors or any change has occurred and if such a change has occurred, where it has occurred and how to handle it. There may be a lot of noise and a lot of data may get corrupted in this fashion. So we will see how or what we can do about this. This data error will also depend on the kind of transmission medium we are using. So that is always there. 

(Refer slide time: 03:37 - 04:20)
So, random errors change bits unpredictably. Each bit transmitted has a probability of being changed. These errors are often caused by thermal noise; because this thermal noise is something very general and is always present in any kind of these things. There is higher thermal noise or there may be low level of thermal noise but this thermal noise is always present. So these cause some random errors. There are some burst errors or a change in a number of bits in succession. They are often caused by faults in electrical equipment or by interference on some neighboring things. So, electrical interference is a very major cause of such burst errors. 

(Refer slide time: 04:21 - 05:17)
So, we take transmission errors as inevitable, resulting in the change of one or more bits in a transmitted frame. The rate of error may be high or low, but there will always be some error; that is what we assume. Let us just see what it means, if there is a finite probability of error for a particular bit. Let us just have a look at some notations.
Let Pb be the probability of a single bit error called bit error rate. So the probability of a single bit error is the bit error rate or b e r as they are sometimes called. P1 is probability that a frame arrives with no bit errors, that means there is no error. And P2 is the probability that a frame arrives with one or more undetected bit errors. So, P1 is no error and P2 is one or more errors.

(Refer slide time: 05:17 - 05:55)
P3 is a probability that a frame arrives with one or more detected bit errors, but no undetected bit errors. Sometimes we can detect which bits are in error and sometime we cannot. So, we will come to this later. 
 Let F be the number of bits in a frame, if no error detection scheme is used. Suppose we are not detecting it specifically.
 So P3 = 0; 
We cannot detect which bits are in error.
 So P1 = (1 ? Pb)f. 
                              
                                            (Refer slide time: 05:55 - 06:01)
If you remember, Pb is a bit error rate. Since Pb is a bit error rate probability of a single bit error, P1 would be 1? Pb for one particular bit. 


(Refer slide time: 06:02 - 06:04)
In one particular bit, the probability of error is Pb. So, the probability for that particular bit has no error, that is, 1? Pb. And we are assuming that these errors are sort of independent, which may not always be the case with burst errors. But for thermal noise, we assume that there are bit errors. We will see that the noise will introduce an error or some other error will creep in. Let us say they are independent. So if there are F bits in a frame, the probability that a frame arrives with no error is (1 ? Pb)f. 

(Refer slide time: 06:47 - 06:58)
And P2 arrives with one or more errors, (1? P1),
P2 = 1 ? P1
So, clearly longer frame and higher Pb leads to lower P1.
Let us try to look at some numbers. 

(Refer slide time: 06:59 - 07:11)
So, this is an illustrative example. A defined specification for ISDN connections ? you remember ISDN means integrated service data network which is the digital service given through the telephone lines. So that is the ISDN. So in ISDN connections, bit error rate on a 64-kbps channel should be less than 10?6 on at least 90% of the observed 1-minute intervals. Suppose that looks quite low, rate of error is 1 in 1016. That means only in a million bits 1 one bit may come in error.
Suppose we want that at most one frame with an undetected bit error should occur per day on a continuously used 64-kbps channel, with a frame length of 1000 bits. So we want not more than one frame should come with undetected bit error. So we are using this for a whole day on the 64-kbps channel and the length of the frame is 1000 bits. 

(Refer slide time: 08:59 - 09:34)

So number of frames transmitted per day = (64K/1000) ? 60 ? 60 ? 24 = 5.5 ? 106.
 So if one of them is desired frame error rate, P2 = 1/(5.5 ? 106 ) = 0.18 ? 10?6 . 
 But, if we assume Pb =10?6, then
 P1 = (0.999999)1000 = 0.999. 
 So if P1 = 0.999, P2 = 10?3, which is about three orders of magnitude too large to meet our requirements. That means it is thousand times more, although one bit in a million looks a fairly good error rate. But we find that at the end of the day, we may have a thousand frames which are in error. Well, it does not work out that way that by multiplying with 1000, we will get so many frames in error. But the point is that whatever probability we require for a faulty frame we have got a much higher probability than we were prepared. We were prepared for say one faulty data frame per day but that is not the case at all. So this necessitates the use of error detection; that means, we have to do something to bring down this error rate. The other point is that sometimes one will not have control over the environment or the transmission line directly. It is quite difficult, because if you are having a wireless transmission. Now, some noise will certainly come from some other source. May be somebody starting a car will also give out some kind of electrical noise and somebody starting a machine somewhere will give some kind of electrical noise, some  mixer or grinder running in the house may give some kind of error. All kinds of sources are there, and you do not have any control over it. So the point is that, whatever the error that is coming, you have to do something at your end to get better error rates. So, we want to detect such errors and if possible we want to correct them. So this is what we are going to discuss now. 

(Refer slide time: 11:11 - 12:49)

Error detection: One general principle of getting greater reliability or less error, as I said is always redundancy. That means you put in some kind of redundancy in the system in order to get better reliability. We have seen this when we were talking about the reliability of optical networks. But in this case, the redundancy comes in the form of extra bits that we introduce into the data, which means that whatever data is being sent, we also send in some extra bits specifically for the purpose of error detection and correction. So EDC are the error detection and correction bits which are actually redundant. That means they are not original part of the data, and they are extra bits, which have been introduced.
Suppose D = Data protected by error checking; it may include header fields.
So the data which we are protecting may or may not include it; it depends on the protocol. It may just be on the payload and the header, all together, we may like to detect the error. Error detection is never 100% reliable. But the protocol may miss some errors but usually we can bring down the level of errors to a very low figure. The larger EDC fields yield better detection. If you put larger redundancy you have better error detection and correction. 

(Refer slide time: 12:50 - 13:42)
So you have the datagram coming. We add some extra bits EDC from there. We send it to the bit error prone link, which arrives at D? and EDC?, now D may be the same as in D? and has no error and it itself may have corrupted or both may have been corrupted. But any way we run some algorithm or some method for finding out whether these are alright and if we think they are alright we say yes. Otherwise, we say that an error has been detected. Sometimes, the errors may be such that although we say everything is ok, it may not be so; in that case, we say that the error has gone undetected. 

(Refer slide time: 13:42 - 14:25)
Now for this error detection and correction, one concept, which is used, is the so-called hamming distance between the two bit patterns. And then there are hamming codes also. We will not go in to the details of hamming codes here, but let me just tell you what is hamming distance. Suppose we have two code words, like it has been shown here. This one code word is 10101110 and the other one is 0100110, these are the code words. By hamming distance, what we mean is that we have to see in how many bit positions the two code words are different. 

(Refer slide time: 14:35 - 16:07)
  
So, in order to find that, we simply XOR the two. So, if We XOR these, we get this. If you can go through 0 1 is 1, 1 0 is 1 and so on. So you get 1 1 1 0 0 0 1 1. Now we count the number of 1s in this XOR, which is 5. So we say that their hamming distance is 5.
 Please note that if the two corresponding bits are different in the two code words 0 and 1, 1 and 0, then we get a 1 in this XOR. If they are the same like 1 1 or 0 0, we get 0. This H hamming distance is the number of positions where the two code words differ. If you have a set of code words, say between these two hamming distance is 4, between these two, the hamming distance is 3, between these two also the hamming distance is 3. Let us see this differs in one position, two positions, these are the same in the third position. This is again the same. This differs in three positions. Similarly, this other H is equal to 3 for other peers. In this set as a whole, the minimum hamming distance between two code words is actually 3. H of the code word is actually 3. We take the minimum of all these H values and this comes out to be 3. So we say that for this set of code words, the hamming distance is 3. Now, if the hamming distance is 3, we have an interesting situation. Let us say these code words are what we are sending to the other side; now if there is a single error; that means if a single bit has flipped from 1, it has become 0. So what has happened is that this code word will move to another code word. So this will appear as another code word in the receiving end; and the one which is received and the one which was sent ? the hamming distance between the two will be only 1, because only one bit has flipped. So, only one position, what was 0 has become a 1 there, or may be what was 1 here has become 0 there. So it has changed 1 in one position. Assuming that there is a single bit error, the hamming distance between the transmitted and the received code word will be 1. Now if the system is known, then this is the set of code words which is being used on both the sides; and first of all you will immediately know that for this particular set of code words H = 3 was there. That means, say, this is the set of keywords that I showed you; those are the only code words, which had been sent. So the hamming distance was 3. So for whichever value, this new transmuted or new changed code that was received is not going to be a valid code, because this is only at a hamming distance of 1 from the transmitted code. So this is not going to be a member of the set. So immediately, we will detect that there has been some error. And not only that, if you assume that there is most likelihood that we have got only one error then it is the one which is at a hamming distance of 1 from this transmitted code. It will be at a hamming distance of 1 or 2 from the code, whereas it is at a hamming distance of 1 from the code word which was already sent. And if you know this and if you assume that there can be only one error then you know that that is the code word. So you can not only detect that there has been an error, you can also correct the error by going to the nearest code word from which the hamming distance is minimum, namely, the one which was transmitted. So this is the general principle. So we will see practically how this is done. There is a coding scheme, for which it goes as hamming code. We will not discuss that, but there are other schemes, but all of them use this idea of a hamming distance. 

      (Refer slide time: 19:08 - 19:50)
The simplest error detection scheme is the parity check. It appends a parity bit to the end of a block of data. What is a parity bit? It is an extra bit, which is sort of added to some data. There are two kinds of parity: odd parity and even parity. Odd number of bit errors can be detected. Why odd number of bit errors can be detected, I will explain that. If an even number of bits is in inverted due to error an undetected error occurs. The technique is not foolproof, as noise impulses are often long enough to destroy more than one bit, particularly at high data rates. 
       
     (Refer slide time: 19:08 - 23:09)
But let us first of all see what a parity bit is.	
Suppose 1 0 1 1 0 1 1 1 was the original bit pattern. In even parity, what we do is that we make the number of 1; we add a bit which is 0 or 1, depending on whether the number of 1s in the original pattern is even or odd. If it is even parity, we want to make the number of 1s even, so the number of 1s were originally even over here. So in 1 0 1 1 0 1 1 1, we have six 1s. So we already have an even number of 1s here, in the added parity bit we just give it a value 0, whereas  in the other one, that is, 1 1 0 0 0 0 0 1, we have three 1s. So this was odd, so the added parity bit was also made 1, so that overall the number of 1s in this data plus parity bit taken together is always even. That is, the number of 1s is always even in even parity if everything is fine; and the number of 1s is always odd. Similarly, the number of 1s is always odd in odd parity. Now if you just think about it a bit, if you are using even parity then all the valid code words have even number of 1s in them. So what is the number of position and what is the minimum number of positions in which they have to differ ? these two different code words? Well, the minimum number is 2; that means, the hamming distance of this set of code words will be minimum 2. And if the hamming distance is 2 and if there is a single error, what is going to come is that we are going to get a code word in the receiving end, which has got an odd number of 1s. And, if it has got an odd number of 1s, we immediately know that this could not be a valid code word. So we know that there has been some error, although we cannot say where that error is, or what the original intended transmitted code is, because if you think of two code words which are at a distance of 2 and the transmitted one ? let us say we had even parity, that is, even number of 1s; their hamming distance minimum was 2. Let us take the worst case: the minimum is 2 and then the transmitted one of this code word was this. Where one of the bits flipped, we get something in the middle, which is at the hamming distance of 1 from the two valid code words. So we know that this has been transmitted as invalid. In that sense, this is not a valid code word. So an error has occurred. But I do not know whether something from here came here or the original transmitted code was here and it came here, because we do not know where the error has occurred. So in this particular case, we can nearly detect an error. We cannot correct it at the receiving end. So the case of parity is a very simple kind of scheme. 
       
      (Refer slide time: from 23:09 to 24:22)
So once again, if you go back to the earlier slide, I mentioned that odd number of bit errors can be detected. Now as I said if there is a single bit error, it will be detected because now we have got an odd number of points. Well, if there are odd number of errors, that means if there are one error or three errors or five errors in that bit stream, then again, we will get an odd number of 1s on the receiving end. So we can detect that there has been an error. We do not know the number of errors, but in any case, we can detect it. But if there are only two errors, then it will again become an even number of ones on other side which may be a valid code word. So we will not know whether there has been any error or not. So if there are an even number of errors, this parity check will go undetected by this single parity bit error checking. Running the parity check is very easy. 
       
     (Refer slide time: 24:22 - 24:37)
You just run them through some XORs and invert it if you want it in odd parity or if you want an even parity, you can get it and this is very simple.
       
       	(Refer slide time: 24:22 - 24:59)
Now parity testing: the receiving device can work out if a bit has been corrupted in a character by counting the number of 1s in the character and parity bit. If even parity bit is being used but there are an odd number of 1s then an error must have occurred. Similarly, if odd parity is being used but there is an even number of 1s then an error must have occurred. 
			
				(Refer slide time: 24:59 - 25:37)
One drawback with parity testing errors is that if two errors occur in the same character, they will not be detected. This is because if there were an even number of 1s originally, if two bits are changed, then there will still be an even number of 1s because either two 1s will become 0. So the even number of 1s remain even or two 0s will become 1, also even number of 0s remain even or 1 1 has become 0 and 1 0 has become 1. So the number of 1s does not change and they still remain even. So it remains undetected. 

			
      (Refer slide time: 25:37 - 26:12)
There is some improvement over this and we can see how to make it more efficient. This is an extension of parity bit, which is called block parity testing. This is how it works. Suppose we have a long stream of 1s and 0s, 1s and 0s, 1 0 1 1, etc. So what we do is, we break them up into blocks of bits. Let us say 8 bits; we take the first 8 bits as the first pattern. Then the next 8 bits are the next pattern, and so on. 
	                    
                    (Refer slide time: 26:12 - 27:11)

So what we do is, for each group of 8 bits a parity can be added. In this case let us say even parity, so the first 8 bits were 1 0 1 0 1 1 1 0. So for even parity if there are five 1s here so I have to add a 1, there are three 1s here, I have to add 1, there are four 1s here; so I add a 0. In this way for each block of 8 bits, we add a parity bit. 
Then for every N blocks the parity is calculated as a new character (in this case 8 ? 8). So this is how it goes. What we do is, if you look at this as some kind of a matrix. So, we calculate the parity on this side. We calculate the parity along the column also. So five 1s, so we make it a 1 over here for this parity. (Refer slide time: 27:12 - 27:40)
Then for every N blocks the parity is calculated as a new character. I have mentioned that. I have got parity on this side. So for this block of, say, 64 characters, I have got say 8 + 8 + 1 = 16 + 1 = 17 parity bits. 
       
     (Refer slide time: 27:42 - 28:43)
Now if there is a single error, which has occurred, what is going to happen is that where ever it may have occurred in that row the parity test will fail. Similarly in the column also the parity test will fail; and since one column and one row intercept exactly at one cell, we know that that is the bit which has become corrupted. So now if an error occurs, we will know which character and which column it has occurred in. Not only can we detect the error but we can also correct a single bit error by simply changing it. So since we know that this is the bit which is in error, I can simply flip it at the receiving end itself without any referring back to the transmitter. And we know that we have got the correct one. We can not only detect errors but we can also correct single errors in block parity testing. 
			
				(Refer slide time: 28:43 - 29:46)
So this is just a better diagram for this; we have got this system. This is using odd parity. I mean there is no reason to use the same kind of parity in both the cases. You can use odd parity for the transverse parity bits and you can use even parity for this and calculate this. So if there is some error we can compute it. If there are more than one error then you may get into some kind of trouble because you will not know which bit is in error. If  there are two rows and two columns which show parity error then you will not know which row and which column will match together to get that particular bit, which has been corrupted. 
			
      (Refer slide time: 29:45 - 30:09)
So we not only have error detecting codes but we have some error correcting codes also. By   comparing error detection and error correcting codes, error correcting codes require more number of bits because we want to correct it. So we require more number of bits, which is a problem.
 
			
      (Refer slide time: 30:09 - 30:43)

In error correcting codes, rather than just detecting an error, sometimes it is useful to be able to correct an error as well. For example, CDs use error correcting codes to ensure high-quality sound reproduction even if the CD is slightly damaged. So we are talking about some music CD. So even if a few bits are in error still it will use error correcting code to handle that. We have already seen that we can correct a single error in a block parity check. There are other techniques that can correct more number of errors. 
			
      (Refer slide time: 30:44 - 32:06)
So what happens is that your one scheme could be error detection. So suppose some alphabet A was sent, this was a valid code word, and there was a noisy channel. So some error got introduced. So, 1 0 1 1 0 1 became 1 0 0 1 0 1. That means the third one from the left has become a 0. So this is an invalid code word. We know that the original one was using odd parity kind of thing, so even number and odd number of 1s were there. But in the even number there was even parity and hence even number of 1s were there. But I have got an odd number of 1s, I do not know where the error is, so what we do is, we request for retransmission; that could be one approach. But if I can correct a single error on the other side, then I do not need to ask for retransmission. Overall my efficiency may improve but of course for error correction, I have to send more bits. So that is the one area where I am sort of losing something. So it all depends on how you do your engineering and how you come to your particular point of say optimal amount of error correction. 
			
      (Refer slide time: 32:06 ? 32:41)
So another way of checking for errors was to use a checksum. This checksum is also used in other fields. So this has been borrowed not just from communication from other fields for accounting also, sometimes checksum is used. This is just used for this sum just for control purpose. So in some arbitrary order you sum it and then with all the data you also check with the sum.  

(Refer slide time: 32:41 ? 33:06)
This is a number calculated from the data and sent along with the data. If any errors occur during transmission then checksum for the received data will differ from the transmitted checksum. Of course, the data may arrive alright, but the checksum may become faulty. But even then in that case the checksum and the data will not match. So checksum can only detect errors; it will not be possible to correct it.  
			
      (Refer slide time: 33:06 ? 33:46)
A simple checksum can be calculated by adding all the data together. For example if the data is 121, 17, 29 and 47, then the checksum (add all the data) will be 214. We could transmit 121, 17, 29, 47 and 214. We know that the last number is the checksum, which should be the sum of all the numbers which have been sent. 
A checksum is usually restricted to a certain number of bits, typically 16 bits. If the checksum is longer than this, then only the lower 16 bits will be transmitted. But checksum is not very efficient and not widely used.
			
      (Refer slide time: 33:46 ? 34:38)
One scheme, which is widely used, is the so-called cyclic redundancy code. What is a cyclic redundancy code?
 A cyclic redundancy code (CRC) is a more sophisticated type of checksum. If a CRC is used, it is extremely unlikely that any errors will go undetected. A lot of different types of errors are caught by the CRC. Although the technique may sound a little complicated, in practice calculating a CRC is easy and can be implemented in hardware using linear shift registers which is a very simple kind of circuit; through that, we can compute it very easily and quite fast. So, the CRC is preferred in many data link protocols. 
			
      (Refer slide time: 34:38 - 34:49)
 So what is CRC?
 Let us just look at the details of it. 
			
       (Refer slide time: 34:49 ? 35:21)
Essentially the data is regarded as being one very long binary number. After all what you are sending is a string of 1s and 0s, and you can take a few of them and just look at it as a binary number. If you are adding the header also, then it got even more mixed up. I mean it was not to be interpreted as a number. But for CRC purpose, we interpret it as a binary number.  
			
      (Refer slide time: 35:21 ? 35:44)
So to this we add some place holder digits at the end and it is divided by a generator polynomial using modulo 2 division. So for modulo 2 division, you do the same kind of division. Only thing is that if they match then we get a 0, if they do not match we get 1. The remainder at the end of this division is the CRC. So let us just look at this. 
			
      (Refer slide time: 35:44 ? 36:48)
So given a k bit block of bits, the transmitter generates an n bit sequence, known as a frame check sequence (FCS), so that the resulting frame, consisting of k + n bits, is exactly divisible by some predetermined number. The receiver divides the incoming frame by that number and if there is no remainder, assumes there was no error. You look at it as a number; find the CRC; add it to the end; so what will happen is that you will get a longer number. This longer number is supposed to be divisible by the polynomial, which the receiver side knows. So the receiver side will do its own division and if the division is ok, then it will assume that there is no error. If it is not ok, then obviously there is some error. 
			
      (Refer slide time: 36:48 ? 37:02)
So we have D data bits and R: CRC bits. So how are the CRC bits computed? 
D * 2r XOR R. So this is done by some division. So we will see how it is done. 
			
       (Refer slide time: 37:02 ? 38:52)
This is the formula:
 D ? 2r/G. This is the generated polynomial. Why it is a polynomial ? that we will see. Actually the bit pattern is represented as a polynomial; ultimately, this so-called polynomial will be a binary number like this. Suppose the G is 1 0 0 1 and what you are trying to send is 1 0 1 0 1 1. So this is what we are trying to send; and this is the generated polynomial. What we are going to do is that we are going to add some extra bits. So this D ? 2r means, adding so many 0s at the end. 
So in this case, say, r = 3; so we have added three 0s to the end and I do a division as it is done. So 1 0 0 1 only thing is we do a modulo 2 kind of operation. So 1 1 becomes 0 1 0. As it gives 1, you got a 1 0 1 and so you got a 0 and so on. So in this way the division goes on till we sort of exhaust this and we get 0 1 1 as the remainder. And what will happen is that if you add this 0 1 1 over here, instead of sending 1 0 1 0 1 1, what we do is, we add 1 0 1. Any way, if you add it what will happen is that this will become exactly divisible modulo 2. The modulo 2 divisions will come out exactly. 

(Refer slide time: 38:53 ? 40:22)
So the CRC process can be very easily implemented in hardware using (LFSR) Linear Feedback Shift Register. The LFSR divides a message polynomial by a suitably chosen divisor polynomial; the remainder constitutes the FCS, which is added to the data bits. Commonly used divisor polynomials are:  
CRC 16 = X16 + X 15 + X 2 + 1.
So actually it is a 16 bit binary number as I said the G is called as generator polynomial. So G will have one in the sixteenth position, one in the fifteen position, one in the second position, and one in the first, that is, 0th position, which means that there are going to be four 1s in this G and all the rest are going to be 0. So how it is generated polynomial?
So we are not going into that, how this particular polynomial, why was this particular polynomial chosen rather then some other polynomial. But as we have seen , they cover a lot of different types of errors. Previously if you remember, in our parity bit we were just getting odd numbers of errors. If there are an odd number of errors then we can detect it here and we can detect a lot of different cases of error. So it is extremely unlikely that if the CRC is alright, then there has been some error. So there are different protocols and different systems with different generator polynomial.

(Refer slide time: 40:57 ? 41:02)
So CRC CCITT = X16 + X12 + X5 + 1  and so on. 
			
       (Refer slide time: 41:03 ? 41:35)
From the above slide we can see CRC 32 and CRC 8. So CRC 8 means this is an 8-bit thing. This is the eighth position, second position, first position, and zeroth position are all 1s, rest are 0s. So this is used in ATM. CRC 10: this may also be used; and CRC 12. So these are the different CRCs. Actually not all polynomials will give you the same kind of error coverage; that is known, and people have found out some properties which are good properties to have in a generator polynomial and then people have found out some good polynomials which will cover a lot of errors. So HDLC uses some kind of errors. 
			
       (Refer slide time: 41:59 ? 42:09)
 So CSMA/CD, FDDI, ATM ? they use CRC 32 and so on. I am just showing you, but you need not remember any of these.
			
       (Refer slide time: 41:59 ? 42:25)
Now, we come to a slightly higher level, data link protocols. We are still talking about errors and error control, but the point is that, we have been talking about this parity and CRC that is error control at a lower levels. We say a lower level because it is a small amount of error. We can possibly handle it using such error detection or error correction kind of schemes. But what might happen is, because of a very vast noise or something it may garble the data so badly, that it is entirely unrecognizable on the other side. Even if you have an error correction scheme, because none of the error correction and error detection schemes are 100% infallible. So there is still a possibility that there will be some undetected errors and then there will be some data which becomes so bad that there is no question of any error detection or correction. We require some higher-level schemes also in addition to whatever we have talked about this parity CRC etc. So we will now see higher layer data link protocols. 
			
       (Refer slide time: 43:34 ? 44:07)
 Once again, data link protocols could be of different types; 
for example, unrestricted simplex protocol: this does not do much as we will see; stop and wait protocol; stop and wait ARQ for a noisy channel means that ARQ stands for acknowledgement request. That means an acknowledgement is expected. One bit sliding window protocol: this is similar to stop and wait, sliding window with go back N ARQ, sliding window with selective reject ARQ, and so on.
						  
       (Refer slide time: 44:07 ? 44:57
The first is Unrestricted Simplex Protocol. Well, it simply sends, it does not do any kind of error control, and so this is the simplest situation. We have a sender to fetch a packet from a network layer, construct a frame; and send the frame to the physical layer. So the sender is just going on sending without bothering about what is happening on the receiver end. So if things are alright, the receiver waits for an event, receives a frame from physical layer and passes the packet to network layer. The faulty things, which have gone in, may be it is the responsibility of higher layer systems to take care of that. But the higher layer application does not bother, if a few bits are bad say in the case of a voice channel. 
			
       (Refer slide time: 44:58 ? 45:02)
			
       (Refer slide time: 45:03 ? 45:04)
			
       (Refer slide time: 45:05 ? 45:46)

When data is sent as a sequence of frames, two types of errors can occur:
One is lost frame: the frame has been lost entirely and the frame fails to arrive at the receiver. This may be due to a noise burst destroying the frame beyond recognition. It may have a recognizable frame arriving at the receiver but some bits are in error. A frame may be damaged or a frame may be entirely lost. See sender and the receiver. The sender is sending a number of frames one, two, to frame n. Frame one is going first. So the point is that some of the frames in between may get lost all together. We have to make the worst case assumptions. 

(Refer slide time: 45:58 ? 46:11)
Error control is concerned with insuring that all frames are eventually delivered (possibly in order) to a destination. So, how?
 Three items required are acknowledgment, timer and sequence number. 


(Refer slide time: 46:12 ? 46:57)

Acknowledgment ? what do we do with acknowledgment? 
Typically reliable delivery is achieved using the ?acknowledgments with retransmission? paradigm, whereby the receiver returns a special acknowledgment (ACK) frame to the sender indicating the correct receipt of a frame; that means the receiver, after receiving the same will send an acknowledgment. In some systems, the receiver also returns a negative acknowledgment for incorrectly received frame. So this is a hint to the sender, so that it can retransmit a frame right away without waiting for a timer to expire. We will come to this timer to expire. The point is that there is a two way communication. The sender is sending the frames, and the receiver is sending the acknowledgments. If it gets a damaged frame, then it will send a negative acknowledgment. Of course, if it does not get any frame, then it will not know whether anything was sent at all. In that case, it may not send any acknowledgment or negative acknowledgment anything at all.  

(Refer slide time: 47:20 ? 48:05)The situation is something like this: suppose 0 was sent by the sender and it arrived. The receiver sent an acknowledgment 0 then a 1 was sent. Suppose I am just taking this bit by bit, just to make it simpler. Suppose 1 was sent it came with some error, we do not know about what kind of error it is. So the frame number 0 was acknowledged, frame number 1 was received, but it was damaged, so a negative acknowledgment was sent. That means, frame number 1 was not rightly received, so frame number 1 is going to be retransmitted by the sender. This is the argument.

(Refer slide time: 47:20 ? 48:50)

The other thing we require is a timer. One problem simple acknowledgment, negative acknowledgment schemes fail to address is recovering from a frame that is totally lost, and as a result, fails to solicit an acknowledgment or a negative acknowledgment. What happens if an acknowledgment or negative acknowledgment becomes lost?
 Retransmission timers are used to resend frames that do not produce an acknowledgement. When sending a frame, schedule a timer to expire at some time after the acknowledgment should have been returned. If the timer goes off, retransmit the frame. 

(Refer slide time: 48:09 ? 50:51)
So this is the scheme, you see a frame which was sent was lost. So the receiver of course did not know anything. What happens is that as soon as the frame was sent, at the same time a clock was started in the sender?s end. I have to assume that the system has transmission time, and this is the time it might require for the receiver to process it and send an acknowledgment. Then the acknowledgment will take some time to reach the sender. So, all that is taken into the account with some amount of time thrown in, and that is set in the timer. So if the timer goes out that means, if there is a time out, now the sender knows that the frame must have been lost, otherwise an acknowledgment would have come. So he sends the frame again. The other thing that might happen is that suppose the frame was sent the 0 th frame was sent it was received and then an acknowledgment was sent, then the acknowledgment was lost. He would not know whether reached at all or the acknowledgement was lost. So after a time out, he will send this frame again. So this is a case where the frame has been duplicated. Depending on what kind of protocol we are using the receiver may or may not understand that this frame is actually a duplicate of this. Maybe two such exactly similar frames were supposed to come one after the other because it is after all coming from some application, which is absolutely unknown to the data link layer. So the frame may get duplicated. But then there are some protocols which take care of that where if the receiving side will know that this is actually a duplicated frame. We will come to that protocol later on. 

(Refer slide time: 50:51 ? 51:39)
And the third technique that we use is the so-called sequence number. So retransmission introduces the possibility of duplicate frames. To suppress duplicates, add sequence number to each frame, so that a receiver can distinguish between new frames and old copies. So that if the frames are numbered sequentially like say 0, 1, 2 etc. as I was showing you the case where the receiver gets two frames which have the same number 0, it knows that acknowledgment must have got lost or something like that happened. The sender sends the same frame twice, so he will discard just one of them. 

(Refer slide time: 51:40 ? 52:01)
So collectively, the mechanisms stated are referred to as automatic repeat request (ARQ). The objective is to turn an unreliable data link into a reliable one. 
Three versions of ARQ have been standardized: stop and wait ARQ, Go back N ARQ, and Selective Reject ARQ.

(Refer slide time: 52:02 ? 52:39)
 So we have the stop and wait ARQ, which is based on the stop and wait flow control technique. The source station transmits a single frame and then waits for an acknowledgment to arrive. No other data frames can be sent until the destination stations reply arrives at the source station. So it waits for the acknowledgment; if the acknowledgment does not come it will resend after a automatic repeat request. So it will automatically repeat the frame. 

(Refer slide time: 52:40 ? 53:39)
So this is the scheme, we have the sender which fetches a packet from network layer, construct a frame, send the frame to physical layer, wait for an event. So if it gets an acknowledgment  then it is fine then it is going to send a second frame. If it does not get an acknowledgment it will send the same frame again. So until it gets an acknowledgment or the timer runs out, the frame which was already sent that has to be stored locally in some buffer in whichever system is handling this data link control. And the receiver side also waits for an event, receive a frame from the physical layer, pass a packet to the network layer, send an acknowledgment to the physical layer. So this is the stop and wait protocol. This is a very simple protocol. Unfortunately, this is not very efficient as we will see that later.  

      (Refer slide time: 53:30 ? 53: 35)
And so we will continue our discussion on this error control and then we will move on to flow control in the next lecture. So this is a simple kind of protocol but this has got efficiency problem, in the sense that this is not very efficient. So we will continue this discussion in the next lecture. Thank you.
Preview of next Lecture
Lecture # 17
Stop and Wait Protocol
Good day, so in the last lecture we were discussing about stop and wait protocol .we have seen what it is. It is really a simple kind of protocol which is used both for error control as well as for flow control in some cases. But since this is a simple protocol we will look at its performance now. 

(Refer slide time - 54:34 ? 54:38)
So we will first finish our discussion on this stop and wait protocol. 

(Refer slide time: 54:39 -55:21)
And this we have already seen , so each  sender station sends one  packet which  are ACK?ed. If  it is comes with some error it may send a  negative  acknowledgement , it may not arrive at all .  In that case the sender will time out in either case that means, it is getting a negative acknowledgement or whether it is getting a time out. It will retransmit that frame.  Retransmit whichever frame was just transmitted earlier and the point to note is that there is only one frame, which is in transit in the channel at any point of time. Whether it is the frame or the acknowledgment, now we have discussed some of the major techniques which are used in the data link layer, so what we will do now is that we will look at just one more protocol. This is somewhat general version that we have discussed.  PPP that is the point to point protocol which is used for point to point communication but there could be a point to multi point communication also.  For this we have this high level data link control, this is the name of the protocol. So this is the name of the protocol or HDLC in short.  I mean this is not so high level any longer this is not considered, so high level any longer but even then this is an important protocol. So there are some variations of this,  which  are used in various places. So we will just look at some generic the way how this HDLC is important. 

(Refer slide time:56:32 - 56:59)

Now this is an important data link protocol this HDLC or some variant   say this is widely used, it also is the basis for many other important data link control protocols.  So we suggest three types of stations. One could be primary station this is responsible for the operation of the link. By  the way  you  do not  have to remember  all these  because when  you are  trying  to use it  or design  it for some  particular  situation you can always look up the reference  and  see what exactly the different frames  are what are their formats etc . I am trying to give you  a  flavor  of the kind of information about which  a typical protocol  would need  to exchange.  So  with  this example  we  come  to the end of this general data link control, but still  we  have seen  some  examples of data link control, for example we have seen some examples of MAC  like with token bus,  token ring, etc. We will see other examples of MAC as we take up other kinds of networks. Similarly we have seen some kind of error and data control.  By the way the similar approach is used in some other higher layers also. We will talk about it when we come to it and we will talk about more specific and more widely used systems in the next class onwards. So  in  the next class  what we will do  is,  we will look at  another  type of communication  and how MAC is done on that in satellite communication .So that will be the next topic. Thank you.



 
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science& Engineering 
IIT Kharagpur 
Lecture 12 
Protection and Restoration

(Refer start time:  00:43)
Good day. In this lecture we are going to discuss the various protection and restoration mechanisms which are usually employed in optical networks.
(Refer slide time:  00:57-00:59)

We have to discuss what is protection and what is restoration and why we need them. (Refer slide time:  01:08-01:18)

What is protection, what is restoration, comparison between the two and the different schemes of protection would be our general outline of presentation .
(Refer slide time:  01:19-01:26)

Network is unreliable; and then so many failures can occur ? a node may fail, a link may fail or a link may get cut, some fiber optic line may cut in-between because somebody cut it while digging a hole or something. A node might fail, there may be power failures at nodes and so there are failures in the network. But if you  remember, one of the most important places where we deploy optical networks is in the core of the network. The core of the network connects so many people to so many other people and it is very vital that we cannot allow the services to be severely disrupted because that would have very grave consequences and anyway the service provider attempts to give a high level of service. Although failures are unavoidable in real life, we have to find some way of combating this; that means if there is a failure we want to recover from it as soon or as fast as possible. That is what protection and restoration is all about. Another thing is that when we want to give some reliability, protection, restoration, etc., in some form or the other we always have to bring in some redundancy. Without any redundancy, a system  cannot be  protected, or it cannot be restored without any replacement, etc. There has to be some redundant capacity in some form in the network in order to achieve protection and restoration. So how this is done is what we will discuss. (Refer slide time:  03:20-03:25)

Protection and restoration are the mechanism to recover from network failure: their difference will be discussed in the following parts. (Refer slide time:  03:26-03:37)

Why we need protection and restoration is now clear: to recover from network failure, to prevent the lot of data loss. Now another point is what would we mean by prevent lot of data loss? The point is that we will be talking about protection at the optical level, at the lower, transport level, that is, the low physical level. Above  this  physical level, there are a whole lot of other layers like data link layer, network layer, transport layer, etc. They may have their own protection mechanism and they might be able to tolerate. In some cases, if such a protection is sort of implemented at higher level, such protection usually would tolerate a small amount of data loss, which they will retransmit or do something in the protocol to take care of that. We will be talking about this later on. Within that limit we do not have a 100% tight case, but we have something to play with. Within that limit, if the physical layer can come back up, then that is nice; then the end user who is sitting at the top of the application layer would not notice that  a failure has actually occurred. So this is our general goal and, of course, to prevent lot of data loss. (Refer slide time:  05:13-05:18)

To provide reliable communication service is a reason for having protection and restoration. (Refer slide time:  05:19-05:39)

Protection is the primary mechanism; this is fast and routes are usually preplanned ? we will come to this part later on ? whereas restoration is the secondary mechanism, used to provide more efficient routes or additional resilience, etc., over and above protection. We will see both of these later on. (Refer slide time:  05:40-06:11)

Techniques for protection: we could protect a path, which is called path protection; we could protect a link; that is called link protection. There are various schemes of protection: like 1+1 protection, 1:1 protection, 1:N protection, M:N protection. This depends on what kind of redundancy we have built into the network. So we will look at some of these techniques one by one. (Refer slide time:  06:12-06:26)

So what are the considerations and tradeoffs that we have for protection? For support for fast protection, time is dictated by the client layer. This is what I was talking about earlier ? in the client layer, whatever the higher layer we are talking about, all of them are clubbed together, and we are calling them client layer, let us say. In the client layer we may have some resilience; that means, we can tolerate some small amount of data loss. So depending on that, and beyond that, it will be taken that the service will fail. This is dictated by the client layer and that is the constraint within which we try to do our protection or restoration at the bottom layer, that is, the physical layer. We require some switching technologies (Refer slide time:  07:13-07:33)

as we will see, and how to implement protection. It could be through dedicated hardware or through software. Here we will only be talking about ? since we are talking about the physical layer ? hardware protection, where some hardware has been put in order to take care of this protection. (Refer start time:  07:34-07:51)

Support for low priority traffic ? that is another consideration we might have. So low priority traffic supported using the protection bandwidth; traffic dropped in case of a failure. As I mentioned earlier that in order to give some protection capability into the network we have to build in some kind of redundancy over there. Now when there is no failure, naturally the redundant link would be idle. What you could do is that if you have some low priority traffic, which could be dropped whenever there is a problem, under normal circumstances you can use your provision in the network to carry the low priority traffic, and as soon as there is a failure, where naturally the provision is going to be used up for providing protection, restoration, etc., the low priority traffic would be discontinued. So that is one thing we could do. (Refer slide time:  08:46-09:03)

Support for mesh topologies ? mesh topologies are bandwidth efficient, fast signaling mechanism, flexibility in choice of routes by preplanned routes, etc.  If we have support for mesh topologies, it is also nice. As  a matter of fact, if you remember our discussion about different types of topologies ? we have star, tree, ring, mesh etc., so these are the different possible topologies. Out of all these,  point-to-point connection, ring and mesh are the three topologies, which dominate most of the WAN. When you want to communicate between two points, you first set a point-to-point link. Now consider that link to be quite important. Then what you would like to do is that you would like to put in some kind of reliability over  there, by making it into a ring. For example, we discussed SONET. A SONET gear quite often is put in the form of ring, rings that would touch, mutually touching rings. For example, the telecom people put up all these SONET rings through fiber optic networks. The rings are quite easy to handle and we will discuss the protection mechanism in ring quite a lot. But if you go to a wider geographical area ring, it may not to be feasible due to various reasons. What you have is a mesh. A mesh, if you recall, is just a graph, where the nodes are connected in some fashion. It has to be a connected graph and actually later on we will see that for giving proper protection, it has to be not only a connected graph, it has to be biconnected graph as well. That means between the two nodes, which are communicating, there have to be two alternatives paths, which are linked somehow. Otherwise, you will not be able to give protection. Giving protection in mesh networks is also an important consideration. (Refer slide time: 11:17-11:55)

Other important considerations are maintenance of large distributed routing tables, that is, precomputed routes or up to date topology maps. So you have to maintain this dynamically, because a network is a very dynamic thing as links come up or go down or nodes come up or go down, this may have to be recomputed and stored in a distributed fashion. We would like to have support for all failure modes: node failure for mesh networks and for ring networks, so or it may be of course ring failures.  (Refer slide time:  11:55-13:05)

First, let us talk about path protection. It uses more than one path to guarantee that data be sent successfully. If you look at this graph, it will have a 6-node graph where 1 and 6 are communicating and on the top, through the dashed line, we show the primary path, which is the primary connection between 1 and 6. Now what might happen is that the link between 2 and 3 might snap due to some reason. We have already got a backup route, which is calculated going through 1, 4, 5 and 6. So we will channel our communication through the backup link. Please note that this backup or secondary path from the source to the destination does not share any link with the primary path. That is the requirement: if any link in the primary path fails, assuming there is only one failure, which means that the backup link is all intact and you can switch to the backup path for this particular communication.  (Refer slide time:  13:06-13:16)

Now path protection: there are various ways to protect a path, that is, various ways of provisioning the extra bandwidth capacity in the network so that you can give protection. You need to build in some kind of redundancy in the  network. So there are various ways you can build in the redundancy and the schemes may be divided as dedicated path protection, (Refer slide time: 13:40-13:50)  

or shared path protection. Dedicated path protection may be shown as 1+1 protection and shared path is 1:1 protection or 1:N protection. So we will look at this one by one. (Refer slide time:  13:51-15:31)

This is an example of 1+1 protection. So you have the source on the left, and then, you have destination on the right. So it is communicating. What you can see is that from the source, the signal is coming to the splitter. If you remember, the splitter is going to split it into two halves, may be of equal power or something, and then the same signal is been carried through two different links and over there, we a switch. The switch determines which signal is better and may be switches to that and a communication is going on. If that particular link, say, this   top one fails, it automatically switches to the other protection link. So this is some kind of hot redundancy we have. That means there is redundant source of information in the destination side; so if the primary one fails, the secondary one is already on. So protection would be very fast. The only trouble is that for each such path, you will have to give an alternate path, which is also being used at the same time. This is a dedicate path protection ? for this particular path there is a dedicated alternative path; although this is very good, it is costly. (Refer slide time: 15:32-16:57)

Now we come to shared mode protection; in the shared mode, the figure looks almost the same, expect if you note, this splitter on the left has been replaced by a switch. So what we have is we have a working fiber and we have a protection fiber. Unlike the 1+1 protection, this is not a hot standby. This is cold in the sense that the protection fiber, to start with, is not carrying any signal, let us say. The source is just simply passing through the switch, the signal from the source is passing through the switch, then down the path through the destination switch to the final destination. If the working fiber goes down, then this switch will flip and the protection fiber would be in place. Since this is not a hot standby, meaning that since it is not carrying any signal under normal circumstances, you could share this path with something else. You can use this to send some other channel or some other information, etc. So that is why this is a shared mode of protection. (Refer slide time: 16:58-19:35)

Generalizing this, we get this 1:N protection, where N line is sharing 1 protection line. We have the inputs from 1 to N lines, so these are the N sources, and let us say so many destinations. So they may be going ? 1- to 2- and N to N- and so on; this is the normal mode of operation. This part of the network is for normal mode of operation and each of them is connected to a switch over here. Another link from the switch comes to one bigger switch, so this is an N input port over here and then there is a single link from this switch to this switch, on the destination side, which again feeds to all the switches. What would happen is that in case there is a failure in any one, let us say, ith link from 1,2 to N, out of that, the ith source to ith destination, which is going through the ith link and the ith link fails, what this ith switch could do is that the ith switch could switch the signal from the ith source to this particular switch at the bottom and now the protection fiber would be carrying the signal that was flowing down ith channel over there and  then again it will feed it to the ith switch on that side. Of course these two switches have to communicate that the ith one has failed over here, so you switch it to your ith mode, or this switch may sense it that this line has gone down, so it will take signal from this line. This one protection fiber is been shared by these N working fibers, and as I mentioned earlier, when everything is fine this protection fiber could be carrying some low priority data. When everything is fine these N working fibers are actually carrying the most of the important traffic so some low priority data could be flowing down the protection fiber. As soon as there is any failure anywhere, the low priority data would be stopped or it will be dropped and then this protection fiber would switch to give the services between the nodes that have experienced a link failure. Just as we have a path protection that means for a path you try to give an alternate path, similarly you could do it at the link level also. In general, it may be more efficient or more proper to do that because if you have N nodes and if it is slightly large, potentially you have very large number of paths through the networks and for each path having an alternate path may not always be a very good idea. So we concentrate at the link level and for each link level we may give a protection. (Refer slide time: 20:21-20:43)

So use an alternate path if the link has failed. This is the primary where the link may have failed so I have an alternate path to that from 2 to 3, 2 to 4, 4 to 5, 5 to 3. This alternative, please note, is for the link from 2 to 3. We will have another diagram for that. (Refer slide time: 20:44-21:09)


Dedicated link protection is not always practical although sometimes we may have it; shared link protection is practical and it is quite often implemented. This link protection may fail because here you are only provisioning for the failure of a link, but if a node fails, then it may lead to some complication as we will see. (Refer slide time: 21:10-21:28)

To compare between path switching and line switching, path switching is a coarser scheme and line switching is finer scheme; and line switching can again be a span protection. Span may be several links together; that may be span or a line protection.
(Refer slide time: 21:29-21:57)

In mesh networks of course restoration is possible only if the graph is two edge connected i.e., biconnected, which means that there are two edge disjoint paths between any pair of nodes so that no single edge failure can disconnect the network. This is a necessity and we usually try to keep it that way unless its very difficult or very cost it?s not cost effective, etc. (Refer slide time: 21:58-22:42)

Protection in a mesh network is more complicated then a ring. Simple minded scheme would be two edge or node disjoint paths for each connection, 1+1. As is mentioned, it is not very efficient. There may be many paths and provisioning double the number of paths, which are pair wise mutually node or edge disjointed may be very difficult. That may be a lot of extra provisioning in the network. A better approach would be line protection, which of course has the problem of coordination. I will show that and protection cycles in mesh net later on. (Refer slide time: 22:43-23:24)

In the path layer and mesh protection, there is protection of mesh networks to protect the mesh at the single unit. Pre-computed routes means all possible routes and alternative routes are pre-computed. 1+1 protection is protection route per light path, protection route per failure. We will discuss this later but, as I said, this is a costly alternative. Or what we might do is that we can do on-the-fly route computation; that means it is not pre-computed. As there is a failure, centralized route computation and coordination route computation and coordination are done at end nodes or distributed route computations ? all these are possibilities. (Refer slide time: 23:25-24:03)

This is an example of mesh network, where let us say this is the primary path and this is an alternate path. This alternate path may be pre-computed or it may be computed on the fly when there is a failure. Similarly, from here to here this may be the primary path and this is an alternate path. Please note that this communication as well as this communication are going through the same fiber. Maybe they are going through different wavelengths or maybe they are combined together; so there are various way of handling this. (Refer slide time: 24:04-24:51)

Let us look at some diagrams: this is a mesh network. Naturally once again, it has the same 6-node network and this is the normal operation, that is, communication from this node, node 1 to node 6. Now there is link failure over here; what you might do is you might switch the entire path like this. So this was the pre-computed path for this path. You switch to that ? this is one possibility. Or this particular span, the span could be live from here to here or from here to here. That may have an alternative path, to which you switch. (Refer slide time: 24:52-25:50)

Or this particular line may have this alternative; that means from this node to this node. If this is the link, the alternative to this link is this section from here to here to here to here. It is trying to go through the same path, but then it takes a diversion when there is a link failure through the alternative, which is provided for this particular link. I have shown only one node coming into this but you can appreciate that this path may be very long so you do not have to re-compute the path over the entire length; rather locally, you can re-compute that for this particular link as an alternative. So the alternative may well be through some local nodes only. So this path just gets a slight diversion; that is line protection. (Refer slide time: 25:51-25:59)

We talked about protection cycles in a mesh network, now for protection cycles, what we do is that for each of the paths we try to form a cycle ? cycle of  provisioning of light paths, let us say.  The point in the cycle is that suppose we are going through some arc of the cycle and if some link in-between breaks, you can always go in the other direction. So you try to form these cycles in the mesh which you keep ready and pre-computed. So whenever there is a failure you can switch. You can see here (Refer slide time: 26:38-27:03)

there are pairs of a fibers going in both the directions and there you can form  cycles over there. One of them would be protection edge, maybe the inner one; and one of them would be the working edge. If one of them fails, the other will automatically take over; the other direction will automatically take over. (Refer slide time: 27:04-27:25)

So this is another example of a network with both working and protection fibers.  The working fibers have been shown in solid lines whereas protection fibers have been shown with dashed lines. Once again you must realize that you may not provide the same level of protection to all paths or to all parts of the network; depending on which part you consider more sensitive or more important, you may put your protection there. A mesh network may be partially protected. Some of the parts may be protected or some of the parts may not be protected. (Refer slide time: 27:51-29:53)

So some more cases: line protection in a mesh network. What we have is a unidirectional light path from node A to node D; so from node A to node D we have an unidirectional light path going may be like this through nodes B, C, and E. We are talking about this path A to B to C and then to E. Now after the link BC fails, the light path is rerouted by nodes B and C along the route A B F E C E D. The unidirectional light path was going from A B C E and then to D; so this was the path A B C E D. Now BC has failed; so A B F E C E D. You can see what has happened is that there was no point in coming from E to C and then back from C to E. What has happened was that we were doing line protection; that means, for BC my protection was from B to F to E to C because I wanted to go from B to C so I am going from B to F to E to C and so now BC has failed. So that is what I do and from that point I continue wherever I was going and actually I was going from C to E to D, so once again from C, I go back to E and then to D. Such a thing is possible because we are taking a local decision; that means, for this particular link what to do in the case of failure that has already been pre- computed? We are not taking a global picture; here I have shown you a very small graph so you can immediately see the entire global picture with your own eyes. But for local nodes we may not have the global pictures where this node is coming from and where this path is finally going to. So all the intermediate nodes may not have the global pictures because if all of them had the global pictures maybe they could have computed a better path but this is for line protection.  (Refer slide time: 30:41-31:57)

Now line protection in mesh network: here what might happen is that erroneous connection due to the failure of a node is being treated by its adjacent nodes as link failure. This is one case of the so-called race condition. What might happen is that node 1 has failed; so what would 6 do is that it may assume that the link from 6 to 1 has failed, whereas 2 may assume that the link from 2 to 1 has failed. So both of them perceive as two different failures, so they take some local decisions, and it might lead to a funny situation where you are going in a cycle. Maybe other kinds of things may happen after node 1 fails ? node 5 gets connected to node 4 after node 6 and 2 invoke line protection independently. If they perceive the same failure, the actual failure was that of node 1; but if they perceived differently as failure of  link 6 1 and failure of link 2 1 and they have independent actions, it may lead to race conditions. (Refer slide time: 31:58-32:14)

The advantages and disadvantages of protection: we will also be talking a little bit of restoration. Protection is simple; it?s quick; does not require much extra process time ? and this is the important part, since this is quick. As I mentioned earlier, there will not be a lot of data loss. For example, a SONET ring would sort of come back up from a failure in a less than 15 milliseconds. So that is a benchmark; you are back in action in less than 15 milliseconds, so whatever little you might have lost during that time would be taken care of by the higher layer protocols. (Refer slide time: 32:51-33:08)

But usually they can only recover from single link faults. If there are multiple link faults, all kinds of funny things may happen. There is inefficient usage of resource, because protection needs a lot of resources, even if we are sharing them.
(Refer slide time: 33:08-33:19)

Dedicated protection needs even more resources; we talk about path restoration and link restoration.
(Refer slide time: 33:20-33:30)

What we do is that we compute the path after the failure and the resource is reserved and then used. So in restoration what you try to do that is that you try to look at the current actual situation. You try to have some protocol for keeping track of the present situation and then you compute some root and then you reserve the path and then restore the original service. This has got some software parts, so some data has to be processed, etc. This usually takes more time, so this has a hardware as well as software aspect to do it .(Refer slide time: 34:00-34:25)


The path is discovered at the end nodes of the failed link; but this is more practical than path restoration. We have both path restoration and link restoration. Path restoration means the path may be long and to find out an alternative, it may be more difficult, whereas links are between the adjacent nodes so they may quickly find an alternative. (Refer slide time: 34:26-34:53)

Advantages and disadvantages of restoration are the following: usually it can recover from multiplex element faults because you are sort of having some protocol to exchange information and then find the current situation and form the alternatives, etc. There is more efficient usage of resources; it is more complex; it is slower; it requires extra process time to set up path or reserve resources. (Refer slide time: 34:54-35:19)

So for comparison between protection and restoration, in protection the resources are reserved before the failure; they may not be used. In restoration the resources are reserved and used after the failure. So this is the main difference between the two. Route: in protection it is predetermined, in restoration it can be dynamically computed. Resource efficiency: in protection it is naturally low and the restoration is comparatively high. (Refer slide time: 35:20-35:55)

Time used for protection is short; for restoration it is longer. Reliability: protection is mainly for single fault, whereas restoration can survive under multiple faults. Well, it is not that it can always survive under multiple faults; it depends on the where the faults are, but if it is survivable after multiple faults, it will take global view and say that ok, still I can give the services. So restoration will take care of that, whereas in protection it may not be possible to handle multiple faults. Implementation: protection is simple and restoration is naturally more complex. (Refer slide time: 35:56-36:21)

For optical networks, we not only talk about physical links we talk about virtual wavelength paths. So in light paths also routing can be centrally controlled or distributed; resource reservation, forward reservation, as well as backward reservation are done as we do in optical networks. We will talk more about this later. (Refer slide time: 36:22-37:15) 

Now let us come to this all-important topic of fault management in ring networks. As I said, ring networks are very ubiquitous in WAN. All the telecom people will love these things because naturally it allows them to give very high level of service. So we have so many rings; these SONET rings are very common, and  rings are a very common kind of topology. One of the chief attractions of the ring topology is its capability to allow some kind of protection and restoration with some redundancy in-built, as we will see. We will look at two different cases, unidirectional path switch rings and bidirectional line switch rings or UPSR and BLSR. (Refer slide time: 37:16-39:22) 

This is a diagram of UPSR. We have AB, means A to B and BA, means B to A. First of all you see that we have these two rings ? we have this working fiber as well as a protection fiber. Working fiber is going in only one direction and protection fiber is going in the other direction, in the counter clockwise direction. So for connection from A to B and from B to A ? this is A, and this is B ? my working fiber is going in this direction. A to B is really going in this direction; B to A is coming in this direction. So A to B, B to A: this is the protection fiber part, A to B the working fiber is going in the other direction. A to B is going like this and B to A is coming all the way like this. Let us say the outer one is a working fiber, it is through the outer ring. For A to B, this is the path; and B to A, this is the path, whereas for protection purpose ? and please note that this is a 1+1 scheme; that means for this the alternative is already provisioned and maybe it is something like a hot standby. So A to B is through the protection fiber and now this path from A to B is going via D and C, so A D C B: that is A to B and B to A. The protection path is from this through the protection fiber. So if there is a failure anywhere, this can still continue, so this is a unidirectional path switched ring. (Refer slide time: 39:23-40:29)

There are some limitations of UPSR. It does not spatially reuse the fiber capacity; so what is happening is that since this is unidirectional, what is happening is that even if two nodes are side by side, if they are in wrong direction then it has to come all the way through and naturally all those links ? even if we are talking about particular ?s I mean WDM systems ? in the ring are entirely sort of covered by this. Otherwise even if we are talking about a WDM system what is happening is that at least one ? around the whole ring is getting occupied because two side by side things want to communicate. So it does not spatially reuse fiber capacity and if there is some, you could use it for some other purpose. That is not possible in a unidirectional ring. Each bidirectional SONET SDH connection uses up capacity on every link. That is the thing; if you look at the previous picture (Refer slide time: 40:30-40:44)


we are just talking about a connection from A to B and B to A; that?s all, so this entire ring now has been used up assuming only one wavelength. The entire ring now has been used up; these ADMs of course are the ADMs of SONET. (Refer slide time: 40:45-40:50)


So it is efficient for lower-speed access networks, one to multipoint only. (Refer slide time: 40:51-41:27)

And the other point, which may become a problem, is that delays are different for the two paths because one of them is small and the other one is quite large, all the way around the link. The relays in the two paths are quite different. A remedy could be bidirectional lines; that means, bidirectional line switched rings or BLSR. So we were in UPSR, now we will be talking about BLSR, bidirectional line switch rings. This provides spatial reuse capabilities and additional protection mechanisms and adopts span as well as line protection. We will have a look at both of these. (Refer slide time: 41:28-42:26)

This is  a four-fiber bidirectional line switched ring; so once again we have two working fibers, say, the outer two ones and two protection fibers or the inner ones. So we actually have four fibers. This is a four-fiber system and it is going, so we are using two such working fibers. One of them is going in one direction the other is going in the other direction, so this is bidirectional. We can communicate both ways, so if A to B and B to A both can communicate on the working fiber in this part at all, the rest of the space can be used for other communication. So spatial reuse is much better for bidirectional line switched rings, and the protection fibers are there because if one of them fails then the protection fiber may take up. (Refer slide time: 42:27-43:10)

This is a span protection so we are talking about bidirectional traffic supports, maybe 16 nodes or some distances, etc., from A to B. You may pre-compute a span that if this span goes down what is the alternative span. If this is the case of line protection, if this line fails what is the alternative line? Now the alternative line of course may go in the other direction, depending on what the failure is. So there are many different possibilities with this BLSR. (Refer slide time: 43:11-44:24)

There is also two-fiber version of BLSR, namely BLSR/2. Here both the fibers are working as well as protection fibers. That means if one of them fails the other one will give the protection in the other direction of the ring. If you note, the rings are going in the opposite directions and there is a line failure in both of these working as well as protection fiber. That means A to B is communicating in this direction, B to A is communicating in this direction. But if one of them fails, the other one will give the protection by going in the other direction. In that case we have to go all the way round, then you will have to reserve the resource and if there is something already going on here, you may be blocked or this may have to be dropped and so on. So naturally you have two fibers, we have less flexibility. But with two fibers we can get this bidirectional thing going on. (Refer slide time: 44:25-46:19)

Comparison of different types: these are all self healing rings. What we mean by self healing is that the nodes, these ADMs and the SONET, etc., are programmed in such a fashion that as soon as they sense a failure they know what local action to take and how to adjust the switch internally, so that it automatically switches from the working fiber or from the working span or working fiber or whatever through the protection side. That is why these are self healing rings, so they heal automatically. And as I said these SONET rings really do this in less than 15 milliseconds; the entire thing will again be up. So we can compare the three: UPSR, BLSR/4, and BLSR /2. For example, fiber pairs 1, 2 and  transmission receiver pair 2 4 and 2 spatial reuse in UPSR it is none, in BLSR it is there, and in BLSR/4 it is there. Production capacity is equal to the working capacity. Link failure is path protection in the case of UPSR or span or line protection in the case of BLSR/4. In case of BLSR/2, it is only line protection because it will have to go in the other direction. Node failure: it is path protection, line protection and line protection restoration is faster in UPSR; somewhat slower in BLSRs and restoration speed is this. Node complexity is low, high and high. Another thing we talked about is the dual homing. (Refer slide time: 46:21-47:50)

By dual homing we mean that suppose we want to deploy your network in such a way that it is very mission critical and no failure is acceptable and we want a hot standby. So what you might to do is that you may get connected through two different hubs and what you want to do is that you want dual home to these two hubs and these two hubs take independent paths to your destination. This shows a dual homing to handle hub on node failure; so we have these four ADMs: once again four nodes, let?s say A B C D. So the end node is A and these are the two hubs, B and C. What we want is that not only some link failure but even if one of the hubs fails, I should still be able to communicate. So what you do is on this ring you communicate with hub 1, let us say you are communicating with hub 1 through this A D C B and you are communicating with hub 2 through A B C. As we will note, even if one of the hubs fails, you can still communicate through other one. So this is another kind of  protection. (Refer slide time: 47:51-48:14)

And finally I have mentioned this point before ? just a reminder that a network consists of many layers and each layer may have its own protection mechanism built in, independent of other layers. So there are both advantages and disadvantages to this. We have already talked about the advantages. (Refer slide time: 48:15-49:51)

This is an example of a WDM link carrying SONET traffic. So there is a WDM link, so there is a SONET ADM. This is a working fiber pair and protection fiber pair. Please note that the pair has been shown as one line over here because usually as you know that fiber optic line is a simplex line; that means it goes in only one direction; there is a source in one side and the detector on the other side. Usually these fibers always come in pairs. The other side is for the communication in other direction so we have a working fiber pair over there and protection fiber pair through this WDM link. Look at this 1:2 protected scheme. What is happening is that there is one protection fiber pair, there may be protection fiber pair and through the working fiber there may be multiple virtual links going through the working fiber using different wavelengths. A is a normal operation and B link is cut and the traffic is restored by the optical layer. That means you automatically assign new wavelengths and new paths through the fibers etc., to bring it up. So this you may do at the optical layer rather than at the electronic layer. (Refer slide time: 49:52-51:58)

But the case we are talking about here is that the SONET is riding on the optical layer. So we have the optical layer at the bottom then you have a SONET on top of it. On top of SONET also there will be the data link layer and then so on all the other links. What we are saying is that each layer may have its own protection mechanism. So for example I mentioned that the SONET will also have a protection mechanism of its own. So if a SONET LTE, i.e., the SONET line terminal equipment, senses that it cannot communicate to the next LTE, this will automatically reroute the traffic and try to reroute the traffic in the other direction.  Sometimes it is good to have multiple protections at multiple layers but what might also happen is that they might sort of cancel each other or they might go into a race condition. So these are the disadvantages of having protection at various layers. There could be some disadvantage also if they are not very well coordinated, which usually would not be because these layers are sort of independent of each other. They talk only to their peers and go through their own protocol and give protection. Apart from these of course the other disadvantage is that you may redo some part of the protection unnecessarily. You may be unnecessarily duplicating the work at various places. So these are the disadvantages of having protection at multiple layers. (Refer slide time: 51:59-52:34)   

What is the advantage of optical layer protection ? speed and efficiency. Limitation would be detection of all faults may not be possible; protects traffic in units of light paths. So this is another problem. As I mentioned, in light path the granularity is very coarse. It may be 2.5 Gbps; so you are really giving the protection at a level of granularity, which may be quite high as I just now  mentioned; it could lead to race conditions when optical and client layers both try to protect against the same failure. (Refer slide time: 52:35-53:28)   

Of course, on the optical layer you have one more dimension to play with, which is in the case of a WDM. That means you have different wavelengths, so instead of 1+1 link, you can talk about 1+1 wavelength path selection. You can sort of try to select two independent light paths and the signal is bridged on both protection and working fibers if you are doing 1+1 protection kind of thing; the receiver chooses the better signal. In case of a failure, the destination switches to the operational link that is operational light path; there is revertive or non revertive switching; that means, if the original link comes back, it may revert or it may not revert; and no signaling is required. (Refer slide time: 53:29-53:30)

So that is the unidirectional light path. I have just shown you some of the schemes which are used for protection and restoration. And as I mentioned, the schemes may be partially deployed and some of the parts may be protected and some of the parts may not be protected  and so on. But the essential idea is the same ? that you build in some kind of redundancy and the redundancy may be in the form of entire fibers or the redundancy may be in the form of light paths or wavelengths. It may be pre-computed and pre-reserved like 1+1; it may be pre-computed but not pre-reserved like in a shared one or it may be computed as and when the failure occurs; that is, in the case of restoration. So there are various approaches to it. Depending on how critical the problem is or how critical the application is, what is the cost and how much extra provisioning you can do, you can choose your own way of protection and restoration. Thank you. 

Preview of the next lecture
Lecture ? 13
Multiple Access
Good day so today we will talk about multiple access ok now what is multiple access  (Refer slide time: 54:59-57:54)



If you remember  that we had  seven layer in the so called OSI stack the top on being application then we have presentation session transport network link and physical ok  we had been mostly talking about the physical layer till now although  for optical networks we sort of ventured into some of the hired layers  but from this lecture onwards we want  to concentrate on  the link layer ok now  what medium access does is it coordinates  competing  request request for what for medium that means that there is a medium  which may be an object of contention meaning that I mean several nodes may want to use it and  this medium access control protocol has to do with how to handle that so sharing of link and transport of data over the link that is an general the description of what the data link layer does  so when we share a link there is a question of committing request and we have to have some way of reserving that  and of course there is a also question of transport of data over a link  link if you remember when we say a link I mean that two nodes which are connected  the nodes may be or computers routers switches etc they are two networking nodes they are directly connected now the when  I do like this  it might mean   cable copper cable or a  fiber  or it might mean a shared medium like the free space ok  so there is a but there is some way of communicating directly between these two nodes that is what we mean by link that means that is just one hop in the network that is what we are talking about in the data link layer so there is a question of reliable transfer of data over these link and if this link like when you have a free space transmission  with so many nodes in the network now so many people would like to transmit so there is a question of sharing this medium and there is a question of who would access it when and just as I said free space could be a shared medium similarly  if you remember that if you have some kind of a bus  if you remember our discussion about topology of networks when we have some kind of a bus  from which the number of nodes are hanging and  that bus  may also be an object of contention so that bus is the medium through which communication is taking place and there is an  there is some kind of  competition or sharing between the of this shared medium between the nodes so we have to handle that that is the other thing  so examples of contention based or ALOHA and slotted ALOHA (Refer slide time: 57:57-59:00)



 these refers to some protocols which we are used in satellite communication so we will discuss these and we talk about satellite communication we have CSMA  it stands for  carrier sense multiple access or CSMA CD which is carrier sense multiple access with collision detection there are other variance of these like carrier science multiple access with collision avoidance  and things like that  so these ALOHA slotted ALOHA for satellite CSMA CSMA CD or specific specifically CSMA CD is used by Ethernet  in many situations and then we have   CSMA CA may be for cellular communication etc so these are contention based MAC and  Round Robin there are these are  token based protocols and so two very common ones are token bus and token ring  so actually we will discuss these  in the next in this lecture as well as the next we will be discussing these 





COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture-18
Satellite Communication (Refer slide 1)

Good day! In this lecture we will be discussing one very important component of world wide communication, which is satellite communication.
(Refer slide 1)

Slide 1

Communication through satellite has been going on for quite sometime now.   Satellite Communication is fairly mature technology although some improvements are taking place. Satellite Communication is an important component of world wide communication infrastructure. We will also see how specifically MAC, which is Media Access Control, is handled in Satellite Communication. This is important, since we obviously need MAC because the communication medium is the electro magnetic field around us, which is common to everybody. (Refer slide 2)


Slide 2
 We now talk about satellite communication. Satellite can be looked upon as a big microwave repeater. Repeater is something, which repeats the signal. It takes in the incoming signal, amplifies it and then sends it back. It contains several transponders. Transponders listen to some portion of the spectrum. Each transponder is listening to some portion of the spectrum so that several transponders together can listen to the wider band of the spectrum. It amplifies the incoming signal and broadcasts in another frequency back to earth. Satellites are up in the space; they take the incoming signal, amplify it and broadcast it back. So satellite has to broadcast in a different frequency so as to avoid interference with the incoming signals. (Refer slide 2)


Slide3
This can relay signals over long distance ? this is one strength of satellite  communication, because, if you send it all the way up and then when it is sending all the way down, and if it does so at an angle, the signal can reach a very long distance. There are different kinds of satellites and the most commonly known ones are  geostationary satellites because they are above the equator at a distance of 2300 miles approximately and are in the geosynchronous orbit. They travel around the earth in exactly the time the earth takes to rotate. (Refer slide 3)


Slide 4


We have an uplink station from where some signal is going and then it is being listened to by one particular transponder in the satellite. It amplifies signal and sends it back to earth where the downlink station (another dish) which is facing the satellite and tracking the satellite receives the signal over here. This is the just reflected part which then gets on the horn. This signal is taken down and amplified. Then we can use it. (Slide 4)


Earth stations communicate signals to satellite on an uplink. They	 are	called earth or base stations on the ground. The	satellite then repeats these signals back to the down link. This feature is attractive for distribution of TV	programmes. It is known that many of our television channels(unless coming through cable) are coming	from satellite, which is beaming back  the signal over a very wider area and all the receivers around can receive the signal and then amplify it and use it. So it is very popular for such kind of applications like distribution of television programming.

Slide 5
There are other applications. These signals are used to transmit signals and data over long distances for weather forecasting, television, internet communication and     global positioning system. These are the various applications of a satellite. The space orbit allows more surface coverage (Refer slide 5)

Slide 6
The spectrum is usually used by the satellite and is divided into sections; each of these bands have names like C band, Ku band or ku band and Ka band or ka band. The C band may have four downlinks and six uplinks; C band is from 3.7 to 4.2 GHz. This part of the spectrum is reserved for the C	band communication through satellite; that is, 3,7 to 4.2 GHz for downlink and 5.925 to 6.425 GHz for uplink. There is a 0.5 GHz  bandwidth for downlink and another 0.5 GHz bandwidth for uplink.
Since the uplink is at a higher frequency, you can have more channels over there. The capacity is not very high by today?s standard, especially, if you compare with fiber, this is really low. But at the same time, at one point of time, when transoceanic fibers were not there, satellite was the chief medium of communication across continents.	The capacity is low but still it is useful; however, terrestrial interference is a problem because when the weather is bad or when there are other kinds of extraneous sources of some electromagnetic noise, etc., these interfere with the signals. But still it has got a lot of strong points; that is why it is still an important component of communication these days. (Refer slide 6)




Slide 7

Next set of transponders come in Ku band. They can accommodate greater number of transponders ? 12	on the	downlink to 14	on uplink. Rain interference	is the problem here. Ku band is from 11.7 to 12.2 GHz and 14.0 to 14.5 GHz. In this band, rain is a problem though it has higher capacity and is less crowded than C band. C band is very crowded, if you consider people with a sort of dish antenna for receiving satellite signal ? I am not talking about the base station ? those using 1-meter antenna are possibly using C band. For Ku band, the antenna size is smaller and pizza shaped at something like 18 to 20 inches. They have a higher bandwidth; however, rain and terrestrial interference are the problems faced in C band. Different parts of electromagnetic spectrum are most susceptible to interference and noise, etc., but Ku band has higher capacity. The Ka band is at an even higher frequency ? 19 downlink and 29 uplink transponders are needed. The equipment needed for this is quite	expensive.	It is from 17.7 to 21.7 GHz and 27.5 to 30.5. It covers   a greater bandwidth, but it is still not very widely used.

Slide 8
Satellite can be used for point-to-point transmission, to transfer large volume of data; voice data and communication and for video conference. Satellite is not just for broadcast; satellite can be used for point-to-point communication. You may look it as point-to-multi-point communication. So we can say that point-to-point and point-to-multi-point communication are supported. Also, standard broadcast is supported. Point-to-multi-point communication can be data communication, internet, and video conference. Broadcast services include television. (Refer slide 8)

Slide 9

The advantages are that you can reach a large geographical area and have a high bandwidth; it is cheaper over long distances. It is certainly cheaper than wiring it up, but if you are covering long distance, because of its inherent high capacity, the economy is in favor of the fiber optic cable and it is almost even in some cases. But in many situations satellite retains its advantage. It can transmit to places where the cable cannot reach. These are applications where satellites have an undoubted advantage over any wired kind of system. It may be a remote area, which is not very easily reachable and road or other kind of communication is not well established to take cable over there. But the satellite, since it is sitting high up in the sky, can cover any area without any difficulties. Hence this is a very strong point in favor of satellite. It is especially useful for technology deployed at multiple sites regardless of location, like mobile technology. Nowadays the first thing people think about to be mobile is the cellular phone, which uses the nearest base station, which is usually wired. But in some places where it cannot be wired, it may go through  the base station and may be connected to the backbone through a satellite. That is one thing; and if you want 100% roaming all over the earth wherever you go, then you may not have a base station around the place where you are. In that case, some satellite or the other would be visible to you and you can communicate through the satellite. So mobility and accessibility to places which are difficult to reach otherwise are very strong points of satellite and of course standard communication is also a point.

Slide 10


The disadvantage is the high initial cost. You have to build the satellite and then to put it up in the space through some launch vehicle involves high investments. But  laying fibers all over is also costly so we have to work it out on a case by case basis and determine which one comes out to be better. We cannot do much at the physical level about susceptibility to noise and interference because the physical media is such that all kinds of noises are being generated all over the place and crowding this shared medium. So we have to handle it in some other way at a higher layer. Then there is a propagation delay; and this is a significant disadvantage compared to terrestrial communication for many satellites, especially the geostationary satellites. We will come back to this point later on. Geostationary satellites are thousands of miles above earth. So although your electromagnetic signal is travelling at the speed of light which is very high but even then to go all the way up to the satellite and then come all the way back down takes significant amount of time and this has lot of implications. It has the implications on the MAC and it has the other implications like delay and may be quality of service in some cases so this is an issue, a potential disadvantage and security can also be an issue since this medium is open to everybody. Whatever you are communicating, anybody else can listen on to it. If you are trying to send some very sensitive data through satellite and you do not want other people to listen to what you are sending, then you have to take some other measure like encryption, etc. We will talk about encryption much later in the course. (Refer slide 10)


Slide 11


There are different types of satellites and they have different types of orbits, circular or elliptical orbits. The circular orbits will centre at the earth?s centre. There is the   elliptical orbit with one foci at earth?s centre. There are some equatorial orbits above earth?s equator. This is quite common and necessary, especially for geostationary satellite. It is necessary that they go around the equator but then you could also have a satellite pass over both the poles. Other orbits are referred to as inclined orbits. (Refer slide 11)


Slide 12

The altitude of the satellites and their distance from the earth have significant implication in terms of the time it takes for the signal to travel. We have three different classes of satellites: geostationary orbit satellites, GEOs; medium earth orbit satellites, MEOs; and low earth orbits, LEOs. Out of these, GEO is the most common one.  (Refer slide 12)
GEOs go around the equator and have a high bandwidth. But they also require high power ? this is an important point ? and long latency. These are the important issues. When you are trying to communicate with some satellite, which is very far away, then your transmitter has to be strong enough so that the signal reaches the satellite. If the transmitter power has to be large then you require lot of power. In order to power this, it may be difficult to power it with a battery. That kind of battery power may not be enough to reach a geostationary satellite. The mobility becomes difficult from this power angle. So this is an important issue. (Refer slide 13)




Slide 13



MEO has a high bandwidth; medium kind of power; and medium kind of latency. LEO has low power and latency, but you require more number of satellites. They have smaller footprint. We have VSAT, which means very small aperture. When we talk about VSAT, we are not actually referring to any satellite. We are referring to the ground equipment that we use and VSATs have small apertures, which private WANs can use with smaller antenna. But if they are using C band or Ku band they use antenna which has a dimension of either 1 meter or 18 inches. But for the main base station of the satellite, usually a much larger antenna is put in place. We will first talk about geostationary satellite.
This is the most common type of satellite today and it is in a circular orbit. We say it in miles about 23,000 to 22,000 odd; and in kilometres it is 35,000 odd kilometres above the earth in the equatorial plane These satellites remain in the same position over the earth as it rotates.
(Refer slide 14 and Slide 15)


Slide14 and 15


Slide 16
As the earth is moving, the satellite is moving along with that. From some point on the earth, it would appear that the satellite is stationary over its head at all times.  That is why this is so sacrosanct about this geostationary satellite, this distance of 35,785 kilometres. You can always calculate this distance by finding out about the centripetal force and the earth?s gravitation etc. You can put it in that equation and calculate that this is the exact distance at which, if you put a satellite with a particular angular velocity, which is the same angular velocity as that of the earth, what will happen is that to the people directly under it, it will appear as if the satellite is stationary. So this is the good thing about this geostationary satellite and this is why this distance is so fixed. (Refer to 16)



Slide 17

From such a long distance, if you send a beam which has a reasonable solid angle over here, it will cover a large portion of the earth. As a matter of fact, it has been  calculated that with three or four satellites you can cover the entire surface of the earth; but of course, three or four satellites would not be sufficient to handle the bandwidth that we require these days. Furthermore since this distance is so fixed, there is only one band in space where the satellites can be parked. The other point is that if two such satellites are very close to each other, the signals will start interfering with each other. So there has to be a minimum distance between two geostationary satellites. The geostationary orbit and these parking slots are internationally decided, but these parking slots are quite crowded. Some nations may not put the satellite but would have reserved some parking slots. These parking slots are quite crowded today with so many geostationary satellites up in the space. (Refer slide 17)


Slide 18


These have a  coverage of about a fourth of the earth and have good tracking properties. That means you can track it very easily but their signal weakens over great distance and the propagation delay can be large. The propagation delay we are talking about is of the order of 0.24 seconds. Usually we talk in terms of milliseconds but here we are forced to talk in terms of seconds. It may also be hard to get coverage at the polar regions because this geostationary satellite has to be over the equator; it is difficult to get coverage at the far northern and southern hemispheres. (Refer slide 19)



Slide19

GEO satellite provides universal connectivity in its footprint. Its footprint means the area of earth, which is covered by one particular transponder, let us say on one particular satellite. So it covers all that area at the same time. So now within its footprint it covers universal connectivity. From any particular point, you can communicate with the satellite and possible satellite parking slots are quite crowded. Wide beams are circular, whereas spot beams, which are more focused, are elliptical. Apart from broadcasting, they may also be used with VSATs for point-to-point communication. We will come back to this point later. (Refer slide No.20).

Slide 20
So they are at this kind of distance that requires large transmitter power making them large and expensive. There is considerable space delay and large cell size, which means smaller number of channels. This is one particular point and we will come back to this point in greater detail when we discuss terrestrial wireless communication. But the point is that a satellite has some amount of bandwidth which is assigned to it which it can handle; now within that bandwidth for communication,  namely, the voice channels require about 64 kbps rate. One particular transponder can handle 800 such channels. If the footprint of the satellite is very large, that means it is  covering a large number of people. But all these large number of people are constrained with those 800 channels. A large cell size also necessarily means a smaller number of channel density on earth. (Refer slide 21)


Slide 21
A typical satellite has 12 to 20 transponders, each with a 30 to 50 MHz bandwidth; a  transponder can carry about 800 voice channels. FDM, Frequency Division Multiplexing, was used in early satellites. Nowadays TDM is also used; as a matter of fact, a mixture of TDM and FDM is also used. The cost of transmission is independent of distance. Sometimes this is an advantage. When you are communicating via a communication satellite with somebody who is, say, 10 kilometres away or  with somebody who is 100 kilometres or 1000 kilometres away, the cost is constant over the entire footprint. That may be an advantage in some cases.  (Refer slide 22)


Slide 22
For some applications this may be useful. Security and privacy pose a problem as mentioned earlier. So encryption is essential. Mobility is easily achievable and setup time is not required. If the satellite is in space, then you do not require any setup time. If the satellite is not there, you have to send it there. GPS is an interesting application we will see. (Refer slide 22)


Slide 23


We now come to the middle earth orbit satellites. These are used for global wireless communication coverage. They maintain orbit about 8000 miles from earth. The moment you come out of the geostationary orbit, the satellite cannot remain stationary over somebody?s head any longer. Now the satellite will necessarily move over your head so in MEOs and LEOs what will happen is that you will find that the satellite is coming over your head just as other stars move. So the satellite will move over in the sky and it will be there for sometime and then it will be out of your reach because it will go down below the horizon. This is what is going to happen with MEOs as well as LEOs. How soon will they come back depends on how far they are and what is the speed, etc. So it orbits around earth once in about every 2 to 12 hours depending on its parameters. So more satellites are needed and some handoffs are required as the satellites orbit now. What is this handoff we are talking about? You are communicating through the satellite to somebody else but the satellite has moved away. So now the communication link will break. In order that the communication link does not break what will have to happen is that another satellite will have to come and take its place and there has to be a handoff of communication from this satellite to this satellite before this satellite disappears altogether so that this communication between the two end points can continue. So these handoffs are required as satellite?s orbit and transmit data rate at 9.6 Kbps to 38.4 Kbps. Transmission delay is less than 0.1 seconds. So this is considerably less than the 240 milliseconds we had earlier. (Refer slide 23)


Slide 24
Taking it still further down, we have  the low earth orbiting satellites, launched like a large flock of birds. You require a lot of satellites because each of these satellites is visible to one particular point in earth only for a short duration of time. So in order to give continuous coverage, you need a large number of satellites. This cell size or the footprints will also become smaller. So in order to cover the entire earth, you need a very large number of satellites orbiting at constant altitude of 400 to 1000 miles. They must travel very fast to avoid gravity forces because in that case they will fall down to lower orbit, which allows for  transmission of the 2.4 to 9.5 Kbps. It travels at 17000 miles per hour, circles earth approximately every 90 minutes. So this goes around very fast. It is used for mobile voice low- and high-speed data; internet access via mobile phones and PDAs and GPS, etc. So the low earth orbiting satellites are what are used for the so-called SAT phones. It is dual linked due to some reason. Since this is low earth orbiting, the power that you require to reach the satellite is much less. So now you have a handheld device, which may be a little bigger than your standard cell phone, but not very much bigger. You can still carry it in your hand and with that now you can phone from anywhere from the earth because you are just communicating through the satellite. (Refer slide 24)


Slide 25
Low earth orbiting satellites are individually cheaper.  They also give lower space delay, which is much less because we are that much nearer. However a large number of satellites need to be deployed. As the satellites keep moving, ground stations which are communicating, will have to switch from one satellite to another and quick handoffs will be required. (Refer slide 25)


Slide 26
One of the examples was the IRIDIUM set of satellites. There were 66 satellites, which offered mobile telephony, paging and data communication. Unfortunately by 1990 they went out of service. What happened was that, SAT phone services became very costly. By that time, other kinds of technology like fiber technology and the terrestrial wireless technology developed to such an extent that a major part of the market was captured by those, which were giving the same service to the end user at a much cheaper rate. Furthermore we require 66 of them, all of them moving very fast and you have to have this complicated handoff from satellite to satellite to make it low enough so that you can communicate from a handled device etc. This could not be supported in the market since there are very few people going to such remote places where there cannot be any other communication infrastructure. Majority of the users would have some base station for some terrestrial wireless nearby and that is why this price became very uncompetitive. There were some efforts to revive this company,  but finally it did not work out and they became bankrupt. At present there is the ambitious ongoing project called TELEDESIC in LEO, which includes 288 LEO satellites to provide low-cost high-speed internet access, networking and teleconferencing across the globe. (Refer slide 26)

Slide 27
Here, as I mentioned, the space delay is lower and becomes comparable to terrestrial lines in the L band. It is possible to communicate with it using battery powered handheld devices. However cell sizes are still too large compared to terrestrial cells. (Refer slide 27)


Slide 28
IRIDIUM system had 66, so each satellite was to have 48 spot beams, giving a total of 1600 cells, each with 174 channels. But 174 channels or total of 183,000 channels are not so many globally because terrestrial service providers give millions of channels over the area. So finally, the cost turned out to be too high and the project went bankrupt. (Refer slide 28)


Slide 29


Now we talk about VSATs, which are small terminals with about 1-meter antenna and 1 watt transmitter power. Often the downlink capacity is more than the uplink speed for point-to-point communication usually goes through a hub. If two persons want to communicate the communication will go from here to the satellite and from the satellite to a central hub, which will have a very big antenna and it can handle all the MAC part of it. We are not going into the details of this but anyway this will go to the hub and from the hub again it will go to the satellite and then go the point B. Let A be the sender and B be the receiver. So it goes in two hubs; so the space delay is doubled because we are going in two hubs, sometimes with the help of the satellite MODEM kind of thing, you can go in one hub. So those channels are more costly. Otherwise, with the usual two hubs, there is considerable delay of the order of a half a second for this communication between A and B, but then there is no setup time.  So various combinations of TDM, TDMA, FDM, and FDMA are used for handling the MAC and all this is controlled by that central hub. (Refer slide 29)


Slide 30						As I mentioned, one important application was the GPS satellite constellation. This is a global positioning system and this is operated by US Air Force. Another GPS system is on the drawing board; it will be deployed by some other consortium. So there are 28 satellites in this and it has six orbital planes at a height of about 20,200 kilometres. Since this is not the geostationary orbit, they keep on moving and a minimum of five satellites are visible at all times. So with this what you can do is that you can locate any position on the ground, if you have a GPS terminal somewhere. A very common application of GPS is to put that antenna on a car so the car can be tracked anywhere on the earth. (Refer slide 30)


Slide 31
For GPS we do a Trilateration. Suppose there are a number of satellites which are visible to any of the ship, plane or a car as shown in the picture. Suppose we are measuring the car?s distance from a number of satellites. Since for the satellites we know relevant parameters, from three different readings and three different distance measurements, if measured quite precisely and accurately, you can get the accurate position of the car. There are so many different applications, which are possible. For example, there is a central database, if you give information about where you want to go, then it can tell you  the path to take or the alternative paths and which of them is crowded. In many places in Europe, you can get very precise information about where you are, which road you should take to reach the final destination. (Refer slide 31)


Slide 32
The advantages of AVL (automatic vehicle location) are fast despatch; customer service; safety and security; digital messaging; dynamic route optimization and driver compliance. Slide 32 shows a mobile GPS unit located on a truck. With GPS satellites, you can give faster dispatch of goods. For example, some company has to deliver a lot of goods to a lot of  warehouses. So it may ship something and now the customer wants to know where it is. You can immediately know where it is because you know into which truck you have put these goods and where it is exactly.  So, for courier service and similar services, you can give enhanced and sophisticated level of service through GPS.   (Refer slide 32)


Slide33
In conclusion, we can say that satellite communication will continue to serve where broadcasting is essential or where terrain is hostile or very sparsely populated. It also has a niche where rapid deployment is extremely critical. Let us consider battlefield kind of situation where you want to rapidly set up some communication network. If you have a satellite up in the sky already, you can take that transponder. May be there is a disaster where you want to quickly set up disaster recovery and relief operations. You can use this satellite because satellite communication will still go on whatever may happen. Even if there is a flood and everything is flooded, the satellite communication will still go on moving with your mobile units. In other combination with terrestrial radio links fiber is likely to hold the advantage (Refer slide 33)


Slide34

Now we come to an interesting part of the satellite communication. We see how this MAC works for satellite communication. This is another class of media access control technique, which we mentioned earlier. This is the first example that we see. It is Random Access Protocol. (Refer slide 35)


Slide 35
We mentioned Random Access Protocols earlier just as we talked about token based protocols, etc. We also talked about random access protocols. This is the first example of a Random Access Protocol. Random Access Protocol is actually very simple. When a node has a packet to transmit at full channel data rate, so just do not bother whether somebody else is transmitting or not transmitting or wants to transmit or it?s going to transmit. Right away, you just do not bother about anything. You just try to put in whatever you want to transmit on to the channel. Now what will happen. If you are lucky since you are doing the random thing without any knowledge or consideration about what other people are doing, your transmission may get go through. So there is no a priori coordination among the nodes but your transmission may still go through. What will happen is that two or more stations may try to   communicate at the same time or very nearly the same time. What may happen is that the packet or the frame sent by one station A and the frame sent by the station B  may collide and what will happen is that both the frames will be lost and will become garbled. Since you are talking about a medium, which is shared by everybody, these two transmitting stations A and B would also be able to listen to this and find out   that both of their messages are garbled. If they find that both are garbled, they will  retransmit as a sort of backup for a random amount of time. That is very important once again the second random is also very important  this backing a random amount of time because if the protocol says no both of them backup for a fixed amount of time and after the lapse of that fixed amount of time both of them will start communicating again. It is important  that the backoff for a random amount of time and hopefully the random number generated by one station and the random number generated by the other station happen to be two different numbers. So now they are going to stay communicating at two different points of time but they will not collide. They may collide with other stations. That is a different issue. To ensure the random access protocol, if two or more transmitting nodes start transmitting at the same time we have a collision. Random access MAC protocol specifies how to detect collisions and how to recover from collisions; for example, via delayed  retransmissions. These are the two parts of any random access protocol.  (Refer slide 36)


Slide 36
This satellite protocol, which is called ALOHA or slotted ALOHA is the simplest kind of random access protocol. We will see other examples like CSMA CSMACD CSMACA etc later on. So   these are the examples of Random access MAC protocols   ALOHA which was the original grandfather of all these protocols first came, then slotted ALOHA and then all these CSMA CSMACD and CSMACA etc were used (Refer slide 36)


Slide 37
The ALOHA protocol was originally developed for packet radio in 1970s. This was applicable to any system with uncoordinated users competing for a shared channel. There was no carrier sensing. Carrier sensing means you do not try to find out whether or not somebody is already communicating. That may sound a little strange at first; because if you could find out that somebody is already communicating, then by trying to transmit at that point of time you are not only sure to fail because your own message will get garbled and somebody else?s frame will also get garbled, but then  you will also have a more crowded media. For satellites there is a problem. You cannot always do this carrier sensing, due to space delay. Since we have a 240 milliseconds space delay, whatever you are listening to now was actually transmitted 240 milliseconds earlier. So if you find that the channel is quiet now, there is no guarantee that after 240 milliseconds also it will also remain quiet. That is why it is difficult to do carrier sensing in the case of a satellite. That is why this latency, space delay, has a very significant impact on how we handle the MAC. Here we do not do it; 240 milliseconds is a lot of time in which a number of frames may be sent. So we do not do any carrier sensing. We simply send whenever we have to send something and then later on listen to the medium to find out whether or not there has been a collision. This is the pure ALOHA protocol. (Refer slide 38)


Slide 38
Users transmit whenever they have data. If there is collision within a time frame, dictated by the space delay, it backs off for a random amount of time and sends it again. If the first bit of a new frame collides with the last bit of a frame that is just finishing, both are taken as garbled. So there is no such thing that the frame has gone through 95%. Either the frame has gone entirely, or even if one bit has collided, both the frames are taken as garbled and lost. (Refer slide 38)


Slide 39
Collision detecting by listening to broadcast channel or by absence of acknowledgement: if collision occurs, each user waits random length of time as already mentioned. Various collision resolution algorithms are available. Station does not transmit new frame until old frame has been successfully transmitted. The station is stuck with the frame until it can successfully send it. (Refer slide 39)


Slide 40
The slotted ALOHA has better performance. All frames are of the same size. Time is divided into equal size slots and each slot is to transmit one frame with some   distance. If the transmitters and the receivers are synchronised they try to send frames only at the beginning of a slot. So nodes start to transmit frames only at the beginning of slots. Nodes are synchronized; if two or more nodes can still try to transmit in a slot, all nodes detect collision. There may be collisions even after making the slots. For example, suppose within the span of the fifth slot three different stations are ready to transmit something. What will happen is that at the beginning of the sixth slot, all three will be communicating and all three will collide. (Refer slide 40)

Slide 41

When node obtains fresh frame it transmits the next slot. As soon as it gets a frame to send, it sends it in the next slot. No collision node can send new frame in the next slot. If there is a collision, the node retransmits the frame in each subsequent slot with probability p until it attains success. (Refer Slide 41)

Slide 42

This is a picture of a slotted ALOHA. There is a collision over three nodes ? nodes 1, 2, and 3. All three of them wanted to transmit. They start transmitting and then collision occurred. Then all of them backed off. It may so happen that the second slot went empty. Then in the third slot, what happened was that number 3 had backed off randomly for a long period of time but node 1 and 2 may have decided on the same number over here. So 1 and 2 collide again. What might happen is given as an example: may be in the fourth slot, 2 has tried again and it has been successful. So there is a success over there. Then next slot is empty. Node 1 and node 3 are trying and then they collide again. Then may be in the next slot, 3 succeeds and so after 9 slots, the three have been able to communicate these three frames. So you see because of this, collision affected efficiency. (Refer slide 42)


Slide 43

In slotted ALOHA pros a single active node can continuously transmit at full rate of channel. If nobody else is transmitting one active node can go on transmitting. This is highly decentralized. Only slots in nodes need to be in sync; so that may be taken care of from the hub or through the satellite etc.; it is a very simple kind of protocol. (Refer slide 43)

The cons are that there are idle slots and hence low efficiency. (Refer slide 44)


Slide 44
Now what is the efficiency? Efficiency is the long run fraction of successful slots. There are many nodes each with many frames to send. Suppose N nodes with many frames to send each transmitting slots with the probability P. So probability that first node has success in a slot is equal to p (1 ? p*(N ? 1)). So it is the probability P that that particular node sends and it is the p1 ? p that all the other N ? 1 do not send. So only then you will have a success; and probability that any node has a success is Np(1?p)N?1. (Refer slide 44)


Slide45


So for maximum efficiency with N nodes, we have to find a p* that maximizes Np(1 ? p)*(N ? 1).Hence this value. For many nodes, we take the limit of Np*(1 ? p*)N ? 1 as N goes to infinity and this gives a value of 1/e, which is 0.37. So this is the efficiency ? 37% is the maximum theoretical efficiency that you can achieve in slotted ALOHA. So at best, channels have useful transmissions; 37% of time, which may not look like a very high figure, but then you are absolutely uncoordinated; but if you have low load then that may be alright. (Refer slide 45)


Slide 46.

For pure or unslotted ALOHA, is simpler because there is no need for synchronization. When a frame first arrives, it is transmitted immediately. So collision probability actually increases. A frame sent at t0 collides with other frames sent in t0 ? 1 assuming that each frame takes one unit of time to send up to t0 + 1; so there is a big interval from t0 ? 1 to t0 + 1, where things may collide. (Refer slide 45)


Slide 47
So this will overlap. So this frame will overlap and now there is no question of any slots, so anybody can start transmitting at any point of time. (Refer slide 47)


Slide 48

What will the efficiency be like? P of success by a given  node is equal to P that the node transmits multiplied by the probability no other node transmits in [t0 ? 1, t0] ? P, a probability that no other node transmits in t0, t0 + 1. So this is p(1 ? p) N ? 1 ? (1 ? p)N ? 1 so p (1 ? p)2(N ? 1). So choosing optimum p and then letting N tend to infinity gives as an efficiency of 1/(2e) or about 18%. So this is a rather low efficiency for  transmission.  (Refer Slide 48)


Slides 49 and 50
When a computer network developed there were a number of  competing  LAN technologies  but today ethernet has a come to dominate a LAN almost totally excepting for the newly emerging  wireless part which we will discuss later.  So this ethernet is not only a LAN. Nowadays people are talking about ethernet in the  MAN that is metropolitan network area also. So Ethernet, as you understand is very important. So we will talk about Ethernet. It?s a dominant data link layer technology (Refer slides 49 and 50)

Slide51

The multiple access scheme of ethernet is CSMACD now  (Refer Slide 50)

Slide 52

So this minimum time is which may vary from system to system, that means a network to network. So this is how it is calculated so far. Let us see how the minimum time would comes. Suppose this is the shared bus we put two nodes at the two extreme ends. Suppose now packet starts at over here at time t equal to zero. Packet is almost reached b at time t-? , at which point of time,  it does not transmit. Suppose the propagating time of the packet for moving from one end to the other is ?, at ?- ? it has almost reached b. At that point of time b starts transmitting, because we have been finding that the bus has been very quiet. There is no signal  at end so b can start .So b starts and if there is a collision and this collision is  at one this end of the of the medium .So  the collision bus  starts transmitting back the jam signal. .So it again take s almost tau to reach .Soa so the total time is ?
noise. So collision detection can take as long as 2 ?. ( Slide 52)
COMPUTER NETWORKS
Prof.Sujoy Ghosh
Lecturer Name # 19
Ethernet ? CSMA/CD
(Refer slide time: 00:41)
Good day. Today, we will talk about Ethernet. This is possibly the most ubiquitous LAN technology today. As a matter of fact, when computer network developed, there were a number of competing LAN technologies but today Ethernet has come to dominate LAN almost totally, excepting for the newly emerging world spot also, which we will discuss later. Not only LAN, nowadays people are talking about Ethernet in the MAN, i.e., Metropolitan Area Network also. Ethernet as you understand is very important so we will talk about Ethernet. (Refer slide time: 01:37 - 01:59)  

It is a dominating data link layer technology and the multiple access scheme of Ethernet is CSMA/CD. We will talk about Ethernet and CSMA/CD;  this will be our first lecture on this. We will continue on this topic in the next lecture also. (Refer slide time: 02:00 - 02:54)

What is the broad outline of the Ethernet? First of all, it was conceived as a broadcast network, so single communication channel that is shared by all the machines on the network. This would be the kind of medium we will have. We have a shared medium where everybody broadcasts. It is supposedly one of the common broadcasts in the early days, one of the common physical organizations of Ethernet where some machines were connected to a coaxial cable. This is a LAN technology unlike the satellite technology that we have talked about ? remember that in satellite technology, which is also a shared medium, we of send data and then detect collision. This is further advancement. The multiple access technique of Ethernet is just some refinement over that but this is essential in a LAN environment, meaning it is for a small geographical area. We have packets; that means packets are sent over this, we will see later on how it frames them. So short messages sent by any machine are received by all others. Since this is a broadcast network, a message which is sent by any machine is received by all networks and so naturally you have to give the destination address. There are various fields in the Ethernet header and we will look at that later on. One of them has to be the destination address so that we understand because otherwise the recipients will not know for whom this frame is meant. (Refer slide time: 04:05 - 04:57) 


Since it is a shared medium, all these machines are supposed to be in the so-called same collision domain; because if two machines are sending simultaneously, their packets will collide. Suppose you have the sender over here and sender has a packet to send, it puts it on this common bus. The packet travels both ways and then it is received by all the recipients. All of them copy the frame into their NIC. They would compare the destination address with their own address, if it is not for them, they will simply ignore it. If it is meant for this node, it will absorb it and then send it to the higher layer.  (Refer slide time: 04:58 - 05:58)

All machines receive the packets but only one would process it; i.e. the mode of operation is broadcasting. It is also possible to address a packet to a subset of machines; this is called multicasting. This mode of operation is called multicasting, meaning we have two ends of the spectrum: one is a point-to-point communication, one sender and one receiver. On the other end of spectrum is a broadcast where one sender and everybody else is a receiver. Somewhere in between is a multicast; meaning there is one sender and several receivers. In order that several receivers can receive it, we have to do something with the address. So we have to give a special kind of address; that is also possible. It is possible to multicast since it is possible to broadcast. (Refer slide time: 05:59 - 07:22)


Just to remind you about our discussion on satellite communication, if you remember, the satellite communication used a MAC protocol called ALOHA. There were two versions of ALOHA and one was the pure ALOHA, whereby if anybody has any data to send he simply sends it. If there is a collision, he would back off and maybe send it again. That is a pure ALOHA scheme and we saw that this is the form of S = Ge?2G where s is the throughput, that means how much you are actually able to communicate successfully, and G is the number of attempts per packet time. This is the s versus gee of pure ALOHA, which gives about 18% of efficiency at the maximum. Slotted ALOHA is where you can send only from the beginning of a slot time; that has considerably better performance then it comes to about 37%. The formula is S = Ge?s. Refer slide time: 07:23 - 09:39)

Ethernet is something similar. That means we broadcast and then we detect collision, so it should be the same as some kind of ALOHA. The only thing is that since this is a local area network, a small network, what we can do is that we can sense the carrier; that means we can sense whether somebody is sending. If somebody is sending, we refrain from sending anything; so that is called carrier sensing. We discussed this earlier that it is not possible to do carrier sensing in satellites because the space delay is so high, whatever you are listening to now must have happened may be 250 milliseconds earlier; that is a long time back. But that is not the case in a small LAN. The timeframes are much smaller, so you can actually listen to the medium and find out whether somebody is actually broadcasting something at that particular point of time. In that case, even if you have something to send, you do not do so. That reduces the collision considerably and that naturally leads to an improvement in the efficiency. So this stream is called carrier sense multiple access and we find that even if you are doing carrier sensing, there can be some collisions although there will be reduced number of collisions compared to pure ALOHA, slotted ALOHA. We have to detect the collision and this CD part. The full MAC protocol that is used by Ethernet is the CSMA CD; this is the dynamic channel allocation technology or the random access MAC for this, used in old Ethernet. Nowadays we have moved to fast Ethernet, where we have reduced collision and switched Ethernet, which we will discuss these in the next lecture. (Refer slide time: 09:40 ? 09:55) 

So we have carrier sense multiple access. We improve the performance of our simple network greatly if we introduce carrier sensing; with carrier sensing each hosts listens to the data being transmitted over the cable. (Refer slide time: 09:56 ? 10:25)

A host will only transmit its own frames when it cannot hear any data being transmitted by the other hosts. When a frame finishes an inter frame gap of about 9.6 ?s is allowed to pass before another host starts transmitting its frame. So he listens to somebody sending and then he naturally waits and then after that frame is over, he gives a gap of about 9.6 ?s and then starts transmitting (Refer slide time: 10:27 ? 13:03)

Collisions can still occur mainly because of the propagation delay. Propagation delay means two nodes may not hear each other?s transmission. As before, if there is a collision, the entire packet transmission time is wasted; and naturally the distance and propagation delay has a strong role to play in the collision probability. Let us look at this. Suppose this node B starts transmission at time t0. Naturally, the transmission goes both towards A as well as towards C and D. So there is a cone like this ? if you plot it against time and distance ? over which B?s transmission is going. What happens is that C was listening and at this particular point of time, t0 or even later than that, C did not have any carrier to sense, because B?s transmission, although it had started in terms of absolute time, had still not reached C. So it will reach C only at this particular point of time. During this period, the transmission from B will reach D at a much later time, let us say t2 or something. If D starts its transmission t1, what will happen is that somewhere in-between they are going to collide; and these garbled bits will now start propagating on both sides, which is shown in this hashed figure. What happens is that, naturally B?s transmission is lost and D?s transmission is also lost. So there will be a finite probability of collision and this probability will depend on the time it takes for the frame to reach all the nodes. Because if it had reached some other node, that particular node would not start transmitting; but it takes a finite amount of time, and within that finite amount of time, if somebody else starts transmitting, we have a collision. So we can still have collision; that is why CSMA is not enough; we have to do CD also. (Refer slide time: 13:04 ? 14:53) 

So CSMA, as we mentioned, means listen before transmit. If a channel is sensed busy, defer transmission; and persistent CSMA means retry immediately when channel becomes idle ? this may cause instability. This is called persistent CSMA. That means retry immediately when channel becomes idle; because what might happen is that during the time when the frame is being sent ? that may be a considerable amount of time ? more than one node may become ready to transmit.
If more than one node is ready to transmit, naturally all of them will pounce on it after giving a 9.6-?s gap and start transmitting, and naturally they are going to collide. If they collide they know that is bad, so they allow that collision to subside and then they start again. By this time, quite a lot of time has elapsed. Somebody else may become ready and somebody else may again start persisting; so this may grow to some kind of instability. Being totally persistent is not very good, as we will see the performance of each of this; but that is persistent CSMA. We call a strategy as pure persistent; that means, we retry in the next immediate available time with the probability of p, which means, with the probability of 1 ? p, we do not persist. That means, we defer still more. (Refer slide time: 14:54 ? 15:34)

There is a non-persistent CSMA; that is, somebody is ready to send something; finds that it is busy. Then he will retry only after the random interval. He generates some random numbers and decides by himself to wait for this random amount of time. Collisions may still exist, since two stations may sense the channel idle at the same time or within a vulnerable window, which is equal to the round-trip delay. In case of collision, the entire packet transmission time is wasted; so we have pure ALOHA. This is the group of protocols: we have pure ALOHA, then we have slotted ALOHA, persistent CSMA, p persistent CSMA, non-persistent CSMA; and here is how they behave. (Refer slide time: 15:53 - 17:54)

We have already seen these two curves, pure ALOHA and slotted ALOHA. They are used in satellite communication because we cannot do area sensing over there because of the significant space delay. If you do carrier sensing and if it is persistent, the performance improves somewhat; so it may go slightly above 50% and then comes down. You can have different values of p, remember p is the probability that the node will try immediately when it senses that the channel is available; that probability will be 0.5. At probability 0.5, it will not try at all. At 0.5, persistent CSMA is good, 0.1 persistent CSMA is even better; that means what it will do is with 0.1 probability it will try and with 0.9 probability it will differ. If it is non-persistent, this is the 0.01 persistent CSMA and non-persistent CSMA. Non-persistent means it will always go back. If you draw the throughput versus load curve, this is the kind of curve you get. But the exact throughput will depend on how many nodes are there and what their distances are, etc. These are somewhat idealized figures but you get something like this. As you can see, with non-persistent or very low persistent CSMA, we get very high throughput even when the load is going up. So when the load is going up, it is not falling down steeply like ALOHA or slotted ALOHA. This is the channel utilization by the various MAC protocols. (Refer slide time: 17:55 ? 18:08)

 So CSMA/CD is carrier sensing, deferral as in CSMA; collision is detected within a short time; 
colliding transmissions are aborted, reducing channel wastage. (Refer slide time: 18:09 ? 18:39)

Collision detection is easy in wired LAN; it measures signal strengths and compares transmitted and received signals. This is how you detect a collision. It is somewhat more complicated in wireless LANs. We will discuss wireless LANs later on as we move on, but the receiver is usually shut off while transmitting. This is a problem with the wireless LANs, where collision detection is difficult. We go for some other kind of scheme over there, which is called collision avoidance, we will discuss it later. (Refer slide time: 18:54 ? 19:49)

As I mentioned, Ethernet is the dominant LAN technology, and since this is dominant, people who followed the Ethernet line had the great advantage of having a huge market. That really drove down the cost and today Ethernet is cheap. Ethernet network interface card costs in the order of some $20 or something of that order. For a 100 Mbps card, which is quite fast, it is very cheap. It is the first widely used LAN technology; it is simpler and cheaper than token LANs and ATM ? so that is a great advantage and it kept up with speed rates of 10,100, or 1000 Mbps. Ethernet started with 10 Mbps, then went on to 100 Mbps, which is a standard today. People are talking about 1000 Mbps or 1 Giga bit Ethernet, which is also making inroads into the backbones of LANs etc. Maybe in a couple of years? time, it will go into the desktop also, if it has not already reached some desktop. So Ethernet is a very widely used technology. This is (Refer slide time: 20:26 ? 20:54)

a diagram to show you how the original Ethernet looks. You have a cable, which would be terminated on two sides; it would be tapped and then a transceiver will tap into it, to connect it. There will be an interface cable from the transceiver to the interface of the controller. This is basically the NIC, and this is how it will be connected to various machines, which should be tapped into the cable at various points of time, so this is the so-called Ethernet. (Refer slide time: 20:55 ? 21:34)

Now, we go into the details of how exactly this is done. We have discussed the basic technology already. Ethernet uses a bus topology; it assumes a bus topology, carrier sense multiple access. In CSMA/CD, each station has equal access to the network but it can broadcast only when the network is idle. Before transmitting, a station: (1) listens to the network to sense if another workstation is transmitting, which is carrier sense. If the network is still idle after a certain period, (2) the station will transmit. (Refer slide time 21:35 ? 21:42)

It is possible that two stations will listen and sense an idle network at the same time. Each will then transmit a message, which will collide. (Refer slide time 21:43 ? 22:05)

While transmitting, a station must perform collision detection to detect if its message was destroyed. If a collision is detected, the detecting station broadcasts a collision or jam signal to alert other stations that a collision has occurred. Each transmission station then waits a random amount of time ranging from 10 to 90 ?s before attempting the transmission again. (Refer slide time: 22:06 - 22:36)

So this is the same algorithm step by step; adapter gets datagram from the network layer and creates a frame. That means it adds a header to create a frame. If the adapter senses that the channel is idle, it starts to transmit the frame; if it senses that the channel is busy, it waits until the channel is idle and then transmits. If the adapter transmits the entire frame without detecting another transmission, the adapter is done with the frame. If the adapter (Refer slide time: 22:39 ? 25:50)

detects another transmission while transmitting, it aborts the transmission and sends a jam signal. This is an interesting point ? it needs to detect collision only while it is transmitting. After the transmission is over, there is no detection within that frame transmission time; you can forget about collision. That means just some time after that, you may still detect the collision. That collision would be presumed to be due to another station, not your frame ? because if you detect a collision after sending a frame, you have to understand whether it was your frame which collided or your frame went through and somebody nearby started transmitting and they collided. So the idea is that while you are transmitting, that time only should be enough for any collision, any inadvertent collision to happen; we will come back to this point. So if there is a collision, it aborts; after aborting, the adapter enters an exponential back-off algorithm. What is an exponential back-off algorithm? What might happen is that the frame sent might have collided and then it might have backed off for sometime; it may have tried again and it may have collided again; maybe the network is busy. What is the remedy if the network is very busy? Obviously if the network is very busy, what is going to happen is that more and more people will try to send; they will fail and then they will try again. This may bring down the overall throughput of the system to a large degree. So what is done is that if the network is busy, everybody tries to make the network less loaded, which means that they wait for longer before they try to send it. What they do is that they generate this random number over an exponentially increasing period so that the probability of a higher random number becomes more and more if the network is more loaded, which means that you are backing off for a larger amount of time. That is exponential back-off, so after the mth collision, the adapter chooses a k at random from 0,1,2?1. If m is 10, it chooses a random number between 0 to 1023. The adapter waits k = 512 bit times and then returns to step 2, which means, suppose k is 100, it would be 100 ? 512 bit times. It waits 512 bit times and tries again. (Refer slide time: 25:51 - 26:36)

So jam signal makes sure that all other transmitters are aware of the collision. We send a jam when it detects a collision and backs off. We calculate the amount it backs off as follows: Let us say bit time is 0.1 ?s for 10 Mbps Ethernet for k = 1023. If you remember 512 into 0.1, 512 bit time should be about 51.2 ?s and 51.2 ?s into 1023 will be approximately 50 msec, i.e. it waits for 50 msec, if k has to come out as 1023. (Refer slide time: 26:37 - 28:16)

The idea of this exponential back off is to adapt retransmission attempts to estimated current load. If it is heavy, the random wait will be longer. Once again, we do this because if the load is heavy at some particular point of time, then people in general should refrain from sending so that the load comes down. If the load is kept high and everybody starts to transmit, there will be even more collision and even less throughput than what you would achieve by a number of people backing off. This backing off is done exponentially, so first collision, choose k from 01, and delay is k ? 512 bit transmission time, which means that with 50% probability, we try to transmit immediately, and with 50% probability, we would back off for this k ? 512, i.e., 51.2 ?s before listening and trying to send again. After the second collision, we choose k from 0123. The aim is increasing; the number of collisions is increasing. After 10 collisions, it is not allowed to increase more than that. This range of k becomes fixed and you always choose from this 0 to 1023. This is the exponential back-off algorithm; for heavier load presumably a node will choose a larger k and wait for longer, reducing the load on the system and increasing the throughput. This CSMA/CD system can be in (Refer slide time: 28:23 ? 28:40)

one of the three states. There may be contention, there maybe successful transmission, or it may be idle. There may be contention slots or frames or idle period. (Refer slide time: 28:41 ? 29:50)

IEEE 802.3 defines this Ethernet protocol. If you remember 802.2 was LLC and 802.3 was for CSMA/CD and 802.45 etc., like token bus token ring etc. There are a couple of variants of Ethernet: one is IEEE 802.3 and the other is Dixie Ethernet. We will maybe mention it later on, but they are more or less the same, excepting for minor differences. 802.3 is the more dominant one these days; besides, this is the standard. 802.3 is the number of the standard and it describes the format of the frames and the type of encoding used for transmitting frames. It also describes what the frame format is and what kind of encoding is used for transmitting the frame. The minimum (Refer slide time: 29:59 ? 31:01)

length of frames can be varied from network to network. This is important because, depending on the size of the network, the frames must be of a suitable minimum length. Remember that a node has to detect collision only during the transmission time. If transmission is over and there has been no collision detected, it assumes that its own frame has reached whatever destination it was supposed to reach. After that, if there is a collision, it is supposed to be between two other nodes because once there is a collision only the sender would know whether his own frame has collided because you cannot really decipher the address field of the collided frame. The collided frames are gone, they are garbled. The standard also makes some suggestions about the type of cabling that should be used for CSMA/CD, bus LANs, etc. (Refer slide time: 31:02 ? 32:41)

This minimum time may vary from system to system; that means, network to network. This is how it is calculated. First of all, let?s see how the minimum time would come. Suppose this is the shared bus. We put two nodes at the two extreme ends, the packet starts over here at time t = 0, it almost reached B at time (t ? ?). Suppose this propagation time of the packet, from one end to the other, is ?, at (? ? ?), it has almost reached B at that point of time. B starts transmitting because while B has been finding that, the bus has been very quiet; there is no signal on it. B can start transmitting and it starts sending. There is a collision at one end of the medium. The collision bursts, starts traveling back and the jam signal is sent. That starts travelling back, it again takes time ? to reach A. The total time is 2 ?, the collision detection can take as long as 2 ?. The point is that your transmission time has to be greater than this 2 ?. There are a few other issues; we will discuss them later. (Refer slide time: 32:42 ? 35:19) 

This is what we were discussing, minimum frame length to ensure that no node may completely receive a frame before the transmitting node has finished sending it. Ethernet defines a minimum frame size, i.e. no frame may have less than 46 bytes of payload. Remember payload is the packet, which the data link layer receives from the upper layer. What happens if the packet is much smaller? The application itself is such that you have to send maybe 1 character or 2 characters; what happens to it? Ethernet does not allow less than 46 bytes, so you have to pad it up; we will come to that later. The minimum frame size is related to the distance which the network spans, the type of media being used ? because of the time it takes for the signal to travel ? and the number of repeaters which the signal may have to pass through to reach the furthest part of the LAN. Repeater if you remember is something, which enhances the signal. As a signal travels down the transmission line, it tends to get weaker and weaker because of attenuation. At some particular point of time the signal may have to be amplified. A repeater does just that, it takes some incoming signal and amplifies it, maybe some wave shipping or something. There may be a number of repeaters. Each of these repeaters might introduce some delay because the repeater will not go through instantaneously. There will be some delay in this repeater and if there are a number of repeaters, this delay would get added. Remember what we are trying to find out. When the packet being sent from one end of the network, reaches the other end and almost at the other end, there is a collision and that collision comes back. What is this total time? During this whole time, this packet must be transmitting because it will detect the collision only during the packet transmission time. The number of repeaters also is important as are the type of media and distance to the furthest part of the LAN. Together these define a value known as the Ethernet slot time, corresponding to 512 bit times at 10 Mbps. (Refer slide time: 35:20 ? 35:56) 

The longest time between starting to transmit a frame and receiving the first bit of a jam sequence is twice the propagation delay from one end of the cable to the other. This means that a frame must have enough bits to last twice the propagation delay. The 802.3 CSMA/CD bus LAN transmits data at the standard rate of r = 10 Mbps. This was the earlier standard of 10 Mbps. The speed of signal propagation is about V = 2 ? 108 ?s in this coaxial cable. (Refer slide time: 35:57 ? 36:28)

The cable maybe of 400 m length, transmission speed = 10 Mbps, so propagation speed is 2*10**8 ?s. Let us see what the delay is and what the minimum frame size comes out to be. Propagation delay time is tprop; the round-trip propagation delay is twice this number of bits. We can fit into a round-trip propagation delay, the minimum frame length nb. (Refer slide time: 36:29 -37:04)

tprop = d/V = 2 ? 108 and distance is 400 m, which makes it 2 ? 10?6 sec or rather 2 ?s, 2 ? tprop; that means, for the signal to travel all these 400 m and come back, it takes 4 ?s. For the 4 ?s, the transmitting station must go on transmitting and the transmitting station is pumping data at the rate of r = 10 Mbps. (Refer slide time: 37:05 ? 37:52)

A bit time, the time to transmit 1 bit is 1/R; it is 1/10 megabits, which is 10 ? 106 which is 0.1 ?s. The minimum number of bits the frame must have is 2 ? tp/tb. tp is the propagation delay; 2 ? tp for the round trip propagation delay divided by tb = 4/0.1 = 40 bits. This calculation brings us to 40 bits. As I said the minimum size is much bigger than 40 bits, and we will see why. (Refer slide time: 37:53 ? 38:07)

The minimum frame length is thus 40 bits or 5 bytes. A margin of error is usually added to this, often to make it a power of two, so we might use 64 bits or 8 bytes. (Refer slide time: 38:08 ? 38:53)

Let us see another example, calculating something else but it is closely related. Two nodes are communicating using CSMA/CD protocol, and the speed transmission is 100 mbps. This is A fast Ethernet and frame size is 1500 bytes. The propagation speed is 3 ? 108 meters per second ? that is the speed of the light. Calculate the distance between the nodes such that the time to transmit the frame equal the time to recognize that the collision has occurred. We want to calculate the distance. (Refer slide time: 38:54 ? 39:12)

Since frame time is given we calculate the propagation delay from this. So Tround trip = tframe = 2 ? tprop so tprop =  tframe/2, and what is the tframe? (Refer slide time: 39:12 - 39:22)

We have 1500 bytes, which is being pumped at the rate of 100 mbps. (Refer slide time: 39:23 - :) 

If you multiply 1500 bytes into eight, that means how many bits will get 12000 bits which is 1.2 ? 10?4,  i.e., 6 ? 10?5 (Refer slide time: 39:53 ? 40:17)

The maximum distance is calculated using the propagation velocity, tprop = d/V = tprop ? V = (6 ? 10 ?5) ? (3 ? 108) = 18 ? 103 = 18 km. With this kind of minimum frame length, you can go up to 18 km. Remember, the minimum frame size has become 1,500 bytes. This standard (Refer slide time: 40:29 ? 40:48)

frame length is at least 512 bits or 64 bytes long, which is much longer than our minimum requirement of 64 bits which is 8 bytes. We only have to start worrying when the LAN reaches lengths of more 2.5 km. (Refer slide time: 40:49 ? 41:41)

802.3 CSMA/CD bus LANs longer than 500 m are usually composed of multiple segments joined by in-line passive repeaters. As a signal travels down transmission line, what would happen is that it will become weaker, so we will have to put a repeater, and one kind of repeater maybe called a hub. In-line passive repeaters output on one cable the signals received on another cable or we may simply have an amplifier over there. When we work out the minimum frame length for these longer LANs, we also have to take the delays caused by the passive repeater; each passive repeater introduces a delay of about 2.5 ?s each, so we have to take this into account as well. (Refer slide time: 41:42 ? 43:23)

Let us say we have 500 m on each segment, we have 4 repeaters, so 5 segments. That is about the maximum you should go. Nowadays we do not use such coaxial cables. What happens is that 64 bytes sent at 10 Mbps would take 64 ? 8 = 512 bits; 512 bits at 10 mbps is 51.2 ?s and 500 m per segment. Four repeaters between the nodes means there are 5 segments; 2500 m in all. This gives rise to 25 ?s propagation delay, because remember our propagation velocity is about 2 ? 108. If you calculate, it comes to about 12.5 ?s from one end to the other, twice that is about 25 ?s. The frame should be long enough for the sender to detect the collision in 2 ? 25, this is 125 and the other 25 is these 4 repeaters together, about 50 ?s; 64 bytes sent at 10 Mbps is 51.2 ?s. If you have a frame length of 64 bytes, that is quite good. (Refer slide time: 43:24 ? 45:27)

Let us look at the details of the Ethernet frame. All information on an Ethernet network is organized into frames also called packets or may be packet is what the upper layer gives and then this is formed into a frame. contents of an Ethernet frame: Sending adapter encapsulates IP datagram or other network layer protocol packet in Ethernet frame. This is what the Ethernet frame looks like: we have a preamble of 7 bytes, we have a start of frame of 1 byte, we have a destination address of 6 bytes, we have a source address of 6 bytes, we have frame length of 2 bytes and then data may vary from 46 to 1500 bytes and then followed by CRC of 4 bytes. If you remember CRC is the cyclic redundancy code which Ethernet uses for error detection on the other side. So you have actually about 72 bytes, which is even larger than the 64 bytes. We require 46 bytes  the minimum and 1500 bytes is the maximum length of the payload, this is the payload part; to this we have to add this header and trailer part. In the trailer you have the CRC in the header the source address destination address, etc. This frame length is required because you might have padding over here. In some other version of Ethernet, there is a type field over here, which gives the network layer protocol type. Let us look at the (Refer slide time: 45:28 ? 46:08)

details quickly. Sending adapter encapsulates IP datagram Ethernet frame; preamble has 7 bytes with the pattern 10101010. Please note that 1s and 0s are alternating ? and there is a reason for that ? followed by 1 byte with pattern 10101011; so these 7 bytes are called the preamble. This 10101011 is the start of frame delimiter, 7 bytes of pattern. This 1 0 is required; it is used to synchronize receiver and sender clock rates. That is why we alternate 1 and 0 etc. (Refer slide time: 46:09 ? 47:47)

We have an address of 6 bytes. If the adapter receives a frame with matching destination address or with broadcast address, for example an ARP packet, it may be broadcast, it may be multicast or it may be unicast. If actually a frame would be broadcast, only 1 would come up. ARP is the address resolution protocol; we will discuss this in the next lecture. It passes data in the frame to the net layer protocol; otherwise the adapter discards the frame. So this address has to be there. The destination address and source address are also put over there. Type: as I said, in some form the type indicates the higher layer protocol, mostly IP, but others are supported, such as Novell IPX and Apple Talk. What type of network layer protocol is it talking to? The same Ethernet ? as I mentioned before ? the same data link layer protocol may be supporting a number of network layer protocols. In that case, there has to be multiplexing and de-multiplexing. In the receiver side when a packet comes, maybe this data link layer knows that this is for this particular machine because its address is there so it will absorb it but then send it. If there are two different network layer protocols, which are running at the same time, whom will it send it to? That type is mentioned in the field so that it can do a de-multiplexing properly. CRC or cyclic redundancy code is checked at the receiver. If an error is detected, the frame is simply dropped. (Refer slide time: 47:48 ? 49:53)

The other thing to understand is that this ? Ethernet gives you an unreliable connectionless service. It?s connectionless, and there is no handshaking between sending and receiving adapters; so it is connectionless. The sender simply sends some packet and there is no handshaking between the two. The actual reliability, whether there will be error or not, will depend on the quality of the cable, the quality of connections, whether all standards have been followed very meticulously, and the quality of distance and the ambient noise and these kinds of things. That is not what we are talking about, it is unreliable in the sense that whatever physical layer parameters are there, they are given to the data link layer, that is, the Ethernet layer. But the Ethernet does not really try to take any special care to see that the communication is reliable. It has a CRC for sure, so on the other side if the receiver sees that the CRC shows some error, it may drop the packet. But the receiver does not send any acknowledgement to the sender. The sender does not know about it. If that packet is dropped that frame is dropped. If that has to be handled, it can only be handled at a higher layer. So it is unreliable in that sense, receiving adapter does not send ACKs or negative acknowledgements to the sending adapter. The stream of datagrams passed to network layer can have gaps; that means some of the frames may have been dropped. Gaps will be filled if the application is using TCP. This is one kind of transmission protocol, which we will discuss later, which tries to take care of such errors. Otherwise, the application will have to handle these gaps. (Refer slide time: 49:54 ? 52:20)

Just a couple of words about the addresses ? remember there are 6 byte addresses. Every network card in the world has a unique 46-bit serial number, called a MAC address. We are talking about the Ethernet, there is 46 bit; 246 is about 64 trillion, which is a very large number and these numbers or these addresses are distributed by IEEE. A manufacturer who manufactures Ethernet NIC ? that means Ethernet network interface cards ? will apply to IEEE and get a whole chunk of addresses, whole block of addresses and will put these addresses one by one into the network cards that are produced and then sell in the market. What happens is that one of the network cards may have gone to India and the next network card may be used in Beijing. It may so happen that the network cards we have and not the network addresses in the LAN will have very contiguous addresses. They may not; anyway, what is guaranteed is that since this is distributed centrally from IEEE, no two addresses, i.e. no two MAC addresses of two network interface cards or two Ethernet cards are going to be the same. They are all going to be distinct, because we have this pool of 64 trillion addresses being distributed by IEEE. IEEE allocates these numbers to network card manufacturers, who encode them into the firmware on their cards. It is almost in the hardware, encoded in the firmware in the card, the destination and source address fields of the MAC frame have 48 bits set aside. Remember 6 bytes is 48 bits. There is a 2-bit discrepancy. These 2 bits are used for something, the standard also allows for a 16-bit address but they are rarely used. (Refer slide time: 52:21 ? 53:47)

In these 2 bits, the most significant bit is set to 0 to indicate an ordinary address, and 1 is to indicate a group address. This is for multicasting, which means that frames are sent to several hosts. Remember, in the beginning of the lecture we said that although the medium is broadcast, usually there will be 1 sender and 1 receiver, i.e., it is a unicast; that means 1 sender 1 receiver.
Sometimes you may have a broadcast; that means it is meant for everybody, and sometimes you may have a multicast; that means it is meant for a few of the hosts in the network, not for all. How do you indicate the address? You indicate a very special address starting with a 1 for multicasting. If all 48 bits are set to 1, frames are broadcast. So for the destination field if all bits are 1, there is a very special address, not the address of any particular machine, which is meant to match with any Ethernet address on that particular LAN. If 2 most significant bits are both 0, the 46 least significant bits contain the MAC addresses of the source and destination hosts. This is how you make out. (Refer slide time: 53:53 ? 54:49)

Also this is connectionless and unreliable in the sense that it does not take any special care for reliability. This is also non-deterministic network; this means that no host is guaranteed to be able to send its frame within a reasonable time, just a good probability of doing so. Theoretically what might happen is that you may try to go and find it busy. Again you come back and then go; it might collide next time you come back and this might happen an indefinite number of times. Although the probability of its happening in a large number of times becomes lower and lower, with a very good probability, we will be able to send it but there is no guarantee that within this time you will definitely be able to send your frame. When the network is busy, the number of collisions rises dramatically and it may become very difficult for any host to transmit its frames. (Refer slide time: 54:50 ? 55:19)

This makes it not suitable for a real-time computing application. At least this bus type network may not be suitable for a real-time computing application such as an assembly line, which will demand that data be transmitted within a specified time period. Since the 802.3 bus LAN cannot guarantee this, its use for real-time applications may not only be undesirable but sometimes potentially dangerous. There are other ways of very high-speed network when this is low; then the chance of not being able to send at all becomes very low indeed. Just one (Refer slide time: 55:36 ? 56:42)

last word about the CSMA/CD efficiency. We will not go to the details of this discussion. Suppose each station transmits during a contention slot with probability p. The probability A that some station acquires the channel in that slot is A = kp(1 ? p)k?1. So p is the probability that a particular contention slot is taken by one node and 1 ? p is the probability that all the other k ? 1 nodes have not taken and that a particular slot is being used, i.e. kp(1 ? p) k?1. So A is maximized when p = 1/k with a tending to 1/??as k tends to ?. The probability that the contention interval has j slots in it is A(1 ? A)j?1.  (Refer slide time: 56:43 ? 57:02)

The calculation of channel efficiency would be p, where p is the time that a node needs to transmit mean frame and 2 ? is slot duration so p/p + 2?/A means that if p is large, that means if we are sending a large frame, the channel efficiency increases. (Refer slide time: 57:03 -57:15)

As you will see in this slide, with 1024-byte frames, the channel efficiency is very high. If you send in small frames, the channel efficiency tends to be smaller. This plot is with a number of stations trying to send. With this, we come to the end of this lecture. In the next lecture, we will look at the modern versions of Ethernet like switched Ethernet and how different networks are connected together. Thank you.
COMPUTER NETWORKS

Lecture name #20

Prof. Sujoy Ghosh

Dept of computer science and Engineering
I.I.T kharagpur
Modern Internet
(Reference time: 00:46)
Good day! In this lecture we will continue our discussion about Ethernet. In the previous lectures, we have seen the basic data link protocol used in Ethernet. Ethernet is a very popular and nowadays it is almost ubiquitous in the LAN area. It is a very widely used system and it has evolved from whatever was the approach to Ethernet in the earlier days and the basic Ethernet protocol that we had discussed. People are sort of shifting away from it because as the technology develops and as things like switches etc., become cheaper as the speed of network goes up, there is a slight shift in emphasis in modern Ethernet. In this lecture, what we are going to do is that first, we are going to have a look at the physical layer of the Ethernet. There is not much to discuss about that, and then we will see how we are shifting from a shared medium to a switched Ethernet kind of concept, and how speed is increasing. Then, we will discuss a little bit about Ethernet, LAN as a concept is. So, we discuss the modern Ethernet that is how the Ethernet is evolved. (Refer slide time: 02:15 - 02:20)

Before we go into that, (Refer slide time: 02:21 ? 02:47) 

we will look at the topology and some of the limitations of the earlier versions of Ethernet. The topology of a network refers to the pattern of workstations, cables and repeaters. In an Ethernet, there are two primary limitations to the topology: transmission accuracy and end-to-end transmission time. Transmission accuracy means that Ethernet really under a data link layer is an unreliable protocol in sense that it does not do a lot of acknowledgement or negative acknowledgement and things like that. Therefore, essentially the transmission accuracy should be fairly tolerable. What happens is that the maximum distance for accurate transmission is determined by signal attenuation along the cable, and it would depend on the on the quality of the cable, the kind of cable we used and the data rate. The other important point is the end-to-end transmission time as we have seen, because of this collision and other things this end-to-end transmission time is important. It?s not just for the data rate or things to move faster but for other reasons, also this end-to-end transmission time is a factor. (Refer slide time: 03:48 - 04:56)

These are the Ethernet physical layer standards that we have ? some of them, they are actually more. So 10Base5, that is, 10 Mbps base band transmission and 500 m; this 5 here stands for 500 m cable length, this is called thick Ethernet or has thick coaxial cables and this has become obsolete. Then 10Base2 is 10 Mbps base band transmission; once again all of these are base band transmissions; that means we do not modulate them to a higher channel or frequency channel. So, 10 Mbps and 2 for above 200 m cable line, or lines of 180 to 200 m length. Then, 10BaseT; this again is for 10 Mbps base band transmission, which uses UTP cable. There are various versions of UTP cable like categories 3, 4, 5, etc. 100Base-TX is 100 Mbps base band transmission, also uses UTP cable. (Refer slide Time: 04:58 - 06:28) 

So, these are the of maximum segment length. As we can see, 10Base5 is 500 m; 10Base2 is 185 m; 10Base-T is 100 m. Fibre optic cable 10Base-Fl goes up to 2 km. Actually, 10Base-F is not in much use these days, because fibre optic components became cheap and available, and people have moved from this 10 Mbps rate to the 100 Mbps rate. The point is that, the fibre optic cable goes much longer than copper cables and it is best between buildings; that means when it goes through some open space, etc., we avoid copper cable most times because of electric interference and other problems, not to mention the distance. The nodes per segment in 10Base-F could also be more; we will come back to this topic of nodes per segment later. As we have seen previously, 10Base5 and 10Base2 have many limitations; for example, in 10Base2, you see the figure of 30 and a network segment having only 30 nodes is a constraint these days because all kinds of things are being networked. For completeness sake, I will make a mention of 10Base2 and other kinds of (Refer slide Time: 06:51 - 07:18)

Technologies, although they have become obsolete now ? 10Base2 has 10 Mbps, 200 m maximum cable length, is a thin coaxial cable in a bus topology; this is a classical bus topology. Repeaters are used to connect up to multiple segments. Repeater repeats bits; it hears on one interface to the other interface, so this is a physical layer device just for amplification, strengthening the signal. (Refer slide Time: 07:19 - 07:53) 

This is a figure showing how they are connected. 
We have this thin cable, which has terminated on both the ends and these nodes would be connected using some t connectors over here, and this is the network adapter. When a network adapter pushes some signal on to the cable, the transmitted packet travels in both the directions. (Refer slide time: 07:55 - 08:32) 

This is just a mention about 10Base5, because connecting a 10Base5 is more cumbersome and in 10Base2, connection would get loose quite often in the earlier days. Anyway, this is a 10Base5; it has a thick coaxial cable, the transceiver, and transceiver, drop cable and there would be a network interface card here with this MAC unit and protocol control firmware etc. As I said, this has become obsolete now. (Refer slide time: 08:33 - 09:21) 

Let us talk a little about repeaters. Repeater is a physical layer kind of device, which regenerates the signal, and provides more flexibility in network design because it depends on where we want to extend your LAN. So if you want to extend your LAN in some direction, make running into this distance limitation 200 m for 10Base2 etc. You may sort of increase that by just putting another segment on the other side of repeater so that you can go another 200 m. Therefore you can extend the distance over which a signal may travel down a cable; an example of a repeater is Ethernet hub. A hub has two purposes: one is to provide a collision domain and replace the cable. The other function of the hub is the repeater function; that means it regenerates the signal. This connects (Refer slide time: 09:48 - 10:02) 

together or a repeater or a hub. It connects together one or more Ethernet cable segments of any media type. If an internet segment were allowed to exceed the maximum length or the maximum number of attached systems to the segment, the signal quality would deteriorate. (Refer slide time: 10:03 - 10:36)

You can see that you have one cable segment here. This goes up to its maximum distance there is a cable segment too again going up to the maximum distance. But we put this repeater in between so that, we can increase the total distance that may be covered. These are the different computers connected to the cable. It is used between a pair of segments; this is a simple repeater to provide signal amplification and regeneration to restore a good signal level before sending it from one cable segment to another. (Refer slide time: 10:37 - 13:02) 

Hubs are essentially physical layer repeaters; bits come in one link and go out all other links. There is no frame buffering or CSMA/CD at the hub. Adapters detect the collisions and provide net management functionality, which means that this hub, although it is an active device, in the sense that this has some electronics in it, in its action it is more passive. Its active component is restricted to the fact that it regenerates the signal; it amplifies and regenerates it. So far as the other intelligence is concerned, like detecting collisions, it does nothing like that. If two signals coming at two different ports of the hub come to the hub at the same point of time, they will collide. So, a hub is a whole co-axial cable, which has been collapsed into one collision domain inside the hub; we could look at that way. But it does provide some net management functionality. That means, if you have a managed hub, there are two kinds of devices. The point is that a network is usually a geographically distributed, dispersed kind of entity; but this has to be managed. Therefore if there is a problem somewhere, may be some user has put in a complaint, what we will do is that there are so many things which connect from the user to the central network. There are cables, may be switches, hubs, and other things, so it is important that you should be able to remotely tell whether some active device is functioning. We cannot do this on passive things like a cable but there are managed switches and managed hubs etc., whom a central network station may interrogate and find out whether its health is alright or not. So if you have a managed hub, you have net management functionality; of course, the managed hubs are costlier than unmanaged hubs. (Refer slide time: 13:03 - 13:24) 

So this is the picture of hub. This is a co-axial cable coming in here and this is a t joint; the t joint is feeding into the hub. There may be a number of ports and a number of segments may be connected together. (Refer slide time: 13:25 - 14:40) 

Now, we come to 10BaseT; so far as the physical part is concerned, this 10BaseT can go on to become 100BaseT; that means, from 10 Mbps it can go to 100Mbps. But for this UTP, unshielded twisted pair cable of various categories like 3, 4, 5, etc., are the dominant physical medium and the varying technology today for local area networks. We can have category 5 cable data transmission up to 100 Mbps and we can go a little bit more on this depending on the distance. The impedance of a UTP cable is about 100 ohm, give or take another 15 ohm over the useful frequency range of the cable. The transmission speed is 0.6 to 0.65 ? c, which is about 2108 m/s. Time delay is about 5 nanoseconds /m. (Refer slide time: 14:42 - 15:40)

Here, you can see there are some 10BaseT repeaters and then 10Base-Fl segments. This is just to give you an idea about the various delays which are involved. The first segment delay is about 0.57 ?s for this 100-m segment, middle segment delay is about 10 ?s and last segment delay of about 0.57 ?s. The transceiver delay is about 7 ?s, Fl repeater delay is about 1.7 ?s, 10BaseT delay is about 21 ?s and so on. All these delays really add up and you have some limit on this in order for your CSMA/CD to work properly. (Refer slide time: 15:41 - 15:57) 


So this is the same data in tabular form. It has various types of cables and these are all fibre optic kind of cables, which go up to 2 km or 1 km. These are obsolete now ? these are the maximum lengths and these are the delays, the receiver delay and the delay at the repeater and so on. Now, what happened was that the people moved as the demand for bandwidth grew; so at least in the LAN it moved much faster when LAN bandwidth became quite cheap and everybody wanted to move from 10 Mbps to the next speed, which is 100 Mbps, and this was called fast Ethernet. One thing was that, when we come to fast Ethernet, this co-axial cable really was out. So the thing preferred was the unshielded twisted pair or UTP. Category 5 is preferred for 100 Mbps operation, although category 4 would also work. It is possible to transmit it over category 4 cables also; but category 5 cables are preferred. Now one thing about this UTP cables is that because they are used for connection from end to end unlike the coaxial cables ? in coaxial cables, we have one cable and many nodes may be connected at intermediate points ? you put a connector, usually an RJ45 kind of connector on the two ends and then go into two nodes. It is as simple as that. So if there are no nodes in between, just two end point connections, where this part of the medium is not shared, at one end may be the computer, and the other end might go into another machine straight away. These two machines network together. Usually the other end would go to a network device. Let us say, to start with, it might go into a 100 Mbps hub. So, all the nodes which are connecting, that means which are part of the network, all of them are connected by this UTP cable to the central hub. So the central hub is a shared medium at the collision domain. But so far as the UTP part is concerned, there is no collision. 

Just as we had the 10Base-T, which supported 10 Mbps, may be with the same kind of cable with this UTP cable, we could support fast Ethernet, that is, 100Base-T. There was 100Base-TX, But there are some problems with this: one is to increase to 100 Mbps, we have got one order of magnitude jump in the speed. That is not a mean achievement on the same medium. Depending on the quality of the medium, as the speed goes up, its performance tends to decrease. So depending on what kind of medium it is, there is only a certain amount of bandwidth in a certain range that you can support on that medium. So to increase to 100 Mbps, different encoding is used to reduce the bandwidth of the transmission. Encoding data at 100 Mbps using Manchester encoding would create a bit stream of 200 Mbps. So, we cannot straight away use Manchester encoding here, because this is for supporting 200 Mbps, we requires at least 100 MHz of analog bandwidth, which is too much. Category 5 UTP cable is rated up to 100 MHz but for keeping some margin etc., transmitting a signal of
100 MHz bandwidth would be unreliable. So what we do is that instead of Manchester encoding, 

we use a different kind encoding, which is called multiple or multilevel encoding. To reduce the bandwidth, a different encoding scheme is required. 100Base-TX uses a multiple level encoding scheme, that is MLT3, Multiple Level Transition 3. The bandwidth required in this case has come down quite sharply to 31.25 MHz. The encoding is somewhat like this ? 0: there is no transition and 1: there is a transition. And this transition could be of various types from low to 0, high to 0, 0 to low or 0 to high, depending on the context. 

For example, this is a 10110001110 that is being encoded. Suppose with the 1 there is a transition from 0 to low, that is, 0 to low level, then for 0 there is no transition, after 0 there is 1. Now there is a transition; since we go to low now, and the only place to go to is high, we go to a high and then there is another 1 and since we are going high, we go high some more, that is, another 1. Should this go from 0 to high, then three 0s, that is, no transition at all. Then from 0 to 1, now from high since 1 has come, you will have to come to low and again another one has come; we are going in the downward direction. So we continue from 0 to low and then, another 1 from low to high and then 0; there is no transition. So, this is the way an MLT encodes using different levels or multiple levels of signals. This reduces the net analog bandwidth requirement for the cable. So this is how, the ten-fold increase in the speed was achieved on the same physical infrastructure. So bandwidth required for MLT is half the bandwidth required for a two-level scheme.

Half is a rough figure. Long stream of zeros will cause the line to hold a constant voltage and lose clocking, synchronization is necessary to read the signal. This is one problem, which is handled in various ways. 

One way is to encode each 4-bit sequence as a 5-bit pattern 4B5B encoding. In this 4B5B encoding what is done is that instead of a 4-bit sequence we introduce a 5-bit sequence so we get some transition and we can hold on to synchronization. 

This pushes up the bit rate to 125Mbps for100BASE-TX, where data is only 125Mbps/4 = 31.25 MHz. Category 3 or 4 cable is used for a rate of 100 Mbps by using all four twisted pairs in the UTP cable; so this is also possible. Three pairs will transmit data while the fourth pair is used by each station for collisions. As in 10BASE-TX, a three-level signal is transmitted in 100BASE-T4. T4 is when we are using category 4 cable but the data rate must be minimised further. Therefore the signal is broken into 8-bit sequence and each 8-bit sequence is represented as 6 three level signals, or 8/6t. If we are trying to put in a new network today we are never going to use category 4 UTP, we are always use category 5 or better. You know about the increasing requirement of speed in the LAN, but these techniques were developed so that Ethernet could move into some existing physical infrastructure like wiring, where category 4 cable is used for moving from 10 to 100 Mbps; that is why these were developed. 

Fast Ethernet is done with cabling of the following types ? 100BASE-T4 with category 5 100BASE-TX; and 100BASE-FX is quite common. As I said, 10BASE-Fl has become outdated but 100BASE-FX is still there, which uses fibre optics and goes up to 2 km. Category 5 UTP goes only up to 100 m or may be some people are conservative and limit it to 75 m. At full duplex at 100Mbps, fibre is again full duplex at 100 Mbps; these are the advantages. The advantage of T4 is that, it uses existing category 3 UTP. 

These are the repeater delays. 

10BASE-T and 100BASE-TX wiring goes like this ? from the end station a maximum distance of 100 m to the hub. We can use multiple hubs, a maximum of four, to increase the distance between any two stations; a hub may go to another hub; that may go to a station again. Now, there are three segments. We can use a maximum of four hubs and five segments with 100 m from the node to the hub. 

For upgrading from 10BASE-T to 100BASE-TX, we need new hubs or switches. You may have some 10 Mbps ports to handle 10BASE-T; NICs may have auto sensing 10/100 ports that handle either. Auto sensing ports are quite common. As a matter of fact, most of the network interface cards you get today are 10/100; that means they are auto sensing ports. An NIC senses the network device at the other end. If the other end supports a rate of 100 Mbps, this NIC will operate at 100 Mbps. If it senses that the opposite end can handle only 10 Mbps, it will use only 10 Mbps. By the way, if there are problems with the varying connection, etc., it may send 100 Mbps port as 10 Mbps. So, if you want to upgrade from 10BASE-T to 100BASE-TX, you may need new NICs, and only for stations that need more speed and no need to rewire ? that may be a big advantage in some cases. 

We may have 100 multiple hubs, one connecting to the other; you can have multiple hubs for connecting etc. 

In 100BASE-TX there is a limitation that you can have only maximum two hubs and they must be within a few meters of each other. So, the maximum span using these hubs is only 200 m, which is shorter than 10BASE-T. It could be a big problem and that is why 100 Mbps hubs are also becoming obsolete. People have migrated from hubs to switches, and that is what we will be talking about. So if you had used hubs, this would have been a limitation; but nowadays people do not usually use hubs. 

We will come back to the topic about hubs and switches later on. What happened is that this development went on and now we are moving into the era of Gigabit Ethernet. The Gigabit Ethernet in the desktop is still not very common, but it is coming and for servers etc., nowadays we routinely get Gigabit ports and, may be, soon your desktop computer may also be connected with Gigabit ports. So, this is a depiction of a two-station Ethernet ? two computers connected straight away, this is a switch or a hub which operates at Gigabit speed. So, these allow point-to-point links and shared broadcast channels through some hubs if they are there. In shared mode, CSMA/CD is used for the short distance between the nodes to be efficient; but this is not very common.

It uses standard Ethernet frame format 802.3z; the normal mode is full duplex mode with a central switch. This is the most common one connected to computers. In this configuration all lines are buffered. 

On the right-hand side, you see one switch and the nodes are connected straight to the switch. So this is a switch rather than a hub, where each of the ports has a buffering. 

Since each of the port is buffered, contention is impossible; because data can always and gets it on the buffer. So, it makes no sense to the channel to see if it is idle or not because if there is no contention, CSMA/CD protocol is not used at all. And this is the mode, in which Gigabit is usually used, 


although there are buffered distributors also. Now people are talking about moving to 10 gbps, at least in the metropolitan area network. They use copper and fibre with 850 nm or 1300 nm lasers and some kind of encoding. The Ethernet is moving really fast, because it is upgrading quite fast ? it went from 10 mbps to 100 mbps; from 100 mbps we are moving to 1gbps; and from 1gbps, we can see the possibility of moving to 10 gbps, and 10 gbps is already in operation today. From 10 mbps to 10 gbps is a 1000-fold increase in a very short span of time, which is really amazing. For Gigabit (Refer slide time: 32:16 - 32:56) 

Ethernet, we have 1000Base-SX; this SX shows the multimode fibre for 1000Base, so the maximum segment size would be about 550 m. 1000Base-LX uses single or multimode, so this goes upto 5000 m. Shielded twisted pair would only take to 25 m and 1000BaseT can go up to 100 m using category 5, category 5b or, for gigabit Ethernet, category 6 is preferred. We will talk more in detail about the switching, which is very important. So, Ethernet is (Refer slide time: 33:17 - 33:38) 

usually considered as a shared media LAN. Only one station can be transmitted at a time even when you have multiple hubs, these hubs are all shared domain, which means that only one can transmit. All other stations must wait while one station sends. So this is the big limitation of latency and congestion with hubs (Refer slide time: 33:39 - 33:45) 

To improve this, people moved over from hubs to switches. Packet switches are very useful. They give you improved security. Users are less able to tap into other users? data. In a shared medium, we know everything is being broadcast so all packets are going to every other node. That is poor security; this gives you much better security; better management ? we can control who receives what information. That means, even using a switch, even within a LAN, we can create a work group and make users members of different virtual LANs. These virtual LANs may be packet filtering between the virtual LANs, etc., to give a better management and limit the impact of network problems. This switch will also give you full duplex communication, rather than half duplex. One side effect of having a shared medium is that only one sends, and every body else is in the receiving mode. Obviously that channel cannot work as a full duplex mode. Because two of them cannot send simultaneously at the same time, only one may be sending and the other may send. At most, this works as a half duplex channel, whereas over here this can work as full duplex channel. A hub simulates a single (Refer slide time: 35:20 - 36:07) 

shared medium, whereas switch simulates a bridged LAN with one computer per segment. For example, we have only one computer and  many segments, and there is only one computer for each segment. So it does not have any collision and from two different computers you can send data at the same time to the switch. The switch will buffer it and  there will be no collision. So collision detection may be eliminated. Some of these may connect to a hub instead of going to a computer. So a switch may be connected to another switch, that switch may connect to hub, and that hub may connect to so many computers. So there are many possibilities. (Refer slide time: 36:08 - 36:01) 

Ethernet switches are highly scalable. We had 10Base originally; we went from 10BaseT hubs to 10BaseT switches. 10BaseT switches are increasingly being replaced by 100Base-TX switches with auto sensing codes. 100Base-TX switches are very common these days, they have higher performance and their costs have come down. These days we get 100Base-TX switch at the cost of what a 10BaseT hub would cost a few years ago. We can see how the technology and the economy have improved. Gigabit Ethernet switches are still expensive; it will take some more time in order for them to move to the desktop. (Refer slide time: 37:02 - 37:16) 

Other advantages of switches are that there are no limits on the number of Ethernet switches between the farthest stations and no distance limit on size of switched networks. You remember when we were talking about 100Base-TX and when we were using 100 base TX hubs. You can only have a maximum of two hubs and these two hubs have to be close together; they cannot go far long. So, the maximum span of a local area network could almost be 200 m, which was really a limitation, but then people moved to100Base-TX. They also moved from hubs to switches. With these switches, now there is no problem because the distance limitation had to do with the round-trip propagation delay and things like that. Since there is no contention and the switches have buffers, there is no distance limitation at all. You can have any number of switches connected to each other; there is no distance limit on size of switched networks. So often individual hosts, are connected to switches and may be different switches are also connected. This is Ethernet without collisions or CSMA/CD. (Refer slide time: 38:17 - 38:55) 

Here, we have A, B, C, and so on. Suppose A wants to communicate with A' and B wants to communicate with B', both the communications may take place simultaneously. This is a switch. If this had been a hub that would not be possible but since this is a switch, B can communicate to B' and A can communicate to A', just as in a telephone switch, two pairs or distinct pairs of people can communicate at the same time. Similarly, two distinct pairs of machines, through this packet switch, can communicate at the same time. (Refer slide time: 38:56 - 39:22) 

This is essentially a multi-interface bridge; it has a layer 2 frame forwarding, filtering using LAN addresses; LAN addresses mean MAC addresses. We will talk more about MAC addresses presently. It allows switching A to A' and B to B' simultaneously, but there are no collisions. This can have a large number of interfaces. A modern switch may have hundreds of ports, so hundreds of machines can be connected to the same switch. Similarly, a number of such switches can connect to each other. Ethernet switches (Refer slide time: 39:34 - 40:04)

must be arranged in a hierarchy or daisy chain with only one possible path between any two stations. Suppose, from 4 to 3 there is only a single path that is 4, 2, 1, 3; you do not make a loop connecting two ports of a switch because that would really create a lot of problem. Therefore, Ethernet switches must be arranged in a hierarchy or daisy chain. There is only one possible path between any two stations or switches. (Refer slide time: 40:05 - 42:27) 

A station is an Ethernet frame switch plug-in card. It checks to see if it is destined to the same card. If so, the frame is copied there. We mentioned that in a packet switch, usually the ports would be grouped into small groups and each of these groups will go into a particular line card. This line card is similar to the Ethernet. A switch is nothing but a packet switch; there will be line cards over there and may be 4 lines coming to one particular line card and may be there are 8 line cards for a 32-port switch. Now, for those that are coming into the line card, there are variations of how they are handled by the line card. It could be that a frame which has arrived in a switch plug in card is destined to the same card; that means it is destined to some other line in the same card. If so, the frame is simply copied there; if not, the frame is sent. Now these line cards should get connected to each other through the backplane. If the frame is sent over from the high-speed backbone to the destinations card, it will travel to the other distinct destination through the switching fabric. This backplane may be active or passive; they are of various types. Anyway, the backbone typically runs at many gigabits per second. As a matter of fact, nowadays it is possible to have a high-end switch with something of the order of 100 gbps backplane switch. It is a very high-speed switch that can support a lot of machines at the same time. If two machines attached to the same plug in card transmit frames at the same time, what would happen? There are two machines attached to the same plug in a card and they transmit frames at the same time ? what would happen would really depend on the way the card handles it; here there are some variations. For example, let us say, there are 4 line cards ? 1, 2, 3, 4 ? coming into this one particular line card. Line 1 and line 2 frames arrived simultaneously and both of them are destined to line 3; what is going to happen? It depends on how it is handled. One possibility (Refer slide time: 42:54 - 43:41) 

could be that all the ports on the card are wired together to form a local, on-card LAN. This LAN card itself is a local on-card LAN, and on this on-card LAN, there will be a CSMA/CD network. CSMA/CD may operate on that particular card itself; in that case, there will be a collision that will be detected. Only one transmission per card is possible at any instant of time, so if you are doing that, you have made that line card to be a shared medium. All the cards can be transmitting in parallel; the different cards may be transmitting in parallel. Each card forms its own CSMA/CD network independent of the others; that is one possibility. (Refer slide time: 43:42 - 44:04) 

The other possibility is that the plug-in card of each input port is buffered; so incoming frames are stored in the cards on board. This allows all input ports to receive or transmit frames at the same time for a parallel, full duplex operation. This is not possible with CSMA/CD on a single channel. Previously we had talked about buffers in packet switches. We mentioned how buffers could be on input side, how buffers could be on output side and distributed through out the switch fabric. That was from the point of view of handling head of line, blocking, switching, speed etc. Here, it is slightly different in the sense that if there is some small buffer at each of the ports and on input side, full duplex transmission at the same time is possible on all the lines. Instead of making a local CSMA/CD and allowing some kind of collision, this naturally is a better kind of switch. Once a frame has been completely received, the card will then check to see the frame is destined for another port on the same card, or for a distant port. In the A case, it can be directed to the destination; in the B case it must be transmitted over the backbone to the proper card. (Refer slide time: 45:12 - 45:21) 

Each port has a separate CSMA/CD so collisions do not occur. There is now no question of any CSMA/CD because each port has only one machine. (Refer slide time: 45:23- 45:51) 

Then there is another term called cut through switching: frame forwarded from input to output port without waiting for assembly of the entire frame. That means even when the frame has not fully come in, the bits are transmitted and there is a slight reduction in latency. So, combinations of shared dedicated 10/100/1000 mbps interfaces are possible. On the same switch you might get some 10 mbps ports, some 100 mbps ports, also some 1000 mbps or Giga bit ports for uplinking to the main backbone. (Refer slide time: 46:03 - 48:04) 

This is a typical LAN of an IP network. Let us look at this figure ?  some hubs are remaining, but these hubs will give way to switches from 10BaseT to 100BaseT very soon, but let us say, there is a legacy system, through which some 10BaseT hubs and some computers are connected to this. These hubs may be connected to a switch. This switch could be connected to another switch and that switch might connect to some other nodes, etc. This constitutes the LAN ? may be there is a 100 mbps connection from this switch to the mail server, because mail server may be something which everybody is using. This is a server, and you might want to have a higher speed of this one also ? these are 10 mbps ports. You might want to make it 100 mbps. There is a www server, that is, a web server, which is also connected through a 100 mbps port. So from the same switch, you may connect to some hubs, you may connect to some servers, you may connect to some individual desktops, PCs or machines. So this is what the LAN part looks like. And of course, nowadays nobody would want a LAN which is standing by itself. People want to connect their LANs to the internet, which is the great network of networks. There are probably lot of networks, may be millions of networks, in other places; we want to connect to other networks also. So we have to go through what is known as a router. We go through a router to the external internet. We will be talking about this part when we talk about the network layer in more detail; so this is a typical picture of a LAN. (Refer slide time: 48:05 - 48:34)


If we abstract this a little bit, we have a LAN, and different nodes or the different machines are connected to this LAN. And then there is a router, which connects you to the internet. This internet may be a connection between a lot of different networks, may be a lot of different LANs to their own routers etc. This is a slightly more abstract view. (Refer slide time: 46:35 - 48:39)

And then looking into it in detail in the LAN part of it, if we go to the previous picture, (Refer slide time: 48:40 - 48:44)

this is the LAN part of it and these nodes are connected. These nodes are usually connected (Refer slide time: 48:45 - 51:34) 

through adapters. That means the network interface card, NIC, or the adapter. These nodes may communicate with each other in the LAN if it is connected to the internet. Naturally it may communicate with the other machines in the outside world also; but let us for the time being just focus our attention on the nodes which are in the LAN, and they are sort of trying to talk to each other. If they want to talk to each other using either a hub or a switch, or in this case, the Ethernet frame format, what is going to happen is that the frame format has certain fields and those fields have to be filled up. That means the destination address has to be there, and the destination address is the LAN address or the MAC address or the hardware address. Sometimes this is also called an Ethernet address. And this Ethernet address is distributed by IEEE; the manufacturers of these adapters buy blocks of addresses from IEEE. And these are all 6-byte addresses, something like this: 1a23f9, each of then is 1 byte. And if you look at this figure, say 1a23f9cd069b and 88b22a do not have any relationship with each other. They are just 6 bytes, may be 6 arbitrary bytes. This card may have come from some manufacturer and got entirely different blocks from IEEE, the other may have an entirely different address, which has no relationship with the other; but they are all sharing this LAN. There is a small problem ? how does the source node get to know the destination address? One thing is sure ? since one central agency, namely IEEE, gives this Ethernet addresses, each adapter on LAN has a unique LAN address; no two adapters will ever have the same address 
Sometimes, there may be some nodes with a number of LAN addresses, with a number of adapters etc., but one particular adapter will have one unique address, two addresses will never be the same. The point is that, how do get to know this is the source? This is a destination ? how does the source know the destination, LAN address or the MAC address? (Refer slide time: 51:35 - 51:46) 

There are at least two kinds of addresses, which are used in networks. One is the IP address, which is used by the network layer to decide on the total route from the source to the destination. What will happen is that it will know where it will have to go; in order to reach its destination address, it must have some kind of relationship with geography. And then, there are these hardware or MAC addresses, which are at a lower level and are just as unique; they do not have any relationship with each other. So we have this 32-bit IP addresses, (Refer slide time: 52:32 - 52:50) 

network layer addresses used to get datagram to the destination IP. We have LAN, MAC, physical, Ethernet or hardware address used to get datagram from one interface to another physically connected interface in the same network. So this is a 48-bit MAC address for most LANs burned in the adapter ROM. (Refer Slide Time: 52:51 - 53:01)

Analogy of MAC address is like a social security number; IP address is like postal address. The question is how do you get to know the (Refer Slide Time : 53:02 - 53:16 )

other person?s address? Address translation: how does a host know the MAC address of the other host in its local area network? And secondly, how does a router determine the MAC address from the IP address of a host? This are two interrelated questions. (Refer Slide Time : 53:17 - 53:36 )

We need some kind of a translator for IP addresses from MAC addresses. Address translation can be static or dynamic; static translation means you keep some static table. Static translation requires a lot of book-keeping work from system administration and this is not feasible. (Refer slide time: 53:37 - 54:39) 

We have this address resolution protocol or automatic resolution protocol, the so-called ARP protocol, which is a dynamic address translation scheme. If the destination IP address is not in the local ARP cache ? ARP cache is a table, which is stored in the local machine and which gives this IP address to MAC address mapping (the broadcast query for that address) ? at the most one host will reply because the IP address will match with only one of the host addresses. So, the ARP table entries are cached for up to 15 minutes, unless if refreshed. It is firmly at 15 minutes so that a machine can go from one LAN and can also be shifted to another LAN. What will happen is that these old entries must automatically wash out. And if the new entries communicate to their communicating machine, their MAC addresses are going to come into the cache. So this is how it is always kept fresh. ARP uses layer 2 broadcasting (Refer slide time: 54:40 - 54:59) 

Each IP node on LAN has an ARP table. ARP table is IP, MAC address mapping for some LAN nodes. And there is a time to leave, something like 15 minutes or so, it depends on the time after which the address mapping will be forgotten ? it is typically around 15, 20 or 25 minutes. (Refer slide time: 55:00 - 55:36) 

So starting at A, we want to send a datagram addressed to B. We look up the net address of B or link layer. What will happen is that we want we do not need B?s MAC address. That means this 2 23.1.1.1 is an IP address ? we know it is B?s IP address, but we want to know its MAC address. So A?s IP address, B?s IP address and some IP pay load form a frame like this, which is broadcast. (Refer slide time: 55:37 - 56:08) 

A wants to send datagram to B and A knows B?s IP address. Suppose B?s MAC address is not in the ARP table, A broadcasts an ARP query packet containing B?s IP address. All machines on LAN receive the ARP query. B receives the ARP packet; replies to A with B?s MAC address; the frame is sent to A?s MAC address, which is unicast. It is now specifically given as MAC address is known, because A sent the query with a source address. (Refer slide time: 56:09 - 56:34) 

A cache saves IP to MAC address pair in its ARP table until information becomes old, that is, it times out. But if it is being used, then it will not become older. So in a soft state, information that times out, goes away unless refreshed. ARP is plug and play, otherwise it will get washed out and new entries will come in automatically. Nodes create their ARP table without intervention from the node administrator. (Refer slide time: 56: 35 - 56:53) 

All hosts receive the request but only B responds with the IP address; however, other nodes may update their own cache from the transaction. The cache is required since broadcast is a very expensive proposition and more than one packet transfer is very likely between any two stations. (Refer Slide Time:56:54 - 57:12 )

This is an important question, what is a network? We always talk about networks; it is the interconnected nodes in the same broadcast domain. We are doing the broadcast ? this broadcast is limited to that domain. Networks are connected through routers. (Refer slide time: 57:13 - 57:30) 

When we want to route to another LAN, it is more involved. We will talk about going through a router and then we will specifically discuss bridges. Thank you. (Refer Slide Time: 57:45 - 57:47) 


Today we will be talking about local internetworking. (Refer slide time: 57:56 - 58:18)

Our topic of today is local internetworking, what is internetworking? Internetworking is connection of different networks, everybody is aware of the term internet today why are comes to this term internetworking and by internetworking we mean connecting different networks. Just to remind you, (Refer slide time: 58:20 - 58:40)

What is a network? We discussed this in the last lecture that the interconnected nodes in the same broadcast domains. And networks are usually connected through routers but as we will see for local internetworking we may not need a router we may need something called the bridge we will come to that. This is the set of nodes which are in the same broadcast domain, this broadcast may be a good thing to have for some applications. But for the operation of the network there is one very crucial reason, why we require the broadcast that is to discover the MAC addresses of the different computers.




	











