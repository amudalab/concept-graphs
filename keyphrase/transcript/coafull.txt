

TRANSCRIPTION: BRINDHA
                                  
COMPUTER ARCHITECTURE

Prof. Anshul Kumar
Department of Computer Science and Engineering
IIT Delhi

LECTURE# 21
Processor Design - Control for Multi Cycle Design


Um we have been discussing design of a processor where execution of each instruction is divided into	 multiple clock cycles 

we have seen how the data path is designed and today we will look at the controller aspects how such a data path could be controlled 
(refer slide time 00:01:20)



in the sequence of lectures on um designing of processor we started with um very simple design where everything about an instruction was done in a single clock cycle 

we notice the problem with performance and um there are other issues 

so we have um moved to a different style where the instruction is divided into multiple clock cycles 

so we will um look at how wha what are the actions which are do done in different clock cycles for various instructions (refer slide time 00:02:44)



and um then by putting these sequences of actions together we will try to build the flow of control for carrying out these instructions

um we will then try to identify what control signals are required to control this data path in each of the um control steps 

for doing so we will group the control signals into um groups and we will define some meaningful operations called micro operations

so each instruction would be viewed as a set of micro operations which are done either together or in sequence and um that will somewhat simplify establishing a relationship between control states and the signal values

um finally we will see how control states um transit from one to other and um that will complete the design of control part 


so this is starting point we had arrived at this data path last time 
(refer slide time 00:03:00)




where the key resources are um memory register file and ALU and we have tried to use this um as far as possible so that there is a maximum utilization of all the resources

and um you could also recall that um all intermediate results we try to store in some registers  

so for example that when instruction is um brought out from memory it is stored in instruction register when data is read out from memory it is stored in data register (refer slide time 00:03:37)



similarly operand from register file are brought out into registers A and B and um the results of ALU operation are kept in register called Res short for result 

so now we we need to control all these components um all the registers may not change their states in every cycle 

so so each register will have a signal indicating when does we write something into those registers um we have control for the multiplexer as usual and similarly control for ALU register file and memory 

but um since we have new components like registers multiplexer also has multiplexer also have um either changed in their size or in their organization we will have to redefine some of these control signals 
(refer slide time 00:04:36)



so um first lets get back to the instruction and see how those instructions are divided into um operation which are done in different cycles okay

so what activity is done in which cycle that needs to be clearly recorded um before we can start working with the control signals [noise]
(refer slide time 00:06:44) 



so starting with R class instruction um in the first cycle we um read one word from memory into instruction register that form the instruction address of memory come from PC 

and at the same time um PC takes on the new value so both these operations are done concurrently within first cycle 

in the next cycle um we read the operand from the register file [noise] um and these two are brought into A and B

the addresses of register file um [noise] excuse me are provided by the instruction and the relevant fields here are bit twenty one to twenty five and um bit sixteen to twenty for the second operand  

so so this corresponds to the first one corresponds to RS the second one corresponds to RT 

um once again what we meant by putting these two operations um in a single box was that both of  these are done simultaneously within a clock cycle number two

okay the next clock cycle um for these instruction would see the actual operation being performed by the ALU 

so i i have written in a generic sense A op B where op is the operation as would be [noise] guided by um the function field of the instruction IR zero to five 

and the last cycle we will involve transferring this result to the register file [noise] the address come from bits eleven to fifteen which corresponds to RD or the destination register [noise]

so this is how things have been divided and um we have made a careful choice of what gets done concurrently and what gets done in sequence
(refer slide time 00:08:12)


  
um the second instruction we will take is um store word and the first cycle involves um access to memory to fetch the instruction and updation of program counter

the second cycle involves um bringing the registers out okay we we need again to access two registers 

one which will participate in address calculation and second will carry the value to be written into the memory 

so um with same fields of the register file the same fields of instruction register file is accessed and registers are values are brought out in A and B [noise]

in the next cycle we calculate the address okay by adding offset coming from bit zero to fifteen of instruction with sign extension to A and this value is um temporarily kept in um this register called result 

in the last cycle we will see an access to memory where um contents of Res will be used as an address and the data to be written into memory is B and memory write is performed 

so once again we require four cycles (refer slide time 00:09:19)



the next is um load word instruction first cycle again is similar the second cycle involves um reading one register from register file um here we need to access only A because um the second address corresponds to um the destination here	 

what we read from memory will be stored in register RT so we need to read only one register and in the third cycle we calculate address in the same manner 

in the fourth cycle memory access is performed the data is read from memory address is carried in Res and the data which is read is brought into DR

in the last cycle um we store the DR value into register file the address comes from RT okay

so this is the sequence of five cycles which complete load word instruction
(refer slide time 00:10:26)



then we move on to beq first cycle is same in the next cycle we are doing lot of work one is to access A and B the two operands which need to be compared

and we also keep the result [noise] um ready um for the target address in case branch has to be carried out because the ALU is free in this cycle [noise]

in the next cycle we are going to use ALU for comparison so in this cycle it is free we calculate this address um but don t keep it in don t transfer it to PC immediately

um we we keep it in Res the condition is yet to be checked so after checking the condition we will either transfer it or not transfer it 

that is cycle three where the ALU will compare A and B the two operand which were brought out from register file and if the condition holds then this new address is transferred to PC

so um this req this requires only three steps (refer slide time 00:11:24)



um lastly we look at the jump instruction again the first cycle is same and in the second cycle we um compose this address of the next instruction by taking bits from PC and IR with um okay i missed out oh no there is no sign extension here

um this with two bit shift is  concatenated with  four  bits of  PC  and then transferred to PC 

so this this is done in two cycles [noise] now you recall that the impression i have i might have given earlier [noise] was that this can be done in a single cycle because only resource which is time consuming which is required here is accessing the instruction

after that there is um no memory access required no ALU operation required so the the total delay which we always use to think for this was max of t i and t plus 

but why are we um occupying two cycles here the the reason for that is um we want to do this operation sequentially the four bits we need to pick up from um PC part is actually after PC has been incremented

so after PC is incremented um one cycle is over okay in in the first cycle we are incrementing PC and the result is a put back in PC

so those four bits are picked up in the next cycle and put together with instruction 

al also you would notice that it takes one cycle to fetch the instruction so although what is being done here um is not a time consuming activity but since it has to be sequenced after that um it is occupying additional cycle 

so in in a in a single cycle approach what um was roughly taking equivalent of um the um memory access time or addition time one of the two now its taking um more or less twice of that (refer slide time 00:12:57)



so so this is just to recall that um in the single cycle approach the data path [noise] was like this that we we did instruction access and um PC update and directly we picked up the bits and formed the address alright

and we said that the time required is max of t plus and t i right 

so now if you have decided one clock cycle to be let us say um something which encompasses these um we we are still requiring two cycles because of the need of sequencing pins (refer slide time 00:13:27)



um the these were the timings um we had um conceived earlier and we we were actually imagining the time for jump to be [noise] just this much

so basically first clock period um which will accommodate t plus at t i would be used but we we are doing something now a very simple activity but now since the time is quantized discretized [went] off the clock periods we need to go beyond this and we cant do anything less than a cycle 

so we we use two cycles for this instruction is is that point clear why we have to [noise] um go to two cycles 

um and actually as we will we will  proceed further we will change things even for little worse but um that becomes essential okay 

now we have seen um the division of each instruction into cycles the division of activity of each instruction into cycles separately and once again we need to put things together to form overall flow of control 

so um these are the five instructions or groups (refer slide time 00:17:14)



and we will put their actions one after another in the same picture so that we have a global view of the whole thing okay

now i i have just to accommodate everything on a single screen i have omitted some um pieces of text which are more a matter of detail but what i have tried to retain is um um all the destinations of the operation where ever the results are going and the main resources which are [noise] being used right

so for um um for the first cycle in all cases you would see that there is a exist memory i have omitted some details here there is updation of PC

excess of um values in the register file performing um this arithmetic or logical operations storing in register file um storing in memory reading from memory and so on

so you can see that the essence of all these the the skeleton of all the  operations has been captured here and now um the task is to [noise] put these together

so so we have different instructions taking different cycles you would notice some commonality something is common and um that that that s where we can merge 

but um in the flow of control at some point we would need to branch because different instructions require um different actions 

so the first part is um first cycle is apparently [noise] common to all of them right so we we could start always with that action and um a a common state for all the instructions and um once things start differing we we branch to different states 

so e um different boxes will correspond to different state in which the controller would be and um given a given a state of the controller we would know clearly what actions are to be carried out in that particular state [noise]

so what we are trying to arrive at is some sort of state transition diagram which would describe the control 

so at the moment we have um five different change or five different sequences but we want to have a single graph which indicates um how one moves from one state to other state and what action is required in each state

so um obviously these can be merged okay the the first cycle um seems to be carrying out same activity and um we can put we can merge this
(refer slide time 00:17:43)



so after merging um we have a single state and then a pro point where we are branching okay

now another um important thing which needs to be noticed at this stage is um we are contemplating a bifurcation here we are splitting from this point onwards after first 

but um this um requires us to look at the opcode okay um um before we can split we need to see what the opcode is and which instruction or instruction group it is

um but op you can you can start looking at opcode only after you have fetched the instruction and brought into IR okay

so now [noise] um this is available therefore instruction is available to you second cycle on onwards and therefore you can start looking at opcode second cycle onwards 

um in in in a typically in a complex design um this decoding of opcode or understanding which instruction it is could also be a time consuming activity

the circuit which is going to identify particular instruction um would typically be allowed a full cycle to do it 

of course we don t need that but still the opcode decoding will have to take place in the second cycle although we may not have um second cycle fully occupied 

so the therefore um practically um this bifurcation or split must occur after a second cycle because um if you have to go from this state to one of these states we should know what the instruction is here but instruction is known only after this first step is completed 
 
so is it only after  second step we can branch off to different chains and um do special action necessary for various instructions 

so so so therefore um second step will also have to be common 

 
Now let us see what is required to make it common um one one thing is that not all instructions are trying to um read two values from register file here you need two values two values one here you are reading two values here you are reading none 

is there any harm if um  all instructions read two values they may use it or may not use it 

the answer is there is no harm okay um it it is not cause any problem in the functionality of the instruction it might you might consume some energy in doing so 

but lets keep that aside and agree to fetch both the values in all the instructions in in an attempt to come up with a common action for the second cycle 

the other thing you notice here are um that address is being calculated here which may be useful for branch instruction and um this value is kept in register called result [noise] 

so what what if we repeat this also in all the instructions once again ALU is free we are not doing anything with it

so if we occupy ALU in an activity which we may discard later on still there is no harm apart from energy consumption also um the result register is not holding  any value which will get overwritten here 

so what we will do is the lets have to make  this as the common action for all the instruction the only trouble come with jump here okay

 because jump um requires that we transfer a new value to PC but that we cannot do for all the instruction tha that would mean that the next instruction um after every instruction a jump will be carried out 

okay so therefore we need to postpone this to third cycle right we will do a common action  in the second cycle which is same as the beq action alright

for for all other instruction there may be some superfluous or um unnecessary activity which may get discarded but um there is no harm the only only harm which is occurring is that this action PC getting a new value is getting postponed (refer slide time 00:23:40)



so this is the picture with a common decoding cycle so this this cycle is actually often referred to as decoding cycle or a operand fetch cycle [noise] okay

so so now this is um this jump instruction has taken another hit here and we are eventually using three cycles for it 

but please remember that um this is an instruction whose frequency of occurrence is comparatively much much lower than others and therefore the overall impact of this loss in the performance will be um negligible 

so so we don t really mind it and now we have a very clean situation that there are two common cycles um by the end first cycle we know what instruction it is and we are ready for the next value of PC

in the second cycle um after the end of second cycle our operands if we need are ready in A and B and um a branch address is ready in Res if we need later on and then we can move on to one of these separate branches 


so two for the cycles are required for R class two for store word three for load word one for beq and one for jump and with with that all the instruction will be um over [noise]

so now this is the cycle this is a broader cycle which needs to be repeated over and over again so therefore after these last states in the chain we come back to this 

so now as far as controller is is concerned its it s a small finite state machine with the you can see these um about ten or so states um and it keeps on cycling through this 

so here um you do the fetch decode you come to know what instruction is follow one of these paths and then you are ready for the next instruction 

so as long as power is on on the processor um it will keep on going through um this overall so so this whole thing is called instruction cycle and within this you have clock cycle 

so an instruction cycle would um require four clock cycles or five clock cycles or three clock cycles as the cases that is in this particular design 

but they are of course um machine where um this range of number of clock cycles could be much much varied okay

now we need to worry about what action we perform in each of the states okay 

so these are the states but before that um there is another um small improvement possibility here which i will notice (refer slide time 00:25:01)



is that um load word and store word have one more cycle common where the address are being calculated and we can actually merge that 

and as far as load store are concerned we keep them together up to the third cycle and then bifurcate into load and store 

so that reduces number of control states by one more so total down i have how many states one two three four five six seven eight nine [noise] and ten 

now they are total of ten states um and we will proceed further by looking at now the control signals okay (refer slide time 00:25:46)



so this is back to the same data path right um we have now roughly seen how we need to exercise these okay 

what needs to be done for various instruction in different cycles so that picture has been um made very clear now

um we we get back to each of the component which require a control and try to understand um how we need to control it 

so for each of the register i have indicated a control signal so for PC PC write i am abbreviating as pw 

this is the IR write IW DR write DW and then A and B have their AW and BW signals and um Res write is ReW

so they are signals control signals which will indicate whether a particular register changes its state in a cycle or not okay

um and um in which cycle the cha state is changed we will know from this flow chart so we will always refer to now this flow chart or this state transition diagram an and then decide the values 

now we have um also one two three four five and six multiplexers okay each each one requires some controls 

so control for this multiplexer decides whether we are accessing instruction or data and accordingly the address comes from different sources

this multiplexer decides whether we are writing into rt or rd this decides what gets written into register file

we are calling it M two R memory to register or it is um ALU out to the register okay 

um the name of this signal is same Rdst we had earlier um this multiplexer is controlled by signal called A source one this is controlled by A source two um and this we had earlier two registers which were hand two multiplexers which were handling the next PC value

now it is the single multiplexer with three inputs and the control signal is labeled as p source [noiselor PC source 

the memory register file and ALU are their usual control signals [noise] same as what we had in the single cycle data path [noise] okay

now you notice that there are um many more control signals as compared to the single cycle data path we had 

so um i will not try to build the table exhaustively for all of these what what we will do is we will group the related control signals together and also identify um the meaningful operation which we call as micro [noise] operations okay

for example PC plus p PC plus four going to PC will be considered as a micro operation right its say it s a understandable action within itself and it will affect some of the control signal in the data path 








so um we we will be grouping the signals according to um our logical needs and then try to look at things group wise that will simplify the matter substantially 

so first um we talk of a group of signals called PC group which which are related to PC program counter and its address

um so i will build a table where i list micro operations which are related to um PC and the signals [noise] which are related to PC 

so the signal we can see write now one is Psrc the the last multiplexer which is being controlled by this and um the [noise] write signal for PC i have split into i have used two signals PWu and PWc um PC write unconditional and PC write conditional    

you you would notice that in some micro operations like PC gets PC plus 4 we are writing it unconditionally okay

whereas um there was an operation like this where we are writing into PC and with some condition okay

so this this state will generate a signal which i am calling as PWc conditional and a state like this or like this will generate a signal which i am going to call PWu unconditionally

so um the the signal PW which is going here would be derived out of these two i will explain that in a moment 

but but let us see um different micro operations and the signals which they imply 

so for doing PC gets PC plus four i make PC PWu as one this then i dont need to care okay and the source i am selecting is one okay

so so three things going in to this multiplexer if you recall is um output of ALU directly output of ALU through res register [noise] and um then the address which is for the jump instruction

so let let me just go back and check if um i have indicated the correct source here 

so these are three inputs to this multiplexer after the register before the register and this jump address 

so um recall that we will write output of ALU directly into PC when we are doing [noise] PC plus four so we don t bring register into picture 

um but the target address for a branch we are temporarily keeping into this we are not directly transferring to PC 

and therefore when we transfer to PC we will take from output of the register so therefore both paths have been provided 

um one more things which we should see here is the way z is going to be used in the single cycle design we used z directly um to control a multiplexer through AND gate 

so there was a signal coming from controller um that was AND ed with z and  we controlled a multiplexer okay 

so um basically we are making a choice between PC plus four and PC plus four plus offset one of the two going to the PC

um but now things are handled little differently because PC plus four is sent to PC unconditionally in the first cycle 

in the second in the third cycle the choice is either to transfer the new address or not to do anything okay.

so therefore effect of z will be brought into the way PW is being generated right 

so so in the first cycle we will have um output of ALU directly through this multiplexer going into this 

um in the second cycle um the address is calculated and kept in Res and in the third cycle we will um bring this out here without looking at z but we will look at z and decide whether to transfer it or not okay

so you will um see that z will come into picture when i define how PW is derived from  PWu and PWc  okay 

so so this is next micro operation where we conditionally transfer an address to PC um here i activate this signal and do not activate this signal the source is zero which means um this this value which is in res is being taken and the third case is that the jump address goes to the PC 

so um again this is unconditional PW u is one PW c is x and source is two 

so in this table i am going to write um zero one two three etcetera when the signal takes multiple value more than two values 

but actually you can think of um the binary code of these um go going to the components and not this um decimal value

so apart from these activities um i would also like to define what is the default value of the signals when the am i am not doing any of these what should i feed to this particular points

so the default value um should keep everything inactive so both write signals are zero and then of course once that is the case the source doesn t matter so i put an x and um here is how PW is derived from Pwu and PWc 

so if um PWu [noise] is one um PW becomes one irrespective of what is that we have here 

but when PWu is zero and PWc is one then it is Z which dictates what we get here if both are zero then again Z gets ignored right

so um basically i need one and gate and one or gate um so p that the controller will produce these two signals [noise]

and with the two gates i will derive PW which gets connected to um um the PC alright is is that clear any question about that

yeah the third this is the address um which is being um formed for jump instruction okay 

and it in one of the cycles right now we are not worrying in which cycle what is happening 

all we are worrying is that given an action like this to per perform how do we control the data path 

so to make this happen we need to make the unconditional write signal as one um we don t care [noise] what PWc is um because you see when PWu is one okay the values here don t matter the result is going to be one it is a sort of overriding um signal right 

but on the other hand um when PWc we when we want to activate PWc we have to make sure that this is zero because this will otherwise suppress that 

so um PWc is don t care and we need to select the correct value to go to PC 

so in in all these operation um notice that p pc is the destination but the sources are different so they are three different sources this is an multiplexer is selecting the sources 

this is zero sorry this corresponds to zero input of multiplexer one input of multiplexer and two input of multiplexer this is all things have been connected in the data path 

so um so that takes care of what the value is being transferred um and these two signals are taking care of um whether this transfer is taking place conditionally or unconditionally 

and that default so they will there are many control states where we are not changing the value of pc and there we need keep both these as zero and um the the value of P source doesn t matter 

so um i would give some names to these which will um make things convenient in subsequent discussion 

so the first one i will call as pc increment this is branch and this is jump and this i call [noise] no op or no operation okay

so so just some names for convenience i have assigned to this	
(refer slide time 00:41:04)



now lets look at um operations which revolve around the memory 

the relevant signals here are um memory write and read control signals this i or D instruction or data this signal decides um what is the source of address for the memory 

and um these two signals decide um where we keep the things which is coming out of the memory when we are reading from memory

so um this controls writing into IR register this controls writing into DR register and um one operation is fetching the instruction okay

so um we are not writing here we are reading it is an instruction and um the multiplexer code for that is zero the value which is being read is written in IR register so we keep this one keep that zero

next is getting data from memory so again we are reading we are not writing um it is data so that makes it one we are not storing to IR we are storing into DR so that is one and that is zero 

um next is writing into memory so now we make write as one and read as zero um once again it is um data so i or D is one and um we are not storing anything into IW IR or DR so both these are zero 

um you also might heard um it might occurred to you that um there is some kind of redundancy in these signals

it may appear that um you [noise] may be you have to derive one of these from others or one from one of these from more than none of others okay

so there there are yes many possibilities um you you might even noticed that some signals can be totally omitted for example um IW and MR seem to be identical okay

so we we can make such observation and simplify the controller design so it is indeed possible but we will we will um we will just um limit at this and not get into those details 

finally the default here is to keep a write read both signals low also the register load signals low zero and then um this doesn t really matter 

some convenient name for this this is fetch this is memory read memory write and no operation okay

so now in later discussion this um once we say fetch it would mean that this is the operation we are performing and this is the set of values we have given to control signals of this particular group (refer slide time 00:43:45)



the third group is register file group and here we are talking of writing a signal RW register file write Rdst [noise] this decides where the address comes from when you are writing whether it is RT or RD 

um this tells where the data to be written is coming from whether it is coming from memory or from ALU and um whether we are writing into A and B register 

so reading um RS into A we make RW zero Rdst don t care and two R don t care 

um these Rdst and M two R will be relevant only when RW is one okay so when when you are reading [noise] um this don t matter 

so we are um we give a write signal to it and um B doesn t require  

similarly reading into B similar except that we write into we we make BW as one 

now here it is writing into register file so we will make the write signal one we need to define Rdst so it will be one in this case M2R is zero and we are not modifying A and B 

this is writing DR into register file um so again RW is one but this has a different value Rdst is different um because the address is coming from different points and also the values being written are different 

AW BW both are zero and default is um to keep RW zero and to keep so we don t um um make any change in the state here so all write signals are kept zero 

so once again you would notice that um these two signals Rdst and M2R are complementary of each other okay 

so one could reduce the signals um name for this rs2A rt2B res2rd mem2rt and no operation okay

so these five names i would use later on (refer slide time 00:43:50)

um finally the ALU group the um signals are opc it s a three bit signal sorry this is a two bit value which we derived from the opcode okay 

um what goes to ALU finally is a three bit signal which which will look at opc and um the function bits [noise]

and the that part of circuitry will be totally unchanged then we have the multiplexer control signals A source one and A source two and um signal which controls writing into result register [noise] 

so a micro operation which we saw earlier is appearing here also because doing pc um gets pc plus four influences the pc group of signals as well as the ALU group of signals because we need to ensure that addition is done here 

um so opc we will look at opc later um look at the source and um get back to this diagram (refer slide time 00:45:04)



yeah A source one has a choice of pc and A and A source two has the choice of B four um this offset for load store and offset for branch 

so these are the four possibilities here and two possibilities there [noise]

so let me um put all these together actually yeah [noise] 

so um A source one has value zero or one zero for pc here and here and one for A okay

A source two has four possibilities zero for B that is here one for four two for load store offset and three for branch offset okay

um for this operation again we are comparing A and B so it is its like this as far as source are concerned one and zero this and this okay

um this is indicating whether we are writing into res or not so in this three steps we are writing here we are not writing into res here we are not writing into res and um this opc and coding is same as what we had done earlier 

um for [noise] those instruction where we have to simply perform addition without doing anything else 

we um make it zero so we had done this for load store instruction okay and um now even for um um these address calculations we will use zero 

because wha our logic would be that whenever there is a zero here um that ALU controller would ensure that ALU perform the addition and one would mean that it perform subtraction unconditionally

and two it would mean that we look at the function bits okay so same encoding [noise] is used and accordingly we fill this up 

um the the last case is the default where these are don t care doesn t matter what ALU does as long as the result is not written anywhere 

so we ensure that ReW is zero and um ALU may do something which is don t care okay 

so now with this done we have seen the relationship between micro operations um which we picked out of that um flow chart and how they how they assert various control signals what values they um imply for various control signals

um the name of these are pc increment arithmetic memory address um pc address branch and no operation (refer slide time 00:48:53)



so now i can tabulate this um before that let me redraw the diagram with um these new symbols put in the boxes okay 

so instead of though [sinamic] statements i have um replaced those with the new micro operation symbol i have um described in previous few slides right

so for example in the first cycle i am doing fetch and pc increment in the second cycle i am doing rs2A and rt2B and Paddr 

all these three are done concurrently within the second cycle and so on 

so all these signals have been um all these micro operation have been put into appropriate um state

also these states could be numbered cs zero to cs nine right

um now i i need basically um two tables um one table will define for a given control state what micro operations are you perform which will also directly imply what are the signal values for various control signals okay

so that that is one table next table um would define that given a control state what is a next control state and this transition may be um conditional it will depend upon the way i am bifurcating so it will depend upon the opcode 

so let us see both these tables one by one (refer slide time 00:52:24)



so first relationship between control states and signal values 

so i am not listing signal values here i am only listing the micro operation in the particular group 

so these are the four groups i identified PC group Memory group [noise] RF group and  ALU group

and in cs zero the operation i need to perform is PC increment this actually shows the pair also as i mentioned that it requires to control signals at both the ends and in memory group the operation is fetch 

in cs one um i am fetching these operands and calculating um branch address 

in cs two um this is the first um sep um first distinct state for R class instruction 

so i perform the arithmetic operation here and um so the R class instructions go through cs zero cs one cs two and cs three 

in cs three the value gets written um and all those which i want to keep inactive you will find there is a no offsetting there 

so here the result is being written into register file then c four is the common state for load store here memory address gets calculated

then c five completes the store operation we have a memory right here c six performs memory read operation and c seven performs transfer of data from memory to register file okay

c eight completes the branch instruction so again a branch operation branch micro operation shows up in pc group as well as in ALU group

because it requires a comparison here and it needs to change the state of pc so both these get infirmsed

and c nine completes the jump instruction so um it it is influencing this pc group um what i can do but i will not do that here is that each of the micro operation symbol um replace it by a bit vector okay that bit vector defines the relevant control signals 

once i have that i and i also encode these nine states in binary so you can use four bits to encode this state

i form a truth table where um this is the input the code of the state is the input and um the the bit vector which i write here are the output

so these are control signals which go from controller to um the data path 
(refer slide time 00:53:30)



the the second part of the control design is how control state transitions take place 

so again for each of the states let me fill it up for each of the state i am defining the next state but the next state could be [noise] different depending upon um what is the opcode value 

so from cs zero i am unconditionally going to cs one in all cases

from cs one one goes to cs two or cs four for load store it is common again for branch it is cs eight for jump it is cs nine 

from cs two which is for R class instruction um go to cs three and these conditions wont occur 

once you have gone to cs two you know that it is R class instruction and others are not relevant 

so so you can see that R class instruction go through cs zero one two and three and back to cs zero 

for store instruction we start with cs zero go through cs one um cs four cs five and then cs zero 

for load it is cs zero cs one cs four then cs six cs seven and then cs zero

for branch it is zero one and eight and then zero for jump  it is zero one nine and then zero

so um now what kinds of truth tables i have here here um the opcode an four bit of um the control state

these form one input of one um combinational block and um the the next state value or the four bits which encode the next state are the output 

so um um with if you implement  these two tables okay the table shown here
and the table in the previous slide and the this one um um you you can use two PLAs to implement this 

so these two PLAs plus one register holding the control state four bit register hold in the control state will form the controller okay

um so i i will draw that picture next time and look at some alternative 
(refer slide time 00:55:42)



so today we will stop at this um let me just summarize that we um saw how instruction get divided into sequences of micro operations 

how we group the control operations control signals um and then defined relationship between micro operation and control signals 

um then we associated control states with the micro operations and we also identified control state transitions 

thank you    

   





TRANSCRIPTION: BRINDHA

COMPUTER ARCHITECTURE

Prof. Anshul Kumar
Department of Computer Science and Engineering
IIT Delhi

LECTURE #22
Processor Design   Microprogrammed Control


In the previous lecture i started design of controller or the multi multi cycle datapath um for the processor 

we will complete that design and i will also take another style of design  called Microprogrammed design (refer slide time 00:01:11)





in the overall um scheme um we are here um we are going to concentrate on Microprogrammed control and also finish this control part design 
(refer slide time 00:01:25)



um we we will also look at um two different alternatives of um design PLA and ROM 

so we will um look at the implementation of controller which has two parts one part produces the control signals other part decides the next control state

and each of these could be either implemented by a PLA or a ROM as we will discuss later 

and then finally we will um talk about Microprogrammed control design (refer slide time 00:01:54)



so this was the um data path we  um arrived at for um multi cycle approach um the picture shows all the control signals um which are useful for um controlling various components (refer slide time 00:02:12)



and this is the overall flow chart indicating how control control state changes um from cycle to cycle and what action is taken in each control state
(refer slide time 00:02:22)



um we have identified um groups of control signals um which need to be controlled together to do some basic operation which we call as micro operation 

so all micro operation were identified and the pattern of control signals required to make them effective will have also been identified um some symbolic names were assigned to these um micro operations
(refer slide time 00:02:47)












and um then in brief we replace all the assignments and all the transfers by name of these micro operations and also we label all the states

so now this is this is the most crucial part of the design um after this the implementation starts (refer slide time 00:03:07)



so we tried to capture um all this by two tables one table um describes the control signals for each control state okay

so for for different groups of control signals we are indicating what micro operation they are supposed to perform and each of these corresponds to some pattern of control signals 

so if you substitute that (refer slide time 00:03:32)



if you replace each micro operation by the pattern of control signals from um one of these tables okay what we get is this okay each micro operation has been replaced by a pattern of ones and zeros and also um there are X es indicating don t care wherever appropriate

so now this basically is like a truth table description and all that is required is that we encode these control states also in binary form which you see here that each state can be encoded in four bits since there are ten states 

um this forms the truth table where um the part showing black is the input and part showing green is the output 

so basically it s a four input and um this i think numbers upto twenty so twenty output and four input is um what this table describes
(refer slide time 00:04:33)



so this can be implemented by a PLA as we have discussed earlier for single cycle design um this PLA will have four input coming from the control state and um four groups of control signals um group of four group of five 

another five and six so altogether twenty signal which go to the data path [noise]  (refer slide time 00:04:59)



the other table is what describes the next state um given the present state and which instruction or instruction group we are talking of

so we we have the same ten states and um instruction has been grouped as usual R class sw lw beq and j and for each of each row and column combination we are describing what is the next state

so this is a these two tables together are basically capturing this flowchart or state transition diagram [noise]

this table can also be um um replaced by a binary equivalent where um we substitute code for each of the control state and also we specify the opcode value for various instructions 

so after having um replaced the um control state codes in this table we get this (refer slide time 00:05:59)



um on this column also we replace okay all the states are replaced by their codes and opcodes also put there

so now it um describe the truth table with the a group of four inputs here and group of six inputs there 

so this this table is defining four bits of four outputs and there are four plus six ten inputs (refer slide time 00:06:24)



so we can um have it um described by implement by a PLA but before that i i am showing another way of looking at it

so here [noise] this this is a more compact representation because here you notice lot of sparsity alright (refer slide time 00:06:41)


so we can instead of having a two D table i am showing it as the um one dimensional um table where all the input combinations are listed vertically 

so um the current state and instruction combination um but we don t have two raise power ten entries here although there are ten bits of input because um the there are lots of X es or don t cares 

so all the relevant combinations are captured so for cs zero irrespective of what when the other input is the next state is cs one and so on for all these the next is a a fixed state

so so same um table um is rewritten in different form okay and once again you can um replace the state numbers state labels by their code to get this right

so now now you can look at it in a more conventional form the truth table where you have um first two column represent the input would together ten bit input and four bit output (refer slide time 00:07:44)



so here is the PLA which will implement this um four bits of control state and six bits of opcode to produce um nex next state which has four bits 

now how do you um what is the overall picture of the controller we have these two PLA s implementing those two specific tables um what all put together we also need um register which holds the state value

and um at every clock this value will change as per [noise] this tables [noise]
(refer slide time 00:08:14)



so here is the state register okay its a four bit register which contains the current control state and this drives both the PLA s um this PLA is generating the control signals going to the datapath and this PLA is also looking at the opcode and deciding what is the next state okay

so one one could have thought of this as the um single combination circuit with total of ten inputs and twenty four outputs but that would be that is possible and um it will be correct but that will be much more complex than these two smaller PLA s put together (refer slide time 00:08:58)



so at at this point i want to bring in another alternative to PLA again another general purpose component um which can be derived directly from a truth table 

so suppose you have a truth table um fully expanded truthtable with n inputs corresponding to two raise power n um rows and defines m outputs okay 

um this can be um implemented essentially by putting this pattern of outputs in a memory 

so in particular we are talking about read only memory the kind of memory we use for instructions where you are not modifying the contents you give an address as input and out comes the data 

so these n inputs which you um want to apply to the combinational circuit you are designing you apply as address to the rom and you read out contents of the address word [noise]

so um it effectively works out as an n input and m output um combinational circuit

that definition of the function which this implements is directly given by um [noise] the the output column to the truth table okay

so so each row of the output part um corresponds to one word in this memory okay

now how um how do this alternative compare with the PLA alright
(refer slide time 00:10:24)



the the difference is in terms of size whereas um the memory like this will have two raise to power n words each of m bits okay 

so total number of bits is m multiplied by two raise to power n 
(refer slide time 00:10:39)




on the other hand um a PLA which is implementing an n input m output circuit um will have an and plane and or plane 

um so so there will be rows running for all the inputs okay in rows corresponding to true logic and false logic 

um there will be vertical lines which will implement and gates and there will be rows here um one corresponding to each output which will form the or function of some of the um product terms 

so what is the size what governs the size of this okay size is governed by um these three factors one is n other is n and the third is the number of column number of vertical lines you have to run

each vertical line here corresponds to 	um a product term or you can alternatively say a row of the truth table okay

so it corresponds to suppose the k term in the truth table but um remember that the truth table here it could be not in a fully expanded form it could be in compact form with the um lots of don t cares

so if there are k terms then the total size would be um which you can see as area of roughly area of this rectangle will is promotional to k that is this dimension multiplied by n plus n 

because this n plane accommodates um rows um corresponding to n inputs and the R plane corresponds to m rows corresponding to the output okay

so the the height of this would be proportional to n plus m and the width in proportional to k 

in general k is likely to be much much smaller than two raise to power n and that that s what makes a PLA much more um economic and um compact as compared to um rom001234

so if if [noise] the if a [noise] truth table have lot of sparkcity there are lot of don t cares and you can um um compact the whole thing in the form of PLA 

otherwise um rom is the reasonable alternative (refer slide time 00:12:48)



so um to illustrate this point of um compact [ ] suppose um in your truth table there was a term like this zero x one one zero x and for this the output is one zero one zero

so in PLA you can take it as it is it will correspond it will contribute one to that k okay

whereas in for a rom um you need to define output for each combination exhaustively right

so such a term will actually show of us four terms where you take all possible value for these x es 

you substitute zero zero zero one one zero one one you get four possibilities and for each of these you need to ensure that the same output is there because um with any input the output must be um defined here

all all we are saying here is that um four terms are grouped here okay so irrespective the value second input and the last input 

if this is the pattern zero one one [noise] zero here then this should be the option so um in in a fully expanded truth table this will correspond to four term and um you know this is just one simple illustration 

um on the whole you could see an example of a  (refer slide time 00:14:07)



now lets say if you want to implement this by a rom okay um this would require four plus six which is ten inputs that means two raised power ten or thousand twenty four um words in the memory

each word will be four bits and total number of words will be thousand twenty four um but in PLA you will have one two three four five six seven eight nine ten eleven twelve thirteen fourteen terms okay so k will be fourteen here right

each of these um will correspond to product of for example what you are saying here is is that if this is zero zero zero one one zero irrespective of any value of this and zero one one here that should be the output okay 

and and so so a term like this expands to two term in case of a rom things like this will expand to two raised power six or sixty four term and so on 

so that s why um PLA [ ] will be much more compact here 

okay now what is this other style of um control design which i mention Microprogrammed control (refer slide time 00:15:21)



um here we try to view the the controller has um something which is a small program okay something like a small computer and to control the data path 

if you if you look at this for example (refer slide time 00:15:38)



you you could think of this as a program flow chart [noise] alright

if you if you take that view um why not think of controller as a small computer which executes a simple programs that program has a basic operations which are micro operations and and it all it does is generates control signals for the data path okay 

this is a program which does nothing else um it does not deal with manipulating data in the main memory

it does not deal with um register files one all it does is that it goes to steps each step involves generating some control signals for the data path 

so with that view [noise] um we think of a memory here containing micro program and um all we are doing is we are reading out words of this memory okay

um so you you must think that now i am again talking of memory although i have mentioned some disadvantage of a memory based design but um there are differences you will notice 

so imagine a memory which contains a micro program um what is micro program its nothing but each word contains the bit pattern which needs to be be applied as control signals to the data path okay

so tho those signals are not generated by circuit um they are simply read out from this memory alright

you you step through different word and um each word um generates the [ ] control signals for the data path 

so um this will have um the what we have twenty bits plus few more bits that i am going to describe in a moment 

so these twenty bits will simply um go as inputs to the data path and um control it as required 

now the the question which now remains is that how do we sequence through different words of this um micro program okay

so some time we will need to go sequentially word after another sometime we mean mainly to jump from one point to the other point

so here is the little arrangement to make sure that the right address is presented to this memory so that the correct word is read out in every clock cycle 

and um how we generate this address is by another box  which we are calling as micro sequencer 

so micro sequencer is ensuring that right address is put in this register which we are calling as micro pc or Microprogrammed counter so it is the counter part of PC as we saw in the main program okay

in main program pc is stepping through memory locations which contain the instructions 

here it is micro pc um which is just a four bit register it steps through a different words of this micro programmed memory to ensure that the right signals are generated at the right time

um to ensure that this works correctly each micro instruction will say something about where the next micro instruction is okay

so each micro instruction um we say that um either we follow sequentially in the address or we branch to something else

so so that information somehow will be communicated by a few bits of the instruction and this micro sequencer will look at these bits it will look at the current value of the micro programmed counter and um if necessary look at some signals coming from the data path so in this case it is the opcode bits 

now you might be wondering that um we have [noise] um we are now talking of a box which has um um these four inputs plus this six inputs ten and a few more here 

so isn t this going to be very complex um the answer is no because um you would notice that quite often you would simply go to the next instruction or the next micro instruction or the next word 

so it is only in those cases we are not doing so um the logic has to do something else but otherwise um a simple incrementer adder will um take care of um bulk of the task

and we need um some more logic to take care of those condition where you were branching off

so in in our [noise] flowchart what all was happening let us see 

so we we are going sequentially for example here um here here um the po point we are branching is here this is one point we are branching another point we are branching and um these are few points were we are restarting the whole thing

so we need to take care of this [noise] these are the four possibilities
(refer slide time 00:20:59)



that you follow in simple sequence um in one of the instructions in one of the steps we are branching to cs two or cs four or cs eight or cs nine that s one branch point um we call it dispatch one 

so um in micro program terminology a a multi way branch like this is called dispatch um there is another point where we are branching to either cs five or cs six 

so lets call that dispatch two and there are several state where we are going to cs zero so lets call that reset 

so now that um um few bits in the instruction which i was talking of here it needs to specify one of these possibilities 

so the possibilities are sequence dispatch one dispatch two and reset these are the four four possibilities we have identified um in in a this is a specific case okay

in general when you are talking about more complex um design um these possibilities could be larger but in our case a two bit field here will be sufficient to tell what will what we want to do um as far as determining the next address is concerned (refer slide time 00:22:14)



so the micro sequencer needs to work with these four possibilities and it will selected by the two bits coming from the micro program which we call as sequence control

so this will select either current mu pc Microprogrammed counter plus one or zero or um an address which is which we have called as dispatch one or dispatch two

so so these are small boxes which are looking at the opcode and generating the correct address okay

so once again you can think of the small tables um depending upon the opcode value you want to pick up one of those four possible addresses

here depending upon opcode you want to pick up one of the two addresses so these could be small roms small PLAs so we leave that here

so that the main point is that there are a very small number of ways in which you need to determine the next address and a few bits in the micro program will control this multiplexer (refer slide time 00:23:22)



so now how does a micro program look like um we have a micro program containing ten instructions

so this is um a very low level program lower than the a single language program which we studied initially in the course [noise]

and a very simple primitive operations are being done each line um here i have written so that each line represents one micro instruction or one word in the micro program memory 

um incidentally micro program memory is also called control store [noise] so the first instruction i have um labeled where you fetch do PC increment and and the um sequence control is that you do in normal sequencing

in the second we are doing this this this and doing dispatch one

um here we do dispatch two and here again example of sequence example of sequence and the rest are reset 

so so there are um labels i have put when you do dispatch one you branching to one of these labels beginning with one one a one b one c one d is choice of these in dispatch two there is a choice of two a or two b okay

so so this is a micro program written in a symbolic form and um once we understand what each of these symbol is a a micro assembler can translate this into the contents which will go into the control store of the micro program memory right

um there are variations in the way you um structure your micro program the two style (refer slide time 00:25:02)



one is called horizontal micro programming and other vertical what the approach we have followed is what is actually horizontal microprogramming where you allow many operations to be done concurrently whatever the data path can support you you have a provision of doing many micro operations together within same instructions 

there is a high degree of concurrency um vertical would be low degree that way you try to do only one thing at a time 

so fetching an instruction in pc incrementing there would be done in two different instructions alright 

um the idea there is to conserve the space here um you as you have seen that altogether the space we require is just ten words and each word having twenty two bits

so its not too much of space and you wont worry

but um in past many processor have been designed which have which have been very complex and micro program run two thousands of words okay 

apart from um the concurrency of micro operations there is also a question of how you encode each um micro operations

so the way we have um encoded was that we should directly get the control signals out of the pattern of bits um but since the number of patterns which i um really utilize is much less than the number of much less than um the log of the number of bits um we we sorry um the number of pattern in general could be two raised the power of number of bits 

but the the number of useful pattern is much less and we can use a more compact encoding 

so a vertical approach would try to do um a more compact encoding and um and [] is to have low memory requirement whereas in horizontal micro programming the idea is that you don t lose performance 
(refer slide time 00:26:52)



but on the whole um the micro programmed approach versus um non micro program which in contrast is called hardwired approach or um finite state machine based approach 

um what are the pros and cons the the specific advantages of um micro programmed approach is that um it is often easy to write um it like a program

the the definition of control can be written in a flow chart like form and then you can easily capture in the form of a program

um and um one good work with design of the architecture and design of micro program um independently

um in terms of performance however there are issues um in in past it was thought that you could um processor could be once it will have data path 

but the control could be moved out into a separate memory chip that could be drawn and you could change that okay 

um so that suppose you want to um make a change in a processor design want to add few instruction you could simply modify just the contents of this memory and rest could remain the same 

so so in fact many many several times the families of processor were designed like this that um you would have different micro program implementation all that the data path may be same 

um also there were attempts like emulation of one architecture by the other um the that means you you have a processor which was designed may be um with the with the fixed architecture in mind 

but um you you can have a micro programmed um um hardware with which can implement one or more architecture so that is the process is called emulation

so you you try to emulate um emulation is something like simulation in microprogramming okay

you try to simulate the effect of another set of instructions and a micro program would have access to all the internal registers temporary registers 

example in our design we have register A and D okay [noise]

um a person working at a signal level will not know there are two registers A and D but one who is working at Microprogrammed level could make access to those registers 

if it is required for [] or something but penalty of all that is that suppose makes it slower because um memory is um tend to be slower than um much much slower than the PLA for example which is the other alternative [noise]

and so so now this approach is no longer um popular um partly because there is a lot of performance penalty and partly because um you have now tools where tdf final state machine design are not to be done by hand okay

you could leave for example at a point like this and the tools will take care of the rest okay 

so coming up to this point is not difficult and um this was considered as the advantage of micro program earlier that is from here one could by hand or by an assembler fill up those um bits in the memory um but tools can design very um today tools can design very efficient um um finite state machine or hardwired controllers starting from this kind of description [noise]

so now looking at um all these possibilities we have [noise] talked off um we we have multiple options at various levels various stages in the process of design at the initial representation level you can draw a finite state machine diagram or you can write as a program micro program 

at sequencing control level you could have either express it next state function um coming out of um a PLA or memory or you could have micro program counter plus dispatch roms okay

and um this arrows shows that you can start with this go to this al alternative or the next alternative at the next level

similarly we can start with this and go either way at the next level you have logic where you can write is logic equations that you can write as truth tables and finally at implementation level you can have PLA or read only memory

so you you have many um you have multiple options at each level and um technically all combinations you can start here go to this come to this finally implement like that all those are possible 

okay to summarize um we completed the design of the controller which we started last time and we saw that we re require two PLA s one takes care of generating the control signals for the data path and taking the next the um next state for the controller 

we saw rom as the alternative to the PLA but there is a problem of um large size and this difference could be very very significant in many cases

and thirdly we looked at micro programmed approach to control where the three main um things are there is a microprogram memory which contains um so called micro instructions

um there is a microprogram counter which um drive this memory 

and there is a microsequencer which determines contents of the microprogram counter 

i will stop with this 

thank you


 






TRANSCRIPTION: BRINDHA
                                  
COMPUTER ARCHITECTURE

Prof. Anshul Kumar
Department of Computer Science and Engineering
IIT Delhi

LECTURE# 25
Pipelined Processor Design: Datapath and Control

In the last lecture we discussed a new type of design called pipelined design where the objective is to get a low cpi as um as a high frequency of clock

the idea here is that you attempt to initiate one instruction in every cycle and um at any given time there are number of instructions which are there in the datapath at different stages of execution

ideally each stage should have one execution to keep the pipeline full and get maximum benefit but we have seen that there are um situations were you cannot keep the pipeline full and um delay is get introduced which um causes the loss in the overall um performance

we um briefly hinted on some ways of handling those situations these were called hazards

we looked at um three different types of hazards structural hazards data hazards and control hazards 

um by design we have tried to eliminate structural hazards but data hazards and control hazards are somewhat inherent to the whole concept and cannot [noise] be always rules out entirely 

so so we have to do something to um reduce their effect as far as possible um what we will do today is um look at the skeleton datapath we had discussed last time 

complete this and then see how such a pipeline can be controlled 

so first we will see um we will ignore the hazards and see in normal condition in ideal condition how pipeline could be controlled and instructions could be initiated um one instruction per clock cycle
(refer slide time 00:03:11)



so um we will first as i mentioned complete the datapath design	[noise] okay introduce all the components multiplexers and um shifters um bit rooters and so on 

then introduce the controller and then finally see that how this simple pipeline design behaves in context of um hazards 

and possibly in subsequent lectures we will um augment our design to handle these hazards (refer slide time 00:03:27)



so you recall that we started with a single cycle datapath which was formed which was used as the basis for designing um pipelined datapath

so just to simplify things we had um omitted um instruction j not because it is too complicated but just to reduce the size of the datapath so that it becomes easier for discussions [noise] (refer slide time 00:03:53)



and another simplification we did was that we omitted um some of the components like multiplexers and sign extenders and shifters so that um again um the number of components you see in the diagram is smaller and you can easily analyze and discuss

so um the approach to the datapath design was basically to introduce um the registers which would separate stages from one and another

so for example [noise] um first stage is considered um instruction fetch stage which involves accessing the instruction memory and at the same time updating the PC value um to PC plus four 

so this is one stage and we introduced a register here next stage is the instruction is getting decoded control signals are getting generated and operands are being fetched from register file so that s the second stage

and after that again there is an there is a register third stage is [noise] where ALU comes into um action

um it may perform ei either address calculation or for R class instruction it could do add subtract and or facility and so on 

and at the same time um at this calculation for branch instruction may be required to be done and there is an another adder for that 

um and the final stage not the final the fourth stage is memory access [noise] for read or write of data and the final stage is writing results back into [noise] register file 
 
what we said was that all instructions will see these are the five stages if some instruction needs to skip some stages it could skip some stage by simply wasting some um cycle

 so that the whole pipeline operates in a uniform manner and um this is the design (refer slide time 00:05:46)



this is the design we have these registers um are basically cutting all the forward going paths okay any any line which is going forward um is cut by this 

which means that um the um size of this register for example which we are seeing as the first register will be um thirty two bits of instruction plus thirty two bits of this address so it is a sixty four bit register

the register which you see here is the its cutting one two three and four paths so its probably a one twenty eight bit register 

this is cutting three paths so its um um ninety six bits register this is sixty four bit register and so on

so these are these are large registers and um what we are saying is that all forward going paths [noise] have to pass through 

so that means the information is available after the register after one cycle of delay 

so in in this um there are couple of um backward going paths one is the path which is used to write back to register file 

so that is not passing through register file um through this register um this branch address um is going back 

PC plus four is going back okay they they have to go back to PC 

now we um deliberately kept this path going from pc plus four to pc um turning back within the first stage 

and this is very crucial to um be able to get one instruction in every cycle okay

um you recall that an intermediate design was that this multiplexing which we are expecting here was actually happening here which would mean that you you cannot initiate next instruction unless first instruction has gone through some three stages or so okay

bu but that s not in the spirit of what we are trying to do therefore as a special case we are turning this back right here and um this this loop can turn out new PC values every cycle and therefore a a new instruction gets pumped into this pipeline in every cycle okay

so now what is missing in this datapath are the multiplexer which we had removed for convenience of discussion and we can put this back 

so um here we have a multiplexer um which is computing the next PC value out of these two choices

now you recall that there there is a peculiar thing happening here is that we are um taking this PC plus four value generate for instruction which is here

but this branch address is coming from an instruction which is here okay 

so what is the consequence of that we will have to analyze but lets leave that there 

we have um a multiplexer here because the right address comes from two different sources um a multiplexer here because ALU may be adding data coming from register file or from the instruction the offset a multiplexer here which decides what goes to the memory output of what goes to this um register file output of memory or output of ALU 

so notice here that if um there is a R class instruction where memory stage is not being used then data is simply passing through and experiencing a delay of one cycle here 

okay so essentially we are going through that cycle but not doing anything idling through one cycle

so that um pipeline operates in a very homogenous and uniform fashion okay

now there there is a slight problem with this datapath um i one day if you can notice [noise] yeah [noise] exactly that s a problem that s is um normally which we have introduced  look at the right back operation

right back is happening as um fifth stage activity okay so so data which is going back to this is um coming here after having been delayed to the fifth stage

but um when the data comes here the address which is coming is from an instruction which is still here it s a different instruction 

so what we need to do is we also need to take this address through the same amount of delay um and basically carry the data and address together um let them experience same delay and then when you write they are available to the register file at the same time okay

so the change we require is as follows (refer slide time 00:10:31)




i i have removed um the logic here which is feeding the address to the register file and we introduce um another path which is actually going through these three register stages and then coming back here okay

so this multiplexer is same thing i will just um position it here the output of this multiplexer is going through these three stages so that um the data and um address all move together and are presented to the register file in the same cycle um and therefore they are consistent with each other otherwise there is a mismatch 

you are taking data of one instruction and address of another instruction and trying to use them together 

um so now there may be um lot of options here for example should i keep the multiplexer here or should i take both these addresses all the way and then multiplex just before feeding in

so logically both both are equivalent okay um the the consequences of multiplexing later would be that i would here i i am consuming five bits in these registers okay

if i don t multiplex i have to pass on two five bit pieces through these registers and therefore by register length will increase okay

so ID EX to multiplex them early and um then pass through this um um using the same argument if you look at the way um sign extension is being done 

um just try to imagine the consequence of positioning the sign extension unit after this register

see see input of sign extension is sixteen lines output is at two lines now by positioning at here i am actually allowing thirty two lines to pass through this register

so i am actually wasting on the other hand if i position this afterwards i i have positioned it just for more convenience of the diagram 

but the the right solution would be to keep it afterwards so that i am only consuming sixteen bits here in the registers okay

so so this is not the best position best position is here is is that idea clear

so so these are only positioning these are matter of um saving a few bits here and there um but more more crucial thing is that the address is made to go hand in hand with the data and reach the register file at the same time 

so that s the idea that s the correction which was necessary um to do WB operation correctly [noise] okay now lets move to the control 

so um we will introduce um control controller and try to generate control signals for all the components which require for example register files multiplexers and so on um which one um thi thi this multiplexer 

this can be actually this yeah you are right this can be brought back before the register okay 

right now we are passing two things okay so so we are consuming [noise] sixty four bits here

if we bring this multiplexer before um then then we will share something here but lets um while we are discussing this point lets also understand other implications of making such changes 

um lets also see what is the influence of such repositioning of components of multiplexer in particular um before register after register 

what is the influence of this on the delay or on the clock period [noise] 

now the clock period is the dictated by the longest path within any of the stages	 [noise]

so for example in the first stage the paths are from PC through this IM through this register or through PC through this adder through this multiplexer back to PC 

basically we in each stage we need to consider all path going from some register to other register or some storage element to other storage element [noise] 

now um if let us say this multiplexer [noise] um is positioned where it is then it is going to form the part of the path which starts from this register goes to this multiplexer goes to register file and ends in um this register okay

so delay of this multiplexer which we have um we have been um neglecting treating them as zero but in real life they will be some small delay that delay will get combined with this register file um when you calculate the maximum path delay or such delays are called critical path delays

on the other hand if you keep it before register file then it gets lumped with um data memory delay okay

so you you need to see which one of the two which one of the two choices achieves the better balance of the delay okay 

suppose [noise] um the delay of data memory was much larger than delay of RF that s how the case would be typically 

then keeping this multiplexer before would add to the DM delay and make things worse

on the other hand there is some slack some room here on the RF side to accommodate some more delay and therefore keeping this here may help from that point of view

so here here is the trade off [noise] um there is a influence on the clock period there is also a influence on the hardware cost in terms of register length register size 

so one is there is no um universal answer here one has to see for a specific design which were the things changed and what is what is that you are trying to um optimize [noise] okay (refer slide time 00:16:39)



so coming back to this we want to add control [noise] now question is which style of control we are going to follow 

are we going to follow the single cycle type of control design or multi-cycle type of control design 

in single cycle the key of the vision was that	um the controller was purely a combinational circuit it looks at the instruction generates the sig generates the signals okay

whereas in um multi-cycle design there was a finite state machine now which steps through um states okay

and in each state control may be generated differently because different action is required in different states 

therefore we need to look at the current state also look at the opcode and and then decide the control signals [noise]

if fact we had um organized things such that the the control signals gets determined directly from the state and we don t even have to look at the opcode 

because in some sense the influence of opcode would have been taken care of while making the control transition 

so um in fact there was a questions which some students asked later on that um can we not reduce the number of control states there an and allow the control signals to generate by looking at um opcode as well as control state okay

we we are trying to um branch off after the first two states you know we have fetch state then we have decode state and then we are branching off depending upon the instruction

suppose we don t branch off okay we will go through a sequence of um three states four states or five states um just in a chain [noise] um and repeat that 

um because we have the information about which what is the opcode looking at the state and the opcode we can always determine what control signals are they 

so if you um recall your theory of finite state machines this will be a mealy machine type of approach 

what we have followed is a moore machine type of approach okay

now coming back to the question of um what style of control we can follow what will suit the pipeline design 

um will will will the controller be combinational circuit probably or will it have states would you need to do something different [noise] cycles and therefore remember the states [noise]

so the um answer is that um it cant be a purely combinational circuit okay because an instruction is going through multiple cycles and an instruction needs to do different things in different cycles and control need to be generated differently [noise]

but remember now that things are even more complicated here that um its not just one instruction which is in flight there are um up to say five instructions which can be in different stages 

and um in in a given stage we have a particular instruction and we need to generate control for that

so it might appear that you need to look at um either you maintain several control states one for each instruction or or look at the opcodes and and decide control signals differently for different instructions 

um but fortunately what turns out is that again we can start with um the control design of the single cycle approach and and by simple change is there we can actually derive the controller for this particular case 

so so lets um reconstruct the controller for the single cycle
(refer slide time 00:20:12)



lets um superimpose over it and then then see what changes are required 

so um we have control signal goint to um this multiplexer next there is a control signal which controls the write operation register file 

this is a control signal which um goes to the ALU and um [noise] remember that this has to go through another small controller um which will look at some bits [noise] of the instruction okay

so here we are going to look at the function bits the six lsb s of the instruction um that will be another input to this let me connect that as well okay

so so this looks at um some control signal which actually was two bits and then six bits coming from here generating signal for ALU next is control for the multiplexer which is feeding ALU 

then um write control for data memory read control for data memory the um the and gate which generates um signal for um taking care of branch instruction okay (refer slide time 00:21:24)



so there there is a zero signal result of comparison coming out of the ALU that is gated with the signal coming from the controller and that together um controls this multiplexer 

so so we we still havnt really seen that what are the influence of um mixing of datapath from two different instruction okay

so this is the address coming from this instruction which is here that is the address coming from instruction which is in that stage

but anyway the signal which is getting generated is found by and ing um a signal coming from controller and Z output of ALU [noise]

finally we need one more for the multiplexer at the output end 

now um if we leave it this way what is going to happen is that this controller will look at the opcode which is sitting here okay 

the instruction which is in decode stage is going to drive the controller and the control signal seem to be going to all stages okay

um the consequence of this is that um um we we are ignoring the identity of other instructions which are sitting at different places 

um what what could be done is that um if you recall how we solved the problem of synchronizing address and data while writing to register file okay

all that we did was that address was also delayed the way data was getting delayed 

so what what can be done for control is exactly the same thing that is you generate the control signals when the instruction is in second stage or the decode stage 

but um do not apply all the control signals to all the stages immediately okay you you apply them as the time comes

so signals which are relevant for for the cur current stage you apply them [noise] immediately signals which are relevant for the next stage you delay them by one cycle 

signals which are applicable two stages later you delay them by two cycle and so on

so um essentially what needs to be done is that we extend these registers to accommodate the control signals also and all the control signals which are um they are um except for those which are going in within this stage those which are going forward um are passed through this registers same way as we do for data signals and um addresses okay (refer slide time 00:24:05)



so all that i have done is i have extended these walls to separate um control um along with the data [noise] right 

so so basically the signals which are um effective within this cycle are going directly 

the signal which are required here in the in the ALU stage are going through one register 

so basically you put so these two signals okay going to um ALU control i am going to this multiplexer [noise]

signals which are required in the memory stage are going through um two units of delay um they pass through this register they pass through this register and then they are applicable here

signals which are used for write back um are going through three stages and getting delayed okay

and um of course also signal which is going to this um multiplexing is also um moved um later on passed through these two register

so that um this address which is going and the control signal which were which is going there they all reach the multiplexer together okay

so so basically this is a simple arrangement um where we can start from um controller of the single cycle datapath and then simply by inserting register extend it to multi cycle datapath 

so now um do you see this as the um controller which is combinational circuit or it is a sequential circuit

it it s a sequential circuit um because these portions of registers through which control signals are passing okay um these are in some sense carrying the control state okay

so so let us see um you have one plus two three four five six seven bits here four bits here eleven and one bit here twelve 

so so effectively we have a twelve bit register which is remembering in some sense um um that what is what are the control signals required for instruction which is going to [noise] um wha what are th control signals required in the next state what are the control signals required in the next to next stage and so on alright

so this this is the storage element here and combinational circuit is identical to what we had for single cycle design

um well once again the there is some correction required because some signal have not correctly timed and um again you need to focus your tension on that WB activity okay

in in the signal required for WB um are um one is that unit do multiplexing here okay so that signal had gone through three walls correctly

but um signal which enables write of the register file is being sent directly okay so so that is not correct

this signal also needs to root through all these so go through these and then get applied here right

now this multiplexing is also actually irrelevant for write back but um since you are multiplexing here within this stage and then the output of multiplexing is going through revolve 

so this is fine this can be um taken out from controller directly but this signal needs to be um modified (refer slide time 00:27:41)



so we will remove that and um root this and just for convenience tapping it from here taking through these three registers and bringing back and applying into register file [noise]

so so now all everything is correctly timed all the um datas addresses and control signals [noise] and this is um um complete design datapath and control of course we have taken out jump instruction 

but for remaining eight instructions this is a complete design yeah [noise] yeah [noise] yeah [noise]

um um which you are talking off um this multiplexer um [noise] um o okay let us see what will happen if we don t multiplex it here okay 

suppose we were to position this multiplexer also here um um [noise] 

o okay okay what you are saying is yeah that s an interesting point but um there is a problem 

so what is being suggested is that um instead of delaying the data you let the data be available here directly but um um bu but you giving write signal at appropriate time 

is that what you are saying um but see what will happen is that this address which you are computing and um giving here will not stay there okay this would change 

this register file is not actually keeping that um address and internally holding it right

so if you just leave it like that um as the instruction change um in subsequent cycles um this output will also change okay

so while the instruction is here we we need to pull out whatever you want out of this instruction code okay an and whatever we need later needs to be carried through the registers 

so therefore you you need to [noise] do something here um you if you want to move this multiplexer let us say here um one consequence was the there will be more bits you will have to pass through these register 

other consequence would be that um this control signal will also have to be passed and made available in that particular cycle okay

so um the that would actually mean that more bits are being passed through um these registers and the other consequence would be where the delay is getting carried

so if you keep it here that delay will get added to the register file operation okay right now it s a um this delay is not series with anything else 

from um this register um you are going to through multiplexer and then to this register that s a very small path so it its not bothering of it all [noise] yeah [noise] yeah [noise] yeah [noise] yeah yeah that s one possibility 

um that if you um if you store this after having generated this address you store it here but also remember that its not only this address which you need to store um suppose next instruction comes which also needs to write so that also will have to be stored   

so so two three addresses will have to be stored and effectively what we are doing in this register is same thing okay

this registers are basically doing the same thing um we o only thing is that [noise] the we are not storing at one place um in one cycle we store here next cycle we move um and store it elsewhere and so on

so storing in a sort of a first in first out kind arrangement alright okay 

now um having done this design um we we need to understand what is missing here um um as you recall that last time we discuss that for datapath for data hazard and control hazards um we we have two approach 

either we um introduce appropriate delays okay we suffer the delays okay we suffer  the delays or we do something so that these delays are reduced 
(refer slide time 00:32:20)


um that when um you have data hazards which comes because there are two consecutive instructions with um dependence among each other that means the value computed by one instruction is used by other

actually um the the instruction which may cause data hazard may not be consecutive they could be with one gap or two gap depending upon the length of the pipeline

in a very deep pipeline when you have many stages um the the opportunities for data hazards are many many more [noise]

so here in this case for example lw is putting a result in t one which is required by add instruction 

so we need to delay the register read stage of second instruction um to the extent that um it matches with register write or write back stage of the first instruction okay 

um that that is possible because we are sharing a cycle between read and write of register file half the cycle for writing half the cycle for reading if you don t do that then there will be one more delay 

so the there are two ideal cycles or two bubbles we we have introduced 

now what will happen in this design is um that we have not made any arrangement to check this situation and introduce delays 

so in absence of that what will happen is that um an instruction which follows and there is a dependence its going to read old results which are not valid and computation will go wrong

so so that s that s the problem which exist here um for um so what is required is that we we need to um first of all detect that there is a dependency

there are two instructions following close to each other um we need to match their um re register fields okay

um now what it means is that we would need to look at the the register field of more than one instruction and therefore um we need to pass on part of the instruction also um through these registers okay

the relevant information [noise] so for example suppose we are trying to compare  an instruction which is in this stage and instruction which is in that stage okay

lets say there are two instructions which could be um in consecutive cycles okay 

we want to see if this instruction which is here is going to write into register which the instruction which is here is going to read from and we will not allow this instruction to proceed further if that is happening 

now the the read um address of this instruction are available directly because instruction is stored here but um the instruction which is reached here we have um in this design we have not carried this information forward 

so so fields lets say um um actually write field is here so that that information is available here actually 

if we if we look at this information which is the write address for instruction which is now here 

compare it with the both read addresses of the instruction which is here we can come to know if there is a dependency between these and um um um as a result we would need to take some action um 

if we have um forwarding path we need to enable those paths otherwise we need to halt these instruction and not allow it go through 

ho how do we do that um basically we we require um to control um transfer of information into these registers okay

um every time you clock let this register it means you have passed instruction which is here to the next stage 

if you don t clock this here that instruction doesn t pass okay um [noise] yeah so so we need to um keep the instruction here so so we also need to um um disable the clock of this okay

so that um what is here doesn t change and um what is here also i mean that that limit is so this actually okay 

what we should do here is that we should um put a no op instruction here okay 

so so um we should um if we have designed all of the control signals so that zero value means inactive what we can do is we can simply zero this zero the contents of this register okay

so we will see how it is to be done um [ ] the next class but i am just giving the basic idea (refer slide time 00:37:10)



the the second point was um whats happening when control hazards occur 

so we have an instruction which is taking some decision and then deciding whether to follow sequentially or go to some address L 

so now um if this instruction is [noise] um following through these stages um normally there 	will be tendency the pipeline to start with I plus one and I plus two 

it is only in this stage you will realize that a mistake has been made by us and then you need to um nullify this you need to um flush these instructions and effectively um the a instruction label L is delayed is starting at this point okay

so um detecting that there is a branch instruction is very easy you don t need to look at two instruction here you just look at one opcode or you look at the control signal which is coming out indicating that it is a branch 

an and then um be ready to flush the instruction so other machine there could be several possibilities here 

one possibility was to um just moment you see a branch instruction freeze don t do don t bring in more instructions and at this point decide whether to start with L or I plus one

other approach was that you let I plus one I plus two etcetera come through if necessary you flush them out um if you go wrong 

um yet another approach was to do a prediction at this point you decide what is likely to happen if I plus one likely to happen or L likely to happen and start with that 

when you know the correct thing um and if you have gone wrong then then you flush whatever it is okay

so so that s it that s called branch prediction it may be done statically that that means by by um there may be some logic some um heuristic which um which you will use to decide whether to predict um the inline operation or the branch operation okay

it could depend upon the opcode what kind of branch it is whether you are branching forward or backward and so on 

typically what may be done is that um if it is backward branch it means that it is a last instruction of a loop typically um and most often loops are iterated several times okay 

so more likely take a branch okay in such a case you you will predict that um the the next instruction is most likely help

of course such an arrangement also means that you should be able to calculate the address L either you should have stored somewhere or calculate 

so so this is static prediction but there is also a dynamic prediction where you keep track of what happened last time when same instruction [noise] was encountered

this instruction could have been part of a loop and will be done many time so um last time it s a very simple prediction that last time if it was um a branch taken you think that it may be taken more likely to be taken this time also 

so so this is a most nave way of dynamic prediction but they are more sophisticated one is that 

so um in any case um lets um just be clear about the actions we require in light of the hazards (refer slide time 00:40:33)



um we will work out the design changes to take care of this um in the next class so for data hazard the first thing is to detect instructions with dependence and um [noise] you introduce suitable number of no op instruction

okay sometime delay of two is required sometimes delay of one is required um so you in in in the pipeline where things are moving smoothly you insert bubbles okay or no op instructions
 
more complex approach is um when you have data forwarding path you have to suitably enable the paths

um the introduction of bubbles may be required over and above this okay some sometime data forwarding will eliminate the delay entirely sometime it it will only reduce 

so instead of two cycles of delay for example you may need only one cycle of delay 

so so that delaying logic is still necessary for yes please [noise] 

yeah all that is to be done if you ba basically um as the instructions are flowing at some the instruction which have gone beyond a point you will allow them to move whatever is behind that is held up 

everything behind that is held up including fetching next instruction and um moving of forward [noise] yeah yeah exactly right yeah um i am coming to that in the next slide actually 

so control hazards require that you detect identify branch instructions and um there may be need to flush wrong instruction which were brought in okay

either they may be you may blindly inline instructions or you may do prediction or something else 

so in any case if this one is wrong that has to be um flushed out um and predictions could be fairly straight forward it could be very complex dynamic prediction is somewhat it involves um much more hardware 
(refer slide time 00:42:37)



um the last question where i want to leave is um are there software solutions okay

given these hazards can we do something with the program because you you will call that um unlike un like structural hazards which we took care by introducing more hardware resources 

um these the other hazards are coming because the way um program is okay the way the program is interacting with the hardware 

so can we do something in the software the answer is as i was hinted by somebody um that there is something which can be done in the software um assembler or compiler whichever is generating the code can do the analysis and rearrange instruction so that um the the impact of the hazard is minimized  

for example for the data hazard is coming when dependent instructions are close to each other 

so often it is possible to rearrange instruction without changing the meaning i think that that s the key line that any rearrangement if you do you should not make the program go wrong okay

so um all if um instruction A is depend upon instruction B the order cannot be changed okay

but two instructions are independent then it can be reordered

so by reordering you can separate out instruction which are dependent insert something useful between them which is independent 

and in worst case if you don t find anything else you nothing else you can insert no ops 

so if instruction sequence is organized in this way that you do insert something um if nothing else you insert no op um then then you can have hardware [noise] not worry about data hazards okay

so um hazards are data hazards are removed by construction of the program itself

um and then okay how do we handle the branch instruction similar spirit 
um after a branch instruction again you can put instructions which need to be done in both part lets say you have if then else structure there is something you do in then part something you do in else part

if there is something common which needs to be done both ways then um you can put those one or two instructions immediately after the branch instruction and imagine the branch um [noise] has to be effective always two cycle later or three cycle later okay

so um if this extra instructions if they are not available after the branch i mean if if you cant if you cant find something common in then and else part you might move it from um the code which is before the branch 

as long as this is not dependent okay you you can sometime bring something before the branch instruction and place it after the branch and and positions are called delay slots of the branch instruction 

so suppose um normally you are able to decide two cycles later um what is the next instruction um then this delay can be filled up by um defining those as two slots two delay slots where you are going to use some hopefully useful instruction 

if you don t have useful instruction  to be put there you put no op 

but um um the branch is going to the effective after these two okay either you go to um the the instruction which was normally to follow branch here or um the tagged instructions 

um and this um these transformation in the program  can done by the compiler assembler or the programmer if program is written directly and um simplifies the task of hardware 

in the case of data hazard data hazards need not be checked and in case of a branch  the hardware needs to just um make sure that the the branches are effective only after the delay starts okay (refer slide time 00:46:41)



okay to summarize um what we have learnt today um we completed the datapath design and um datapath design we have is um nothing but  the single cycle datapath design with inter stage registers 

and we have to make sure that um every line goes through right number of register so carefully um one has to do that 

um the con the next thing was controller and the controller also turn out to be same as the single cycle datapath controller but control signals are to be passed through um the inter stage registers so that they get time correctly	 

um we need to take care of WB stage because WB stage involves many thing get fed back the data control and address so they need to be time um carefully 

so handling of hazards requires detection stalling flushing um um we can also have some support from software which simplify the task of hardware

so um often we have some combination of both which is used to minimize their effect 

i will stop at that thank you 












TRANSCRIPTION: BRINDHA
                                  
COMPUTER ARCHITECTURE

Prof. Anshul Kumar
Department of Computer Science and Engineering
IIT Delhi

LECTURE# 26
Pipelined Processor Design: Handling Data Hazards

There are three types of hazards which a pipeline design may face 

these are structural hazards data hazards and control hazards

we started with the design where by um the processor design itself we took care of structural hazards that means enough resources are introduced so that therf are no structural hazards 

um but the problem of data hazards and control hazards is still there [noise] in the design [noise] we have talked of

um today we will see how to handle data hazards in particular 
(refer slide time 00:01:40)



initially we will see that um how we can introduce no ops or bubbles in the pipeline so that um the the functionality remains correcst

and then we try to introduce techniques of data forwarding and improve the situation 

so um first we will examine um the question of stalling th pipeline in detail
(refer slide time 00:02:02)



um for different types of instruction sequences what is the number of stall cycles required and which instruction or in which cycle do we need to introduce the stall

um we will see what additional control circuitry is required to actually cause these stalls to be introduced um then we will define the conditions which needs to be detected in order to um exercise this control 

um after having done that	 we will talk of [noise] data forwarding that that s a technique where you sort of bypass the register file and um pass on the data from one instruction to other instruction um as early as possible

so that require the additional path we will see what these are 

um once you introduce additional paths you need um additional control and that s what you will see next 

um finally we will see that um in spite of introducing data forwarding you may sometime needs to stall so under those cases how stalling is done 

okay so here is the um pipelined datapath which we have designed in the previous lectures (refer slide time 00:03:08)



and you would recollect that all we have done was taken single cycle datapath and introduce inter stage registers

and um um after introducing controls this is how things looked
(refer slide time 00:03:20)



You have datapath and control which is much the same as single cycle um control but again we make the control signal pass through um the the inter stage registers so that um they they are applicable in the right time 
(refer slide time 00:03:44)



okay now um lets look at the issue of um stalling when um you find a pair of instructions which are data dependent

so suppose you have an instruction which you calling IF instruction which is the load instruction storing something in T one 

and immediately an add instruction follows which this time to read T one

so um we had earlier seen that this instruction which is following load instruction it was one um would be able to begin with the delay of two cycles okay

and the picture which i had shown earlier was this where we said that um this cycle goes waste and this cycle goes waste and this instruction starts after a delay of two

so that register read for second instruction and register write for the first instruction happen in the same cycle 

um this makes sense when reading and writing of register file is being done in a single cycle 

first half of the cycle you write in second half of the cycle you read in that case the value which is being written lets say middle of the cycle here is getting read in this and um the data is correctly forwarded 

but um the this picture is not quite correct because what will happen is that in in this cycle remember that in this the horizontal axis is the time axis 

so in in this cycle um instruction um I plus one will get fetched okay

so IM cycle will get complete and um it is the next cycle which will get actually held up 

so the correct picture is to show like this that IM this I plus one instruction is in the um fetch stage here okay it is in the instruction memory

um then the next cycle wha what one would expect is RS [noise] cycle but um it cannot read correct values here so it is this we want to um prolong 

so this instruction would remain in RF stage	for three cycles and its only the last or the third one which will be fruitful where actually you will read values and proceed further okay

so so this is the way um you will actually implement um delaying of instructions it is somewhere in the middle you need to delay you you got the instruction the pipeline 

you notice that you cannot proceed further forward you get stuck at RF stage um this instruction stays there at that stage and um then two cycle later it proceeds normally

so now um we can view the same thing um you remember that um we discuss that pipeline can be seen in two different views 

in one you show instruction by instruction okay and um have horizontal axis indicating the time 

so so for each instruction you put show which stage it is passing through in different cycles

the other approach is to um show all stages and um have the time um time time shot snap shots of um the pipeline in different clock cycles shown one after another vertically (refer slide time 00:06:52)



if we do that this is the picture which will emerge and it will throw more light into what s happening in the pipeline 

so these are the five stages and um what we say here is that at time t in cycle t let us say the instruction I is here in the ALU stage okay 

then um clearly um I minus one and I minus two the previous instruction have ahead they are here 

I plus one instruction which is our add instruction that s where the tower is it has reached this point it has reached RF and I plus two has reached IM now

so from um from this cycle we will trace what happens in subsequent cycles 

so um you would also notice that i have not shaded these two because these ins these stages or these cycles of instruction I plus one and I plus two will not be able to complete and these two instructions will remain stuck 

so its remember that its not only I plus one which gets stuck for two cycles it is the one which is following 

so everyone behind the um instruction in the queue um gets held up 

okay now since in t plus one this is instruction is not able to move forward we have to introduce our no op no op means no operation it s a null instruction which um does nothing okay

and i have not shown ALU shaded which means that um instruction here but it s not utilizing anything it just um occupying a cycle 

we go to next cycle um i this is still not able to proceed okay so um third cycle will be spent by um instruction I plus one within RF stage and here it will culminate it will complete here 

so another no op gets introduced here because this no op moves forward um this is no no nothing stopping that moves forward and you introduce another no op here okay

um and I plus one I plus two are stuck there but this time they will complete and then move forward 

I plus one will move to ALU stage here because here it has got the operands and um behind it other instructions will also follow okay

so so now effectively what has happened is that in this pipeline two bubbles have come okay 

so lets tell fluid was flowing and there is um some air bubble um which is um indicating no activity and of course with time this will get um passed off

so um we we therefore need to um be sure how to introduce these two no operations at the correct time instant

um the that is crucial with this understanding lets go back to the instruction wise background (refer slide time 00:09:34)



and see more accurately what s happening

so um instruction I was like this okay and in instruction I plus one we had introduced um these two inactive stage i mean an instruction is still in RF but not doing anything here 

um in between we have nop one and nop two two nops introduced um which don t go through fetch stage okay

no operations get introduced from ALU stage onwards if they are not being fetched from memory as they are not going through RF stage 

but they get into use from ALU onwards and then move forward so nop one um has this profile nop two has this profile um then I plus one I plus two  

so I plus one you would notice is getting stuck at IM for three cycles 

so now um what is happening basically we need to check if an instruction which has reached um RF stage it can proceed forward or not okay

if it cannot proceed um further then we need to introduce a no operation and um this condition we we will work out a condition 

if this condition is true for one cycle one nop gets introduced if this condition persists for two cycles then two nops gets introduced okay
(refer slide time 00:10:56)



so now um lets look at this design there you have datapath and control okay its same thing i might have um re-rooted some wires um ju just for the sake of clarity of the diagram [noise]

so now um where is it how how do we introduce no operations and how do we prevent any instruction from moving forward [noise]

so um we have this register and um here is pc if you do not clock this register if you do not put in new information in this then um whatever is here in this stage does not change okay

so lets say in some cycle instruction was fetched brought into this register 

if you do not change it for another cycle um this instruction which is here remains as it is okay it does not move forward 

and similarly if you do not um update pc okay um new instruction doesn t get fetched okay [noise] the pc also remains stuck with the old value

so we had not explicitly shown control signals for these two registers but now to take care of stalling we need to um define control signals for these which may be turned on or off depending upon what is the requirement [noise]

um secondly um in order to introduce a null instruction or no operation instruction um we could [noise] load all zero control signals all null control signal into this registers 

see remember that um control signals which are applicable for subsequent stages um gets stored in this register for a cycle land then some of them gets stored in this register for another cycle and then so on

so um we assuming that we have defined our control signal such that zero always means inactive situation okay

we have put a multiplexer here through which controls are passing so there are some six seven or whatever number is um all those are going as one input of multiplexer 

other input of multiplexer has all zero so depending upon how we select this multiplexer either these control signals get passed on or all zero values get passed on

so for introducing a bubble or a nop all we need to do is this um control signal which we are calling as bubble needs to be made one

when you make it one then effectively you have introduced a no operation nop instruction in the EX stage okay

um so so now basically to um take care of the situation which we had just um illustrated um if you make pc write as zero IF ID register write as zero and make bubble as one okay

um this combination of control will achieve the desired result right um what we need next is to figure out when do we do this okay

we need to detect that condition we need to basically detect the dependency between two instructions at the right time [noise] (refer slide time 00:14:08)



so here is the condition which you need to check to know that there is a hazard 

so first first of all stating this condition in words what we need to do is instruction which is in RF stage if that read from a register in which the instruction the ALU stage or DM stage is going to write okay

um so so we look at instruction which is sitting in the RF stage we also look at the instruction which  are sitting in EX stage and DM stage 

see where these instructions are going to write and 	see where this instruction is going to read from

if there is a match then there is a hazard condition so now um this suppose um at some point of time there was s hazard between instruction um in RF stage and ALU stage okay

um that will cause instruction RF stage to held back instruction ALU stage will proceed to DM stage and we will still see i have that condition that means we we are still waiting for that instruction to move forward and write

so so this con conditional persists for two cycles and um tow nops will be introduced 

the the second instruction which is dependent will get delayed by two cycles 

um but suppose we notice the hazard only when the instruction is in this stage 

um um i am sorry in this case you  will always first see it here and then here 

so actually um the way we have described um in in this there will always be two cycles introduced because any instruction on which something is depending will pass through this stage and this stage and on two occasion you will see this condition

okay now how do you precisely check this condition um um i am introducing some notation here there is register name okay this is the inter stage register and followed following the dot there is name of the control signal okay

so what we are saying is that instruction um which is stored in this register ID/EX so that means instruction is actually in EX stage okay

instruction in EX stage has this RD RW signal active that means its an instruction which is going to write into register file okay

it could be a load instruction it could be a R class instruction and one of these two condition is satisfied this or this and this where we are comparing the register addresses 

so we are comparing RS and this is where we are comparing the register addresses

so we are comparing RS address in IF/ID that means an instruction which is in RF stage has a [fourth] register which matches with RD um specification of instruction which is in the ALU stage okay

or um this destination is matching RT so one of the two may be matching um the destination here is matching with RF or RT 

or the second part of the condition be instead of ID/IF we checked with EX/DM so that is instruction setting in the um DM stage okay

so if that instruction intends to write and there is a match in the um source and destination register 

now um there is one um little cache here is that um we we are we are trying to make sure that the instruction which is ahead is going to write 

but we haven t really put the condition which says hat instruction which is behind actually reads the register

because we we know that some instructions like jump do not read the registers okay 

so the there is no point in trying to hold back a jump instruction um so so therefore this condition needs to be um refined little bit so that you you check it only if the instruction um actually reads the registers

so there are some registers which will read only RS but not RT right 

so we we need to um take care of those um but i am omitting those details okay

so um if an there are two conditions which which are corresponding to two parts of this condition 

another point which must be noticed is that when we are looking at RD field in that instruction um here um this one or this one we we are assuming that a two alternative destination addresses had been multiplexed 

so what i mean here is that these two addresses are multiplexed here and then we are checking it here or we are checking it there okay

um for some reason suppose you have placed this multiplexer somewhere else you you can do that logically 

it it will have influence in terms of number of register bates and clock cycle and so on 

but um technically it is possible then in that case you need to check with both okay

but advantage of having multiplex is showing up here also that we need to check only one field here or there i am sorry this one this one or this one or this one okay

so so this is the condition which needs to be checked and once this condition is true we need to set those um IF/ID write signal PC write signal and bubble signal

any any question in this okay

so basically this is um some combinational logic um which should be considered as part of the controller okay

now um this is the simple solution when you don t have data forwarding paths (refer slide time 00:20:04)



you were always introducing two cycles of delay if there is um dependence between two consecutive instruction 

yeah [noise] i think there is a situation where um the the delay introduced will be one cycle um this is if um the the two dependent instructions are separated by one more instruction in between 

so so in that case this match would occur when the earlier instruction has reached DM stage and the one behind is in the RF stage okay 

so um when the first instruction is in the ALU stage then um it will not find any conflict there 

so so there could be situation where we introduce only one cycle okay 

now lets look at the data forwarding techniques um as you recall that idea here is that as soon as a result is generated we pass on to um any following instruction if it requires it and we try to do it as early as possible 

so the there are various scenarios i i am going to repeat those slides so suppose there is a add instruction following an add instruction 

then the result of first is available out at the output of ALU and it needs to be sent to the second instruction at the input of ALU so that s one possible path um i am calling this p one

um this path there could be activate means two paths leading to this input and another one to that input 

so um we we will actually require both um another possibility is that a load instruction is followed by a dependent instruction like add

so in this case the data needs to be forwarded from DM state to ALU state and here you still need one cycle delay

again um this is not correct actually um we should show IM as the solid thing and RF as the dotted one here 

um yet another possibility is that you have add instruction followed by a store instruction um particularly when see store instruction um requires two registers um one register um has to be read in the third in in is required actually both are normally they are read in second cycle in RF stage 

um but actual requirement of the value um is in ALU stage for one of the registers and in um DM stage for another register

the value which has which is to be written in a memory is actually required in fourth cycle [noise]

so we are talking of that dependency where um the data coming out of ALU is required in this okay

so now we we don t pass directly from ALU to DM because that um that that is no longer available that value would have moved to next stage okay

so we will have to tap it after the DM stage and get it to input of the DMA and fourth possibility is that you have load followed by a store 

so output of DM goes to DM so so now these um (refer slide time 00:23:18)



we um we have these paths okay um this slide actually summarize all of them 

you have P one P two and i am looking at P three P four all together 

so p one goes from ALU output [noise] um strictly speaking from the output of EX/DM register to ALU input one or two 

then P two goes from um [noise] out um it is basically output of DM or ALU okay

um we have to tap it from DM/WB register and send it to ALU input one or two okay 

so um why i am saying output of DM or ALU is because um [noise] okay well its um um because we will actually um try to tap all these um after the multiplexer okay 

af um basically there is a DM/WB register after that we have a multiplexer which will look at two possibilities and we will take it from there and take it to the input of ALU input one or two

and then from the same multiplexer we take it to DM input 
(refer slide time 00:25:01)



so um i have taken part of the datapath okay augmented with these forwarding paths and shown here 

so so lets identify these paths um P one is here this is P one from output of ALU um we are not taking from here we are taking after the register 

we are taking it to um two inputs of ALU so you notice that there are additional multiplexer which have been introduced okay

the normal input is zero okay which is either coming from register files through this register okay or from register file um through this register or the offset 

one of these two gets selected and coming as the normal input 

so um input labeled as one are those which are part of P one okay 

so so this output is available here and there [noise] okay depending upon where you need or may be you need at both places right you you could have an instruction which is trying to add um you might say add A comma B comma B

so that B may be dependent and its required at both the inputs 

so you may situation there um you may pass on same thing on both the cases 

um then P two is this we are taking from this multiplexer output of this is normally going to register file okay

we are bringing a copy here at input two so this is P two 

for P three which is from DM to DM we have introduced an multiplexer here the normal path is which is coming from register file okay and um the forwarding path is the the copy of this is brought here

so now we require three control signals [noise] forward A [noise] forward B and forward C [noise] right and we need to work out conditions um which will activate these or which will give proper value to these signals 

so forward A is actually two bit signal okay but symbolically we will say that it has the value zero one and two similarly forward B 

and forward C will have only one of the two values okay is this clear 

um this is how exactly the paths are organized and next we need to see the conditions to energize these paths

okay so lets look at um P one and P two the paths which are leading to the multiplexer at the input of ALU (refer slide time 00:27:38)



and these are coming from different stages the control we require are forward A and um um forward B would have similar equations 

so when is forward A equal to one that is the condition when is forward A equal to two that is the condition (refer slide time 00:27:55)



um if these conditions don t hold by default you will make sure that forward A has a value zero okay none of these holds the value zero 

um condition for forward B would be similar but first lets look at this

um what we are saying here is now again the same notation is used that instruction in EX/DM are basically in the um yeah instruction EX/DM stage um intends to write in the register file okay

and it um is in intending to write in a register which is not addresses a zero 

so we are taking special care of zero register because um that does not cause dependency 

actually we should have made this check in earlier case also when we were specifying other condition without forwarding

because um if one digit one instruction is writing into zero for whatever reason right or wrong

another instruction trying to read zero there is no dependency because the value is going to be zero 

so we are checking here that this instruction intends to write it intends to write in a register which has non zero address and that address is matching with RS of the instruction which is currently in the ALU stage okay

so the instruction from where we are picking up the data obviously that has reached here okay it is this value we are feeding back 

so instruction here intends to write into some register from where the instruction here intends to read that s what we are checking 

um similarly this is a forward A equal to two if in DM/WB stage the instruction which has even moved further intends to write into non zero register and um that matches the destination matches the source here

also we need to um make it two when it is not one okay imagine that there are um there is some instruction in this stage which is reading from some register where instruction here is trying to write as well as instruction here it is trying to write

so so actually we should forward the data from this one because this will be um more current value which is supposed to read okay because if this writes to some register the instruction here will overwrite over that 

and it is that value which will be latest and should be read by this instruction 

so so therefore we have um introduced this additional um check here that this is not matching that means the instruction which is between the two is not intending to write in the same register okay

um the that is required so that we read the la latest value only

okay the condition for defining forward B is exactly similar all we do is rs is replaced with rt 

okay because forward B is nothing but controlling the multiplexer which is below and um here the relevant register field is rt [noise] in place rs rest of it remain same

so this was for data forwarding path P one and P two 

for P three P four we are trying to forward data from WB stage to the DM stage 

so there is a similar condition that instruction in WB stage intends to write into non zero register and the destination there matches with the instruction in DM stage	um which actually require rt register

so um these are the conditions which can be checked by control and um take energize or enable the right forwarding paths 

having done that now what is still required is um that in spite of forwarding there are cases when um null needs to be introduced and that may go back to this case 

so the only time this happen is that um the earlier instruction is the load instruction okay because that is the only one which writes um the the result is ready after fourth stage

and the following instruction needs um the result in the ALU stage okay 

so we need to check this condition and we know how to introduce um stall cycles 

its only the condition now we need to define freshly 
(refer slide time 00:32:50)



so stalling condition when data forwarding is happening in the manner we have described the condition we need to check is that instruction in the RF stage reads from a register in which a load instruction we are not saying any instruction but specifically a load instruction in the ALU stages going to write okay

so the the check is happening actually um again um referring to this diagram um we we have to check when this load instruction is here only okay 

its not that we check when instruction reached there um it is still at this stage that load is here and um the following instruction is here um we hold the following instruction at the RF stage for one cycle 

so that will be the effect we will achieve if we check this condition that is um we we are looking at MR signal which is memory read the only instruction which does memory read is load instruction okay

so we are looking at the signal if this signal is active in the instruction which is in EX stage and um the the destination of that matches with either rs or rt of the instruction which is at RF stage 

then we um exercise our controls for holding the instruction back by the cycle and introducing a nop

so so we have um again same shortcoming is here in the condition that um if if the instruction which is following is a jump instruction we don t want to do all that okay (refer slide time 00:34:30)



okay so um i will summarize at this point basically we have looked at the data hazards um we analyzed what are the different situations um we saw its effects in two different ways when we are seeing instruction by instruction how the picture looks like 

we notice that instructions are um getting introduced nops are getting introduced from ALU stage onwards

you also saw in the um in the stage view that is you you look at all the stages and take times um times time slabs

so um [noise] this was seen in two views to get a clarity of the situation and we identified how we are going to how we are going to introduce um stall cycles okay

um and nops are inserted the control signal were defined for that then conditions were defined we check the hazard and activate those control signals

um then to improve the performance we talked about um data forwarding paths okay

they have they have a three types of data forwarding paths actually four but two of them we consider together P one P two P three P four 

P three P four we consider together because we took the value af after the multiplexing okay so both were combined

and then um this requires its own control because there are some multiplexers which need to be activated correctly

um further there are still situations where stalls are required and we define conditions for introducing stalls and um along with the data forwarding 

so so basically the conditions which we develop for stall in the end um would be used in place of the condition we developed earlier okay 

so if if you do not have forwarding paths then there was one set of control condition

if you have forwarding paths then um you you need two things you need um control signals to um select these multiplexers correctly and also in special cases introduce the stalls 

so um if they are any questions i will take that otherwise we will stop here

no questions 

okay thank you


 





TRANSCRIPTION: BRINDHA
                                  
COMPUTER ARCHITECTURE

Prof. Anshul Kumar
Department of Computer Science and Engineering
IIT Delhi

LECTURE# 27
Pipelined Processor Design: Handling Control Hazards

We have been discussing pipelined design for MIPS processor um last time we had seen how we can handle [noise] data hazards by um introducing stalls if necessary and um we also saw how we can introduce um [noise] bypass paths or um forwarding paths so that um delay can be cut down 

we notice that in some cases delay may still be required or the stall cycle may simply required and we worked out the logic which is required to be put in the controller to take care of um this situation 

so you need to detect when hazards are occurring you need to basically find out the dependency and um then introduce appropriate control signals 

for today we are going to look at the issue of um handling control hazards [noise] (refer slide time 00:02:02)



(refer slide time 00:02:04)



so um we will take the design which we have discussed so far um modify to handle the control hazards um then we will see how we can improve performance in view of um control hazards [noise] or branch hazards 

and a couple of techniques we will look at briefly will not go into too much of detail 

we would look at the possibility of eliminating branches altogether in some special cases or trying to speed up the execution of branches or um introduce something what is called prediction of branch

so you do branch prediction and try to take action accordingly 

um finally i will um make a brief mention about um dynamic scheduling which is used for instruction level parallel architecture and tha that s another way which is used to keep the pipeline full um in in view of branch hazards and data hazards [noise]

so just to recollect what was the effect of data hazards on um execution of instructions (refer slide time 00:03:07)



we had seen that um when data hazards are there you need to introduce um null instruction or nop instructions or bubbles in between instructions 

so for example looking at a sequence of instructions in the stage wise view where um you are showing various stages and vertically you are looking at various cycles or time instants you would notice that nops are getting introduced when um there are two instruction which are dependent on each other 

same thing seen in another view (refer slide time 00:03:43) 



where you are looking at [noise] instruction by instruction and um here are the two nop instruction because [noise] um the instruction which is dependent is getting delayed and this delay is um um passed on to subsequent instructions also 

so everything behind um this instruction in the pipeline [noise] gets delayed [noise] (refer slide time 00:04:08)



so to improve things we had um identified where we can have forwarding paths 

so this diagram for example shows that um you can have paths going from output of ALU through this inter stage register back to ALU okay 

or you could have um output of um the memory stage after this register and possibly after multiplexer going back to memory and also that too ALU 

so now you need to derive control signals which guide these multiplexers 

so so you need to detect which um source address is matching with the destination address in the two instructions and accordingly [noise] enable this paths 

okay now coming back to the control hazards (refer slide time 00:05:02)



so we now imagine a branch instruction like this okay and these are the stages it is going through and um here we take some decision and either continue sequentially or branch to an instruction label L 

so we assume that if um the processor continues getting instructions in sequence at some point suppose we come to know that there is a branch and the condition has be become true 

then the address change is the sequence is broken and you um you can fetch the target instruction here 

but now you realize that the two instruction which had entered the pipeline are not the one you intended and therefore they need to be flushed out

so so you need to actually generate a control signal which will do this 
(refer slide time 00:05:53)



so here is um the um the datapath and controller which we have designed so far 

now um there is one thing which we need to notice here is that pair actually the way we have designed the branch will take place in four cycles 

unlike what we expected here we thought that um the condition regarding tested in ALU and the next instruction can be fetched in this cycle 

so um but what s happening in this datapath is that we are looking at um the target address after this register

and we are also looking at the condition which is tested by ALU after this register 

so that means it is in um memory stage only	we are um looking at this um outcome of the branch and here is that that the control signal is being generated for feeding to this multiplexer

so so basically if this is how it is done then there will be one more additional delay

okay so this instruction actually can start only um the instruction  will be fetched and be available um this cycle onwards okay

so so because at the end of fourth cycle you are latching the new address into um program counter and therefore the fetch cycle shifts to this 

so we will try to prove this by um trapping this output the new address and the control signal um before the register 

so this is the change which is shown here um we are not making this pass through this register 

um consequences of this or that the delay calculation which ultimately determines the clock period um would undergo a change 

so now you need to um ac ac um account for the multiplexer delay of this which we have ignored so far um along with this adder and also with this ALU 

um to to look at the to consider the um path delays okay so now um if you look at the paths going from this register to um some registers in this case pc 

so so you need to account for this ALU this and gate this multiplexer and then to pc so so this is one path 

similarly um the path you need to consider is that um going through this s two adder then and gate and multiplexer and so on 

so so your um the delay is getting redistributed and um whatever is the influence of this change on the clock has to be borne in mind 

um what we the objective here is that we should be able to decide at the end of third cycle what the next instruction is okay

so so this is the change which will en enable that 

now having done that um we need to flush the instructions which have entered the pipeline um in anticipation but are not required 

so um when we are loading the new value in pc when we are loading the target address in pc that is the signal which is actually making it possible um we need to use the signal to um flush out the instructions which are um in in these two stages okay 

so the instruction which would have normally um gone from this stage to this stage okay and the instruction which have which would have gone from this stage to this stage um would be flushed if you make the contents of these two register as zero 

so effectively you have made these as nop instructions okay we we um what we had realized is that if you make everything um in this register zero it acts like a nop instruction 

that s what we were trying to do when we were introducing um nop s in the case of data hazards

so similar action somewhat similar action is required that we need to make contents of this register and this register zero when the signal is active okay 

so if effectively we have flushed away these two instructions and do not allow them to proceed forward 

is that alright is there question about this okay

so this is the how we can handle the branches in the most simple manner um in general now we realize that there are um two things which are happening in a branch instruction in general

that there is a condition which is being evaluated and um the there is an address which is being evaluated 

so two things would typically happen um which would happen in some particular cycle (refer slide time 00:11:03)








now suppose there was a deep pipeline so somewhere there is a condition evaluation which is getting completed somewhere target address of the machine is completed

so what is the earliest you can have next instruction if you are going inline that means you are not branching and what is the earliest you can have the next instruction if you are going to the target that is the branch is taking place

so the the particular cycle is where these two activities are happening would determine how you can place the next instruction 

so whatever technique you are applying [noise] if if you are predicting um one of the two alternatives and going that way then um we need to keep in mind these delays

so to make things better we this is what we need to cut down 

in our um example here um this was happening we started with this where um basically the computation of next address and condition evaluation were getting completed in the third cycle and [noise] in fourth cycle we were using it 

so um if you can um reduce this for example (refer slide time 00:12:20)



we we finish this in the third cycle itself and took decision there um so thereby we reduce the delay [noise]

if we can reduce it further as we will see later on um we can make things even better 

so so this is what has to be kept in mind

another factor is that um there are architectures where condition evaluation and branching are split into two instructions 

so so you would have for example subtract instruction and the result of that will be used to set some flags

the branch instruction will simply test the flag it will not actually do the comparison it will only test the flag and um carry out branching 

so condition evaluation there is trivial um in fact the real evaluation takes place in an instruction which is earlier 

so it could be immediately preceding instruction or it could be an instruction which is um occurring um one or two earlier 

so um so so we need to therefore we need to see where exactly in the previously instruction the condition is getting evaluated  

[noise] now um there are several ways in which you can improve the performance um in view of branch hazards (refer slide time 00:13:34)



there are cases when you can eliminate branches altogether and therefore get rid of the problem [noise] um you can speed up branch execution this is what i i was discussing previous slide is that you can um take these decisions earlier or you can move these events as early as possible

or increase the gap between um decision making and actual branching 

so we will see that in um some more detail 

um then third thing is branch prediction where um you do something in anticipation in in a manner that most of the time you are correct and um when you are correct you are saving a lot of time

um um if you in the [noise] rare event of not getting it correctly you lose time 

but in probabilistic sense if you take average	there on the whole you would have improved the situation 

so this branch prediction can be done statically or dynamically i will um distinguish between these two how that is done [noise]

and lastly um particularly when you are doing dynamic predication you are looking at what happened in the past 

so when you are doing that you can also remember the target address and the thereby um also speed up not only the decision but calculation of the target address [noise] 

so here is the small illustration of branch elimination 
(refer slide time 00:15:02)



so suppose you are testing some condition [noise] and if the condition is true you want to do one step um one instruction and if the condition is false you want to skip that 

so it s a very small if um or conditional structure um so there  may be one instruction or two instruction not too many

um these can be replaced by um what is called conditional instruction or predicated instruction 

so some processors um have this [noise] feature [noise] that with almost every instruction you can attach a condition 

so so you would have um some field which will indicate that this instruction needs to be done only under certain condition 

so essentially you have fused these two you attached the condition with the instruction itself and um if the condition is true the instruction gets done otherwise it is like nop 

so um suppose there was um conditional branch here so this is not MIPS language it is testing if the condition um this specifies which condition is being tested 

so condition being tested is um the zero flag which indicates if result of previous instruction was zero 

so if z flag is set then you branch to current instruction plus two that means you skip that okay

so um if add instruction had a provision of specifying a condition um you could put that together

so um here we are saying that you do that if z flag is not set NZ means non zero okay 

so we are um removing this branch instruction explicit branch instruction and putting that condition with the um add instruction itself

so so now you might wonder that whatever you do the condition has to be some somewhere or the other tested right so yes that is true [noise]

but now condition testing is part of the add instruction so so you could start preparation for doing the addition and depending upon the condition you may store the result or you may not store the result okay

so if you look at this as a whole you would have saved a few cycles [noise] (refer slide time 00:17:30)



okay how do you speed up execution of branch okay um now you can um speed up target is calculation and condition evaluation 

um so far um calculating the target address early what you can assume is that each instruction is a branch instruction okay and um generate target address 

what may be done normally is that you may test you may check you may decode the instruction find out if it is branch instruction then do the target address calculation because now you know that it is a branch instruction 

but um we can calculate target address in anticipation actually in the design we have done this is um already happening okay 

so recall in the in the second cycle um when you are fetching the registers A and B we are also calculating the target address and we keep it in a register and use it if necessary 

so so this this technique we have not mentioned explicitly but we have already used this 

um situation could be more complex if virtual memory was involved so address calculation also involves um page table look up and doing that translation from virtual address to real address 

so its not simply um address calculation does not simply involve making um doing an addition but it may involve something much more

so um starting in anticipation like this um is definitely advantageous and um you can also omit page um this translation if the target address is in um in same page in which you have the current instruction okay

so so all that check could be performed early and if the instruction um is not found to be branch instruction you can discard this so no harm done

also um now secondly condition evaluation again you try to move this as early as possible in the in the sequence of cycles um and we will see an example of that 

so i am showing a part of the design okay last is i have just stretch things horizontally and the last stages have been thrown out um just to make space (refer slide time:0:19:53)



so um what we have done is um instead of checking for the quality in the ALU we have introduced a comparator in the second stage itself okay in the in the second cycle itself 

and also the target address calculation which was actually happening um here in the third cycle although in the multi cycle design we had described earlier we had done this in the um second cycle because ALU was free at that time 

so our [noise] motivation there for doing that in the second cycle was that utilize the ALU while it is free okay

but now we we know that it has advantage it is advantageous if we do all these as early as possible

so i am moving that adder here i am moving and introducing a comparator here okay 

so now what are the implication of this in terms of clock cycle is that um this path may not be um making so much of difference because um whether it is here or there it is just coming in series with and gate and multiplexer

these dont cause any delay but um the main um effect would be that [noise] the delay of register file and the delay of this comparator they are coming in series within a clock cycle okay 

so um it will definitely have some adverse effect on the clock cycle now um um remember that testing for equality is um much simpler as compared to testing for less than or greater than right

um in this case you need to do bit by bit comparison and then take and of all those you don t need any carry to be proprigated 

so it may be possible to afford um equality or inequality comparison

um less than greater than comparison may still be done within ALU the fast  ALU will be sitting in third cycle 

so so at least beq bna kind of um instruction can be speeded up by doing this right

um so so we are putting a simpler comparison here comparator here which will add to the delay but only marginally okay 

so now um wi with this done um we are ready to get the target instruction in the third cycle itself 

so we need to only flush this one okay we don t need to flush instruction which is going into this alright

so so this is a this is a um design with slight improvement where we are losing one cycle basically when branch actually occurs instead of two cycles okay

um now another way of speeding up is to increase the gap between condition code evaluation and um actual branching (refer slide time 00:23:03)




so so now imagine that you you were having separate instructions to evaluate condition and the branch instruction was only looking at some flags okay

so so flag can be easily looked at in a cycle as soon as we have the instruction 

um so what can be done is that you can increase the gap between the instruction which is actually setting the condition code okay and the instruction which is testing right

so so that um the situation is somewhat similar to um data hazard situation that one instruction is dependent upon the previous one 

so here one instruction is setting cc um the condition code other instruction is testing and if you increase the gap between these that means have other useful instructions in between then the waiting is cut down

so um as soon as the branch instruction can calculate its target address you can branch okay

um condition testing um um is done early 

the other approach was to do delayed branch that means um um do the target address calculation and um assume that branch has to be effective whether it is true or false um after a few cycles 

so lets just um say that you want to delay by one instruction that means after a branch instruction there will always be um an unconditional statement which has to be done whether condition is true or false okay

so um the the role of compiler or the code generator is that it tries to find um suitable independent instruction which has to be done under both conditions and keep it after the branch instruction 

and the role of the hardware here is that um you you take a decision about branching um well you you take a decision but actually make branch effective one cycle later 

um next is the technique of branch prediction (refer slide time 00:25:16)



where effectively we try to treat branches other as unconditional branches that means don t worry about the condition and try to branch 

or as no operation that means um don t branch continues sequentially and um when you know whether you have gone wrong or right if necessary you can undo 

so now this is the basic idea question is how do you um do the predication 

there is three three ways of doing it one is fixed prediction that you always guess inline

so effectively this is what we were trying to do we allow the pipeline to get subsequent instructions into the pipeline 

and um if you find the branch condition is true and branch you should taken then you undo so um this is called fixed prediction  

second is static prediction um here you make choice between going inline or going for the target by using  some criteria and the criteria is fixed

it does not depend upon situation which is created at the run time 

you should know ahead of time whether um for this particular branch you need to predict um inline or you need to predict the target

so the the basis of such prediction could be the opcode or the target address okay you might say that some kind of branches are more likely to be taken and some kind of branches are less likely to be taken 

or it could be based on the target address for example if if you branching to an address which is earlier that is it s a branch back which typically indicates and of a loop um then you may predict that es branch i more likely to be taken 

because loops are often iterated several time um otherwise if it is a forward jump you might say that its um either equally likely or less likely to be taken um if its an exception condition normally you go through 

and the condition checking is to um take care of situations which are occurring um less occasionally and less likely

so depending upon any such feature you can decide 

the dynamic branch prediction um tries to take into account what has happened when the program has been executing 

so there you um keep a record of what happened in the past that if there was a branch instruction um which occurs several times inside a loop then you see that on previous occurrences what happen to this whether condition was true or false

so um um a simple minded um approach there would be to um think that the pattern which was there last time is going to repeat okay

um so so you you could just look at the last one and say that this is what is going to happen next time or a more sophisticated decision would look at last several occurrences and based on that you may decide

um when you look at um branch prediction and um delay slots the then you together you can make the design little more intelligent 
(refer slide time 00:28:39)



by um giving the programmer flexibility of selectively bypassing the delay slots okay so that is called um delayed branch with nullification [noise] or annulment 

so the delay slot um is not something which is fixed by the hardware um it is possible um and the programmer has the choice to use it or not to use it

so so wi with the branch instruction you you could have another field which will indicate that you have to enable the um delay slot or you don t have to enable okay

so now it would depend upon how you are predicting okay depending upon your prediction you may um actually exercise option of um using a delay slot or not using a delay slot 

okay a simple branch prediction which is based on just looking at the last occurrence of the same branch [noise] um may sometime um work poorly whereas some um some very strict forward logic can do better 
(refer slide time 00:29:45)



so just um look look at a simple loop like this that you have a few instruction at that you have instructions which loops back okay

so now um suppose this itself is in a larger loop okay an outer loop where um um every time you come here you do a few iterations and then come out

then you will come again here do few iterations and then come out

so now suppose your strategy is to um predict based on the previous outcome 

so if last time we had last time the condition had become true you c you think that it will be true this time and vice versa

so with every um instance of the loop you will have too mispredictions okay

um that is when you enter the loop first time when you um come to this for the first time um in some bra some loop instance um you will find that last time the condition is false you will think it is false again but now its beginning of a loop so you will iterate 

you might go wrong at this point and um last time when the condition becomes false you might also think loop is going to continue and you will make a mistake 

so twice you will make a mistake um when you are entering the loop and when you are exiting the loop okay

but um on the other hand if you had chosen a a static branch policy here saying that um always you predict that loop um will be taken or the branch will be taken you will make one mistake per loop okay

so apparently um the dynamic prediction strategy does worse than static prediction strategy in this case

so to to make branch dynamic branch prediction um more effective um we we need to modify our dynamic prediction strategy and um one possible scheme which is commonly used is shown here (refer slide time 00:31:54)



so instead of just remembering what happened in the last [noise] branch you try to remember little more 

so um here i am showing a scheme where you have um where you actually um imagine that there is a state machine which can be in one of the four states and these states can be represented by two bits

so instead of remembering one bit information you are remembering effectively two bits of information um what this machine remembers is um some summary of last several outcomes 

and um what what we are trying to do here is that um you don t change your you don t change your decision just by looking at one um one change [noise]

so for example suppose you are in um state zero and you are predicting that branch is taken okay suppose branch keeps on getting taken you remain in that state and you keep on predicting that branch is taken

at some point um branch [noise] doesn t get taken okay so N means branch is not taken T means branch is taken

so um now the these arcs are indicating how your transiting the state okay 

the label in the arc indicates um what was the actual outcome and based on the actual outcome you are going to some next state

um while you are in state zero and one you predict that branch is taken while you are in state two and three you predict that branch is not taken 

so the situation i am trying to um depict here is that um when you are comfortably in the state and continuously branch is being taken and you are predicting that branch is being taken 

if um once the condition becomes false you are not taking the branch you are still in a state where you will continue to predict that branch is taken

un unless you get another end and then you go to a different state okay 

so um in one also you will predict that branch is taken and if it turn out to be true you get back to zero okay

if branch is not taken then you go to state three where you now start predicting that branch is not taken 

so the idea here is that in a situation like what i just depicted with with a single loop um you you will actually continue in state zero or one you will not come to state two and three
so so this is general mechanism which can be used for dynamic branch prediction and um you will avoid making double mistakes when there is a simple loop 

um lastly um we are trying not only to know the decision early either as an anticipation or by actual computation 

we also want target address to be available to us as early as possible 

so if you are keeping the history (refer slide time 00:34:57)



if you are looking at what happened in the past we can also recall what was the target address in um the previous occurrence of the same branch 

and if that is available in a buffer we we can um simply pick up the address from there um rather than calculating

so so actually speaking um the address calculation will still occur just to ensure that what you have picked up from the buffer is same as what you um actually get

and if the two are same it is fine otherwise you will try to undo what was done in anticipation 

so the assumption is that um the 


occurrence of um next occurrence of [noise] same branch

um there are situations where this may not hold for example if you take JR instruction okay

the branch address is coming from a register	and um you cannot be sure that what comes next time will be same but if you if you take um a jump instruction with with a constant address or a beq type of instruction where um the address is obtained by adding a constant to the program counter 

so none of these two things are changing and therefore the address next time is bound to be the same right 

um so keeping in mind that JR will occur much less often than um beq bne and J instructions um this scheme will work 

so here is the picture of how this um buffer could be organized this is one way there are many different organizations 

um it could be organized as um an associative or contrantive decibel memory where you look at the current um instruction address and um each word here has [noise] three fields

one one field carries instruction address um second field carry prediction statistics like um those bits of the finite state machine and the target address 

so you um given the address of the current instruction which is the branch instruction you you try to look up in this table 

so um looking up in this table is that you in in parallel it will search and try to match that address with all the addresses in the first field 

if any match occurs you pick up the prediction statistics and you pick up the target address [noise]

again here there is a variation possible that you may store target address or you may store target instruction directly okay

so instruction to which you are branching um can be directly fetched from there 

so so you go a step further here um the the only thing you need to keep in mind is that if you have target instruction here um the instruction following that um is not available here okay

so for for that you need to calculate the address but it is assumed that it will be calculated in the due course 

so in due course of time instruction branch instruction carries out all that it has to do okay all these things we are doing in anticipation um and we of course have to make sure that what we anticipated or what we predicted or really correct okay 

but al allows us away of doing it um early most of the time 

so so let um typically for example probability of changing of target could be less than five percent

and therefore at least ninety five percent of the time you are doing it fast and you are doing it correctly

now um so far we assumed that we look at um repeated occurrences of same branch 

so so you have branch somewhere in the program and because of loop you are coming to this again and again 

um sometimes there is some coupling or some correlation between various branches (refer slide time 00:38:51)



so for example look at this sequence on on the left you are testing some condition here you are testing some other condition there

then you are um looking at something which is dependent on both these 

so suppose z which is being tested here was x and y right 

so now if you um once you have gone through this and you have gone through this and the value of x and y have not changed in in this um course 

then its um varies it to predict what z may be okay right because um you have evaluated x and y 

if um both are two for example then you can predict that um the branch will be taken here

so B three can be predicted with hundred percent accuracy on the basis of outcome of B one and B two

so um what this means is that there is [noise] there may be often this is a very um simple case but in general what this points out to is that there could be some correlation between various branches [noise]

so if you are looking at not um not just the history of this branch um but global history okay that means history of branches which have recently occurred in time [noise] there may not be um recent occurrences of just the same branch but other branches

so um for example a if you encode um outcome of this branch by zero one outcome of this branch by zero one so so you look at the string of ones and zeros which represent last few branches 

um and looking at that pattern it may be um um possible to predict with much more accuracy the outcome of the branch you have currently at hand [noise]  

so by um going for global history you can improve your prediction of course more and more of these things you do you are incurring cost somewhere okay 

you are putting in more and more control hardware to carry out these things [noise] and also more hardware to undo the effect of wrong decisions [noise] 

so um um finally um one very powerful method of [noise] trying to avoid stalls um whether they are due to data hazards or branch hazards is to do what is called dynamic scheduling (refer slide time 00:41:20)


[noise] so instead of um putting the instruction in the pipeline in the order in which they are occurring you you try to analyze the instruction and push those striking the pipeline which can go through without causing any stalls okay 

so so that is dynamic solutions right um wha what we are doing now for example when we were doing data forwarding we were trying to check the dependency between instruction and trying to take correct action 

on the other hand um si similar mechanism could be built in hardware which will um if one instruction is producing result which is to be used by next instruction the next instruction could be pushed in pipeline a little later 

you you can put something else in the pipeline somehow these things can be done by compiler also um but co compiler doesn t have complete um picture of what is happening dynamically okay

so um dynamic pipeline is um is an expensive thing you you need to have lot of um extra hardware 

but it tries to find instructions which can keep the pipeline busy as far as possible 

if necessary it can um change the order of instruction okay out of order instruction um and a it can nicely support speculative execution and dynamic branch prediction the kind of stuff we have been talking of

this also forms the basis of what is called super scalar architecture 

in um super scalar architecture you you try to um fetch decode and execute several instructions at the same time and um the pipeline there have has to be necessarily a dynamic pipeline

so so where um you you look at a window of instructions in your stream of instructions and um pick up something which can be pushed in the pipeline um 

so so the same idea of dynamic pipeline actually is getting extended to multiple instruction and um this is called instruction level parallelism 

okay so you you are not talking of parallel parallelism in terms of multiple processors doing multiple instructions but same processor is capable of fetching and decoding and executing several instructions at a time okay

so um the the key here is the hardware which looks at um a set of instructions and finds out which instruction can be initiated [noise] um of course the hardware which executes these instruction in parallel also has to make sure that the the results are obtained consistently

um so coming to ILP or Instruction Level Parallelism where there is a a different approach to this also (refer slide time 00:44:06)



that is um we rely entirely on compiler to identify what instructions can be done um together in parallel and um and don t leave this query to the hardware [noise]

so here each instruction could possible be um carrying multiple operations okay

so it its like a long instruction word um whe where you have coded multiple instructions put them together 

so so that s where the term VLIW come into picture very large instruction word okay it it s the meaning of VLIW 

so whether we are having	 a super scalar architecture with dynamic scheduling or a compiler driven VLIW approach um the the basic thing is that you have multiple functional units so multiple ALU s which can handle multiple instructions 

so so that is the basic requirement and um all these have to access the register file 

so register file also must support multiple read write ports um so that all these can actually do in parallel

between VLIW and super scalar the difference will be um the way instructions are um fetched and um pushed in the pipeline	 

so so in the case of VLIW we will expect that um compiler forms um long instructions carrying multiple operations an and the hardware will simply take them one by one and um execute them

on the other hand um super scalar um architecture will have a complicated decode and issue unit which will um look at many instructions which are fetched simultaneously

pick up the right one out of these and then um assign to different functional units to do them in parallel okay

so um here we are um showing that multiple instructions are there but they they are in terms of stream of instructions they are following each other

each instruction as you see in in the program is is like a scalar instruction but um in case of VLIW each instruction is a is a special instruction which has many operational fields (refer slide time 00:46:30)



so if you look at the timings of um these two alternative architecture versus timing of a simple pipeline this is how it will look like 

so um this top picture shows [noise] um a four stage pipeline lets say the four stages are instruction fetch decode execute and write back right 

um then in ideal case you have instructions which are overlapping this manner 

so at any given time you have up to four instructions in flight um a super scalar suppose again in ideal case that um the degree of parallelism is three here

um so in every cycle it is fetching three instructions decoding three instructions executing three instructions doing write back for three instructions 

um BIW on the other hand will look at these as um single instruction being fetched single instruction being decoded but each instruction will actually carry out multiple operation 

so in in the in the same um of the scale it does three execute operations right um so so this is how one could place three architecture in a common perspective 

now um most of the modern processors are actually super scalar processors whether you take Pentium or um power pc or IBM s power four or PA risc and so on

all these high performance desktop computing machines are all super scalar processors

VLIW s are used only in a specialized in varied applications so question is why are super scalar is popular for general purpose computing
(refer slide time 00:48:20)



the main reason there is of binary code compatibility 

so if if you take for example um Intel s um four eighty six processor and then Pentium a super scalar version of the same thing 

the code compatibility exists okay the the code which was available for older machines can be um made to work on the new machines without any um change in the code 

code can directly taken from the old machine and run on the new machine um in the in the same series as long as instruction set is same okay

so the the difference comes only here in the hardware so as far as program or a user is concerned it it just um another machine doing executing the same code faster

architecturally it is different fundamentally okay whereas if you were to achieve speed through VLIW technique you would need to regenerate the code

you have to have a specialized compiler which can detect um parallelism at instruction level pack the instruction appropriately and um then um you can run the code 

so there could be source level compatibility but there no object level compatibility and actually this code compatibility from a commercial point of view okay is the is the very very major issue [noise]

um so um this VLIW um processor require very specialize compiler okay its almost impossible to code them by hand  

whereas um um if you can code normal scalar machine by hand you can code super scalar there it doesn t require anything extra

of course um a compiler which is designed for super scalar processor would um try to take into account some features and produce better code okay

so um if you know that the machine is super scalar um how many functional units it has in parallel and um how it will be better to keep the pipeline busy the compiler can produce better code but even the earlier code which is not generated specially for super scalar will still run correctly okay

um sometime code density in VLIW can be very poor okay you may not find many instructions to be packed together and they could be you have to fill them with nops right

um in terms of um area or the cost super scalars are more expensive okay um in terms of performance um it is possible to achieve a higher degree of performance in VLWI technique provided you are able to have a good compiler

okay finally before i conclude um let me say few words about exception handling

we we discussed exceptional handling in case of multi cycle-design um how how is that placed in case of pipeline design

in fact in pipeline design exception handling becomes more complex um the the main problem is that since there are many instructions in processing you could have many exceptions which could be generated at the same time 

in the same cycle you may find that um one instruction is showing overflow another instruction is um showing um page fault or illegal opcode okay

so an and knowing that um different types of exceptions get um detected in different cycles it is also possible that an instruction which is coming later may show an exception earlier okay

so so for example lets say the there was an instruction um which results in overflow okay overflow will be detected only when you have done the arithmetic operation 

whereas an another instruction which is coming behind it um has wrong opcode okay so immediately you will come to know it may happen that um you you may find um one exception earlier which was actually supposed to occur later in the instruction sequence 

so so this this makes things very difficult and um you have a concept of um precise interrupt or imprecise interrupt 

um in case of precise interrupt means that you detect interrupts or exception in the same model in which instructions are occurring alright
(refer slide time 00:52:48)



so some somehow um you have to um time your detection so that the order in which you detect is not inconsistent with the order in which instructions are occurring okay 

so some some machines insist on precise interrupt um some some do not worry about it and allow imprecise interrupts to happen 

um the saving of status is naturally very complex here because um if you need to handle the exception and come back in resume the instruction you need to know how far the instructions were executed 

and with many instructions in flight um each having done to some stage you you need to remember all that that has happened so that we can continue 

um some time what you do is that instruction which are partially executed um you you may simply flush them rather than trying to remember how far they have done you may simply flush them out and restart them all over again when you come back 

so so there are a lot of possibilities and um we will leave this at this point um realizing that there are many complexities involved in [noise] exceptional handling when there is pipelining 

okay finally to summarize we we looked at the stalls which occurred due to branch hazards (refer slide time 00:54:11)



and um [noise] we saw how we can um flush the instructions which may come up wrongly into the pipeline

we looked at several techniques to improve branch performance including branch elimination branch speed up branch prediction in a static or dynamic manner and um dynamic scheduling

so from dyna dynamic scheduling we it was led to um superscalar and VLIW architecture which are basically instruction level parallel architectures and try to improve performance beyond cpi of one 

so um pipelining um ideally it lies to make cpi one but um in real case it will be slightly worse than one 

um to to cross this um cpi one barrier you need to do instruction level parallelism either in a VLIW manner or in a super scalar manner

i will stop at that

Thank You









Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 3
Instruction Set
Architecture-1


today i will start discussion on instruction set arhitecture as i discussed earlier in the first and second lecture instructions form the interface between hardware and software instructions provide the primitive operations in terms of which computation has to be described and from hardware point of view instructions are the basic behaviour definitions which hardware has to implement (Refer Slide Time 1:38) 

so what we will do today is to take very simple instructions try to understand what action they perform and also where in a program they can be used to do useful things 
so first of all we will look at some instructions for carrying out arithmetic operations simple ones such as addition subtraction then we will see how instructions are used to move data the data is required to be moved for example between memory and registers then we will see how decisions are made in program so how flow of control is defined using instructions and then we will take special care of how to handle constant operands 
there are many situations where constant operands which have to participate in computations 

so these are the simple instructions which are almost used in all situations in almost all programs (Refer Slide Time 2:27) 

so now we will have to carry the distinction between um assembly and machine instructions so we will be talking of um the relationship between machine language and assembly language um machine language is basically the most basic building blocks for any program and it is something which harware can interpret so when you are designing an instruction set when you are designing a new architecture you have to have certain goal certain goal is to um provide a set of operations where computation can be expressed efficiently 
so you need to maximize performance maximize efficiency and at the same time the cost of implementing these instructions hardware should be as little as possible also there could be other goals as power consumption and so on and it should be simple so that design time is as by possible (Refer Slide Time 3:32) 

now when you talk of an instruction set for the purpose of reaching you have to always start with an example um often one takes a toy example a toy machine and describe that but on the other hand we will take real machine um but a simple one in those series which is called MIPS  (Refer Slide Time 3:55) 

this was developed by people at stanford in early eighties 
so this is typical architecture which is come to known as risk reduced instruction set computers um we will talk more about that later but one thing which is important is that from eighties onwards this these are the typical architectures which have been developed so subsequently there have been all developments along these lines and MIPS in particular is used in several applications not all of them are general purpose computing example you can see here are NEC nintendo which is um video games silicon graphics computers and also sony play station 
so it s a real architecture that s plus point we are talking of a toy architecture but it at the same time it s the its fairly simple one to understand and within short period you will get a full grasp over this simple architecture (Refer Slide Time 5:13) 

we begin with arithmetic instructions so the simplest arithmetic operations has been are add and subtract and in simplest form you need to add two numbers to produce the result 
so there are three things to be specified with the instruction which are the operands and where is the destination of the result 
so um suppose in C you have an assignment like A is the sign B plus C sum of B and C then equivalently in MIPS assembly language we will write add here is a register name dollar s zero dollar s one another register dollar s two another register 
so this dollar is just to specify just to signify that it s a register name it s a special symbol s zero and the association between registers like this and the variable names like A B C would be established by compiler so for example compiler comes across a statement like A gets B plus C it would translate this into an add instruction and at the same time figure out various B going to be sitting whereas C going to be sitting and there is A going to be replaced 
so we have taken arbitarily some decision that these are in s zero s one and s two these are some registers of MIPS machine 
now one thing you would notice here is the order in which s zero s one s two are appearing in the instruction is very significant okay unlike the C code where you are saying A assign B plus C by this infix symbols its very clear which are the opponent where is the destination but here it just appearing as a list 
so its clear by conventionally that the first one is the destination and next two are operands so we are adding contents of s one register s two register and putting the result in s zero register 
(Refer Slide Time 7:38) 

so now this is simple and infact simplicity has been one of the design goals for this architecture and simplicity favors regularity what it means that you would try to follow certain uniformity various operations for example if you take subtract instruction this will also follow similar format and also if you think of addition of three numbers or four numbers or five numbers there are no separate instructions for doing so 
so you have to define all those in terms of the same primitive instruction which adds two numbers now the operands in MIPS are thirty two bit numbers and therefore registers are thirty two bit registers um question here would be that why we are limiting why not say that add instruction can add two number but they could be of arbitary sides 
once again you have to limit the size because working on arbitary sides would not necessarily give you the efficiency um if most commonly your numbers can fit within thirty two um then its good to have a limit like this and atleast make sure that addition of thirty two bit numbers is fast can be done in a single step if you want to add larger numbers then occasionally when ever you need that you can go through a sequence of instructions and possibily spend more time but the common case can be done at a fast beat 
so when you have complex expressions such as addition of three numbers or something which involve several operations then it has to be broken down into a sequence 
so now look at these two statements A gets B plus C plus D and another statement which subtracts A from F which is the result of the previous instruction 
so so this could be written as a sequence where you have two additions being done um we assume here that C is value of C is present in s one D is present in s two the result is going to t zero which is a temporary location um it does not correspond to any thing directly in the high level language program 
now the what did i say did i say C and D or B and C it will be B and C which will be added and then t zero contains the sum of B and C and D which is the third operand in the first statement is in s three and the result now of these two is A so s zero is the seat of A and for the second instruction we have to be now careful that it is this value of A which has to be used in the second instruction so you find that s not is there as the second operand in the subtraction operation 
so this this is the restate forward was you know how to do primitive operations anything which can be done by putting these together can be done in straight forward manner 
so now if you have everything in register what is memory for of course memory is there to hold the program but memory also is used to hold bulk data (Refer Slide Time 11:22) 

larger data structures for example um records structures arrays or other complexing you build they have to be in memory which can have much more capacity the the number of registers is limited in MIPS particularly there are thirty two registers 
so so basically you can have only a few scalars which can be mapped to the MIPS registers um but when you are talking of arrays then they have to be kept in memory and which means you need to have instructions to move data between memory and registers 
so that is what we will see next um but before we go for that (Refer Slide Time 12:08) 

we need to see how we get data from specific locations of memory 
so memory you could view as a large one dimensional array consisting of bytes okay so this is again a convention that each addressable unit in the memory is considered to be a byte whereas operands i am talking of are thirty two bits 
so which basically means that mostly you have thirty two bit operations but sometime you may need to look at half the word of thirty two bits or one quarter 
so the addressability is provided at a final resolution that means you can address each individual byte 
so each word will have four bytes and each bytes can be each byte can be addressed 
so addresses are all in terms of byte addresses um so memory address is nothing but an index into this array and this specify the byte number 
















(Refer Slide Time 13:22) 

so with thirty two bits of address you can specify two raised to power thirty two bytes and these are addressed from two raised power thirty two minus one if you are talking of words thirty two bit words each consisting of four bytes um these will have typically addresses zero four eight and so so on that is multiple of four and therefore you have total of two raised power thirty words which go from zero onwards and the last one would be two raised power thirty two minus four um what is the relationship of bytes with word there are two possibilities here these are two different conventions which are called little um well just ignore this spelling error little endian or big endian little endian means that within a word you start numbering bytes from um just a minute i think these these two are reverse this is actually big endian you are starting numbering from the most significant side okay um so on the left side i am showing most significant bit of word and the right side there is least significant bit of the word 
so this is a big endian convention whereas this is a little endian convention 
so you are starting from little end or the LSB least significant bit end um different machines follow different conventions for example um intel processors follow little endian convention spark processor follows big endian convention and when you are going to work in lab with simulator depending upon which machine you are running it on 
you will see different convention 
so there is a simulator in particular adapts the convention of the host machine 
so if you are running on pentiums you will find one convention if you are running on CC seven fifty in computer centre you will find different convention 
okay now this is um as far as when the words are aligned with an address which is multiple of four okay um but there may be also situations where words are not aligned to address which is multiple of four 
so this shows an example at the bottom 
so here we have a word beginning with this byte this is the next byte this is the next byte and this is the fourth byte 
so this is a logical word logically it is beginning at this point ending at this point physically these bytes are grouped as shown here 
so physically in the memory its one word which consists of byte zero one twoand three  another word which consists of byte four five six and seven but your program logically can pick up a word from byte address one 
so what it means that you you are looking at a word consisting of byte one two three and four 
so this logical word is spread over to physical words  (Refer Slide Time 16:41) 

okay now lets come back to instructions which excess memory which allow data to be moved between registers and memory in other direction 
so the two instructions are load and store 
so suppose you want to pick up an element of an array add something to that and store it back okay A is an array and you want eigth element okay lets assume that its an integer array 
so here is the load instruction which is loading something from memory one word from a memory specified by this address into a register t zero 
so here in lw actually l stands for load w stands for word we are loading a word there are instructions for load byte and so on lets not worry about that right now 
so the address is being specified in two parts i am writing a number thirty two and a register s three 
so at this point i am imagining that register s three holds the starting address of the array okay and this offset or the index eight we are saying eight elements of type integer which means that there is a byte of thirty two 
so the sum of these two numbers one number is thirty two another number which is contained in register s three these two together get added and define the address 
so a word from this address is loaded into t zero and next we perform an addition corresponding to this add 
so t zero now contains the data loaded from memory we are assuming that the value h and the result is put in register t zero okay and then next instruction writes this value t zero into address specified by this 
so this is store instruction sw stand for store word the address here for store is same as load 
so basically we have put it in A eight suppose you have to put it in A ten then you would change this constant okay alright now you may be wondering what happens if it is something like A i so we will worry about that later 
but i have described a simple situation where you can have constant indices into an array there could be other complex situation for example the index of array could be a complex expression itself 
so we will look at that later but in very simple situation we have seen how we can get data from memory perform arithmetic put the result back in memory 
(Refer Slide Time 19:56) 

so now another example which tries to be little more there is no arithmetic involved here its simply a matter of moving data what we are trying to do is trying to interchange two elements of an array okay and here you will see an example of a variable index okay 
so its written in the form of a small function which is taking an array and an index in as an argument as two arguments um it uses a temporary variable to interchange this two okay it s a standard thing you do in programming for interchanging two variables or two elements of some structural view go through a third temporary and cycle them like this okay now this will involve basically its clear that you require two loads where you will get this two v k and v k plus one and two store where you put values back in v k and v k plus one 
so um the first two instructions are actually preparing to get the right address 
so this is a another instruction which i have not talked it s a multiply instruction 
so its trying to multiply contents of register five with four um i am using four as a constant here i am not saying register four its not dollar four its four 
i will elaborate on these constant operands little later but here just um interpret this as multiply instruction takes one value in five which is in this case um k okay and it will multiply by four now this multiplication is being done because k indexing into an integer array right and for a byte offset we require four times that 
so what we have in register two as a result of this multiplication is four times k um that is added to um starting address of array v which is contained in four to begin with and the final address is prepared in register two okay 
so now you can load from this memory address which is zero offset zero offset and the variable part is in register two okay so because we have done entire address calculation by these two instructions um the complete address is in register zero sorry register two and offset is zero 
so load a word from here into register fifteen the next word you pick up from a same address with four offset 
so basically you will pick up v k plus one okay so this is effectively loading v k in register fifteen this loads second load load v k plus one into register sixteen and all that we are doing now while storing we just store in opposite order okay that is sixteen is stored with zero offset and fifteen is stored with four offset 
so the two get interchanged so essentially we have not explicitly used temporary or other way of looking at this is we have used two temporaries okay we load it both and then put in reverse order um ignore this for the moment this is basically there because this is written as a function and this will this is like a return okay again we will talk of this later 
now (Refer Slide Time 24:08) 

all along i was talking of instructions written in symbolic form 
so basically i was writing not quite in machine language i was writing in assembly language um how do machine machine instruction or machine language looks like the instructions in binary form are thirty two bit long okay so again there is uniformity the data is typically or most commonly thirty two bit long and the instructions also thirty two bit long 
now within an instruction we for example if you take add instruction we need to say that the operation is add operation and they are three registers involved registers themselves are numbered zero to thirty one because we have total of thirty two registers 
so for example registers t zero t one etcetera are numbered eight nine and so on registers s zero s one etcetera numbered sixteen seventeen so on 
so you might be wondering about the registers before eight or after twenty three we will again talk of that later but as we have seen in previous examples we have been using this registers labelled as t zero t one and so on or sometime we also use register directly by number  (Refer Slide Time 19:56) okay register two register five register fifteen and so on that s one way you can address other way you can have this in symbolic form t zero t one t two or s zero s one s two 
so now taking this example add dollar t zero dollar s one dollar s two how do we represent this in binary form 
so a thirty two bit word is divided into several fields each field specifies a different part of the instruction 
so in this case we have six fields okay this is a six bit field six five five five five and six the two at the end are six bit fields and all in the middle are five bit fields the first bit is called the op code field or op in short this specifies which operation is being done which operation the instruction is asking the hardware to do 
so the code op code for add is all zeros subtract will have a different code multiply will have different code divide will have different code 
so this signifies um what what the instruction is about um but this being only six bits it will actually limit the total number of instructions to should i limit to two raised power six or sixty four but actually um a real machine requires more than that 
so an extension of this field is actually the last field which is called function field okay 
so this part the op part will be same for a group of instructions and its only the fuction field which will distinguish them
so infact subtract instruction will also have the same pattern here contrary to what i said um many of the instruction add subtract and a few several others will have this part same and it is this part which will distinguish them okay now out of these um four fields five bits each three are used for specifying three registers 
so that s the reason why these are five bit field because registers are five bit numbers numbering from zero to thirty one 
so in this particular instruction this filed is unused um what i have written as name for this shamt which is short for shift amount 
so there are some shift instructions where there will be something here but in this instruction its all zeros okay and now these three register fields correspond to the three registers in the instruction but not in that order the the what appears first in the symbolic form is the destination and that is here rd rd stands for register destination and rs stands for register source 
so s one is here okay this is the code for s one which you see seventeen and this is the binary code for seventeen t zero is numbered eight 
so this is the binary code for eight is zero one zero zero zero and rt stands for register third which is this s two which will have as you can see from this it will have code eighteen which is one zero zero one zero 
so now this string of ones and zeros is a thirty two bit number which defines this instruction 
so add t zero s one s two is this number as far as the machine is concerned 
so now rather than writing a long string of ones and zeros you can also write in compact form in hexa decimal form okay going further to load and store instructions 
(Refer Slide Time 29:25) 

we now need to deviate from the format for these instructions load store in machine representation we cannot follow same form uniform uniformly and therefore we need to make a deviation from the idea of regularity that all instruction would be of similar nature 
so again we we need to realize that good design demands a compromise so we cannot um be very rigid about this regularity 
so here is a new format which is called I format the one which i talked earlier was called R format R stands for register I is for immediate immediate as i will describe later is actually a term used to specify constants 
so lets look at this format it has less number of fields basically four fields only one field is six bits next two are five bits and then there is a sixteen bit field
so how is a instruction like load t zero thirty two dollar s two expressed in this format 
once again this is an op code okay so this here we don t have that function field 
so it is only those six bits which have to be used for defining what the operation is um this is the rs field so corresponds to s two eighteen so i am not writing in binary now i will just put decimal equivalents 
so you have binary code of eighteen line here in this field corresponding to s two and well i think this is a this should have been eight t zero is corresponding to eight and the offset thirty two is put here right 
so now basically you would notice here that this constant part which is coming in load instruction has to be a number which is a sixteen bit number it has its own limit 
right these were arithmetic instructions and instructions for moving data 

now next we go to instruction which define flow of control which allow you to take decisions (Refer Slide Time 32:00) 

we have two simple instructions in this called bne and beq bne stands for branch if not equal and beq stands for branch if equal to illustrate this lets look a look at an example of a simple if statement in c if you are saying if i compared with j they are equal then you perform this addition h gets i plus j okay so we will make this comparison although it is appearing as equality comparison what we are actually trying to do is that if they are not equal we are skipping this 
so that that is how we will interpret this in machine language or symbolic assembly language we are saying if s zero s one are not equal then branch two a statement with this label okay 
so statements can have or the instructions can have label which are arbitary in names 
so this instructions allowing you to skip the following instruction if the equality doesn t hold and h equal to i plus j is simply add s three s zero s one so i and j are in s zero and s one and h is going into s three 
so whatever instruction follows here will be tagged with this label 
so this label is linked to that branch statement okay now apart from conditional branches which are testing condition we also have unconditional branch which is even simpler j is the instruction symbol j stands for jump and a label 
so we are not specifying any operand for comparison here you simply say unconditional jump we need this in a in situation for example if then else if you are saying i not equal to j you do this else you do that okay 
so here if this condition doesn t hold we are checking it with beq if four and five register s four and s five are not equal then we will do this if they are equal we are branching to label lab one okay lab one is tagged with this instruction subtract which is subtracting s five from s four putting the result in s three and this instruction subtracts s five from s four puts the result this adds s five to s four puts the result in s three 

so after this instruction addition we don t want to do subtract okay so to skip this we need an unconditional jump here jump to lab two this label is label of the instruction after subtract okay 
so the after this instruction the control flows either like this to lab one and then to lab two or goes to add and then jumps to lab two 
so lab two is a common point which would be in c language statement after this here we are either doing this or doing this so this flow of control is organised by one branch which is conditional and one jump which is unconditional 
now i talked so far about comparison for equality equality or inequality but you often need to do a less than or greater than type of comparison how do we carry that out for that there is an instruction called slt which stands for set if less than okay slt stands for set if less than (Refer Slide Time 36:16) 

so you set a register to value one or zero depending upon comparison 
so this instruction for example slt dollar t zero dollar s one dollar s two has t zero as destination where you will set value to zero or one and it compares s one and s two 
so if s one is less than s two then you say t zero to one otherwise you set t zero to zero 
so its equivalent definition is this if sone less than s two then t zero equal to one else t zero equal to zero all right 
now this is not a branch instruction it does comparison but the result is available in a register so it does not alter the flow of control you need to combine this with beq or bna to achieve an effect of something like lets say blt suppose you want to say branch if less than branch if less than s one s two to certain label okay in the spirit as beq we said compare two register for equality and branch if the condition holds suppose the comparison is less than and we want to branch you will have to combine a comparison like this and then um combine this with a branch instruction or beq or bne types basically you will make compare make comparison result is in a register then you check if the register is equal to zero or one okay 
so we will probably discuss that in i told you how to do that 
now (Refer Slide Time 38:05) 

after having introduced this jump instruction lets look at formats of all the instructions we have introduced for decision making and flow of control 
so bne and beq follow the I format um this is an op code two register which is specified and a sixteen bit number which defines the label okay the purpose of the sixteen bit field is different here in load store it was an offset in the address some where you have to pick the data here again in some sense you are trying to define an address but address for instruction okay 
so in in assembly form symbolic form um we have written a label but eventually it actually has to be an address 
so the number which appears here is essentially an offset um of the destination address from the current instruction um i will elaborate on the exact thing of this later um but just remember that this is an offset which you are specifying to define the destination where jump has to be carried out in the in the jump instruction there is nothing else except for op code and remaining twenty six bits are for specifying the jump address slt instruction on the other hand goes back to R format where you have op code three register fields shift amount field and function field okay once again shift amount is not being used op and function together define slt instruction and they are three register fields 
okay now finally (Refer Slide Time 39:50) 

lets move to handling of constants 
so you have lot of cases when you have to deal with constants in your computation 
so for example if you say A gets A plus five B gets B plus one or C gets C minus eighteen how do we do these things there are various ways it can be done and different processors follow different approach you can have these constants put as some data in the memory and when a program is loaded in the memory this constant data could also be intialized that could also be loaded the other approach is um that you can put the constants as part of the instructions 
so both possibilities exist in this and we look at the later one at the moment 
(Refer Slide Time 40:37) 
here i am putting same instructions um but one of the operands and it is the last one is written as a constant instead of a register number and to distinguish it from normal add there is a suffix i which stands for add immediate now the reason why we call it immediate is that the operand is immediately available in the instruction itself  it does not have to be brought from register or memory location 
so that is the sense in which the term immediate is used so you have add immediate you have slt immediate and the instruction and and or for logical operation which i have not described the meaning is obvious and immediate or immediate so on and these instructions also follow I format this is the op code two registers are being specified and we have a sixteen bit for a constant 
so although we deviated from R format to get this constant but you notice that most of these constants are sixteen bit number what if you use you need larger constants will come to that but there is a special constant zero which is (Refer Slide Time 41:57) 

hardwared to register zero 
so register zero in among register zero to thirty one is a special register whose contents remain zero
so you cannot by a program suppose you write an add instruction with register zero as the destination um so add instruction will just leave it unchanged it will not do it will perform add operation but the result will be thrown away 
so for example if you say add if you are using zero as a source or as an operand if you say add s two s four zero it means you are simply moving data from this to this okay there is there is no separate move instruction um add instruction can be equally while used for moving by virtue of this constant zero 
so it can be written as dollar zero or dollar zero 
okay now coming back to the large constants (Refer Slide Time 42:57) 

which cannot be contained in sixteen bits so we should be able to work with thirty two bit constants if need b um now since instructions deal with sixteen bit constant rate time one one could have designed for larger design one could go for eighteen twenty twenty two and so on some processors do that but you cannot do you cant have the single instruction handling thirty two bit constant because there will be op code okay um you have total thirty two bits and you cant devote all the bit for one operand 
so we try to do this with two instructions one instruction is in the left half left sixteen another instruction fills in the right half right sixteen bits and we do it by  
(Refer Slide Time 43:50) 

using a special extraction called lui which stands for load upper immediate okay lui upper means it loads upper part of register left half of register and i because its having constant 
so lui dollar t zero and this constant sixteen bit constant this instruction will bring this constant into upper half or left half of a register t zero in this case then we bring in another instruction ori or immediate um same register and another constant which corresponds to the lower half 
so now this instruction will or the contents of register t zero and this constant put the result in t zero so what whats happening is shown here after the first instruction is executed register t zero has that constant in the left half the right half is all zeros the second instruction is taking this constant and this in the right half left half zero the two are odd which means the two are actually put together in this form and the sequence is able to load a constant whose first part is this second part is that 
okay so now  (Refer Slide Time 45:19) 

the idea here is that if you have a small constant you can work with single instruction which will be the most common case but as and when you need larger constants you can spend more time you can use two instructions and do the job 
so to end let us summarize all the instructions that we have learnt and what is the format used for their binary or machine representation 
we have arithmetic instructions in two flavors um with register operand and with constant operands 
so one operand of course a register here also add and subtract add immediate and subtract immediate add and subtract follow R format add immediate subtract immediate follow I format okay then we have logical instruction and and or and their immediate counterpart once again the formats are R for and or I for and immediate or immediate slt also has an immediate version okay the formats are R and I beq bne are two branch instructions which we have discussed the format is I j is unconditional branch instruction called jump the format is j 
so that s the only odd man out here load word store word also form um I format lui load upper immediate is also I format 
so these are the instructions we have learned there is little bit you can do with these you would often need more instructions which we will see in the next class if you have any questions i can take up now  
okay question is what is full form of bne bne is branch if not equal beq is branch if equal 
yes [noise] um okay this is from hardware point of view if you are working with less number of registers smaller in size adding smaller numbers the circuitory which can do that is faster okay the the hardware for adding two thirty two bit numbers is faster than hardware for adding two sixty four bit numbers why it is so we will see later in the course yes please 
[student : if you entererd swap once again ] 
okay swap instruction swap operation yeah for swap (Refer Slide Time 19:56) basically the key part is here these two load instructions these two store instructions 
so we are reading two words from memory v k and v k plus one into two registers fifteen and sixteen and writing in the opposite order okay so they get swapped what remains is um preparing the addresses of memory from where you are going to read 
so before that we actually saw very simple case where the index of the array is constant okay that constant can be placed here (Refer Slide Time 16:41) okay in the I format this goes in that sixteen bit field and a register which contains the starting address of A okay A is an array where compiler would put some where in memory okay the starting address is we are assuming that before this some where we have ensured that the starting address is in s three similarly here we are assuming that the starting address of array v is in register four and index k is in register five 
so now as far as integers are concerned they occupy one word but our addressing is byte 
so we want to take starting address of v and add four times k to that (Refer Slide Time 19:56) so first multiply instruction is actually preparing four times k this contains five this contains um value of k multiplied by four that gets into two 
so register two and four are added to again bring them to two and that s being used as address here 
so for v k two has complete address you need zero offset for v k plus one you need additional four offset because v k plus means it is four byte ahead after v k is it clear any other question [noise] okay 
so or instruction (Refer Slide Time 43:50) basically takes two thirty two bit words and performs or operation bit by bit here we are using ori that is immediate form of or it takes one register and one constant okay the constant we are specifying is only sixteen bits but actual operation which is performed is a thirty two bit operation 
so remaining sixteen positions are filled with zeros 
so when all operation is being done we are taking this this number first number is contained in t zero second number is obtained by prefixing sixteen zeros to this constant which is part of the instruction and the instruction ors this two this is the result which will go to t zero again which is the destination register here 
okay so this is the way we have used to put two parts of a large constant together to form a thirty two bit constant in a register 
any other question okay we stop at this thank you 





Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 9

Performance


When we see several computers around in the market the natural question which comes up is which is better and which is worse particularly in terms of performance which performs better and which performs poorly also as a designer if you look deeper into how instructions are organized and how computer is designed the question is that with so many different alternatives which alternative is better from performance point of view 
so in this lecture and next one we are going to look at the issue of performance we will define what exactly the term means and what is the relationship between performance and architectural choices 
so the issue of performance the question can be asked (Refer Slide Time 1:38) 

from a users perspective okay user may like to know about performance of the computer which are available and designer on the other hand may try to see from the point of view of design alternatives what is the best 
so for both these perspectives what you need is a basis for comparison how do you compare one versus other one computer with the other computer or one deign choice with other design choice and this has to be a quantitative metric 
so you should be able to say that A is two times better than B or three times better than C and so on 
so quantification is a must and therefore a precise definition is required 
so although initially i will talk of users perpective as well as designers perspective but particularly what is relevant for this course is that we try to understand the relationship between performance and the architecture (Refer Slide Time 2:54) 

so from users perspective or or a purchasers perpesctive suppose you want to buy a computer system for your lab or your lab organisation or for yourself as a personal computer you would find that the there is a lot of variety okay they are different manufacturers different vendors and for each vendor you will have a set of choices okay there is a range of machines 
so the question you would like to ask his how do they compare performance wise across the vendors or within the same machine vendor and what is the cost implication because possibily as it is intuitive you may not get performance without any cost and which indeed is so that performance and cost may often be having some trade off 
so you put in more cost you can get better performance or you want to save money you will have to settle with the lower performance 
so is there a way of putting these two together you may like to ask the question of what is the best performance for a given rupee okay what is the performance price ratio okay 
so which alternative gives you best performance for the same size 
so there are different ways you can pose this question on the other hand from designers perspective we have seen that first of all there are so many different types of instruction sets okay we have talked about some major styles load store style or memory memory style stack style okay accumulator type of machine 
so there are different philosophies of designing and instruction set and also something which we have not studied given given an instruction set there may be different many different ways in which you can build the hardware 
okay so you can do the instructions in such a manner you can execute them in a manner that they take less time or more time and there are implications of those choices on cost and performance 
so once again (Refer Slide Time 5:11) 

among the different options which you are considering at any stage either at instruction design stage or at hardware stage what gives best performance okay and what is the cost implication 
so you can similarly ask the question of performance for a given price you may fix the price and see what gives you best performance or for a given performance which is a cheapest option 
so you could work it out different ways so the study of performance has several aspects one is the question of of course you need to define what  (Refer Slide Time 5:52) 

performance means in very quantitative and precised terms and there should be a way of measuring the performance okay it should be a measurable quantity 
so how to measure it how to report it how to summarize it because measurement may involve may be several experiments and can you come up with a summary number which summarizes all that you do as an experiment and based on all this how to make good choices okay so it may not as i mentioned that there are two issues involved performance and cost 
so you have to make a judgement which is the best alternative you might see that people who are trying to market a system may try to project certain things okay so there may be hype about this machine does this does this and this is a performance number but you have to see does this match with your definition and your definition may possibily take into account your requirements 
okay so so you have to have an evaluation or definition which actually reflects the way performance is going to affect you particularly 
so this understanding of performance method of measurement method of summarizing method of comparing is isimportant from the point of view of understanding various design choices which you will study as you go along 
okay so we would like to understand why certain piece of hardware or certain design choice performs better (Refer Slide Time 7:47) 

than the other and we will also see that it may depend upon the program it may happen that A performs better than B for a given program but for another program 
so for example you might find that when you are doing word processing A is better than B when you do emailing B is better than A okay so those things may happen and which factors influencing the performance are related to hardware design okay or which performance factors are related to architectural issues there may be factors which are also beyond this beyond the processor design in particular 
so we would like to get some idea of that also um and what is the influence of instruction set on the performance okay including or not including certain instruction does it have influence on performance or instruction style has some influence on performance 
so all these questions are there and we we should develop certain understanding of these questions 
so to bring some basic points into attention let me take an example from a different domain okay we are talking of although computer performance but the issue of  performance could be seen in daily life in many different context and you would see that there are parallels between what we see in this problem and what we would see in computers 
so here is a set  (Refer Slide Time 9:36)  

some data about a set of aircrafts shows a set of aircrafts which for example an airline may be debating which one we should buy okay and the choice may have to be declared by some kind of perfomance comparison 
so the first three columns gives certain data okay ignore the last two columns for the moment the first second column says what is the speed of the aircraft okay expressed in miles per hour here um what is the range in terms of miles for which the aircraft can go before refueling okay the third is the carrying capacity in terms of number of passengers if you look at these three parameters you would find that um its not a very simple matter of telling which has the best performance if you simply go by speed concorde has the maximum speed all right if your target was speed then concorde is the plane we must buy okay if you want to look at for example the range you have you are interested in long flights without any stoppage in between okay suppose you want to connect two airports which are eight thousand miles apart then your choice is DC eight fifty all right others cannot give you a non stop flight for that distance then on the other hand if you are talking of passenger capacity okay you want to carry lets say four fifty passengers at a time then your choice is boeing seven four seven right others will have to make two trips to do the same thing um so this is this is just to indicate they are depending upon how you fix your targets what is your area of interest what is what is that you want to achieve whats your application is you might find that the the performance would be different okay with one given criteria A could perform better with other given criteria B could perform better 
so in this particular column of example you are seeing the time taken in hours to connect two points lets say across the atlantic um which are say about four thousand miles apart then the number of hours you would take is given here 
so this is another measure which is nothing but inversely proportional to the speed 
okay so again concorde is the best one here you could also look at something like throughput okay number of passengers carried into miles per hour 
so it s a it s  kind of composite measure of the carrying capacity and the speed 
so if you multiply these two the figures you get are shown here and from this point of view boeing seven four seven is is the best okay it s a combination of capacity and and the speed (Refer Slide Time 13:05) 

okay so another thing which this for example brings to our notice is that there are multiple criteria okay multiple machines of performance particularly from context of computers there are two notions which we would be talking of one is the time okay so in aircraft context it is the travel time um so for computer time would mean response time or execution time or latency 
so for example to be more precise you may ask a question how long does it take for my job to run so moment i say run it takes certain amount of time um that s what is of concern to me 
so other question could be how long does it actually take to run how much time processor has actually taken to run okay the first one is the time i see on the wall clock okay the second one would be the time machine has taken to execute the program third would be for example in an interactive environment lets say database query or ATM type of environment where you you give a command or you make a request and you get a response 
so what is the response time in different context you may like to ask questions you may like to word it differently but you are talking of basically time in seconds or minutes or hours what ever it is the other type of measure is called throughput 
so again it say generic term where we are trying to talk of the over all work which is being carried out the rate at which work is being carried out number of tasks per unit time okay how many jobs can the machine run at once what is the average execution rate how much work is getting done 
so you as an individual user may like look at time as an important factor but let lets put ourselves in the shoes of computer system manager okay whose catering to a user community and the concern there would be how many user programs are being run every hour okay um it is secondary that a user may have to wait or user may get immediate response 
so each individual user may see his or her own response time but as a manager of a service one would like to see how many program are being executed by a collection of computers or a single computer as the case is per hour or per day or what ever the needed time you choose to be (Refer Slide Time 16:02) 

so this these are two measures which sometime may go together okay or sometime they may contrast with each other right you you may for example find that in order to get high throughput some waiting times are introduced 
so some people have to wait and while others may get quick access but some may have to wait and may be different people have to wait at different times but on the whole your attempt to keep the machine busy okay so that may maximize throughput but as an individual you may like immediate attention and you may like to get complete grasp of the resource 
so it may not necessarily lead to better throughput um but of course there are suppose you have you replace all computers by better computers it may improve response time it may also improve throughput 
so some some changes or some policies or some design choices may help both whereas others may help one at the cost of the other so one has to keep in mind what is objective 
so the question may be when is throughput more important than execution time and vice versa okay so it depends upon perhas the person who is asking the question 
so (Refer Slide Time 17:33) 

now try to define performance we will try to focus on the time aspect okay and not the throughput aspect we will predominantly talk of the individual concern of response time or execution time 
so we want to define performance in a manner that a bigger number a larger number represents better performance 
so so we could define it as reciprocal of execution time right so we say that performance is one upon execution time 
so performance of lets say machine x is one upon execution time for x so you take a machine you take a program see how long it takes measure it and then say reciprocal of that is your performance number right so it s a very simple and straight forward definition (Refer Slide Time 18:32) 

and this can be used to also talk of relative performance 
so you want to say x is n times faster than y okay or x is n times better than y in terms of performance then basically what you are saying n is the ratio of two performance numbers performance of x so performance of y and in terms of execution time it is reciprocal of this ratio which is execution time of y power execution time of x 
so you have you take same program run over two machines A and B measure the time and take the ratio so that gives you relative performance 
so for example suppose machine A runs the program in twenty seconds and machine B runs the same program in twenty five seconds 
so how many times A is faster than B or how many times B is faster than A 
so it s the ratio of these two number A is twenty five by twenty times faster than B which is five upon four or twenty five percent faster okay or you could say that B is  point eight  times faster than A which means effectively you are saying it is slower than A 
okay now  (Refer Slide Time 19:55) 

coming to the aircraft example or airline example um you could compare the time time of concorde versus boeing seven four seven right you could take the ratio of speeds on way or ratio of the travel times in the other way 
so you could say that concorde is one three five zero mph divided by six ten mph 
so you take this ratio which means  two point two  times faster or in another words it is one twenty percent times faster than boeing alternatively you could take the ratio of the travel times 
so  six point five  hours divided by three hours so now you have taken the reciprocal ratio which will again of obviously give you the same figure um one could also have compared throughput okay suppose we define throughput as we did in that chart as persons or passengers carried multiplied by miles per hour 
so you take the ratio of this pmph figure passenger capacity multiplied by speed and so this is for concorde this is for boeing 
so concorde is  point six two  times faster or alternatively you could say boeing is so many times or  one point six  or sixty percent time faster than concorde 
so it is now faster in in throughput sense all right whereas the first comparison was faster in speed sense or travel time sense 
so we will be focussing on the first one of these that is the time okay because that is what is that of concern to an individual and the current discussion which will have on architecture would be more closely linked with that  
so when you understand the question of our performance you you may like to ask understand and ask questions like this that in order to improve performance you may consider a change like (Refer Slide Time 22:25) 

upgrading a machine with a new processor with a faster clock what will it improve when you you have a machine you take the processor replace it with it um faster processor okay i mean lets say pentium four  two point eight  giga hertz is replaced by pentium four with  three point zero  giga hertz 
so what change is the the throughput change is or the response time change is both will both will improve okay increasing the number of jobs suppose you you have a system which is which is running a sequence of jobs all right 
so imagine that people come with a programs run it and go away 
so increasing the number of jobs what will it improve response time or throughput it will always improve throughput 
so as a policy if you start taking multiple jobs trying to run them in time scheduled manner it would improve the throughput infact it can slightly reduce the response time because of there may be some overhead of switching from one to other okay overhead of doing time sharing may be there 
on the other hand if you knew that there is no time sharing processor has to just take one job as it comes and finish it then you are not incurring some overhead which otherwise you are incurring 
so response time actually could deteriorate okay suppose you are in a lab environment where there are couple of machines five machines are lying there  if you add a couple of more machines what does improve response time or throughput certainly throughput will improve more jobs can be done there could be if you are counting from the time you are on the machine of course the response time doesn t change 
but if you if you look at the total time you spend from the time you step in the lab and you go out with your work done that may improve because with less machines and more students trying to do it you may have to wait in front of the machine okay so so something will improve here in terms of time as well if you are talking of total time you have to spent in the lab that could improve but the time which processor takes to execute your job will not if you have five machines or ten machines or fifty machines of the same type 
if you lets say in an network of ATM machine if you increase the number of processors okay then suppose there are number of network of ATMmachines which are controlled by certain number of processors if you increase the number of processors which are working at the back end for supporting this ATM machines what will happen will you improve throughput or will you improve response time it will improve both okay it will improve throughput right more machines are there to do more jobs 
but again some what in the similar environment to the lab you are waiting waitin time may get cut down some where right because you may fire a transcaction on ATM but processor is busy so it waits for a time waits for a while but if there were other processors take care of your transaction it could respond fast 
so continuing with this such kind of practical question now suppose you have two processors two machines from different vendors the pentium three and power PC if one takes eight seconds and the other takes ten seconds  (Refer Slide Time 26:47) 

its clear from what we have discussed which one is faster okay but could they be reasons to buy the one which takes longer yeah there could be still reasons it may be costing you less okay so you have to see performance not in isolation but at what price you are getting what performance 
so the cost factor cannot be all together ignored um in lets say again going back to the aircraft situation the the concorde would be faster okay if you are concerned about the travel time but then the ticket may be more expensive 
okay so if you have a budget limiting budget then you have to make choice accordingly so it may be you task may be to get best performance for a given cost or or if your target is a particular performance you like to see with what minimum cost you can get the same performance 
okay so now as i mentioned we will focus on time aspects  (Refer Slide Time 28:10) 

so and not so much on the throughput aspect but even time issue could be fairly complicated depending upon how you view it 
so you could have response time which is a total time taken by CPU to do the job okay plus time for example disk might take to access files um the time a program may be waiting for okay it may wait for some input output to happen or it may wait for some of the tasks okay in a multi tasking environment there may be wait involved 
so the time which you see ultimately is sum of all these okay on the other hand the CPU execution time um would be the time which CPU actually spends doing your program 
so it will exclude the disk access time it will exclude the waiting time and this itself will be consisting of the time which has been spent actually executing your instructions 
because there will be a component of OS instruction there is OS overhead um OS is doing some service for your program okay it loads your program it takes care of IO it takes care of communication and so some overhead which OS incurrs is attributable to your program so so strictly speaking the total CPU execution time is sum of these two the the time spent on user code and the time spent on OS code and that part of OS code which is executed to serve your program 
so from them many factors which influence these okay we are not talking of OS design um we are not talking of so much of peripheral design at the moment if you if you are concerned with the processor architecture okay then what this will influence is essentially user CPU time and also of course to some extent OS time okay 
but OS time will also be depending upon what kind of OS you are writing what is OS scheduling policy and so on 
so our immediate concern would be the user CPU time that means you have user program and how long a CPU will take how long you will give an architecture will take to run that program (Refer Slide Time 30:51) 

so as an example to clarify these points let us say a CPU time for a given program is twenty two seconds with sorry the OS component time of the CPU time is twenty two seconds user component is ninety seconds 
so the total CPU execution time could be one hundred twelve seconds which is some of these two and with memory time or this time and waiting time all put together the time could be total one sixty two seconds okay which is one twelve plus some other time which is fifty seconds in this case (Refer Slide Time 31:41) 

now having said that we we are looking at the time CPU spent in executing a user program 
okay so we like to break it down further and try to express in terms of the clock periods okay as you know that all processors run with a clock okay when you say a processor is running at two giga hertz that means there is a its running with a periodic signal at the frequency of two giga hertz um or half ano seconds cycle time 
so this two giga hertz is the rate at which events takes place within the processor 
so any hardware activity in the processor will take place at discrete edges at which clock changes state 
so the clock cycle time or the clock period is the reciprocal of the clock frequency 
so for example if clock frequency is two hundred mega hertz the cycle time or the clock period is five nano seconds (Refer Slide Time 32:46) 

so relating the time to execute to cycle time we could say that the CPU time is CPU clock cycles into clock cycle time 
so suppose your cycle time is one nano seconds and you are executing one million cycles for doing something then one million into one nano seconds is how much one milli second 
so CPU will spend one milli seconds of time alternatively we could say that CPU time is CPU clock cycles divivded by clock frequency or clock rate okay same thing because clock rate and cycle time are reciprocals um it can be further rewritten as CPU clock cycles depend upon how many instructions are there in a program and how many clock cycles are taken per instruction 
okay so the product of instruction count and CPI stands for cycles per instruction if you know these two figures you can multiply these two to get the idea of the number of cycles which CPU takes to run a program 
so with that we can write CPU time equal to instruction count multiplied by CPI multiplied by clock cycle time or clock period alternatively CPU time equal to instruction count multiplied by CPI divided by clock rate or clock frequency 
so units of thee quantities can be very easily seen in this equation CPU time let us say we are talking of seconds 
so second CPU time seconds equal to instruction count which is instructions per program CPI is cycles per instruction and clock rate is seconds per clock cycle 
so you could see dimensionally how this is balanced okay instructions cancel with instruction um cycles get cancel with cycles 
so what you get is basically this should be seconds per program okay on the left hand side seconds required to execute a program or seconds per program 
okay so lets illustrate this with an example suppose you have a processor with clock rate of fifty mega hertz how do you find execution time for a program which has one thousand instructions and given that CPI for the program is  three point five  okay now you might wonder when i am talking of cycles everything happening in cycles why i am  why am i talking of the fraction here (Refer Slide Time 35:54) 

so it is a fraction because we are talking of average and i will elaborate on that little later 
so lets take this value and try to find CPU time in terms of these instruction count multiplied by CPI divided by the clock rate was the formula we just saw 
so substituting the value the instruction count is thousand CPI is  three point five  and clock rate is fifty mega hertz so fifty into ten raised power six 
so you can multiply this and get seventy into ten raised power minus six seconds or seventy macro seconds okay 
so now suppose we have situation where everything else remains same and clock frequency increases okay i asked this kind of question earlier 
so suppose in some case clock changes from two hundred mega hertz to two fifty mega hertz other factors remain same so how would the time change 
so obviously time which is depended upon three factors others remaining same when you take the ratio of old time and new time others will cancel out and basically you will get the ratio of inverse ratio of the clock rates or direct ratio of clock periods  
so two fifty mega hertz by two hundred mega hertz or  one point two five  
so old time and new time are related with this ratio or that is new time old time is  one point two five  times the new time 
now coming back to the point of fractional CPI (Refer Slide Time 37:33) 

as i mentioned that CPI as we have put there is average because the time taken the number of cycles taken by different instructions may be different okay and the reason for that would come from the way we implement the hardware okay you may notice that hardware takes longer for some instructions and shorter for other instructions 
so given that we need to find an average so CPI is a essentially a waited average of CPIs of individual instructions suppose there are n different instructions or instruction types and we know CPI of each individual instruction and what we need is a weightage Fi which is how frequently this instruction is occuring in a given program 
so as an example suppose we have these five instructions rather they are five categories of instructions arithmetic instructions which are present fifty percent of the time in a program that means half the instruction in a program are add subtract multiply divide of that category twenty percent of instructions are load ten percent are store twenty percent branches and this of course totals up two hundred the CPI is different for these let us say for ALU instruction CPI is one ALU stands for arithmetic logic unit instruction which do arithmetical or logical operation they take one cycle load takes five cycles store takes three cycles branch takes two cycles and then weighted average can be obtained by this formula 
so we find CPI into Fi instead of percentage i am taking fraction while computing this 
so one into  point five  this is five into  point two  this is three into  point one  and this is two into  point two  
so now you can sum these and you get  two point two  so on the average an instruction spends  two point two cycles  for its execution 
so interestingly you can also find out what is the fraction of time CPU would spend doing ALU instructions what is the fraction of time CPU will spend doing load instrution and so on     
so that also could be found out from this data and is shown here how will you find this what formula have i used can you figure out 
[ student: the CPI into Fi divided by total CPI ]
yeah so basically if if you take this as a total okay what fraction  point five  is of  two point two  
so  point five  is twenty three percent of  two point two  this is what i am saying  one point zero  is forty five percent of  two point two   point three  is fourteen percent of  two point two  and so on 
okay so it gives you an idea of where this processor spends time in this case its very clear that such a processor will spend maximum out of time doing loads and if you were to figure out how to write a program in an efficient manner you you will try to you will keep your attention on minimizing the loads 
okay so here is some explaination of why some instructions take more time or what kind of instructions take longer and what kind of instructions take shorter 
so for example typically although in this example we group all ALU instructions together but there would be many situations where multiplications and divisions will take longer than addition and subtraction okay now between integer operations and floating point operations floating point will take longer than integer operations memory accesses take longer than accessing registers 
so if a processor has two instructions for adding one picks up operands from memory one picks up operands from registers obviously the one taking from register will be faster 
so when you change your cycle time okay suppose you you are trying to redesign a faster clock it can also have an influence on the number of cycles because number of cycles required for doing an instruction depends upon how much work you do in one cycle 
so if you make the cycle faster you may take more cycle right so so let us say multiply operation so there is some work to be performed and how you divide into cycles would determine how many cycles you need 
so if you if you make your cycles longer okay you can do more work per cycle and therefore number of cycles may be less and vice versa but what would eventually matter is is the product of clock cycle time the period multiplied by number of cycles 
so you you cannot just attempt to pull down one quantity and hoping that other will not change 
so one change can influence the other and one has to see the composite effect  
so now coming back to (Refer Slide Time 43:38) 

this formula seconds per program is cycles per program multiplied by seconds per cycle okay we of course had divide this into instructions per program multiplied by cycles per instruction but now for the current discussion if you look at this 
so to improve performance everything else being equal you can either do something to the number of cycles for a program okay you can reduce the number of cycles for a program right or do something to the clock cycles or you can reduce this seconds per cycle or equivalently you can increase the clock rate 
so these simple equations can tell you very easily which way you must make a move but if your changes are influencing if a change which you do influence more than one factor then you have to see the composite effect okay you may be improving one but you may be doing worse in the other 
okay so (Refer Slide Time 45:00) 

before closing let me just present an example to you 
so it say it looks a complicated statement lets go through it gradually 
so there is a program which runs in ten seconds on computer A okay and the computer has a clock of four hundred mega hertz 
so now we are looking for another design okay we want to help a computer design and build a new machine B which will run this program in six seconds 
so ultimately we want performance to be improved as far as our program is concerned 
so the designer can use new or perhaps more expensive technology to substantially increase the clock rate but has informed us that this increase will effect the rest of the CPU design causing machine B to require  one point two   times as many clock cycles as A for the same program what clock rate should we tell the designer to target pardon [student : eight hundred] okay how did you the answer is giving is eight hundred how do you get that 
[ student : sir the number of cycles which  one point two  times so the time taken equivalently should be twelve seconds so the speed should be doubled reduce it to six ]
see you have to look at these things where did it go 
okay so these are the values given for first case A and the time let me write it seperately 
okay so we get ten seconds by executing n instructions with certain CPI with the clock rate of four hundred mega hertz 
okay on the other hand we want to get six as a time same number of instructions okay we are talking of same program we are assuming that no instruction set is instruction set is not getting changed okay if instruction set changes this figure would change and what we have been given that CPI is  one point two  times the old CPI 
so C into  one point two  and divided by a frequency of F mega hertz 
okay so given that you have to find F right so what you will get is F is equal to  one point two   times four hundred multiplied by ten over six 

okay so that s the formula effectively used right and you will get eight mega hertz as the answer sorry eight hundred mega hertz is the answer all right okay any questions about this we will stop at that [student : sir what is the difference between system CPU execution time and OS overhead ] same thing where when you said system means operating system okay 
so the question was what is the difference between system CPU time and OS overhead both are one same thing just word it differently any other question okay thank you 




Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 1
Introduction


welcome to this course on computer architecture today we are passing through very exciting area we see computers in all walks of life they have become integral part of most of our daily activities um you find them in various shapes and sizes and if you look at a typical computer of today it packs in more power more storage capacity much more io capability than a typical computer of nineteen fifties when they began and they occupied a big hall much bigger than this way we are sitting today 
but although this is the most common form of computer you see today there are many applications for which this is insufficient such as weather prediction nuclear simulation for astronomical calculation and bio informatics you need much more computing power but on the other hand they are very tiny and small computers which you don t actually see as computers they are hidden in your home appliances cameras mobile phones even in the projector which we are using today and even the remote to control this projector 
so our question is that going through this entire variety of computers entire diversity in space and time what is common and what is different 
so this would be one of the things we are going to learn in this course that what is the common principle which binds all these different forms of computers which you see today and also which you don t see today 
so um to outline what we are going to do today (refer slide time 02:39) 

firstly i will introduce you to what is the objective of this course what is it i expect you to learn um why should we study this course what is the motivation for studying this um i will define the term computer architecture and also finally talk about how this course is structured on the whole (refer slide time 03:06) 

to begin with the course objectives what we propose to learn is how computers work what is the basic principle and we will find this basic principle is common across all computers small and large old or new um its not sufficient just to learn how computers work we also have to worry about how efficiently they were 

so performance is an important issue we will try to understand what we mean by performance and also look at some ways where we should not look at performance people have developed wrong notion which are misleading 
so apart from performance there are other factors which are important such as power consumption and the chip area or the cost 
so our focus will not be so much in to those we will look at computer architecture more from point of your performance 
after having looked at (refer slide time 03:06) the basic principles that is how a computer works we will try to see how these could be designed or built okay so that is the go deeper into the design and construction aspects of computers and we will look at various sophisticated techniques which are important in todays architectures they are responsible for giving them the kind of performance we see for example how memory structure into cahes and virtual memory and so on and how a operation can be pipelined how instructions could be put in parallel 
so some of these modern concepts would be introduced of course the scope of this course is not to go very deep into these because they are more advanced courses probably you would do at a later stage where these things will be covered in more detail but you will get a glimpse of what ever basic features of most modern computers 
why are we trying to learn all this okay question is are you going to build a computer are you going to design a new processor or new computer um the answer is unlikely the opportunity for designing a new processor or new computer is very very rare and you may or may not get it (refer slide time 05:30) 

it is very less likely that you will get that opportunity what might happen more likely is that you will probably design an improved version or a new version of an existing system 
because the number of new architectures or new processors getting designed is very very small today 


on the other hand many of you would go into software stream where you will be devolping software and the question they would be how to get the best performance of the software um for that you need to have good understanding of the architecture and the hardware 
so even though you may go in a software stream understanding of architecture how it influences the performance what is the relationship between hardware and software that is going to be very very important also quite likely is that you might be a manager or administrator where your responsibility may be to buy a set of computers 
so once agin understanding of the system is important and this knowledge of basic principles and the relations with basic performance would be important um there is another exciting area of embedded computers where computer is part of over all system it is embedded some where deep in the system and that throws open many more design opportunities 
so its quite possible you might end up in designing a new application where computer is an embedded component (refer slide time 07:27)


okay let me now define the term computer architecture um simplest way of giving an idea what computer achitecture is to relate it to architectural buildings building architecture is basically a plan of what different parts of building are to be used for 
so it its an over all layout where you decide functionality of different components of building that for example if it is a residential building you will say that this is living area this is dining area this is kitchen and this is balcony and so on 
so you you make a plan of what is the function of different parts of the building on the other hand a civil engineer would look at this plan look at this architecture of the building and do structured design which means he or she has to wory about how this building is going to um stand it shouldn t fall um it should be durable um he has to take care of stresses and strains durability if any and so on 
so the relationship you see between what an architect does for the building what a civil engineer does for building is some what similar to what a computer architect does for a computer and what logic design or circuit design it does 
okay so architecture is once again an a plan of over all functionality of the computer and um what ever the basic operation it can perform how they can be sequenced and so on 
so that that s what architecture is about whereas to realise this functionality realise this plan you need circuit designer which has who has to put basic components together transistors registers capacitors and so on to make all this happen 
so our focus would be on architecture but we will move little bit towards the design aspect not go all the way to circuit but will have some idea about how the computers are designed and built (refer slide time 10:03) 

so um now immediately it brings up um the issue of various abstractions um you find abstractions in all disciplines of engineering and science but in computers particularly when you talk of architecture you find that they are layers and layers of abstraction what abstraction means that you leave out details which are less relevant and focus on um focus on matters and issues which are important at that particular time of a some particular consideration 
as you go into depth you get more and more information as you have more and more abstraction by information is reduced 
so when um when you look at the relationship between architecture and circuit essentially they are separated by several layers of abstraction
similarly the architecture and the software are seperated by several layers of abstractions the abstractions help us in coping with the complexity because if you try to look at computer at level of basic components from circuit point of view there are millions and millions of basic components and you if you start defining your computer in terms of these components um its extremely complex and mind boggling (refer slide time 11:35) 

lets look at software abstraction which you are quite familiar already you all have done programming and typically you c c plus plus java so given a problem you can write a program which is essentially solution for that problem 
so for example this is a trivial function which simply adds two numbers and returns the result
so as a high level language programmer in c you will just write these few lines and that s your solution but computer does not understand that directly this solution has to be translated into a language which computer can understand more easily 
so a compiler will take this program in c and produce an eqivalent program in assembly language okat this is a process which is automat by a compiler and as you would notice that this simple computation is broken down into number of steps each step is an instruction i am not going to go into details of  what the instructions are basically what you see as addition here is this add instruction okay there are some move instruction which you see above this add and below which are essentially preparing the data before addition can be done they are putting the data in the right place from where the circuitary inside processor can perform addition and the first and the last statement here um are essentially required to actually link this function to main program where this could be used but once again this is not the form which is understood by the computer this has to be further translated into ones and zeros 
so that s the job of a program called assembler assembler will take these the task here is much simpler um each step here or each instruction here can be translated into a number or a sequence of numbers which is a very straight forward process and what you get is called a machine language program or machine code 
so each step here is essentially a number number notation i have been used here is hexa decimal this zero x stands this indicates that the number is a hexa decimal number where which is nothing but number to the base sixteen so two hexa decimal digits would mean that we are talking of a eight bit number or bytes 
so essentially it s a sequence of bytes which represent um the same program and in turn it represents this program which is originally written by programmer 
so although i have written not in terms of ones and zeros but hexa decimal notation has a very that corresponds with binary notation inside a computer these would be binary numbers and interestingly whether it is a data or program data or instructions the numbers don t distinguish from each other 
okay this was one one side of the picture that what you see as high level language program you normally deal with that how it is related to something which hardware understands the hardware itself again has several layers of abstraction one major module there is CPU (refer slide time 15:40) 

or the processor CPU is the central processing unit you have memory you have input output devices and their controllers for example here you are using mouse keyboard display disk drive they are connected to the system through a bus and some controller the processor is connected to memory again through some buses this is a system bus this a memory bus there is a bridge connecting these um if you go further to this box which i have labelled CPU you will find that there are it is further sub divided
so you have the program counter register file ALU and bus interface program counter keeps track of current instruction being executed register file stores operand which are currently in use ALU is the one which performs um various operations arithmetical logical relational comparison operation and bus interface is the one which connects these modules to the outside world um namely the memory and io 
one could go further into this for example if you take ALU um it is composed of several gates of different types it may have AND gates OR gates exclusive OR gates NOT gate and so on large number of those 
these gates again would be consisted consisting of basic devices transistor register and so on 
so as you see that harware level also you have sub systems within that you have components you go down further eventually you reach a level um of transistors of basic  devices which can be um fabricated 
i probably would have shown these pictures to you earlier so physically this is how you would see if you peep into a PC what you are seeing is a motherboard okay 
it s a complex electronic um assembly where you have number of components mounted on this um on this side you have power supply um on this side on the bottom side you have some peripherals okay a floppy disk drive hard disk drive CDROM drive and so on this is what you see is fan actually actually what is under this is you see parallel bar these are fins of heat sink and under that there is a CPU or the heart of the whole system the memory is here on the side and here you have some input output cards 
(refer slide time 17:52) 

we will go further into this you can see the processor area more closely zoomed in further 
okay um what you are seeing at the bottom some part is visible this is um part of the processor these are slots where zoomed in further okay um what you are seeing at the bottom some part is visible this is part of the processor these are slots where you can plug in input output  (refer slide time 19:27) 

contollers these are PCI slots PCI is the name of the bus through which various cards are connected various controllers are connected to the computer this is a memory module okay this is in a package form this is open another one these are memory individual memory chips all this put together form a memory module and several of these form the complete memory system this is a seagate hard disk drive okay shown from two sides 
(refer slide time 20:08) 

this part is the disk controller this is just a case in you are not actually seeing the medium on which recording is done that s inside um this is the controller circuit you can see closely 
(refer slide time 20:08)


okay now you have had some idea of now hardware and software what ever layers of abstraction  and this picture is trying to put these together 
so you have in the software at the highest level you see your high level language programs c lus plus example and at the lowest level you see machine instructions um in between there was assembly language which i showed you and there are ways in which you can move from here to there 
on the hardware side we we saw major building block registers adders and so on and at the bottom you have individual components and transistors 
so our focus would be some where here in the middle where you see hardware software boundary okay so what exactly is hardware software boundary its where you have set of instructions which define the basic capability of a processor and a major harware components which are able to understand those instructions okay 
so um if if you are a programmer you will you will see the machine defined by a set of instructions whereas if you are hardware designer you will see software in terms of those machine instructions which you need to interpret okay (refer slide time 22:16) 

so there are levels of hierarchy here within hardware levels of hierarchy within software  
architecture itself um which is around interface between hardware and software we are going to specifically look at two layers or sometimes it is called architectures um of at instruction set level or at micro architecture level 
so instruction set architecture refers to the lowest level visible to programmer 
so programmer is not concerned about your transistors or your gates or your flip flops or adders or so on um the basic unit of computation is an instruction whereas micro architecture is what concerns a hardware design a little more and it s the it fills up the gap between the instruction and the logic modules 
okay so logic modules for example an ALU or an register file themselves do not define any instruction but the way they are put together the way the information is made to flow through these is what defines the instruction  
so um this is the typical view of where we place instruction set architecture um in between software and hardware at top you have application programs which we are able to run on a processor with the help of some system software  
 (refer slide time 23:44) 

for example compiler is required to translate these high level programs to machine code and os is required to manage the resources and make it possible to compile compile load and execute programs um below ISA we have the design the broad CPU design then at a lower level you have circuit design okay and for fabricating the circuit for physically realizing this you need to have a layout okay where you need to worry about where you place the transistor where you place a wire how you interconnect them and so on 
so so its not just a network but physical dimension physical manifestation is assigned to these components 
okay so as they mentioned earlier assembly language view defines what a processor state is okay and how the processor state changes from instruction to instruction as you execute instruction um the processor goes from one state to other state 
so how is the state defined state is defined in terms of contents of its memory contents of various registers and flip flops which are the in the processor 
so memory whereas memory contents some what more permanent data and program what registers contain is the something some data which is currently under use you are performing an addition typically the operand will come from register file and the result will go back to register file um the programmer at assembly language has to also worry about how instructions are represented okay what is set of instruction how each instruction is represented in terms of ones and zeros or hexa decimal digits for convenience 
so above this layer of abstraction we have compilers and os and then you have high level language programs below this layer you have hardware components high level as register files ALU and so on and low level as transistors and resistances which actually make this instruction set posssible um but an architect has to worry about how to put these components these blocks together to do the instructions efficiently 
so there are lots of tricks and techniques which have to be employed to make everything happen fast okay um we will be talking mostly of performance but as i mentioned there are other issues such as power consumption particularly when you are talking of hand held devices devices which have small computers lap tops mobile phones so on where power consumption is important if you if you are consuming power at a large rate then the battery which you are carrying would not last very long 
so your task would be to carry out computation while consuming as little power as possible okay so sometime performance is the dominating issue sometime power is the dominating issue and sometime you need to have a suitable trade off you cannot let go performance and go to extreme power saving you need to have often combination of the two and with all the this had done ultimately the cost is very important 
so you may find different ways of executing same instruction but some methods could be um very fast but they might be very expensive they may require lots and lots of transistors to do the same operation 
so as we progress with this course we will see these options and you will see that the judgement of an architect lies in making right choice while designing a processor 
so um what is the basic principle on which a computer works is captured here 
 (refer slide time 23:44) 

that you have the memory which contains code or program in machine language and the data on which it is supposed to operate of course they have to be means to bring in the data which one has to operate and after computations are done take the results are out um often memory contains a specialized structure called stack which is used to create functions about hierarchy of the program abstractions  
the other block or the heart of the whole system is CPU where the key components as i showed earlier is the program counter and ALU registers condition codes 
so where whereas ALU is the one which basically perform all arithematic and logical operations um the operands are contained in registers 
so they may have to initially brought from the memory and after having operated on them they are send back to the memory PC is the one which keeps track of the current instruction 
so the way it proceeds that PC will help in picking up an instruction ALU will carry out the desired operation and PC will then point to the next instruction and so on 
so a sequence of instruction will roughly go on in this manner um the instruction will differ from each other in terms of how they are encoded what operations they invoke what ALU supposed to do 
so there is a controller within the CPU which has not been shown but that s the one which will guide all this components to do the right thing at the right time typically after a operation is done there are some condition code which are set okay these often could be a part of the register file itself sometime in some processor this could be separate and these could be used subsequently for decision making and that what helps in providing branches and loops as you know in high level languages 
so there may be very sophisticated processor or simple processor but this is the idea which is common to all of them and we will elaborate on this see specific examples of this um there are architectures which have been deviced which are which deviate from this basic idea they are some architecture which are revolutionary different we will whereas talk little bit about those time permits but this is the most common thing of most principle behind everything (refer slide time 31:21) 

so although we might make an abstraction like this simply say that um a processor works in terms of simple instruction which are representation of um the real problem but there are some real situation which are often have to which often get ignored 
okay so when you do abstraction some details are getting ignored and some information may get lost 
one has to be careful when you are making an abstraction you you must know when what kind of simplification is being done what what is getting ignored 




for example in your registers or memory words you will store supposedly integers and  reals okay
but what you are storing actually is only an abstraction only a representation which may not be exact okay 
so integers for example as you understand mathematics are unbounded okay but when you talk of real programs and real hardware they have to be bounded okay
integer will have to be represented by a finite number of bits 
so when you perform addition of two integer in mathematics and you perform addition of two integers and finite number of bits um the two may not exactly coincide so one has to understand these differences
similarly what you represent as far as real numbers is concerned is again an approximation because of finite number of bits you have finite precison and you have finite range 
so sometime there are nice properties which real number would satisfy when you are working in mathematical domain the same may not hold when you are talking of real instructions okay 
for example um when you have to add three numbers a plus b plus c you can group them anyway okay what is called associativity is a property which you simply assume 
but you will find that it may not necessarily hold when you are talking of numbers as abstracted in your computer system 
so you must you will have to understand to understand a processor you need to understand its assembly language um fairly well 
although you may not in real life program an assembly okay now (refer slide time 31:21)
almost almost all or i should say all programming is done in the high level language but still understanding of asembly language is required at the back of your mind to understand how programs are going to behave when they are on real hardware 
okay so what what you might if you if you have no feeling of what assembly language is what instructions are what their limitations are um you may um you may program in a world which is somewhat isolated from the physical reality 
similarly um an abstract model of memory may say that it is simply um an unbounded array of bytes or words but physically memory is bound 
you have limit[ed] number of word or limit[ed] number of bytes each has fixed number of bits and it may not necessarily be equally fast to read different words of memory 
one acess to memory may be fast next time you may another access to the memory it may be slow 
so what makes that happen so there are memories not just a flat array of bytes or words it it is a hierarchial structure in real practice 
so um that has to be kept in mind and when you are talking of performance in program often the discussion focuses on the asymptotic complexity 
but in real life what you have to worry about is the execution time okay so that that is a bottom line given a program and given an architecture how long it takes to execute that program on that particular architecture 
so that s the question which has to be answered okay from theoritical consideration um whether the program has one complexity than other it is important but ultimately um what one would like to see is in terms of milli seconds seconds and minutes if program is taking that long 
i mentioned about embedded computers domain which which would probably if you are in that domain it will provide you opportunities for making new designs 
it would typically require designing hardware as well as software (refer slide time 36:53)


okay and that is harder than designing just software or designing just hardware it s a task where one has to worry about both the issues 
so embedded computers are um treated more as components okay embedded processor is part of a system it appears just as a component which performs some intelligent function rather than the conventional view of a number [ ] or computing engine okay 
so um a small computer which allows you to um control a proces information or perform some communication opens up lot of application possibilities and that s what embedded computer domain is 
typically one has to worry about real time operation there a computer in embedded domain has to work with information as it appears in real time it has to respond to that immediately and as i just mentioned that there are lot of design opportunities um because each embedded application has a customized design you you might use a standard processor um but more often you may have to customize a processor you may have to add or subtract to the architecture even if it is not a completary design 
so when the basic principle behind all the computers is more or less same okay the simplest part the key part is same what is the difference between the variety of processors you see   (refer slide time 38:50) 

you you see different processors in different applications from tiny ones to large ones you have in desk top computers lap top computers mobile phones washing machines so on um what is the difference um all work in the principle of having an instruction set and executing instructions one by one okay what is difference in these cases of course there may be difference in instructions there is they may be tremendous difference in terms of performance or speed there may be lot of difference in terms of power consumption cost um and in terms of instructions some may be very specialized and some may be general purpose general purpose means that you do not apriory know what kind of application it is going to be used for 
so a desk top computer for example may be used for power point presentation at one point of time next time it may be used for emailing and yet another time it may be used for listening to music whereas special purpose computer for example one lets see in mobile phone has very specific task okay it has to process the calls okay um and that function is unchanged 
so unlike in desk top processor where you um can change the program and do different things at different times in a in a mobile phone the program is fixed and over and over again it is same program which is executed 
so here for example you have two different CPUs um this one on the left is a pentium processor and one on the right is a again from intel same manufacturer but a micro controller 
so it is this type of processor which is found typically in embedded application the actual chip is this small rectangular tcq see inside this circular window (refer slide time 40:45) 

whereas in this case the actual chip is of that size this big space which you see um the size here is larger because of the packaging requirements you have there are some thing like forty pins through which it will connect to other circuitary 
so to accommodate forty pins it has to be made so big um but the same device same circuitory is also available in much smaller packages roughly lets say a twice this size approximately this size okay where pins are much tiny and they are much much more closely spaced 
so these two processors are two contrasting devices the one on the left is a general purpose processor um very high performance consumes lot of power and is much more expensive whereas one on the right is um typically used for special purpose applications it has much lower power consumption um doesn t care about performance whereas the one on the left would work at giga hertz of frequency one on the right works only at few mega hertz um now this transparent window which you are seeing is essentially a quartz window through which you can um erase the program it has memory which can store the program (refer slide time 40:45) inside 
so since program has not to be changed frequently you can put the program inside its memory and then forget about it okay generally if you want to make correction you cannot make some upgrade you can expose this to ultra violet light the memory content will be cared of and you can change the program but that you will not do every day 
so once you have made it a part of a circuit typically it will um that program will remain keep on operating over and over again and its only at some infrequent time if you want to make a change you can do so um they are those were you cannot make a change you just write once and you don t chage at all 



okay so finally let me summarize with list of major topics which we are going to cover in this course um and that forms an outline of the course (refer slide time 40:45) 

we not necessarily go in the order in which it is written here um we will define what is meant by performance of a computer how you define it how you measure it and how you relate it to various architectural aspects we will take up specific instruction set a simple but powerful 
so that you can see how instructions work how you can express computation in terms of doing instructions and thereby you will get a feel of what an architecture at instruction set level looks like 
then we will move over to the design aspects we will see how an arithmetic unit can be designed when um build okay how basic operations are carried out in terms of binary numbers and how we can build circuitory to carry out those numbers um those operations then based around based around an ALU how will you build a processor to execute the instruction 
so ALU is just one um part of the processor you have to put register file you have to put buses you have to put program counter and you have to put a controller to make the data flow into various components properly 
so so next will be dealing with a design um the flow of data the data path an the controller briefly i will introduce how a common technique of pipelining is used to improve performance 
so all high performance system which you see today have pipelining of some form or the other 
then the next important component is memory which is actually structured in a hierarchy there is cache memory there is main memory there is virtual memory 
so you will see how all these are put together to give you a good performance at a reasonable cost and finally we will talk of input output devices input output controllers and how they connect to memory and processors um let me give you an idea of the course homepage um which i am going to use to carry out various announcements put all the information about the course here the description of all lab exercises and so on 
so here once again there is an overview similar list of topics which i talked of incidently doctor kolin paul would be associated with me in conducting this course um he will be taking some of the tutorials and also he is joining me in preparing presentation material for this there will be teaching assistant i think more than one at the moment i know of harsh dand um he will maintain a lab homepage which you can reach from here where exercise will be announced and all instructions about lab exercises would be given 
so even for the first lab exercise which i have announced there are some more instructions which you must see here before you make a submission 
so guidelines for how to make submission of assignment will be given here 
so before submitting please have a look at this um initial few exercises would be using a simulator called spin which i mentioned to you earlier (refer slide time 48:13) 

you can download or you can use it in our department if you have an account but don t worry if you don t have an account you can use it in computer services centre um the textbooks this is the main book which will be used (refer slide time 48:34) 

i don t know if it is readable this is computer organisation design the hardware software interface by g l henasy and d a paterson this is the main book other books which will be vocationally refered to computer system design and architecture by huring and jordan and computer architecture and organisation by j p haze 
you would notice that these terms computer design computer organisation computer architecture are often used interchangeably 
so you you can infact in just list a book we will see that they are being used in various combinations but if you if you it little rigerously the term architecture is used in terms of describing in over all functionality whereas design refers to the hardware building aspects of it 
we will have minors majors lab exercises um weightage of lab would be twenty percent and quizs or any other class asignment all together consist of another twenty percent 
okay so i will stop at this and if you have any questions about what i discussed today or about the course in general you are most welcome to ask 
[noise] um PCI i think stands for um i am not recollecting what is P for but C is for computer and I is for interface peripheral computer interface yeah 
any other question [noise] mobile phone mobile phone um you would consider it not as a computer system but it has a computer okay infact many mobiles may have not just one multiple processors 
so it has one or more processor it has memory and it has some IO okay you can call it a special purpose computer because as we understand computer has a processor memory and IO okay so it has all that although its main functionality from a user point of view is not computing it is communication right but seen from computer point of view yes it is a special purpose computer [noise] like what [noise] palm palm top yes palm top is a computer okay it infact it is a more general purpose computer it can it can it has various programs which can be run it can do calculation it can do your um it can be used as a organiser it can be used for communication variety of function it can be used for [noise] 
yes i can make them available on the net yes i will infact i will put them but they would be generally a gap of a day or two 
so its only after i have delivered same day or next day typically it will be put on the net any other question 
okay thank you 




Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 2

History of Computers 


in the previous lecture i talked about what is important in this course what we are going to cover in this course and in this lecture i am going to look at history of computers history is important because by studying history we get good insight into the subject whether it is political history economic history or history of science or history of computers and we get to know the present better as well as the future better 
so we look at the developments which have taken place in past have made um significant impact on the computer we see today and we are going to see tommorow 
so first of all we will look at past half a century or so and see how this period gets divided into different generations (refer slide time 01:49) 

then we will look at what had been the landmark development which i had significant impact on history or on the design and architecture to look at some of the interesting old pictures of computers and finally have a look at briefly this mentioned what technology is are being worked upon in various labs they may decide what kind of computers they are going to have in the future 
this period of about half century or so is divided into five generations there may be some differences of opinion here (refer slide time 02:35) 

some people only recognize only four generations but some talk of five but i will talk of all the five and mention which one is the fifth one which is not a regular generation as recognised by many people 
so these different generations as we see are predominantly different in terms of the technology which is used to develop computer systems or electronic system in general 
so as some major technological development takes place um everything changes the way is significant very drastic sudden change in in the shape of the things and it gives rise to a new generation these changes the fundamental changes are in terms of size size of computers as you have as they mentioned last time that what is to be a room full has not shunk on to something small which can come on desk top 
in terms of cost the affordability has been changed over time um earlier computer used to be affordable by large organisations now large number of people can afford to have the individual computers 
and infact if you count all computers um the computer density in many cases may be more than one per person if you look at all all devices all appliances mobile phones and various equipment the number of computers you have you can account for per person could be even more than one that s the situation today 
the power consumption has changed drastically by several large of magnitude the the efficiency in terms of performance what they could do um for a given input that has changed and of course reliability 
so there was a situation when we now and then something will go wrong and has to be taken care of but now computers can work very reliabily for extended periods 
so the beginning in forties (refer slide time 04:55) 

so forties to fifties is what is considered as the first generation where the basic device was vacuum tube 
vacuum tube is the bulb like device as you can see in the picture here um it its basic electronic device which can amplify signal or it can switch voltage at current levels 
so this is this is expensive bulky and reliable and powerguzzlers the computer is built using this again huge roomful of system and the input output devices were punched cards and paper tapes the memories were in the form of a rotating drum magnetic rotating drum and the language used to program was machine language 
so this was how the first generation computers were 
the second generation   (refer slide time 05:55) 
as you would notice a period which i am indicating are overlapping okay its not that suddenly on day one the generation ends and next one starts so there is a transition but when there is a change there is a rapid transition otherwise all the time there is growth development improvement but a generation change is marked by not absolutely abrupt but rapid transition 
so that change here accord with arrival of transistor so the picture shows some old transitor from texas instruments these transistors perform roughly similar function what vacuum tube did (refer slide time 05:55)
but they were much smaller faster cheaper and more efficient in terms of energy also reliability was much better in terms of the time after which transistors likely to fail there there was also develop for the development in terms of software instead of machine language people started using assembly language and also within this period some languages high level languages the early versions they appear such as fortran cobol then the technology moved further (refer slide time 07:13) 

became possible to put several transistors together on a single silicon chip okay that was called integrated circuits so you can integrate many transistors and that appears as a single device the picture here shows the layout of some early chip depending upon the level of integration these were characterised as SSI MSI or LSI SSI stands for small scale integration medium scale integration and large scale integration or small scale integration would mean may be a few devices few transistors may be about ten fifteen or so medium scale integration would take to a few tens of transistors large scale would scale few thousands of transistors 
so um with this naturally equipment computer became more compact the speed increased the efficiency increased additional peripherals became available the computing became more interactive with cards and paper tape it was batch processing environment entirely that is you um submit your program for running and a day later two days later depending upon the load and efficiency of the system you will get your results 
so its not that you press a button and immediately see the results you you will have to prepare a deck of cards on which progam is punched and data is also punched or prepare a roll of paper tape by punching program it and on it and submit to a central facility and sometime later somedays later i should say the result will come all you know you might find that there was a small syntactic mistakes you had some full stop or comma missing somewhere and the program result is nil it will just say error message 
so you work on it make that correction submit again and so on it was very tedious process 
so now this time the the computing started becoming little more interactive in the sense that there would terminals and keyboards through which you can immediately interact instead of a batch it was more interactive and online mode and operating system also evolved which were used to manage all the resources (refer slide time 09:48) 

then the next change next major change occur when um VLSI appeared on the same 
so now it became possible to make entire processor on a single chip the number of transistors you could integrate on a chip was large enough to make it possible for entire processor to be within single chip and with this the level of integration in this arise LSI to VLSI VLSI stands for very large scale integrated circuits um after VLSI also terms have been defined like ULSI ultra large scale integrated circuits but they have not become popular and we we from seventies late seventies we still continue to be talking of VLSI also although the number of times you can put now has gone to several million um infact tens of million tending to hundreds of millions whereas initially VLSI basically meant that if you um tens and hundreds of thousands of transistors can be put on the chip 
so um with this level of change in the technology um it became possible really to have individuals owning their computers personal computing home computing embedded computing that means computers becoming just a component in a large system all that became posible when processor really shrunk to a single chip 
then in terms of capablities of computers there was more emphasis on graphics graphical user interface driven by a mouse um hand held devices also started appearing that was the fourth generation 
(refer slide time 11:51) 

now many people um do not talk of further generations after this okay they believe that it is basically same microprocessor era or the fourth generation which is continuing but there is in literature you find mention fifth generation also where one doesn t see so much of technological revolution but only change is in terms of capability of computers um computers becoming more intelligent use of artificial intelligence more and more use of more and more parallel processing to increase the performance and input outputs becoming more natural in human life 
so infact that of fifth generation computing came from a massive project which was undertaken in japan is there a question okay 
so um unlike previous definition of previous generation which were defined by change in the hardware technology here it was changed more in style of computing and level of performance okay we leave now the generations here (refer slide time 13:01) 

and look at a few um technical features of how various parameters which are important to us have changed over time for example lets look at the performance um has with respect to the cost of the computer 
so over the period over the time performance has increased there the cost has decreased if you really combine the effect of these two and look at performance divided by the cost per unit cost of a system you get very astonishing improvement 
so on a relative scale um four technologies are compared here vacuum tube transistor integrated circuit and VLSI the four different generation we are talked of and roughly look at the years nineteen fifty one sixty five seventy five and ninety five the performance per cost per unit cost raises from one to thirty five to nine hundred to  two point four  million 
okay so you can see the amount of progress which has taken place in terms of performance you can buy for a rupee or dollar um another interesting thing you would notice is how memory capacity has changed (refer slide time 14:18) 
okay so we are looking at not the entire period but the time when DRAM or dynamic ram the basic semiconductor memory which you find in all computers today um how it has changed in terms of capacity   
so we are talking of capacity per chip so you see almost a linear rise on a log scale so which means essentially there has been a exponential rise in the capacity as theyears have passed by from nineteen seventy six to nineteen ninety six um whereas in early period here you see a sixteen k capacity sixteen k um bits okay
now you can see in ninety six so about twenty years later its sixty four million bits okay 
the the rate at which the capacity change is from year to year is roughly in one and half years the capacity doubles okay if you take a period of three other it will quadraple 
so you can if you look at the clock frequency at which processors run that also you would find that has seen a exponential rise okay so again roughly every um two years you will see that one and half to two years you will see that the frequency of processor um keeps doubling (refer slide time 16:10) 

this graph shows the overall performance on some relative scale of complete computer system which will actually include performance contribution made by cpu as well as the memory um as as a single figure 
so now again just over a ten years period from nineteen eighty seven to nineteen ninety seven you could see an exponential rise in performance right
so some of the early machines in this decade um sunfour two sixty and then there are few mix machines ibm machine hp bakel and lastly you see bakel for twenty one thousand two sixty four um where the performance has gone from almost ten on the scale to more than thousand (refer slide time 17:19) 
okay now um lets go through some key developments which took place so we so far saw in terms of generation and how technology changed but now lets look at the events 
the significant events which occur um the year in which they occur um what was the event what was the impact 
so lets go through um next couple of slides some of the major events beginning with introduction of the first programmable computer by konrad zuse which is called Z one in nineteen thirty six okay nineteen forty four um computer called harvard mark one was introduced um this computer is known for what is called harvard architecture um this architecture essentially means that the memory used for storing program and memory used for storing data are too separate okay what we see most commonly today is the single memory which accomodates program and data um but here there were two different memory units for program and data and in in some forms you see that idea even today although the main memory is the single memory but you often find that the caches for program and data are different okat this basic idea could be actually traced back to that time 
then there is um in a ENIAC one computer and its developers were eckert and mauchly this was significantly large as compared to previous attempts actually in that period there were several attempts some some were partially successful some are fully successful in this periods of forties um many many groups who were working who were researching on development of computers developed their own system but this one ENIAC one is considered to be the most significant in terms of its um size its capabilities and also it formed basis of many things which came later on 
so often its difficult to pin point who can be um considered as the inventor of computer because there were many competing claims some some had some virtues on others 
but its john fon newman who was the first one who who is not involved in developing of any machine which you see listed on this file 
but he first wrote down very neatly very precisely the concept of a store program which we consider as the basic principle of computer this is what i mentioned in the previous lecture also is that basic idea is that you have a program in the form of  a instruction and with the help of program counter the processor takes instruction by instruction and executes 
so that s a simple idea but in this form it is first stated by john fon newman um whereas around the same time these computer which were being made some of them were not really programmable um you could not store program you have to actually plug in wires from one slot to other slot to change the functionality 
so they were um sort of computer which were not very easy to program 
then nineteen forty seven eight is the time when transistor was invented so that that is the one event which changed history 
okay first commercial computer is considered as UNIVAC and people who were involved with ENIAC one were actually responsible for development of this okay 
so um its it s a different matter that you develop one of a kind system in a research lab and it s a another matter that you have something which is repeatable it is commercially available and people can go and buy 
then IBMs first computer is IBM seven o one okay so that way IBM enters the history of computers 

(refer slide time 21:23) 

and has remained very significant player in the industry the language FORTRAN um the first high level language which easy when used today in some form its not not in the original form there are FORTRAN one two three four FORTRAN seventy seven eight nine and so on 
so various versions of but original um event is nineteen fifty four when this first version was made available um 
okay nineteen fifty five um by stanford research institute and bank of america and GE um computer was developed which was the first use of the computer in the banking industry and also MICR was made available that time MICR you would know is um is magnetic ink corrector reading (refer slide time 23:20) 
the ics integrated circuits date back to nineteen fifty eight okay um first computer game nineteen sixty two spacewar um invention of computer mouse which used so frequently now is nineteen sixty four first computer network ARPAnet in nineteen sixty nine 
so this was actually a network developed by defence funding ARPA stands for advanced research project agency it say it s a defence agency which funds research projects in USA um large scale integration begins here um intel first memory chip (refer slide time 23:59) 

one one zero three first dynamic RAM chip and first microprocessor is intel four zero zero four um the flexible storage unit flexible disk or more commonly known as floppy disk because of its flexible nature um ethernet computer networking is used in that nineteen seventy three um first computers used by consumer so first home computer or consumer computers in seventy four seventy five (refer slide time 24:43) 
um some some more popular computers which actually span many years from apple TRS eighty and commodore um spreadsheet program which is very popular now the first spreadsheet was visicalc developed in nineteen seventy eight first word processor wordstar nineteen seventy nine  (refer slide time 25:15) 

IBM PC once again this again a very very important landmark in the history um in nineteen eighty one and it made this the significant impact of this was actually that computing came to personal level a series computing the earlier consumer computer which i mentioned was more for hobby but with PC the series computing also moved to individuals and the companies which actually reaped maximum benefit out of this event were microsoft and intel and what they are today is largely because of this 
so the first operating system first version of microsoft operating system MSDOS came up in nineteen eighty one at the same time um apple lisa computer was the first computer with extensive GUI graphic user interface till then primarily computed in used with textual interface you will give command textually and see the results 
but with graphic interface it became much more convenient another popular computer of apple apple macintosh um windows appeared in nineteen eighty five and the it still being used today in different versions 
so i have not traced history of events beyond this um but as you see that lot of significant events took place in this period 
okay now before we go to an end lets look at some of the old photographs of old computers and you will get a feel of how different they looked as compared to as we see today 
so this is one of the early computer from IBM IBM s SSEC it was used to compute tables indicating moon positions (refer slide time 27:55) 

so and this is used to use for the apollo flight to moon in nineteen sixty nine the speed of this was only fifty multiplications per second 
okay now we talk of millions of operations per second okay the input and output was using cards and pumched tape the technology used here was vacuum tubes and relays 
so relays are devices again which can be with the control voltage the current they can switched on or off 
the ideal size you can get by looking at this picture and it required floor space of twenty feet twenty five feet by forty feet  
UNIVAC one which i mentioned as the first commercial computer um again a few thousand of operation per second input output was in terms of magnetic tape printer memory size was just about thousand words thousand numbers it could store each with twelve digits and the technology used for storage was delay lines apart from delay lines this had vacuum tubes magnetic tapes these are the technologies (refer slide time 29:16) 
the cost is that of seven fifty thousand us dollars okay so which is several orders of magnitude as compared to todays system and you can also look at cost of printer um which is of similar order um two very popular systems IBM three sixty on the left and CDC sixty six hundred on the right (refer slide time 29:23) 
architecturally CDC sixty six hundred was very significant um it introduced the idea of  pipelening and idea of um some of the ideas of the modern day processors are actually traceable to this computer 

(refer slide time 29:56) 

so i will discuss that when we go in to more architectural details ILLIAC four was um a system a parallel system developed by university of illionos urbana champaign 
this was first major attempt to develop a large parallel system these are mini computers 
so in seventies the level of integration became high computers computers shrunk from room full to may be a cabinet or a something on a pedestal and these are called mini computers (refer slide time 30:36) 




on the left you have PDP eight on the right you can see its our table its HP two one one five and introduction is through the type writer like device and its not a crt monitor it has a keyboard um but the output is in the form of a printer its like a type writer but it acts as terminal for the computer (refer slide time 31:03) 

on the left we have in this picture xerox alto machine um which is considered one of the first work stations so work stations are basically powerful desk top computers which have full fledged capability of large computer but good user interface graphic user interface 
so xerox did some pirating work in developing window based interface which you almost take for granted today 
on the right there is super computer one of the early super computers CRAY one 
so um there have been many machines from this company called clay which fall in the category of super computer 
so this was very high performance computer perhaps the highest performance computer at that part of  time and you would see that it has number of cabinets which are in a which arrange in a circular form and the idea of that to reduce the length of wire length of cables which connect back panels of these cabinets the the reason for keeping this cable short is to reduce the delay 
so this required extensive cooling to make it work at a high speed 
okay these were some interesting pictures of computers of aragon by um what kind of technology we see in the future there are lot of things which research is taking place the devices are shrinking further and further to the extent that they are reaching the atomic dimensions 
so the transistors would be built using just a few atoms of some specific elements 
so we are getting into what is called nano technology area
nano technology means all dimensions are nano meters of that of nano meters um grid computing refers to computing which is spread world wide 


so you have massive number of hundred thousands of billions of computers all netrworked together trying to solve a single problem 
so that s another way the computing capability is getting extended 
quantum computing based on quantum principles it has (refer slide time 33:41) 

a promise of solving some of the tough computing problems in very very shorter time 
DNA computing again DNAs are very complex structure and computing based on those structures is the possibility in the future
so we we don t know at this point of time which of these technologies would be successful to what extent and depending up on how these grow how these technologies develop you might see totally different kind of computers in future 
we will stop at this and in the next lecture we will start looking at a very simple architecture and go into depth understand how we can um use it from a programmers point of view how we can design from how you design a part of it 
thank you  




Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 4

Instruction Set 
Architecture-2


we are discussing instruction set level architecture with the help of very simple instruction set for MIPS processor we have seen that this architecture has thirty two registers each of thirty two bits and they are very simple instructions with which we can perform arithmetic add subtract and other logical operations and we have also looked at some branch instructions we will continue further in that direction 
here is the summary of instructions (Refer Slide Time 1:44) 

we have discussed so far add subtract add immediate subtract immediate the letter two are for operands which are constants 
similarly we have and and or and they are immediate version and immediate or immediate for comparison of less than or greater than type we have slt instruction which also has an immediate version 
so in all immediate instructions one operand is still a register and second operand becomes a constant then for equality and inequality comparison there are beq and bne instructions for an unconditional jump there is a j instruction or jump instruction for moving data between memory and registers they are load and store instructions 
in particular we have seen load word and store word these two instructions um when you have to load a constant which is a large thirty two bit constant we need to do it in two parts and first we have to worry about um load upper immediate which loads upper half and then you can put back the remaining half of the right half 
so the its important to know what is the format of the instructions format is the way in which instruction is divided into fields um they are instructions which are of  R format or register format where apart from opcode you specify upto three registers I format is required where ever there is a constant either for performing an operation or for specifying an address 
so these are I format where there is a field of sixteen bits so that a sixteen bit constant can be provided um yet another format was J format which is for jump instruction  where twenty six bits which are left after having a six bit opcode are used for specifying address okay now um today we will continue further from this point and talk of how we can have a further controlled constructs 
we have seen a case of simple  (Refer Slide Time 3:53) 

if then or if thenelse type of structures where a simple check using typically beq bne instruction possibily in conjuction with slt could be used um same instructions can be used for executing loops we don t need to introduce new instruction and we will see that um we will see that typically in a loop you access an array and that can be accessed with index or a pointer we will see what is the difference between these 
we often need um switch type of statements where you need to take a decision which has multiple outcomes 
so we will see that and finally we will go into little more depth of how addresses are actually given in various MIPS instruction there is a variation from instruction to instruction and we will have to understand some of those subtle differences 

so lets start with a very simple example typically probably the first 
(Refer Slide Time 5:09) 

you learn in your programming is summing elements of an array okay so it s a straight forward thing although i have written in um some what unusual form where instructions using a for structure have put in terms of conditions and go to so that i can easily correlate with what we are going to write in assembly language 
so A is a array and n is the size of the array they are n elements 
we begin by intialising a variable which is going to hold the sum then there is an index and this is the main body of the loop where you take element of array and submit update the index and perform loop termination check and go back 
so L is the beginning of the loop and this is the body of the loop so this is a header or prefix of the loop 
now how we do this in assembly is what is of test in today  (Refer Slide Time 6:20) 

these instructions these steps are implemented by very straight forward move instruction where you move a zero value into some register 
so now remember the correlation between the variables and the register we are using we are using s zero register for holding the value of the sum we are ging to use t zero register to hold the value of the index okay now here we have written a single statement s equal to s equal to s plus ai but this needs to be broken down into a number of steps first of all we need to repair the address to make an excess to the array 
so has you know that each element um if you are talking of integers its going to occupy four bytes 
so from the starting point of the array the offset of A i would be four times i 
so we actually multiply t zero with four and t one now contains four times i um this is added to the starting address of A 
okay here i will just put a comment that s one contains address of A zero address of first element 
so these two were added together give you the address of A i once we have that we can use load word instruction and the offset here is zero the word loaded comes to t two 
okay so t two is a register which is temporarily used and finally this gets added to the running sum which is in s zero as you notice in the beginning these two are added the result is in s zero 
so it is one two three four these four instructions are actually doing the job of this statement and then simply update the index compare with s two i am assuming that s two contains n the size of the array okay so t zero which contains i is compared with s two and if t zero is less than s two we branch of to L that means start the loop again okay now i have not talked of blt instruction blt is a pseudo instruction okay its not a real instruction in some tutorials i have mentioned that there are pseudo instructions which may correspond to one or more real instructions 

so what exactly is this particular one  (Refer Slide Time 9:00) 

blt compares two registers and if first one is less than second one goes to label specified here okay this is equivalent to actually two real instructions one is slt slt compares these two but the result as a zero one value is stored in at then you can compare at with zero and then make a branch all right 
so what will happen if r one is less than r two register at will set to one and it will not be equal to zero and therefore branch will occur 
so in tutorial we are going to see um many more instructions like this and see how they can be implemented using slt and beq or slt and bne okay so various kinds of comparison can be done um once you understand that we can simply use blt as long as we have assembler which expands it for us or we can expand it ourselves um this works fine 
now here where is the cursor yeah um i have introduced another register at at stands for assembler temporary 
so typically i am expecting that blt will be expanded by the assembler and as you know there is a need for a register to hold some temporary value some intermediate value and by convention one of the registers is actually dedicated for this and in terms of numbers this is register number one we have already seen that register number zero has a special role okay 
so which is my hardware you cannot modify the contents of register zero it always remain zero um at or register number one is used by assembler its more by convention 
there is nothing which hardware does in this case if you want to use at for some other purpose you you can do so hardware will not restrain you but then you can get mixed up with assemblers usage of it and your own usage of it 
so the convention is good to be followed so you would typically not use this directly and leave it for assembler to use okay now that was the core of the program which i talked of how does the over all program look like (Refer Slide Time 11:39) 

we have some assembler directives which define the data area and the code area 
so dot data is to be used when you want to define some data area 
so we are saying that A is a label here there is another assembler directive dot space and there is space which you are trying to leave of four hundred bytes what we are saying is that in the data area leave space for four hundred bytes um i am assuming here that n equal to hundred we have array of size hundred um then there is dot text which indicates beginning of the code or the program 
so you will initialize these two registers which we assume we assume that s one contains the starting address of the array 
so here we we are using instruction la which is load address into this register and A is the address we are loading 
so as a symbolic language programmer or assembly language programmer you need not worry about what address actually A is you are leaving it for the assembler to figure out um if you do want to bother with the data directive you can also specify an address here 
okay but here we have chosen not to and assembler will assign some address in some default data area 
so A for us is label but internally A is an address and that address is getting loaded into um s one assuming that A is an address which could be of arbitary size thirty two bits at most this instruction is actually a pseudo instruction which will get split into these two real instructions and this is what we had learnt in the last lecture that loading a large constant into a register requires two steps first step loads the upper half next step loads the lower half 
so the two halves of the address i am denoting by A upper and A lower right A upper is loaded by lui instruction into upper half of s one and then this or instruction simply um superposes the lower half into the right portion of the register 
our second initialization i wanted was that register s two contains n okay so i am saying load immediate s two hundred hundred is a small constant 
so a single instruction get loaded you or zero and hundred and put in s two 
so with this preparation i am ready to go um you need some code to input the data fill up the array for which you are going to perform the sum then there is code for computation this is what we have seen already and there will be some code for the output okay once you have done the summation you have to output the result 
so i am skipping the details of input and output code we are discussing that in tutorials right now (Refer Slide Time 14:54) 

we have written some code which works but can we improve it 
so here we would notice that using pointers or addresses directly rather than index we can have faster code or improve code 
so on the left side here you see the same code which we have and there is some modification made here i mean using a pointer p so this is still in high level language i will i will show you how we do that in assembly language 
so p is a pointer which to begin with is pointing to starting address of the array okay and when you are performing sum instead of indexing the array A we are dereferencing the pointer okay 
so instead of saying s gets s plus A i you have s gets s plus star p um then in addition to incrementing the index we are incrementing the pointer and rest is same so these are the two extra statements we have introduced one is initialization of pointer updation of pointer the summation gets replaced by this 
so although it may appear that i am introducing more statements but when you translate this summation into assembly you will find that you can do with um lesser number of instructions okay that s were the gain is occuring and initially there is one extra statement but in general we are more worried about making the loop efficient because that s the one which is done several times in this case its just hundred times but it could be thousand it could be million 


okay so here is (Refer Slide Time 16:52) 

what we have indicated and these are the instructions let me see how to 
so here is main body of the loop where you would see that the number of instructions has reduced okay this is the initialization of the pointer which i have introduced i am sorry this is okay say jump two steps further okay i wanted to look at this first here 
so this is the initialization of the pointer i am saying load address dollar t one A okay we have seen how such a statement gets expressed in terms of real instruction and the loop has become simpler this is simply a loading the word and performing the addition because address is already ready in the previous case we had to multiply the index by four add it to the starting address of the array and then get the data from the memory 
so those two initial address preparation steps have been skipped here you can straight away load the data from the memory and from the addition and that s where the improvent is this is an extra statement which has come alright 
so although we have cut down some thing we have added but what we have cut down is much more what we have already is not that much the remaining part is same okay is there any question about this okay 
so having done that we find that there is more opportunity for improving the code you would notice whats happening here is that we are working with pointer and an index both are there okay we are using index simply to control the termination of the loop question is can we use the pointer itself to do that instead of maintaining a index which we are not using to access a array we have the pointer and at the end of the loop we can check pointer if it has reached the final value (Refer Slide Time 22:26)
so if you do that then i can be eliminated all together okay how do we go okay so here is an attempt to improve the code further you have (Refer Slide Time 20:44) 

i equal to zero initialization and i plus plus and this comparison these all change okay first of all we have removed i equal to zero we have added a statement which computes the final value of the pointer okay um we say that q is p plus hundred okay and instead of comparing i with n we can compare p with q so i plus plus is also removed and this comparison changes um once again we have introduced a statement here but we have removed there so there is not much of difference and the loop is even shorter 
so we go back to assembly and see how this is done this is a statement this is the instruction which is preparing the final address okay the final value of the pointer and we are keeping that in s two 
so s two is equivalent to our q now um although in high level language i said p plus hundred but i must be careful to add four hundred here okay if it is four hundred which is added here in place of hundred and s two now contains the starting address of A plus four hundred which is now at the final address (Refer Slide Time 22:28) 

so um rest is same and the comparison here is between p and q right 
so this is our final code after having all these improvements 
okay now having understood how a loop is programmed lets take up some thing little more complicated what if they are two nested loops and you typically require that in sorting okay 
so here is a (Refer Slide Time 23:18) 

simple sorting algorithm okay this is not definitely a efficient one what it does is simply iterates over the array okay finds out the smallest one smallest one place here at the first position and looks at the remaining goes over those finds the next lowest and so on 
so they are two loops here once again i have not put for structures i have used comparison then jumps 
so you would notice that there is one loop here okay which is beginning at Y and this is contained in another loop which is beginning at X 
so that is the outer loop u in a loop makes a pass over the array the main step which i have not detailed here i will took look at it seperately main step basically um looks at one element of the array um which you are currently scanning and compares it with the element placed in the key position where you are trying to bring it it compares and if necessary interchange is done 
so this key position in the first outer loop first iteration of the loop is the first position then becomes the second position and so on 
so what i am doing here is with the with the first with encoded the first position you scan rest of the array then anchor with the second position scan the array and so on 
so this is a comparison and interchange step alright what you need to focus here what you need to worry about here is how these um indices are changing or in this case basically i am working with pointers 
so p is one pointer to begin with it is pointing to um initial position of the array q is again pointing to the end value okay so i am doing p plus ninety nine 
so it is not pointing to um the final value at which you will terminate it is pointing to the last element okay so r would be second pointer q will not change in the process it is p and r which will change r will change in the inner loop and p will change in the inner loop 
so r gets initialized to p plus i think it should have been p plus one okay in assembly i will say p plus four please correct it to p plus one 
so um now r is initialized to this in the inner loop it is updated 
so we are basically comparing elements pointed by p and r and if necessary interchanging and this inner loop simply does that p is not changed r is updated comparison is done and you go through this once you come out of this then you update p okay nd repeat this process (Refer Slide Time 23:18) okay r starts with a position which is next to that 
so once again it will translate very simply the pointer initialization is load address 
so t one is carrying our p value um it prepares next statement prepares q for comparison purpose um s two contains t one plus three ninety six right t two is initialized to t one plus four t two is r main step again i will elaborate later then update r okay do a less than equal to comparison here between r t two is r s two is q update p which is in t one do a less comparison t one and s two which is p and q okay 
so this this is this is how the over all flow of control is and now what remains is to elaborate on the main step (Refer Slide Time 27:44) 

so basically we are comparing limits pointed by p and r if p if p a limit or element pointed by p is less than element pointed by r we skip this integer instead okay this interchange i am just um showing it in abstract sense that p and star p and star r are interchanged okay this would require three assignments going through temporary and so on 
i am not going to go into those details 
but how we do interchange in assembly that we had seen in the earlier lecture 
so all you need to do is load the values pointed by p and r into t three and t four okay do this comparison less than comparison i am once again using blt statement which is pseudo instruction um if if i need to interchange all i need to do is simply store the values um in a cross fashion 
so t three and t four the order was t one and t two here it becomes t two and t one 
so just store them in interchange manner and here i am simply Z is a label which i am putting to the staement which i am following it if you go back after the main step this was r plus plus statement or (Refer Slide Time 23:18) add immediate t two t four 
so basically that statement its not that you are putting once more on this indicating that Z is a label which is there just to make sure that you come out of this part alright 
so once you understand how comparison is done and how branch and jump instructions are used the loops can be implemented in a very straight forward manner um another control structure which you often encounter is that you have some value which could lie in certain range and for different values you need to do different actions 
so unlike boolean comparison where you have true and false outcome here you have some expression which you evaluate and the result falls in certain range 
so as an example suppose you have a value k which can be either zero one two or three and small pieces of code you have for each case you will capture it by switch statement okay and with after each you want to break (Refer Slide Time 30:28) 

that means after performing f equal to i plus j you want to go through all of them you want to skip and come to end you put a break statement of course the last one is not necessary but just to make it look uniform i have put it there 
so how do we do that one option of course is that you can make repeated comparisons right you compare k with zero check if it is equal to zero or not do this action then compare with one compare with two compare with three um with the range just being four its not all that difficult but if the case if the switch statement had lots of cases then this will become very tedious and messy 
so there is a more elegant way of doing it by doing a multiway branch by a single instruction (Refer Slide Time 31:22) 
so a new instruction which is jr jump on register content which will be used here 
so we we will do this with the help of a table okay what we have is in a table we have all the starting addresses of different cases okay and then we simply look at the value of k use it to pick up the right address from the table and jump to that 
so lets see how it is done um initially i am making sure that the value k falls within the range zero to three if it is outside then i want to skip right because my table will not have entries corresponding to it 
so assuming that s five contains k first it is compared with zero if s five if k is less than zero then i skip second comparison is made against four okay so if this is greater than or equal to four again i exit okay so label exit would be some where in the and here i have not shown it whats that is done then i need to prepare an index for the table from where i am going to pick up a destination address 
so basically i need to obtain four times k each address in the table is going to occupy four bytes or one word
so here you will see that there is a different way of performing a multiplication by four i am simply doubling the number twice and that gives you four time its value 
so add t one to itself the result goes to t one um i think i should have there should be one more add statement like this i am sorry there is you do it twice this is doubling t one no i am sorry okay okay yeah we are starting with s five first we have doubled s five according to t one then double it further got into t one again 
so we have four k then t four is assumed to have address of the jump table starting address the way we prepared the index for the way we prepared address for the array same thing is done starting address is added and final address is in t one 
so this contents of memory location address by this are loaded into t zero and then i say jr t zero okay 
so so this is this exercise is to jump to an address which is contained in a table and which address from the table we pick up is determined by k okay 
so the four cases we had are now listed here (Refer Slide Time 34:38) 

they are at label L zero L one L two L three and the table is shown here L zero L one L two L three these are the four addresses kept in this table and register t one finally is made to point to the properiat address in that table and we are jumping to that 
so the statement we had in the first case (Refer Slide Time 35:10) 

 now we will call this we have i plus j in one in this case g plus so there is an addition here and another addition there and then two subtractions you will find straight forward code doing that s three and s four i had put it in s zero this is in the other addition subtraction another subtraction 
so what ever code is what ever is the body of the case can be put here it can be one statement it could btehe more statements and finally you  jumped to exit which is corresponding  to the break we have 
okay now this looks nice for a simple case but problems could be offered by will be more complicated if you are talking of values which are not contiuous here we had zero one two three all four values were relevant 
but if you have non continuos values then the table could be sparse you would waste the space and if it is too sparse it may still be bared to do repeated comparisons but if the range is compact that means you are interested in almost all the values of range then the table approach will be fine um in in very specific cases when the when these four labels were let us say equally spaced okay if they are uniformly spaced or they can be or these bodies or cases can be paded up with blanks to make them uniformly spaced then you don t really need that table okay so suppose um suppose these addresses are thousand lets say thousand twenty thousand fourty and thousand sixty they are uniformly spaced alright then instead of table what i need to do is we can i take the starting address thousand plus twenty times k so i need to simply compute this and then do jr with that right instead of looking up these four addresses in the table this is fine this is a general approach if the addresses are arbitary but if they are uniformly spaced like this then i can simply compute the address  by taking a base address plus some constant time k 
so that sometimes that is much more efficient 
okay now lets get back to this range of instructions we have talked of and see how address is being actually specified in these 
so we have so far seen um branch instructions (Refer Slide Time 38:42) 

eq ne comparison  i will not look at blt bge and so on because they ultimately built using beq bne then we have load store instruction which all these used I format where a sixteen bit number was specified and that that was used in computing the address j instruction had different format j format where there is a twenty six bit number specifying the address and we have now today seen a new instruction jr which is actually in R format 
so although R format provides fields for three registers we require only one here others are left unused okay but the format is R and um the remaining these three fields are unused 
so now in these three cases how do we specify how do we really get the address given an instruction and we will even look at these two and lw sw seperately 
so first lets begin with load and store (Refer Slide Time 40:02) 

so imagine a instruction of this type load word r one C r two where C is the constant part and r two is the register register this is also called index register or base register 
so the exact effect of this is that um the memory location which is addressed is obtained by C plus r two these two are added and the thing to be noted here is C is an offset which could be positive or negative right and as a sixteen bit number the value of this offset lies in this range two minus fifteen to two fifteen minus one okay this is in twos complement form if you are familiar with twos complement notation you will immediately recognize this but if you are not we are going to talk of this representation detail later on 
so at the moment you can just notice that the offset could be positive or negative in certain range right and the address which is being specified here is a byte address in beq bne (Refer Slide Time 41:15) 

we will actually have we are worried about word address 
so whereas data could be in terms of bytes or words or double word or half word all those varieties may exist instruction in this architecture are all one word instruction um later on we are going to see processors where instructions could also be of varying size but here we have simplicity that all instructions are four bytes or one word 
therefore the instructions are always located at aligned boundaries the instruction addresses are always multiples of four and that fact is made use of 
so the offset the address which is provided here is actually an offset again positive or negative with respect to the current position in the program  
so in a program if you are jumping four instructions ahead then you will give a positive offset corresponding to that if you are jumping back like in aloop we had you will have an negative offset 
so when you are using assembly language you simply write a table and the label of destination could be after the current instruction it could be before the current instruction assembler will automatically generate a corresponding offset either a positive or negative the offset here is in terms of word its not a byte offset it s a word offset because we know that addresses for instructions are always multiples of four 
so the meaning of this instruction exactly is shown here beq r one r two C compare r one r two and PC which is the program counter that s the register specifying address of the current instruction it becomes PC plus C but strictly speaking notice that there is a plus four also  
so the offset is not with respect to the current instruction it is with respect to the following instruction okay um there is there is a certain reason which is related to the hardware implementation of this that why we are keeping PC plus C plus four and why we are simply why not simply PC plus C 


okay so so its for some hardware convenience which we will see later 
so for the moment you can just notice that depending on outcome of comparison the next value is either PC plus four or PC plus C plus four it should actually be PC plus four plus C plus four because C is a word offset the next instruction follows similarly okay what happens in jump instruction in jump instruction (Refer Slide Time 44:06) 

we are specifying twenty six bit number as destination address but we need a full fledged thirty two bit address finally before we can proceed further 
so the effect of this instruction is as follows that once again PC is incremented to PC plus four okay although we we are not going to the next instruction this happens automatically this happens sort of without in terms of hardware PC gets PC plus four happens without even bothering it s a jump instruction or a branch instruction or something else and after that we see how rest of the address is computed um so these twenty six bits of the instruction which i am saying instruction dot dot twenty five 
so these are the LSB twenty six LSBs of the instruction corresponding to this field they are placed in position two to twenty seven of the instruction of the PC okay bits twenty eight to thirty one of PC are unchanged and bits zero to one are zero and they remain zero okay so now what it means that um using j instruction you cannot jump any where in the memory space you you can specify range which is defined by twenty six bits okay and this twenty six bit number is taken as a word address 
so effectively in terms of bytes it is effectively twenty eight byte twenty eight bits 
so your range of jump is within the current segment of size given by two raised to the power twenty eight or two fifty six MB
so if you if you imagine that the memory is divivded into segments with thirty two bits you can get four GB of memory two raised power thirty two is four GB imagine that four GB space is divided into segments of size two fifty six MB
so your jump is occuring within the current segment so let me illustrate that 
so suppose this is four GB and each of the segments is two fifty six MB 
so if your current instruction is here um you can jump any where in this region of course there will be an exception case if the jump statement was the last statement of a segment then jump will occur in the next segment because of this initial value of PC PC is initially implemented by four and then you are figuring out where to jump 
so typically you will be some where within the segment and jump will occur within the segment if if you are at the last point at the last word of the segment then the word will occur in the next segment okay 
lastly will look at jr which is very simple you are specifying a register which is 
(Refer Slide Time 48:09) 

thirty two bit register all that you are doing is transfering the contents of that register into PC 
so here you are specifying a full fledged thirty bit address and this can be used to jump any where 
so most of the time um j instruction specifies when you are simply jumping to a constant address but if you want to jump to a far off segment for some reason you will have to use jr again we have seen jr used for a different purpose we have seen how jr is used for a switch statement but jr can also be used for carrying out jump when the destination is far off (Refer Slide Time 49:05) 

okay finally let me summarize about what we have learned today i began with examples of some loops how we can use simple compare and branch statements to carry out simple loops nested loops we have seen that when arrays are being accessed sequentially in a loop there are opportunities for improving the code um you you can do more computation before the loop begins so that the loop is a tight loop it is a efficient loop the body is more compact and now whats the importance of these improvement um as i mentioned earlier you are like likely not to program in assembly language in practice 
so what is the point in bothering about these small nitty gritty improvements um these are important from the compiler point of view 
so the way we try to improve the code um that understanding is required for compiler writters because they have to generate code which is efficient and these transformations these improvements um should be kept in mind by code generator of the compiler so that efficient code is generated 
so given the kind of example we took in the beginning actually comppiler can automatically generate the final improved code from the inefficient program we wrote initially 
so that means you might write using index but compiler can always convert it to final form these are systematic tranformations which can be used to do that um we also saw how we can implement a switch statement using a table and jr instruction and finally we have seen in a very subtle manner that how various instructions address the memory there are different ways in which address is computed in different instructions and we have gone through those details 
so i will stop at this point 




Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 5

Instruction Set 
Architecture-3

Today we are going to look at very important programming language abstraction namely function or procedure um this is very important in the sense that you can build program in a hierarchial manner in a top down or bottom up fashion and without this construct large program would be impossible 
the set of instructions we have already learnt is summarized here  (refer slide time 01:17) 

this includes arithmetic instructions and logical instructions which you see in the first two rows when both the operands are registers or one is register or one is in a one is in the form of a constant comparison and branch unconditional jump load store load upper immediate and jump with register containing the address 
so we use this as a means to jump to an arbitary location and also as a mechanism to carry out multiway branch um as you went along we also felt the need of some pseudo instructions and some of the pseudo instructions we have defined are shown here move is something which is frequently used simply moving a data from one register to other register essentially a copy of value is made load address load immediate which loads a constant into a register and some variations of branch 
these instructions are implemented by one or more real instructions 
(refer slide time 02:26) 

if some of these we have seen how they get expanded some of these we will discuss in tutorials 
so um today will talk about what actually is involved (refer slide time 02:39) 

in procedural abstraction to implement a procedure what all we require what activities or what function the instruction has to support um we will illustrate all this with an example i will take i will continue with that sorting example which we had dealt up with the last class and try to do in the form of functions there and finally we will find that they are registers which are although general purpose but certain conventions have to be followed in order to develop a program smoothly 
okay so what actually we mean by procedural abstraction essentially procedural abstraction means that there is a piece of code (refer slide time 03:23) 

which you can write once and use it one or more times thinking of that as a single statement okay so it could be an arbitary piece of code which does the computation which has a well defined well identified meaning and this this becomes your basic operation either single operation or single statement um which can be used with the same ease and convenience as you do for the basic operations 
so here it shows example that there is a main program there are two procedures P and Q and P is being called here this computation is performed then there is a return which is made there is another point here where Q is being called and then there is a return 
similarly P and Q are being called again and there is a return 
so the number of times we use could be arbitary and to implement this we require several things okay several things have to be kept in mind first of all there is control linkage you have to worry about flow of control that means from main program you should be able to call that means tranfer the control okay in such a manner that when the procedure 
(refer slide time 04:41) 

ends the control returns back to the point where you  made a call okay
so there is a linkage which is required it is not simply a matter of using a jump statement a one way and the jump statement other way you need to know where you came from so that return can be appropriately made because call may occur from several different points and ruturn has to be made accordingly so that is a key part here 
secondly everytime you invoke a procedure it may work on different set of data 
so there is a set of parameters of procedures and when procedures called sub data flows into the procedure parameters have been passed and when computation is over result flow back to the calling program 
okay so the parameter which carry values into it and those which carry value back to the caller apart from the parameters which are decided by the caller or the results which are consumed by the caller they are also often local storage decorations which you may have inside a procedural function okay and if you if you call a function multiple times then its considered to be fresh allocation of storage okay so how do we handle that um at the same time a procure may make an access to the data which is defined outside so there could be global data there could be local data and that both need to be accessable um in what i depict depicted in the previous picture was a very simple case where main program and there is a function or procedure which is called but they could be nesting um in the previous case P could have called Q okay Q could be called by the main Q could also be called by P and matter gets complicated further if there is a recursion there could be a direct recursion and indirect recursion that means t could have called itself or P calls Q and Q calls P 
okay so there could be direct or indirect recursion and all this all these issues of control flow data flow organising local and global storage become more complex when you have to take into account the need for nesting and need for recursion 
so lets take the first thing first how do you organize the flow of control 
(refer slide time 07:17) 

i am taking here the same example of a very simple minded sorting program okay where we simply had double loop and the main operation inside the loop was into comparison and interchange 
so that comparison and interchange we suppose define as a function or a procedure okay which i am calling as xchg exchange and now it is basically this exchange with some other overhead which is enveloped in two nested loops okay now what happens at the assembly level um rest of it is same okay the only change is here um earlier what i have done here was that although i have not shown in the same um same screen but this comparison and exchange was basically a set of instructions some seven eight instructions which were placed here okay now we don t place instructions here we put a call instruction or which is called jal jump and link jal stands for jump and link and i am treating xchg as a label okay so some where there is an instruction with this xchg as label attached and the effect of jal instruction is to tranfer the call transfer the control much in the same way as jump instruction does 
but it does one more additional thing it saves the current address of instruction into a special register well it is special in terms of functionality but its one of those thirty two registers which we symbolically denote by ra which stands for return address 
so the effect of this is that dollar ra or the return address register gets the value of PC plus four 
so PC is pertaining to this instruction um ra wiill now contain address of the instruction which is following this is the point where you have to return after completing the prodedure okay 
so this address would be ready in ra register and when you have done with the procedure you can use that address and link back 
so rest of it are not changed and here you can see how call actually has been established 
okay now lets see what happens at the other end 
so that what i call as main step earlier i have encapsulated in the form of a function okay 
so (refer slide time 10:05) 

we are not returning any values so our return value is void there is no parameter being passed its simply looking at global values and doing something with it all right
so i have just added a return statement here and in terms of MIPS language this is same piece of code okay i have put this as a label and at the end i say jr dollar ra okay to same jr instruction which takes contents of a register and uses that as a destination or target address 
so since jl had stored return address in this register you can simply do jr and get back 
so so these are the two instructions which actually provide control flow and linkage of caller and the callee 
okay now lets look at the question of passing parameters okay we have seen how control is let now we see how to take care of data (refer slide time 11:08) 

now in this case what i have done is from previous picture the only change is that i am making p and r as arguments the parameters okay so the function need not look at value which were with the main but it is explicitly passed on p and r the two pointers right 
now what 11:40 is done the simplest method is what is shown here is use some specific registers which are designated for passing parameters 
so the values which have to go into the procedure are loaded into these specific registers in this case um you can see dollar a zero and dollar a one these are the two registers which are the part of set of register from where i can convey the parameters 
so what i have done is i have changed the register which i was using there okay i was using something arbitarily but now i am just making sure that p is passed in a zero and r is passed in a one and rest of it rest of the program is being accordingly modified okay so no extra statement it is just that i am careful about which registers i have to use for this purpose and typically i will avoid using them for something else 
so what happens if you have large number of parameters the convention is that if you have upto four parameters there are four registers designated for it which are labelled as (refer slide time 12:54) 

a zero a one a two a three okay and actually that will cover lots of common cases similarly the values being returned or the output from the procedure or function is through two registers v zero and v one 
so once again this would 13:17 for many common cases um what happens when the number of values going in or coming out is more than four and two respectively in such a case we have to resort to memory 
so any additional parameters which you have you can place them in specific memory locations and the function is restricted to load them from their work with them and the results can be returned partly through registers if they are more they can be returned to memory locations 
so that that is a simple extension of what we have seen through registers okay the next issue was that of defining local storage suppose within the procedure within the function 
(refer slide time 14:05) 

there is an array declaration they are tructures which are defined so what do you do um you can organize each of the function or procedures with its own storage area with that is the data area and the area where its code is kept 
so for example if you look at this picture this is this is the main okay this green one is the data area this is the code area then there is t green is the data area code area for q data and code okay i will just place them one after another in continuous allocation it is not necessary that data has to be before code um any convention you can follow all that i am trying to say here is that each of the functions or procedures has its data and code together another alternative could be that you have an overall data area where you keep data of each function or procedure and then there is overall code area where you have code part of each function or procedure 
so um any of the convention can be followed and of course a compiler would follow always a specific convention and produce the code accordingly 
so it has to process all the procedures um they look at their code part look at their data part and do storage assignment accordingly 
now lets (refer slide time 15:52) 

move to the case of nested calls that means a function can call another function 
so i am showing here an example where this is the main okay at some point you are calling fuction P this is P and there is some computation here some where you are calling Q and this is Q so this is return point of Q this is return point of P how will appear in assembly you have jal P for this call 
so that will bring the control here and at some point you say jal Q the control gets tranferred here okay here you expect that jr dollar ra brings you back here go further you expect that jr dollar ra brings you back here okay that is how you want control to be lengthed but now what will happen when first call occurs the ddress of the instruction here which is after this jal gets stored in ra okay but when you reach this point and if you have another jal instruction then the old address which was there in a array gets replaced by address of the instruction following this okay there is no way you can return to the main and here this call will occur correctly this return will occur correctly but after this point again when you say return the program will get back to this point 
so it will it might get into a endless loop here um and the solution is very simple that s you must take the precaution that the return address which was made available here should be saved some where and before you do jr it should be restored okay so this procedure would have some local storage to that you can add one more location where it preserves its return address all right 
so now in between many calls can occur Q can call something else and so on but as long as P takes care of where it has to be returned um the job would be done you will not make a mistake 
so Q can take care of saving its own so what you could do is the first thing when you enter the procedure huh you save the contents of ra into some memory location and just before call the last thing you do before returning sorry not before call just before return the last thing you do is from that memory location load into ra okay 

so once you have done that in between array is free you can make any calls you may or may not make a call okay and there could be any nesting of calls 
so here is an example um what i have done is that in that sorting case sorting problem the inner loop has been redefined as another function we will recall that it was time to find a minimum and put it at the right place (refer slide time 19:25) 

okay a minimum of certain number of elements 
so it was being passed as um its being passed as two parameters p where which is the pointer to place where minimum value has to be kept and r is the pointer to the area in array from where you need to start scanning 
so r is made p plus one and then you make a call okay so we what min will do is it will go through that loop go through scan up the array from r onwards and bring the minimum value back to location pointer by p so rest is same and now we are left with a single loop here there is single loop call to min um min has that inner loop basically okay it performs that exchange condition conditional exchange compare and exchange update r and keep repeating okay 
so this is as it is this i have not changed this is the inner most activity which you do compare and exchange lets see now how these will be done each of these 
(refer slide time 20:53) 

so the main body um we are simply jal to min okay as they replacement for this and we are making sure that the two parameter okay there is a mistake here there two parameters p and r 
so i am making sure that the two parameters are in a zero and a one so that both caller and the callee understand where the values are to be look looked at and this is the call to min procedure this is the same min procedure and this is how (refer slide time 21:36) 

we have implementation 


so i have added these two instructions here right apart from that it s a simple loop this is the same loop makes a call to exchange updates r compares and goes back 
so this is a simple loop here um that basically forms body of this function all i have done is i have actually padded up with save and load here 
so here i am saving ra value into this location and i am loading it to this location 
so um now please notice here that i have used this load and store some what like a pseudo instruction okay i am not worrying about whether this address call ra save is is a small constant or large constant or how it is to be handled 
so i am just leaving it as it is assembler will translate this into um possibly two instructions okay or two or more instructions which will prepare the address into a register and then with the suitable offset it will use sw lw instruction 
so again elaboration of this we will see seperately um but all that we need to understand here is that ra is getting saved some where and from the same location ra is being restored okay now (refer slide time 23:13) 

lets move a step further okay and go for what is called recursive call which means a procedure directly or indirectly calls itself 
so there is a cycle which is formed and we have basically changed this loop which was inside min into recursive calls okay so um it does not its not necessarily just its not necessarily improving the program but just for the illustration i have rewritten as a recursive call here 
so if r is less than equal to q instead of saying jump to exchange okay i am saying make a call to this function and meanwhile the value of parameter exchange or exchange
so you will keep on calling this and return when you find that this is false okay then r exceeds final value then you start return and r is unchanged so you you will keep on returning keep on finding this false and all chain of return will happen okay so is it are you able to follow this program right now let us see its translation in minutes 
so i took care of saving this and restoring this right earlier i was making this comparison and looping back instead of looping back i am making a call to min again 
okay now the parameters i am maintaining in a zero and a one 
so i don t need to do anything else i simply make a call and i am hoping that um the control will repeatedly enter this and appropriately exit um now what will happen if i do this where do you see the problem [noise] 
yes i am every time i am saving ra value into same fixed memory location right so first time the value which gets stored is value corresponding to call which came from outside subsequent calls are getting generated inside 
so subsequent returns are to this point you see here there is a call and the return takes place here okay so these um the everytime now return will take place here because the original entry point from outside has been lost 
so the solution of this is that i should not lose any value which is saved and in natural structure natural data structure where this value can be saved are is a stack which is a last in first out structure because the order in which calls occur and the order in which returns take place are in a last in first out manner and therefore as i enter into function from where ever call is occuring the return address gets pushed in a stack okay and just before returning i popped the latest one from the stack and use it for jr instruction 
okay so irrespective of how calls are occuring whether it is nesting of calls to different procedures or there is a recursion to the same procedure directly or indirectly you can simply keep on pushing the return addresses into a stack as soon as you enter a function push the return address into stack and just before exiting take it off from the stack and return 
so now the question is how do you do this um there are no direct instructions in which available for pushing and popping (refer slide time 27:25) 

the stack is created basically by using special register called stack pointer so once again it is one of the thirty two registers which is used to implement a stack 
(refer slide time 27:38) 

so pictorially let us say this is a stack and conceptually you can make stack grow towards reducing addresses or increasing addresses 
so i am imagining that lets say address zero of memory is at the top and maximum address is at the bottom so some where i defined bottom of the stack and start building stack towards lower addresses towards zero and this register sp will always point to top of the stack 
so for push what i need is that decrement stack pointer to create space for putting in data and then store the value you want to put in the stack 
so add immediate sp sp minus four um then store word ra at sp now one more thing i like to notice this is just a side observation here is that add immediate also the constants can be positive or negative okay and actually its because of this reason that is probably i had listed subtract immediate as an instruction but actually there is no subtract immediate instruction you add immediate with a negative constant is nothing but subtraction okay so here you are subtracting four from stack pointer and pop is just the opposite 
so you pick up value from the stack as pointed by the stack pointer put it in the desired register and update the stack pointer increase it by four 
so that the value is no longer considered to be the part of the stack 
so these are the instructions which you would use to save and restore the values of return address 
okay (refer slide time 29:42) 

so this has taken care of the control flow in a recursive environment all right we are ensuring that information about return doesn t get lost um what about it 
so when you are passing parameters in a in a recursive call in the previous example situation was simple that after the call when i return back i don t need those values okay so basically um everytime a call is made the fresh values are passed on and what happen to our value i don t have to worry about but you could have a situation where there is some code after recursive call also okay you passed you passed on values p and r but you are still working with them you need to do more operations 
so in that case you cannot afford to lose whole values and the right solution again is use a stack 
so the parameters could be passed through registers if situation is simple but if if you need parameters to be um available even after the nested calls whole parameters you need to keep them in stack so you can save the parameters you can pass the parameters through stack okay um also recall that i mentioned that if number of parameters is more than four you use memory locations 
so now that question also gets answered which memory locations so additional parameters you can simply push in the stack okay so before you call you load the parameter in the stack and when you are inside the function you can take it off the stack stack is also used for allocating the local data 
so now imagine that you have recursive call to a function which has its own local data 
so it means for every occurrence for every instance of the function a new array has to be declared suppose there is a local array 
so with every new call a new array has to be declared and they cannot be all located at a fixed address once again the natural place for them is stack so on the top of the stack when you enter the function you can create you can earmark space which corresponds to a local data so you can create local arrays or all local structures at the top of the stack and before you exit you can clean up that area okay so its no longer required 
okay one more problem one has to pay attention to (refer slide time 32:38) 	

when talking of procedure is when you are writing a program you assume availability of certain number of registers and you may freely use them but when a call occurs control is getting tranferred to a function okay which will again require register for its own computation and when the control comes back to the main the the main program will continue with the computation which was done partially 
so some intermediate results may be available in the registers and if you are not careful you may conflict imagine a situation that main program is written by one person one programmer and the function is written by another programmer 
so either they write in sequence once a finishes tells that i have used these registers you go and use other registers or they could be a fixed vision all right 
so there is a there is a convention which can be followed which ease the task and if if necessary you save the registers 
so if if lets say you are writing the calling program you have done partial computation some results are in the registers you you need to make a call and come back and continue 
so the values which you need to preserve you have to take care that you save them okay so whether caller should save or callee should save again a convention helps in this area and this question this conflict will not be there otherwise you would write programs two people will write program there will be a conflict one would destroy the value of others and then fingers will point to each other 
so the convention which is followed here in MIPS is given here (refer slide time 34:34) 

that registers s zero s one s two etcetera these are called saved temporary registers okay it the caller can assume that it is safe to leave values in these registers and everyone has to ensure that values of these is preserved across calls 
so if some partial results for left in s registers you can be safe if everyone is working everyone is complying with the convention then you can assume that values will be returned and if callee feels the necessity of using registers it will be made in a transparent manner it will save the values before using save the callers value which work left in these registers before using make a make use and then restore those values 
so as far as caller is concerned caller will stick to that assumption that values in these is not disturbed if it is disturbed it is done in a manner that you don t come to know of 
similarly registers t zero to t nine are called simply temporary registers where values are not aspected to be preserved 
so calleee has no responsibility of leaving these untempored okay and on the other hand if caller requires these values to be saved okay if it needs more value than this eight to be saved across calls it can put in these um but then there is no guarantee 
so if there were values in this caller is expected to save them some where safely then make a call and when you come back recover that recover these values 
so these are not preserved across calls and they are saved by caller if necessary right 
so these are the these conventions define who has what responsibility okay in terms of um temporing with the values or touching the values and saving in case required 
so now all put together we have been talking of lots of register names 
(refer slide time 37:00) 

and there is some convention in some cases there is hardware constraint um so here is the summary of all the registers which we have talked about
so starting with zero okay this is ensured by hardware that the value is constant zero in this um okay this is one thing you missed out here dollar eighty is register number one okay it was assembler temporary which is used for expanding pseudo instructions 
so when expanding pseudo instructions requires a temporary calculation for example preparation of address or storing the comparison result then assembler dollar u u at or register one and to normal is expected not to use that because if you use it and at the same time you also using some pseudo instruction you can run into problem 
then the next two are for parameter passing v zero and v one are basically number two and three they are used for returning values a zero to a three are passing values into the into the procedures they are numbered four to seven then we have t zero to t seven these are temporaries s zero to s seven are saved temporaries um for some reason t eight t nine are not continuous with these um then there is a gap there are some registers which are used by kernel again they are reserved um then we have global pointers 
so if you have global data okay which is shared by many function many procedures then that could be in some continuous area and a register gp could be made to point to that and with suitable offset to that register you can access various components of that data sp the stack pointer which i talked of ra is the return address and fp is frame pointer i will describe its use shortly 
so um these are the names which you can conveniently use internally they are dollar zero dollar two dollar four etcetera and that s the usage 
so hardware wise it is ensured that register zero has certain value and jal instruction assumes that value is to be reported in ra okay others are by convention okay so so the fact that we are using these for parameter passing is only a matter of convention the hardware doesn t know this 
similarly that the convention about these about t s and s is again a convention and hardware doesn t understand this stack pointer is again by convention that we are using this particular register for pointing to stack anyone can be used okay once again its an convention um with ra there is a role of the hardware in the sense that you execute jal instruction the return address is put in this particular register okay the return instruction the jr is a general instruction it starts specifically for return we have seen other usages of it 
okay finally i like to show you what is called an activation record or a frame um we have talked abou putting so many things in the start (refer slide time 40:55) 

okay once you come to recursive calls then basically everything um the solutions were everything i mentioned was stack 
so all this information in the stack is organised in a particular manner um which is again a convention and different systems follow different convention 
so what is typically followed is shown here 
so everytime function or procedure is called you create an activation record on top of the stack okay and when you when you return you clear that off 
so as nested calls occur you build these activation records in the stack or the frame in the stack and typically the stack pointer will point to top of the activation record and other pointer which i mentioned other register fp its called frame pointer points to the beginning okay it could point to the first location or may be the last location of the previous activation record okay convention could slightly vary 
so i have tried to put almost everything which i discussed earlier the arguments which are being passed on would given part of this record the return address which is to be saved is saved here um any s registers you need to save they get saved here local data is allocated here 
so this whole thing is the is the frame so a function basically was with this okay this is what it sees as as local local data area and apart from this it may make reference to some global data 
so it will use that will be accessible through gp pointer so through gp fp and sp access is made to all the data within a function now the question would be why we are having two pointer sp and fp um so one might imagine that suppose you have a pointer either here or to either to the bottom or to the top everythinh could be referenced in terms of some suitable offset from this point or that point okay but there is a difficulty in that um particularly when the size of this changes dynamically okay if within the function you are doing any dynamic storage allocation we have not seen at assembly level how you do that but suppose the program is sophisticated and dynamically allocation gets done 
so this moves up so now this can no longer act as reference point accessing the data because we we want the offset to be constant okay typically the way you like to access is load word some constant offset and may be register in the bracket is sp 
so with respect to this pointer the offsets are these are constants okay but with respect to this point offsets may not be constant um we need a pointer to this top to keep track of how far the stack is filled up so that is required in any case but another pointer here so that all these can be accessed with the constant offset is required okay so lets summarize lets go at this point we have seen some of the basic ideas how we create procedural abstraction in assembly language (refer slide time 44:48) 

first issue was that of arranging procedure call and return then we saw how parameters are passed we talked of complications which arise because of nesting of calls and also recursion the solution was to follow lot of conventions and also to use stack um for all storage allocation 
so with that we have seen the basic idea i would in the next class take an illustration and show how a complex recursive procedure can be programmed where you need to do um activation record creation okay i will stop at that if you have any questions i would answer 
(student : can you explain frame pointer ) okay yeah frame pointer um the basic idea here is that we have all the data allocated on the stack okay if if the data was of a size which is fixed when you are writing the program then every every piece of data can be accessed with constant offset with respect to the top of stack okay but if the situation is different that means let us say there is some data structure may be you have prepared a there is a local link for example where the size may grow or shrink as the as the function proceeds so um all that allocation (refer slide time 40:55) of the link record will be on top of the stack therefore the top of the stack will keep changing 
so stack pointer can no longer be used with constant offsets to say for example these arguments or the return value or these registers 
so we have a pointer to the bottom and with respect to that the offsets of all this part is constant all right of course the access to the dynamic part um will be changing and it will not be accessed in same manner for that we would need more complex methods but atleast the part which is unchanged whose location or seat is unchanged during the life of the function we can simply access by constant offset to the frame pointer okay there was some question over there 
(student : stack pointer initially you said we push and pop )yeah (student : you said that it couldn t calculate all the necessary ones pop will be done only once ) um yes actually in aggregate terms the pushes and the pops have to match okay so you might push different things in PC for example you may push arguments then you may push return address then you push registers 
so several instructions may use and stack pointer may be made to grow in many steps um now when you need these things you can use them and you may clear off activation record in one go or again you may do it in parts  all you have to ensure that the space which isallocated should be equal to the space which gets deallocated eventually okay so these have to be matched infact this could be one very very common source of error if your pushes and pops match there could be serious problem in the program 
( student : how much memory is allocated to the stack pointer ) um how much memory is allocated to the stack pointer okay um the memory allocated to the stack pointer stack stack pointer is just one register okay so your question should be how much memory is allocated for the stack um there is no often no fixed memory will be allocated you may start at lets say one end of the memory and allow this to go in one direction okay sometime you have okay let me okay um often you may have in your entire memory you may have a very typical case that you have two areas two data areas which grow in opposite direction okay one is called heap and other is called stack 
so for example if this is stack it grows in this direction grows and shrinks other could be heap which grows and shrinks 
so heap would be used for random allocations for malloc and so on and this stack is used for automatic allocations when calls and return happen and you could actually what you could do is you may make a fixed partition and say that above this is the area for heap below is the area for stack 
but that may constraint sometime you need more stack area less heap area sometimes you need more heap area less stack area 
so a count policy is to not have this line okay and let them grow freely so that total space is available as long as they don t clash into each other your program works successfully
so if if it does then you run out of memory okay so i hope i have answered this question of how much space you allocate 
so you may often not allocate a fixed space and leave it to grow or shrink dynamically okay thank you 




Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 6

Recursive Programs


in the previous class i discussed how sub routines or procedures can be defined and used in assembly language programs we saw the use of instruction like jl and jr which are used to link the flow of control we also talked about convention of register usage which helps in usage of temporary areas temporary words and also parameter passing when it comes to recursive procedures you have to make use of a structure like stack where the information which goes in first comes out last and that is to match the order of cause and returns in the recursive procedure 
so what we have seen that you need to define an activation record which is basically just a layout of memory in stack how you are going to kep the temporary values how you are going to keep the parameters and where you keep the local data um today i will take an example and try to elaborate on these ideas and show and illustrate how we can actually create activation records in order to define recursive procedures then we will see how at the time of function call these records get created and how they can be disposed off when you return some functions (Refer Slide Time 2:31) 


so i will actually do part of the examples and rest of the work would be carried out by you in your lab exercise (Refer Slide Time 2:44) 

so we are calling this picture that you have a main funtion which would call one or more funtions or procedures and these calls could occur at several times you have to arrange for flow of control as well as flow of data between these procedures and the main 
i take an example of um sorting procedure it s a merge sort with you are familiar with um it can be implemented in a recursive manner or non recursive manner 
so just to illustrate the point i will take recursive implementation first describe it in language like c and then we take up some crucial points in that implementation and see their assembly implementation (Refer Slide Time 3:36) 

okay so will begin with description of main function which will as usually take care of inputting an array of integers then call the sort function and then output okay the sort function would be a recursive function 
so X is the array X is the array of dimension m m has been defined as some constant here um this is a input Y would be the output okay 
so m is actually the maximum dimension and in this specific case we will have the dimension as input and that will be in variable N 
so N is a actual size in a particular case um this takes care of entering the value of N which is dimension of the array or size of the matrix then we go on to input the values okay so we are assuming that these are integers and integers are input and stored in X then we make a call to sort so X Y N are three parameters here um then finally you have to output the sorted values (Refer Slide Time 5:14) 

so this is taking care of the output all right 
so it s a very simple and straight forward function you input perform a sort and output i don t think this needs for the elaboration 
lets look at the function the merge function merge sort function would be a recursive funtion and what we will do here is that use some local arrays in which we will keep the um sub matrices sub arrays which are sorted and then merge them 
so a one and a two arekept for this purpose um special case is when n equal to one okay then you don t need to really split the array that array itself is sorted when size is more than one then you split this into two parts sort each of these and then merge the two  
so that s the basic idea if n equal to one we simply copy from A to B else we find out two integers n one and n two okay n one is roughly half of n and and two is the remaining part 
so the array A is going to be split into two halves roughly equal halves of size n one and n two and we sort them one by one okay so we start with A okay so this is the input that is the output output goes to a temporary array and the size of data is n one the second one begins n one words later the result goes to A two and the size is n two 

once you have done this then you simply need to merge the two okay so the parameters are the two sorted arrays A one and A two the result goes to B and the two sizes are n one and n two (Refer Slide Time 7:31) 

so that its the end of the else part and that s the end of the function all right 
so um its really stright forward in terms of its structure one small thing which you should notice which is important here is that we have declarations of A one and A two here and the dimension is kept as m okay c language will not permit a variable size array to be defined  
so we go by the maximum value which is possible so as you keep on spliting the array it will become the split portion will become smaller and smaller you would actually need less and less space but the way we have written we have to define some where else we have to go by the maximum possible size 
so there is a wastage of space but just for the sake of simplicity it has input in this particular manner 
so infact i would like you to analyze how much space is getting utilised in this case okay every time you have a call to sort it creates two local arrays of size n 
now the function merge would take two arrays which i have been sorted and simply merge them in such a manner that the order is maintained 
okay so P and Q are the inputs R is the output and small p small q give the dimensions these are i j k are some local variables initialised to zero these will act as the indices to the three arrays okay i j and k will respectively index and to p q and r 
so here is the beginning of a while loop which will go over the arrays as long as both the arrays are not fully scanned 
so i and j are indices to p and q and as long as these are less than the max limits you keep on scanning and try to find which is the smaller element which is the larger element and pick up the one which is smaller 
so we are creating array sorted array in ascending order 
so if P i is less than Q j you transfer one element from P to k P to R otherwise transfer one element from Q to R okay and also take care of the updation of the indices then you reach a point where possibly you have exhausted one of the arrays okay or may be both the arrays you need to see if there are remaining elements either p or q they need to be passed on to R as they are okay they are already in order and they they go at tail end of R so if i has not reached p yet you need to pass on elements from P to R and if j has not reached q you need to pass on remaining elements from Q to R right 
so this is end of while um now we look at how we implement these recursive procedures in assembly and as i mentioned earlier that how our implementation will be based on defining activation records creating them at the time of call and disposing them of at the time of return 
so lets define the structure for these activation record so first we look at the merge procedure um it has these parameters P Q and R three arrays and two scalars P and Q which gives the dimensions okay so accordingly we will create activation record we also will need to take care of (Refer Slide Time 11:55) 

the local variables so we had three local variables i j and k 
so this is a possible activation records so what we have done is we have placed the parameters here okay there is a return address and the local variables 
(Refer Slide Time 12:19) 

so local variables i j and k parameters addresses starting addresses of P Q and R and the values small p and small q right the order is there is no activity of the order in which i have put things here th order could have been changed but i have just put in the order in which things are appearing 
so what we are saying is that when this merge has to be called we would reserve space on top of the stack which will accommodate all these parameters return address and the local data at the time of return the stack would be shrunk and this area would be inaccessable okay now this is one way um but we can simplify it further you recall that there are four registers (Refer Slide Time 13:23) 

zero to a three which are designated for passing parameters 
so we can also use those registers for passing parameters and since merge is not being called recursively it is good enough to put those put upto four parameters in those registers  
so i have shown a possible way of creating activation record where P Q and R addresses are put in register a zero a one a two we have two more 
so let us keep them in the stack and the local variables i j and k can be kept in registers t zero t one t two etcetera which are temporary registers right 
so with this we have a smaller activation record which needs to take care of 
(Refer Slide Time 14:21) 

return address and two of the parameters small p and small q 
so basically my entire requirement is now split between registers following the usual convention and some thing is still on the stack okay you could arrange to go little bit beyond convention for example use some additional registers because we have five parameters here and convention is defined for only four as far as registers are concerned but i can put in some more registers if they are not required else where to pass on all the parameters through registers 
but let me show you next approach where some thing has put into registers and something is still in activation record 
okay for procedure sort this indicates the requirement we have three parameters here two array addresses and (Refer Slide Time 15:26) 

one integer value and there are there is much more of local data here there are two local arrays A one and A two and two scalars n one and n two 
so we will put all this in activation record 
so at the bottom again (Refer Slide Time 15:53) 

following similar idea we keep the parameters okay 



A address B address and n vaue these are here then there is space for return address now out of the local data A one A two n one and n two um we have put them in this order the entire array A one the entire array A two value n one value n two so these are here in upper half upper part of the activation record 
so we are not initialising these locations its only that you need to keep space okay 
so space has to be allocated here which only means that you simply move the stack pointer there and the function and the procedure will have to explicitly initialize any of this data if necessary 
okay now having defined activation records we need to see how we use it how we create it how we use it 
so first let me show you the usage so here i am showing part of the code part of the code of merge function okay you remember after the main loop there was one loop where you are transfering elements fom P to R after one of the arrays has been exhausted  
so let us see how we will encode this part um keeping in mind (Refer Slide Time 17:41) 

where our parameters are kept and where our local values are kept 
so i am going to follow the second approach where some of the parameters are passed through registers and some are passed through the stack and the location where i j k the local variables are kept is in temporary registers okay so we begin by accessing the value P okay which is kept in second location from top in the activation record 
so if you say load word within offset of four and register being sp basically your stack is stack pointer is here okay pointing to top of the stack (Refer Slide Time 19:09) 

so the address of p is contents of stack pointer plus four so you are basically loading p keeping in register t three where it will be compared with i i is located in t zero okay its already available in a register 
so you can compare i am using a pseudo instruction bge branch if greater than equal to 
so if i is greater than equal to p then i branch of to a point X which will be some where towards the end of the program otherwise it begins loop 
so i need to prepare the indices for arrays R and P (Refer Slide Time 19:50) 


so first i take t zero which is i multiplied by four and add to the starting address of array P 
so which is available in a zero okay we have done this kind of stuff so i will just go through this and should be able to follow quickly
so so now after these two steps multiplication and addition t four has the address of P i okay and we can bring one word from P i into a register t six okay um and we also take care of i plus plus that is t zero is implemented then we are preparing address for R k okay so k is available in t two multiplied by four then add the starting address of R okay now t five has the address of R k the value which was read from array P was in t six here okay now it is stored back in register stored back in array R (Refer Slide Time 21:09) 

the address or the index k is incremented by one right and that completes this particular loop 
so j l will take you back to the beginning of the loop and the label X is where the loop ends 
so basically what i have tried to show here is that you keep this picture of activation record in front of you and you know what is lying where 
so accordingly you can pick the data either from register or from activation record um what ever you are picking from activation record you can actually find out the offset required with respect to stack pointer and access that value  
okay this was how we used once the activation record for merge was created and you were inside the procedure merge i have not described the entire procedure but just took one sample out of that now at the calling point when you are calling this function merge you need to create activation record and now we will focus on that 






so (Refer Slide Time 23:23) 

on the left you see the picture of activation record for merge and here is the statement which is calling this okay i just a minute i am sorry this is this is activation record of sort procedure okay and when a call is made um the activation record of merge would be created on the top of this okay and the values we were trying to put in the registers will be put in the registers 
so so this is the effect which we want to achieve that starting with this okay this is where the stack is at the moment and execution of this call should result in creation of one additional record on top of the stack and filling up of the registers in appropriate values 
so let us see how it is done 
so um a zero is supposed to contain the address of p okay which is now the value which is being passed the array which is being passed as an argument for p is a one 
so a zero should contain actually address of starting address of A one which is top of the stack right
so the register sp contains the address of starting address of A one and that value is being moved into a zero 
so this statement takes care of filling up appropriate value in a zero as a parameter um a one should contain starting address of Q okay and the parameter being passed for Q is A two 
so A two is starting twenty word down after A one or which means eighty bytes so sp plus eighty is the starting address of second array second parameter or Q which we are putting in A one all right  
then the third parameter which is to be kept in A two is R and actual argument is B so B now array B is not here it is a address of B which is here and you can see how how much below it is we have twenty plus twenty which is forty forty one forty two forty three forty four okay so you leave a gap of forty four words or one seventy six bytes and you you are at a place where you have the third argument 
so i think this is not correctly okay i think thethird statement requires correction let me i have put this address of B there 
but we should put the contents of this location there so it should be let me write it down 
so  (Refer Slide Time 27:30) 

please replace this with that you are not storing the value sp plus one seventy six there you need to read that location and what ever is content there that address need to be put in A two all right okay the next parameter is n one which we need to keep in the stack here okay and n two needs to be kept in the stack in the activation record 
so these are available in the previous activation records 
so from here we need to copy them so this is copying it into first we bring it to some register t eight okay and n one is at how much offset from the stack pointer stack pointer is still here okay so twenty plus twenty forty words down the top of the stack which means one sixty bytes 
so we read from offset of one sixty from top of the stack um its brought to some temporary registers t eight and where do we store it at minus eight offset from top of the stack okay so top is still here and we store it at two words above the top of the stack which means minus eight offset 
so look at it carefully we have brought a value from plus one sixty offset and stored it at minus eight offset that means we have read it from here and kept it there 
similarly the next two steps next two instruction will bring the value of n two 
so you take it from one sixty four offset bring it to minus four offset 
so you have copied these two values to create the activation record and stack pointer is raised by twelve bytes (Refer Slide Time 30:00) 

okay so basically we have created activation record of three words two words we have already filled in and after jl instruction when you reach the beginning of the merge function then you will fill up the return address there 
so now you make a call and the first instruction in the merge function would be saving the return address at top of the stack (Refer Slide Time 30:30) 




okay so this takes care of creating small activation record filling in the parameters which will be passed through registers into appropriate registers and when you when the call has occurred when the control has been transferred then you take the return address and put it in appropriate place in the stack okay so this is how the call would a simple single statement which you see in see like this would actually result in that much of assembly code 
is there any question at this stage no okay now let us see how you return from merge function so we will have to dispose off the activation record which is created 
so we had added one activation record on the top and again showing that at the top there is activation record for merge at the bottom there is record for sort and registers are containing parameters and temporary data (Refer Slide Time 32:05) 

so before you could return basically these three statements before returning you must bring the return address into ra okay so that we can simply do from from top of the stack  okay so first load statement loads return address into ra um adding twelve to the stack pointer actually shrinks the stack and basically the activation record is disposed off 
we don t have to fill in zero just bring the stack pointer down and this jr jump register brings the control back so return is comparitively simpler 
okay now we go to the sort function okay and particularly we look at the point where sort is being called recursively from within the function um you would call that there are two calls to sort and we look at the first one of these (Refer Slide Time 32:47) 

so once again i will imagine that there is a activation record which corresponds to the current invocation of the sort function over this we are going to build another activation record i am showing only part of that which corresponds to the parameters okay so when you create a new record you will create a copy of the same thing okay copy of the same activation record but the value is filled in differently  
so now in this recursive call the parameter being passed are A A one and n one 
so these three parameters which i am labeling as A prime B prime n prime would get these values adress of A address of A one and the value n one and let us see how this is done 
so i am showing the first statement of sort function which will save the return address okay return address is being saved um in a location which is one sixty eight offset from top of the stack so twenty plus twenty forty forty one forty two and forty three sorry after forty two you have to store forty two times four is the byte offset which is one sixty eight 
so store return address at um location one sixty eight deep in the stack 
 (Refer Slide Time 34:47) 

now coming to the point which is corresponding to this sort call so here i have to take this address place it here um the address of A one place it at the second position and the value n one place at the third position 
so these two addresses and one value have to be kept 
so first lets pick up the address of A which is lying here so it is a address which is here we need to take um this is at offset twenty plus twenty forty forty one forty two forty three okay forty three into four is one seventy two 
so at that offset we load the address of A into t eight and store it at minus twelve offset which means the top position of the picture um next we look at the contents of stack pointer which is pointing to top of the activation record which means the address of A one that is brought to t eight and stored in minus eight offset that is this position 
then we we look at um value n one which is at one sixty offset and stored at minus four offset okay um let me put down these offset so so that you can  (Refer Slide Time 36:38) 

see things there clearly this is offset zero this is offset forty sorry eighty eighty this is one sixty one sixty four one sixty eight one seventy two one seventy six we don t need this immediately one eighty and this is minus four minus eight and minus twelve 
now you can easily correlate we have brought the address from one seventy two placed it in minus twelve um we have taken this address in stack pointer itself placed it in minus eight 
we brought the value from one sixty offset placed it in minus four okay 
so now the activation record generation is not complete yet we have taken care of the parameters but we need to create space for local data okay and all you need to do is um find out what is the total space required just push the stack pointer by that much value 
so um its one eighty four is the total size right you can tell that two arrays twenty each that is forty one two three four five six um so total of forty six words or one eighty four bytes (Refer Slide Time 38:47) 

so um after you create the space simply tranfer the control to appropriate location okay how do you return from sort again the procedure is basically you need to recover the return address and dispose of the stack 
so i will not dispose the stack dispose of the activation record 
so we are recovering the return address from one sixty eight offset and for while creating activation record we said stack pointer equal to stack pointer minus one eighty four 
now we just do plus eighty four and recover 
okay so this was some portions of this exercise which i have done and i think the others which can be now easily filled up by you 
so you you should complete this assembly encoding of this exercise as part of your lab work okay so you complete this write also the main function so that you can test it and test it for different values of small m small m was the array size okay um the way i have written is i have used indices but in one of the lectures you recall that we discussed how you can make the code better by using pointers or addresses 
so you write a pointer version of this first write in C write same thing in pointers and then write corresponding assembly version 
okay now as you would have noticed that every time a call to sort is made it we are creating a large activation record of size one eighty four okay this was with the assumption that m equal to twenty so basically the the area we are allocating is essentially two times m in terms of words or you could say sorry yeah two times m words or eight m bytes plus the few more bytes for rest of the stuff 
so now depending upon the depth of recursive calls you will have storage on the stack going on shrinking and we would like to find out how much the stack grows to at its peak okay as you go deeper in the chain of calls it grows when you return it shrinks 
so depending upon the array size you will go upto certain depth and based on that you will have maximum use usage of stack at some point of time 
so you can calculate it analytically all right you can also find experimentally for analytical calculation you would need to relate it to the depth of recursion related to the size of array and you can find you can come up with the expression which will describe the maximum stack size used in terms of the array size okay so as a function of m you can also check it experimentally by introducing some additional code um which will try to keep track of the the maximum value or let us say in this case since the stack goes to load address the minimum value of the stack pointer okay you start with an empty stack assign some um base address to the stack pointer and as stack grows the value decreases 
so you can update value internally when ever the stack goes and you can keep track of the maximum size (Refer Slide Time 42:54) 

okay so then i would like you to work on um an improvement what you can do is um instead of allocating an array of size m always you allocate array of size n the actual value which being passed on as parameter um in assembly you can do though do that instead of creating activation record of size one eighty four um since you know the value which is being passed as parameter value of n let me show you what i mean okay here for example look at this call you are passing (Refer Slide Time 36:38) input array this is the space for output and this is the size of the array 
so when a call occurs it is this size which you can use for creating a local array 
okay so you need not (Refer Slide Time 38:47) way space and reserve space as much as needed and see how much improvement is required again you can see analytically you can also see experimentally (Refer Slide Time 44:18) 

finally you can rewrite this function okay so that you don t need to create so many arrays all right you can restructure and it is possible that the space you use is only proportional to the data size okay the way we have the present algorithm written the space usage is more than proportional okay as you will see analytically and experimentally um but you can rewrite such that you do not create array everytime you make a call because here depending upon the depth of calls the space gets multiplied but i would like you to rewrite first first C and then assembly where the space usage is proportional to the data size okay um and do still recursively right 
so that s the exercise i like you to carry out okay i will stop at this point 
so basically just to summarize we have seen that recursive functions entire usage of activation record on top of the stack and crucial decisions are deciding a format or the structure of activation record once you have decided the structure the usage becomes very easy you can you know what data is where um although offsets are constants 
so you can with constant offset you can um easily access data which is on the stack and then what remains is that at the time of call you appropriately create space on top of the stack fill up the values at the time of return recover the return address and dispose of the activation record okay that s all for today 




Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 7

Architecture Space


So far we have tried to understand instruction set architecture by taking a simple example of MIPS architecture and that we have used to look at the basic principles of how things work at a level close to the machine um we would now try to see what is beyond this simple architecture which we have discussed 
so will look at what is called architectural space which means set of all architectures which are which are possible or which have been in existence 
so we will look at main key features which we have learnt about MIPS architecture and see what variations are possible towards and we will look at some examples  
so first we will summarize the key points key characteristics of MIPS architecture um at instruction set level look at all the variations which are typically found in other machines um will look at the these two terms RISC and CISC which stand for reduced instruction set computer and complex instruction set computer and these two represent to um broadly different architectural styles (Refer Slide Time 1:52) 

and mention a few examples will elaborate on these examples in the next lecture today i will just mention some of these 
okay so when we are talking of um this architecture versus other architecture what are the features we are talking of what are the main things which characterize a instruction set architecture (Refer Slide Time 2:16) 

so these things are predominantly what is the set of basic operations so it has to be a set of operations out of which you can build all the computation which you want to solve a different problem 
so these are the basic operations or primitive operations and this is what could be different from one machine to other machine how second point is how storage structure is organized um storage is in terms of registers which are part of the processor and a main memory which is outside the processor 
so what is the number of registers are they registers of different types or different sizes is there some purpose which is specific to some registers that means the registers are general purpose registers or special purpose registers and what is the memory address space what is the range of addresses whether it is accessed by bytes or word or one could mix it to
then in an instruction how do you specify the operands how do you specify their addresses how many of these you have whether you have two of two operands three operands less operands or more operands and so on and finally how do you represent instructions in terms of binary patterns okay how the word which contains an instruction is divided into fields and what is the meaning of each different field [noise]
so now in terms of these predominant features these may in points how do we characterize MIPS architecture which we have discussed so far um what we have discussed is not complete MIPS architecture its let us say the basic instructions 
but that still gives you flavour of what this architecture is about and we should be able to characterize this in terms of these features 
so first lets look at the primitive operations which we have studied 
(Refer Slide Time 4:20) 

they are arithmetic operations we have talked primarily about add and subtract but there are also common operation multiply and divide logical operations AND OR exclusive OR we have not talked of these again in detail but a small set of these operations exist relational operation we have branch an equal branch an equal which does the comparison then we have the slt operation which compares for less than um other operations have to be built using these 
so this is a small set of these operations available then you have branch and jump where flow of control is changed okay so typically you go through sequentially but at some point under some condition or unconditionally you need to go to another point another instruction um to actually represent the logic of the computation or to um procedure linkage okay you need to call the procedure or return from procedure there are instructions for those then there are instructions for movement of data bringing data from memory to registers or registers to memory or movement within the registers      
so these are the these are all the operations we have seen so far we there are also operations which work on non integers that is real numbers and so on those we are going to see 
but broadly this list actually indicates the class of instructions which we have the storage structure is shown here you have sixteen registers as far as instructions are concerned almost all instructions can use any register with equal ease (Refer Slide Time 6:13) 

so you have register numbers from zero to fifteen um sorry zero to thirty two which requires a five bit field to access them and where ever there is register field you can put any registers 
so although we have seen that there are cases where some register play a specific role um so one exception is register zero which always have a value zero you may use it as source you may use it as destination but its value doesn t change it is ensured to be so by the hardware and register number thirty one um which is used for return address by jl instruction okay apart from these there are no other exceptions and any register could be used for any purpose of course there are some conventions which are followed to ease the task of programming particularly the procedure linkages okay but that s the matter of convention and given the same hardware one could follow a different set of convention um there is nothing harder fast about that particular convention  
then apart from these registers there is one special purpose registers which is called PC or the program counter keeps track of the current instruction being executed 
so it always has the address of curent instruction which is under execution um the memory on the other hand is an array of two raised power thirty two bytes or two raised power thirty words okay each word being four bytes and again these are numbered from zero to if you are taking these as words zero four eight and so on with steps of four um it goes upto two raised power thirty minus four 
okay sothis this is what we call as address space that means this is the range of addresses which can be um supplied but physically the whole space may not be filled 
so in a particular system you may have for example two fifty six mega bytes of memory okay it means that rest of the spaces empty and nothing is there um so how much memory physically have depends upon what type of configuration you want to have okay what is specified by the architecture is the maximum you can have  
(Refer Slide Time 8:50) 

okay the next issue is how you access the operands okay and this is called addressing mode we we need to access operands which participate in for example arithmetic operations we also need to specify destination of the results of arithmetic or logical operations and we need to specify targets for branch or jump instructions 
so these things are specified by one or more addressing modes which you see listed on the right column okay immediate addressing mode register addressing mode base or index PC relative pseudo direct and register indirect let me elaborate each of these immediate addressing mode (Refer Slide Time 9:50) 

is the one where a constant operand is put as a part of the instruction and we have seen this with the number of instruction um all the immediate versions of arithmetic or logical instructions like add immediate okay we i mentioned there is no subtract immediate AND immediate OR immediate slt immediate okay 
so all these instructions can work with constant operands which are provided as part of the instruction in in the case of MIPS these constants are sixteen bits uniformly um next addressing mode is register addressing where one of the register field which is a five bit field can specify again i have numbered zero to fifteen take it as zero to thirty one five bit field can specify one of these registers 
so the operand or the destination of the result is in the specified register 
now the first addressing mode which i mentioned the immediate addressing mode is applicable to only operands not for the result 
so going back to the previous one there are three different situations where we are specifying an address okay either source operand or destination result or a jump target um the first one is applicable only for source of operand um this is applicable for source as well as destination okay the destination address is typically put in this field and source is in one of these two 
so um lot of instructions such as add subtract multiply divide slt beq bne AND OR and so on they all refer to operand in registers and many of these also use registers as the destination   (Refer Slide Time 12:21) 

base addressing involves two things a register specified by one of the fields in the instruction and a constant which is another sixteen bit field in the instrution these two values are added and the resulting address refers to some memory location 
so um in principle such an addressing mode could be used for source as well as destination for variety of operation for example one could do addition and specify one of the operands to be in memory but in MIPS architecture we have seen that this mode is available only for load store okay we have load store instruction which make reference to memory but your arithmetic instructions or logical instructions always assume operands to be in registers or constants okay this mode is not available for instructions like add or AND or OR um so this is available as source as well as destination source in case of load and destination in case of store okay next we look at PC relative addressing which is in principle very much similar to base addressing okay the register here is an implicit register PC 
so we do not have one of the register fields specifying a register here but we assume that PC is a register to which we had this constant another difference in these two cases is that whereas the constant in base addressing refers to a byte offset okay the constant in PC relative addressing refers to a word offset 
okay so strictly speaking this is actually multiplied by four and then added to this address okay so because it is if you are saying this constant is hundred basically we mean an offset of four hundred bytes hundred words or four hundred bytes 
so if this is a thirty two byte address we need to add this constant after multiplication by four 
okay so in principle these are same but there are these two subtle differences and whereas this refers to data in memory this refers to an instruction in memory that that s the difference okay (Refer Slide Time 14:57) 

then we come to direct addressing the meaning of direct addressing is that the instruction specifies address of the source or destination or the target okay but here we do not call it direct addressing we call it pseudo direct in the sense that the address field in the instruction is not the complete address in itself it needs to borrow a few bits from program counter this is a mode which we have in jump or j instruction or jal that is jump and link instruction 
so as you would recall in these two instruction there is a twenty six bit constant field which together with four bits taken from most significant end of PC form a thirty bit word address and that suffixed with two zeros forms a thirty two bit byte address which is used to access an instruction the next instruction is accessed by this address okay a stricly speaking direct address would mean that entire address is coming from the instruction directly 
okay finally we have register indirect addressing which is in jr okay jr is the jump on register um the field in the instruction specifies the register and that register points to an instruction where you need to jump 
okay so there is only one instruction which uses this in MIPS 
so this is the these are the addressing modes and you would notice that each mode is applicable to specific instructions as we will see later there are cases where um specification of mode and specification of instruction opcode can be done in totally independent manner that means there are a set of modes they could be six or eight or twelve or sixteen and each mode is applicable with each instruction 
so they are too completely orthogonal parts of the specification but here that s not the case okay finally the last feature we wanted to focus on was the encoding the way instructions are represented in the machine(Refer Slide Time 17:28) 

so we have seen exactly three formats which were called as I format J format and R format um I format has a provision of sixteen bit constant J format has a provision of a twenty six bit constant and R format has no constant but it has three address fields okay so the most common format the largest number of instructions actually follow R format i have not listed all of them when i say add it means all arithmetic logical and comparison instruction they will fall here J is very limited j and jal I is also used by several instructions 
so many of the instructions which are of R format type they have their I version not all but several of them 
okay in summary (Refer Slide Time 18:33) 

what do we say about the MIPS architecture and its important to summarize this because from here we will take this as a reference point and see what other things are available 
so firstly all instructions are same size okay all instructions are four bytes or one word um they are very limited number of instruction formats we have just seen that there are only three formats there is a fair number of general purpose registers thirty two in our case with very small exception um the set of operations is fairly simple and the thing to be noted here is that each instruction tries to do just one thing when i say that what i mean is that an instruction will either doan arithmetic operation or a logical operation or do comparison or do memory access or it will do control flow okay branch or a jump um there is no instruction which tries to do more than one of these things you would notice that in conditional branch instruction we had beq and bne 
so there is some comparison being made okay but as we will realize later when we discuss um their implementation comparison for equality and inequality is much simpler hardware wise as compared to comparison for less than or less than equal or greater than and greater than equal okay so there is a case where some comparison is being done with branch but the comparison in this case is very simple we are not doing any arbitary comparison and branching with the same instruction 
so deliberately each instruction perform one very simple instruction um there are limited addressing modes okay so with each instruction there is a fixed addressing modes and each mode is applicable to specific instructions and there is no orthogonality um along with a wide number of registers we have provision for specifying three fields in many of the instruction which perform arithmetic or logic operation 
so source one source two the two sources can be specified independently and so can be the destination okay now what are the constrasting architecture where do things change if we pick up another processor where are the changes which are likely to happen 
(Refer Slide Time 18:33) 

the common feature that there is a program counter which runs sequentially through instruction is the basic idea of store program computer which you will find everywhere 
so the difference comes in some of these features which i had mentioned 
so firstly in terms of operations there are processors which define very complex operation has single instruction 
so here are some example you could have an instruction which does this it takes a variable in memory increments it compares it with some value and branches to some target if the comparison succeeds okay so you have memory access arithmetic which is incrementing comparison and branching all happening in single instruction 
so it does make a logical sense to have it and um with that understanding there are processor which provide such instructions okay or you could look at other operations which are commonly encountered for example copying a block of data from one area in memory to another area in the memory 
okay so um there are processor which have single instructions to do this and goal of including such instructions in your instruction set is to make the program compact and shorter okay you should be able to do same computation with less number of instructions but the danger the negative side the flip side of this is that the instructions may become slower the you may not save the time on the whole on the contrary it may make the machine slower either by um slowing down the clock cycle okay as you know that each processor works with certain clock when you say a two giga hertz pentium that means the basic time reference is a two giga hertz periodic signal and operations occur with um lets say each individual cycles of that clock 
so trying to include more complex instruction in the set may have a negative impact on the clock frequency okay so if fast clock is possible if you have simple instruction in addition to this or alternatively this can have impact on the number of clock cycles required for each instruction 

okay so as we are going to see in one of the subsequent lectures that performance does depend on these two factors the rate at which the clock ticks and the number of clock cycles which are taken by instructions on the average 
so it say um it s a trade one has to perform right you have to have instructions which are sufficient for doing any computation but fortunately that universality comes at very small cost okay as you see in the logic that um AND OR and NOT together can implement any logic 
similarly with very small instruction you can actually express any computation 
so its not a basic necessity to have very complex instructions and any decision to include a complex instruction when you are designing an architecture should be based upon what is its overall impact on the performance does it actually improve performance or does it make it worse 
okay one crucial factor is where the operands of instruction are located okay in MIPS we have seen that operands  (Refer Slide Time 25:32) 

were instructions like add are always in registers um it is faster to access operands which are in registers as compared to accessing data or so in a time in memory okay it takes much longer there and therefore instructions which work with register work faster that s the philosophy behind restricting arithmetic operations to um register operands but there are other architectures which do not necessarily stick to this idea um they are architectures which are called RM architecture where one operand typically comes from register one comes from memory or MM where both comes from memory and there are those which actually deal with a mixture of these which will support RR operations RM plus mm all possibilities do exist 
okay so RR refers to instruction instructions where you have operand both operands in registers 
so this also this issue is also linked in somwhere  to the previous one they are not independent um so restricting the operands to register is also from the philosophy that you want to separate out memory access and arithmetic and do them different instruction and the fact that registers and registers can be accessed fact is the second factor which is in favor of having RR type of instructions (Refer Slide Time 27:15) 

okay the next question is about the number of operand fields um well when i say operand i mean operand sources as well as destination for the results 
so according to this number the machines or architectures get classified as three address two address one address or zero address 
so the architecture we have studied is basically a three address architecture because the main computing instructions have three fields for specifying two operands and one destination 
so which means that we do operation like this r one equal to r two plus r three and these three registers in general could be different okay you can choose for two or more of these two coincide but the architecture allows all this three to be different then you have two address machines where typically the result replaces one of the operands okay so commonly you will have r one equal to r one plus r two that means its basically r two getting added to r one that s how one could interpret 
then you have one address machines where you specify one address and the other thing becomes implicit so acc stands for a special register called accumulator so many machines have a special register which is always participating in such instructions as one of the source as well as destination of the result 
so the instruction needs to specify only x and acc is assumed 
so therefore instruction become one address instruction finally there are they have been some real examples of what we call as zero address machine where instruction does not specify any source or destination all are implicit for example the so called stack machines where the operation is always performed on operand which are lying on top of the stack and the these operands are removed the result of the operation actually is put back on the stack 
so instruction does not say anything you just say add and implicitly pick up two values from stack perform addition um put the result back um a prosessor an architecture may have actually a mixture of these instructions 
okay so but we we when we classify the machine as whole we go by typically what the bulk of the arithmetic instructions do it 
so in case of MIPS for example the arithmetic and logical instructions are basically three address instructions um whereas if you look at simply a jump instruction okay that has need for only one address if you look at beq instruction that again is specifying three things two operands being compared and one target address 
okay the next point is about register organization (Refer Slide Time 30:44) 

we have seen MIPS architecture which has thirty two registers which is a fair amount fair number of registers there are those which have few registers lets say eight registers only 
so the number is very limited um they are also extreme cases where you have a single register which is called accumulator and you cannot really hold much of data in one register you can you only have the data which is currently participating in operation 
so everything has to basically come from memory eventually and go back to memory you also have registerless machine the stack organization could be of this kind 
so registerless which means zero registers accumulator based machine which means one one register there are also cases when you have when you have more than one accumulator 
so two or four but not very large then machines which have not too many registers but each register has its own special purpose okay then MIPS like architectures then there are those which have much larger number of registers for example as many as two fifty six registers um SUN SPARC is an example of that but there these registers are divided into groups each forms what is called a register window at a time you work with one window but you can switch from one window to other window and we will se what is the purpose of this and how this kind of switching is done 
okay we have seen some addressing modes um there are instances of many additional addressing modes which machines have and more over  (Refer Slide Time 32:37) 

the orthogonality between addressing modes and opcodes is also seen in many cases 
so we talked about pseudo direct for example there are architectures which have direct addressing mode the entire address come from the instruction 
so of course instruction size becomes crucial here if your address is thirty two bits and you want to direct address instruction the instruction size has to be um more than thirty two okay indirect address we have seen in case of jr instruction that is specifically called register indirect that means the address which you are interested in is kept in a register um there are also machines which support address is being kept in other memory locations 
so that becomes that is simply called indirect and meaning there is at um you are making access to memory first from where you are picking up address and then making another access 
so of course its complicated but there are examples of that i talked about base addressing where we have a register that can be called the base register and there is a constant offset which is added to that um in principle it is similar to i mentioned PC relative but there is another addressing mode which is similar to this called indexing mode um it is just a matter of the interpretation here the idea once again is that a constant coming from instruction is added to contents of a register to get the address um the but the interpretation or the perspective is little different in base addressing we are seeing that the register contains the data value the base and there is an offset over that whereas in indexing mode interpretation is that constant is the base and register index is over that 
so for example there are processor where you would expect that the starting address of the array is provided by the constant 
so constant field has to be large enough for that and a register contains the index into array so if you are accessing ai the starting address or the address of a zero is the constant part and i is in a register whereas the way we try to access an array our case was that um the starting address of the array was in a register which you call a base register and the offset corresponding to a constant index was in um in the constant field okay but if both in our case if both are large then this doesn t work we have to actually do address calculation separately 
okay where many times you perform a sequential access to data in the memory okay either in increasing order of address or decreasing order 
so when you have a register which is providing the address it may be natural to automatically increment or decrement the address 
so this could be done by more which is called auto increment or auto decrement what it means that everytime you make an access okay it is understood that address has to be made ready for the next access okay either by incrementing or decrementing depending upon which way you are moving um with this there comes a variation of whether it is a pre decrement or post decrement or pre increment or post increment that means whether you do incrementing or decrementing before making a memory access or after making a memory access 
so that that variation also has to be catered for and many architecture is provide for that then stack addressing which actually could be considered as a special case of auto increment and auto decrement okay you are using a register to make memory access and also with every access you are incrementing or decrementing 
so there are processors which provide a stack based addressing with auto increment and auto decrement um now one thing which must be noticed here is that when you provide something which is complex um if you try to take care of all generalities it becomes really very complex 
so for example with increment and decrement there would be an issue of whether you want to have increment or decrement of one when you are accessing a sequence of bytes or sequence of textual data characters for example or increment or decrement by two when you are accessing half words or four when you are accessing full word or four when you access full words or eight when you are accessing double words lets say for floating point numbers 
so um if you try to provide all this generality and an option within the addressing mode that any of these could be specified then it becomes very complex in stack for example we have seen that its not just increment or decrement we require with stack pointer sometime we were incrementing by one sixty four or decrementing by one sixty four depending upon how much allocation or deallocation we make on the stack 
so um specific provisions for comon cases are helpful but complete generalization may be difficult at times 
okay so now after having looked at all these variations we come to the concept of RISC and CISC RISC and CISC RISC stands for reduced instruction set computer and CISC stands for complex instruction set computer 
so this is a RISC was a term which was coined in early eighties by henassi and patterson um in contrast to the most popular machines of the day which existed at that time and they were called CISC because they had very complex instruction set and the argument in favor of RISC was that it this approaches one which can lead to better performance 
so the main features are what we have seen in MIPS is that there is a uniformity of instruction in terms of size is and a limited number of formats simple set of operations and addressing modes and register based architecture with three address instructions um what are the implications of these (Refer Slide Time 39:27) 

choices on hardware implementation and performance we will see in detail later on but these ideas were propagated basically targeting for achieving high performance at comparitively lower cost 
so basically with a machine called architecture called RISC um there were RISC one RISC two and so on they were designed at you see berkley by patterson and contemporarily by henassi architecture called MIPS was designed what we are studying is the MIPS architecture 
so MIPS became a company later on when they were the company which took up this architecture and they were various versions of MIPS processors and they have they are in general purpose computing application also in some video games and so on um on the other hand the basic ideas of RISC architectures developed berkley found their place in SPARC architecture which was taken up by SUN 
so in fact beginning with eighties all the new architecture developments new architecture which would design one of RISC type and of course the CISC architectures do continue today and the one which is most popularly used is the intel x eighty six architecture which is of a CISC kind but because of historical and commercial reason describing 
so lets let me mention a few examples  (Refer Slide Time 41:32) 

we will go into details of these in the next lecture but let me just mention 
so SUN s SPARC architecture is has its roots in RISC architecture of berkley um HP s PA RISC PA stands for precison architecture that s the name of this architecture from hewlett packard motorola developed what it called power PC and DEC which was actually a leading CISC machine manufacturer um in eighties came up with alpha architecture 
so all this are RISC architecture and its not only that modern architectures are RISC but the example of this architecture goes back to sixties actually um CDC sixty six hundred which was available in around sixty four or nineteen sixty four the term RISC was not coined there but many feature we talked of today actually could be found in this machine which was a very high performance machine of that time 
okay the classical example of CISC architecture is VAX which is from this company DEC which stood for digital equipment coorporation 
so VAX (Refer Slide Time 42:59) 

um had its history in terms of most popular mini computer called pdp level and this had very complex set of instructions the instruction size could vary from one byte to fifty four bytes 
so you could see the extent of non uniformity here right and hardware which has to interpret instruction such a wide range actually very complex objective of this machine was to have very compact code because at that time the programming some crucial components of the program were often developed in assembly language particularly in system program operating system many critical parts of the os typically used to be written in assembly of course now almost entire operating systems are written in high level language and considerations are different 
so the idea was to make assembly language powerful and easy to use um but at the same time what happened was that compilers found it difficult to use um the entire set of if you have several hundreds of instructions to generate optimal program generate good code trying to make best use of all the instructions was difficult for compiler 
on the other hand with simple instruction set compilers are now able to generate very efficient code and unless we are talking of small 44:40 programs um compilers can produce machine code which is much more efficient by hand written code when it comes to code programs of substantial size um other classical examples of CISC machines are six eight zero x zero series from motorola 
okay so starting with sixty eight thousand and sixty eight ten twenty forty sixty and so on um intels eight zero x eight six which has very long history and one could one can see that um some of the feature which were there in four bit version the first microprocessor four thousand four then eight bit processors came then sixteen bit processors now thirty two bit but some features got added over and no place the architecture has been freshly redesigned so carrying old baggage is being carried on which makes the architecture some what clumsy and hard to understand and discuss um but this compatibility of code that means you take up code which is used to run eight zero three eight six twenty years back and you you can possibily still run on modern pentium 
so that compatibility has helped this commercially and more money means you can pump in more investment in technology and um have high performance 
so these are very high performing processors today (Refer Slide Time 42:59) um not necessarily because there is an elegant architecture or a beautiful architecture but because you know the technology has been so perfected that you can make things run fast 
okay so lets close by summarizing a few things um (Refer Slide Time 46:45) 

so we have seen that instruction set complexity is one of the main issues and this influences the performance by impact in the clock frequency and the cycles you require for each instruction execution 
so we are going to elaborate on this later on how this actually effect when we go into details of the design um some good design principles which we have seen so far are that simplicity requires that you have regularity and uniformity um smaller is faster 
so if you are accessing smaller structure working on smaller operands it always is faster 
so although uniformity is most desirable but you need to make compromise and make a few exceptions and good not to make too many exceptions um then it pays off to make a common case more fast 
okay so something which is done lets say ninety percent of time if you can focus your tension on that and make that fast as compared to that remaining ten percent the efforts would be well spent 
okay i will stop at that 




Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 8

Architecture Examples


We began understanding instruction set architecture by taking example of MIPS processor which is conceptually very simple 
in the previous lecture we tried to go beyond MIPS and saw in what directions architectural developments have taken place to bring in different variety of features um i also mentioned about RISC and CISC stacks of architecture where in one case there is emphasis on simplicity and efficient implementation whereas in the other case the emphasis on providing powerful features for the programmer 
today i will take some specific examples it is quite time consuming to take any architecture and study in detail but what we are going to do is just look at a few salient features particularly those features which are different from what we have studied in case of MIPS 
so (Refer Slide Time 1:57)


just to recall RISC or the reduced instruction set computers um basically deal with instructions which are simple and emphasis on uniformity of instructions there are very few formats and all instructions are expressed in terms of those the instruction size generally the same 
each operation tries to do one simple thing at a time and there is no attempt to do many different tasks within same instruction the architecture typically provides several registers um and each instruction performing arithmetic typically takes three operands two for sources and one for the destination some of the examples of RISC style of architecture i mentioned are SUN s SPARC HP s PA RISC motorola s power PC DEC s alpha and even one earlier machine of nineteen sixties namely CDC sixty six hundred followed that kind of style 
so out of these um will look at some more details of SUN s SPARC and also motorola s power PC 
on the other hand (Refer Slide Time 3:22) 

um the numerous examples of CISC particularly machines of earlier times seventies and also some of eighties some of the seventies machine which grew to later machines VAX is the climax of such development where the attempt was to have very compact code minimize the code size and make it very easy for assembly language programmer the variation in instruction size could be as much as one to fifty four bytes um another example is of motorola sixty eight zero x zero okay again there is a series of processors starting with sixty eight thousand sixty eight thousand ten and so on and intels eighty x eighty six starting with eight zero two eight six onwards and these machines have their origin the idea we needed from earlier sixteen bit eight bit and four bit macro processors 
so will look at VAX and intel eighty x eighty six (Refer Slide Time 4:36) 

so starting with power PC architecture um this was in effort of more than one company including IBM motorola and apple 
so motorola was primarily the semiconductor manufacturer they have been along the intel they had been in the business of macro processor development right from early seventies and that time main rival of intel apple started with small desk top personal computers and IBMs experience was in mainframes and later on PC s which is based on xcxc series 
so these three major companies with some what  different experience got together in ninety three and defined um an architecture called power PC um his had its origin in another RISC architecture which came from IBM system six thousand and that was essentially a machine used for servers 
so although initially power PC was thirty two bit architecture um their later versions in late nineties were of sixty four bit kind also okay particularly nine hundred series 
so this has been used extensively in machintosh systems and also there are embedded versions which find a wide variety of applications 
there are lots of similarities between MIPS and a power PC architecture roughly similar principles are followed but power PC goes little further and has actually presented some instruction which are different from MIPS particularly in branch instruction area they have lot of variety 
so like MIPS they have thirty two registers (Refer Slide Time 6:48) 

apart from that there is  condition register 
so condition register is essentially register where different bits have different meaning and they are set or reset depending upon previous arithmetic or logical operations 
in case of MIPS we have seen that there is an instruction slt which does comparison and the result is put in one of the general purpose registers but in power PC and several other machines um the instruction result of instruction for example comparison or even arithmetic is put in special bits of a register or flags for example when you are comparing one bit of the condition register would carry the information about the result whether A was less than B or not less than B or even if simply subtraction is done or addition is done whether the result was positive or negative or whether the result was zero or non zero whether the parity of the result was odd or even 
so there are lots of such conditions which are defined and they all combine together in a special register called condition register there are two other special registers which are worth mentioning one is the link register okay so unlike dollar ra we we have seen in MIPS there is a special register um where link addresses are stored and this has a double purpose not only for procedural linkages it can also be used for loops 
so for example when you are beginning the loop you with the help of the instruction you can have that beginning of the loop address stored in this register and at the end of the loop you can simply use this information to jump back you don t need to specify explicitly the address but you can simply refer to link register 
so it is also used to jump back to the beginning of the loop (Refer Slide Time 6:48) then there is another register called count register which can be used to store iteration count for a loop 
so there are special instructions which will allow this to be incremented decremented and tested so that facilitates execution of loop where the iterations are driven by a count 
so lets look at some of the instruction formats of power PC and also associated addressing modes (Refer Slide Time 9:22) 

so you would find again lot of similarity in the sense that there is a six bit opcode and five bit register filelds 
so the first one which you see here is quite similar to the immediate addressing or the base addressing which we see in MIPS so exactly same format um again the base addressing is used for memory access and immediate addressing is used for arithmetic and logical operations to provide an operand then register addressing is slightly different in terms of format the idea is same but the format is different um once again since opcode the main opcode field is six bits you often need to extend this you need to have another few bits which will actually help you in expanding the number of instructions 
so a group of instruction will have same opcode and they will be distinguished by opcode extension 
so that field is here and the size of this is could see sixteen minus five and minus one so this is a ten bit field here okay now in this you notice a one bit field R which actually controls whether the instruction is to make an effect on the condition bits or not okay 
so you can same instruction you can um enable condition setting or you can disable condition setting by by choosing one or zero in this particular bit 
so what might happen is that you may set condition by some instruction and you may not be requiring to test it immediately there may be few other instructions and then you may test it later 
so what you would like is it the instructions which are in between should not change the value of the condition codes
okay so therefore you need a control where you can precisely allow or disallow an instruction to modify the condition bits there is additional mode which is used for accessing memory is called indexed addressing um you would recall that i mentioned in context of MIPS that base addressing and index addressing could only be a matter of different interpretation basically you have a constant part and something which is coming from register and so basically the two are added but how you interpret the two components is different the indexing here is taken little differently that you have base register as well as index register both specified and these two register contents are read 
so one carries the base which for example could be starting address of an array and the other carries an index which would be corresponding to the index all right and now since you may be accessing array of bytes or array of words you need further control 
so there is a field here um which specifies what is the size of your data when you are indexing you need to multiply it by certain constant indicating whether it is byte or half word or full word or double word and so on 
so information about size  (Refer Slide Time 9:22) um the the sign of the offset whether you want to add or subtract and after addition after adding this index do you want to update the um base or not 
okay so something like auto increment auto decrement where address gets modified and the modified value goes back to the register so there is a control on that this particular field actually carries information about these three aspects and makes it very versatile um these are a few formats (Refer Slide Time 13:46) 

used for branch instructions 
so there is a long immediate in the sense that there is a long constant apart from six bit opcode and other two special bits rest of it is available for a constant 
so that means a twenty four bit constant can be used in instructions like jump 
so this is an this is used for unconditional jump instructions and um this one next one has a smaller constant 
so it s a fourteen bit constant all right so fourteen bit displacement can be specified um the bit A in both these cases indicates whether this displacement is absolute or its PC relative 
so whether this has to be added to PC to get the final address or this itself is the address all right so that you have the control and L indicates whether you are making a link to a subroutine or not 
so basically transfer of control can be made to a subroutine by any of the branch instructions unconditional as well as conditional um by setting this bit to one 
okay so as you have seen the difference between j and jl the only difference is that the return address gets saved in a specific register 
so um now this facility is available with any of the branch instruction for example even with beq so you may make a call to a procedure if two values are equal or unequal 
so you may actually chck a condition and make a call to a procedure 
so this is indirect jump if for example we had jr there 
so here you make make a control transfer based on the contents of link register and count register 
so count actually also apart from having count in some context can also be used as address so here a few examples (Refer Slide Time 15:58) 

which will show how some of these instructions will shorten the code which you write 
so on the left side you see sequence of instruction in MIPS and on the right side corresponding instruction in power PC 
so in MIPS for example we often require address to be computed by adding two registers and then we use that in lw
so here we are typically um we are wasting this constant field okay we don t use this in any case because the lw instruction does not have a mode where two registers can be added but with this index index mode you can have a base and an index both added by the instruction itself then next is the example of update um here for example in MIPS you will say that you are accessing memory with s three offset is four result is brought to t zero and then you update s three by four possibily for the next iteration but here with lwu which is update version there is lw and lwu means lw with update 
so this i am sorry this is dollar s three 
so s three is being used with this offset and that offset also updates it 
so next time s three will be four more than what we have in this um here is an example of using a counter 
so in MIPS you will have a counter maintained in a register lets say t zero which you update and you are checking um for example you could start with a negative value and you could increment till it becomes zero all right you are testing if it is not zero you go to beginning of the loop in power PC there could be a single instruction which is branch and count to loop and the condition being checked is count is not equal to zero 
so again lot of conditions could be specified here but this instruction in particular is saying that branching is to be done when counter is not zero 
okay the next example of RISC i am taking is SPARC architecture 
(Refer Slide Time 18:46) 

the term SPARC stands for scalable processor architecture okay and scalability here means that same instruction set could be um implemented in different ways with different technologies and the architecture would scale 
so with same instruction set you could get different performance also scale to a higher word size 
so infact this architecture also has been scaled to sixty four bits um and this is an open architecture which is been which is implemented by several companies sun was the main one texas instruments toshiba fujitsu cypress tatung etcetera um this has again a limited number of instruction formats 

(Refer Slide Time 19:38)  

but certainly more than what we have seen in MIPS there is a two bit opcode 
so which is a which is some what  peculiar and clearly this would need extensions because you can only specify four different patterns here um there are opcode extension fields here as you see op two here and in this case an op three in this case and in this case 
so this is a three bit extension field and this i think is a this is probably five or six bit extension field right rd rs one rs two these are again register fields and size is five bits um you would see that constants are of various sizes thirteen bit twelve bit in these two cases and thirty bit here 
so this would be an uncnditional jump instruction and you could see that you get full fledged thirty bit address here 
okay so which means you can address if you take this as a word address you have complete addressability all right you don t have to take bits from PC 
so when in thirty bits you can address entire memory space okay that s the reason why opcode field has been squeezed to two bits and then exceptions are made and extensions are made this is for conditional branch this is for um also for a branch where there is a displacement of twenty two bit with reference to um PC here you have format for immediate instructions you are limited to thirteen bit constant okay if if the operand if the constant is larger then you have to load it separately in a register and then work and this is a typical um three address arithmetic type of instruction 
(Refer Slide Time 22:09) 

the most interesting feature which was brought out by SPARC architecture was the concept of register windows 
so what what they have is there is a large number of registers which could be as much as one twenty eight to two fifty six or even more in different implementations actually the size differs but a program sees only thirty two registers at a time 
so so to say it has a window of thirty two registers at any time which can move up or down as you go from one context to other context 
so we have seen that in MIPS when a procedure call is made you have to worry a lot about what register is used by caller what register is used by callee some how you have to share the registers if you if you have a larger requirement then you need to do lot of transfers to memory save them and restore them 
so it is this problem which is addressed by register windows that when you make a call to aprocedure or function you get a fresh set of registers okay so that the caller and the callee can work independently on their own set of registers 
but this is another extreme you also want some linkages some flow of data between caller and callee in terms of parameters 
so so therefore the two windows are not totally disjoint they are overlapping and overlap is in very regular fashion 
so for example suppose you are in the main program you will see total of thirty two registers and these are divided into four parts um input local output global 
so as the windows change the global registers eight of them remain unchanged 
so you see same eight registers which are lets say out of array of two fifty six registers the first state always part of the window and local are those which are totally exclusive to a given context whereas in and out are overlapping 
so when for example from this context you make a call to another function then the set of eight registers which was out here become in for that so essentially this is a way you pass parameters what ever information upto eight values which you want to pass on you can load into these eight registers and then when you make a call the window slides you get these twenty four registers the globals remain same and what was out has not become in because this is a these are the registers which have carried information into it and what appears is out will become in for the next level 
okay so out of these twenty four registers eight are used for getting information in eight are purely local eight are used for getting information out to the next level and of course this is convention once you have got some value into in registers you can use them for local usage also right 
okay so at any given time you have these twenty four or some other twenty four registers and eight global registers [student : what does CWP stands for ]
yeah i am coming to that um now to keep track of current window you need a current window pointer or CWP 
so um suppose CWP or the window pointer is pointing to this window next it will point to that window when return occurs it gets back to the same thing 
so suppose you have one twenty eight or two fifty six what ever be the number there is a finite set of windows which are possible and if depth of calls become more than that um then actually windows act like i mean this set of registers acts like a cyclic buffer okay
so you can keep on going deeper and the window keeps on sliding but of course as it loops back the old window the first one have to be saved in memory otherwise it will be overwritten 
okay so that saving and restoring has to be done but when you go to a depth of lets say eight or so then you will have to worry about saving or restoring um for most practical purposes the depth would typically not be more than four five or six and often these windows are fives 
so its only in rare cases that you need to spend time and efforts in saving and restoring the windows otherwise call to a procedure is speeded up by this facility 
okay next we take example of VAX architecture  (Refer Slide Time 27:11) 


which was developed by DEC or digital equipment coorporation in nineteen seventy seven this was a successor of PDP eleven a mini computer which was lets see the most successful mini computer and all the PDP eleven was sixteen bit machine this was VAX was a thirty two bit machine 
so this of course has been now discontinued um DEC started with architecture called alpha which is a RISC architecture of course the company DEC is no longer it first was taken over by compaq which was a predominant PC manufacturer system manufacturer and then compaq itself was taken over by hewlett packard so all that is history un VAX brought in the concept of virtual memory okay which actually gives you illusion of a larger memory than what you have physically okay we will discuss this topic in detail later on um it has sixteen general purpose registers okay unlike thirty two in MIPS or SPARC or power PC which we have seen this is only sixteen and out of these sixteen one of them is PC okay program counter is out of these its not a separate register also stack pointer is one of these beyond this you have besides this you have condition codes similar to power PC 
now the key feature here is that there is extreme orthogonality that means the way you specify operands is independent of largely independent of the opcode
so you have different ways of specifying operands or different addressing modes and they can be independently combined with any of the instruction opcode 
so all instructions support all addressing modes whereas in MIPS we have seen that for each instruction there is a specific addressing mode all right here you have whole lot of instructions whole lot of addressing modes and all combinations are possible um what we will see next is intel machine there are lots of instructions lots of addressing modes but only some combinations are valid some are not valid and it s a it s a bit hard to remember which are and which are not  
so the instructions are taken only as byte stream because as they mentioned the size could be varying quite a lot one byte two something like fifty four bytes and so decoding is not all that easy the opcode specifies many things opcode is typically a byte but sometime it actually extends to another byte 
so not only it specifies operations which operation is to be done but how many operands are there 
so for example add would be available with the two operands two operands three operands six operands different modes are there so number of operands um an operand types whether it is working on integers reals double precison bytes and so on 
so all that information is packed in either one byte or two bytes um 











(Refer Slide Time 27:11)


the data types varying from eight to one twenty eight okay um it could deal with character strings where one byte represent a character or string of decimal digits one digit coded in four bits theer are lots of addressing modes there are six bit constants eight bit constants sixteen bit constants thirty two bit constants um they are register addressing mode that means register carries the operand or you would find the term deferred with many of the addressing modes 
so deferred refers to some kind of indirection that means what you are getting is only an address and then you take that address and make access to memory to get the operand 
so register mode means that register carries the operand register deferred means register carries the address and you carry on one memory access cycle 
so displacement displacement with respect to registers and one of the register is as you know is PC one is SP 
so PC relative and register relative are not two different modes they are you could have eight sixteen thirty two bit displacement with respect to any of the registers and its deferred version that means register plus a constant gives you can give you an operand it can also give you an address so indexing is there indexing with scaling that means the um the index which is getting added can be multiplied by one two four depending upon operand size autoincrement autodecrement with deferred okay so all these addressing modes are available um variety of data transfer variety of operations i will not go through all of them some are common  
(Refer Slide Time 27:11) 

um you could see peculiar things like operations which work on polynomials directly or operations which do insertion in queue right 
so here is an example of instruction for example add l three three refers to three operands here well when i say operand it actually means we are also counting the destination and this l stands for (Refer Slide Time 33:28) 

the datatype 
so l implies thirty two bit long integer integer here means sixteen actually 
so thirty two bit they call as long integer okay and the three addresses are R one which is a register addressing mode this is a um you could say a base addressing okay and this is a immediate addressing 
so each of these could appear in any mode okay so you are specifying two operands and and a result each one can be in any mode all right and there is a whole variety of addressing mode which is of course destination cannot be in immediate mode 
so that thing naturally don t make sense they are of course excluded 
so the way instruction is represented is one byte will specify this opcode and it is operation the datatype and number of addresses or those three informations are three pieces are there byte two specifies the mode of one of the addresses and register r one 
so actually four bits specified mode and you can see that there are sixteen modes and four bits for specifying register number 
so since they are only sixteen registers similarly for second second one you have byte three which specifies the mode and say the register is (Refer Slide Time 35:00) 

register is r two um the mode which has been specified is that it say base addressing mode that means the constant needs to be specified 
so next two byte carry the constant okay that is how that is one reason why instruction size varies depending upon modes and number of operands the instruction size varies okay so if all the modes were registers suppose it was add l three r one r two r three then same instruction would occupy less number of bytes 
but here you have specified a base addressing mode so two additional bytes come in you specified a constant immediate mode for this 
so four bytes come for that okay byte seven eight nine and ten carry this constant right 
so this will give you some glimpse of how instructions vary in size because of all this variety which is possible um there is another example of instruction add p six okay so it has six operands and why do we need six operands um it is actually adding two decimal numbers okay each decimal number is represented as a string of digits packed digits means that you are packing it in the smallest number of bits 
so four bits for every digit unpacked would mean that you pack pack a digit in one byte or one word 
so um the two sources and one destination each requires two two operand specification so op one op two are for first number three and four for second five and six for the result 
okay and why you need two for each is one case the length of the string and the other gives the starting address 
okay so you could add long you could take a long numbers okay and not necessarily confined to thirty two bits or sixty four bits there could be long strings of digits and single instruction could add these 
okay finally lets talk of intel x eighty six which has a very very long history 
(Refer Slide Time 37:28) 

beginning with four bit um eight bit sixteen bit thirty two bit of course um there were major breaks four to eight and eight to sixteen um but starting with sixeen bit processor eighty eighty six which was introduced in seventy eight um there has been a reasonable amount of compatibility which has been maintained 
okay so eighty eighty six was announced in seventy eight which was essentially sixteen bit architecture then few years later its companion floating point processor coprocessor was added in nineteen eighty two eighty two eight six was introduced which increase the address space to twenty four bits earlier it was sixteen bits okay with sixteen bits you have addressability of only sixty four kilo bytes as the demand increased the space had to be increased 
so the twenty four byte twenty four bits you get sixteen mega bytes okay also some instructions are added with eighty three eight six um this series has got thirty two bit architecture and more new newer addressing modes were defined and also paging in terms of virtual memory was introduced here then after that we have eighty forties eighty four eight six pentium pentium pro pentium two three four 
so the whole series and they have the emphasis has been on adding more instructions all the compatibility is maintained but more instructions have been added for higher performance MMX for example multimedia extension then there are SIMD instructions for again performing operations on arrays 
so although the addition have taken place but the core instruction set has remained
so lets have a look at that part before that i would like you to read some interesting comments these are from our textbook (Refer Slide Time 39:48) 

okay the history illustrates the impact of the golden handcuffs of compatibility what it means is that to maintain compatibility the architecture has got tied up and you are sort of constrained you can keep on adding features but you need to maintain what is there so that baggage has to be carried and its like adding new features is something like adding clothing to a packed bag its an architecture that is difficult to explain and impossible to love its very complex architecture um its very difficult for a programmer to be showed that yes out of these instructions i am using the right ones i am using them in best pursued manner even for compiler it may be very difficult to make best use of all possible instructions and all possible variations which are there with the instruction um but on the other hand what this lacks in style is made up in quantity 
so because of compatibility it has been a commercial success and which means its widely used and available in large quantities (Refer Slide Time 41:14) 

so lets look at some of the features it has large number of instructions about three fifty and they also vary in size like VAX but not as much okay one to seventeen which is still enough to give you headache when you are designing hardware for it now this is primarily a two address machine okay whereas VAX can be three address two address from whole varieties there this is primarily a two address machine 
so one of the sources also doubles up as a destination um out of two operands one can come from memory and one has to be register 
so both could be register but one can come from register so it is R RM type of architecture all right you remember i talked about RR RM MM MM is one when both operands come from memory 
so VAX is a combination of all VAX supports all of them and often one would call R plus M but this is RM type of architecture um there are complex addressing modes almost similar set as you have in VAX um there is a base addressing mode or scaled index with the displacement can be eight bit or thirty two bit and index can be scaled with one two or four depending upon whether you access in series of bytes or series of words now difficulty is that the permitted combination of instruction in addressing modes are very irregular 
okay so the two extremes of MIPS is that there is a one to one sort of one to one correspondence VAX says that you apply you can use with anything and anything 
but here you have very peculiar combinations 
so there are lots of exceptions which are difficult to remember so lots of special cases which are hard to learn and the effect of each instruction on condition codes is again some what complex and irregular 
on the other hand there is a saving grace that the most frequently used instructions are not too difficult to build and compilers have learned to avoid the portions of architecture that is not very efficient 
so the compiler also focus on a small subset of all the instructions and those instructions can be built efficiently 
so here is a glimpse of register structure not all registers  (Refer Slide Time 43:48) 

i am showing all the major thirty two bit registers of eighty eighty six 
so they have name like EAX and EBX ECX EDX this is one set where E and X are both there actually E stands for extension extended extended A register 
okay so actually extension has taken place in two stages so there were eight bit architectures with registers called A B C D and so on and then mark it to sixteen bit architecture where you had AH AL BH BL H stands for high L stands for low 
okay and then they were extended further to get thirty bit registers 
so still old names can be used so although EAX EBX ECX EDX when you write in assembly they refer to thirty two bit registers you can you can also use AX BX CX DX that means you can work with half of the registers that means take data from half of it put the result in half of it that s possible and you can also work with quarters refered to them as AH AL BH BL and so on um then there is another set of registers um which in earlier versions are called BP SP SI DI or here called EBP ESP EDI E again stands for extended BP is base pointer SP is stack pointer SI is source index DI is destination index 
so base pointer stack pointer have their usual meaning SI and DI are referred to operations which work on string
okay so there is a source string and destination string and SI and DI would typically be used for indexing individual element in those strings the other registers um particularly they are sixteen bit segment registers um which are labelled as CS SS DS and others CS stands for code segment stack segment data segment and so on 
so D E F G are (Refer Slide Time 46:04) 

all actually for data 
so segmentation was a technique which is brought into earlier versions of this architecture to address larger memory then what can be done with a sixteen bit address register 
okay so sixteen bit register is limited to sixty four k bytes but how do you access larger memory is with the help of a segment register 
so in in a segment register you set up a base and around the um starting from that base you can access sixty four register sorry sixty four k bytes of memory you want to addres another area you can move your base and then address um sixty four k bytes there 
so although these segment registers are also sixteen bits but they are used with some um some multiplication factor okay 
so for example to get twenty bit twenty four bit addressing you imagine that segment register is shifted by eight bits okay so you take sixteen bit value with eight zeros and then add another address register that s how you will get full fledged twenty four bit address of course they are flags and program counter which is called instruction pointer here um lastly i will mention another peculiar feature of this architecture is the concept of prefix that means you have an instruction you can put a byte before the instruction in the sequence of bytes in your code an instruction could be proceeded by a byte which is called a prefix which in some sense modifies the meaning of the instruction 
(Refer Slide Time 48:01) 

and there can be upto three prefixes for an instruction the kind of modifications these prefixes do are shown here for example instructions work with some default data size okay unlike VAX case where the data size was carried as part of the instruction for example add l three l was referred to the data size but here default size is set somewhere but these prefixes can override that there is always a default segment register you might for example put ES as a default segment register in some part of the code but for some instruction if you want to override this could do so um they could also help in implementing some special instruction like semaphore um they require exclusive as to bus and bus has to be locked um prefixes can be used to repeat the following instruction certain number of times okay the default address size can also be overridden 
so in these ways and some others these prefixes can be used to modify effective instruction and effectively it gives an impression of infact a large number of instructions with all these prefixes 
okay so finally let me conclude by summarizing one or two important features of each of the four architectures we have looked at (Refer Slide Time 49:30) 

beginning with power PC we found that it s a RISC architecture but with very powerful set of branch instructions um SPARC is another RISC and its very significant feature is the concept of register windows VAX we took as example of CISC machine which has wide variety of addressing modes and strength of this is that all these addressing modes are orthogonal to opcodes finally we had some talk of x eighty six intel it s a CISC where the strongest feature is the code compatibility and that s the reason for its commercial success i will stop with this  

 






Computer Architecture


Prof. Anshul Kumar
Department of Computer Science
and Engineering,IIT Delhi


Lecture 10

Performance (contd.)


We will continue with our discussion on performance evaluation of processors in the last lecture we saw that there could be different angles or different perspectives when you look at the peformance issue and therefore the definitions may change um we tried to follow the definition which would be true for a individual users perspective and from that point of view we try to define performance in a quantitative manner essentially trying to say that it is execution time of a given program over an architecture over a machine which matters we will go further into this look at more examples and try to see how this is done professionally so that a large community of architects and users could share same ideas 
so essentially the (Refer Slide Time 1:54) 

formula or the definition we came up for performance tried to interlink three aspects the number of instructions which you require to execute a program the number of clock cycles which are required to execute a program and the total time which is spent in executing the program 
so these are the three ways or three aspects which are of concern to you um the one at the bottom is the number of seconds okay how much time how many seconds or minutes or hours are taken to run a program this could also be expressed in terms of number of clock cycles because everything which is done in a computer is in terms of clocks and also one could say slightly higher level that everything is done in terms of instructions 
so the time taken could be measured in these three units where this is more abstract less abstract and least abstract so this is a real measure and there are quantities which we define which link these 
so for example the relationship between instructions and cycles um could be expressed as cycles per instructions okay number of cycles you require do an instruction you can talk of individual instructions you can talk of collection of instructions in terms of its average 
so when you are talking of entire program what would matter is average CPI or average cycles per instruction for that particular program 
now remember that this average could be different from program to program um same thing actually said in a inverse manner is often called IPC or instructions per cycle 
so the notion here is different that as a few are trying to do multiple instructions in a cycle and there are machines there are processors which actually are at that level of performance where they do multiple instructions per cycle and there it becomes perhaps more important talk of instructions per cycle 
but for this particular course where we are talking of basic architectures we will essentially confine ourselves to CPI one could say that the two are reciprocal of each other but it is the perspective which is different then what relates cycles to the absolute time is the clock period or conversely clock frequency 
so that s something very straight forward finally the relationship between instructions and the absolute time okay you could talk of the total time each instruction takes okay seconds per instruction or you can also talk of instructions per second 
so when you are talking of time seconds per instruction there is another thing which we actually talked last time was seconds per program 
okay for entire program how many seconds are required or for one instruction how many seconds are required one should ask both questions 
so when you are talking of time per instruction it is actually talking of instruction rate and may not really capture the entire picture of how long a program takes okay a program may execute at a fast rate okay but the total instruction may be large 
so it is the overall time which matters but but still often one talks about the rate at which instructions are done 
so instructions per second and says this to make a more convenient unit you talk of million instructions per unit or MIPS 
okay so this is the average rate at which instructions get executed right reciprocal of this would be the number of micro seconds you are spending per instruction 
so one could talk of MIPS as an average people have also talked of peak MIPS which is because the if you look at the instantaneous rate at which instructions are done it could vary there are slow instructions there are fast instructions 
so peak MIPS would refer to if you are doing only the fastest instructions what is the rate at which you will do it (Refer Slide Time 6:30) 

okay so in nut shell what we have said is that performance is determined by execution time um particularly an individual programmer or individual users perspective 
at different stages people have talked of other ways of looking at it and the question is do any of the other variables truly represent performance and some of these possibilities are number of cycles to execute the program the number of instructions in the program number of cycles per second average number of cycles per instruction average number of instructions per second 
so i have talked of many of these things but ultimately it is this which is important the execution time here when i am talking of number of instructions in program if this is a factor which is often looked upon in two different ways one could be the length of the program as you see statistically a program has so many instructions okay how many mega bytes or kilo bytes it occupies okay that s linked to the number of instructions it has but when you are talking of performance you are talking of dynamic number of instructions that number of instructions executed would be the right term 
so if the program has some loops of course some instructions gets repeatedly 
so from performance point of view you have to count those those many times or if there are conditional something gets skipped something gets executed you have to see what gets executed 
so to do some computation for a given data program may execute certain number of instructions and that s what one is often talking of 
so a common pitfall (Refer Slide Time 8:00) 

is thinking of other possible indicators okay which are not true indicators or performance they they may be related but they are not true indicators 
okay now lets look at few examples we we looked at last example one example in the last lecture towards and we will see a few more today 
so suppose we are talking of two implementations of the same instruction set architecture or ISA lets call these A and B 
so instructions are same okay but there are two different ways of implementing them in hardware right so now for a given program suppose A implementation has a clock cycle time of ten nano seconds and B has twenty nano seconds um the CPI the rate at which instructions that executed or number of cycles per instruction on average is  two point zero  in case of A and  one point zero   one point two  in case of B 
okay so the question is that for this particular program which machine or which implementation is faster (Refer Slide Time 9:25) 

so we can apply the formula which says that you take the number of instructions executed multiplied by CPI and multiply it by clock cycle time
so one question you need to answer is that if these two machines (Refer Slide Time 9:47) 

as the case now which have same ISA which of the quantities remain equal number of instruction because it is the same program expressed in the same instruction set which we are talking of 
so it is the number of instructions which will remain same other things could vary clock rate is given to be varying CPI is given to be varying um MIPS or the number the rate at which instructions are done will vary and so would be the execution time okay its only the instruction the number of instructions executed that will remain unchanged 
so now we are not given the number of instructions in this case all we know is the the two are same 
so our objective is just to find relative performance or just find which is faster which is slower so that can be done and therefore we can knock off that common factor which is number of instructions essentially you product of these figures okay ten nano seconds and  two point zero  which basically says it is twenty nano seconds is the average time you are spending per instruction and to the same instructions where twenty four is the twenty four nano seconds is the time you spent per instruction in the second case and clearly A is faster 
okay another example here situation would be such that the number of instructions would vary okay so suppose put yourself in the shoes of compiler designer who has to decide between two alternative sequences of code to translate a given high level language computation okay there is some statement in high level language it can be translated in two alternative ways okay and one has to make a choice between these two 
so the architecture which is available to him the instruction set architecture are first three classes of instructions A B and C and they differ in terms of of course their functionality but in terms of cycles the number of cycles spent is one two and three in the three cases 
okay class A instructions take one cycle clas B two cycles and class three three cycles each (Refer Slide Time 12:22) 

so the options which the compiler designer is debating are these two code sequences first sequence uses two instructions of type A one instruction of type B and two instructions of type C okay for same computation 
so this is one one possible way one could translate same computation second way is four of A one of B and one of C all right 


so now how do you evaluate this choice you can find out the time which will take for both these cases and you can see that in first case okay that s the same sequence the sequence will require a total of two plus two plus six 
so that is ten cycles and in the second case it will require four one and one so which means the four plus two eight and three eleven okay [students : nine] sorry what did i do four plus two six and three nine yeah and so second is faster all right although the total number of instructions in second one is longer 
so if you were to just compare number of instructions first sequence is of five instructions second is of six all right if you don t go into timing you might say first sequence is shorter and one may be tempted to do that 
but only when you look into the cycle information you realize that essentilly what second sequence does is it trades off one slow instruction for two fast instruction and still make  sense okay so one (Refer Slide Time 14:11) 

could quantify the difference how how much is sequence two faster 
so it is nine versus ten all right and you can also find CPI for each so what is the average CPI in the two cases can you do it quickly [noise] two it will be basically you you add these you take average of these with these as weightage 
so it is two plus two plus six okay so basically all right we have already counted the number of cycles we can divide by number of instructions 
so ten cycles of five instructions which means two so CPI on the average is two and here you have nine cycles divided by six so you have  one point five  
okay yet another example so we have two compilers which are being tested for a machine again there is kind of similarity once again the compiler is the one which is varying 
so the clock frequency is ten hundred mega hertz and there are three classes of instructions um similar situation one two and three three classes are A B and C 
so compiler one produces for a for a given program now we are looking at entire program situation is similar but instead of some piece of code we look at one whole program 
so compiler one produces five million instructions of type A and one million of type B one million of type C  and second compiler produces ten million of type A one million of type B and one million of type C 
so it is just producing more instruction of type A and others are same 
so now which compiler gives you higher MIPS okay um the first let us say all right you can you can add these add the cycles of these instructions right and you know that the cycle time or the (Refer Slide Time 16:47) 

rate at which cycles are running is hundred mega hertz 
so total of cycles required for these instructions will be five million plus two million that is seven and three million that is another three total ten million cycles are required and each cycle takes  point one  nano seconds all right let me see no we are trying to talk of instructions per second 
so  point one  nano sorry ten nano seconds multiplied by ten miliion cycles is how much lets see its hundred so one by thousand 
so one by one by ten seconds is what you would require okay so you have am i right  yeah so so you have executed seven million instructions in one by ten seconds is that right one by ten just check my calculations 
okay so seven million instructions in one by ten seconds so what is the instruction rate it s a seventy MIPS okay seventy million instructions per second right lets do it for the second case 
so the number of cycles here is ten million for this two million for this and three million for this 
so basically fifteen million right and the total time spent would be fifteen sorry it should be  one point five  times that so  point one five   point one five  seconds has a pose to  point one seconds  okay in case one we spent  point one seconds  now we are spendind  point one seconds  and the number of instructions being done is twelve million 
okay so what is the MIPS we are getting eighty MIPS 

so the answers are seventy MIPS in first case and eighty MIPS in second case 
so if if you were to take MIPS as the comparison criteria it might appear that in the second case you take the second program generated by the second compiler um its giving you faster MIPS more MIPS right but it is obvious that the total time spend here is more and therefore compiler two is producing a poor report 
okay so execution time is lower in first case (Refer Slide Time 20:33) 

okay so in that sense its better whereas MIPS is higher in second case 
so apparently the second may look better but one has to be careful here so is it clear 
okay again in this example we have two implementations of same instruction set architecture right (Refer Slide Time 21:03) 

so again instruction set is same these two are being called M one and M two um they are different in clock fifty mega hertz in one case seventy five mega hertz in the second case the CPI is also different  two point eight  here  three point two   here 
okay so apparently whats happening here is that you you have a longer clock cycle but more work is getting done in each cycle and here you have shorter cycles 
so less work is getting done in each cycle 
so you take more cycles here less cycles here now how do you find the composite effect of these two how do you compare 
so the ratio of execution times of M one and M two would be the instruction count multiplied by CPI divided by clock rate of M one and instruction count for M two CPI of M two and clock rate of M two 
now it is the instruction count which is common being the same instruction set architecture as we have seen earlier 
so we can look at the remaining factors  two point eight  by fifty and  three point two   by seventy five which comes out to be  one point three one  okay so it means that M one takes longer as compared to M two and the factor is that it is taking thirty one percent more thirty one percent longer then what M two takes now question could be asked in different ways also okay so if you had redesigned possibility for M one and you were able to increase the clock rate keeping everything else same keeping that CPI same what clock rate for M one would give you same execution time 
okay so here what you can do is you can keep this factor as unknown and put this as one and that will give you the clock rate you require right okay so i don t go through that calculation you can check it out (Refer Slide Time 23:36) 

okay so now we have seen these examples where we try to see what happens if you change the design what happens if you change the compiler similarly you can examine the combined effect of both 
so in tutorial probably we will take exercises which will try to look at more complex situations okay but now we have faced the question of which program you should use to check out the performance right if you are a very focussed user you have a single program which you want to run over and over again then there is no problem you you can just see how long system works for that particular situation or if you have if you have not just one but a small set of programs or your work load typically some five six program you do over and over again you also may have a idea of how often you use program one how often you use program two and so on you you could test for all of these and get take some kind of average okay depending upon your area of application for example you could you do compilations editings run some scientific application 
so depending upon your area of application or your typical work load you can 25:07 but the question is that can you do something in a more general sense okay so this is very personalized way of evaluating performance but are they general attempts okay in general can you say A is better than B or so on 
so for that one has what is called benchmarks okay benchmarks are 
(Refer Slide Time 25:34) 

basically programs used for testing the performance it could be a single program or a set of programs which collectively are used for testing the performance um but one has to bear in mind that benchmark has to be or a set of benchmarks have to be appropriate for a user or a set of users all right if a user runs commercial applications and benchmarks should be of that type if user runs away the scientific application then benchmark should reflect that kind of application and a benchmark should be such that they test a given machine extensively it should not be that they test some particular feature and a machine which is better in that feature but not better on the whole would give a false improvement of false sense of related performance 
so the benchmark should not be overly depended upon a few set of features so so that a machine vendor can also misrepresent the level of performance 
so one can think of benchmarks of a varied complexity could be very small simple benchmarks and they could be very complex and accordingly their usefulness would vary 

(Refer Slide Time 27:08) 

so who defines these benchmarks okay benchmarks can be defined by users or the vendors or may be mutually agreed upon small benchmarks are easy to write okay and they are nice for discussing the performance by lets say designers and architects you can you can closely see what is happening how a benchmark is performing how where it is not performing and you can analyze it very nicely um they can be easily standardized but they can be abused 
so for example there have been cases of small benchmarks and cases where computer vendors would generate very highly optimized code for that particular benchmark 
so so you could infact have a switch in your compiler that if the code is this just if the source is this put this as the code and it could give again a false sense of high performance 
so a consortium was developed in middle of nineties which is called SPEC and it stands for system performance evaluation cooperative so there number of industries who have joined hands to standardize this process of defining benchmarks declaring performance summarizing and tabulating the performance and bringing a common platform to this issue of performance evaluation 
so the attempt here is to take real programs do not write small artificial or synthetic programs but take real programs which users use and standardize them take some standard implementations define some test inputs and so on and make them available on a common platform 
so one could still abuse these because even in large programs what may happen is that lot of time gets spent on small pieces of code 
so so if you could really attack those and make a machine work faster for those things can go wrong but since you are talking of not just one benchmark but a collection of benchmarks number of programs and it ma be some what harder to pick out the critical code of each of the programs and make a machine work faster all of them okay 

so one could abuse but chances of such abuses are much less and these benchmarks are valuable indicators of performance of hardware plus software the total environment in which you use that means you have you are talking of a machine a hardware and a compiler which generates a code because these benchmarks are not defined in assembly language of a particular machine they are defined in standard high level languages and therefore role of a compiler is important 
so what one is evaluating is not just the machine but compiler plus the machine combination 
so this is a typical result (Refer Slide Time 30:32) 

of carrying out a set of benchmak tests for a given machine all right here you are showing comparison of benchmark figures for two different compilers and the same machine 
so this this lighter colour is for one compiler and this orange one is for second compiler which is an enhanced version of first one improved version does certain optimization and the text is actually very small here so you may be not able to read um what we are showing on the vertical axis is the performance rating 
okay so this is a number where higher number means higher performance okay and this number is obtained by running running the program finding its time and taking reciprocal with the certain scaling factor 
okay so higher the number means higher is the performance or more is the execution time and you would see a pair of bars each pair corresponds to one particular benchmark 
so this one is for example gcc which is a C compiler 
so it is the time compiler takes to compile a program which which we are actually measuring here 
so gcc compiler was compiled by this compiler and that compiler and the two times are different there is a marginal improvement in the rating you don t see much difference here this is a stress over which is some circuit optimization program SPICE is a simulation program and so on and this is another this is another this is lisp interpreter 

so here you would see what is happened that between light orange and dark orange there is a dramatic difference 
so essentially what this compiler did was some clever trick that this matrix were into our program it could optimize some very crucial part of that program some some loops and give a dramatic performance improvement 
but if you look at in totality there is an improvement but not so dramatic if you had looked at this in isolation you would have startling results  (Refer Slide Time 33:02) 

so what are these benchmarks beginning with sometime around ninety five a set of programs get designed a set of benchmarks okay they are two sets actually those which are involved in integer computation 
so SPEC int and SPEC fp which is floating point okay so because you know user who work with floating point heavy numerical computation they have different set of typical programs those who work in symbolic computation or or in non numeric computation the set of benchmarks may be different so in in the integer case you have things like compiler simulator lisp interpreter and so on 
so here air is a set of benchmarks and names and you will get some idea 
so go is some artificial intelligence program plays the game of go m eight eight ksim is a motorola eighty eight k chip simulator so simulates a processor gcc is gnu C compiler 
compress is a program for compression and decompression of files li is lisp interpreter ijpeg ijpeg is graphics compression decompresion perl is a benchmark which is written in language perl it manipulates strings and prime numbers vortex is a database program tomcatv is a mesh generation program 
(Refer Slide Time 34:42) 

swim is a shallow water model i am sorry these are not just these are actually mixture of integer and floating point i can see both here then there are benchmarks from quantum physics astrophysics three D equations solver partial differential equations something which simulates turbulence in a cube solves problems regarding temperature wind velocity and pollutant distribution this is from quantum chemistry this is from plasma physics and so on (Refer Slide Time 35:15) 

so these benchmarks have been revised from time to time because as time changes the applications change the machines change the the amount of memory which is available in the machines which were at which were prevalent in ninety five is not same as what is available today 
okay so you need different programs different set of programs to evaluate in benchmark content ready machines and therefore periodically these keep getting revised and it is always meaningful to work with the current set of benchmarks  
so using these benchmarks you can answer variety of questions for example you may take same machine and look at its two different versions with different clock rate okay and ask a question whether doubling the clock rate doubles the performance or not 
so here you you run pent[ium] some program of SPEC int set over pentium and pentium pro 
okay so this refers to pentium this refers to pentium pro for each case you have done it with different machines 
so for pentium it is hundred mega hertz one fifty mega hertz and two hundred mega hertz and how much is the performance varying here its something like three and little over three and here it is little over five 
so the clock has been doubled from hundred mega hertz to two hundred mega hertz but as you can see this number has not doubled its it is less than double 
so what could be the reason in our formula we we had clock frequency of a proportional factor what happened to that [students : cycles per instruction it seems ] okay suppose cycles per instruction is also same yeah what has happened here is that our formula does not take into account the memory speed we are assuming that there are no additional cycles for memory 
so either you have to count them separately or actually adjust them in CPI itself 
so you could have effective CPI which would try to take into account extra time which may get spent if memory is slow 
so when we are doubling the processor frequency we have not said that have you doubled the memory speed also or not 
so we will look at memory effect later um this is just to remind you that we have looked we are looking at still in a very small domain just looking at processor but when you account for other things memory IO perpherals and so on i think it will become little more complex but these benchmarks are run on physical systems 
okay so memory is there IO is there everything is there and you have run a program from end to end from input to its output 
so in this this case in this diagram the question which is being asked 
(Refer Slide Time 38:39)  

is can a machine with slower clock have a better performance again simple question so same same two machines pentium and pentium pro same three points the benchmarks we have taken are SPEC fp that means for floating point performance 
so here you put notice that pentium pro with one fifty mega hertz is out performing pentium with two hundred mega hertz al right 
so although they have same set of instructions pentium pro has a few extra of course but the way the instructions are implemented is different and therefore with the lower clock itself it can out perform 
okay so (Refer Slide Time 39:36) 

we we are actually concerned about speeding up execution of programs eventually by various means by improving the architecture improving instruction set improving the hardware implementation improving compiler the way code is generated but we must keep in mind very simple but important law called amdahls law okay so first lets define what is speedup 
so you have the old execution time and you have hopefully reduced new execution time you take the ratio and that factor gives you speedup so simple or alternatively you could take the ratio of performance figures which are  reciprocal of execution time so performance of new or performance of load now what happens is that quite often you may speedup only part of the program execution 
so let us say this big rectangle represents the entire program or the time spend by entire program and your attention was focussed on this shaded area 
so it is this part your you are focussed on you have designed your architecture of compiler so that this gets speeded up 
okay so or there is a technique which you have in mind which effects only this part does not effect rest of the part so suppose this time shrinks to this by certain amount where the remaining part remains unchanged 
so what is the effect on the whole let us say the we tried to find figure out the fraction which is enhanced fraction which is not enhanced 
so fraction subscript enhanced of the task is speeded up by a factor called speedup subscript enhanced okay so so this is a fraction lets say you might say that i am taking three fourths of the program or seventy five percent of the program and speeding it by a factor of two 
so these are the figures i am talking off this is a fraction of the program you are speeding up and that s the factor why which you are speeding it up 
so the new execution time can be related to old execution time by this factor 
so what you are saying is that one minus this fraction remains unchanged okay and the fraction which you have enhanced is reduced by this factor okay so if you had for example seventy five percent of the program speeded up by a factor of two
so this  point two five  remains unchanged and  point seven five  divided by two is the contribution of that speeded up part 
so the speed up would be the ratio of these two execution time old and execution time new and what you will get is one upon this so now basically this is how the noise captured and this law says that you have to keep in mind the part that which is being speeded up and often you are not speeding up the whole thing 
so the effect of this then the over speedup may get limited by how big this fraction is 
so if if you are only taking half of the program and speeding speeding it by a large factor still the total speedup cannot be more than two okay you get that suppose your attention is only on half of the program and you might infinitely speed it up 
but the over all time cannot be more than two because other half remains as it is 
so execution time after improvement is time which is unaffected that we take as it is and time which is affected is divided by the improvement factor 




so now quantifying that with an example suppose the program runs in hundred seconds on a machine (Refer Slide Time 43:41) 

with a multiplication instruction responsible for eighty percent of the execution time 
so eighty percent of the time you ar only multiplying and remaining twenty percent you may be adding subtracting loading data from memory taking decision branching and so on 
so it s a for whatever reason it s a multiply multiply or multiplication dominant program how much do we have to improve the speed of multiplication if you want to have the program run four times faster 
okay so from hundred seconds you want to reduce it to twenty five seconds 
so so what is the speedup factor required for multiplication alone pardon [student : sixteen times] sixteen times okay so you can see that there is a huge improvement you need just to get four times improvement 
so your effort is to make it sixteen times but the net result you get is only four times improvement okay so one of course one principle which underlines all this is that you  always have to find out which is the common part and that has to be made fast 
so you have to pick up well eighty percent is a good enough fraction here right had this been even lower as i mentioned earlier fifty percent or if it is small fraction then your achievement is less 
so you have to take something which is common and try to improve it as much as you can 
(Refer Slide Time 45:30) 

so another example um suppose you want to take a machine and enhance its floating point capability okay suppose floating point instructions are speeded up five times execution time of some benchmark before the floating point enhancement is ten seconds what will be the speedup if half of the ten seconds is spent executing floating point instructions 
so so now we are talking of a smaller fraction only half the instructions are floating point and we make them run five times faster 
so so basically you could say that this ten second was divided into five and five okay five seconds are remaining unchanged remaining five seconds are speeded up five times so that gives you one second so total time now is six seconds 
so from ten you have gone to six and therefore speedup is ten by six or something like  one point six seven  that that s the factor by which you have speeded up okay again a question suppose we are looking for a benchmark to show off the new floating point unit described above in the previous case and we want the overall benchmark to show a speedup of three okay one benchmark we are considering runs for hundred seconds with the old floating point hardware how much of the execution time would floating point instructions have to account for in this program in order to yield the desired speedup on this benchmark 
okay so the previous program where you had only fifty percent of instructions fifty percent of the time spent in floating point will never show you a speedup of even two okay it will always be less than two in the previous case we got  one point six seven  okay but suppose you wanted to project your floating point improvement what kind of benchmark you may have to choose right you want to choose a benchmark you want to synthesize you want to take up a benchmark which will show that with this floating point improvement the whole thing runs three times faster 
okay so so now we are asking the question little differently so can you tell me what fraction of the original program should be floating point computation [student: eighty three percent]  pardon [student: eighty three percent ] eighty three percent is that does everyone tally with that 
okay you can work it out if you cant quickly work it now okay here now we have lets look at improvement which takes place with technology (Refer Slide Time 48:30) 

okay improvement with time assume a processor currently requires ten seconds to run a program and a processor performance is improved by fifty percent every year okay let us say in simple terms you are you are getting x mega hertz today um next day you will get  one point five  times x mega hertz next time it will be  one point five  into  one point five  times x and so on  
so every year you are seeing that the performance is going up by fifty percent 
so by what factor performance improves in five years but its very straight forward one plus  point five  raised power five or you get a factor of  seven point five nine  and execution time after five years would reduce by the same factor okay so if it is ten seconds it will become ten by  seven point five nine  or  one point three two   seconds 
so it is nice but can you spot what assumtions we have made what simplifying assumptions we have made in this calculation sorry [noise] yeah that s one of the assumption and again let me again remind you that we have not said anything about the memory is memory improving by same factor or not 
okay so here we have effectively assumed memory also improves by same factor if it does not then things will be different 
i have mentioned already about possible performance indicator which people have used is MIPS another one which has been used in past is MFLOPS or mega flops 
(Refer Slide Time 50:20) 

so particularly for floating point operations people have talked of MFLOPS which stand for million floating point operations per second all right 
so its it s a metric again similar to MIPS which is talking of million instructions per second but here the focus is floating point 
so which is some what relevant for people who do predominantly floating point computation but once again there is the other things which are there in the program its  not entirely floating point 
so advantage is with such measures is that they are easy to understand measure and project but there are fallacies they may not be actually reflecting performance once again tutorial will take a 51:08 which brings out the problem with these 
(Refer Slide Time 50:20) 

so to summarize what we have learnt that you need good benchmarks standardized benchmarks which are avilable across the industries for evaluation and performance such aspect measures like MIPS and mega flops look easy and simple but they could be misleading and one has to keep in mind amdahls law while talking of speedup due to some enhancements (Refer Slide Time 51:48) 

so now on the whole in previous lecture and in this lecture we have seen that there are two mesures of performance execution time and throughput we have focussed on execution time which is given by a simple formula and these factors do get influenced by compiler technology instruction set architecture micro architecture which means how instructions are implemented in hardware and the basic circuit technology or fabrication technology 
so when trying to improve performance we should try to capture a large fraction the common case and try to make it as fast as possible okay i will stop with that 




COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   11

Binary Arithmetic, ALU Design

today we are moving to a different topic our discussion is so far was on instruction set of architecture 
and now we will start discussing about micro architecture which means how instructions which we have studied so far how they are realized in hardware 
um so we we will begin by looking at how basic operations are done in hardware will talk of arithmetic operators um logical comparison and so on 
and eventually we will see how um a heart of the processor called ALU arithmetic logic unit is built 
so we have so far seen um what instructions are what is there usefulness what what is the purpose of instruction and whats the functionality they achieved 
we have also talked about how performance is defined and to some extent we have seen the relationship between performance and instruction 
so when we continue with this we develop and understanding micro architecture and relationship between instruction set of architecture and micro architecture and idea of performance will be even more clear (Refer Slide Time 01:38 min)



so essentially what we are [noise] aiming at is designing something like this and ALU um which typically works on two operands in our case for MIPS processor they will be thirty two bit operands 
and produces a result under control of some signals which are determined from the instruction 
so depending upon which instruction you are trying to execute this unit will perform some operation it might perform addition subtraction multiplication division or do some comparison or do some logical operation
okay so in next few lectures the focus would be design of this [noise]
so um talking of this topic this is the sequence of sub topic you will look at um i will today starting today i will talk about binary arithmetic and design of alu um in a very simple situation then i will try to include um idea of overflow okay 
when when a result exceeds the limit what do we do um will separately talk of multiplier and divider design which are more complex operation then simple add subtract and or 
um we will look at some techniques to speed up the key operation of addition or subtraction um then we will move on to non integer operations floating point how such numbers are represented and what kind of operations are performed over these 
and finally the hardware to carry out the operations (Refer Slide Time 03:00 min)



so [noise] today in particular starting with binary representation we will see how addition and subtraction are done and how the circuit is built 
will talk of um comparison little bit logical operation and finally come up with alu design for some of the MIPS instructions (Refer Slide Time 04:01 min)



okay [noise] i i presume that you are familiar with binary representation to some extent we will just refresh it so that you are comfortable and you can quickly follow what we discuss later on 
so as you know inside um computer everything is represented in bits okay 
each bit um represents two states zero one and there is no inherent meaning a assign two bit 
a bit could represent different things okay it depends upon the quaintest which you are looking at a string of bit 
it could represent an integer it could represent a fractional number it could represent a piece of text it could represent um some logical value or some symbols in encoded form 
so um the quaintest in which you look at set of bits um dep[end] depends decides wh[at] what actually the mean
so you have to interpret them in one of the many difference ways depending upon um when and what circumstances you are looking at those [noise]   
so to begin with lets talk of integers represented as string of bits 
essentially from mathematical point of view we are talking of binary numbers which are essentially with radix two or base two [noise]
so a string of n bits can represent a number which will correspond to a number in the range zero to two raised for n minus one okay because with n bits you can represent two raised for n pattern um you you get a range of numbers zero to two raised for n minus one if you considering only positive numbers or non zero numbers i should say [noise]
so but with this simple idea we need to um talk of if you more complication for example um in a macro ticks you talk of integers ranging from minus infinitive to plus infinitive okay 
here we are talking of a finite range so it is not a strict one to one correspondent between integer as we understand mathematics and integer we are trying to represent here 
so there is a question of overflow which we are going to discuss later 
um we also up to worry about sign of the numbers how to represent positive and negative numbers number of choices existent we should understand the implication of your choices 
um how do you represent fractions okay so that makes thing little more complex than simply looking at test string of n bits as um a pattern representing numbers from zero to two raised for n minus one (Refer Slide Time 04:48 min)



okay so um starting with the negative numbers they are several representation particularly [noise] three which i have been used commonly 
one is called sign magnitude where um if you have n bits you keep one bit for the sign and remaining n minus one to represent magnitude of the number 
so the the first 	bit typically could represent as sign typically zero corresponds to a positive sign and one corresponds to negative sign 
so in a three bits situation um these a pattern would corresponds to 	the integer shown here okay 
so um the magnitude can range obviously from zero to three with two bits gap for magnitude 
and therefore the range is a zero to three on the positive side and zero to minus three on the negative side interestingly you would notice that there is the plus zero and minus zero of course with mathematically mean the same thing 
but all the same zero magnitude can be prefix with the positive or negative sign and they are two representation of same number 
ones compliment um is the notation which says that if um number is positive you represent as a few word handling a positive number um or without any change 
but if it is negative then you flip all the bits okay 
turn zeros to one and ones to zeros so if you take um zero zero one for example which represents plus one you make it one one zero like this it becomes minus one okay
so um once again the range of number you can have is same zero to three on positive side and minus zero to minus three 
once again um zero gets two representations [noise] the third column here shows whats called twos compliment representation 
here um positive number coincide with what you have seen in other cases okay 
the representation of positive number identical in all cases 
but negative numbers here or with the one of set from this 
say for example um there largest number which is here is minus three but here we get minus four okay 
minus two becomes minus three minus one becomes minus two and minus zero we have got read out that in that is minus one okay 
so essentially this representation um remove this ambiguity each number has a unique representation that s the plus point we are getting 
although the negative points inst to be that the range is unbalanced right 
um range is unbalanced in sense that on the negative side you can go up to minus four but on the positive side you can go up to plus three 
so in general whatever be the number of bits the range in negative side will be one more than what you have on the positive side 
so so that is the um show to say a negative point of this representation but more significantly um an important positive point positive feature here is that um it actually easies the arithmetic operations and makes the hardware so work simpler (Refer Slide Time 07:11 min)

 

okay now what is the definition of this representation 
this is basically um for negative number it is um in magnitude one more then ones compliment 
okay ones compliment is defined as um flipping all the bits and if you let us say [noise] if you take say number like minus two okay its ones compliment is one zero one obtained by inverting all the bits 
and if you add one to that you get one one zero you get minus two so ones compliment plus one 
this shows the range of number you will get in a thirty two bit word as we have in mixed and following twos compliment representation 
so all zeros is a zero on the right side i am righting num[ber] the equivalent values in decimal base time okay
so this is zero then one two and so on the largest number is a zero followed by all one that s the largest positive number and it corresponds to two raised for thirty three two raised for thirty one minus one okay 
this thirty one is one less than the number of bits you have 
um the smallest negative number is um all ones so basically if you take ones compliment of plus one you will get all ones with the zero at the end and add one to get this okay this represents minus one 
and if you go this way the magnitude is increasing the [noise] largest negative number you will get in one followed by all zeros okay
and it will minus two raised for thirty one 
it shows these are these two numbers are max int maximum positive integer value and min int which is the most negative integer value [noise] 
so approximately um two raised for thirty one would be um about closed to over two million over two billion sorry okay (Refer Slide Time 13:08 min)



um how do you perform addition and subtraction with binary numbers again i i presume some familiarity so i will go quickly 
basically um the method is this nothing new in the method you add or subtract exactly the way you do with paper and pencil um as you learnt in school [noise]
only thing you to represent only thing you to remember is that numbers are in base two okay 
so when you add one and one for example you get a zero and a carry right 
so lets look at a few examples this is adding positive three and positive two adding five subtracting two from five subtracting five from six 
so you get five in first case and you would expect three in the second case of course now this is showing as the bits are generated from left to right but when you work it out you are actually have to work from right to left 
because carry a borrow will flow from right to left 
okay now i took this examples carefully so that the result is with in the range which is permissible by four bits we are having at hand okay um how what happens when the number is larger than what the word can accompanied that will discuss later (Refer Slide Time 15:10 min)



okay now if you bring in the sign if you bring in negative numbers what we did was adding or subtracting positive numbers 
um if you have negative numbers and you are representing in twos compliment form then again it does not complicated life at all okay 
so when you adding subtracting you continue to think of them as positive integers okay don t worry about the fact that they are sign twos compliment number perform addition subtraction as the would do um without sign 
and if if your result is within the range then you would have got correct answer okay
so the key is that you perform add or subtract operation ignoring the sign 
so here few examples we will trying to do seven minus two we expect basically we adding minus two to seven okay
we should get an answer five and there is there is a subtraction of minus two from five you should get plus seven is an answer 
subtracting five from two should give a negative answer which will expect to be minus three right 
so just quickly make sure that these representation these number which i have written correct this is representation of minus two okay 
um you can you can do either way if you are if you want to find out um what is the representation of minus two you could take positive to take ones compliment and add okay 
if you given a negative number like this and you want to find out what um what it represents again do the same thing [noise]
um take it ones compliment okay or invert all bits and add one so if you um compliment you will get um zero triple zero and one and add one you get two 
so basically this is representation of minus two and you would notice that you get correctly plus five when you perform addition alright 
i i have i have just without knowing that it is a negative or positive you simply add as um this is the positive number there is a carry going out just ignore it right  
if the result is within the range then you will get correct answer 
so same thing is happening here all there is a carry flowing out 
we have just thrown it what you are getting here is um plus seven as we expected 
in this case let lets do it um step by step 
so you subtract um zero one zero one from zero zero one zero so here you get a one there is a borrow that it s a way this one you get a zero 
here again there is a borrow you get a one there is again a borrow you gate a one here okay
so now um there is a ultimately a borrow from the left side which again like carry we are neglecting 
so you have here one one zero one what does this represent you find it its compliment zero zero one zero and add one to it so you will get zero zero one one which is three okay so this is representation of minus three 
okay so the examples just listed the point which i mentioned earlier 
question is why does it happen why is it you are able to ignore the fact that design 
the answer lies in the fact that um twos compliment representation is actually nothing but two raised for n minus X 
so if you are trying to represent minus X lets X is the magnitude of a number and you are talking about minus X is a negative number 
a negative number with magnitude X so it s a twos compliment representation um is nothing but two raised for n minus X okay 
so you take this large positive number two X will be always less than two and okay
so this positive number minus positive number could be thought of the positive number 
so if you had unsigned number systems right um representation of this positive number in that is what you are representing minus X as okay (Refer Slide Time 19:35 min)



so lets look at some example 
suppose we take you want to see how will represent minus three so it is a two raised for four okay we are talking four bits 
um so of course two raised for four is a five bit number but it is just some thing you are doing on the paper and pencil out of this you subtract zero zero one zero and what you get is one one zero one okay 
so this is same as um complimenting these bits getting one one zero zero and adding one okay (Refer Slide Time 20:25 min)



because going back to this one um two raised for n can be written as two raised for n minus one plus one okay 
so two raised for n minus one is the let me write it here okay two raised for n minus one is a all ones okay 
you subtract binary number from all ones basically each bit will get flipped right 
and then you are adding one later on 
so this this just shows the relationship between two ways of looking at that (Refer Slide Time 21:40 min)



so now um what we are doing we adding and subtracting twos compliment number is that um minus negative number are bringing additional two raised for n in to the picture 
so suppose in fact if you have a several number X Y Z W four numbers to be added some of them are negative okay 
so with the negative number we are carrying an extra two raised for n okay so if you are adding series of number some positive some negative um we we would we carrying if you extra two raised for n which will all go out as super flows carry on the left side okay 
and and the result we get is um correct value 
okay now this representation this way of relating negative numbers to their representation bring us to an easy way of doing subtraction 
suppose you want to carry subtraction of X and Y you all do X minus Y Y you can look up on as ones compliment of Y 
and sorry minus Y it can be looked upon as ones compliment of Y together with the one okay (Refer Slide Time 23:10 min)



right so so basically this subtraction can be looked upon an addition of X and minus Y and minus Y is this right 
okay there is a one additional fact you need to keep in mind is that there are some instruction programmable add immediate which require one operand to be sixteen bits and one is thirty two bits 
so how do you perform addition or subtraction with the one large integer one small integer the the hardware um which is there inside a processor would be common for add instruction or add immediate instruction okay 
the addition is added capable of adding two thirty two bits number so basically what is done is the shorter integers sixteen bit integers are actually rerepresented in thirty two bits okay
so how do you do this conversion the most significant bit which is actually the sign bit is repeated to fill up though additional sixteen point addition sixteen places okay 
so if you have a positive numbers like zero zero one zero in four bits in eight bits they get represented as four zeros followed by zero zero one zero 
but if you have a negative numbers it is this one which will get extended okay one one one one then one zero one zero right okay 
because this this programmable this what is this number its ones compliment is zero one zero one which is five and if you add one you get six so it is minus six 
now minus six when represented as a four bit negative number gets a this 
if same minus six we can represented as eight bit negative number we would got all this positions filled with ones okay 
so in in simple terms um we will look up on this conversion as a signed extension the sign bit is extended to fill up the additional bit spaces which you have okay 
so now because of this some instruction like load byte which i have only filling up um part of the word we we have talked about load word which gets one word or four bytes from memory and fills up a register this also an instruction load byte there is also one load half word 
so load byte pixel one byte from memory and places in a register 
now the question is what happens to remaining three byte position or remaining twenty four bits of the register 
in a um if you are looking upon this byte as a sign byte then they should be filled up with the sign 
if you are looking at this is unsigned byte they get filled up with um signed bit
so so they actually two different instruction load byte and load byte unsigned lbu 
so lbu will fill up the remaining twenty four bites with zero lb fills with the sign of the byte which you have got okay (Refer Slide Time 25:11 min)


so bit bit number if you are numbering from zero to thirty one bit number seven which is signed bit replicates and filled up all the positions okay 
now um let lets move towards a circuit design a performing addition the method is simples method is to follow what we do on paper and pencil okay 
basically we move from right to left and perform addition bit by bit so if you know how to perform addition of one bit that means one bit position adding two bits and also carry coming from right side 
you can just repeat the same thing for thirty two bits 
so each bit let us say we are talking of ith bit of two operands a and b so ai and bi represents the ith bit of two numbers a and b 
si represents ith bit of sum 
ci represents the carry coming into this position from right side 
and ci plus one is a carry going to the left side okay 
so this is a unit this is the module which performs one bit addition and a circuit two add two thirty two bit number nothing but um a cascade array or an array of thirty two such units right 
so the the carries the carry signals are though which connect one unit to other unit and form a change okay 
initial value of course zero which um is the initial carry for bit number zero so i am i am zooming that bit zero is the least significant bit and bit thirty one is the most significant bit 
so now following twos compliment notation i don t need to worry whether a and b are signed number unsigned numbers whichever they are um i simply pass and through circuit like this which to begin with the design for unsigned numbers okay (Refer Slide Time 28:35 min)



now what is this um one bit adder module it can be um design by trying to look at various possible inputs it has to cater for which can be listed in the form of a truth table um this truth table defines row by row what happens for each combination of inputs okay so we  we have a these columns label by the inputs ai bi and ci 
and here are two column label by the outputs ci plus one and si  
so we have these three inputs and eight different combinations which you can form for these three inputs zero zero zero zero zero one and so on and finally one one one 
for each of these we need to exhaustively defined what the outputs are okay 
so what we can do we can find out how many ones are there okay if this is the case when there is no one so both sum and carry are zero 
if this case and in this case this this and this these are three cases where you have a single one 
so sum is one and carry is zero like this like this or like this these three okay (Refer Slide Time 29:25 min)



um then there are some case where you have two ones example this here you have two ones here you have two ones here you have two ones 
so basically two one make it equivalent of binary two which means sum is zero and there is a carry okay um same way here same way here
and this is the unique case where you have all three ones which means the number three sum is zero sum is one carry is also one right 
now this is this describes the relationship between input and output and which can be captured by Boolean equations 
a Boolean equation then defined whats circuit you require to implement this 
so as far is sum is concerned it is those four combination of a ai bi and ci where we had sum is one so this one where you have a single one a single one a single one and all three ones 
another way looking at this is an exclusive OR of ai bi and ci which means that if number of ones in is odd you have a one if number of one is even you have a zero here 
okay carry and other hand is one when you have two or more inputs as one 
so other they could be ai and bi ai ci and bi um these two or these two or these two okay 
in the case of all three being ones is actually covered by this 
um you can you can actually if you familiar with crado map if you are done in this digital electronics or might be doing it you would know how to get this systematically by drawing a cardo map and finding a minimal representation in a sum of product form 
so now once you have got Boolean expressions representing output you you can set straight away um come up with circuit design which will do this 
so you have AND gates and OR gates you connect them exactly as declared by this to get the design i will not going to that (Refer Slide Time 31:46 min)



um one one thing which i will have to point out here is the the number of possibilities is large they could be several ways of writing the Boolean equation and therefore you can have different circuits 
so for example incases some i show to and then many more way of representing um  depending upon which gates you want to use 
and there are also variation in different representation in terms of whether you have two input gates or three input gates if you have term like a b c that means you are talking of AND of three okay 
so question is you you want to have you want to work with two input gates or two input and three input gates or whatever your choices are okay
what what is fan in of a gate you want to limit to the other issue may be that you have stages of logic um AND OR representation means that you have two two stage logic okay 
you have AND terms formed by AND gates and then oaring all those 
so um more stage is more gates a signal has to go through more would be delay okay okay so so implication of all these choices is different speed different cost may be different form connection okay 
so that itself is the um more complex issue which in course like digital hardware design you will see it more closely more deeply [noise] okay 
so our purpose we will um often try to use circuit which are easy to comprehend easy to rivet and comprehend okay (Refer Slide Time 33:25 min)



okay now we have seen how to build an adder how do you build a subtractor 
so basically we are going to use this formula that X minus Y can be represented as an addition of X and Y is a compliment and then you can add one more 
so now um please note that X and Y themselves could be signed numbers okay
this this formula is not just for X and Y as two positive integers 
X could be positive or negative 
Y could be positive or negative okay this will hold [noise]
so to get um ones compliment of Y you simply need to put inverter um for the second operand so now we we had a and b as our operands 
so bi is inverted before making it as input to the adder right 
so that takes care of doing X plus Y prime or adding ones compliment of Y to X and when you put these modules together to form a thirteen bits subtractor um this plus one can be taken care by making the initial carry as one which is in which is normally zero in case of an addition okay (Refer Slide Time 36:20 min)



these are two changes each b bit is inverted and the carry is initial carry is made one right 
now with um these two circuits for adder and subtractor um we are um we can see easily that they can be combined okay there is a slight difference we can combined and come of with this single circuit by using a multiplexer 
multiplex what multiplex does it it selects one of the many input it has okay 
so in our case that choices between taking the bi bits directly or through the inverter if we can put multiplexer to choose one of those two then we have a single circuit which will do addition and subtraction depending depending upon how we control the multiplexer 
so we have the same adder um the bi input can go either directly or through the inverter okay 
and in this multiplexer is choosing this right it requires it control signals or in control input which we calling binv standing for whether b should be inverted or not 
so so its controlling if you give a zero here the multiplexer will select this input which is labeled with zero 
and the circuit will do addition if you put it as one then this inverted input will be taken and circuit will do subtraction 
so putting um these boxes okay i am not showing that details now each of the blue boxes is repeated here okay i am calling at adder or subtractor 
um the carry input also needs to be control and its so to around you can use the same signal binv 
so when binv this control signal is zero the initial carry is zero and the bits of number b go without inversion 
when this input is one initial carry becomes one and bits of b b zero b one etcetera go away inverter okay 
so so this control signal is going to all these modules (Refer Slide Time 38:21 min)



alright now these are two arithmetic operations we have seen in the implementation we have not looked at multiplied and divide yet 
um lets move towards logical operations AND and OR okay 
so now the instruction in which you have in MIPS AND instruction OR instruction what they do is they perform logical operation bit by bit on thirty two bit vectors 
for example if you have a one register containing this another register containing this and you perform AND operation you will get this okay all all that you have done it we have performed bit by bit AND operation 
so only when both are one you get a one otherwise you have zero 
similarly OR operation over the same two numbers will give you this wherever you have either them being one you get a one and only when both are zero you get a zero okay (Refer Slide Time 40:08 min)



so the circuit is doing this very simple there is no relationship between what you are doing for bit i and what you are doing for bit j 
so for each bit position you have an AND gate right so ith AND gate AND ai and bi to give you si 
similarly ith OR gate out of thirty two OR gates OR ai and bi and give you si which is OR of these two (Refer Slide Time 40:24 min)



so now lets combine all these four we have addition subtraction ANDing ORing these four operation could be combined and let try to construct a very simple 

you would need to anasit further to bring in more operation but um lets keep this as the present target and once again all you need to do is put various thing together and add in multiplexer 
so this multiplexer has now three inputs which i have labeled as zero one and two and depending upon a control input which is labeled as an operation either you will select this or you will select this or select this okay 
so the input ai and bi or spread out to all these three components okay to AND AND  circuit to OR circuit and to this adder come subtractor 
of course for purpose of subtraction we are actually having this bi routed through this inverter and multiplexer okay 
so so this is one module one bit unit of ALU which is capable of performing one of the four operations and it needs to be guided by this input which can have three possible values and this input which can have two possible values of course um this is a um ignored or it is don t care when you are performing AND OR operation okay 
so so when you are actually choosing this or this what you are doing here it doesn t matter okay 
this this part basically effectively what we are doing is um the circuit is always there for performing all the operations right so you you give some ai and bi and the circuit would do AND operation as well as OR operation 
and one of the two among addition and subtraction but you are picking up only one of the results right 
so um when you are doing AND operation for example this whether it does addition or subtraction it doesn t matter okay 
so um therefore the value of this is ignored when this says you pick up zero or pick up one it is significant only when you picking up to here okay 
now 4331 here is a cascade of thirty two or such units which are called ALU zero ALU one and so on up to ALU thirty one 
so you notice that on one hand we have this bi inverter going to all of them and particularly in the first unit it goes to two positions in in the initial carry position and binv position and this operation control also form another control input which goes to all of these okay
so um now what what we have done here is we are taking care of the the core of these four instructions um AND OR add subtract 
and in fact strictly speaking we have actually a counted for not just the full instruction but additional four instruction which corresponds to there immediate version 
so there is a mis[take] so well subtract had known subtract immediate because the constant if you add negative constant you can effective low subtract immediate 
so um effectively that means seven instructions AND OR add subtract four and then AND immediate OR immediate which ANDI ORI and add immediate 
okay this seven instruction will all utilize this part of circuit um going further actually there are um as far as the add and subtract concern there are two different types of add instruction add and add unsigned so there is an instruction add and aadd and addu 
so um now question would be why we have two separate instructions add and addu when we said that addition can be done without with the sign ignored when you are using twos compliment representation 
then what is the purpose of having two different instruction okay 
the reason is um there is some thing which we have ignored here is the reduction of overflow okay 
what happens when the number which is coming out as a result here is not within the range of number which can be represented as 
so when you are adding two very large numbers um the the result may be beyond the limit or are you adding two large negative numbers still you you may exist the result on the exist limit on the negative side 
or you ar subtracting a large negative number for me large positive number or vice versa i will subtract large positive number for me large negative number 
again there range could be exceeded and it is here in the um in these the limits were positive and negative numbers differ or signed and unsigned numbers differ okay
if you are working with signed numbers you are you are range is um from some thing negative to some thing positive okay um to be more precise you you are maximum negative number is minus two raised for thirty one and maximum positive number is um two raised for thirty one minus one 
but when you are working with unsigned number the range is different zero to two raised for thirty one two raised for thirty two minus one okay 
so therefore detection of overflow is different depending upon whether the numbers are signed or unsigned 
so the process of addition subtraction remains unchanged but it is overflow detection is different 
and we will look at separately so effectively um this circuit will cater for these instructions there immediate versions okay 
they are unsigned versions of coarse there is addu and subtractu and there is also unsigned version of add immediate addiu 
so all this instructions um are taken here of by this small unit of course um we are not saying that this itself its sufficient to implement that an instruction 
so the the processor has to have other things the register file the program counter this is the mechanism of getting an instruction from memory interpreting it storing the result back and so on (Refer Slide Time 43:31 min)



but this is the key unit which performs the operation that you have seen okay 
so in let to summarize what we have learnt today we have looked up twos compliment representation for signed number and seen that its first up all brings unique representation for zero which is the problem in other 
and secondly simplifies our mechanism for performing operation so circuit is simpler we can ignored the sign while we are working with it 
we also notice that addition and subtraction hardware had lot of commonality and since we are either doing addition or subtraction in the processor okay 
we can share the hardware 
um we have seen that logical functions AND and OR can be easily included that design of the ALU simply by putting those gates and multiplex together 
now um one thing which we have left out is of course overflow detection but secondly comparison what we have not looked at yet okay 
so we we would now not today in the next lecture we will extend the circuit to a commutate instruction like slt or instruction like beq so beq makes comparison for equality right 
so we will argument our ALU to do equality check alright you will also segment it do slt operation which means the comparison for less than and generating a result which is either zero or one okay (Refer Slide Time 48:10 min)



so these two instructions will be accumulate in the LAU next time 







COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   12

ALU Design, Overflow 

we are discussing design of ALU which is the key component in the design of a processor 
so for we are discussed on ALU which can perform addition subtraction and logical operation like AND and OR 
today we will extend this design to include comparison um equality comparison as well as less than comparison 
you will also look in to the issue of um overflow how do you detect that the range of the result is a beyond permissible limits 
so that s in a overflow condition which defers from signed numbers and unsigned numbers 
so um this is again same thing which we looked at last time this is overall plan of the lectures on design of the arithmetic and logical part of the system
so we have discussed binary arithmetic how numbers are represented in signed and unsigned fashion and we are started with ALU design (Refer Slide Time 01:42 min)



so today in particular we will first look at comparison operation how do you compare signed numbers and unsigned numbers 
um then we will extend the ALU design which we have discussed so for to include these operations 
then look at the issue of overflow detection for signed and unsigned numbers in addition operation and subtraction operation 
um i will then summarize what is the difference between instructions which have u suffix which stands for unsigned 
so the various instructions which have there unsigned counter parts what exactly is a difference 
and finally i will talk of another operation which is not an arithmetic operation it basically manipulates the pattern of bits in a register and its called shift operation [noise ] (Refer Slide Time 03:00 min)



okay so the ALU we have designed so for um has this outline [noise] that you have um two inputs each thirty two bits and produces the result which is thirty two bits under the control of these signals which is desired which operation is to be performed [noise]
so what we have discussed so far they are four possibilities AND OR add subtract okay 
so which of these operation is done is desired by these controlled inputs and the number of signals here would depend upon how many different operations this is capable of performing [noise]
so the ALU design we have discussed is essentially um an array or or a change of identical units each looking at one bit 
and that design is shown here there is a one bit adder and the help of inverter and small multiplexer its able to take care of addition or subtraction operation 
and there is an AND OR gate with take care of the corresponding logical operations and there is a multiplexer here which selects output of AND gate OR gate or the adder okay 
so so you would require here for example two bits to decide between one of these three inputs and you require another signal one bit to decide whether it is addition or subtraction 
so you can put these blocks thirty two of them and change the carry through to get a thirty two bit ALU [noise] (Refer Slide Time 04:58 min)



now lets address the question how do you compare two integers
one possibility is that you perform subtraction and then check the result okay 
so if you if you have hardware built already for subtraction one could make use of that so let us say the same ALU which would perform you will control this ask it to say that you ask it to say perform the subtraction and look at the result check whether it is positive negative or zero
and that will decide whether a is greater than b a is less than b or a is equal to b so that s one way 
another position to design a circuit which is independent it can directly perform comparison of any of this times 
and here we [noise] can have a circuit which produces one of these three outputs directly without resorting to subtraction and then looking at the results (Refer Slide Time 06:02 min)



so we will look at both these possibilities one by one 
so first lets look at direct comparison because the other one is some what straight forward 
we will look at two different ways of performing a direct comparison i am i am looking at comparison for in equate comparison for greater than first okay 
so you could have comparison giving a result of a equal to b a greater than b or a less than b so we are looking at condition when a is greater than b [noise]
so here is an um we cursive definition here is a recursive definition of this comparison 
so what we are seeing here is that [noise] um we if you to look at the result of bits zero to i plus one of a and zero to i plus one of b 
this part of a is greater than this part of b this condition is equivalent to comparison of bits zero to i and some addition compa[rison] and some additional conditions
so if the comparison up to bit i says that a is greater than b and this bit that is ai and bi um says that either a is one of b is zero that means um as far as this bit is concerned a is either equal or greater than b 
and so for it is greater than b that s one possibility other possibility so for it is equal and now is greater okay 
so so we we are trying to define um result of comparison over actually this is i plus two bits interval i plus one bits okay 
so um in a recursive manner you can define so that means at one bit position you need to look at this logic and this logic and um you have cond[ition] condition generates for one additional bit 
so here we are trying to go from LSB to MSB least significant side to most significant side yes 
(student : ai plus one bi plus one is which i which i equal to the )
um yeah you are right you are right i think this should be okay read read this as i and read this i minus one or alternatively you could make this i plus one bit this is also i plus one bit (Refer Slide Time 06:24 min)



yeah so please note that correction 
so you you have you know the result up to i um you are comparing i plus one bit and updating the result for one additional bit [noise]
um yeah there is a same problem here okay 
now we are looking at results of bits i to thirty one okay 
and from that we are generating the result for i minus one thirty one okay 
so again the comparison here is starting from one end and going to other end the the difference between the two cases that in the first method you are starting from LSB and propagating the result to MSB 
here it is just the reverse okay so so clearly the second one has some what simpler logic okay and and the reason is clear that um it is the most significant bit which met us more okay 
so if you have the results of a few higher bits then then a simple logic yields a result for one additional bit 
so what we are saying here is that if you are found on the left side that a is greater than b um then don t have to do anything else or if so far you are found that a is equal to be then you should see that at this particular point a is greater than b (Refer Slide Time 09:13 min)


okay comparison for equality can also be written in a um recursive form in the same way [noise] um this is going from MSB side to LSB side 
so what we have to say ut if the comparison so for is the equal and comparison now is equal then the comparison um for one additional bit inclusive is equal 
and similarly um going from the LSB side to MSB side 
you can also write it directly that you you comparing these bits individually and you take a product or product in the sense of logical AND okay 
so so this product from i to thirty one i equal to zero to thirty one means that you are ANDing or conjuncting all these um individual bit comparisons okay 
so so this is a direct comparison we are not subtracting and then looking at the output (Refer Slide Time 11:13 min)



on the other hand if you want to subtract then a simple logic can be used to check whether the result was positive negative or zero [noise] 
so first of all this sign bit suppose a S zero to S thirty one are your comparator or your subtractor outputs okay this is the result of subtraction 
if you look at the sign bit if the sign bit is one then clearly a is less than b okay to check whether the result is zero you can put an OR gate across all this 
so that the output will be zero only if all this are zero okay and if you invert this we get an indication at this this line will be one if a is equal to b it will be zero otherwise okay 
so so this will be um one only this is zero which is the case when all these are zero okay 
finally um a greater than b if this is not zero that means you have a one here and the sign is also zero okay 
so so this is time to distinguish between um non negative and strictly positive okay 
so what we are saying here is that result is strictly positive excluding the zero (Refer Slide Time 11:42 min)



so now let us extend ALU design to include slt instructions 
slt instructions says that you compare first operand with the second operand and set the result to one if first is less than second okay
so so we need to have an indication of a less than b right which from the previous diagram can come out of this 
so so lets ignore these for the moment we are interested in a less than b
so a less than b signal can be picked from um the result of subtraction for ALU thirty one okay 
we we need to look at only one of the bits you have subtraction output the MSB which indicates the sign can be picked up of coarse there are some question mark which i am putting here 
so so there is we will come back to this an explore but calling to what we are discussed if you take this signal for ALU thirty one this will indicate whether we have to set the result to zero or one 
now what it means that um if a is less than b then you have to set all bits to zero and LSB to one 
and if that is not the case then you have to set everything to zero so in any case bit one to bit thirty one have to always set to zero 
and its only bit zero of the result which needs to made a zero or one 
so we we extend this multiplexer to include one more possibility which will come from an input which i am labeling as less i 
so in ith ALU you have an input which is labeled as less i and we will see how we make it zero or one okay (Refer Slide Time 13:19 min)



and this is tapped from the ALU thirty one so putting these putting thirty two of such unit together we get this okay 
so the the less input of all the bits except the first one that means the from ALU one two ALU thirty one the the less input is connected to zero okay irrespective of comparison irrespective result of comparison we have to set these thirty one bits to zero okay
and this bit will be set to one if you are getting a one here okay if the result of comparison says that the result is negative we get a one here and that s set is to one otherwise that that is a two zero okay all other bits are set as zero in any case 
so so now the roll of this controlled input and this b invert the these two inputs together will insure that when you want to execute slt instruction ALU should be instructive to perform subtraction and the multiplexer should be instructive to um select the fourth output (Refer Slide Time 15:35 min)



so in terms of this for performing slt binv should be one so that this does subtraction this together this whole thing does subtraction and um this mux should be control to select the last input 
so so that it is not that subtraction result which is passed down it is what your setting here one less which is passed on 
so basically subtraction is done but the result we are picking up is a zero one setting which is decided by the logic we have just discussed (Refer Slide Time 13:19 min)
okay so so that slt instruction we would also put some thing additional here to take care of beq okay once again um it is equality test so we will do it by checking whether the output of subtraction is zero or not 
so for beq also we are asked the adder come subtractor to perform subtraction and this gates will check whether all bits are zero or not okay 
so we we have i have not put inverter separately but instead of all are just made it in OR gate so it will give a one output only if all the bits are zero as such input which means the result of comparison a result of subtraction is zero or a equal to b 
so so now the ALU is capable of handling this entire set of instructions okay 
is there any question on this so for okay (Refer Slide Time 15:35 min) 
um now we going to slightly move involved issue of overflow detection okay 
how do we a figure out if the result which is coming out is exceeding the range if it is a beyond the limits 
so suppose we are doing unsigned addition okay i am i am just restarting this point um in this space we are we have two number a and b each is three bits okay just for illustration i am taking a small word size so the values can range from zero to seven b also ranges from zero to seven 	 
and of course this shows the sum okay so as the value increase the the sum increases but beyond seven which is the maximum limit um the value will wrapper on okay 
so for example if you take four and five the the sum should be nine okay um but it will what you will get is one because the eight value of eight will overflow go out and what will be left is one 
so um anything in this domain anything in this region which is shown in red is corresponding to the overflow area okay 
and the the indicators of the overflow or that they will be a carry at the extreme left end so if you are talking of n bits cn will be one the output carry from the last page so with n equal to thirty two it is c thirty two which will be one and that indicates overflow in unsigned addition okay 
an another indicator is that the sum you are getting will be less than any of the operands okay which you can be verified here for example the sum here is three and you would trying to add six and five okay 
this this will less than either of two so so when you are adding to one signed number the the result is either more than both of them or less than both of them okay 
so so one corresponds to first one corresponds to correct result and the second one corresponds to overflow the result now um lets look at subtraction unsigned subtraction so we will resume that you have two unsigned numbers two positive values and you are trying to subtract (Refer Slide Time 18:55 min)



the also the result you want is unsigned or positive okay 
so so what i have done basically i am lets look at again i am zooming that subtraction is being carried out by adding twos compliment of the number
so for example this row we will correspond to um subtraction of six okay to to these numbers if i add to i get this equivalently from this these numbers if i am subtracting six so which is a if you take six and its twos compliment ones compliment of six will be zero zero one and you add one you get zero one zero which is two  
so the result of subtracting six will look like result of adding two so all that i have done coming from this diagram to the next one is a these numbers are same okay 
you would notice that a progressing from zero to one diagonally overflow here and going to six but what all um change here is i have put there twos compliment
so so zero one two three four five six seven and in this case it is these numbers these results which are correct and these results which a invalid okay 
so so lets verified for example from five if you subtract three you you get this which is okay 
if you subtract four one if you subtract ze[ro] five zero if you subtract six that s invalid case okay 
because um to get a positive or unsigned result i should subtract only a smaller number from a larger number 
so so this red region corresponds to the case when the the second number which you are subtracting is larger okay 
now just to put the b value is right order high i can i can just flip this part okay and you will see that this red angle flips (Refer Slide Time 22:22 min)


so so basically um i could have written it directly also that these are a value these are b value so wherever b is more than a that is a invalid case and this entire region is the valid one okay 
so obviously along the diagonal i get zero and as i move away from the diagonal i get larger and larger numbers 
so the indicators here are lets cn equal to zero okay there is a in this green in this red region there is a value of cn which is zero and um in this case a will be less than both s and b okay 
a less than b is what we know directly that that s the result of that indicates that um things of out of range and so is the case in a less than s okay 
the value s you are getting here is more than the value of a we have (Refer Slide Time 18:55 min) 
okay now lets go to signed addition so it gets slightly more involved um now the numbers range from minus four to three again we are talking of same three bits so i am showing on both sides of this axis numbers go from minus four to three a is minus four to three b is also minus four to three 
and um it is this region where where you are adding two large positive numbers or this region we are you are adding two large negative numbers okay 
so basically what you can see is that as you move away from this diagonal at this diagonal you have zero and as you move you increase and when you go beyond three then there is an overflow 
similarly you go on the negative side beyond minus four there is an overflow okay so what are the indicators of a overflow condition lets look at this first that either you are in this region or in this region in in that top red region a and b both are non negative okay but the sum is negative okay 
you you are adding two positive numbers positive or non negative numbers but the the sum is you are getting is a negative number so which is a normalness condition and that s overflow or you are adding to negative numbers and you are getting a positive sum that is a region so that also overflow okay (Refer Slide Time 24:50 min)



so this is one way of checking that there is an overflow 
other way would be that um that last two carries will be of opposite polarity so what it means that for example there is a carry flowing in to the MSB but there is no carry flowing out from the MSB okay 
so that s one condition or there is no carry flowing in to the MSB but there is a carry flowing out 
so the the first condition that there is a carry in to MSB it means that number is turning in to negative when it is not supposed to okay 
you you are going towards a large positive number side but the number is exceeding on the positive side and its turning the sign to negative 
on the other hand you are you are suppose to have a negative value but um a carry is turning it in to positive 
so that means there is a carry in to the MSB um i am sorry there is no no carry in to MSB so it is turning in to positive but there is a carry out okay which was indication of normal negative numbers 
so so this is an another way of looking at the overflow 
is there any question what is that is that clear 
so now lets look at signed subtraction um signed subtraction what i will do first take the same values and propagately change b two is negative value so again we will do subtraction by um adding twos compliment value 
so the the pattern of numbers will remain same um zero will remain zero this will become minus one minus two minus three and one two three minus four will remain minus four because there is nothing like plus four in three bits okay 
in three bits the range is minus four to three so um the numbers are same ignored the color for the moment what i have changed here is that the sign of these three rows and this three rows has been changed okay 
so so the color has changed here because here i am trying to subtract minus four value from these values right 
so so with the subtracting minus four from minus four i get zero which is correct subtracting minus four from minus two i get plus two which is also okay but subtracting from any of these non negative numbers would mean that i am getting value which is um highly positive which is beyond the range 
so these are the one which turn to red okay 
so the sign of others remain unchanged as you can see um so for example check it out 
suppose um i subtract i take minus three here and plus two here okay 
so two minus minus three gives the plus five which is beyond the range 
so it is this region which continuous to be beyond the range this region is also giving me wrong values 
so i i should be getting positive values here i am getting negative i should be getting negative values here but i am getting positive (Refer Slide Time 28:33 min)



um okay so so now lets flips the rows to get the right order here 
and this is the picture i get (Refer Slide Time 30:24 min)



so so basically in in case of signed addition i had problem in these corners but now the problem area there this corner and this corner okay
so the internal sign the indication is that when a is non negative and b is negative and you get a negative result then the result is wrong okay 
or if a is well the first one corresponds to this region um corresponding to this region a is negative b is non negative and s is non negative okay 
so again some thing is wrong that s the overflow area and this indication remains same okay 
there there is a change of sign by um by by getting wrong carry in to the final bit position okay 
now with with this understanding of overflow we we know how to detect overflow in signed case unsigned case 
so when you are trying to use subtractions for less than  comparison than one has to be careful 
we we simply looked at sign of the result and took that as an indication of whether a is less than b or not 
but that that works only if there is no overflow when there is no overflow this result needs to be flip back okay 
so so you need to look at the sign but if there is an overflow just reversive diseaser
because that distance simply goes reverse when there is an overflow so so this is a correction which needs to be applied 
okay now lets get back to the instruction MIPS and look at the precise difference between what they do in case of signed and in case of unsigned numbers these are the instructions um which have there u counter part add subtract add immediate mult multiply divide we have not discussed multiply divide in detail 
but you roughly understanding the meaning lb is the load byte lh is the load half word okay 
slt and it s a immediate counter part slti 
so these are the instructions which have there u counter part say u suffix means unsigned what exactly is the meaning um the meaning differs from instruction to instruction so as for the these instruction is concerned um there is first of all a question of whether overflow is detected or not detected (Refer Slide Time 33:07 min)



so in instruction with the u suffix overflow detection is not done where as in an instructions which which are in the green column overflow detection is run 
what happen then overflow occurs in in a in the processor when some instruction results in overflow the processor has to take some special action it could be halting the program or it could be printing an error message or some thing 
so that we will see later right now we we remember that it s its an up normal condition which changes the cores of the program 
so we will we will thing of it that whereas the in the instructions which are grouped now these instruction there is also difference in the results okay that means from between these instructions add and addu the the result which will go in to the destination register will not differ whether it is add or addu or subtract subtractu and so on 
so so in these instructions which are outside this the the result will be same in both cases whether you are using with u or without u 
the only difference is that overflow is being detected or not detected in these instructions the result would differ depending upon which instruction you are using okay 
so you you would notice that divide actually falls in both category that means there is a difference of overflow detection and also there is a difference of the result 
as for as the add and subtract is concerned we have seen that twos compliment of representation the the result actually does not differ whether the num its only a matter of interpretation you take two numbers and perform addition 
if you interpret these numbers as unsigned numbers you get correct unsigned sum 
if you interpret these as a signed number you get correct signed sum okay 
as long as there is no overflow so when overflow occurs then of course we are discarding the result and taking some special action 
so as long as there is an overflow results identical 
that is not the case with multiplication but for some reason whether you take mult multu the result produces a same okay (Refer Slide Time 34:12 min)



so mult multu is not multu unsigned multiplication in the true sense okay
now um comparison also we have seen that signed comparison unsigned comparison the result would differ so if if you take two numbers let us say simply um two and two and minus three okay 
if you are doing sign interpretation then two is not less than minus three um but if you interpret these as unsigned numbers so minus three will actually look like a large positive number when you take an unsigned interpretation and you will find that first one is less than second
so the result of comparison differs depending upon whether you are interpreting this number is signed or unsigned 
and that difference you will find among these instructions as for as load byte load half words are concerned since these instructions are not loading a full word in to a register 
you need to define what goes into the remaining part so the remaining part gets filled with zeros if the instruction is lbu or lhu and signed extension is done if the if the instruction is lb or lh okay 
because lb lh will treat the byte or half word which you are getting from the memory as is a signed number and the sign will fill up the remaining bits 
so it is these instructions which are circle where signed extension is done so in lb and lh extension is done but not in lbu lhu um 
in add immediate instruction again the constant is sixteen bits but operation performed over thirty two bits so there there is a question of filling up the remaining sixteen bits in add immediate and add immediate unsigned in both cases sign extension is done 
so in that sense this u is a slightly a misnomer because the the number is taken as a signed number and extension is done okay its its only that interpretation result would be an unsigned results okay 
but but the constant is sign extended in both cases in slti um sign extension but not in sltiu so so this is the true interpretation of unsigned comparison 
is that clear so so there are three issues to be seen when you are talking of this instructions whether overflow is detected or not detected that needs to be seen um whether operation has different meaning in signed case and unsigned case and are those meanings preserved (Refer Slide Time 37:55 min)



entirely whether sign extension is done to covert small numbers into large numbers okay
finally um we move to another kind of operation which a called shift operations 
and these are used manipulate bit pattern or extracting bit fields out of a long bit pattern and so on 
um one one of the operation is shift left logical so this restriction shows um shifting left by three bits 
so suppose you have some pattern in a register a zero to a thirty one shifting at left by three bits means all bits up push to left and um on the right side you fill in some zeros 
so you would notice that three bits have been thrown out and lost from the left side 
on the right side you have three zeros which i have been filled in 
it s a counter part is shift right logical where you are shifting the pattern of ones and zeros to write so zeros get filled in from the left side 
and you loose bits on the right side okay um 
now with the right shift there is the complication which is a rising here if you if you are interpreting these numbers as signed numbers 
then possibly sign could change in this operation okay 
um why would you like to interpret these numbers as um as an integers 
if you do so you can take shift left as a multiplied by two operations okay one bit left shift is multiplied by two three bit left shift is multiplied by eight okay 
similarly [noise] right shift is um integer division by some power of two so shifting right by three bytes you would like to see it as division by eight okay 
so if the if the numbers are unsigned numbers this this is fine this is no problem but if the number is the signed number and you are shifting it right hoping to have it interpreted as division by power of two then problem will occur because you are possibly changing the sign 
if a thirty one is one earlier now it is made zero okay so there is yet another instruction yet yet another operation which is called shift right arithmetic 
so what we do here is something like shift right um shift right logical but instead of feeling in zeros you do sign extensions 
so the space which i getting vacated or filled by sign bits 
so for example here the picture shows shifting by two bits and a thirty one is replicated to fill up the vacating positions 
so so if this is done then you are correctly dividing a signed number a sign integer by a power of two 
so of course what you are getting is the quotient the remainder is being thrown now right (Refer Slide Time 41:55 min)



so so yeah 
(student: in the multiplication it may also happen if a a thirty one was zero)
yeah that is the case of overflow 
okay if number was large let us say um you had two zeros on the left and remain there was one after that suppose eight twenty nine was one okay eight twenty nine in eight twenty eight they were one after that and these were zeros also there is a large number which you are trying to shift left multiplying it by eight 
and if a one comes up here it might look like on negative number now so this is the case of overflow actually alright 
so if you if you word actually a keeping one keeping a thirty one here and you are not allowing other bits to enter you are getting wrong value in any case 
so this is nothing which can be done here but here we can since we are reducing the number its not an overflow which is occurring um but you are artificial putting zeros here so that can be prevented 
what are the instructions in MIPS which do this 
there is a instruction called sll shift left logical it takes two registers destination register and source register and a constant 
so so this is a constant which can vary from zero to thirty one and um this is this constant is in the shift amount field you remember that when i talked of add instructions i said that there is a field which i labeled as shift known but i said it is not used 
so so this is the instruction where it is used that five bit field can carry a number zero to thirty one and that indicates amount of shift you are having 
but once again still we have we have one field which is not being used rt field is not being used here in this instructions alright 
so there is still some wastages there is another version of this instructions sll sllv which stands for shift left logical variable instead of providing a constant as a parameter as the amount of shift here we use a third register to specify this number 
so its like um this like immediate specification and this is like a register mode okay 
so this value part of this instruction here it is part of the register um 
now since this register thirty two bit register um it can have a large value okay but what we are looking at is the least five bits of that okay 
similarly there is srl for shift right logical and srlv shift right logical variable and shift right arithmetic and it s a variable part okay (Refer Slide Time 45:45 min)



what kind of hardware is required to do this kind of you its very simple suppose um you are talking of eight bit number and you want to shifted by two positions 
and and i am showing a circuit where you want to do it selectively you either you want to carried state or you want to shifted 
so so these are let us say eight bits of a number um you give it to multiplexer and wire wired the other input of the multiplexer with the these position shifted okay 
so let us say this is the LSB which i am connecting to third position and so on 
the the first two positions are filled with zeros so this multiplexer will either select this set of eight inputs and passed them on to output or this set of eight inputs and passed them on to output okay
you you would notice that in the second input these two bits are getting left out and zeros are filling in to the left position 
so simply a multiplexer with some wiring at the input can select between no shift and shift by two bits (Refer Slide Time 45:54 min)



what do you do if you want arbitrary shift anything from zero to thirty one you can put um multiple units like this okay 
um so so they are five of the precisely the first one selects between no shift and one shift this can take care of zero shift two shift this selects between zero shift and four shift and so on 
so by suitably choosing combination suppose you want to total of um nine bit nine position shift 
so you will select eight and you will select one so the combination will give you shifting by nine position 
so so basically that five bit number which specify the amount of shift is used as a control input for these five shifters okay
i am showing i am not showing the details of the shifter but each of the shifter is essentially a multiplexer with probate wiring 
the the shifting in the wiring is the according to this number by which this mult this box is required to shift so this combination suitably can shift from anyway from zero to thirty one positions (Refer Slide Time 47:15 min)



okay to summarize what we have seen today we have seen how comparison can be done either directly or using subtraction we have discussed how to do overflow detection in case of sign addition subtraction or unsigned addition subtraction 
we have discussed shift operations we have seen what instructions are used for comparison and what instruction is used for shifting how how the instruction differ in terms of overflow detection or action of overflow detection 
and we have also looked at the circuit to support the instructions (Refer Slide Time 48:28 min)



are we stopped at this if you have any questions thank you 




Transcriptor Name: G.Sathis Kumar 

COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   13
Multiplier Design (Time 51:35 min)

we have discussed the design of ALU with respect to addition subtraction comparison and logical operation we also discussed other operation like shift
we will continue on this and take a slightly more complex operation namely multiplier multiplication 
so we will see how um multiplier can be design both for signed case and unsigned case 
and in the next lecture we will move to another operation which is complex that is division 
so in the plan of um lecture we have reached this point talk talk of multiplier design(Refer Slide Time 01:31 min)



so we will begin with the very simple design shift and add multiplication um we will see how you could build the circuit following the shift and add approach 
we will see um multiple ways which defer interms of cost of hardware okay or simplicity of hardware then we will talk about signed multiplication where you can either do unsigned multiplication then take care of the sign separately or do directly um signed multiplication 
we will see what are the essential differences in the circuits which are required for signed and unsigned multiplication (Refer Slide Time 02:16 min)



so lets begin with multiplication as you do with a paper and pencil okay simple multiplication method as we are all used to um only thing is that we do in decimal system and the same thing translated to binary in fact is even simpler as you would notice
suppose you have a three bit number A which has to be multiplied by another three bit number B 
so as usual we will multiply A by each bit of B with suitable vertex and simplify add them 
so multiplied by the LSB of B we get zero one one multiplied A with middle bit of B you get a all zeros multiplied with left most bit of B you get zero one one 
so so basically multipli[cation] individual multiplication is multiplication with either zero or one which is very straight forward 
multiplication with zero results in zero and multiplication with one results in the same multiplicand itself okay
and these partial products okay each line represents a partial product they have to be weighted propagately and the weighted here is power of  two which means the place shifting it propagately towards left and simply add all this 
so so that s the simple way which we will try to capture hardware and the the way hardware work is that you will start with um zero um value in some register okay 
you can add first partial product to that you get this add second partial product in this case of course there is no change 
add the next partial product and you get a final results okay 
so so basically you are carrying out addition of initial zero value and this partial product to get this then next one is added and in the next one is added so as you can see the whole thing breaks um into multiplication by zero and one shifting by um successively one two three four so many positions and adding all these up 
so um we have already seen each one of this individually we we know how to add we know how to shift and of course all we need is to multiply with zero one which means um performing essentially AND operation okay (Refer Slide Time 04:35 min)



AND operation actually um is equivalent to multiplication by one bit 
so we can express this as a summation where A is multiplied by bits of  B that is Bi with a weighted for two raised for i and we summit to over all values of i going from zero to n minus one 
so this is an unsigned multiplication and the circuit for doing this can be easily build um first thing we are trying to do is multiplication with zero or one okay 
so it is effectively A is multiplied by one of the bits of B so here I am illustrating with a four bits um situation 
so this AND gate actually represents an array of AND gates where um [noise] each bit of A is ANDed with B zero okay 
so um in a set of i am showing a single AND gate which is where we are taking a vector of bits A and single bit B and output is a vector okay 
so so strictly speaking i should um i should a mark um the the size of the vector going into or out of it 
so each each of this AND gate is effectively an array of four AND gates 
so what i am getting here um add this point is the partial product A is multiplied by one bit of B (Refer Slide Time 06:38 min)



and then i require um shifters okay so each of this shifter is doing shifting by a fixed amount alright 
so its only a there is no active logic here its only propagately wiring it right 
these shifter do not have to select between shift and no shift it s a fixed shift with each is doing so its simply a matter of wiring it propagately um so that effectively shift takes place 
and the next stage is addition so starting with a zero here we add first partial product then second partial product third and fourth and finally we have a product of A and B 
so um when i say that these shifter is simply wiring it means that these four bits are connected at propagate points at the input of adder okay 
you you connect directly or shift by one position or shift by two position or three position as the case is this alright 
so so this is one very simple way of capturing this idea of summing the partial products to get an unsigned multiplication (Refer Slide Time 07:50 min)



is any question about this 
okay now this requires n adder if you have n by n multiplication to be carried out this will require n adder each is adding one partial product um we can simplify the circuit by using same adder several time okay 
so we can captured this in the form of an iterative algorithm 
so i am keeping a count i a sum is S is initialized to zero and then i do some thing repeatedly there is a loop where A into Bi multiplied by two raised for i is accumulated i is updated and i repeated for all values of i okay 
so what i am trying to indicate here is there is an initialization step which is step one 
and step two which is repeated n times 
so um step two is both these assignments so actually what i am implies that all these both these are done together in the same clock cycle 
so um for n bit multiplication this would be done this loop will be done in n clock cycles although i have written as two separate assignments 
but my intention is to do these two in hardware in a single cycle 
now um they could be some improvements made in this instead of um adding A multiplied by two raised for i what i could do is i could actually keep on shifting A itself okay keep on modifying A so every cycle A will get doubled alright and then i need to add A only 
so let us see the algorithm which is modification instead of saying s accumulates A into Bi into two raised for i
this i am putting as the condition let if Bi is one then i do this accumulation otherwise i i don t do this addition okay 
and in any case whether si whether Bi is one or zero A is doubled every time so i could say A is doubled or shifted left its same thing 
and all these three activities added in a single step okay as i add A to two s i i prepare the next value for A for the next cycle in the same step okay 
so the the whole thing is a one clock cycle as for as hardware is concerned updating i doubling of A and conditionally accumulating of A alright 
so so basically shifting of A here is taking care of taking care of this multiplication by two raised for i
so since i am doing it sequentially i can keep on incrementky shifting it every time 
the the next modification here is that instead of looking at different bits of B in a successive iteration i i can keep on shifting B in a register so that i always look at B zero that will also simplify the hardware to some extend rather time to look different bits in different cycles i i look at same bit 
but move the bits in such a manner that i need to focus at the same point always so here i am checking these zero always but to um compensate for that i am making B as B by two or which means B is right shifted every time um to to get the same effect okay 
so now lets take this and look at it s a hardware equivalent (Refer Slide Time 12:07 min)



so it s a same algorithm [noise] what i require is essentially um a mechanism to um add A to s okay here is an adder um A is an one input s is another input i am not showing how i am making s is zero initially 
so so that detail is limited but this will take care of performing this type of S gates S plus okay 
and to take care of this A will be shifted left after A supplies while A form input to this adder A will also make itself ready for the next cycle so at the edge of the clock S plus A is a stored in to this and two A is stored in to A okay  
so these two events will happen concurrently with edge of the clock (Refer Slide Time 13:10 min)



now one thing is you should notice here is that this adder has to add two n bit numbers when you are talking of two n bit A and B both are n bits but as the partial products are effectively shifted to left we need adder of double the size okay 
first addition would be done um at one extreme position and last addition would be done at the other extreme position 
so adder has to be wide enough essentially double digits to accumulate all this values 
and this register which will hold A um also is of double the length so the the operand initially placed in the right of  in the right of the value is placed and it keeps on shifting to the left and by the end its reach the other end okay 
so so this is the key part of the circuit and to control this we require another register holding B which will experience right shift every in every iteration we shift B right so that we are always looking at B zero position which is the LSB of  this
so this has to work in conjunction with control which firstly takes care of this iteration count the process has to be repeated thirty two times so there is it does the counting and it also times operation of all other units okay 
so i am showing these red signals which a controlled signal this is working under control of B zero soon
 um with depending upon the value B zero it will actually instruct addition to be done or either this circuit will pass on the value of A here sorry A plus S or just S itself okay 
there is some logic here which either performs the S plus A or S plus zero okay 
so that control is here and um this is this and this are basically to time the operation of these registers when they shift and when they stores in any value okay
so so this is an essence of circuits they are some details which are omitted here but um what i would like to notice here is a how we are going from step to step and what what is each closure step here okay (Refer Slide Time 15:26 min)



each step basically involves these things checking B not performing addition conditionally left shift of A and right shift of B and update of  a counter 
so that counter i am not showing it explicitly its part of the control okay 
now lets do some thing so that requirement of two n bit addition can be cut down so we we can mange with n bit addition then some thing here will be simplify and the idea of this comes from let me back to this diagram this one (Refer Slide Time 04:35 min)
so you would notice that [noise] at any time one of the value which you are adding is only n bit and it its only when you look at the whole thing it is two n bit wide 
but if you look at each addition you are adding one n bit value in some position so if you focus on that you you need to have only n bit adder okay 
there is nothing changing on right of it there is nothing changing on left of it so if you arrange your information such that you you are taking care of those n bits where the new value is positioned you can work with n bit adder okay 
here is the modified circuit where what we are trying to do basically is that we are adding A to the left half of S okay SH is denoting the high high end of S or the upper half of the S 
so A is always added in that position and you are making sure that the two are correctly aligned um so what we will do is we will be shifting S to right every time 
so so initially S contains zero you you add that to the left half of S um then its shifted okay as a shifted to right then you add A to again the same position 
so so it is the partial product which are accumulated that keeps on shifting right instead of shifting A to left in every cycle we are shifting the part[ial] partial product obtains so for to right which which actually maintain the same reality position and achieve the same effect 
and the consequent that is that adder needs to be only on n bit adder 
so A is now n bit register A doesn t shift at all the value A gets added to left half of  S and after addition you can shift S to right 
so i will introduce another step here um that after the addition S becomes S by two so which one is you shifting it right and rest of it is same okay B has been shifting right um the counter increments okay 
so the crucial change is here that addition is only to the left half of  S and S shift right although i have introduced this as another step um but it is actually possible in principle to do all this in a single clock okay 
i will not going in to details um you can just take by words for it by moment by it is possible that the final value which has to be there in S as the result of summation and shifting you can actually look at that and placed that in directly in a single single step 
so that is not for difficult but just for conceptual clarity we assume that first A is added okay and then shift is done 
so so there is one more circle point here is that when you are shifting S right um there there is a carry which will come out of this which might come out of this that has to enter so its not a pure um simple right shift with the zero getting stuff in to the vacant position it is carry which you don t want to loose it s a individuate carry which has to be accommodated here 
so apart from that um shuttle consideration its very straight forward what we have achieved is we have reduced the size of this register by a factor of two we have reduced the size of adder by a factor of two okay 	
and having done this um there is another interesting observation which can be made is that um we have now two registers S and B which are shifting right okay 
so initially S is all zeros and as you perform addition and shift right you you keep keep the bits keep truckling in to right half of S okay initially everything is zero
so as partial product can added one by one bits keep on entering the right half of S okay
at the same time B is a this register is filled with value B as it shift right it keeps on making spaces on the left side 
so this observation leads to the possibility of combining S and B in the same register okay 
so so what what i am saying is that initially right half of S is vacant and it is getting filled up gradually bit by bit from from the middle okay
whereas B initializes full and getting vacated bit by bit on the left side  
so so that two actually match so the two together never require more than two n bits 
so what we can do is we can actually um initialize as with zero in the left half and B in the right half okay 
and um then look at the right most bit of S instead of right most of bit B and rest remain the same 
so the initialization of S is different we have zero and B put together performing to n bit word which is put in S and okay i yeah here shifting of B is wired there is no B register now so s become S by two which is a right shift and rest of it remains same 
so so this is the final circuit which we will i will leave at it requires one two n bit register one one bit register and a single n bit adder okay 
any questions about this [noise] 
um in in MIPS we have mult multiplying instruction which actually takes two operand in two registers and it produces a two n bit results okay um which are kept in two special register one is called high and one is called low Hi and Lo 
so there are special instructions which can be used to move data between these registers and one of those thirty two general purpose registers (Refer Slide Time 24:00 min)



so but the output of multiplication goes to those specific registers high and low [noise]
actually mult there is also pseudo instructions for multiplication which takes three registers 
so two for operand one for result and that actually expects um small numbers so that that product with in six within a thirty two bits and it ignores Hi only Lo is looked at and automatically brought into one of the registers [noise]
okay now lets move towards sign multiplication okay so there are two approaches to this one is we handle sign in magnitude separately that means if there two signed numbers we find them magnitudes multiply them get magnitude of the product and determine the sign so sign you know is a if the two numbers are opposite sign then the sign is negative they are of same sign sign is positive 
and must we know sign of the product you can put it back in the propagate form twos compliment or whatever approach you have 
the the second approach is to directly multiply sign integers okay 
so unlike addition and subtraction where this twos compliment representations made it possible to look at sign and unsigned addition subtraction identically except for overflow detections but the addition subtraction process was oblivious of whether there is a sign or not okay 
um for multiplication that is not exist okay 
so we we have to device a method which can directly multiply signed numbers 
and what that requires is a common expressions representing the values of positive as well as negative integers (Refer Slide Time 26:32 min)



so we we expressed um the number in terms of it bits in this form okay 
um here you see basically we have expressed B as a summation of Bi into two i for different values of i (Refer Slide Time 26:48 min)



we need to find some thing which handles positive as well as negative numbers again representation we will use twos compliment 
and this representation is shown here 
so you would notice that there is similar kind of summation but it goes to bit n minus two only the last bit which is sign bit is handled separately 
so all of that we have done is put a negative sign with this right 
so again the weighted is two raised for n minus one here um but only difference is that this comes with the negative signs (Refer Slide Time 27:16 min)



so why its so we can see it here 
so lets look at two cases what what happen when the number is non negative okay and what happen when the number is negative 
so if B is a non negative we we can we could have expressed this in this form okay that s the usual thing um for unsigned number but since B n minus one is zero because this number is a non negative the sign bit is zero 
therefore if you pull out the last term and put a different sign it doesn t matter this time is zero in any case okay with B n minus one is zero whether you put plus or minus it doesn t matter because this part is zero so um a positive in non negative integer B can be expressed in this form alright 
so this part is this this case is very straight forward the other cases that of negative numbers which needs the little bit of analyses 
so when B is negative um its value is basically minus magnitude of B okay
so this represents this is the magnitude of B prefix with minus sign is a is what B is all about okay
now let us see what the magnitude of B would be so since it s a negative number we can express we can find the magnitude by knowing that it s a twos compliment representations 
so twos compliment representation means two raised for n and minus waited some of the bits okay 
so so this is the interpretation of that number had it been an unsigned integer 
so to find it s the equivalent um magnitude retained this as a negative number 
we we take that value and subtracted out of two raised for n 
so so this is by definition of twos compliment representation that you you subtract this from two raised for n 
um so so now this summation can be broken up we take n minus two terms keep them inside summation and bring out the one which corresponds to sign bits okay 
now B n minus one is one in this case right so so this thing is nothing but two raised for n minus two raised for n minus one okay 
since two raised for n is nothing but two times two raised for n minus one this difference will correspond to let me write it here 
this this whole thing outside the summation is the equivalent to two raised for n minus one i am sorry um one in the subseq[uent] okay 
so so this whole thing is equivalent to this which i have rewritten um bringing B n minus one back because that is this is equal to one so i can write this multiplied by B n minus one which is one to to get in this form 
um so so this thing substitute here so i get minus B n minus one two raised for n minus one and then becomes positive so i get this 
so so i have shown that both positive or negative numbers then they are expressed in twos compliment form can be captured by a single expression okay 
so now now we have an expression which ignores whether it s a positive or negative number and we can multiply the two together (Refer Slide Time 31:15 min)




so direct signed multiplication um is a basically A multiplied by this expressions but i i work on this little bit more on this expression to get to convenient form so um first of all let me expand this what i get is first on negative sign all other term in positive sign okay 
now all other terms are broken in to two parts so for example this Bn minus two into two raised for n minus two is written as this okay sum of these two term 
where as effectively have put a factor which is double of that okay if you take Bn minus you you can actually verified by working reverse
take Bn minus two common out of this you get two raised for n minus one minus two raised for n minus two okay 
this is the this actually is twice this okay so what you will get is so if i break each of this terms like this i i can then combined the term with same power of two 
so for example here these two have same multiplying factor two raised for n minus one what i get is Bn minus two minus n minus one 
similarly this term will combine with the next term involving Bn minus three and so on 
so what i get is this summation of Bi minus one minus Bi weighted with the two raised for i and summed over i goes from um i think this should have been zero yeah 
so this is zero but to facilitate writing in this particular manner i have introduced a dummy B minus one okay this this goes up to B minus two but otherwise this term will remain unpaired to combine it with another term i have introduced zero term which is actually zero by definition assuming that B minus one is zero by definition 
i can reduce the last term here so that every time gets appear 
and in this formula can be written uniformly right so given this representation of B you can write A multiplied by B as this summation okay just that in this i bring in A with in summation 
so now what does it require it requires partial products to be formed according to this okay 
instead of multiplying A with Bi which is zero one i am multiplying A with this term and this this can have what other values it can have it can have a zero value plus one or minus one okay 
so still things are not too difficult minus one would mean that i i take actually minus A so this partial product would be A zero minus A 
and adding minus A means basically subtracting A right so um i have some thing which is very similar to what we have done 
they are just if you difference which i will um enumerate insteadly this is called booths algorithm 
so lets compare unsigned multiplication signed multiplication here um in unsigned multiplication what we saw earlier we look at Bi which can be either zero or one and accordingly either perform no addition of perform addition of  A whereas signed multiplication we will look at two bits together okay 
Bi and Bi minus one actually we are looking at this minus that so which can have value zero one or minus one so when both are zero we have no addition nothing to be done no addition no subtraction when this is zero and this is one since we are doing Bi minus one minus Bi we need to addition of A
when this is one this is zero we subtract A and when both are one again we require no addition 
so so with with this small change introduce all the circuits which we had could be made to work for um signed multiplication okay the only thing you remember is that what we had shown as adder needs to be capable of performing addition and subtraction okay (Refer Slide Time 36:10 min)



and they has to be little logic which looks at the pattern of Bi and Dm it looks a two bits and decides instruct that circuit to perform either no addition or addition or subtraction 
well this algorithm which i mentioned as booths algorithm was the originally devised not for signed multiplication its motivation originally was some thing different and which has follows 
that um if you have lets look at the multiplier B if you have a row of ones okay as per the logic we we are doing some thing when there is change from zero to one or one to zero (Refer Slide Time 37:55 min)



let me go back to this one see when both are zero both are one there is nothing required when you are going from one to zero there is an addition when you are going from zero to one there is a subtraction okay (Refer Slide Time 36:10 min)
so lets this is part of um the word comprising B and you are scanning the bit from LSB to MSB so when you have a run of zeros there is nothing required when you have a change from zero to one then you perform subtraction 
and when you have a change from one to zero you perform addition alright 
so another way of understanding this is like this that you can write in number like this as one zero zero and subtract a one from this position okay 
so so actually it is a this one which corresponds to one subtraction and this corresponds to an addition alright 
so um when this algorithm was devised by booth the the attempt was made to minimize the number of addition subtraction which are required okay (Refer Slide Time 39:10 min)



and that was in a context where a step involving in addition will take longer and a step not involving the addition will take so at a time 
so attempt was to um see if there if there is chain of one if there is string of ones instead of doing one two three four five six addition if you perform one addition and one subtraction so so there is a speed up there 
but of course in a modern hardware um each iteration takes one cycle whether you are doing addition or you are not doing addition 
so so its not consider as a mechanism to speed up things but it it gives as a mechanism to look at positive and negative numbers in a uniform manner and carry out signed multiplication directly [noise]
so now um finally lets lets look at what is the range of values which a multiplication would produced okay 
so when you are talking of unsigned numbers the result would vary between zero and this term which is nothing but a square of two raised for n minus one so this is the largest unsigned value you have and square of that is this okay 
so this is the largest um sum you can get alright so this this number could be expressed i illustrated with the eight bit example and equal to two eight this what you will have is the two raised for two n 
basically corresponds to a one here and all zeros so from that you subtract two into two raised for two n which means two raised for n plus one that means you you subtract a one in this position 
so you you get this number and this add one which is here so this is the largest number as seen in the binary you will get that means [noise] there will be first n minus one ones there will be n zeros and there is a one so this is the common pattern you will find irrespective of what n is 
and obviously it requires two n bit registers to hold the result
on the other hand on the other hand when you take sign multiplication the the range would be so so the largest most negative number is this and most positive number is so if you consider square of this and square of this 
this will give you the range alright so so this is two raised for two n minus two within a um i am sorry this should not be a negative sign okay the most negative value will come no this is not correct this will be yeah this will also positive actually 
so so i should this is the most positive value and most negative value will come when you multiply most negative with most positive okay 
so so lets work it out what you get is minus two two n minus two plus two raised for n minus one okay so um okay most pos[itive] lets look at each of this first
most positive value will be two raised for um two n minus two so which should be this followed by all the zeros 
so so that s the largest positive value you get okay and the largest negative value you are getting is a you have [noise] minus two n minus two is a is actually this to that you add two n minus one which is a one here okay 
so this is the most most negative value this is the most positive value you get um roughly speaking you you are not you you are getting about half the value okay if you take approximately you you are getting roughly half the value half the value in the magnitudes okay 
but still you you have to use two n bit registers you you are not utilized in the last bit fully oaky 
so [noise] um MIPS has a mult and multu two instructions are there mult and multu i i think in the last lecture i ment[ion] i placed it wrongly in the group where overflow is detected and there is no difference in signed and unsigned operation but actually there there is a difference um between these two multu will interpret that two operand as unsigned integers right
so the ranges are different whereas mult will treat them as signed integers and perform multiplication get results in this range okay there is since you are accommodating all the values in two n bits you are providing for two n bits 	
um the two registers are Hi and Lo okay um so in both cases irrespective what the result is it can be contain with in two n bits it never goes out so there is no problem um of  overflow (Refer Slide Time 46:20 min)



and the other hand the pseudo instructions which tries to look at only n bits of the results okay there is a pseudo instruction these instructions let me let me 
so let me write this instruction if you have an instruction with these two operands each is a n bits the result in Hi and Lo and you need instructions move from high and you say r three of move from low um let me put different register 
so the result which is in two register can move to one of the gprs the high part another gpr contain the low part 
on the other hand the the pseudo instruction which works with um i think multiply with overflow it will take three registers 
so so the programmer is not bothering about the fact that the result first comes to high low and then it is transfer to r three sorry in this case result r two r three operand result is going in to r one 
so we are looking at only n bits of the results and ignoring the high higher n bits 
so if if the product is a bigger than that if it requires more than n bits than it s a case of overflow alright (Refer Slide Time 48:15 min)



so so this is the exact description of this instructions okay 
so to summarize um we behind with our paper pencil method as we understood from earlier of school that you you multiply the multiplicand digits by digits so same thing translates to multiplying the multiplicand by bits of the multiplier and then adding these partial products with appropriate weightage 
so this we could translate that into a circuit which had one adder from each partial product and then we sort of wrap it around out that in a sequential it yet process where same adder performs addition of various partial products [noise] okay 
then gradually we made certain observation and try to improve the circuits 
the first improvement was that we reduce the requirement of addition from two n bits to two n bits [noise] and made one of the register which is holding the multiplicand sorter 
then after having done that we also notice that [noise] the register requirement can be reduced by sharing in same register between the accumulated sum and the multiplier [noise]
then we moved on to the case of signed multiplication which was basically um derived from a common representations for positive and negative integers okay (Refer Slide Time 49:08 min)



and the resulting algorithm was booths algorithm although it s the original motivation was different um which has to save the time it s a mechanism which allows signed numbers to multiply directly okay
and then we have also discussed range of the values and different MIPS instructions which look at the number differently and whether the detect overflow or do not detect overflow 
i will stop it that 





COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   14
Divider Design (Time 54:36 min)

today we will discuss how binary division is carried out and how we build the circuit to carried that out for processor 
we have been discussing various arithmetic operation and logical operation and we have discussed design of ALU (Refer Slide Time 01:15 min)



last we discussed was how we carry out multiplication and we talked about different designs of multipliers will continue with that talk of binary division and design of divider 
first of all i will summarize how we did multiplications because there is some overall similarity between how multiplication and division is carried out
and we will illustrate division operation by an example by taking two numbers the same way we did for multiplication then from that we will derive the algorithms we will ensure that algorithm we are talking of a correct and discuss the circuits
initially we will talk of unsigned division and then talk about signed division (Refer Slide Time 02:00 min)



so just to summarize what we did for multiplication we looked at an example and so that when you work on paper how you manually carry out multiplication 
so the same process was captured by circuit we went through a set of refinements (Refer Slide Time 02:20 min)



and finally we had a circuit in algorithm which look like this okay essentially the multiplication operation was broken down into shifting and adding [noise] 
so we then saw that for signed multiplication you need that absolute to add and subtract and we talked about booths algorithm 
so we also discussed the range of values which could result when you multiply and that two types of multiplication then MIPS which type to produce signed result or unsigned result
and they also pseudo instructions which work with the a single word output normally when you are multiplying two one word integers the output could be accommodated in two words 
so there are pseudo instructions which will check if the output is exceeding one word and accordingly give overflow [noise]  (Refer Slide Time 02:57 min)



okay so now lets start with a division by taking two small four bit numbers and trying to see how division is carried out [noise]
so let us say we have a number zero zero zero zero one one zero one which stands for thirteen how that is carried out how that is divided by a number zero one zero zero which is four interms of which binary equivalent 
so we we will expect that when you divide thirteen by four you get three as the quotient so in anticipation i have put that here and we should be left with one as a remainder 
so naturally this division would be carried out as a set of shift and subtract operations so initially we see if this multiplier this divider can be placed in this position here 
and we can subtract 
so so we examine um this position because this will correspond to um a bit in in the LSB position for the quotient okay
so so we are now targeting for a four bit quotient we have four bit divider and [noise] if you are able to subtract zero one zero zero in this position then it result in a one here 
but in this case we can not because this is larger in this position we are only subtracting a zero okay 
and we place a zero bit as part of the result so after subtraction we still have the same number and next time we try subtracting in this position so that its to another zero bit for the result and there is no change here okay 
so we get zero zero one one i am i am just dropping of zeros on the left side which are insignificant 
then we try subtracting in this position okay 
and indeed now we can subtract so we get a one in the quotient and after subtraction we get zero zero one zero one 
then finally we subtract in this position which is right justified okay and this results in a one in the LSB of the quotient
and finally there is a remainder one [noise] okay (Refer Slide Time 06:17 min)



so we carried out these four steps at each step we try to see if we can subtract or not if subtraction is possible subtraction is carried out and one gets regarded in the result is subtraction not carried out is zero gets regarded in the result 
so that that s the very simple mapping of normal decimal division we are used to in binary domain okay 
so we we require basically comparison before we can subtract we compare if necessary we subtract and we record as zero or one 
and from step to step we shift the position of the divider so so that in additional is the results 
you would notice [noise] lets call this as A which is the dividend divisor is B quotient is Q remainder is R and the values which can be subtracted or nothing but [noise] multiples of B the divisor um multiplied with powers of two okay 
and weighted with either a zero or one so in the first position it was B multiplied with two raised for three which we try to subtract from here so you could see that B has been shifted three bits to the left in a relation to A 
and in in position number three okay counting it zero one two three in Q we record the results as a bit zero [noise]
so so next time the attempt was subtract B into two raised for two and then B into two raised for one and B into two raised for zero (Refer Slide Time 07:10 min)



the last value being subtracted is actually zero one zero zero and not zero one zero one 
what we are trying to get is [noise] given A and B we are trying to find Q and R which will satisfied this 
so we are take a register R [noise] which we initialized with A A is the dividend and from this we will keep subtracting hopefully what will be left in A left in R would be the remainder okay 
the quotient Q initially is put as all zeros and another register D which will hold the value B which is the divider 
okay so then there is a loop where we repeat this comparison and subtraction step so if [noise] D multiplied by a propagate power of two is less than R okay 
so so this is D positioned at propagate positioned so um you you would notice that when i equal to zero in the beginning in the first step you need to displays it by n minus one position okay 
so i equal to zero the the power of two is two raised for n minus one so that is in our case n was four okay 
so we are shift we have shifted the divider by three position to the left we comparing it with R and the wholes then R is replaced with R minus D in to same factor 
and Q n minus one minus one that particular bit of Q is recorded as one okay 
if the condition doesn t hold we don t really have to carry out either you say subtract zero or you just leave while it is and the corresponding bit of the quotient is recorded as zero okay 
simply then step up i and repeated for n steps (Refer Slide Time 10:06 min)



okay is that clear so this over basic unsigned division algorithm we will analyze it to make sure that what we are doing is correct and then we will do some modification and improvements transformations and implement this in form of circuit okay 
so we want to make sure that what we are doing actually computes what we want okay our our requirement is that ultimately 
um [noise] there there is a loop which is executed and we will ensure that there is certain invariant it maintains okay um we will ensure that Q into D plus R is always equal to A 
so as as the loop proceeds this will be ensured and also we will ensure that R remains within this limit okay 
so so intuitively we have put up this invariant and first of all we must make sure that this invariant is a useful one which will give us the required result 
so at the end of the program when the loop ends i would have reached a value of n and if we put n in this invariant put i equal to in this invariant this will ensure that the remainder we are getting is less than that divider 
is um thus a unique this equality A equal to Q D plus R can be satisfied by many Q are combination right 
we are given A and D D is initially B okay and we are finding Q and R quotient and the remainder 
so so the this is single equation with two given values and they are multiple Q R combinations which can satisfied we want the one where R is within zero and D okay the remainder should be less than the divider and it should be positive non negative as you would say 
so with i equal to n this condition will be ensure at the at the end of the program um if you are ensuring the loop invariant um then we have the right quotient and remainder and remainder is within the range we except to be okay 
so therefore R is the correct remainder and Q is the correct quotient 
so so this is the this is to show that the loop invariant we are talking of is the meaningful one it will actually ensure that we are getting a correct results [noise] (Refer Slide Time 13:02 min)



now lets um prove it inductively okay first we will ensure that this hold in the beginning okay when i equal to zero when you start after initialization this holds or not 
so our step one which is initialization is making i equal to zero or equal to A Q equal to zero and D equal to B 
so if you put these values [noise] um then we we can initially we store that this holds right because Q is zero and um R equal to A right which ensured by this initialization okay this holds 
in this if you put R equal to A and Q equal to zero it is satisfied and also since i is zero [noise] um this relation ship that A is less than if if A is less than B into two raised for n then this will also hold okay 
um now how do you meet this condition initially um R is same as A and D is same as B so i i is zero so in order to ensure this what we need is A should be less than B into two raised for n okay 
so if we assume that A is n bit number okay and B is not zero so B is atleast one um then A is less than two raised for n or A is less than B into two raised for n
so this is satisfied if you choose A as a n bit number [noise] although we can in the illustration which i took i had taken eight bits for the dividend and that that was intuition that when you multiply two four bit numbers you will get eight bit product 
so i started with an eight bit dividend and try dividing it with four bits divisor um but they are [noise] cases where this may not work okay um so so we are to begin with we are talking of a specific case where um A is a four bit number or n bit number in general (Refer Slide Time 14:30 min)



that means the left n bits will be zeros let me go back to this so i although i had the right value i was taking i working with the eight bits but i had the right value because i had left four bits zero [noise] okay (Refer Slide Time 07:10 min)
so so this is um what could happen is if you have A larger um then algorithm may not work so we are trying to we um what we are learning here is that we will ensure that this algorithm works correctly if A is contained within n bits okay 
now lets try to show this in general in inductive manner okay so what will do is we will use mathematical induction assume that before a particular iteration this holes and we will show that it holds after the iteration that means it will hold through out 
so there will be two cases because of the algorithm we have a condition which is being checked action depend upon that [noise]
so we will take the first case here we assume that the invariants hold before the iteration i okay 
and we also assume this condition D into two raised for n minus one minus one is less than equal to R which means subtraction will be carried out this is one case 
so what happens in this case is the what would been examine now so um given to us is that this condition holds this inequality holds and we know that under this condition subtraction will be carried out 
so the new value of Q plus Q into D plus R um is given by this where we have [noise] value of Q updated okay we are setting this particular bit in Q and R has changed because we are carried out subtraction right 
so Q has become Q plus two raised for n minus i minus one and R has become R minus D into this power of two right 
so the the new value of this expression Q D plus R is given by this so if you expand this um you will be again left with Q into D plus R 
so so what it means that atleast this equality holds even after the iteration 
we can also check if this in equality also holds 
so so the condition of subtraction is ensuring that D is less than this okay 
and we know that um D into this greater than R which means that D into two raised for n minus one n minus i minus one is less than equal to R less than equal to D into two raised for n minus i okay 
so the left part of the inequality is ensured by over a function and the other part is ensured by um noticing that invariant holds before the iteration okay 
now R lies in this range an after the iteration once you subtracted this quantity from R you can see that R is going to lying in this range okay 
so R was in in this range from both side you can subtract this quantity and you can say that now after the iteration R lies in this range 
so now after the iteration i also changes to i plus one so therefore we can say that for the new value of i R lies in this range and hence um the invariant both components of invariant hold after the iteration alright (Refer Slide Time 20:00 min)



is that clear 
so so this was actually more difficult case which we have ensured 
the other case is when subtraction is not carried out is in fact easier 
so we assume that invariant holds before iteration and the subtraction condition is false that means D into two raised for n minus i minus one is greater than R 
now in this case [noise] the new value of Q in to D plus R is a basic it trivially same as Q D plus R because [noise] you are setting a bit zero you are recording a zero Q was initially zero so essentially there is no change there is no change in R 
so there is no change in Q D plus R 
and condition of omitting subtraction that is this one ensures that initially we have R lying in this range okay 
so basically R in in the half range of this and we are not changing the value of R what we changing is the value of i 
so with the new value of i you can say that R lies in range zero to two raised for n minus i okay 
so in this case also invariant holds and therefore we have regressively ensured that we have a correct algorithm (Refer Slide Time 21:24 min)


now we will modify it and prove it so that we get a suitable hardware implementation 
is there any question so far okay 
so now in in the algorithm which added we are talking of D multiplied by two raised for some factor okay 
so rather every time multiplying by that factor we will shifted step by step as we did in case of multiplication 
of course the directions of shift may be different here 
so we start with a value in D which is B into two raised for n minus one so initial shift is there by position n minus one and then we will always ensure that D has the right value so that you can always subtract D from R straight away okay
and this will be ensure by um making D shift right every time so we do D equal to D by two and the the this comparison becomes comparison of D and R and subtraction becomes subtraction of D from R alright 
also um instead of shifting instead of setting different bits of Q we will be shifting Q left okay effectively multiplying it by two and adding a one or adding a zero okay shift um left and set the last bit set the least significant bit by bit to zero or one okay 
so operationally this becomes easier it s a basically same thing um first time we were placing B such that there there were shift of n minus one position 
so we ensure that D is initialized to this number and subsequently move D right by one position every time okay (Refer Slide Time 23:25 min)



so once you have done this we can um draw a circuit which carries out this operation 
so you would notice that the the structure is quite similar to that of a multiplier except that um the shifts are in different directions addition has replaced by subtractions [noise]
so so here is a subtractor which will subtract D from R okay so R is one input D is another input result goes back to R 
um this D shift right by one position every time 
Q shift left by one position every time 
and um on the right side we will insert a zero or one as the space gets created in Q okay 
so there is a controller sitting here which will um look at the the result of comparison basically we are also assuming that this subtractor is doing compressed comparison okay 
so the result of subtraction is available actually at output of um this subtractor we may not store it actually if the result is negative okay 
so so basically we assuming that there is an indication that result is positive or negative and this control acts accordingly 
it will modify the value of R or not accordingly and it will set a zero one Q as Q shifts to the left 
so so this is a this circuit captures the straight forward algorithm which we had just worked out 
now we we will carry similar kind of improvements in this as we did for multiplier okay 
we will first make sure that subtractor is reduced in size right now it is a two n bit subtraction 
because D um D gets D has the divider which can be placed anyway initially its quite to the left and graduality shifting right 
so um all all bits are significant at some time or the other and therefore we are keeping a two n bit subtractor um what will do is that we will change this situation and make D stationary achieve the same effect by shifting R to the left okay (Refer Slide Time 23:55 min)



so there relative position is same right we we are shifting D right keeping R stationary instead of that we will keep D stationary and shift R to the left [noise] 
so interms of algorithm what we do is as follows um now you would notice that D will start with the value B okay its not B into two raised for n minus one D will have value B D will be an n bit register whereas R has um R has the dividend placed in correct position so that um the dividend divisor are relatively position correctly okay 
so you recall that the first time we shifted D was by n minus one bit and not n bits okay 
so so the same same position is maintained here because D will be now subtracted from R units left n positions okay as you will see now 
we are comparing D with the higher n bits of R okay R is still two n bit register and i am assuming that the two parts left n bits and right n bits will be referred to Rh and Rl high and low okay (Refer Slide Time 27:35 min)



so now it is RH or the left half of R which will participate in this subtractions so so let me again show you this diagram (Refer Slide Time 23:55 min)
i think it will be easier if i get back to 
so to to get this same effect um what we have we are going to um place A okay 
if you shift A to left one bit okay we we have you remember that we have placed two A in R okay which means A has been shifted to one position left and then placed in R okay 
so actually what will it contain
let me get this [noise] so initially i i would have obtain this position right not writing very clearly
okay i have placed A um one bit shifted left okay and B will be compared in this position 
so it is i am sorry 
so so this is a Rh this is Rl and i always be working with D which is n bits and Rh which also is n bit okay 
there will be compared and subtraction will be carried out 
um next time R will be shifted left D will remain here okay instead of shifting D we are shifting R left 
so the starting position is here which must be carefully noted okay (Refer Slide Time 31:00 min)



so coming back to the same point [noise] we we comparing D and Rh and if this condition holds Rh become Rh minus D okay and this part is same Q gets two Q plus one 
if the condition doesn t hold Q gets i am sorry this should have been [noise] um ignore this let me correct this 
so if the condition doesn t hold Q simply shifts left okay and R is also shifted left now right [noise] (Refer Slide Time 33:00 min)



so so with these two changes um we can have a circuit now that subtractor is only n bits size of register D is reduced and both the registers shift in same direction 
so now the next modification is very straight forward um [noise] we have R um shifting to the left okay and as the position get vacated here we can keep on stuffing the bits of Q 
so don t need to keep a separate register for Q and utilize the part of R towards the right end which is getting vacated to accumulate the bits of Q (Refer Slide Time 33:02 min)



so that that is a very straight forward change and we have omitted a register Q here 
so so we are basically doing R equal to two R plus one or R equal to two R 
so effect of shifting Q and R is sort of combine and at the end of the iteration Rh the left half of R will contain the remainder and right half will contain the quotient 
so same n bit register contains the results okay (Refer Slide Time 33:52 min)



so um this leads to circuit which is now in the final form we are we have a one register shifting another register which is n bit and is simple control which looks at this result of subtraction and looks at the sign and accordingly controls the operations okay (Refer Slide Time 34:22 min)



now this um we will introduce another form of division which is called um restoring division where what we do is 
um this step which was there where we are i am now getting back to the first form of the algorithm where we had just introduced after the basic algorithm we had introduced the shifting operation okay
so we we comparing D and R and accordingly we were subtracting so this is called actually non restoring approach 
we we first check and then subtract and alternative is that we we carry out subtraction in anticipation and if we have made a mistake then we restore of making a correction so that is called the restoring division 
so this was complete step two as a listed here and in restoring approach it changes the following way we subtract unconditionally 
and the next step in the within the iteration is that if now the result is negative then you correct make R equal to R plus D okay rest of it will remain same 
it is in this case Q becomes two Q and in other case two Q plus one okay 
so if R is not negative then you don t need to make any correction okay and D part D equal to D by two and i plus plus is common 
so so basically change has occurred here that we have introduced an unconditional subtraction and there is a conditional addition 
so um now in this case [noise] um we um we we are actually using two steps in the iteration okay so two clock cycles would be used because first you have to carry out subtraction its only then you can carry out um addition 
so apart from [noise] the fact that you are using ex additional steps the subtractor has to be replaced by a circuit which can do addition or subtraction but that s not a big deal we have seen how that can be done (Refer Slide Time 36:26 min)



the motivation for doing this is a what it is going to follow that we can actually postponed this restorations [noise]
by making this following observation that if you are restoring now in in any particular step you are restoring by adding D to R right 
and in the next iteration there will be an again initial subtraction D would have reduced by a factor of two so now you are sub now you are adding D and in the next iteration you will subtract D min[us] D by two okay 
the same effect can be achieved by not doing any restoration now and in the next iteration the initial unconditional subtraction may be replaced by addition okay 
so adding D and now and subtracting D by two later is equivalent to subtracting sorry adding D by two later (Refer Slide Time 38:05 min)



so what we are doing is at we are wording an additional step of restoring it now and achieve the same effect by choosing the initial unconditional step um to be either an addition or subtraction okay 
so the algorithm now looks like this that initially um we will look at the sign of R okay because possibly um they might be a pending restoration requirement 
in the previous iteration we might has subtracted where subtraction is not to be done and as a result we may have negative R value 
so if R is negative in the beginning of the iteration we um make it R plus D okay else you make it R minus D as usual right 
so R negative R implies that there is a pending restoration which we have postponed and there[fore] we start with an addition
otherwise the normal case we start with the subtraction so i am still showing a separate step where um after this we are recording a bit zero or one in Q and D is halved i is incremented 
and this is that completes the iteration (Refer Slide Time 39:56 min)



so so now [noise] um of course i am not yet eliminated two steps i still have two steps but i have just take in care that restoration is a postponed okay 
so in each iteration you are doing only one addition or a subtraction earlier there there was a possibility of one addition and subtraction both in same iteration and they have to be necessarily done in sequence 
now in this with the some modification we can actually collies these two steps 
but one important thing is that since you are postponing your restoration in the last step um if you postponed no more iterations are left you may still have a pending restoration 
so a final adjustment may be required here 
so i am i am adding a step there at the end if R is negative finally then you you would need to make a final correction (Refer Slide Time 41:00 min)



okay we will improve it further where we R actually combining the two steps so um regarding of that bit in Q is actually brought within this condition in a single step so if R is negative then you are doing initial addition and you are here you make Q equal to two Q minus one 
otherwise you start with initial subtraction and make Q equal to two Q plus one 
so so basically um [noise] what you would notice is happening is that um as you are doing lets this is an anticipatory subtraction which you do okay 
and in an anticipation you also regarding A bit one in Q 
so later on when you possibly correct this you also would correct this effectively right 
so so adjust recording of bit in Q is also done in anticipation and possibly corrected in the next step if necessary 
so um that effectively achieved by doing two Q minus one 
and this part remain as it is and there is a um final step so now finally also we may require a correction in Q 
so so this is a um step which if R is negative finally we we have a correction done in R as well as in Q right okay (Refer Slide Time 42:30 min)



so um let us let me you want me to illustrate how this correction Q is getting done or you can you can see its true okay 
let me illustrate this [noise] 
so what will happen is that see let us say we have some we have some bits of Q we have resome points and let us say we have put one in anticipation okay 
and if it is getting if it is correct it will be left as it is 
if in the next position um we are doing corrections then we will be subtracting one from this position okay next to it 
so so which means this um one i mean if you just see locally this one zero and from that you are subtracting one it will become zero one okay 
or if um let us say um if this needs for correction continues right so if you if you are subtracting a one again here then this this one also become zero and you get a one here and so on 
so so essentially a what will happening is that the one which you are putted in an anticipation gets gets converted to zero and you you are putting a one in the next positions and if even that is not correct next time when correction is done even that gets move further okay (Refer Slide Time 44:33 min)



so um i will suggest that you actually take out you take some examples and works with the algorithm so that you you i have convinced yourself that it works correctly [noise] okay 
now once we have brought the algorithm to this form um effectively we have now made it almost ready for a signed division okay 
what is effectively happening is that [noise] by in every iteration by adding or subtracting D we are trying to bring R closer to zero you are starting with some value and your attempt is to um bring it successively close to zero okay 
so so you subtract initially um large value then you subtract half subtract you are half of that then half of that and so on 
so essentially you are trying to bring it close to zero and if we observe it in that sense whether the value was initially negative or positive it it can still work okay 
if the dividend was negative we are still by adding a positive value we will bring it close to zero or if divisor is also negative by subtracting divisor you will bring it close to zero 
so essentially attempt is to um look at signs of R and D and accordingly either subtract or add so instead of checking whether R is positive or negative we will see if sign of R and D are same or different 
if they are different then we subtract sorry if they are different then we add because um if opposite sign values are being added then the result will be small and if they are same sign then we subtract (Refer Slide Time 42:30 min)
so so that is the change in the logic um i am looking at the MSB of R and D the sign bit and if they are not equal then we add otherwise we subtract okay 
and accordingly Q becomes two Q minus one or two Q plus one right 
so so that s the only change and we can use this same thing for um signed division 
the the final correction step also makes a similar check it compares R n minus one and D n minus one (Refer Slide Time 47:32 min)



if they are still of opposite sign then there is final addition which is being done 
so now intuitively is this correct um what is the relationship between signs of dividend divisor quotient and remainder 
so lets have a look at that 
so we are given basically dividend and divisor and we are looking at all possible sign combinations any of this could be positive any of this could be negative 
and the the two right columns show the corresponding signs of quotient and remainder 
so what is the logic which is intuitively governing this that sign of the quotient would follow similar logic as you have in multiplication okay 
when you multiplied two numbers of same signed okay that is positive into positive or negative into negative result is positive 
so same thing we will do you you are dividing a positive number by a positive number you get a positive quotient 
you divide a negative number by negative number you gat a positive quotient okay (Refer Slide Time 48:02 min)



okay so um the the quotient is following same logic as multiplication right 
so when the signs are opposite the quotient is negative when the signs are same the quotient is positive 
the the remainder always takes the same sign as that of dividend because remainder is some thing which is left out of dividend you try to reduce the dividend to zero but some thing is still left  
so it has to be the same sign and if you um look at this 
so final correction actually which is being made here will ensure that remainder is ultimately of the same sign this one this is no actually that that s not obvious from here [noise] i think it is a one one could prove that remainder would be of same sign although its not obvious from this condition um well actually i would like you to um i have written several algorithms 
and the kind of proof i did using invariants you should try for a few more okay particularly this one right 
so it should ensure that the correct sign is being obtained here um in particular the remainder should be a same sign as the dividend 
and the quotient would depend upon whether the sign of divisor and dividend are same or opposite okay (Refer Slide Time 47:32 min)
i i leave it to you to ensure that very regressively [noise] okay 
so let me summarize what we have done we started with um simple handwork example of four bit division in a unsigned case and we notice that using basically compare shift and subtraction operation you can carry out division in a sequential manner [noise]
so based on that we develop basic algorithm we analyze it thoroughly to ensure that it is correct and from there we derived the circuit 
um in the circuit then some improvements were made first improvement was to reduce the size of the subtractor instead of two n bits subtractor we reduce it to n bit subtractor and then we did some improvisation and reduce the number of registers 
so ultimately you work with one n bit register and one two n bit register 
um then we brought in the concept of restoring division 
so initially you carry out anticipatory subtraction there is no comparison involved here now um subtraction is an unconditionally i should say blindly you subtract and then restore
then we saw that restoration can be postponed which simplify each iteration and of course it made it necessary for us to have a final step where a final restoration of final correction was necessary (Refer Slide Time 52:02 min)



then with slide changes we were able to modify this algorithm some for signed numbers and um remembered that when you are talking of signed division 
we we are doing addition subtraction with twos compliment number okay 
so so therefore um the addition subtraction need not be done um or addition subtraction are not conches or whether the number is signed or unsigned 
as long as representation is twos compliment so that fits and nicely and we can have the algorithm work on two signed integers and performed division [noise] 
i will stop it that 
next time we will take up floating point operations and with that this chapter will be done   




Transcriptor Name: G.Sathis Kumar

COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   15

Fast Addition, Multiplication (Time 61:20 min)

we have discussed how to carry out basic arithmetic operations and i focused was to do them in very simple manner without worrying about performance 
today we are going to see what techniques can be used to speed up the operation improve the performance 
so in the overall sequence of lecture in this um topic um as you see that we have discussed the basic design of various operation 
and today we are going to talk of speeding up addition subtraction as well as multiplication 
and finally then we will move out to floating point operations [noise] (Refer Slide Time 01:15 min)



so um first what will do is we will look at a simple adder which we have discussed and see what actually govern the performance what is the bottle neck we will see a technique which is called carry look ahead 
we will notice that its carry which is taking time and some way to compute the carry fast is what is required 
then we will go over to multiplication and then try to see how multiplication can be carried out in a fast manner um [noise]
the the technique use they would be what is called carry save addition so where we postponed um use of carry rather time to immediately look for carry and wait for it we will postponed that and add at a different state 
so in the process we will speed up the operation [noise] (Refer Slide Time 01:41 min)



so coming back to this simple adder circuit which we have discussed we had discussed how we do addition using paper and pencil and over attempt was to capture that in a simple circuits exactly the way we do it that means we we take the corresponding bits of two operands perform the addition let the carry go to the next stage [noise]
so um this is a circuit which adds two individual bits of the two operands and these one bit adders are cascaded or change together to form adder of the right side 
so for example if you are adding two thirty two bit words then there are thirty two units which are put together like this [noise]
so in this you would notice that um C one which is the carry out of the first unit is dependent upon C zero and C two is depend upon C one and so on 
so it is this dependents of carry from one stage to other stage um which has to be taken in to account to complete the operation and our attention would be focused on this particular aspect of carry going from one edge one end to other end from LSB side to the MSB side [noise] {Refer Slide Time 02:46 min)



what each adder as is a simple task of looking at three inputs ith bit of a and b the two operands and the carry ci as an input and it raise to defined ci plus one the carry for the next stage and si as the sum for that particular stage (Refer Slide Time 03:57 min)



so it s a simple combination circuit with two outputs and three inputs the different ways in which this can be realized one possible realization is shown here in fact two si can be expressed as a sum of product form of expression with the ai bi ci as the inputs okay 
in a more compact form you can write this as an exclusive OR of ai bi and ci 
um ci plus one on the other hand is also a simple a simpler form its in sum of product form of three terms [noise] (Refer Slide Time 04:25 min)


so in this um how do you define the performance of such a circuit 
so basically we should go down to how a gate behave is when input changes 
so at some point of time suppose the input changes let me 
suppose we have AND gate okay and if you look at um waveforms in time suppose a is one okay and b is zero after some point when when it changes two n um then we expect that c would become one in response to this change but they would be some delay okay c is zero at when at least one input is zero 
so it is this delay which is attribute to the gate okay
so so gate has transistor which switch from one state to other state and it is that which takes time 
so also the gates are driving some loader some capacitance and that load has to be charged so um the charging time the capacitance is what dictates that delay 
without going much in to details of this delay we assume that each gate has some inherent delay 
actually there is a also delay in the signal propagating from one point in the circuit to other point in the circuit over the wire okay 
when when you talking of gates which are extremely fast where the delays of or of the row of peek of seconds tens of  peek of seconds then the delays of wired also the the time taken for the signal change propagate over wire that also becomes um significant and comparable 
on the other hand if the gates are working in nano seconds or um larger then the wired delays may be negligible 
for the for the present discussion we will assume that wired delays are significant okay because i focuses on the logic part of the design and we will take in to account that delay which occurs because of the gates 
so now in a larger circuit when there are number of  gates you have one gate fitting another gate that feeds another gate then this delay gets accumulated okay 
and the delay would depend upon how many gates you are putting in a series 
so so more the gates we have in a series the the larger the delays 
so roughly speaking you could say that if in ideal condition delay of one gate is lets the d units of time d pico seconds or whatever the unit is 
and you change k gates then then the delay will be k times d 
now the question is
is this factor d independent or not 
strictly speaking this d would depend upon how many inputs the gate has and how many other gates this gate is feeding okay 
so which means that delay is function of fan in as well as fan out um but it says for example a three input gate would have slightly more delay then two input gate and four input gate will have in larger delay 
but the the delay does not goes grows proportionally okay 
it it varies it increases slowly it doesn t increase but increases slowly so in in a idealize situation when you are not using gates with very large number of inputs or outputs then you might assume up as an approximate situation that D is more less a constant okay 
but we must remember we must keep this in mind that we are um idealizing we are making an approximation (Refer Slide Time 08:30 min)



okay so [noise] as i mentioned speed up the gate is effected by number of inputs to the gate and more strongly speed up circuit is effected by the number of gates in the series 
and there may be many paths you can trace in in a circuit from a input to output the the longest path the path which goes through maximum number of gates in the chain is called critical path or the deepest level of logic 
so in fact this is um really the motivation why we always try to express circuits Boolean circuits as um two level sum of products or you can alternatively do product of sum 
you can have more complex expression where there is a parenthesis and deep nesting of um ANDs and ORs 
but a sum of product form a product of some form is a most efficient from the point of view of this depth of logic consideration [noise]
so in the in the current design of the adder which we have talked of um you could imagine that delay of each one bit adder can be brought down to two times d okay 
you you can always express that as we did earlier as we i shown here um we we will look at this realization of si and ci plus one 
so you will have one level of ANDing you have a series of AND gates and then an OR gate okay i am ignoring again inverters this compliments for the moment they will also have delay which has to be counted for but it will be smaller as compare to AND and OR [noise] 
so this this circuit can compute this sum and carry in two d time (Refer Slide Time 04:25 min)
so now if you have thirty two of these put together or n of this put together in general then the delay will be n times two d alright 
now the question is how can we reduce this so what change is how do we restructure this gates how do we reorganize this 
so that the delay is reduced (Refer Slide Time 09:50 min)



so this is like um trying to improve your algorithm doing same computation in a faster time 
so now as we have seen the cause of the delay cause of the large delay is rippling of the carry from one stage to other stage to the next stage 
so its because ci ci plus one is generated from ci by this expressions 
so that dependents means that c one is expressed as a function of c zero c two expressed as a function of c one and so on
now the question is can we do some look at 
can we um avoid this ripple 
can ci plus one for example be generated directly from a zero a one a two a three up to ai and b zero b one b two to bi and c zero 
so of course the c zero is a primary input which we have to count for but if if you can write for examples can we write c four directly interms of a zero a one a two a and a three as well b zero b one b two b three and c zero 
so so we we can think of c four as a function of these eight inputs okay four bits of a four bits of b and one carry (Refer Slide Time 12:17 min)


so in in principle it is possible okay 
and let us see how do you do it 
so lets go step by step first we write c one interms of um a zero b zero and c zero that s the usual expression we have and we wrote c one we wrote c two in this form okay 
now all you need is you take expression for c one and substitute that in the second one right 
so so and also we expand because you want to ultimately have a two level expression you you could have a left that parenthesis you could say that b one plus a one within brocket and then expression for c one in another brocket okay 
and then you can say that these two are anded but then we have increasing the level of logic 
so our our objective here is trying to keep the level low or number of um gates which you find in a chain low 
so we expanded and we get a two level expression for c two 
next we take expression for c three okay and take the values of c two from this expanded form and substitute it here you get again a large expression but still its possible to gat a two level expression 
one could keep on doing this okay you could from this you can generate expression for c four then c five and you can go all the way go up to c thirty two and um what will happen is that c thirty two will have four billion terms okay 
so although theoretically possible its an impractical solution and the assumption we made initially that we we will consider that delay of each gate is constant that was that s possible only if the number of inputs number of fan ins is not very large 
but now what what s going to happen is that um you will have gates with these product terms will become very very wide okay (Refer Slide Time 13:40 min)



thet they will have thirty two inputs so a thirty two input gate will no longer case as same delay as a two input three input or four input of gate okay 
so although although we will keep things two level but our assumption about delay of individual gate will break down 
similarly um there will be large fan out um for for atleast a primary inputs okay you would notice that each of these variable they are after all thirty two a inputs thirty two b inputs okay and se zero all this will be appearing in so many different terms four billion terms
so there will be lot of fan out as far as the primary inputs all this ai and bi are concerned so it s a unworkable circuit you will able to verify that this is the number of term will be absorbed four billion okay 
because you you know the first one has three terms second has seven terms then you have fifteen terms then there will be thirty one and so on 
so it s a some power of two minus one so you can verify it yourself okay
so why we not take an approach which is some what in between we do want to have look ahead you want to directly compute ci is without looking at intermediate carries so um [noise] what we can do is we can um basically allow some preprocessing to be done on ai and bi 
we we will look in to what what roll ai and bi are playing in determination of carry and um try to do something um before we really compute all the ci 
so so we have actually two stages in one stage we will let individual circuits for each bit position digest ai and bi do something compute something which is more useful and then do something what we try to do in the previous slide okay (Refer Slide Time 17:04 min)



so instead of computing ci is directly from a s and b s will computed from some preprocess form of a and b 
what what we notice here is that [noise] the condition under which one particular stage generates the carry and condition under which propagates carry 
so when ai and bi both are one [noise] that irrespective of what is the carry coming from input side a new carry will be generated 
so so we denote this by gi okay gi is the condition when carry gets generated in ith position irrespective of what is happened towards its LSB side 
secondly when any of ai or bi is one if the carry is coming from the input side it will propagate to output side okay 
so ci plus one will be one if any of these is one or sorry i should say that ci plus one will be one if ci is one and any one of this is one so we denote this um expression as pi or the propagate condition 
so so now um we try to rewrite all the ci values interms of p s and g s okay 
so we can say that c one is true if either carry gets generated by stage zero or carry coming in gets propagated at this stage okay 
similarly we can write from c two c three c four so c two is p one c one plus g one 
c three is p two c two plus g two and so on 
now in this we can make substitution of the kind we did earlier in terms of a s and b s 
so value of c one we take from the first line and substitute here we get another sum of product term this term we substitute here we get this and we substitute here we get this okay 
again we have growing expression but you can see that the growth is much more contain um of course this gives a feasible way of doing it but to what extend we can do 
we we still cannot keep on doing this for thirty two stages so we we will do it for limited stage and then we allow um may be carry to go in the normal way 
actually coming back to these expression we can we can reason this out directly also (Refer Slide Time 19:19 min)



so for example here um for c two what we can say is that either carry gets generated at stage one or carry gets generated the stage zero and gets propagated through one or there is an incoming carries c zero which propagates through p zero and p one through stage zero and one and so on 
so similarly lets look at the last one the condition for c four is that either there is a generation at stage three 
or generation at stage two which propagates through three 
or generation at stage one which propagates through two and three 
or generation at stage zero which propagates through stage one two and three 
or and incoming carry which propagates through all this four stages okay 
so you can either um derive these expression by substitution or reason them out directly um based on this logic of what this terms p s and g s are okay (Refer Slide Time 19:19 min) 
so now this um as i mentioned this can be done to a limited extern okay we will not try do this for thirty two stages and we will do it for let us say four stages okay which is not very large and then see what happened 
so first of all lets understand the circuit which will do this um we have first stage um which i have captured in these green boxes it each one of this contains basically one AND gate and one OR gate okay 
AND gate generates gi s and OR gate generates pi s so so we have this p s and g s computed out of a s and b s then we have this block which computes all carry values looking at p s and g s okay 
c zero is an input and it will computes c one c two c three and c four which has to go out then we have the circuits computing the sums out of a s b s and carries okay 
so on the whole they are actually three stages and we we can see what is how much the delay this circuit will have 
so can you figure it out what is a maximum delay 
see the first stage delay is one unit okay there is either one AND gate and OR gate single level um this second stage has two levels okay 
the third stage also has two levels although i have shown for brevity i have shown exclusive OR but you can write as sum of product 
so so basically there are um one level plus two level plus two level there is a five level in five d time we can compute all the sum then the final carry 
of course the carry is coming in three d time all the sums are coming in five d time (Refer Slide Time 21:58 min)



so um now we will take this as a module and try to built larger adders 
so first we um try to represents this sum what s abstract form i will just collapsed three stages 
now we we are suppressing internal retails so that we will look at the whole thing as a block box and try to um put more of these to build larger adders 
so i am just showing that on one side you have a and b inputs 
one side you have a sums there is a carry c zero and carry out c four and three stages we are not now separating them out 
but there is a box which has three stages and in in a smaller form i will show it like that (Refer Slide Time 23:58 min)



so now suppose we connect four of this in a in a chain form the way we did for ripple carry adder but now we have units of four bit adders 
and within four bits we have look ahead across this blocks of four bits we do not have look ahead we have carry rippling through 
so now there there will be let us see let us try to find out how much delay this circuit will have 
so remember that the carry comes out in three d time and sum comes out in five d times so um let me let me write the time when the thing will come out at different points 
so this is this comes at five d i will omit d this is coming out in time three um and from this three to this output will take another two 
so this um just a minute this will take d plus two d three okay the carry which i coming out in this will be [noise] three plus two five and two this will come out at after seven d right 
you you can see that that three um this purple box will take two d time and another two d time here so seven d um this will come out at five seven um nine 
so first lets look at the carry times so the time was zero here three here five here seven here nine here okay 
and from this three we get seven here from this five we get nine here this seven we get eleven here 
so the whole thing has taken eleven d of eleven d units of time 
if you had allowed normal ripple carry to operate for sixteen bits we would have required sixteen into two that is thirty two d 
so there is a considerable saving and saving will show up more and more as we increase the value of n and number of stages (Refer Slide Time 26:45 min)


so now here there is partial rippling of carry question is can we even over that 
one way would have been that extern this block or this carry look ahead to sixteen stages or thirty two stages [noise] 
but that increases the cost too much and the gates become wider so the delay also starts increasing because of that reason 
but what we can do is [noise] we can [noise] apply the same idea once again at a higher level okay 
so we have two levels of look ahead um these these four blocks are as a OR but rather than letting the carry ripple through i try to do look ahead at that level [noise] that means try to determine um c four directly from c zero c eight directly from C zero and C twelve directly from C zero at the next level [noise]
so what this will require is some thing which is equivalent to propagate and generate signal at block level we have blocks of four bits and each level we generate block propagate and generate signals P zero and G zero P four and G four and so on 
and this this unit which i am showing here which is looking at these capital P s and G s and generating um these carries C four C eight and C twelve and also C sixteen would be exactly identical to this small purple box which i have shown okay 
the same idea that [noise] um C zero is coming here and there are P s and G s signals so either carries get generated through these G s or carry gets propagated according to P signals (Refer Slide Time 27:55 min)



so now what are these blocks propagate generate signals or group propagate generates signals that s what we need to see next 
so we call this group propagate and generate or block propagate and generate signals [noise] 
so recall that expression for c one c two c three c four interms of p s and g s who are these okay 
so on the whole looking at all four bits together we say that um this block propagates carry is all small p s are one okay 
that means p zero p one p two p three if all of them are one then incoming carry will get propagated okay 
so we call this as capital P zero this is the propagation condition across four stages 
G zero is um is generate condition generation condition um in this four bits so which means that generation takes place either at level three or stage three or stage two stage one or stage zero at generation may take place any of these four stages and propagate out to um the extreme left okay 
we we say it is g three or g two and p three or g one and p two p three or g zero and p one p two p three 
so given this you can write c four in terms of capital P zero and c zero that is this block propagates c zero or this block generates a carry right okay 
so so we will have a similar conditions for all the blocks and then carries can be computed in this loop ahead manner (Refer Slide Time 29:25 min)



so in general we say that Pi is the pi plus three and pi plus two and pi plus one and pi 
Gi is a again composed of this gi plus three or gi plus two propagating through this stage gi plus one propagating through these two stages gi propagating through this three stages okay 
and once these are done we can have c four c eight c twelve c sixteen expressed in much the same way as we had expressed the the carries c one c two c three and c four (Refer Slide Time 31:02 min)



so basically same form of expression and i will leave it for you to verify [noise]
um one thing i would like you to notice here about comparison of this form with this form um here the growth of the um number of term was exponential okay 
the number of term is roughly doubling where as in this case the growth is linear okay so that is what as saved as 
and why it is linear here and why it is exponential there is because here you have the the carry appearing at two places okay 
so so lets take this when you take c two and substitute its value okay it gets substituted twice right (Refer Slide Time 31:02 min)



so so that s the reason why it um roughly doubles every time where as in this case you have only one term containing carry and when you make substitution it grows by just that amount (Refer Slide Time 19:19 min) 
so next time again you substitute once so it grows by that amount so so it is it is growing linearly 
and another way of explaining these a p s and g s that [noise] the the generate term is here okay which we have sort of abbreviated as g zero 
and propagate term is essential taking this  as a common factor okay 
what is getting multiplied with c zero 
what is getting ANDed with c zero it is a zero plus b zero 
so that is the propagate term so once we have taken this out um in the in the process we have made things um slow because we added more stages
but we have got a circuit which is reasonable and feasible okay 
now lets move attention to multiplication [noise] you recall that i talked about um two different types of multiplication 
one was when you put number of multipliers number of adders to add the partial products and other one was when we did this iteratively using a single adder 
so lets look at this approach where you have number of orders right and we we don t have to go through sequentially 
because the sequential approach would mean that you you have to perform addition store it some where then redo the addition so on 
so here also the signals propagate through where is adders but there is no storage involves there is some advantage interms of speed 
so such multipliers are called array multipliers for natural reasons what will try to see is how we can speed up such an array multiplier 
they are many different approaches for multiplication you can have actually tree multipliers 
so so for example you are basically adding four terms rather than adding them in cascade what you can do is you add two terms separately two terms separately and then add the two sums okay 
when you have to add several term you can actually formed a tree so that s another clause of multipliers 
we focus our attention on these so called array multipliers where you you are adding different terms in a cascade form (Refer Slide Time 33:46 min)



so so we will look at the question of addition or multiple terms i am not quite showing a multiplier here but a circuit which adds four numbers okay 
in in case of multiplication these number will happen to be those the partial products 
but let say we have four arbitrary values to be added um you will have one adder which adds two terms next one adds third next one adds fourth term 
so suppose we put the usual ripple carry adder for each of these stages 
then what is the consequence 
the the signal have to propagate um horizontally there is a carry rippling through and there there is propagation vertical also you are going from one state to other state okay 
so the last thing which will get computed is the MSB of the final results okay it has to a count for propagation in this direction as well as in that direction 
so here um what we can do is since we have to go to the the next stage we have to wait for a signals to propagate vertically 
we can take advantage of that and prevent propagation of carry to just the left neighbor of one adder cell 
um so logically it is equivalent that lets say this carry instead of going from here to here suppose it was to be send to this one right 
this adder has to wait for some of this to come down okay 
so meanwhile the the carry of this also can be made available here and we can also postponed this carry let it will go to next stage and so on 
so what we are doing here is that rather than making carries go to left we make them go diagonally left and down and therefore the additional delay which might come because of leftward carry propagation will be cut down 
so here is a circuit which will do that um [noise] so now um you you can do this this is called carry save addition because we are saving carry for the next stage 
you you can you can do this up to some stages but the last stage has nothing else to fall back on 
so in the last stage carry has to propagate okay and um which might given impression that you would require one extra stage 
so so you keep on saving carry for the next stage and that the last one you add but what happens that in the first stage since you have no previous stage here to feed carries you can actually have the third operand fed us carries okay (Refer Slide Time 35:22 min)



so if you if you really see lets go back to um this expression of the sum okay or even if you look at that it is symmetrical with respect to ai bi and ci right 
so its only our interpretation that one of these we are calling us carry input but as far as the circuit is concerned it you can thing of this as a three input and two output adder okay 
it reduces three inputs to two inputs and these inputs are interchangeable (Refer Slide Time 04:25 min) 
so what we have done is that um three of the inputs have been added in the first stage itself okay 
so from the first stage you have sums coming out as well as carries coming out 
both of it go to the next stage right 
so so the next stage is taking the these sums and the carries carries have to move diagonally sums have to move vertically 
and the third sorry the fourth input gets added here okay so ai s have got added here um one could have written a b and e in the first stage and f at the next stage but these are all interchangeable (Refer Slide Time 39:30 min)



and well this is showing as plus the resolution problem is showing as minus take it as plus [noise] 
so um in the last stage we have again sums coming out and the carries 
carries are these carries are being added at the sums position and because we have to have a carry propagation here also 
so you would notice that the number of these adder cells is exactly the same no more no less right 
but they have been rearranged in such a manner that delays are um delays are smaller okay 
so um how much is lets compare the delays in in this case the what is the maximum delay what is the longest path you can think of you try for many inputs try to go any output the the longest path would be obviously something which leads to this 
and the many way because um any horizontal movement or any vertical movement um you you could either go like that and like that so that makes it one two three four five six seven 
or if you go like this one two i am sorry one two three four five six seven whichever you go there is a um chain of seven adders okay (Refer Slide Time 35:22 min)
um now here in the in the last stage you have one extra you are noticing one extra adder here we are numbers of four bits because as you keep on adding the the magnitude keep which keeps on increasing and you are to accommodate for extra bits 
so these carries you don t throw away let them be absorbed by an adder here 
so there is a delay of seven units here where as in this case again we can try to find what is the longest path 
you have for example lets lets take paths leading to this we have one two three four five six okay 
there there is saving since we are talking for small circuit it doesn t show so much or we go way other path lets see that nothing is longer than six one two three four five one two three four five okay 
so the other path of five length this is this is the longest path of length six 
um one more thing which can be done here in this is that you can replace this with a the the last stage could be actually carry look ahead adder 
see carry look ahead adder is more expensive but faster we have seen that you have to put extra logic for carry computation so we can of four two have one of these adder that means this the last adder as a faster and expensive adder 
and then effectively it will be less than six (Refer Slide Time 39:30 min)
where in in this case if you to speed up you will have to speed up each stage to make sense and it will be much more expensive 
now let me um show you it s a this principle is applied to um four by four multiplication (Refer Slide Time 35:22 min)
let me switch to let me denote by pi um pij rather um ai and bj 
so let let we have to have A is multiplied by Bi s okay well you cant see that now 
we call that we had A multiplied B equal to sigma of A times Bi two raised for i so two raised for i is only a matter of waiting it or shifting it okay which we will do through wiring um 
so Ai each bit of A will have to multiplied by each bit of B effectively 
and so i am this term simplify things denoting with pij so there there we have a pij s for i going from zero to three j going from zero to three and we have to um some of this feed to various adders 
so to this let me feed p zero zero p one zero p two zero p three zero 
so this is um A multiplied by B zero the first term (Refer Slide Time 46:28 min)



and um the the second term we need to add is p one zero p two zero p three zero sorry one zero two zero i made a mistake let me redo it 
okay p zero zero actually will stand alone um i will put p one zero here p two zero here p three zero then um next will be second index will be one 
so p zero one come here p one one p two one and p three one so this is the first stage we are we have added A multiplied by B zero and A multiplied by B one 
then in the next stage um i will start adding um p zero two and so on 
p one two p one three sorry sorry p two two zero two one two two two and then three two this can be added here 
and in the last stage um we add p zero three p one three p two three and p three three and we have we require eight result bits let me call this S zero S one S two S three S four S five S six and S seven right 
these initial carries are zero 
so we have a this with normal carry ripple through okay and you can see that the delay of this circuit would be um again declared by the path leading to S seven output and the longest path would be this let me trace it out okay 
so so far n equal to four this delay equal to one two three four five six seven eight eight times delay of one adder which is two d 
in general you can say that this is um you you are going to you have a propagation um horizontally okay you have propagation vertical and you also have some horizontal propagation which is across these stages 
so if if you extend this if you excitable this two end you will have one two lets see okay 
actually it will be easiest if you look at this path right okay which is the same length but easier to quantify 
so so you have some horizontal propagation some vertical propagation some horizontal propagation 
so you can calculate exactly but you you would find that um it will be something like three times n okay this is approximately n approximately n approximately n right (Refer Slide Time 51:48 min)



now when we do a carry saving you carry save addition then what do we get 
so we will require same twelve adder units okay and each of this i will look at in this form okay takes three inputs you can put them in any order generates some in the carry 
so p zero zero i let go straight okay um p one zero and p zero one are fed here um i have nothing else to feed here i could have fed one more here but there is nothing else to be fed here so i will may be i can leave one zero here 
here i can feed p two zero p one one and p zero two you would notice that what i am feeding along a column the sum of indices would be same 
so two plus zero one plus one are zero zero plus two 
so here i will feed p three zero p two one and p one two 
so i have a need for feeding p zero three also at this stage so with i can leave here 
so these these a four terms are to be fed in this particular column and here again i will be left with i will have still four terms so i think i left a three only 
so i can take p let me p zero zero p one zero p two zero p three zero this i have finished 
p one zero one one one two and i can have p one three then um zero two one two two two and three two can go here um then next one is zero three here p one three is already accommodated 
two three will come at this stage    
and three three will come this stage okay 
so i have i have added all one two three four five six seven eight nine ten eleven 
there is something missing can you figure out 
zero three one three two three three three um zero two one two two two and three two 
zero one one one two one yeah three one is missing i think i should have put that here [noise] okay 
so that should be where is p two two alright [noise]
so all that one has to do is take care that um a term gets added in the right column okay whether it is added earlier or later it doesn t matter [noise]
um doubtful whether something is left out here or not so so you you can check [noise] but any way this is the overall structure of this adder as this multiplier 
and you would notice that the delay now is dictated by this path okay 
so you have one diagonal movement and one horizontal movement 
so roughly it will be um proposal two n instead three n [noise] (Refer Slide Time 59:05 min)



so let me just conclude [noise] what we have seen is that as far as the addition is concerned and same would be true for subtraction the key idea was to go from ripple carry to um carry look ahead 
and go from um O n type of delay to O of log n 
why we get log n is 
because when you do multiple levels of look ahead suppose we we had we went from four to sixteen okay we got two levels 
if suppose you have to go out sixty four bit addition right so we had one unit looking across four another unit that next level will look across sixteen and then yet another look across sixty four 
so it it grows like a tree and the number of such look ahead levels will be um given by will be proposal to log n i am not putting a base here 
if if you have individually um look ahead across four so it will be log n to base four but that s only propositional factor [noise]
whereas on incase of multiplier we um move from delay which is propositional to three  
n to delay which is proposal to two n by doing carry save addition 
there is a sort of three is to two roughly which is the saving (Refer Slide Time 59:30 min)



i will stop it that




Transcriptor Name: G.Sathis Kumar

COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   16

Floating Point Arithmetic (Time 51:38 min)

so today it is the last lecture in the series of lectures on arithmetic operation and arithmetic unit design 
today we will talking of floating point operations so when numbers cannot be represented as integers you represent them as floating point numbers 
and we will see how one could do the usual arithmetic and other comparison operation on floating point numbers 
so we are being talking of um primarily integer arithmetic we also talked about logical operation and other operations [noise] (Refer Slide Time 01:26 min)



i will first of all talk about why we need to floating point numbers what kind of representation we have and then we will work about the hardware unit for carrying out the operation 
so first what is need for floating point numbers [noise]
how we represent them 
i will talk about a standard representation which is IEEE seven five four 
then define the concept of overflow and underflow in case of floating point numbers which is different from that we have in integers [noise] how operations are carried out will talk over primarily add subtract multiply and divide 
and talk little about comparison as well um the issue of accuracy is very important in floating point unit 
how we can get best accuracy by using reasonable cost that will be discuss 
there are some special numbers which need special care we will talk of that (Refer Slide Time 02:34 min)



and finally i conclude by um talking of what kind of provisions are there in processor like MIPS 
so first let us look at why we need to go beyond the integers um we we are familiar different data types as we encounter in programming languages and also in the world of mathematics we talk of integers which is actually a subset of rational number which is subset of real number and the subset of complex number 
so it s a large space of um numeric word and integers only a small subset [noise] 
so how how do you go beyond integers that that will be a topic of today um 
we we need to also take care of situation when the numbers are very large in terms of their values okay 
one is you may like represents something which is not quite an integer lets a  seven point four  which is neither seven nor four um but on the other hand there also a need for representing values which extremely large or extremely small 
particularly in scientific domain we have to talk of huge distances 
for example in astronomy if you are looking at distance between um lets a pluto and sun this is  five point nine  into ten raised for twelve meters okay 
so you would not like to represent that it an integer um its not that you are interested in a values 
even if you are accuracy requirement is limited you are not looking at fractional values but the number is so large that integer representation is not a suitable one 
on the other extreme if you look at quantity like mars of electron which is  nine point one  into ten raised for minus twenty eight grams again it s a so small value that you will either have to scale it um to that extend or thing of deperent different representation (Refer Slide Time 02:40 min)



so [noise] how do we represents fractions that s one question and how do we accommodate large range that s the second part 
so representing fractions means you you want something which has integer part and there is a part which is non integer um also there could be you may be looking at non integers value as rational number 
so for example you have rational numbers they can be thought of an integer pairs okay 
so five or eight can be represent by two integers or if you are number let select minus two  forty seven point zero nine  you can represent this as a string minus is one corrected two in one corrected four is another seven another and so on okay 
each of these box which you see is separate corrected so you can as a string you can represent um number number which has something to the left of decimal something to the right of the decimal and propagate sign 
or in binary form you can represent um number with a fixed position of the binary point okay 
for example look at this number string of ones and zeros and you might says that okay here is my binary point which is equivalent to which is counter part of decimal point 
so you you may not unlike this representation where you are using a corrector to represent that position this position may be implicit 
so you would say that out of so many bits um the point actually lies after the fifth bit from the right okay 
soum then you need to only worry about the the string of so many bits and the position of the binary point is fixed 
so this could be consider as fixed point notation 
but um this representation although is okay it does not take care of large names so to to get large range both in terms of extremely large value and extremely small value you need what is called floating point where um you have a fraction part which could be represented in any of the manner described above 
and there is the base to which you can specify you can realized to a specified power okay 
so so it is this ability that you have a base and some power associate with it that you can work with large ranges 
you can have a positive power to represent extremely high values and negative power to represent extremely low values um the precision or the actually see of what you are representing would actually come from this fraction part okay 
how many significant digits you have would come from this whereas the this power will take care of the range right 
so [noise] now we we must remember that in all such representation where you are using a finite number of bits or digits 
you you are representing basically um values which are rational okay to represent irrational quantities you need infinite length of storage okay
so we as far as a irrational number is concerned we we would typically represent only reasonable approximation of those numbers 
and all representation which are infinite number bits or bytes would be actually representing rational number in some times [noise] (Refer Slide Time 04:50 min)



so now lets a go deeper and understand the meaning of binary numbers where there is a point somewhere okay 
so for example take a simple number  one zero one point one one  alright so there is a this is a integer part and there is a fractional part 
what is the value of such a number um we we understand very well the part to the left of the point one into two raised for two zero into two raised for one and one into two raised for zero corresponding to this one zero and one [noise]
the the bits on the right of the point have a weighted which is negative powers of two
so so this one here has a weightage of two raised for minus one and the next one as two raised for minus two 
so if you add now all this bits with a propagate weightage which be which is a power of two positive or negative you get this four um that this is the four zero that gives you one that gives you  point five  that gives you  point two five  and all put together we get  five point seven five  in base ten right
so um this shows how you can actually look at um binary fraction like this and get at equivalent decimal value [noise]
on the other hand suppose you have a decimal fraction see  zero point six  how do we get it is binary representation 
so um here is the representation but lets go through the steps which will give you this so the process is that you repeatedly multiply this fraction and keep on collecting keep on multiplying by two and keep on collecting the integer parts okay 
you multiply  point six  by two you get  one point two  so one is the integer part and  point two  is a remaining fractional part 
 point two  is again multiplied by two you get  point four  so look ahead at  zero point four  so zero is the integer part  point four  is the fractional part  
repeat the process you get  point eight   one point six  
and now  point six  since you got  point six  it will again follow same steps
so um interms of its binary representation you will get  point one zero zero one  and this will repeat okay just to show the repetition have shown different colors 
so um now this is um this means that you repeat this still infinity 
but if you are talking of finite number of bits again this cannot be accurately represented because you will have to truncated to somewhere right 
um therefore we cannot represent  point six  exactly in binary by this method 
of course um one of thing which i mention in the previous slide that a rational number you will represent as a pair of integers that could be done you could represent this as six and ten okay 
that would be an exact representation but that s not very convenient to work with um 
what is convenient to work with is a something like this but there is some loss of accuracy which we must keep in mind [noise] (Refer Slide Time 08:38 min)



okay now lets introduce the exponent part okay you have some base and raise it to some power and also let doing in the sign
so typically a floating point representation would have a form like this so minus one raised for S S takes care of sign s is either zero or one multiplied by F F is the fractional part 
so faction is within some fixed point sum that means you have a string of bits and there is some position where you assume binary point to be there 
um the base here is two we are talking a binary now so two raised for E E is the exponent part um this F is the fraction part which is usually called mantissa or significant 
and E is the exponent part this itself can be positive or negative we have seen in need for both okay 
why we need positive exponent 
why we need negative exponent 
so now um with this concept the question is that a given word of memory okay or a given register how do you divide this in to um three parts that means how how you allocate bits to S F and E 
S of course is straight forward requires only one bit but how many bits for S 
how many bits for E 
they could be lots of operations are there and also they could be lots of options as how you represent F where do you assume the point to be right
and how how do you represent E what about the sign of E so the all this issues and they are multiple answers (Refer Slide Time 12:08 min)



so what is happen that over a period of time a standard has emerged which is a largely acceptable and today most of the processor actually use that standard 
this is IEEE seven five four standard IEEE stands for institution of electrical and electronics engineers um so this this is the organization which apart from um publishing the scientific literature 
they also um define various standards okay so this standard defines representation which is called single precision and double precision 
double precision naturally has um larger number of significant digits 
so [noise] um the division of the word which is thirty two bits is as follows the left most bit is the sign bit next eight bits are kept for exponent and the remaining twenty three bits are for F or the mantissa 
in double precision we um not only extend the precision but also increase the range of E so they are three addition bits given here okay 
this small difference you will see makes a big impact so you don t need to really increase too much here so eleven bits for E and the remaining bits of this word and another word it s a total sixty four bits representation for double precision 
so twenty remaining bits here and thirty two of the next word together fifty two bits are used for the F part or the mantissa part okay (Refer Slide Time 14:02 min)


 
now lets go into further details of how um E and F are represented 
so first lets look at F F is the significant part or the mantissa part um where in single precision case we have actually twenty three bits we assume the point to be on the extreme left and we also assumed that there is a one sitting there invisible okay 
so there is an implicit one here which which means that the the numbers here we are going to talk of will be  one point some thing  right 
so this this would decide the range as you would see that the smallest number would be when it is  one point all zeros  and the largest value of F would be when it is  one point followed by all ones  we will see what it means 
similarly in double precision again the point is to the left extreme and there is a invisible one here invisible or implicit that means you assume that one to be there 
so effectively we we are having twenty four bits here one one bit we do not have to represent explicitly (Refer Slide Time 15:28 min)



so now what are the value ranges 
naturally with the all twenty three bit zero the value of F will be one  one point zero zero  or effectively one 
and if you have a get in back to this if you have all the ones here then what is the value it s a nearly two but not quite equal to two 
it how much sort of two it is um corresponding to a one in the positions okay 
if you take a  one point all ones  and then add a one in this position um you will have carries going all through and you will get a two here right (Refer Slide Time 15:28 min)
so so we are only slightly sort of two and two more precise it is two minus two raised for minus twenty three 
because that s the weightage of the last bit twenty third bit its equivalent to two raised for minus twenty three you could say that F lies between one and two minus two raised for minus twenty three or more approximately you could say that one is less than equal to F less than two 
so we are not saying how less but we we know that s nearly almost two 
similarly for double precision numbers we accept for twenty three we in place of that we put fifty two 
so now this this way of representing the floating point number um or the mantissa of this is called normalized representation 
so um if the value actually falls beyond this suppose you are talking of you want to say four five into two raised for some thing 
so instead of saying five you divide by suitable power of two so that the value gets in the range of one and two 
so it will be actually  one point two five  and you make the propagate just make in the exponent 
so suppose you want to say five into two raised for four equivalently you could say  one point two five  into two raised for six okay 
so so you divide here by two raised for two and adjusted by increasing the exponent 
similarly if F happens to be very small you multiplied by powers of two suitably and decrement the exponent propagately 
so so that the value always lies in the range one and two value of F always in between one and two 
if it is exactly two you can divide by two and make it one adjusted adjust the exponent (Refer Slide Time 16:38 min)



okay lets now look at E part or the exponent part um for sim single precision number we have eight bits here okay we represent exponents with with a bias notation  
the sign exponents are not represented as twos compliment there is a reason i will come to i will explain in little later 
so the bias use here is one twenty seven that means if you are exponent is a minus one twenty six okay minus one twenty six plus one twenty seven it will appear as one 
so the the values these eight bits actually can carry a value from zero to two fifty five so leaving the two extremes zero and two fifty five um which are for special purpose i will explain that later 
the real range is one to two fifty four okay and one corresponds to the most negative exponent two fifty four corresponds to most positive exponents and with the bias of one twenty seven one actually corresponds to minus one fifty six and two fifty four corresponds to plus one twenty seven (Refer Slide Time 19:10 min)



similarly in double precision numbers the bias is one zero two three and the range will go from minus one zero two to plus one zero two three 
that s shown here the range in range of E in single precision is minus one to six to one twenty seven okay we are excluding the cases when all E bits are zero or all E bits are one that is [noise] the two extremes or use for something special 
and we will discuss that later (Refer Slide Time 20:35 min)



okay now we are talked of positive and negative numbers what about zero 
so zero would require actually all F bits to be zero right 
and we cannot assume an explicit one to the left of the decimal to left of that point 
because that would mean it is  one point something  you you that range excludes zero 
so zero again has to be treated specially um so what we say is when F is all bits zero um and E is also all bits zero sign bit is also zero then then it actually represents a zero in floating point okay 
so floating point zero a is also same as integer zero in representation okay 
all thirty two bits are zero signifies floating point zero [noise] 
in principle any number which has F zero and any value of exponent would represents zero but that would corresponds to multiple representation (Refer Slide Time 21:05 min)



so we have a unique representation in IEEE seven five four which is all bits are zero 
so now with this um described how do how do we test um the numbers for being zero non zero positive negative how do we compare 
so test for zero as you can now guess is same as what you do for integer 
so so if you have a circuit which test a number for integer zero it will actually test for um floating point zero also 
similar test for sign test for being a negative or positive requires only the sign bit to be look that which is same in the two cases integer as well as floating point 
what about a magnitude comparison 
now magnitude comparison also turns of to be similar to integer because the certain choice  wehave made 
firstly we have kept exponent part to the left of um the mantissa part okay 
so now if two number have same mantissa part but one has the larger exponent obviously that will be larger okay 
because exponent has a larger weightage um only if okay 
even if two number have different exponents sorry different mantissas lets say A has larger mantissa than B but B has a larger exponent B will be larger 
so it is the exponent which is more predominant and in terms of significance its natural places towards the left 
so so therefore when you compare to floating point numbers treating them as an integers and if you find that exponent bits one or larger than the exponent bit in the other then you don t need to really look at the mantissa okay 
that s how it will happen in case of integer also some MSB s give you a decision about A in larger than B or B in larger than A they don t need to look further [noise] 
and and nothing which helps in this is that um the bias representation so irrespective of sign of the exponent this comparison will work out 
if you have for example twos compliment representation then the sign comparison and unsigned comparison differ 
so so now we can do magnitude comparison of two floating point numbers which means we are not we are excluding the sign bit 
rest we compare as the few comparing magnitudes of integers (Refer Slide Time 22:12 min)



okay in integer we talked about overflow we talked of overflow when um you are trying to get to a very large positive integer a very a very large negative integer okay 
if you exceed the positive limit or negative limit you have overflow 
here we have additional concept of underflow okay 
so um we if you look at the largest and the smallest positive or negative number we can represent um you you would notice that there is a limit of values in both directions 
so you you may have a number which is um which is larger than the largest positive number for example 
so that s an overflow condition but if you have a number which is not zero but smaller than the smallest floating point number you can have okay 
it is too small to be represented in the in the given floating point notation then it is underflow okay 
so number is not zero but it cant be in integer the smallest integer is one and you can represented that 
so there is no underflow but here there is a concept of underflow which you must remember 
so now lets look at the extreme values [noise] so we take the largest value of the mantissa which was two in two minus two raised for minus twenty three okay or approximately two and largest positive power is one twenty seven 
so so basically it is um two into two raised for one twenty seven that that s the largest number we have 
um and two raised for one twenty seven interms of to be approximately ten raised for thirty eight okay 
so in in terms of familiar decimal system that is the range we are getting we are we can go up to [noise] power thirty eight of ten 
the smallest positive or negative integer so so here is as far as the overall sign is concerned that is the separate bit 
so si i am not writing separate range of positive and negative 
so i just prefixing a plus minus sign with this 
so the smallest positive and negative value is we take the smallest F value which is one and the smallest E value which is minus one twenty six 
so so this can also be written as two into two raised for minus one twenty seven which means it is two raised for two into ten raised for minus thirty eight 
so basically that is a range [noise] um powers of ten going from minus thirty eight to plus thirty eight 
in in the double precision [noise] here it s a power of two two raised for one zero two three which turns of to be ten raised for three zero eight okay 
so it is extremely large number and which is good enough for almost all practical purposes 
so so another thing you you need to be concerned here is how many significant digits are there let me see i have okay 
another question is how many significant digits you have see we are significant bits as twenty three here and fifty two here so what does the translate to equivalent decimal digits okay 
so so to get an idea of the precision of the number you are trying to represent um you can actually um divide this twenty three by log of ten to the base two okay 
and you will get the number of digits 
similarly divide fifty two by log of ten to the base two and you will get the number of significant digits 
so [noise]i thing this will be some thing eight digits or seven or eight digits or so on (Refer Slide Time 24:55 min)



okay now finally lets come to how we perform operations on floating point numbers 
so first lets look at addition and subtraction 
let one number be minus one raised for S one multiplied by F one multiplied by two raised for E one okay 
so S one F one E one are the three components of this floating point number 
similarly there is other one S two F two and E two and we are require to either add or subtract 
um now before we can do this addition subtraction we must bring the exponents to the same level 
so suppose even is greater than E two okay find out which is larger let let E one be greater than E two hen we can rewrite the second number 
in such a manner that its exponent is made even we do our with normalization here um change the fraction part to a two prime 
we have two prime is obtained by dividing F two with some power of two 
so specifically two raised power the difference is that two exponents 
so um now dividing by power of two actually you can imagine that in hardware circuit it would mean that you are shifting right okay 
so you shift F two right by so man y positions E one minus E two and corresponding adjustment has been done in the exponent 
exponent has gone from E two to E one okay 
so so now you are in a position two adder subtract F one and F two prime 
so depending upon what the signs are and what those operation is you will be performing some one of these operations sum or a difference 
so the result is a minus one raised power S one sum or difference of these into two raised for E one 
by the way this itself could be positive or negative and accordingly that might also changed 
so the logic for determining sign is not difficult to work out um what i like to point out here si that when you do this you may find that this is not normalized okay 
this sum or difference might go beyond the range of one to two which we would like to have 
so the so therefore this operation has to be followed by normalization process 
if the number is become too large shift it right and adjust the exponent 
if it is become too small shift it left and adjust the exponent in the opposite manner (Refer Slide Time 29:06 min)



so here is a not yet details circuit but a lock diagram indicating the the flow of information as a it would be in a floating point adder come subtractor 
so lets say we have these two register holding the two numbers sign exponent and significant part 
the first thing which will be done is compare the exponent um the the circuit comparing these called small ALU because its looking at only eight bit operands 
it computes the difference of the two exponents and that is the amount by which you have to shift one of the significants so that the exponent get aligned
so this is this is called the alignment stage [noise] 
here there is a shifter which will perform right shift on one of the significants and that is selected by this multiplexer and which one selected is determined by this control unit control unit is guided by the difference you have found out 
so knowing whether E one is greater than E two or E two is greater than E one 
one of the two will be selected here other one will be selected to go directly and this big ALU adds or subtracts the two significants or mantissas and you get the difference here [noise] okay 
so so this is this could say is the first stage where exponent circuit is compared 
this is the second stage where um the number which a small exponent is brought with the one with a large exponent 
this is stage where this is the stage where addition or subtraction is done 
this is the stage where normalization takes place 
so normalization would involve output of ALU ignore this multiplexer for the moment i will come back to this in a moment 
this output to the ALU may need to be shifted left or right depending upon um how for you are from the normalized value okay 
so you need to bring it between the range of one and two and accordingly there will be an increment or decrement of the exponent which will be done which is actually coming from here one of the two exponents selected and passed out for increment or decrement 
now after normalization um you need to what is called rounding so rounding means you you like to get to the LSB to the best of the accuracy okay 
so so you may be throwing some thing towards the right and whether you need to make an adjustment here or you you can just refer to through it 
so i will come to this shortly so you have some rounding logic here which may further adjust the mantissa it may need adjustment in the exponents 
and um after rounding off you may find that we have lost normalization once again there may be another round of normalization so that s why these at being fed back 
and multiplexer will select either the original value coming from top or after rounding and finally the result can be placed in another register okay 
this is the hardware for add or subtract [noise] (Refer Slide Time 31:46 min)



um consecutively multiply operation is some what simpler okay if you look at two numbers this is one this is second the the product can be obtain straight away 
the sign of the result is exclusive OR of the signs okay if both are ones both are zeros result is zero and one otherwise 
the fraction part is the product of the two fractions and the exponent gets summed okay so it rather straight forward there is no initial alignment which is required 
but of course you may need to do normalization and rounding the reason for that is this product of two values which are individually in the range one to two the product could be in the range one to four 
so that means on the largest side it can exceed two and it may require one smaller adjustment you may need to bring it down (Refer Slide Time 35:30 min)



if it is exceeded two you divided by two and um increment the exponent terms
divide is similar two numbers same thing um sign is determined exactly in the same way the fractions get divided the exponents get subtracted 
now this ration of fractions could lie in the range  point five  to two
so it will not exceed two because each one is the range one to two but it can become small so you you would you may need to normalized it here and again there is a there is always need for rounding off 
so of course before you do all that you need to check whether F two is zero or not 
so you can proceed with this F two is not zero (Refer Slide Time 36:38 min)



now the question of accuracy and issue of rounding off 
so when when while doing an operations for example when you aligning you are shifting the number to the right you are throwing of some bits okay 
when you are multiplying two fractions you have a two thirty two bit or lets say two twenty twenty four bits including the invisible one bit 
you are two twenty four bit exponents sorry twenty two twenty four bits fractions which you are multiplying the result would be as we have seen for integers result would be forty eight bits in general 
so you have to know throw remaining bits and retain twenty four bits finally
so so there is a loss of precision in this process because you are working with finite word length and of course you you have to have finite word length so some loss would be there but you want to minimize the loss 
in the given number of bits what is the best you can do
so it turns out that if you use a few extra bits for intermediate calculation and finally do round off based on those then you can get to the accuracy which theoretically possible [noise] 
so these bits are called G R or S 
G stands for guard bit 
R for round bit and S for sticky bit [noise]
so G and R nothing but the the next two bits after those twenty four bits okay we we normally have twenty four bits as seen externally 
but suppose we had if you had retain two more bits these are G and R 
so in the intermediate calculation we retain those 
apart from this we keep one more bit which is called S bit and S bit is one if and only if any bit towards right of R is non zero okay 
so S distinguish S is like a summary of the remaining bits which we are throwing off and tells the distinguishes in the case which is all zeros and case where there is some atleast a single one okay 
you you can look up on the number as  one point all this  followed by three more bits GR and S (Refer Slide Time 37:25 min)



now given this bits given the additional bits which are which are kept in the intermediate stages how do you perform rounding off [noise]
okay so now first lets look at G and R um and just for relative sense lets imagine that the point is to the left of G 
so on a relative level lets assume that the binary point is here actually okay it is just for concept otherwise actually point is there
so G has a weightage of in relation to this G has a weightage of   point five  
R has a weightage of  point two five  okay 
so when G and R both are one its like  point seven five  and you will round up okay you will add one to the LSB 
now LSB means the bit to the left of G if both are if G is zero and R is zero it is the case when both are zero it s a um it represents zero to the right of the point 
or when R is one but G zero it corresponds to  point two five  
so when it is  point two five  or  point zero zero  
now you want to um throw away the the value towards right of the point okay you want to round it now 
when G is one and R is zero it corresponds to  point five  you are exactly in the middle so whether you go up or you go down 
in this case we will look at S bit okay 
so so we want to get more information to be fair 
if S is one that means there is some thing which is non zero towards the right of R 
so so we can round up okay we can add one to LSB 
if S is also zero then we are dead at  point five  and which where do we go  
so now assuming that um half the time you want to round up half the time you want to round down  and all numbers are equal likely the logic which is followed in IEEE seven five four is that you round to the nearest even okay
even in the sense of um this bit this bit being considered as LSB so a one here means it is odd is zero here means its even 
so if it is zero here you will um you will leave the number as it is if it is one here its like odd you add one to make it even okay 
so this is the meaning of round to the nearest even 
so so you round in a manner that the the last bit twenty third bit becomes zero okay 
so effectively what you doing is you are trying to be um fair now such things are actually important for example when you are dealing with the financial applications 
so if you if you  if there is a bias in your rounding method then over a period of time the gains may accumulate for one party on loss and may accumulate for another party which may not be fair 
so so you you would like that rounding up takes place half the time rounding down takes place other half the time okay (Refer Slide Time 39:48 min)



i mentioned that some codes for exponents or reserved for special numbers so here you will see the entire picture 
the the first two column are for single precision number and the next two columns are for double precision number and the last one represents last one indicates what object what kind of number we are trying to represent 
so if you have everything zero when it s a special case and it is representing as zero 
when um let me look at this when exponent is at its peek high the highest value two fifty five and mantissa is zero then it will represent infinity okay 
the the sign the sign bit is also there which could be plus or minus and it will represent accordingly plus infinity or minus infinity 
so now um what do programs do with infinity 
when you are writing a program you can always check this condition for example some point you lets say reach a situation that the result is coming as infinity okay 
how do you represent you know want to stop computation there you want to represent infinity and proceed further 
so so you can write you can express this in this way and subsequent part one the program would understand that the number is infinity 
so so you will not try adding infinity to something which is not infinity and so on 
so so this gives a method for doing so 
the the other combination for example suppose exponent is zero but mantissa non zero okay this represents a denormalized number 
so so when underflow occurs you can still proceed with with your computations you can represent numbers um by doing away with the requirement of normalization 
because if you system normalization then there is the smallest value you cant go beyond that so so you do go beyond that you force a way beyond that represent them as a denormalized numbers 
but you you must know that now there is that invisible one is not there okay 
this is the normal situation that exponent is the range one to two fifty four mantissa could be anything 
and the last one is that exponent is two fifty five same as the infinity but you have a non zero value 
so so this is the this is used to represent something which is not in number 
for example zero by zero you cant say it is a infinity its something which is undefined alright 
so they could be different situation which lead to number which are undefined 
and by choosing some code here you can have a programs specific meaning of all those situations (Refer Slide Time 43:06 min)



okay finally i will give a summary of the instructions which are there in the MIPS processor to handle floating point number 
so first of all there are thirty two floating point registers labeled as dollar f zero to dollar f thirty one 
these are in addition to the thirty two register we have for integer um 
and actually it s a entire hardware which does floating point activity is consider as a co processor so it is a combining an any other processor whose special task to work with floating point numbers 
so there are usual operations add subtract multiply divide absolute value negate okay which can work on single precision data or multi precision double precision data 
so you have two versions of each add dot s and add dot d stands for single precision addition or double precision addition 
so like integer MIPS addition this requires three floating point registers to be specified okay you you would say add dot s and then gives the registers 
when you are working with double precision arithmetic you you need to specify register pairs 
f zero f one form one pair 
f two f three form another pair and so on there sixteen pair 
then they are instructions lwc one and swc one so this the load word but we are also saying c one which stands for co processor one they can be many co processor attached with the main processor and floating point processor is considered as co processor one 
so these instructions would load a word from the memory in to the co processor one which is floating point unit that means one of these registers 
so you specify which registers you want to load into and you specify one integer register in the usual manner to specify the address okay 
there is a constant part sixteen bit constant part and a base register so together the specify memory address store instruction is similar 
then they are conditional branch is bc one t and bc one f 
b stands for branch c one stands for co processor one 
and t and f stands for true and false condition okay 
now how do you determine the condition you you need to use one of these compare instructions to set the condition 
so imagine that the condition is one bit register or a flag which is set by one of the compare instructions so in SLT for example in SLT instructions the result of comparison was brought into one of those thirty two registers 
but here the result of comparison is brought into a separate flag which is tested by this 
so test for truth and test for false hood 
these two instructions do the comparison so c means compare then lt means less than s or d would mead single precision or double precision 
so lt could be replaced by le gt ge so they are six different instructions for single precision six for double precision 
then they are instructions for conversion so um because you have a three representation integer single precision and double precision and you need conversions 
so suppose you have a number  three point five  it will have some representation in single precision some representation double precision okay 
so a thirty two bit pattern or a sixty four bit pattern and there may be sometime need for conversions or conversion to integer that is take out the integer part of if you have integer three you can convert to floating point  three point zero  
so these conversions could be some what lossy okay particularly when you are going from higher precision to lower precision you may lose some information (Refer Slide Time 46:20 min)



okay so finally what we have learnt today is a we talked about range of value which we can handle definition of overflow and underflow 
we learnt how to perform various operations 
we talked about accuracy use of G R and S bits for rounding off 
we also looked at notations for some special numbers 
and finally looked at um floating point operations given by different instructions MIPS processor (Refer Slide Time 50:30 min)



thank you



 





Transcriptor Name: G.Sathis Kumar

COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture  17 

Processor Design  Introduction (Time 40:03 min)

in the last few lectures we have focused on design of very specific component of the processor namely ALU or arithmetic logic unit 
now we are going to look at the overall processor design where this would be the key component but we need to add more pieces of hardware to make the whole processor complete 
so first of all we will talk about the building blocks what are the other blocks including ALU which I require to build the processor 
and we will start with the very simple design where we will simply put this building blocks together in the simplest possible manner 
and see how the data would flow what is the data path and how it is controlled 
then we will see the performance of this particular design and we will realize that we need do something more to get a better performance 
we will therefore move over to a little more involved design more sophisticated design which is called a multi cycle design 
this term multi cycle will be explain as I go long and again we will see that two major parts data path and control 
then we will see a very specific style of designing the control called mac program control where essentially one very small low level program is trying to control the overall processor 
and finally we will enhance our design to handle what is called exceptions 
so while we are talking of instructions we talked of thing like overflow okay and they are  many other situation which are exceptional situation which are not normal and then the design has to take care of these situation
so first we will ignore this and then enhance the design to accommodate this (Refer Slide Time 01:22 min)


in particular today we will first begin with taking a set of instructions from MIPS architecture which you are going to implement 
so that it the design we talk of is a very simple manageable design we can understand discuss and understand the class 
and look at overview of the design okay what is the outline what is the basic idea
see how it is divided into data path and the control um then go on to description of the building blocks 
we will see that there are two types of building blocks combinational and sequential 
we will look at the issue of clock which times the whole design and see what are the timing constraints a particular clock frequency puts on the design or given a design what kind of clock frequency you can expect it to have [noise] 
so finally we will see the components which are very specific to MIPS 
initially the initial discussion would be components in a generic sense and then we will talk of some thing which required to specifically build MIPS which has to carry out these particular instructions okay (Refer Slide Time 03:30 min)



so starting with what instruction we want to begin with 
these are the instructions we learnt right in the beginning okay 
so we will take those few instructions and try to work out the design
these are now we have to very very specific because we need to build circuit which will actually do those particular instructions 
so lets take add subtract AND OR and slt 
so two arithmetic instructions two logical instructions and one comparison instruction 
then we will take two memory reference instructions load word and store word 
two control flow instructions branch if equal and jump okay 
so one is the conditional branch which has the simple comparison slt has little more complex comparison as now you would realize that after having discuss comparison and ALU circuits that comp[arity] comparison for equality is simpler as compare to comparison for less than or greater than 
one one simple thing is that when you comparing for equality there is no overflow information across the bits okay 
you you look at each bits separately and check if the corresponding bits of two operand let us say A and B are equal 
and then individual result could be combine whereas if you are doing comparison for less than then either you do by subtraction where a carry flows through okay 
or you do direct comparison in which case also there was some information flowing 
so so you we had written um recursive equation describing the comparison operation so a simple comparison like equ[al] equality test is combined possibly with branch as in beq [noise] okay 
so after looking at the design which cater for these a lets say for five plus two plus two and nine instructions so this nine instructions processor um would have a simple design but we will notice that addition instruction can be added to this design by making small incremental changes here okay 
so in many cases the change will be indeed small in some of course depending upon that nature of instruction
if you want to add an instructions which is two diverse from work we have the change may be little larger 
but once you have the overall structure overall outline of the design then adding other things is comparatively easier 
so probably that discussion will be in tutorials [noise] (Refer Slide Time 06:11 min)



okay so um what is the overall approach how how do we intend to implement a processor design 
so we use a register or a counter which you called PC which will supply address of the instruction to be executed 
so the whole story begins here you take quantents of PC that decides where in the memory instruction is located so you get the instructions from the memory 
this is called technically fetching the instructions 
then for instructions like let us say um add okay you read the registers okay which will give you the operands so from register file you read the values the instructions then tells you what is to be done whether addition is to be done subtraction is to be done comparison is to be done okay so you carry out of that operation (Refer Slide Time 07:51 min)



and a basically have these components I have shown connections roughly okay
so you have [noise] PC which supply the address for a program which is stored in this if you calling a instructions memory from this we get the instructions 
instruction has various fields some fields specify registers some fields specify operation to be perform and so on 
so from different field in the instruction we pick up the register resist access the values of a registers passed them on to lets say ALU for example performing the operation 
and the the result of ALU may have to be stored back in a destination register or in a instruction like um load or stored we may have to access data memory 
a data memory accessing will required again address calculation you need to add quantent of register with an half set which comes from the instructions 
so we can possibly use same ALU for doing this things 
so a constant may come from the instructions register may come from register file 
this will provide address and you may write in to the data memory if it is store instruction or read from the data memory if it is a load instruction 
so the these are the key components we have talked extensively about ALU itself okay again while designing ALU we looked at if you specific instructions for which we made provision in the ALU and the instructions I am talking of today all covered in that 
the the other components like PC instruction memory or data memory we are going to see now also the register file (Refer Slide Time 08:02 min)



so I will will come back to this component [noise] but on the hole one thing we need to keep in mind is that the design on the hole has to major parts one is called the data path 
and other is called controller 
so data path is the one in fact what I have drawn was not the full design I had drawn skeleton of the data path 
so data path is the one where you have a values or operands which you are working so data flows here and data is stored here 
so all calculations get done here but the other part which is the controller is the one which guide this which is actually control this or extract this 
so controller is the one which will tell the ALU to perform addition subtraction or comparison alright 
and controller will instruct memory whether it has to read or write and so on 
so each of the other components each of the components in data path works under instruction of the controller 
but controller for its own decision may look at some information coming from the data path 
so I have shown these two kinds of signals which flows from controller to data path and data path to controller 
so they are control signals which go from controller to data path and the status signal which comes from the data path to controller which help controller in deciding the action [noise] (Refer Slide Time 10:00 min)



so now lets look at the building blocks um the two types one is the kind of element which operate only data values okay we call them combinational elements 
and other is though which have straight which contain some information which you call state and this is called sequential elements 
so combinational circuits have output determine by the current input okay 
so so combinational circuit elements will look at the current inputs and respond almost instantly to that I am saying almost because they might be some delay you remember that we talked about get delays 
but these have no memory okay whereas element which contains state which we called sequential elements they have memory the remember what happened in the past and therefore the output is function of the current input as well as the previous input 
so in some sense um the state contains state represents a summary of what this element has seen in past 
so summary of the previous inputs is actually contain in the state 
so strictly speaking the output of sequential element is the function of the current input and the state (Refer Slide Time 11:50 min)



state encapsulates state captures the relevant part of the past 
so I will give some examples all gates are combinational circuits okay 
if you take AND gate depending upon what the input is it will um almost instantly react to the inputs after some small delay 
so in ideal case we assume zero delay and say that the combinational circuit have output function of the current input
so various gates AND OR NAND NOR exclusive OR inverter and so on all these are combinational circuits 
multiplexer which makes choice between two or more inputs and um selects one of these to be available at the output this is also a combinational circuit 
decoder decoder um is the circuit which looks at the bit pattern and the input and identifies the bit patterns 
suppose we talk of a four input decoder so it will have sixteen outputs and it will activate one of the outputs depending upon which one of the sixteen combination is available at the input 
then components like adder subtractor comparator ALU all this we have discussed these all combinational circuits okay 
multiplier we discuss various designs but the array multiplier which was simply a collection of adders and other logic put together they they are combinational circuits 
the sequential multiplier which goes through iterations using a same adder is is not a combinational circuit that that anything which is not combinational circuit is a sequential circuits (Refer Slide Time 13:15 min)



so we will look at example of these the most primitive ones are flip flops which have one bit of memory okay 
they can store a zero or one depending upon what what value the contain at any given instant of time depending upon this signal which occurred past 
counters are sequential circuits 
registers so registers are extension of essentially flip flops in one dimension okay 
register files and memories are extension of flip flops in two dimensions okay 
the there is a some distinction between some files and memories I will come to that let away talk of component specific to MIPS instruction set (Refer Slide Time 14:50 min)

 


among the sequential circuits we can have circuits which work with clock and those which do not work with clock okay 
so clocked state elements and unclocked state elements 
in the clocked case the changes in this state occur with clock where as in unclocked elements they they do not distinguish bet[ween] well the clocked elements have one of the inputs which is playing a special role thats called clock okay 
where as in unclocked elements um there is no input which is designated as clock and any change in any input can actually cause change in state 
so a clock as we know is a periodic signal and the period is called clock cycle time or clock period 
typically one of the edges is assumed to be active edge so either the rising edge or the falling edge is considered as an active edge 
and that is the edge which um causes the state transition alright
suppose in a particular design we are going with convenience of keeping the rising edge as active edge then all state changes in the clocked elements will take place with that rising edge (Refer Slide Time 15:38 min)



so lets a look at little more details and look at the examples of clocked and unclocked circuits so a simplest um unclocked element is a a large okay 
you you take two NOR gates or two NAND gates and cross coupled them so I suppose you would have started this in digital electronics we dont need to elaborate on this  
so this is a unclocked RS latch okay R stands for reset S stands for set 
so when you um activate OR okay this becomes zero and this becomes one so it is in reset state when you activate S um this becomes one and this becomes zero 
so there is a feedback loop here through this cross couple signals and actually it is here the information gets stored so since they they are two inversion in the path that provides its a stable storage 
so if this is zero this inversion brings one here and that further gets inverted to get a zero back here 
so this is the one stable states similarly this one and thats zero is another stable state so a circuit like this can stay in this state if as long as you want provide there is no change in the input 
and the changes could occur because of the changes in R or changes in S 
an extension of this is what is called a D latch where you put gates at the inputs okay 
this is D input or the data input and this is the clock input so although we calling one signal as a clock but it is not clock in the sense of a clocked element 
because the changes can occurred because the changes in D also in this case 
so normally suppose D signal is changing like this and the clock is like this then what this circuit does it that while the clock is in one state okay 
the output follows the D input okay 
when when clock goes to zero then whatever was the state of this continuous to be there 
and next change would possibly occur only when clock becomes one again 
so at this point clock becoming one but now the D input is zero so it becomes zero 
but you can analyses and convince yourself that while clock is high okay 
if there is a change in D here then the output will change okay so when clock is high you have one here and depending upon this D this lets a become zero you will get a zero here and this becomes a one okay 
so D becomes zero or there is an inversion here right 
so you will get a one here which means this will get flip flop will get reset okay
while C is one again if D changes it can change state so during this period Q could change as when we timed as D changes 
but while Q is zero it holds the last value okay 
you you can actually spend some times on this and analyze it further (Refer Slide Time 17:02 min)



um you can get a clocked D flip flop by putting two of these together okay 
so we we take two D latches of the previous diagram and put them this form one is connected to C and other is connected to compliment of C okay
so here um now this this can um this can change it state while C is one and D varies okay 
but those changes will be isolated here because at during that interval C will be zero here and therefore this will keep on holding the last value right 
so eventually you will you can C changes here only when C goes from zero to one okay I am sorry C goes from one to zero because this is a connected to C bar 
so its say a falling edge triggered um D flip flop right 
in fact this a what I said unclocked elements or sometime you call level trigger circuits 
because this is active at one level of clock while C is one it is active whereas this one is active atleast as in from outside at an edge 
so its called edge trigger circuits 
in our design we are going to use edge triggered circuits 
edge triggered circuits as you see require more hardware okay um but but they give us nice property that um suppose a change occurs at some clock edge here okay 
you see that change in Q and suppose through some path that change results in further change in D 
but now since the clock has just gone that change will not effect it further okay 
so this is an important point to be noticed that imagine and D D flip flop which is edge triggered and there is path through some combinational circuits from output of these back to D 
so that means um a change here can reflect a change here okay but now change at Q will be observed when clock has gone through an active edge (Refer Slide Time 20:57 min)



so let us say in falling edge this case 
so after clock has gone for one to zero um a change appears here 
although that change may cause a change in D but now since the clock had just gone that change will not cause trigger another change here okay 
so so that is that brings this level of convenience here where in this case if Q causes a change in D here while a clock is still active because clock is active now in um in an interval that could change fur[ther] that could cause further changes 
and you know that could be um a period of instability (Refer Slide Time 17:02 min)
so [noise] now how um how do the clock period relates to the timing of the other portion or circuit 
we suppose you have one state element and there is a path through combinational circuit to another state element okay 
this state element changes its state at one clock edge let us say this clock edge and then those changes will result in some changes at the input of this state element  
this would notice those changes at the next clock edge okay because the clock edge for this this clock edge would have gone in possibly cause some change 
but the change here in the element two resulting from the change in the element one here will actually appear here 
so there is um this interval from this point to this point in time which is allowed for signals to propagate through the combinational circuit 
so so now depending upon what delays these circuit presents we can have a fast clock or a slow clock or given a clock we get a constraint on how much delay we can tolerate in this components or in this logic 
there are some timing constraints associated with the sequential elements themselves 
so let us say this is the active edge of the clock this is C of a D flip flop and this is a D input 
so now for edge for this value of D to be registered in the flip flop at this instant we would we want the D flip flop would want it to be stable for some time before this instant and sometime after that instant 
and this requirements are called set of time and hold time requirement 
so if the change in D has occurred just before the C two close to C um the D flip flop may find it two close to react and it may not be able to see the the recent value 
so we have to ensure that the change becomes stable atleast this much interval before this instant and continuous to be stable for that much period after this instant 
so there this is called a set of time and that is called the hold time 
if um D changes within this okay then the output could become unpredictable you know whether it takes the old value or takes the new value okay
or if you have a register continuing in many flip flops some flip flop may react fast some may not and therefore you may have a um a mixed of value 
so so this is an important consideration when we discuss the performance and timings okay (Refer Slide Time 24:10 min)


so now um lets come back to those nine instructions which you want to implement you want to create a design for implementing those nine instructions and lets carefully see what components we are going to require to build the processor 
so we will require registers adder ALU multiplexer register file program memory data memory and some additional components for bit manipulation 
so lets look at this one by one (Refer Slide Time 27:46 min)



we we need um a register to have the PC value program counter so this register implements the PC and all this all the sequential elements we will have as clocked elements 
so clock will be one input it will have thirty two inputs and thirty two output thirty two bit input thirty two bit output 
so output will address um the program memory and the new value which is going to cause change in the PC will be its input okay 
so now at the active edge of the clock PC will change its value okay 
how how this value comes and when this clock edge occurs we will see when we put all the components together (Refer Slide Time 27:48 min)



um we will require adders and ALUs so ALU design we have seen ALU will perform main arithmetic and logical operations 
but we we need addition operation in other situation also 
so for example to prepare PC for the next instructions you need to add four to the PC quantents 
so this adder will perform an addition of PC and a constant four okay 
so this four expressed in thirty two bits so there will be lots of zeros okay um so so in fact one could simplify the design of this adder noting that it has to add only a constant 
but lets no worry about that right now we will see it later 
so we we will just put an adder um where we need to add PC and four okay 
similarly we are like it to have need for adding an offset to PC plus four for implementing branch instructions okay 
for for branch if the condition is true um you you are carrying outer evaluate branch that means the offset which is specified by the instructions is added to the PC 
but but we would have added four to PC by this time (Refer Slide Time 28:42 time)



ALU we have already seen okay it will take two thirty two bit operand produce the result and it will need to be told which operation is to be performed 
because we have designed it as a multi function unit depending upon which instruction is being executed this may do different operations 
this also doing test for equality okay a equal to b test is done 
the the result of slt will come [noise] in this thirty two bits only but result of beq will be a single bit which is to be used decide whether to branch or not 
and not we will not require this immediately but when we build the provision for exception handling will require ALU also you tell us if there is an overflow or not [noise] (Refer Slide Time 30:11 min)


we would need multiplexers perhaps a different point but I am showing just one example here 
multiplexers for example may make a choice between um PC plus four and PC plus four plus offset
so this is how for example branch can be implemented by making a choice of one of the two options (Refer Slide Time 31:07 min)



[noise] so this is the crucial component register file 
now to take care of instruction like add subtract and so on we need two operands to come from register file and also the result has to go back to the register file 
so this register file um is nothing but an array of flip flop either you can think of it as two dimensional array of flip flops right a thirty two by thirty two array of flip flops or a one dimensional array of registers okay 
each register being thirty two bits so so in any case it has a total of thirty two into thirty two bits of storage [noise] 
and it has provision for reading two thirty two bit values and writing one thirty two bit value 
so these are two outputs which I will label read data one and read data two and one input which is labeled as right data okay 
then the address of the register which register you want to read or write is specified by these three inputs okay 
so for the data which is being read out here the address is specified here 
each of the address input is a five bit input because you have thirty two registers 
so two addresses for read one address for write 
and its a sequential elements so [noise] a signal will come here to a clock it [noise] (Refer Slide Time 31:38 min)



this is program memory [noise] one could um work with design which has a single memory for data and instruction or one could have separate one 
so in the initial design we will assume two separate memory just keep thing simple [noise] the instruction memory is something from where we only read instructions 
we dont modify this okay 
so they it has one might debate whether its a combinational elements or a sequential element memories are sequential elements but in the present contexts we will not be changing straight of it okay 
so it it will act like a combinational circuit because we will assume that the quantent of these are fixed
we are not changing it so so I am not show any clock input here 
so basically address will be supplied to it address is an input address which comes from program counter and outcomes the instructions 
so it it responds instantly to the input coming from the program counter much in the same way as a combinational circuit would do [noise] (Refer Slide Time 33:18 min)



the memory which contains data will have data input and data output read data and write data okay 
and of course address so here we assume that unlike register file where would be doing read and write probably simultaneously within the same instructions 
as far as the data memory is concerned as we note is that there is a load instructions which we read from memory and there is a store instruction which writes into memory 
so we we dont do both of these together and therefore we will have a single address input 
this will take care of reading as well as writing operation [noise] 
and there are of course external input which a specified what is to be done whether read is to be done or write is to be done (Refer Slide Time 34:36 min)



then we have a some miscellaneous kind of circuit elements which would be require to put this together and complete the picture  
for example we require sign instruction or many instruction so imagine the instructions which um perform load operation so the address calculation requires a sixteen bit offset to be added to quantent of a register 
so now the sixteen bit number could be a positive or negative so before we added to a thirty bit number we need to sign extension 
so so that its a polarity preserved so we need a circuit element which I am denoting by this okay sixteen inputs and thirty two output 
it does not require any active components or it need is a particular way of wiring the inputs and outputs 
so other any restrictions for four bits okay um suppose you are extending from four bit to eight bits all you need to do is the the MSB of four bit needs to be repeated so many time 
so so this is a this four bits are input eight bits are output 
and you would notice that this particular bit which was MSB earlier gets now replicated and is available as the upper four bits apart from that itself okay 
um shifting so in particular shifting by two um or multiplication by four which is required when you when you take offset which are for branch addresses 
so in beq instructions if you recall that sixteen bit offset which you have 
is actually a word of set okay so to get the equivalent byte of set you need to multiplied by four 
so multiplying by four is nothing but shifting by two and this also it does not require ant active element any gate all you need to do wire it in a particular manner 
suppose you have eight bits and you want to shifted by two the output will have six of the bits connected to the six of the bits here and these two bits will be hardwired to zero 
so now we we are ready we all this components to a put together 
and in the next lecture we will take up um the task of putting these together in manner that the instruction can be correctly executed (Refer Slide Time 35:30 min)



so to summarize we began by looking at the simple subset of instructions for which we want to design the processor we took nine instructions 
and we um looked up the concept of designing um those system interms of data path and control the whole thing is divided into data path and control 
we will look at the building blocks um how they are distinguish 
how come combinational circuits are distinguished from sequential circuits 
how sequential circuits are classified as clocked or unclocked okay 
and we looked at generic examples of a combinational sequential circuits 
we we looked that a clock and timing issues okay 
what are the time constraints post by clock period or um what what are the constraints post by the circuit delay on the clock period 
and then we looked at specific circuits which are required to build MIPS processor namely register multiplexer adder ALU register file program memory and data memory 
okay I will stop with this 








Transcriptor Name: G.Sathis Kumar

COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   18

Processor Design (Contd ) (Time 45:48 min)

in the previous lecture we had looked at the basic building blocks which are required to construct the processor design 
we will now try to put these together to come up with the very simple design and later on we will look at the performance issues and try to improve the design 
so what will do today is a have simplest possible solution to the problem of taking a set of instructions and having a circuit execute those instructions 
so um we will build this design in a small steps that you can see each and every step clearly and get a clear picture of how the circuit is getting designed [noise] 
we will start with um small set of instructions first we will take only R class instructions which includes arithmetic logical and comparison add subtract AND OR and slt 
um so that will be only part of the solution then we will add instructions by instruction and see how the whole thing can be build 
so um with the basic skeleton of the design will include other instructions step by step 
so first we will include load store instructions to the basic set of four five instructions and then um will include the jump and branch instructions
so after having put the data path together we will try to see how you control it what actually it require interms of control signals to do it make it do the right operation at the right time and we will interconnect a controller to this data path which we will build 
we will not go into detail design of the controller that will take up in the next class (Refer Slide Time 02:50 min)



so um once again lets look at the subset of the instructions which we have set out to considered for building this data path 
among the arithmetic and logical instruction we have this five add subtract two arithmetic instructions 
AND OR two logical instructions and slt which does a comparison 
so so they are all of same class in the sense that they take two operands from registers  perform some operation put the result in register 
then we come to load and store which access memory okay 
so basically the data is transferred between register file and memory one way or the other 
and then instructions which influence the flow of control 
beq which does a comparison and then decide which it go and jump which is in conditional jump instruction (Refer Slide Time 03:32 min)



as I mentioned in the last class the whole design will have two parts the data path and the controller 
the signal which go from controller to the data path I called control signals and signals which comes from data path to controller I considered as a status signals 
so controller um times activities in the data path and also a directs what has to be done in which clock cycle or which instant 
so status signal is an information which controller seeks from the data path to decide it s the actions (Refer Slide Time 03:46 min)



okay so we are going to begin with these five instructions add subtract AND OR and slt the the process involve or the action which will be required would be to get the instructions from the memory taking program counter contents as address 
then depending upon the fields which contain register values um we access the register file okay 
so the register the register addresses will come from instruction fields 
and the operand which we get from register file a passed on to ALU then the result produce by ALU is passed on to the register file and we also increment the PC make it ready for the next instructions 
so before we beg[in] begin lets look at the format of these instructions the format has several fields a six bit field is opcode field okay 
and for all these instructions this is common okay there is a same there is a single code which actually defines this a larger group then this 
this only a subset of all the instructions which have a common opcode here 
then there is a source register five bit third register five bit these two are the operands these two specify this is an operands 
and another five bit field specify the address of destination 
so all these or numbers from zero to thirty one and specify one of the registers and then this is unused in some instructions this is used to specify shift amount 
and it is this field which is called function field which will distinguish these five instructions from one another and also from other instructions which a part of the group 
so we we would need to look at um all the field except for this field which is just to be ignored (Refer Slide Time 04:23 min)



so the action begins by fetching an instruction from the program memory 
so we have program counter a register which will carry the address of the current instructions and there is an instruction memory okay 
instruction memory in this design will be assume to be having fixed contents we are not going to change the contents 
so the only input to this is an address input okay and the only output is an instructions which come out of this 
so you given address instantly the instruction comes out right 
we assume that the program is some how stored in this memory already by some means 
infact such memories a called read only memories if you are familiar okay where by a special process you load the contents in the memory and then you can only access it you can rewrite it 
so in that sense this will behave like a combinational circuit does not require a clock you given input you have an output 
but but the function which transforms in input and output is fixed and by special process it can be changed 
so PC feeds the address input of this memory [noise] and we we get an instructions alright [noise] 
next thing is to look at the instructions various field in instructions and um address the register file 
so particularly for these five instructions we are talking of we need to an access rs and rt 
so which was in second and third field in the instruction if you remember and bits of this thirty two bit instructions which you are getting would be used to address register file 
so register file um as a discussed yesterday so it um specifically for this particular design we need register file as an array of register with provision of reading two values at a time you can read two registers and you can write one register at any given time 
so now the address is for these three things two reading and one writing or provided independently they could be in general different they one two or more can also co inside but register file will respond correctly in all this cases [noise]
so the the three address inputs are read address one read address two and write address okay these are three inputs each of this five bits they are two data outputs read data one and read data two 
and there is one data input write data okay 
so we have instructions which is thirty two bits so specific fields are being tapped out of this okay 
bit bit number twenty one to twenty five this is rs the source register 
and then goes to one address bit number sixteen to twenty form the fields which defines rt are the third register 
this goes an address is rad two or second read address okay 
so so these um out of these thirty two bits we are taking two groups of five bits and connecting into register file okay 
now um once addresses are given this register file will respond with data here okay 
and the operand which will come out of this will need to be passed on to the ALU 
so we have an ALU imagine the same design which we did couple of lecture back where by specifying some control signal you could perform addition subtraction AND operation OR operation you could do comparison for equality
you could do comparison for less than okay for the purpose of slt 
so same ALU you are putting now as a block box okay we we are not looking into details of what is inside we understand that design and we are simply using it to build a larger circuit now [noise]
so these two outputs are forming two inputs or two operands for the ALU right 
and next the result which is produced by ALU um would be ha[ve] would be sent to register file for storage okay 
and that that s way the cycle of flow of data or cycle of the instruction would be complete 
so now at the moment I am not worrying about how to control ALU to do the right function 
now we are looking at those five instructions together and ALU would need to be told which of the instruction is 
so we will eventually when we talk of control we will look at bit number twenty six to thirty one the opcode field and bit number zero to five a bit number zero to five the function field we will look at those fields and then pass one some signals to ALU 	 
so that it it does the right operation 
so right now we are not distinguish between those files and just a looking at overall flow of the data 
so output of ALU goes back to this register file to the right port okay
um let me emphasis terminology these are called ports for the register file okay
port means you know there are something like gateways so um there are two read ports in this and then one write port 
and this is going back to the write port 
so now [noise] when we are writing we also need to arrange for the address where it has to be written 
so we need to look at another field um from here and make sure that the address is also delivered correctly to the register file 
so bit number eleven to fifteen or the destination address okay
and that connects to the third address input so so now with this um this cycle is complete so basically starting with PC we we have a sequential element here okay 
so at edge of a clock a new value is available at the output of PC okay which defines address of a new instructions 
and as a function of that we get that instruction as a function of that we get these operand as a function of that we get the result and finally result is available at the input of the register file (Refer Slide Time 11:56 min)



so now at this point when a clock edge comes to the register file this information will get stored at that instance 
so we are assuming that transition in the state of register file would be edge triggered 
so um you you have one clock at which PC gets the value and at the next edge of the clock the the result of this instructions will get stored in the register file okay 
and at the same time we we will arrange we will see in the next slide that PC will have to get a new value and be ready for the next instruction
so lets a complete that part and see how PC is to be incremented 
so all we need to do is have a four added to the PC quantents and connect the result back to the input of PC 
so this is an adder with one input as four one input as PC and output of this is going back to PC 
so you you give lets say at time t the output of PC was available for a new instructions 
and then a time t plus one I am counting time interms of clock cycles not in nano seconds alone
so a time t plus one which is the next clock um you get a trigger here and trigger there so um the current instruction completes by storing its value storing the result in register file and at the same time PC gets the new value and ready for the next instruction
so so that completes one cycle and if the next instruction was also of the same type in the next cycle that instruction will be executed 
so so this cycle it can go on and um the important thing note here is that the instruction is executed at one clock cycle remember that in past we have talked of CPI and we have talked a figures of CPI which add two three four and so on 
so later on we will discuss design where instruction execution does take the multiple cycle and there good reason why we would do that 
but currently this design is aimed at doing the instruction in a single cycle 
and that was the simple component which was talking of we are talking of simple possible design [noise] okay (Refer Slide Time 14:20 min)



now um we have seen design for these five instructions we need to now go further we need to augment this to include more instructions 
and we will see that by making a small incremental changing we can accommodate more and more instructions 
so um let us address load and store instruction is next 
and the common thing there is that um memory has to be accessed okay and the mech[anism] mechanism for calculation of memory address is same 
you have to take um [noise] this number okay which is the signed offset sixteen bit number and contents of this register add the two and apply that as address for the memory okay 
and this field defines the register which will exchange the data with the memory depending upon whether it is load or store right 
we need to look at these two registers specified by these two fields and a constant (Refer Slide Time 15:45 min)



okay here is a same design we have done so far and now we will add more things to it to make it possible to do s store instruction we will then add load instruction 
so first of all we need to bring in data memory right 
unlike instruction memory this will have data input as well and that actually only difference you have a address read port write port okay 
um well actually strictly speaking this will be considered it will not be considered a two port memory it will be consider a one port memory because you either do read or write there is a single address alright 
and we will define control signals which will ensure that either you d d either you perform a read or write okay it s a single port memory but it can do read or write whereas this is a three port memory two read ports and one write port 
this one has a single read write port infact um some memory module have this common okay there is a same set of wires which connected to memory through which you can send the data in or take the data out 
so it that data terminal is is a bidirectional one okay 
there is a one address line and bidirectional data line 
um in this case of course again for simplicity we have separate read and write lines but there is a common address line 
this is also a single port memory with a single read port and that s all okay 
now we have positioned the memory here we now need to connect the inputs and outputs for this 
so first of all the address as I mentioned will be produce by performing an addition 
so we will use a ALU for doing the same thing okay 
because load store instruction do not require other arithmetic operation to be performed we will use this ALU itself to calculate the address 
for doing PC plus four we could not used this because we we had instructions which you are using it some other purpose 
and this PC plus four four was being done over and above all that so we required a different piece of hardware here 
but for load store instruction we will um use ALU to do the address calculation and therefore I have connected in in this manner 
I need to make sure that the right um input comes to the ALU for calculating address and that is the next thing [noise] 
so we don t directly connect register file output to ALU because for address calculation it is constant coming from instruction which will be loaded 
we need to put a multiplexer here okay um which will have this rd two output for R class of instructions but it will have something coming from this [noise] for performing address calculation 
I I have not connected it yet here because those sixteen bits which we get from instructions need to be sign extended before we can pass it on to ALU 
so this is the block which is doing signed extensions it takes sixteen bits as input bit number zero to fifteen of the instruction and it does sign extension okay 
so here it is thirty two bit output so so wherever I have not labeled things assume that you have thirty two bit output thirty two bit signals okay 
each wire is actually carrying thirty two bits exceptions are here we are labeled explicitly or here I have also indicated that there is a sixteen bits explicitly 
so so now you see what is happening [noise] that rs is specified by this field um would bring out um some thirty two bit number here okay 
sixteen bit taken from here sign extended and we will control the multiplexer to select this path okay 
when it is a load or store it is this path which will be selected so we must give input one to the multiplexer control input one 
and for add subtract instruction we must give um zero so that this goes in okay 
so wherever we we have actually two paths conversing to same destination we you will notice that we put a multiplexer 
and then it is responsibility of controller to control this multiplexer correctly so that depending upon what we are trying do the right thing is done or right data gets passed through that multiplexer 
so so this a this part takes care of inputs for um data memory address input in particular 
so it comes from here and inputs of the adder inputs of the ALU are also arranged now properly okay 
to complete the picture we also have to make sure that that data from register file goes to the data input of this memory 
so where does it come from in in this um in this diagram where will the data come from which will have to be connected to this wd yeah rd two 
because it is a rt the the third field which actually specify which register has to be written 
so so that is already happening here actually and that output is available here 
we simply need to connect this to this okay (Refer Slide Time 22:44 min)


 
so now the the complete arrangements for performing store word instruction 
and now we can move to the load word instruction okay 
now in the load word the address generation part is same that we don t need to touch okay this is same same mechanism same paths and that need not to modify 
um what additional thing we need to do for load word is to take the data from data memory and put it back in register file at propagate address 
so first of all we have this line going to wd write data of the register file 
so we have broken this here um so that we can take this and this and put a multiplexer okay 
so there two options are and they need to be joined with a multiplexer so this multiplexer when you give control as one then it will um send this to the register file 
and when the control is zero it sends this to the register file 
so we will have to remember that for add subtract AND OR instructions the control has to be zero 
and for load instruction control has to be one here okay 
so so this is one part of the picture the second part of the picture is to [noise] give the correct write address okay 
the the write address um comes from now lets figure out from where the write address has to come 
see for add subtract instruction write address is coming from this part okay bit number fifteen to eleven to fifteen and for this it will come from bit number sixteen to twenty 
for load instruction it is rt which is to be used here 
for for add subtract it is a rd which which decides the write destination 
now it is rt which will do it 
so again we need to make changes here introduce a multiplexer because it is either bit eleven to fifteen which go there or bit number sixteen to twenty which go there 
so we have to make a provision for that we remove this line and put it through a multiplexer and the two choices are either this or that 
this is the choice which will be taken for load instruction 
this is the choice which is taken for add subtract AND OR slt instructions okay 
so now we complete the this is the complete picture (Refer Slide Time 23:38 min)



for now seven instruction we have done and add subtract AND OR slt and load store okay 
so everything which is required for these as far as data path is concerned is there 
now lets look at branch and jump instruction again we will take one by one 
um they they would need to do something with the program counter because these instructions influence how the next instruction is chosen we we didn t modify this part for load store because this part continues to be same right 
um you have one instruction the next instruction follows but for branch instruction we need to modify this and also we will use ALU for equality test okay 
the comparison will be done and ALU will produce a bit which will indicate whether the two inputs for equal or unequal okay (Refer Slide Time 23:38 min) 
the branch instruction has this format opcode this is I format as we have for load store rs and rt are the two registers which will be compared 
and this number would be added as word offset to PC 
okay and it will be PC plus four to which will be add because that part we want to retain as common for all the instructions okay (Refer Slide Time 26:38 min)



so again start from this point where we come up to 
so we need to do some modification here um instead of sending PC plus four back directly to PC 
we will introduce more options and the options would be that this with something added to it 
so now lets look carefully first of all this is a multiplexer which is making a choice either we take PC plus four okay 
or we take output of this adder which is adding something to PC plus four what is that we are adding we are adding um this sixteen bit constant which is been sign extended but it is also been shifted 
this s two I am using to shift this number by two bits to the left okay which is the effectively multiplying by four and getting a byte number from word number 
so so this as a discussed earlier it s a matter of simply wiring things correctly 
so so we have the correct offset coming here with gets added to PC plus four and is available to this multiplexer 
now this multiplexer has to look at which instruction it is if it is not branch instruction it will simply allow this to go through 
if it is branch instructions then it looks at result of comparison in the ALU 
and accordingly a zero one will be chosen here 
so again we will get into those details when we discuss the control for the moment we assume that some how there will be some logic put together which will take care that um correct value zero one is given here 
so as for the data path is concerned what we have introduced here essentially is one multiplexer and one adder and this um wiring of signals so that shift of two bits takes place okay (Refer Slide Time 27:30 min)



now just to remind you we had seen that signed extension and shifting or essentially wiring there is no active gate or any active component involved 
sign extension means repeating one particular bit and shifting means just rearranging these and supplying a constant zero to some of the bits (Refer Slide Time 29:12 min)



so in the previous diagram the sign extension and the shift are essentially these kind of wiring arrangements (Refer Slide Time 27:30 min)
okay now finally we come in to this jump instruction jump instruction has only two fields 
the opcode field and a twenty six bit field which which decides the next address 
so once again since it is not a full thirty two bit address we need to retain some bits of PC as it is okay 
what we will do is we will take these twenty six bits um and four bits from PC plus four again not PC and a form a new address for the next instruction 
so we would require another multiplexer which will provide one additional choice 
so we we introduce a multiplexer here remove this line and pass through a multiplexer [noise] 
and the input for this multiplexer is coming from instruction bits we are taking this twenty six bits from the instruction field okay 
shifted left by two positions to get twenty eight again same reason we want to get byte from the word number four bits we are picking from PC plus four okay 
bit number twenty eight to thirty one and these four bits and these twenty eight bits are concatenated together to form a jump address which is a thirty two bit value 
and this is available as yet another input for going back to the program counters 
so program counter PC has three possibilities either plain PC plus four or this PC plus four plus offset or this combination of PC bits and instruction bits right
so you you can actually combine these two multiplexer into a single three input multiplexer we that s another way of designing (Refer Slide Time 29:46 min)



but we will just retain it in this manner okay 
now the next thing is to start worrying about how we are going to control this data path okay what what are the points where we need to apply control signals so that is what we will see next 
so all multiplexer would require control and in this exercise of identifying the control we will assign names for the purpose of reference for all the control signals 
so we will call this control signal for this multiplexer as Rdsd or register file destination 
it will select um the destination address in the register file it comes from here or from here 
then register file um needs a control signal to tell it whether it has to right or not okay 
all instructions you would notice or not writing into register file 
so its only first five instructions write and a load instruction these six instructions out of the nine write it into register file 
so so this will have to be made zero or one accordingly 
um ALU source which I am labeling as A source is the control signals for this multiplexer 
and this will distinguish whether ALU is being used for address calculation or for normal arithmetic logical operations 
ALU would require a three bits to control it um recall the design of ALU which we have done um we we had some circuit and then there was a four input multiplexer okay which will select AND output OR output or plus minus output or output for slt 
so two bits are required to control that and another bit to choose between add subtract so there are total of three bits I am labeling that as op standing for operation 
then a status output which will come from ALU I am labeling it as z standing for zero it s a comparison of the two operands 
from the point of view beq instruction so comparison you recall of based on by doing subtraction and checking of the result is zero 
so that s why I am labeled it as z so this is not an input this is an output this is status and other the control 
data memory requires control for read or write so MR stands for memory read MW stands for memory write 
we will make sure that memory does only one operation at a time 
actually in this kind of arrangement where read and write lines are separate it is also possible that you do read and write simultaneously in this okay 
but it has to be from the same address same location in the memory is used for reading and writing simultaneously 
but in our design we we don t do that we will either do read or write okay 
and then another multiplexer here which is distinguishing between what goes to register file whether it is from memory or from ALU alright 
so memory to register file M two R this is called P source PC source basically either here or there 
then whether it is jump instruction or not so another control signals so these other control signals so we require some circuit which will produce so many outputs one two three plus three six seven eight nine ten eleven 
these eleven outputs and that control will have to look at this and also look at two fields of the instructions okay 
the opcode field and the function field so that that is the controller which needs to be designed it it has to have um this twelve inputs infact its to speaking plus one thirteen inputs and eleven outputs okay (Refer Slide Time 32:15 min)



so if you can design that and plug it in here that will complete the task but we will not design it as a monolithic circuit we will design it again interms of some simply parts as you will see now 
so now you want to bring in the controller we will not go into details but we will um just go through and outline of what it is going to be 
first of all we look at opcode bits and these bits are sufficient to distinguish between jump branch load store and this group of  add subtract etcetera all together okay 
so except for controlling ALU from these six bits we can derive all the information out of the eleven signals eight can be straight away figure out using these 
of course generating this will require as to look at z also right 
so lets connect the output of this control I will show the outputs and connect to various signal which can be driven by this 
si if it is [noise] load instruction which we can figure out from here we will allow this to pass through here otherwise will allow this to pass through 
so this hardest can be determined from this whether we have to write register file or not can be determined from this [noise] 
ALU source can be selected okay it can be determined from this memory write and memory read okay 
so if it is a load instruction we will do memory read 
if it is store instruction we will do memory write okay 
then control of this multiplexer okay it has to distinguish between load from other instructions so that also can be done 
here we are doing slightly differently um we are first of all figuring out that it says branch instruction okay this controller will activate a signal which tells us that it s a branch instruction 
and that we AND with the z output to control this P source 
so you can see what is happening if [noise] if the instruction is not a branch instruction you will have a zero here okay which means that P source is zero irrespective of what is z and um the multiplexer passes the upper input 
so when it is not a branch instruction we don t branch we don t even look at z when it is branch instruction we will have this as one okay 
controller will make this output as one now it will depend upon z we will get one here or a zero here 
so if the result of comparison was true okay if the if the two registers are equal this will be activated this will be one and you will get a one here 
so this address will go as a next address of PC next value of PC 
and if the test failed then this will be zero you will have a zero and PC plus four will continue right 
another way if you recall the gross diagram I have drawn where there was a data path and controller we I had shown status signal going to the controller 
so if you strictly speaking controller is not just this is it is this plus this okay 
but for um it has actually simplifies a matter to look at as a six inputs circuit rather than a seven input circuit 
because the influence of this has been handled separately with makes it some what convenient 
then [noise] we can also control this multiplexer by figuring out whether it is a jump instruction or not 
so now um what is left is opcode operation which ALU has to perform and it will depend upon six bits which come from the function field okay 
so instruction zero to five would be seen by another kind of control circuit I am following it A controller ALU control 
so looking at these six bits we are trying to determine what this three bits are but remember that ALU is being used for load store instruction also for a different purpose  
so so we need to look at this also but we don t we dint need to feed all those this six bits here 
what we need to tell this circuit is which class of instruction it is okay 
whether it is a R class one of those instruction or it is load store or something else 
so so we will have some information coming out from here connecting to this and this needs to be just a two bit information because we like to distinguish between three different cases 
the R class instruction add subtract AND OR and slt that s one possibility load store together grouped as an another possibility and although rest as third possibility 
so we we will take two bits out of this and connect to this okay and I am calling it opc which stands for operation class 
so so basically this circuit now has six inputs plus two inputs eight inputs and producing three outputs 
so we um we have taken special care to keep the number of  inputs low 
so the complexity of a combinational circuit like this or like that depends upon the number of input lines and number of output line
but I will not I will not explain that but you can take it that um with the number of  input lines the the size of circuit the complexity grows generally exponentially okay
but but with the outputs it will generally grow linearly so we have more worried about keeping the numbers of input as low as possible okay 
so the larger numbers input then we we can look at parts of its separately and then combine the results that s always the better strategy and that what we have followed 
so this circuit is predominantly looking at the opcode field whereas this is predominantly looking at the function field 
and some additional information it requires about opcode is being actually process by this and made available here right 
so after having done this um what remain this basically to look at design of this and design of this (Refer Slide Time 40:50 min)



so if you can enumerate what outputs you require for what inputs you you can do it in a straight forward manner and same thing here 
so we will take it up in the next class 
so is to summarize [noise] um we have designed a processor except for some controller details for these nine instructions add subtract AND OR slt load word store word beq and j 
and our approach was very simple step by step approach we gradually put the components on the table connect wires and build up the circuits 
so first we took this group of first five instructions which are similar in nature okay 
though only difference comes in the way you control the ALU so we we did that we started with these five 
added store and load and then added beq beq and jump 
then we identified the control signals for the various component which we put there and we placed some block boxes which you called controllers which expected to produce this signals (Refer Slide Time 44:13 min)




and the next class will be to look at the details of these to block boxes and design this 
that s all for today 
thank you 









Transcriptor Name: G.Sathis Kumar

COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   19

Processor design   Simple Design (Contd.) (Time 50:02 min)

we will continue with the simple design of processor which we began in the last lecture what we have done primarily was that we have taken a small subset of instructions and try to design the data path 
what we will need today is that we will look at that design from a slightly different angle and come up to the same point we will then go in to details of control part design 
so data path is one which is doing the computation and control in one which guides or directories of computation 
so that the right action takes place at the right time 
um this is part of the overall plan we looking at a simple implementation where the entire instruction gets executed in a single cycle after we finish with this we will look at the performance consideration and probably go for a more involved design which is the multi cycle design [noise] (Refer Slide Time 01:36 min)



so today we will arrive at the same data path design by by looking at it by different angle what we will do is we will look at each instruction or group of instruction separately and then try to merge the results or merge the outcome of each separate consideration 
and then we will look at what control inputs are required to drive various components in the data path and based on that we will um build the specification of the controller 
and then go into details of the controller design 
we will see one possible implementation using what is called PLA a programmable logic array 
and if time permits I will talk of  the performance consideration how do you analyze performance or analyze the delay of such a design [noise] 
okay at the end we will notice that um this particular design approach has some limitation and one you understand those limitation we will able to talk of an improved design (Refer Slide Time 02:53 min)



okay so lets compare the two approaches which I am talking of one approach you followed in the last lecture and another one will follow now 
what we did yesterday what is that we began with one design lets call the D one where we looked that five instructions which belong to one particular group namely arithmetic logical group 
we augmented or analyze the design by including store instruction we improved it further we expended it further by including the load instruction then we added on beq instruction and then finally we through in place j instructions 
so so that s how starting with a very simple design the degradability more and more involved design this processor actually can be continued 
so you you can you can look at the entire instruction set look at more instruction and by following similar approach you can keep on adding or enhancing the design 
so that one approach you followed 
what we will it do today is that we will look at separate design as it there were only limited number of instructions
so first we will have a design which looks at these five arithmetic or logical instructions of course the answer will be no different from what we had earlier but then we will leave that a part and look at just store word instruction and see what are the requirements for this particular instructions
similarly we will do it for lw separately then beq an isolation then j in isolation and then we will try to merge these five different design which may have something in common 
so that the common part will remain common but we will kind of take a union of whatever the requirements of a different solutions and put them together 
so and since our approach is same that we are trying to do whole thing in a single cycle and the given brought components may be identified we will approach the same design point at the end (Refer Slide Time 04:05 min)



so this is what we had the starting point even yesterday that for doing add subtract operation you need a PC to supply the instruction address from the instruction we will look at the register address fields 
fetch two operands from register file apply them to ALU result of ALU goes back to register file and um mean while we also make sure that PC gets increment it to PC plus four (Refer Slide Time 05:04 min)



so these components can be put together to carry out um the first group of five instructions 
then now lets look at sw alone in isolation 
here ALU does something different ALU does the address calculation okay and we bring in the data memory component so ALU feeds the address for data memory (Refer Slide Time 05:45 min)



and the data input I don t know what happened to the curser I am not able to get curser here okay suppose leave that [noise] alright 
so the address is found by adding one register content and sixteen bits as an offset coming from the instructions okay 
next we have data path portion of data path which will take care of lw if that was only instruction 
so notice that now we are still looking at two addresses from the instructions but why in case of stored both word being used to read now one is being used to read and other is for writing it s a load instruction 
so one register takes care of address generation and other address is where the destination of data coming from memories (Refer Slide Time 06:45 min)



this is for beq so again I have omitted data memory ALU is being used for comparison okay 
we are looking at two operands from the register file which is being fed to ALU for comparison purpose 
and the target address in case the condition is true we need to go to some particular target address that being calculated by adding an offset to PC plus four and note that we have sign extension and we are shift by two although similar there (Refer Slide Time 07:20 min)



finally for jump instruction all that we need is putting the right bits together to form the destination address 
we don t need register file this is instruction doesn t look at register file doesn t require ALU doesn t require memory and all that you require is some little circuitry to wire the bits forming the address propagately okay 
now we have we have come of with this five partial designs we need to put them together to form the final design okay 
so these I am calling these a sub design we are going to merge these so this was one 
lets quickly sight of through this that s the second one for this is for store instruction then this is for load instruction (Refer Slide Time 08:50 min)



this is for beq instruction (Refer Slide Time 08:53 min)



and this is for jump instruction (Refer Slide Time 08:59 min)



so so you would need you would notice that they are few things common each instruction may not utilize all the resources but if you take union of all this we can reconstruct the entire designs 
and that s what you will do now 
so [noise] we we will now okay now we will we will place we will take this five as they are five five sheets of paper and just a superimpose one way another 
and naturally the thing which are common will fall in place um and which are which are not common we will kind of take union of those but there will be another thing that there will be some way conflict in in one sub design we want one piece of data go to a particular destination 
in another design we want something else to go to the destination 
so we will identify those take corrective of majors okay (Refer Slide Time 09:43 min)



so the second one is superimpose of same thing (Refer Slide Time 10:22 min)



third one is again put over that okay so we are not erasing anything 
and some where the lines are getting doubled up some where two different thing will go to same point 
this is a for beq and finally (Refer Slide Time 10:42 min)



this is for jump (Refer Slide Time 10:45 min)



so now we have everything just superimposed and what remains now is to identify the points of conflict where we are trying to send two different things to same point in two different situations 
so we now try to identify conflicts this is one point of conflict okay where add subtract instructions send one thing here okay and the load store instruction try to send some thing else here okay 
we in one case we are adding two operands coming from register file 
in other case we have one from register file one is a constant coming from instruction 
so there is a this is point of conflict 
this is another point of conflict interms of what goes back to the register file 
so in arithmetic instructions output of ALU goes back 
in load instruction output of data memory goes back 
um and associated with that is this point of conflict where there are different address is used to address the right port of the register file okay 
it is either INS eleven to fifteen or INS sixteen to twenty different fields in the instructions are addressing the same right port 
this is also a point of conflict where in instruction rather than branch and jump we simply want PC plus four to be send back to PC 
but in um incase of branch we are something else coming out of address calculation and that needs to be going to PC 
and finally this point where jump has one way of calculating address 
branch has another way of calculating address and ultimately they need to go to same destination (Refer Slide Time 12:28 min)



so what do we do for these from the other design we can actually get an idea that we need multiplexers and multiplexer will be driven by suitable control signals 
so we introduce multiplexer basically we um have to introduce multiplexer and also connect the signal propagately  
so you would notice that except for one place I have put in the multiplexer the the fifth one the point of conflict between non branch and jump instruction and other instruction would get resolved automatically because the multiplexer we have put for branch itself it take care of that 
because it s a conditional branch it it has a provision of passing PC plus four as the next address so it you you are either having PC plus four plus offset or PC plus four 
so the provision which we have PC plus four will suffice for non branch and jump instruction 
so we don t really need a multiplexer there (Refer Slide Time 13:00 min)


and lets now remove the conflicting line and put a propagate connection in place of those 
and we we reach the same design which we had arrive at yesterday
so just a idea is to give you different view different way of  looking added 
so whatever is convenient that approach could be follow nothing secured about one or other [noise]
okay is there any question at this point of time alright 
this is okay lets move towards working on the design of controller 
so you would recall that we are identified control points so since it is same design we are same points primarily we are controlling the multiplexers we are controlling read write operation of memory um write operation of  register file and with we had a gate controlling the PC source so so that controller design is simplified 
so now what we need to do is a look at each instruction group of instruction and see what value needs to be given to this signals what input needs to be given 
so for example lets start with R type instruction or basically arithmetic logic instruction RDST needs to be made one because the write port has to be address by bits eleven to fifteen of the instruction okay 
so we need to give this control signal is one um
RW which tells the register file has to taken a new value or not this also needs to be made one because you are writing something to the register file 
ALU source is zero we want to take the top input 
MW is zero you are not writing into memory 
MR is zero you are not reading from memory 
and M two R is also zero because we are taking the bottom input of the multiplexer 
going further its not a branch instruction we keep the signal zero its not a jump instruction we keep um this signal jmp also zero 
now let me point out that there is a change which I made in the diagram in relation to do breeze one is a label one zero I have reversed in this jump control multiplexer okay 
that is to convenient so that um we have the meaning of the jump signal the polarity is one when it is jump instruction 
um nothing very particular about this but just to call it jump otherwise I would need to call it jump bar that s the usual convention 
so now this is the set of values this is the set of inputs you need to give at control points and that is given the instruction done by this data path will be um the R type instruction we we still have not specified what input we give to opi I will take it separately but lets look at the others for each instruction
so all these put together is a table of values we need to give at different control points right (Refer Slide Time 17:00 min)



lets go to the next one sw instruction and repeat this exercise Rdst is a don t care because we are not writing into register file and that will be indicate by making RW as zero 
so basically when RW is zero value of Rdst could be anything it doesn t matter 
if ALU source is one because we need to take the bottom input 
memory write is one it s a store instruction 
memory read is zero and 
M two R is also x this will also go with Rdst 
so when RW is zero Rdst will be x and M two R will be x this will be don t cares 
doesn t matter of which one you select okay 
so as you would familiar with a logic design that if there are don t cares in the specification these could be exploit it to simplify your circuit okay 
from that module we will try to put x wherever we can 
and further brn is zero jmp is zero put them all this together we have we tabulate all the control signals which are required for this particular instruction (Refer Slide Time 18:19 min)



next is the load instruction similar exercise 
here we are putting thing back into register files so Rdst is zero we are taking the top input 
RW is one ALU source this is same as what we had for store and now we are not writing memory but we are reading okay
so MW is zero MR is one amd M two R is also one because output of memory is being sent to register file 
brn and jmp continue to be both zeros 
so these signals can be put together again in the table and these these green table which I am forming I will have to collide this together to form in overall truth table 
next we want move on to beq instruction in beq instruction neither we write into RF nor we read or write from data memory 
so those corresponding signal will be zero Rdst si accordingly x RW is zero 
ALU source is zero we need to take two operands from register file in compare then 
MW MR both are zero M two R is x brn is one it s a branch instruction now 
so now the z output of ALU will be brought into consideration and that together with brn will control PC source 
jmp continues to be zero okay and all values put together or in this green table (Refer Slide Time 20:00 min)



finally we have this jump instruction so jump instruction again doesn t require anything RF ALU DM all these are in active 
so Rdst x RW zero ALU source is also x now 
sin ce we are not looking at we are not choosing ALU doesn t matter what you give as input 
MW zero MR zero M two R x brn is also x now why it is x because once this last multiplexer this control by jmp where we have made one you are looking at upper inputs so doesn t matter how the bottom input is being computed 
you may selected any of these it doesn t really matter so this is the set of control inputs for jump instruction 
and lets now put this together this is the just sub table for jump (Refer Slide Time 21:00 min)



 um so we will tabulate the instruction and against them we have all the control signals okay 
so if we if we put the opcodes of this instructions from those opcode we need to derive this control signal and these two together will form a kind of compact truth table for controller circuit 
so and we know how to once a truth table is given we know how to design a combinational circuit 
then many ways to do it but I am sure you atleast know one 
so lets a put in the opcodes for this instruction um all R type instruction have zero is opcode okay these arithmetic logical instruction this part is common and that s why we are treating them together 
so there are also opcode output for sw lw beq and j 
so what are the opcodes are we are just taking from whatever definition we have so nothing we are specifying here this is what is given okay 
so now here is a in the overall design there were two control boxes this is the main control and after this now we can look at the second controller which controls the ALU 
as far as this is concern we have completed that design up to this point we are come to the truth table okay (Refer Slide Time 21:47 min)



now we first um define what is how we are encoding opc 
opc if you recall is a signal coming from the main controller and going to the second controller which controls the um ALU okay 
so we said that we will have every all instruction falling in three categories um R instruction one category load store another category and all the rest are yet another category okay 
and basically I should say branch is another category for jump it really doesn t matter so we need to two bits to encode those three groups 
and what will do we will put in another column here which will define this 
so arbitrary let us choose some code or type is a one zero sw is zero zero so is lw 
beq zero one and jump it is don t care 
so our now the truth table is actually complete the this main controller now as you can see as six bits as inputs okay 
the six opcode bits are input for this and you have two bits of opc plus eight bits in the other table all together this turn outputs have to be produced it s a six inputs ten outputs circuit which we now need to design 
but before we do that lets see what do with opc so we need to look at opc and the function field in case of R type instruction and decide three bits input control for ALU (Refer Slide Time 24:00 min)



so now the group of R type is instruction needs to be expanded we need to consider requirement of each individual instruction 
so we repeat this R type entry for five time because we are considering five instruction add subtract AND OR and slt 
so these will defer interms of their function field um opcode filed is same for this 
so um the third column shows individual instructions 
and the last column shows the function field the function field is not to be seen for instruction other than R type instruction okay 
I just put don t care there because in in those particular bits we have something else we have a address offset or some part of address 
they are not specifying function bits 
so now for different instruction we need to figure out what input we need to give to ALU 
so the for the second control circuits for the ALU control circuit there are eight bits of inputs 
two bits of opc and six bits of function and there are three output bits which we will decide now by first noticing what is the function ALU suppose to perform in different instruction 
so in um in the R type instruction the the action to be performed is directly given by the instruction 
so add operation for add subtract for subtraction so on um 
for store and load instruction we we also need to perform add operation 
and for beq instruction we need to perform subtract operationfor jump we need nothing (Refer Slide Time 25:43 min)



so we have enumerated the action required by ALU for this different instructions 
and we also we we now in position two define what control input required for ALU so I am recollecting the ALU design one bit of ALU is being shown here 
and you would notice that we need two bits to control the output multiplexer it has to select from one of the four possibilities and one bit um is actually used to invert b if necessary 
whenever you are doing a subtraction operation okay um then b needs to be invert it 
so we um we need to form ones compliment and give initial carry so that negative of b comes
so three bits are required and we can I am assuming that the left most bit out of the three bits is b invert and the other two bits control the multiplexer 
so zero zero if you want the top input zero one if you want the next input one zero and one one so therefore for AND operation b invert is zero actually one could put it as don t care okay 
if you are not using the adder part of it b invert becomes don t care but I am just put it as zero 
so for AND we are selecting the first or the zeroth input in the multiplexer so the code is zero zero zero 
for R I am keeping b invert as zero and the multiplexer is controlled by zero one and so on 
so based on this the way we have designed ALU we can actually define what is the control input required for this case (Refer Slide Time 27:27 min)



and with this our picture is complete here we have the complete table 
so this is the truth table for the second controller or the ALU controller where we have eight inputs eight bit input and a three bit output 
so um one stage of design actually finishes here 
now it s a way straight forward almost mechanical process that once you given a truth table how do you put  down a circuit 	which will implement that truth table 
given a truth table you can actually write sum of product expression for it okay 
and put necessary AND gate OR gates you can also follow other realization product of sum on NAND NAND or NOR NOR and so on 
but one one approach which is commonly used in a case like this when you have multiple outputs okay 
and for control design typically that is the situation we we follow um PLA based design where where the the design is actually derived in various straight forward manner from the truth table itself 
so this PLA is basically it stands for programmable logical array okay 
logic array means that you have a you have a array of gates which are and this structure is some what universal ypu can program it to implement and in desired function 
so the PLA which will implement this would be if very large I will not describe the PLA which will implement this but I will try to restate the basic principles what is the basic idea behind this a processor design (Refer Slide Time 28:19 min)



so PLA has two parts one is called AND plane and other is called OR plane which corresponds to this this is a basically with AND OR type of implementation or sum of product implementation 
so the AND plane has a row of AND gates okay which implement all the product terms and OR plane has set of OR gates which implement the the sum terms right 
so the input goes to AND plane out the output of AND plane is a set of signals representing the products
and the OR plane actually sums those products to form the sum of products and and gives the final output 
so the the structure this is the overall outline and what goes inside the plane is is um a direct correspondents of what you have put down in the truth table (Refer Slide Time 30:06 min)



so let me illustrate so for a simple situation let us say you have three inputs called ABC and three outputs DEF um you have a set of AND gates which would implement the products of which are required in that implementation right
so each gate here is capable of taking every input either in true form or in complimentary form right 
so so now the connection of each gate to the inputs is what is programmable 
so if you are talking of M input and N output PLA then each AND gate will be N input AND gate that means it can you can form an arbitrary product um of size up to at most M that means each literal 
um for example literal means A or A bar B or B bar and C or C bar so you you can form what you see here is that the left most AND gate is forming the product A bar B bar C 
the next one is A bar B C bar and so on okay 
so in general you you can form um power terms which are of less than M inputs also okay 
sum sum of this may not be connected right um and therefore PLA just needs to form enough number of terms so that all the outputs can be realized 
and then you have a row of OR gates which will pick up necessary required sub set of this product term and sum them 
in this case D is summing output of all the product terms 
E is summing three of them 
F is summing one of them 
and this pattern of what gets connected to AND gates okay which input gets connect to which AND gates in what pattern um could be worked out in very convenient form from the truth table 
and similarly which product term go to which um OR gate is also very easily derivable 
now in actual practice um the gate gates don t look like this um these these gates are actually what is called wired AND gates or wired OR gates 
and real practice they may not be actually AND gates OR gates you would either have NAND NAND organization or NOR NOR organization (Refer Slide Time 31:14 min)



but for concisely lets continue to think interms of AND and OR 
and the structure may look more like this okay 
so the the top part is the AND plane and the bottom part is OR plane where um the number of vertical lines or each vertical line represents one product term 
and you can see that each vertical line is crossing every input and its compliment okay 
so here you have three inputs and therefore the the vertical line which correspond to um AND term or the product terms are crossing all possible input lines and the comp compliments so you you can wherever there is a dot put here it means that the connection is formed 
so um given certain number of inputs outputs in product term the overall structure is same to go from one functional specification to another functional specification all you need to is place your dots differently okay 
you you move this dots around different power term get formed and different product term get summed 
so it s a kind of universal programmable logic where by placing this dots you can form the required connection and get the required function out 
so in in actual practice um the AND gates of formed actually vertically so they are distributed um I am not showing that exact circuit at each cross point there will be a transistor whe[re] which will connect to the cross wire or you will not connect okay 
so um I suppose we leave this those details to coarse on digital hardware design which may do later 
but just in overall functionality you you understand this that its say programmable structure which can be automatically generated from the given specification (Refer Slide Time 34:01 min)



okay having done that now lets a look at what code work we have done okay we we have got a design now 
how how does it fair interms of performance 
so for doing show we need to calculate that delay and their impact on the overall clock period 
so first of all lets look at the component delays so that we build it bottom up 
we have register one register in this case PC for which will assume the delay is zero actually what just to simplify the analysis we will look at the delay of some prominent component take them as non zero others will take a zero 
so register delay will assume a zero 
adder delay now I am referring to the adder which does PC plus four and the one which does PC plus four plus offset 
so lets call lets denote that by t plus the delay of ALU will denote by tA 
delay of multiplexer will assume again zero one doesn t have to assume this I am just doing this to make the expression are analysis little simpler 
so but of course the assumption is a rational assumption in the sense that the delay the delays which are actually small compare to others are being assumed to be zero 
register file tR 
program memory ti 
data memory tM okay 
and then finally the bit mani manipulation components as again um there are only different wirings we take it as zero (Refer Slide Time 37:28 min)



strictly speaking even the interconnecting wires will have some delay 
and if if your gates or logic is too fast then wire delay becomes comparable or significant 
but again for simplicity will ignore that 
so so now interms of these parameters can be determined the clock period 
yes if we analyze the paths we can what we will do is you recall that I mentioned at one edge of clock you have a new address in PC from where the cycle begins that data flows through various components and ultimately you write something in the register file or write something in memory 
and additionally write something in the PC at the next clock 
so the clock period has to be sufficient to allow all this information to propagate to the destinations 
once again yes 
(Student: sir what is the difference between register delay and register file delay)
well register file is an array is a collection of registers register delay I am talking of single register like program counter okay 
now register file has additional circuitry that you are giving an address that address is getting decoded one of the register is getting selected 
and there is some multiplexing at the out output end okay 
so depending upon the size of register file how many register it is it will have some delay 
of course its not difficult to see that that delay of memory would be comparatively much more than delay of register file okay 
and infact it could be larger by one or two orders of magnitude 
um but we will assume that for the moment assume that the memory is reasonably fast and it is not if that was the case then if you take normal memory the bulk memory which you have in the processor and take the access time register file we we can probably set everything to zero except for register file except for the memory right 
in in actual practice what would be influencing the clock period of a processor is not in the main memory but um a cache memory okay 
so we will worry about those details later at the moment lets assume that the way we have been designing we put memory as part of the processor design right 
so which is an over simplification but lets assume that it is a same technology which is being used to build other components of processor and the memory and all the delays are comparable  
so the way I am looking at now in the sceneries is that all the parameters which I have put as non zero or some what comparable okay 
so [noise] we will again to simplify the problem look at each instruction or group of instruction separately see what is the demand on time place by those instruction
and then put the results together 
so if you would consider add subtract AND OR these instructions then um there are two paths we need to worry about 
a path which is computing the sum or difference AND and OR which goes through instruction memory register file ALU and back to register file okay 
so there are ti plus tR plus tA plus another time tR these will all these will get cascaded so all this gets summed 
other path is going through the adder to compute PC plus four okay 
so now if I am saying that um all are comparable then the first one is actually meaningless you know I put straight away neglect that 
and the second expression will dominant 
but let me be little open here and say that the clock period is max of these two 
the reason I want to keep t plus in picture is that um as far as ALU concerns since ALU timing is getting sum with other things 
I will have to make attempt to make this ALU very fast otherwise thing will be back 
so it is typically in the ALU I will put carry look at logic and stuff like that whereas the other adder is has much more room much more lets say cushion 
so I can effort to have a slow adder here because that s going to cost to me less right 
so therefore t plus could be I can effort to have t plus larger than much larger than tA right 
you you would recall that the the delay in carry look at addition and delay in a carry propagate addition the could defer quiet vastly 
one one is propositional to one and other is propositional to log N 
so difference between could be large (Refer Slide Time 40:48 min)



um for sw what come in to picture is ti tR tA and then tM 
now instantly here I am assuming the same time for reading and writing both incase of memory and incase of register file 
in general they need not be same but again just to keep things going out of proportion I I am trying to I am just using same value for each okay 
t plus again remains another path (Refer Slide Time 42:45 min)



for lw the the chain seems to be longest ti tR tA tM and again tR okay (Refer Slide Time 43:20 min)



for branch there three paths I need to consider one goes through both the adders so t plus and t plus 
another path goes through ti the instruction memory from where we are picking the offset and then adding it to PC plus four 
so it is a ti plus t plus okay and the third one way comparison being done is ti tR and then tA 
so so whichever is the largest of these will dominant (Refer Slide Time 43:32 min)



for jump it is t plus or ti okay 
now lets put basically now what will say is that the the clock period has to be large enough to accommodate all all this possibilities (Refer Slide Time 44:05 min)



so it is max overall this max which I can just put as max of all this term 
so each row in in the upper group of expression each row corresponds to one instruction or its group 
so t plus ti plus tR plus tA plus tR this accompanying from R class instructions 
second line is for store third line for load fourth line for beq and fifth line for jump 
now looking at all this together now 
something can be thrown off because the the other thing which are clearly dominating those 
so t plus alone can be thrown out because there is a t plus plus t plus okay 
the the long expression we have for load word lw dominates the big expression we had for add subtract and also one we had for store word 
so this reduces without any assumption to the bottom expressions where we assign it is max of  three of this expression
the first one is coming form load word and the next word coming from beq right 
if you if all this term each individual factor each individual t value comparable then basically lets say each each of this one nano second just for sake of  arguments 
then your clock period would be five nano seconds which means two hundred mega hertz clock is what you have done with okay 
okay now um so this is how the things standard moment we will um we will see what we can do to improve things here (Refer Slide Time 44:21 min)



but let me just summarize now what what we observed this is that the slow slowest instruction is pulling down the clock frequency of clock performance which would most likely would be the load word instruction 
the other thing which are not directly obvious and I am not pointed them so far is that resource utilization is poor 
we have ALU sitting at some place but we have put two adders for doing an operation which ALU could have done 
so question is R is a design possible where we don t need those extra adders can ALU do every thing without loss of performance 
soso we we would need to answer that question 
then um third question is it possible to do any instruction in this particular manner given any instruction it may be complex we have taken very few simple instructions is it possible to do all instruction in a single cycle the answer is again no okay 
there are some instruction which may necessarily require one to take multiple steps and go through multiple clock cycle 
so if for example if there was some instruction which read from memory and also has write in to memory 
with the given constraint of data memory with single port where there is a single address we we can either do read or write or in in best case you can do read write but for the same address 
on the other hand if you have an instruction which requires reading from one address doing something and writing into another address 
that s certainly cannot be done in a single cycle 
or complex instruction which require a moving a block of data from one area in the memory to another area would require several reads several writes 
or instruction which required in similar manner multi pole operation which have to be necessarily sequence 
there is a fundamentally limitation so so because of various reason we would need to go for a different design 
and try to address all this questions 
so I will close with summarizing what we have discussed today 
we first of all started with a different approach to arrive at the same data path design and approaches basically to have several simpler design and merge them together 
resolve conflict by putting multiplexers wherever was necessary 
then we examine the control requirement of various instruction identified the value of control signals which need to be applied tabulated them and came up with the control parts specification as truth tables 
we looked that possible way of realizing which is one that s one possible way of doing it PLA based 
and finally we analyze the design from performance point of view looked that how different paths are getting formed through which data has to flow what what are the implication what is the implication on the performance (Refer Slide Time 48:28 min)



and we notice that um in this particular design approach the slowest instruction pushed on the performance of everything 
and in a subsequent lecture we are going to see how to get rid of that 
thank you 







COMPUTER ARCHITECTURE

Prof.Anshul Kumar	
Department of Computer Science
and Engineering

IIT Delhi

Lecture   20 

Processor Design 
- Multi Cycle Approach (Time 40:55 min)

we have discussed very simple form of processor design called single cycle design 
and in the last lecture we ended by making some observation about such a design 
these observations were about its performance and in ability do certain kind of instruction 
so today we will introduce another type of approach call multi cycle design which tries to overcome this problems 
this is the overall lecture plan 
and today we will start by repeating those problem reobserving those problems which are related to single cycle data path 
we will look at how we analyzed the delays and what for the difficulty which is noticed 
we will see how clock period can be um improved 
how clock could be speed it up by using multi cycle design [noise] 
we will also look at this as a way of improving the resource utilization 
the resources are the main hardware components we have in our data path 
and what basically we will do is we will try to share these components such as memories and ALU 
and to facility that we would need to introduced some more components but half will lower consensually these are registers and multiplexers (Refer Slide Time 02:04 min)



so this is the data path which was designed in the previous lectures [noise] where every instruction is completed in a single cycle 
so all activities begin with a new address in PC and the cycle ends by updating the PC value as well as updating the state of the whole processor in register file and memory [noise] (Refer Slide Time 02:22 min)



so these are the three problems which we have noticed that the slowest instruction pushed down the clock frequency 
if there is a wide disparity in the timings of various instruction then the the whole set of instruction will the running as glow as slowest instruction 
so that is the difficulty with such a design approach 
and secondly resource utilization is poor apart from having a full fledge ALU we had to use two adders 
and we have to keep the instruction memory and data memory separate 
because that type of design its not possible to work with single memory from where you fetch an instruction and then later on fetch data also (Refer Slide Time 02:50 min)



so that will not work out also I hinted upon some kind of instruction which cannot be realized by this type of approach 
we will focus today on first two issues um just to recall that we had analyzed the performance by taking the delays of individual components some simple ones where assigned zero delay 
and other hand some significant delay denoted by t plus tA and so on 
this simples are denoting the delays of individual components (Refer Slide Time 03:50 min)



um and for each instruction or a group of instruction we enumerated various paths from storage element to storage element through which the data has to flow in the instruction (Refer Slide Time 04:00 min)



and identified the possible expressions which would decide the overall clock period so this was the final expression we had 
um the one at the bottom is in a simplified form and it could be noticed that the largest of these sub expressions will actually decide the clock period (Refer Slide Time 04:22 min)



to illustrate this further lets look at some values 
so again I am not putting these values in numeric sense
but trying to depict it pictorially so the horizontal x is here is the time axis and for each instruction I am trying to show the delays which are involved in the flow of information along the data path 
so for R class instruction there is a delay involved in fetching the instruction fetching the operands from the register file then doing the arithmetic operation 
and finally storing the result in register file and so on for each instruction I indicated the possible path which may dictate the clock period 
so I have taken nearly equal values for all this term except that they are slight differences ti and tM I have taken same 
and infact t plus is also same tA is slightly less than that and tR is even little less than that 
so from first three cases I have dropped the t plus term okay because that that s the invariably that s less than all this and we we don t need to look at that 
so now you you can very clearly see that lw with with this kind of values lw will dominate and dictate the clock periods 
so clock period would be from this point to this point remember that horizontal axis the time axis I am showing along that time line 
and um but once a clock period is fixed all instructions are taking same time and you could see so much of dead time in other instruction in R class sw in beq and most in j okay 
so now when we introduce multi cycle design basically what we try to do is divide execution of instruction in to multiple cycles 
so we need to decide what gets done in first cycle what gets done in second cycle and so on 
and they they are lots of choices so we will take one simple choice here which tries to do one major action in one clock cycle okay this is only one one of the possibilities and um what it means is that now looking at each individual times we take max of these and take that of a clock period 
so in this case ti tM or t plus all are equivalent and largest among all 
so that decides the clock period the the R class instruction now get done in four clocks
lw in five 
sw in four 
beq in three 
and j in one [noise] right
so there are still small dead time because of a inequality because all this times are not exactly equal there is some differences (Refer Slide Time 04:38 min)



but you would notice that the overall improvement in performance will be there 
because [noise] the the overall wastage in time is much smaller 
one more thing you must notice is that the total time lw takes now is is more than what it was taking earlier okay 
earlier it was taking one clock which is from here to here now it is taking five clocks which goes from here to here 
so so because there is some wastage of time here also but on the whole everything put together this approach would still give you better performance or save time 
now think look quite okay here quite good here because these different time parameters or nearly balanced okay 
I I deliberately took these values which are only slightly defying from each other 
and therefore the wastage you see is very little 
but um suppose there was a vast disparity in this time 
so so we we will be again having a problem situation 
for example suppose for some reason the adders which we had for address calculation for branch and jump instruction sorry not jump branch instruction and doing PC plus four they were broody slow 
so let us say t plus becomes the bottle neck and this is this will hold up the clock 
so now clock will get dictated by this this time right 
and we we will still follow the same approach or instruction takes four cycle 
lw five cycle 
sw um four 
beq three and j single cycle 
but now notice that the the clock period is larger and it has to be sufficient to accommodate each of this any of this individual activity 
so t plus is dominating and you would notice that now um even these instruction or they are taking four cycles but they are taking longer than what they were taking in the single cycle approach 
of course lw was in any case taking longer than a single cycle 
sw also taking longer than single cycle and beq there is a little bit of saving there is a of course still significant saving in jump instruction but there are few jumps anyway in the whole program okay 
so if you add the cycle per instruction CPI of all this now R has CPI f four has five and so on 
we we know the CPI of individual instruction a class of instruction and we have seen how we can calculate every CPI depending upon the frequency of occurrence of certain kind of instruction ion a program you can find an average 
so um [noise] the total time a program would take good infact here will larger than what it would take in a single cycle case
so if there is an imbalance of this nature then this approach the way we have been implemented um multiple cycle design could be counter productive 
so what can be done such a case we have do something so that there is a balanced okay what we are doing in in clock cycle is a uniformly true um uniformly same 
that means we we identify the activity perform within a clock cycle which is generally balanced okay 
so it should happen that for example here in this clock cycle we are doing little very little and wasting lot of time whereas this cycle here is packed okay that s the unbalanced 
how to balance possibilities are that you can have multiple action in a period okay 
if there are two lets a two action which are taking very little time you can do two in a clock 
earlier we were doing a every thing in a single clock okay 
so the part that kind of idea could still be retained we need not say that one action in one clock cycle although that is the simplest thing but this may not be always very beneficial 
alternatively or in addition in to this you could have multiple periods for an action okay 
for example its not necessary that t plus if it is slow has to be done this action has to be done within a single clock 
you might do everything else in a single clock but may be reserved two clocks for this right 
so you you might still have an overall better performance 
so now once you bring this into picture the number of possibility becomes very large and it s a its not a very straight forward solution 
but the point here is that one could find a suitable clock period so that the dead times or the wastage of time which is due to quantization of time by clock is minimized 
so we will not going in to further details of that 
we will just resume that we we have identified the major actions for each instruction and each can be put in one clock cycle 
so we will follow that approach um keeping at back of mind that there is a problem this is the direction in which we need to look look in to okay (Refer Slide Time 12:02 min)


 
the second issue was improving the resource utilization um 
can we eliminate two adders 
can we manage with a single ALU that s one question 
in general how do we share resources a clock cycles okay you you are using adder in one cycle to do something in another cycle you can do something else in the same adder 
so [noise] the the solution lies in having the results of one operation stored at the end of cycle in some register okay 
you you recall two design of multiplier we discussed two different types of design one was array multiplier where we had cascaded of adders with know storage in between and the the partial some flew through flows through all those adders okay 
so now as as data flows through those um you you can notice that each one is not getting fully utilized each would be active and signals are propagating through that only part of the duration 
on the other hand the sequential multiplier we had we we did something with the adder store the results in register and reuse that adder again okay 
so the key thing is that between two usages you have to store one result okay 
so you do something store the result and then the resource adder in this case is free to do the next operation 
so so same idea we will apply here and um the three key resources we will keep in mind is register file okay 
ALU which will do all arithmetic and logical operation now 
and the two memories will be clubbed into a single memory okay it will store program as well as data (Refer Slide Time 13:19 min)



so lets a take this is starting point um which is the mul[tiple] single cycle design and in this we will see what change is required if we have to um share this resources 
and we we know that we are going to do a multi cycle design so each um major action would be done in a separate clock cycle (Refer Slide Time 15:08 min)



so firstly lets a merge instruction memory and the data memory okay replace both this by a single memory 
so we remove this and replace them with a single block which is doing read as well as write 
and I placed it here in where program memory was kept but what we will do is we will route its inputs and outputs back to a same memory 
so that means the ALU which were supplying the address this will go back and supply an alternative address this was the data input this will go back to this point 
and the data which is being read will actually now come out here okay 
so so these inputs and outputs which were connected to these will now be rerouted and brought back to this particular um block 
so first of all this output of ALU which is the address for memory is connected back 
now we are bringing a conflict here but we know how to resolve that we will introduce a multiplexer here to take care of this 
but that that s a problem we will take a later 
so first of all let lets bring all connections here I have taken care of address input 
next um the the data input which is going here will be now brought to this I have removed the whole connection and it gets connected at this point 
similarly the data which is coming out of memory and was eventually through this multiplexer going to register file will now come from this 
so that is removed and [noise] this is okay that is yeah its its connected here let let me see it again so the output old output of them memory is removed 
and the new one will get connected in to this [noise]
so it comes out of this and gets connected again we are bringing a conflict here we know that the data needs to be brought from ALU or from memory okay (Refer Slide Time 17:30 min)



for load instruction and for arithmetic instruction so this is a conflict point we will dissolved with a multiplexer again 
and what will do is further moment we will remove this multiplexer and reorganize it here later on okay 
so output of ALU is directly connected here for the moment but we know that a multiplexer is required at this point [noise] 
so we have got and data of one extra memory  
now gradually will also collapse the two adders on to this ALU 
so first let me make space let me just shift thing around so that there is a space for carrying out an interconnection just same thing with thing move little apart (Refer Slide Time 19:10 min)



and we will first eliminate this adder and then eliminate that adder and same thing will be done remove this and just route the interconnection on to this 
so we remove that adder it was getting two inputs PC and four 
so the input coming from PC is a brought in to this point of the ALU okay this is a new connection 
and um the second operand four will be [noise] brought to this multiplexer okay 
the output of this adder which was here was eventually going through this multiplexer to be store back into PC okay 
so um that we will have to take from this point now [noise] 
so we remove that connection from top and take a connection from here okay 
now we have still something hanging here this was PC plus four which was the being used for offset addition here 
and generating jump address here 
so this will now come from PC because we are assuming that you do PC plus four in one cycle the the result goes back to PC okay 
so the the input to this is not taken from here we we take the value since it is a multi cycle design PC plus four would be done in a particular cycle 
and that value will be put back in to PC because remember that we are going to put all values into some storage element in some register at the end of the cycle 
so that resource which computed that becomes free okay 
so um these thus input here will come from PC now I simply make that connection there we have taken care of removing the first adder (Refer Slide Time 21:40 min)



now we eliminate the second adder this one will be moved away and it s a inputs again brought to this same ALU okay 
so um we have this output from PC which is already coming to this so so that actually will not bother as that is already there the second operand is coming from the offset eventually and that would be brought to this multiplexer um the output from here will have to go to this multiplexer okay 
yeah this second output second input to the old adder is removed from there and is connected here 
then we simply get rid of the first input because that already taken care of and the output I make some space first for that and then connect the output (Refer Slide Time 23:06 min)



so this is now one one might question actually that sorry that why it is that we um are still have two inputs to this they seem to becomes from same point 
but actually it main belated difficulty for you to see um what will actually happen is that although this this value which is actually suppose to go to PC as this is the PC plus four value which is coming on this line 
it is meant to go to PC immediately but the other thing which is coming here for after offset addition is meant to again go go to PC 
but this is after comparison has been done in this ALU of the two registers 
and this may go may not go 
so what will happen is first this value will get immediately stored in another register here okay 
before we do it before we take further decision of sending else where 
so at the moment time I am keeping these two both which look identical right now 
but one will come immediately from the output of ALU one will come from a register which is following ALU 
so that distinction will show applied to show at this point um you just take it like this that we are simply retaining 
what one was following this kind of design process you might actually [noise] if you are not able to see that you may eliminate one of these okay 
but doesn t matter one if you need one later again separate one then you can bring it back into the picture 
but I I am just following this approach [noise] okay (Refer Slide Time 23:06 min)
now we need to introduce registers at various precious 
so that output of a every resource is stored in register I I simply make some space for that [noise] 
and gradually introduce a registers 
so which are the points where we need to introduce a registers um what we are reading from the memory needs to be kept in a register we are reading two things 
the instruction and we are also reading the data which needs to go to register file eventually 
so two registers will be required here 
we store one will store instruction one will store the data 
then you are reading operands from the register file they will be kept in small individual registers here 
the output of ALU will be kept in another register here so these are other places where we need to put register 
so we place one register here we call it IR or the instruction register one is to be placed here we call it Dr or data register okay 
so both are carrying an information brought from the memory 
then um A register holds the first operand and B register holds the second operand which come out of the register file 
then there is a register which we call RES are the results which comes out of the ALU and here I have made the distinction that PC plus four value is immediately going to the PC from ALU output okay 
so it is it is getting stored but getting stored in this PC 
whereas the other things um what else ALU is doing ALU is doing normal addition subtraction AND OR slt operation 
so that data will get stored in this register before it goes to register file and also the memory address which is being calculated for load store instruction will be resting in this here before it goes to the memory
and also for branch instruction the next instruction address which I calculate by adding offset will sit in this register before I make a choice here whether you take PC plus four or PC plus four plus offset okay 
so that s the purpose this register will serve (Refer Slide Time 26:09 min)



so now um the last thing which remains is to introduce the multiplexers okay 
we will [noise] some at some place we will require fresh multiplexers some where we will simply do a rearrangement okay may be enhance the size or restructure the multiplexer inputs
so we have multiplexer already there which have feeding the program counter okay 
what what I will do I will I will just collapse them into a three input multiplexer three input one output multiplexer which appears simpler and physically just bring it out here 
so introduce a new multiplexer here but I will eliminate the old ones 
this has three inputs one input comes from RES register 
one comes from ALU directly 
and one comes from this um jump address 
those three are brought in here the output of this is going to feed the PC 
so first I switch the output okay then one input comes from register result
one input comes from ALU 
and the third input comes from jump address 
and I have got and rid of the earlier multiplexer so you can see it again 
so this is one connection (Refer Slide Time 29:25 min)



this is second connection (Refer Slide Time 29:28 min) 



this is third (Refer Slide Time 29:32 min)



and this is the fourth (Refer Slide Time 29:35 min) 



so one output and three inputs they are all rearranged [noise] okay 
next we look at the multiplexer which is taking care of the first input of ALU okay 
ALU has two inputs now the first input comes from either A register A or it comes from PC from here (Refer Slide Time 30:11 min)



so simply I need to introduce a multiplexer here and connect these two inputs to that multiplexer 
so this is the two input multiplexer um one input comes from PC an other is already there shown from register A [noise] okay 
then this multiplexer which is feeding second input of the ALU now has four inputs B um B register the constant four this offset which is used for load store instruction for calculating the address okay it is the offset in the instruction with signed extension 
for branch instruction we need to a shift also we need to sign extension forward by a shift 
so these are the two separate values and there are total of four possibilities 
we need to have a bigger multiplexer here now 
okay so place a bigger multiplexer and simply connect this signals properly okay 
so we we would need to see we will need to worry about the control of these multiplexers okay what control value control input is required for different instruction 
so that you analyzed later in the usual manner [noise] okay 
then we move over attention to this area where we have data coming from DR or from RES that needs to be multiplex before we fee din to register file (Refer Slide Time 31:16 min)



so just move that wire at the bottom this one to make some space here placed a multiplexer 
so this is one input from DR (Refer Slide Time 32:14 min)
 

the output is properly connected and this signal coming from result is actually extended brought to this (Refer Slide Time 32:27 min)



so now now this is taken care of finally I need to put a multiplexer at that point [noise] 
so here we have address the two address resources PC for fetching in the instruction and this RES where load store address would be calculated and kept 
this is another inputs so I will it will be simpler to just extend this line here and I I will remove this line which is coming from top same thing I I just put it more neatly from the bottom okay alright 
so now um this is complete design of the data path for multi cycle 
for for basically more effort was there to ensure that resources are properly shared um 
so with for that we need to collapse multiple components on to same thing 
and with the help of multiplexer we are able to feed different inputs to those resource a different times 
and then registers were introduced to break the time interval 
so time interval gets broken in to multiple clock cycles and now the paths which need to be considered for delay analyses or or much shorter path 
so for example you you can take path going from one storage element to other storage element when you are fetching instruction it is it path PC to IR or they are path from IR to A or IR to B okay 
or from [noise] um DR to register file write okay 
and similarly path from A and B through ALU to result register and so on 
so so main sources of delay or these three components memory register file and ALU and if if these three or roughly balanced when this will work very nicely [noise] okay (Refer Slide Time 32:45 min)



um so that is complete design we will look at the control part the next lecture I I will just rearrange this diagram to make redraw this little neater okay 
so we we will go in to details of control signals basically now we have lots of multiplex we have one two three four five six multiplexer are there which required control and multiplexer like this one will require two bits to control them um apart from that we have the usual control requirement for memory register file and ALU 
we also need to now look at these registers in which cycle we load this register and which cycle we do not okay 
so for example if you take lets say add instruction after the first cycle IR gets loaded in the after the second cycle A and B get loaded
after the third cycle RES gets loaded 
and then in the forth cycle this RF writes Rf gets the value written it to it okay 
also we will typically load PC with the new value PC plus four at the end of first cycle itself 
if it is a branch instruction and branch has to be taken that we will we will overwrite that PC plus four value in the PC by a different value 
and similarly in jump we will write something else in that 
so um even PC will require a control okay 
in every cycle we need to determine whether a new value goes in to this or doesn t go because there is no register which is now loading a new value in every cycle 
so this needs to be taken care of and the number of control signal therefore is quite large 
we we have this six multiplexers okay out of which two require two bit input so total of eight control signals we require just for multiplexers 
we have a two for memory one bit for RF 
and three bits for ALU 
so this is another six bits okay and we have one two three four five and six registers so one control signal for each of these register 
so this is the large number of control signals which which are required [noise] 
and also the control would be different in the sense that in every cycle um there is a different set of control okay 
so the control will no longer be a simply combinational box it will be a sequential machine which goes through a set of cycles four cycle five cycle or three cycle depending upon what instruction is and in each cycle it tries to control things differently 
so so the design is a more involved in that sense and that s the reason the single cycle design was considered first being the most simple design (Refer Slide Time 35:24 min)



so in a summary we have moved over from single cycle design to multi cycle design we have compared their performance [noise] okay 
we basically gives you better performance and by resource sharing it um attends to reduce the cost 
but now we must notice that there is there is a trade of okay although we are trying to gain both in time and cost some where they are losses also and we have to be careful that losses do not overshadow of gains 
so for example in performance improvement if the quantization is two crores and there is an imbalance on the values of a time is different individual action 
then on one hand we might gain but on other hand you might lose all the gain 
simply because there is a imbalanced 
similarly for why resource sharing we have eliminated those adders and we have removed we have eliminate one memory [noise] 
but but we have incurred extra cost interms of registers and multiplexers okay 
so you you are sharing but there there is an overhead of sharing 
and we have to be let more careful in our calculation and ensure that the overhead does not overshadow the gain 
so finally just to close we had design with where data path conceptually simple interms of key resources just one ALU one memory and one register file (Refer Slide Time 39:25 min)



but they are register and multiplexer which glue this all together 
so next we will look at the issue of control design for such a data path 
thank you









Transcription: Shobana

Computer Architecture

by

Prof. Anshul Kumar
Department of Computer Science
and Engineering, IIT Delhi

Lecture # 28

Memory Hierarchy 
Basic Idea 

we have been discussing design of processor which is the main component of complete computer system [noise]

our discussion in the recent few lecture have been focused on increasing the performance and what we have been attempting to do is to execute as many instruction as possible in as few cycle as possible 

so all this puts great demand on memory because we need to fetch um in simple pipeline case one instruction per cycle and if we go for ILP then we have to go for multiple instruction to fetch per cycle  

so so that a requirement of instruction fetching which is put on the memory um at the same time you also requirement of data transfer for instruction which access memory like load and store 

so far we have assume that memory is flat that means you have simply an array of words you give an address you can get the word or you can give an address and data you can write the word 

but um with the given memory technology if memory is organized in this particular manner it cannot um meet the demands of performance which is placed on it by the processor

so we need to have a more complex structure which is called memory hierarchy [noise] and that s what we will start discussing today 

(refer slide time: 00:02:25) so um we will quickly look at  what memory technologies are available we don't go into deep um of this issue but um just to notice a point that there is variation in cost and speed and sizes 

and we will look at memory hie hierarchy as in attempt to put these different technologies together in same frame work 


(slide time: 00:02:25)

um this works on the principle of locality that is same thing you can access again and again what you access once gets reused 

we look at impact of this approach on the performance and we will look at a specific case of hierarchy or specific level of hierarchy called cache organization 

um for for a simple cache organization we will see um how um performance parameter called miss rate varies with um some architectural parameter such as block size and we look at some cache policies which are followed in different systems [noise]

(refer slide time: 00:03:34) so um lets look at a typical add which you see today for computer i have pick this up from um an online computer store 

so you you see a typical configuration which is specified here and many of the items you note here actually refer to storage of information 

for example um you you have cache you are talking of L two cache RAM FDD with a floppy disk drive hard disk drive CD 

so all these are technology or media for storing information 

similarly here you again see similar things um of course this part are arid this this this part ge generate doesn t get [] [04:24] what you see is what is um you see processor as just a one component and other thing which are external to that 


(slide time: 00:03:34)

um so so for example um there is typically several mega bytes of memory ok which is called RAM or the main memory 

and and there are backup memory such as FDD or HDD ok um 

cache is faster memory which is typically placed within this within this CPU chip ok 

some some part could be out to the CPU chip also so we will talk of those detail later 

so so you would notice that um whereas one begins by looking at how fast the processor is that is this giga hertz spec 

so you are talking of two point four giga hertz here um one also talks of various size of memory so size of memory is important and how various types of memory have been put together in a configuration is important 

um you you would notice that i have taken two examples whe the the prize differs substantially and what is the reason for the prize difference 

there is of course a better processor of um usi using same giga hertz or same rate of clock but um this is more powerful processor in terms of the way it is internally organized and um also the other issue is that the cache is much larger here ok 

so so what is causing this pi price difference one is better processor ok 

secondly um RAM is same but cache is drastically different ok

and of course there is difference in some of the peripherals like fifteen inch monitor and seventeen inch monitor but um the the cache is equally important as the processor [noise]


(slide time: 00:06:34)

(refer slide time: 00:06:34) so briefly be notice presence of different types of memory technology 

um there is one category which is solid state semiconductor memory ok 

here they are no moving parts involved whereas you have magnetic memory which we find in floppy disk drive and hard disk drive there they they are moving media for optical memory as CD or DVD these are also moving ok

in in these the differences in terms of the way information gets stored here information is in magnetic form ok 

so lets say on a on a on a surface of a storing medium um there region which get magnetized with one polarity or the polarity and that distinguishes one or zero 

similarly in optical memory there region which could be opaque or transparent and that makes a difference between one and zero  

whereas these are purely electronic ok and consequently they are much faster ok 

here here some movement is involved whi which makes things somewhat slower 

whereas in semiconductor memory there is no movement involved um its only that you you give an address and you get the data 

so these are comparatively much faster 

there is also difference in organization in the sense that um these semiconductor memories are random access that is you give an address it takes same time to get a word ok 

so all all are all words are organized symmetrically 

whereas in these memories which have moving parts they there is some sequentiality  

so you you maybe able to get some information faster some information slower because things are organized along tracks and one has to move along the track and also across the track in a sequential manner 

so so these two factors mechanical movement and sequentiality make these memories slower 

but on the other hand the the capacity which you get in um these magnetic or optical memory case order of magnitude higher then the semiconductor memory 

so you you could either say that for same price you can get much much more capacity or you could say that the capacity or the cost of storage per bit or per byte are powered is much lower incase of magnetic and optical memories 

now within within each group there are um differences ok 

for example DVD has much more capacity then a CD um HDD has much more capacity than FDDs also HDD is faster much much much faster than FDD

within the semiconductor memory there we will focus our immediate attention is that you you have registers which are placed as identical part of CPU ok they they are very fast ok 

where whereas um cache memories and main memory which are placed outside are slower comparatively 

so so within um these memories SRAM is faster than DRAM s SRAM in SRAM the information is stored in terms of flip flops ok

so so you imagine cross coupled gates which can be in one of the two stable states zero one and information can be stably stored there 

so so reading and writing are sensing the state of a flip flop or changing the state of a flip flop is much faster than what happened in DRAM where information is stored in terms of charge on a capacitor 

so um it takes much longer to charge and discharge capacitor 

um but again typi typically there is a trade of between speed and cost 

so among these are the technologies which are slower offer you larger capacity and vise versa  


(slide time: 00:10:55)

(refer slide time: 00:10:55) so the idea of hierarchical structure is to use not just one kind memory but several different kinds of memory and try to get the best of various properties 

so you you will place typically the fastest memory close to CPU next slow one after that and the slow zone farthest away 

and typically um this the one which is closest will be smallest in size the cost per bit will be highest and the one which is farthest will be largest in size and cost per bit will be the lowest 

so now wh what you would like is um to get an impression or get an effective um feeling of having the speed of this memory and the cost of cost or size of that memory ok 

if you if you can get these two best of both words then it is ideal 

and in fact the the techniques which have been developed over years in organizing hierarchical memories um tend to give you almost that ok 

um so you you may you will get speed almost same as the fastest memory and you you will enjoy the capacity almost as much as the largest memory 


(slide time: 00:12:26)

(refer slide time: 00:12:26) so um what happens is that you you would have information placed in various memories um 

the processor would start by trying to get it from the fastest memory because that s an attempt hopefully you will get it there ok

if you if you are able to get it there then it is considered a hit if you don't it is consider a miss ok

so one has to organize in such a manner that most of the time it is hit lets say ninety percent of a time or even ninety nine or ninety point nine nine depending upon where we are in the hierarchy um one would target for um very high hit rate or very small miss rate ok that that s the hole that s the key 

so the whole requirement can be captured in one if there is one single parameter which you need to focus your attention most is this 

the probability of a hit or the percentage of the time you get a hit ok which you want to make it as close to one as possible 

so um hopefully thing will work nicely you will get most of the time hit the information will be organized in such a manner that it happens 

but once in a while when you get a miss then you go to the next level and try to get information from their 

so um if you don't same thing will happen at that level 

if if it if it was not available there you go to the next higher level and so on 

so you you are optimistic try to get at the level one if you don't get get at level two if you don't get there go to level three and two and so on 

so the the unit of data which you transfer when you go down the level may also change ok and it is called a block 

so um when when processor is accessing the first level of memory typically it will try to look for a word ok 

le lets say there is a load instruction or you are fetching an instruct instruction 

so you want to basically read a word um but when you don't find it here you go to the next level its not necessary that you will look for just one word there you may bring in more information um keeping the future requirements in mind 

so unit of transfer may differ as you traverse across the levels [noise] 


(slide time: 00:14:36)

(refer slide time: 00:14:36) so this whole thing works on the principle of locality which means that references to memory or localized in sense of time or the temporal sense as well as in in spatial sense are in terms of space 

temporal locality means that if you have refer to some word you likely to refer to it again ok 

um so so if you have access something and kept it higher level in the hierarchy you are like to likely to reuse it ok 

um spatial locality means that if you are accessing a word you are likely to access other word which are in the region around it 

so address features close to that is are likely target in the future  

a a special form of spatial locality is a sequential locality that means you are accessing um neighboring addresses in a very particular way that is you going one after other in a sequence 


(slide time: 00:15:47)

(refer slide time: 00:15:47) so um um le lets look at various levels of hierarchy in a little more analytical way 

suppose you have um several level lets say M levels we call them M one M two and so on Mn the the capacity at various levels in terms of bytes or words or whatever units you have is si 

so s one will be less than s two less than s three and so on 

so as you go down the level the size is increased the unit cost decreases as you go down the level ok 

an access time um missed out one thing ya ok an access time will increase as you go down the level that means thing will become slower and slower ok 

now um in terms of these parameters we can look at the total cost and total effect of time

so total cost is simply are weighted sum of cause at every level 

now there there are lot of simplifications here i simply adding the cost of the storage 

but but there is also additional cost of um controllers or interfaces and so on 

so so just just to give um broad picture i am just looking at the storage cost ok they they are other overhead which lead to be arrayed in general so so this is a total cost 

so now wh what you are given in terms of various technologies the c values ok 

um when you are configuring a system or you are designing a system what you are trying to choose is different values of si s ok 

so um depending upon what choices you make for given c values you you get an overall cost right 

on the other hand um the access time at level i is composed of individual access at various levels 

so tow i is the access time or the time which is spend at level i 

bu but whe when you are actually accessing when you are getting the data at level i you would have gone through previous level earlier 

so so you the overall access time at level i is actually sum of access you have made that level as well as the earlier level 

so ti is sum of tow one tow two up to tow i and these these tow are in increasing order as you go down the hierarchy 

um you expect the hit ratios or hit percentages um also to be increasing as you go down 

so so if you you may have lets say ninety percent chances of finding something in in the first level but chances of finding it in the next level maybe ninety nine next level maybe ninety nine point nine nine so on 

so as you go down since the capacities are larger and larger you are like it to have more and more information there and you are likely to find them um with greater probability 

so um and everything must be present at the last level 

um so so hn the last term will be one 

so now the effective time overall is some combination because sometime you may get information at level one sometime at level two sometime at level three and sometime at level four and so on 

so it it s a combined effect and it will take into account how often we do this how often we do that 

so T effective is some if these ok um the the probability that um ok um you you actually get data at level i when you have gone through previous level unsuccessfully ok 

so mi is a probability that they are misses all through before level i 

so if they are misses before level i and hit at level i then then you are search terminates at level i and you incur time ti ok

so basically you use some ti values with with this as weightage ok

so m one h one t one that means the um m one will be ok m one will be one always and it will m mi will decrease as you go down right the chances of misses are less and less as you go down 

so then this sum is also expressible in this form um you you you are actually making access to ti ok 

the ti component gets into play when when there is a miss at levels before i ok wh whether you are hitting at i or hitting at level higher than i sor sorry level lower than i ok 

um higher larger value of i means i i will use that to indicate lower level 

so first level is the highest level second is next lower and so on 

so so if you are going up to level i or beyond tow i will come into play ok 

so a simpler form is that use sum tow i with weightage mi right 

so either expresses this way or that way 

in fact you can also express mi in terms of h values that is mi means that you have missed that all the level up to i minus one right so one 

if h h one is hit probability one minus h one is miss probability 

so miss at level one miss at level two miss at level three and so on up to misses all the way up to level i minus one 

so product of all theses that you have miss at all the levels before i 

now um what we are trying to say here now basically at two things which need to be considered the total cost and effective time 

so um what what you are choosing um is basically s values 

so so you are given different technologies characterized by c values and tow values and you need to choose s values because s is going to affect the overall cost and s is also going affect the miss probabilities at various levels 

and whereas to choose so that you you get the right combination of cost and time 

so so th this is not very accurate way of analyzing there are many issues which have been actually somewhat overlooked so it s a simplified model 

but what it tries to do is it tries to bring this cost and time together in a in same formulation ok

now so this is [] [23:06] generally about hierarchy of memory 


(slide time: 00:23:11)

(refer slide time: 00:23:11) um we we would talk of um two particular hierarchal interfaces one is the cache which is the first memory or first first few levels of memory before the main memory 

so there is always a main memory which which forms a reference point 

so um before that the level higher in that are called caches 

you may have one level of cache two levels of cache and they are now system with even three levels of cache ok 

and wh what is beyond at low levels beyond the main memory are is called virtual memory 

so so we will talk of um main memory virtual memory interface we will talk of cache and main memory interface so lets start with cache 

the idea is very simple that at any given time the cache le lets imagine only one level of cache in in the initial discussion 

so in the cache you will have some subset of the data and instruction which you have in the main memory and at any given time you may when you need something from memory you first make reference to cache if it is there you pickup otherwise you you get from the main memory 

so for example suppose at some point you refer to a variable Xn or location Xn um and cache contains these in different location 

you find Xn is not there you you bring it in position at at appropriate point an and then if subsequent reference are made to Xn you will find it there 

so they maybe a miss initially and hopefully there will be several hits later on um so so that you can get fast 

if if every data or every instruction is refer to only once and they are no localities then of course you will get the worst of the time you will get the time same as the slower of the memory 

but fortunately that doesn t happen because um if you look at the program most of the time program goes sequentially unless there is a branch 

so there is a sequential locality and their loops bring in temporal locality because same instructions get executed again and again ok

so in fact if if you have a small loop um the whole loop will eventually sit in the cache and you will keep on executing all the iterations in the loop from the cache only also for data you you have data structure such as arrays records were consecutive locations get accessed some computation 

so now question is um what do place where ok 

so you you have memory main memory which is of larger size cache of smaller size 

so so which part you place in the cache and where you place in the cache 

one simple organization is called direct map cache where given memory location there is a fixed place in the cache where it can be kept ok (refer slide time: 00:26:15)


(slide time: 00:26:15)

so so given a memory address from that you can find out its location in the cache 

so for example um here you have a cache of size one two three four five six seven eight words ok 

and you have memory which is four time the size ok that is thirty two words just simple at how example to illustrate the idea 

so we can imagine the main memory also to be divide into um segments of si um or portions or size which are same as cache ok

so so let us say um you look at the first eight words of the memory first quarter of the memory then then you can see a direct correspondence between um locations of memory and locations of cache 

so um you since memory is four time you will have four such quadrilles and you can see that all grey locations of the main memory map to this grey location of the cache and all orange locations map to this orange locations and so on 

so in each part of the first location of each quarter maps to first location and so on 

so so now um that this makes the task very simple given memory address which you are trying to refer to um what you need to do is you need to figure out which location of the cache its likely to be an if at all you go there and check if it exist there and if it is there your job is done otherwise you need to do something else 

so in terms of the hardware structure what you do is from the address of memory ok 


(slide time: 00:28:09)

(refer slide time: 00:28:09) let now i am showing this illustration with thousand twenty four words in the cache ok 

and memory addresses are thirty two bit addresses which means there are four giga bytes or um one giga words of the main memory right 

so so last last two bits are um byte number or byte offset within a word so lets keep that out the remaining thirty word define one of the two rest per thirty memory main memory words [noise] 

the cache here is consisting of one thousand twenty four words which means it can be address by ten bits 

so let us say out of these thirty bits the lower ten bits will take us to the the possible place where um a [noise] word can be found 

wha what we need to know now is whether this is the word we are looking at looking for or not ok 

so we need to carry some identity with each word in the cache right 

so so for example as in the previous case if we go to this one um [noise] you have five bit address is here and three bit address is there 

so out of the five bits of the address the lower three can be used to index and reach one point in the cache 

but the cache will have to tell which of the quarters this has come from ok

the the word sitting at the gray position here could be this one or this one or this one or this one 

so it needs to carry a two bit tag to indicate from which quarter it is come from 

because fondingly in this picture the remaining twenty bits will identify one of the million possible words of main memory which can map to same word in the cache 

so therefore um the each cache word needs to carry a twenty bit tag which will ide identify which one of those million things it is holding at the at the moment right 

um so so as you see here that this is an overhead ok so rather in just store in thirty two bits of data restruction we carry additional twenty bits and i am showing one more bit which which is called we get a valid bit which will tell whether this entry in the cache is filled up at all or not 

because you you would start typically with an empty cache the whole thing is a main memory and only as you bring thing into the cache something becomes valid 

so so you need to distinguish an empty location from one which is filled up something useful 

um now the process can be looked at as follows that out of thirty bits address you look at ten bits use that to index into one of these entries take the remain twenty bits of address and match with the tag here 

if if the if the tags match ok and valid bit is one then you say it is a hit ok

otherwise you you would say if if it is not valid that means it is empty if it is valid but tag is not matching that means there is a valid entry but something else is sitting there and this is not what you want 

so in that case it will be a miss 

so this signal indicates a hit or a miss 

and if it is a hit then here is a data which you can read out write process could be similar 


(slide time: 00:31:48)

(refer slide time: 00:31:48) um the this was very simple case where um the block which are which i mentioned as the unit of data transfer between two levels 

so now we are talking of interface between cache and the main memory do we transfer one word at a time or we transfer more words 

so suppose we look at four word blocks that means every time there is a miss in the cache you are going to get not just one word but four words with the hope that other will also be useful at a rate of point of time because of spatial locality ok locality of space 

so so now same structure i extend horizontally and in each row you can place um not just one word of data but multiple words of data 

and rest of the mechanism is sim sim similar except that now the the field size change 

of course the situation is not quiet same here i am assuming a larger cache here they are um actually there is some inconsistency [noise] ok 

the the cache are much large here but this this number is not correct um 

i am assuming a twelve bit index here which will this should be four zero nine five ok to this part will minus one 

so so now there are four thousand are blocks in the cache each block consisting of four words 

so the total capacity of the cache is sixteen k words and these twelve bits in the usual manner index into one of the rows 

um we we do similarly tag matching the tag size is different now because more bits have been occupied by other fields 

there are two bits now which specify a word within a block and the last two bits will specify a byte within a word ok 

so to to get a word which we are looking for what we need to do is um have an index into this whole array ok pickup one row look at the tag and the valid bits first um do the tag matching and hand it with valid bit to decide hit or a miss

so if it is a hit you can read out four words and um select one out of those four um by using block of set or the word number within a block as a selection signal 

so so this is the data which comes out right 


(slide time: 00:34:32)

(refer slide time: 00:34:32) so what what is the advantage of having a block different from one having a larger block 

as i mentioned that with a larger block you are trying to capture locality of space 

so if if a wo if a word is missed out you are getting from main memory um why not get more ok 

so um with a hope that they will be a near feature references to those words also all in then you don't have to worry about miss at that time    

so one would expect that as block size increases there will be improvement in the performance or the miss rate will go down which indeed happens 

so he here is a series of graphs which show miss rate depicted as a percentage versus block size and um different curves correspond to different total cache size right 

so um you you would notice for obvious reasons as you increase the cache size the miss rate is going down 

for example lets lets go across the curves this um the the top one corresponds to one KB the next one corresponds to eight KB sixteen KB sixty four KB and two fifty six KB 

as you are increasing the cache capacity you are capturing more and more information and more and more likely that you will have a hit and therefore miss rates are lower 

secondly as the block size is increasing you are capturing more spatial locality so um the there is a further decrease in miss rate as you go towards right 

but peculiarly you will find that after some point there is a tendency of um miss rate to increase and it is much more pronounced in smaller caches why do you think that could happen [student : the original values in the cache get replaced by the [inaudible]]

ya you you are saying something close to what you are not word it properly 

what happens is that when cache is small and you try to make larger and larger blocks um the the total number of localities we are capturing are much fewer ok 

so so typically a program would behave um that they will be few locality that that is some region of the program you will spend lot of time and another you will spend lot of time and so they could be many locality in the program as well as in the data 

so um you you will be accessing some data structures more you will spend more time with those and um large larger block size means fewer blocks ok 

so whe when you are bringing a new block you are also replacing something which is already there 

you might hand up in throwing something throwing out something which is useful right um because they are only few smaller number of block which you can accommodate and that that s the reason why beyond certain point it doesn t pay you of to increase the block size ok 

so how far you can go gainfully also depends upon the cache size if the caches are large you can have more block size 

um there there is there is one more impact of block size which doesn t show up here is that the time to process a miss will also possibly increase as the block size increases

so the overall performance will not be we will work work on it in subsequent lecture but overall performance um is not just miss rate 

miss rate is a crucial factor but but you we will derive the expressions for um the over performance or what impact um cache misses have on the CPI ok

we we have earlier seen that CPI is is a key indicator cycles per instruction 

so effectively we will see that processor spends more cycles in executing a given number of instruction and that will be influence by miss rate and also the time it take to process a miss 

so with large block that also becomes worse and we need to take into account this whole picture 

(refer slide time: 00:39:00) so let me throw some light on what happens when a miss occurs 

we we are trying to transfer now multiple words between memory and cache ok how it is done 

one approach is shown here on the left side that um you have CPU you have cache you have memory and you can have buses of one word width connecting CPU and cache and CPU and and cache and memory 

so um now CPU cache connection is fine that you are looking at one word at a time but when you want to transfer a multiple word block between memory and cache then each word has to come sequentially over this bus ok

so one one may tend to think of um this kind of organization that between cache and CPU there is a path which is one word wide but between memory and cache we can have wider path let us say um four word wide ok  

(slide time: 00:39:00)

so so which what it means that um memory gets organized in such a manner that um when when you give an address you get four word simultaneously ok 

um so there is of course cost associated with having a path like this lets say your word is thirty two bits then four words would mean you are transferring one twenty eight bit at a time you have one twenty eight bit bus which will make things expensive 

but definitely it will give you better performance um that is in in in one bus cycle not necessarily CPU cycle one cycle of the bus you can get whole block transferred 

um there there is another organization which tries to get again the performance of this at the cost of this organization ok 

so so here we get good performance but at high cost so can can we get reasonable performance at at a reasonable cost that s a issue 

what we do is the organized memory as banks ok 

so ra rather than making it a wide memory connected through a wide bus we we chopped it into lets say four parts called each as a bank and interleave the addresses  

that is keep address zero here in bank zero address one in bank one address two in bank two address three in bank three four here five here six here seven here and so on 

so addresses are interleaved in this particular manner and now each block you are looking for each four word block is spread across four memory banks 

um so when an address is sent the address can be fanned out to all four all four will respond with the one word each which they have to contribute to the block 

but we keep the bus narrow and keep the cost lower so that the the words gets transferred um one by one ok 

so so memory memories have some access time after you give the address that take sometime to get the word 

so so that happens in parallel for all the blocks or all the four words of the block ok 

if you have narrower but faster bus then these words once you have got them from memory can be quickly transferred to the to the cache ok 

so um you you will get more or less same time as you get here of course will be additional transfer time ok 

so so its not quite as faster that but closer to this than that ok and in terms of cost it maybe closer to this than that so so that that s a reasonable arrangement 


(slide time: 00:42:39)

(refer slide time: 00:42:39) now um the organization of cache which we have seen is very rigid in the sense that um a given word of memory or given block of main memory can be placed at a fixed place in the cache 

so that also cause this problem because um every time you you bring you may possibly replace and and if you happen to be if a program happens to be in a state that it is quickly referring to two or more location which are mapping to same location in the cache 

then then you you are doing very badly you know you um every time you bring one you are replacing something useful 

so you need an arrangement where there is some flexibility of placing things wherever you want 

so um so so look at the picture in the left where i am showing a direct map cache and um so um now a a given block can be placed at a given location and you have to actually um relay on tag matching to see whether the right thing is sitting there or not 

on the other hand suppose you want to have full flexibility ok then then you will take a word and place it anyway in the cache 

but the consequence of that is um that you need when when you are looking for this word with which you are placed you don't know where you need to look at all the possible locations and you can't do that search sequentially 

you you need to do you need to look at all the locations where the given block can be found in parallel 

so so it has to be done using a memory which is called associative memory so cache has to be organized as an associative memory where you will ba basically supply the tag value and see if it matches anywhere or not

what you do in direct map caches that you go to a particular place look at the tag and see there is a match ok you are not looking around anywhere else  

in fully associated match um in fully associative um cache you will take the tag and try to see all the places and see if it matches anywhere if it matches somewhere then there is a hit if it doesn t match anywhere there is a miss 

such memory can turn out to be very expensive because obviously with every word or every block there has to be hardware which will match the tags ok so those comparators must be there 

again there is a compromise um which tries to form sets out of the whole cache area and it gives you limited flexibility what it say is that given word in the memory it can be placed anywhere within a set ok so so it give a limited fle flexibility  

if if you make the set size is one then it this reduces to direct map cache 

if on the other hand you make the whole thing as a single set then you you make it an fully associative memory 

so set associative is somewhere a compromise between these two cases 

in this picture we are showing the whole cache divide into four sets 

what it means that given data can be anywhere in the cache anywhere in a particular sets 

so suppose this is a set which you um have placed in then you need to look at two places whi which means that the tag matching it will be done in a limited context in these two places and if one of then has it then you have a hit otherwise there is a miss 

um ok i think time is running out and i will skip the cache policies and stop here 

so so let me summarize what we have discussed today 


(slide time: 00:46:52)

(refer slide time: 00:46:52) um we looked at memory hierarchy as i means to get best of various types of memory technologies get speed of one and the cost of other 

and this works on the principle of locality that means same data or instruction gets refer to again in time or gets refer to um within locality of addresses 

we have talked of direct mapped cache as a simple simplest cache organization and seen briefly the possibility of associative cache 

we also looked at miss rate variation um with block size we haven t um yet looked at the policies and the types of cache we will do that in the next lecture 

thank you 




Transcription: Shobana

Computer Architecture

by

Prof. Anshul Kumar
Department of Computer Science
and Engineering, IIT Delhi

Lecture # 29

Memory Hierarchy: 
Cache Organization 

in the last lecture we discuss the basic principles of memory hierarchy 

and we notice that the underlying idea is that this locality of reference and you can bring data or instructions from lower level memory to higher level memory and it used it several times 

so thereby you gain in terms of the speed of the high level memory and the effectives of getting capacity of the lower level memory 


(slide time: 00:01:30)

today we will specifically focus our attention on cache memory (refer slide time: 00:01:30) which is um the closest to the processor and lies in between the main memory and the processor [noise]

so i briefly mentioned about the mapping alternatives we will elaborate further on those and then we will start looking at the performance issue (refer slide time: 00:01:44)


(slide time: 00:01:44)

so um how does the processor performance or the time to execute a program changes in light of cache accesses in light of memory hierarchy particularly cache and the main memory 

we will see the influence of choosing different memory organization on the miss penalty is the addition time analyze to spend when a miss occurs 

and we will look at different alternative which exist um at various stages read stage load stage and so on 

um so so the these different alternatives have again different cost and performance implications [noise]

(refer slide time: 00:02:32) so we are talking of um typically three levels of memory in many cases they are more 

so you have cache which is closer to CPU main memory and so main memory is the reference point 

so there is a faster memory closer to CPU called cache and a slower memory but giving large capacity which is beyond the main memory 

so um typically um you have a vast variation in terms of there access times 


(slide time: 00:02:32)

so for example the CPU registers with kind of technology require only a fraction of nanosecond to access ok 

so of the order of lets say point one nanosecond these are figure which change every year 

so we we are looking at these has more have a ball path figure and i am not talking of exact values 

cache which is made of SRAM or the static RAM is of the order of one nanosecond now this could be less or more particularly if cache gets integrate with processor which it does in most cases the value could be lower 

but there are um cache memories of the chip also then it could be even little higher [noise] 

the main memory which is basically DRAM or dynamic RAM technology is typically an order of magnitude slower as compared to cache  

and um the backup memory or the secondary memory which is build around hard disc drive this has much slower 

you can see there are several order of five or six order of magnitude slower but here they are two different kind of timing which are involved 

one is time two reach a point where your data exist ok you are trying to access some particular information on a disk to reach that point on the disk because it s a moving um med medium 

it may take of the order of a millisecond or a couple of milliseconds 

but once you reset point the transfer it takes place faster and the rate of transfer could be um of that order at some ten mega bytes per second on that order 

so um any decision you take about organizing these levels one has to gain mind that kind of order of magnitude of difference between various levels of technologies which exists [noise] 

so we um the the first question we came across was how to map the addresses of the main memory on the cache ok 

so given data or instruction which reside in some particular address in the main memory where should you place it in the cache 

and the simplest solution was to um take the memory address modulo the cache size and you you get the location where it gets position in the cache so there  is a fixed position 
(refer slide time: 00:05:31)


(refer slide time: 00:05:31)

the other possibility was that allow anything to anything to replaced anywhere ok and the there is a cost implication of doing so although it gives you maximum fl flexibility 

so typically most often what you have is something which is in between the two which is called set associative

so given any address you have um range of locations in the cache where it can possibly exist 

so so what it means that a a set of locations in in the memory are set of address in the main memory we will compete for a small pool but it is not they are not mapping to a single one 

so um let us say if sixteen locations in main memory compete for four location cache still you have little flexibility and at least four of them can reside at anytime 

so um as as you go from direct map to fully associative you get increased flexibility 

the degree of set degree of associativity in set associative case gives um the size of each set ok 

mo more the degree typically you talk of two four eight sixteen but generally not more than that 

um mo more is the degree of associativity more is the flexibility and eventually it shows up in performance [noise] 


(slide time: 00:07:04)

(refer slide time: 00:07:04) so um i have um redrawn the diagrams of hardware structure for these and i have try to um have organizing shown for cache access with direct access mechanism um with one block size or larger block size and associative access

so so lets look at it one by one um in this all these have kept the cache size as same and you could see that um the field size is in the address will vary accordingly 

so now i am assuming four k words cache size right and main memory address we assuming is a thirty two bit byte address 

so so you have four gigabytes and here you have four k word which means sixteen k bytes we are talking of sixteen k byte cache memory 

and in direct access case this gets address by a twelve bit field here ok twelve bit field will select one word and you can actually get thirty two words of data from here 

so the the access will require that you index into one row here look at the tag match the tag with the tag field of this ok

so so different main memory addresses which can map to this cache address this cache location would differ in terms of tag ok 

and the word which is sitting here will be identified by the tag value it is holding here 

so so you match the tag and if tag matches then it is a hit if tag doesn t match it is a miss 

so if hit occur you can read this otherwise you ignore this and do something else 

there is an additional bit which we call as valid bit which indicate whether something has been placed here or not 

because initially you may have an empty cache and as lets say as the program begins initially you get lot of misses and the cache will get filled up after that after get it gets filled up it will be a matter of replacement you get new words they replace the old word and so on 

so you you are not only matching the tag you only lo you also looking at the valid bit and if both conditions are met only then you consider it as a hit 

now lets um extend the size of the block ok (slide time: 00:09:40)

in in this diagram effectively each block is a unit of transfer each sorry each word is a unit of transfer each word is also an accessible or addressable unit here 

but now we talk of um block which is lets say four words in this case or sixteen bytes 

so the same capacity of cache would involve now [noise] um one k blocks ok because each block is um four words 


(slide time: 00:09:40)

and you can imagine these four words of a block to be laid out horizontally logically they are forming part of one row and they carry a single tag ok 

so the the mapping here is in terms of one block at a time and as misses occur as you need data or instructions you will typically get one block from the memory with the hope that you are ca capturing some spatial locality 

so if you are referring to this word it is likely that soon enough you will refer to this word and then this word 

so the as we would see later where it in terms of transfer of data between memory and cache it is more economic of or better in performance to um move some some set of words not a single word ok

it its advantages to transfer blocks of data 

so once you has a miss you might do little more work and reap the advantages later on [noise] 

so in this case the index field in the address reduces to ten bits ok and we will still have still have same eighteen bit tags right which need to be matched 

so um given an address we will find either its entire neighborhood that entire block there or entire block absent 

so two bits in the address could be use to select one word out of this four words which you pickup from the cache 

actually another name for the block is line cache you also talk of cache line which means same thing as block

and why it is called a line is because logically you imagine this as one row [noise] ok 


(slide time: 00:12:44)

(refer slide time: 00:12:44) now lets look at um set associative organization something wrong with the diagram some lines are not showing up [noise] um now [noise] ok

so we are considering in particular four way set associative organization that means each block will have four possibilities it can appear anywhere within a set 

so each horizontal line here forms what you called as a set and they are now two fifty six sets 

so again factor one by four um so each sets get indexed by eight bit ok 

um and um you would notice that to compensate for that the tag size has become larger right 

so it it is identity of um um a a particular block in the set which is now twenty bits because the the the index has a reduce so two two bits go to the the tag 

um the the reason for this another way of looking at this is at now they are actually um more words which can actually get placed here ok 

four times as many words which can get placed here as compared to the previous um situations 

so i am assuming the the size have shrunk here now um this is one block which means four words or one twenty eight bits and the whole thing can be thought of as organization with the four four blocks the four quadrants here 

so given an index you are selecting um you are accessing this entire row corresponding to a set and within each a tag matching will be done in parallel ok 

so you are you are reading out tags matching with this tag field of the address and match may occur anywhere ok any of these matches and therefore we have an OR gate ok 

so they are also valid bit which is being looked at 

so if any of these show a match there is a hit if none of them shows a match it is a miss
ok 

now um depending upon where a match occurs we we would need to pickup a word ok 

but before that what we are doing is out of the block we are picking up the righ the required word ok 

these there is a multiplexer with a with four words coming in and one word going out each of these and these multiplexers are being controlled by um this two bit field which we consider as a block offset

so these multiplexers smaller ones are actually same as what i showed here 

and then there is another round of selection down here so these match signals will enable one of these and that could be finally taken as a data ok 

so you you would notice that there is replication of this matching hardware which is a extra thing they are um more bits which you are spending on tags 

so these are the overheads of increasing the degree of associativity and the advantages as i mentioned earlier that you are likely to reduce the miss rate ok 

um the the reason being that if degree of associativity is lower there are more conflicts and therefore um some of the blocks which you may like to use later on may get thrown out 

any any question about these three organizations [student: noise] um three [student: noise] oh oh oh that that s a type of graphic error 

actually all are ya there is a mistake all are twenty bit this is carried from some other diagram 

ya please correct this these are all twenty bit tags uniformly 

any other question [student: noise] um well depend now we assuming here that um you are accessing one word although it is i am showing it as a byte address memory but out of this you you get one word out or you send one word in 

ac actually i am showing there read operation right operation would be similar 

so um the unit of transfer between this memory system and the processor is one word ok 

um if if the requirement is of a byte or a or a half word then out of that you will pickup and if if there are miss alignment again multiple access are get made 

so um at the moment i am talking of reading and writing a word with this memory does that answer your question any any other question ok 

so so basically just to summarize we have memory main memory address divide into effectively three parts there is one part which we call as tags (refer slide time: 00:18:01)


(slide time: 00:18:01)

one part is set index and other part is some displacement within a block ok 

so the tag gets compared to the tag stored in the cache 

set index is used to select a set and this displacement which could again be two parts of word within a block and then byte within a word ok over i have shown it has a single field 

this selects an addressable unit ok whatever is an addressable unit it could be byte or it could be word it is selected with the help of these bits ok 

now um lets again look at the performance issue we we in general try to characterize a performance of a hierarchy n level hierarchy in terms of the time one spend at each level and also the total cost in terms of capacity or the size at each level (refer slide time: 00:18:49)


(slide time: 00:18:49)

so it is a size which we said will influence the the cost as well as the performance and therefore by playing around with the the capacity or the sizes you could achieve a suitable combination of effective time and the total cost 

now lets simplify this and visualize this in context of two levels 

(refer slide time: 00:19:29) so suppose you are talking of now a simple case two levels m one and m two m one is cache m two is main memory 

um the access time t one that means if if you were to get data from cache only if there was a hit then as we formulated earlier it is just tow one time the encounter 

but if we um we get the data at next level the time t two is tow one plus tow two because we would have made an attempt at cache level failed and then made an access at the main memory level 


(slide time: 00:19:29)

so um the the hit ratios or hit probabilities are h one and h two where h two one  we we are assuming that there is no further level beyond main memory 

so whatever you looking for has to be ultimately found at the last level which is main memory in this case so h two is one 

the miss before level one m one is always one ok because there is no zero level so um before that its miss we have to start from level one only and m two is simply one minus h one right 

now in term of these we can express T effective as um this was the formula we the sigma mi hi ti 

so m one h one t one plus m two h two t two 

um now putting these values we get this as h one t one ok plus one minus h h one t two 

um if you replace t one t two by tow values what you will get is tow one plus one minus h one tow two so either way you can lookup on this 

so intuitively you could see this very straight forward that if there is a hit at the cache the time is spend is t one if there is a miss the time is spend is t two or one could have this way that tow one is a time you in any case spend whether there is a hit or miss 

the addition time is spend if there is a miss and that the additional time is tow two ok 

so so in more common term um this is often written as average memory access time is the hit time the time is spend if there is a hit plus miss rate which is one minus h one into miss penalty that is tow two ok (refer slide time: 00:21:41)


(slide time: 00:21:41)

so this is the term penalty is that this is the punishment you get if you are not able to find things in the cache 

so now our ultimate interest is in program execution time that given a program with which execute certain number of instructions how much time it takes and that that s what we had mentioned as the ultimate measure of performance 

so ins so this will be instruction count into cycle time into typically we talk of only CPI but now we say CPI plus the extra cycles which are called memory stall cycles 

because what happens that when when there is a miss you hold back the CPU you similar to the data hazard you introduce stalls ok 

so um you do noting getting for a few cycles still data comes from main memory and is supplied 

so um the the cycle you spend in actually executing instruction normally if there is no miss plus the additional cycle you spend when there is a miss ok [noise] 

um and this factor memory stalls per instruction is a miss rate multiplied by miss penalty multiplied by memory accesses per instruction 

so now memory access per instruction would dep depend upon the kind of instruction you have and how often various instruction are executed 

so in our architecture we have assumed that um each instruction is one word instruction 

so one access is made to fetch an instruction to fetch that instruction and load and store are instruction which make another access to memory ok 

so um now you you can so so memory access is are either one or two in different instruction 

and with appropriate weightage if you find the average you will get one point something as um average number of memory access per instruction 

so miss rate and miss penalty are same thing right um 

so um when miss occurs this this many often the miss occurs every time you um incur so many extra cycles and the this miss rate has to be seen in conjunction with how many times you make memory access per instruction 

so so this product gives you memory stalls or the extra cycle you spend per instruction because of misses 

any question about this is that clear ok 

so now um in in view of this if you were to improve performance from cache organization point of view what is it you could do 

um so basically you could improve on any of these factors ok 

so actually in the second one hit time is sort of subsumed in this right 

um ya ya hit hit time is actually um subsumed in this 

so if there is no no miss um we have assumed for example that you can access memory in one cycle so so that is actually taken into account while counting the cycles of an instruction ok

so um overall performance can be improved if you reduce one or more of these things or if im if you do a change which reduces some but increases something else but um then you have to ensure that advantage is more than the loss 

(refer slide time: 00:25:44) so there are different um techniques which will either try to reduce miss penalty or reduce miss rate or reduce this product in the process there maybe increasing one but on the whole reduce a product or they could also try to reduce hit time


(slide time: 00:25:44)

so we have to keep this at back of our mind that when you are comparing two a more alternative what does it do 

so the miss penalty is dependant upon how you transfer data between memory and cache and we looked at three possibilities 

(refer slide time: 00:26:17) that is you have um one word wide memory and one word wide bus other extreme was this one that you have memory as wide as the block ok and the bus is also equally wide 

so this is the fastest possibility um but very expensive therefore there is alternative arrangement which is interleaved memory 

so well it is not necessary that the width of the memory here or the degree of interleaving should be same as the block size suppose block size is sixteen words even if you have four way interleaving or four word wide memory it will still give us some advantage 

um so so larger the width here or larger the degree of interleaving better it would be 

so lets quantify this to um some extend for some cases (refer slide time: 00:27:09)

as an example suppose you have one clock cycle which is required um to send the address to the memory ok 



(slide time: 00:26:17)


(slide time: 00:27:09)

fifteen cycles are required for RAM to have access internal that means after address um RAM take sometime before it gives data so lets say that is fifteen cycles 

now these are CPU cycles ok one one memory transaction maybe different or the bus cycles maybe different 

but here we assuming that bus also takes one cycle for sending data ok 

though the two may not necessarily match in this example we are taking figures like this also lets assume that block size is four words 

so in the first organization where you had one word wide memory and one word wide bus um you will send address once and then go through four transactions [noise] four cycles every time spending fifteen cycles for access one cycle for transferring data 

so total number of cycles or when this penalty in this terms of number of CPU cycle will be sixty five ok 

in case b this four factor reduces to one in one short you will access four words transfer all the four words and therefore you spend only seventeen cycles 

in the last case um you will make only one access to the memory all four module or all four banks of memory um would be ready with their words within fifteen cycles 

but then you will take four cycle in the bus to send them to the cache 

so one plus fifteen plus four which gives you twenty 

now we can also imagine intermediate situation 

suppose in case b the memory and bus were two words wide ok then what we will have here is a factor of two ok 

we get two words at a time and we do it twice 

so um this will be thirty three is somewhere in between the two 

and if degree of interleaving was two here then what you would do is um this fifteen plus in fifteen cycle you will get two words out spend two cycles in sending though 

so fifteen plus two seventeen and that seventeen has to be repeated twice 

so seventeen in into two thirty four and one so it will be thirty five ok 

one thing i would like you to notice is that what you are achieving here is closer to this rather than that ok and in in the intermediate case also this is thirty five and that is thirty three 

so so difference is not so much but the cost is much lower 

(refer slide time: 00:29:58) here i assume that you physically have um two or four or more different memory banks which where addresses are interlinked 


(slide time: 00:29:58)

um but in DRAM in dynamic RAM what happens is that internally it it it has a structure which gives you similar effect ok 

so what is that s called page mode in a dynamic RAM 

um in in a DRAM the storage cells are organized actually as a two dimensional structure in row and columns 

and it it is organizes that you the address has to part row row address and column address 

so you give row address an entire row is accessed and kept in a buffer from that buffer you can pick out things faster ok 

so so time to access one word out of the row which has been kept access and kept in a buffer is much smaller 

so effectively what you do is um you you can think of this as a page which we have accessed and if you are referring to a block of data with a which lies within this page you can make faster access 

so first word takes longer but subsequent words come out faster 

so reading multiple bit from a row can be done very fast it can either be done sequentially that is you you um gave you don't give even column address repeatedly you give on column address and then um consecutive bits can be read out 

or you you can also randomly read bits out of this page or the row by giving only column address row address is given only once ok


(slide time: 00:31:45)

(refer slide time: 00:31:45) now um this is what we have discuss is the um basic operation within this there are lots of variation which are possible and they have their own implications on um performance 

so we will talk of va variety of va varieties of ways in which you can do read that means initiate data of initiate data transfer between memory and um cache 

load means how the data gets filled up in the cache 

fetch means when you initiate a transfer 

replacement means when a new block comes how do you choose which block to go away and what happens when there is a write hit or a write miss 

so so that we have not dealt with and there are possibilities there also

so lets look at each of these one by one [noise] (refer slide time: 00:32:46) 

um in reading one variation is that when when you are making an access to the cache ok at the same time you might initiate an access to main memory in anticipation right 

um so so if cache um shows you a hit then you can abort that if if it shows a miss then you can continue 




(slide time: 00:32:46)

so you you would gain initial um one or two cycle which are required for cache hit so that is called um sequential or concurrent read 

secondly when when you have um the data brought to the cache one approach could be at first you fill up entire block in the cache and then out of that block you give one word to the CPU for which it had altered ok 

so CPU gets um stalled for one word suppose you are doing load word but because of miss you are getting entire word entire block from the main memory 

so um if these two are sequentialized that is first you fill up the um cache and then transfer data to um processor then it takes longer 

what you can do is that as the required word out of the block is getting fill into the cache you can also transfer that to the processor 

so there is kind of forwarding path can be created so that the data state comes from memory to processor 

so so that is with or without forwarding [student: noise] 

um it it could be typically one cycle two cycle an ideal would be one cycle read one cycle write 

but it it could be more multiple cycles and then again thing vary whether when you have multiple level of cache 

so typically first level of cache will like to have synchronize with the processor um but subsequent levels may not necessarily be ok 

um now the question is also of how do you load different words which form a block into the cache 

suppose you are talking of a four word block right zero one two three (refer slide time: 00:35:00) um then simple more simple taught would be bring these into the cache in that order [noise] ok 


(slide time: 00:35:00)

but one might say that if if the miss occurred for a word number one within that block then why not start with this ok 

so particularly when you are forwarding the data to CPU it maybe um advantageous to fill up one two three in that order ok 

um you you might just leave the words before it unfilled and the the the valid bits would have to be now one per word and not one per block ok 

so you can keep those words invalid and these are valid um if if you require those then you can fill up those or alternatively you could um start in a round robin fashion start with one go to two go to three and then fill up come back and fill up zero 

so in a wrap around fashion you can load all of them but start at the one which you need most immediately ok


(slide time: 00:36:12)

(refer slide time: 00:36:12) um fetching means the when do you start the transfer 

there typically what you do is called demand fetching that only when you encounter a miss then you fetch the data ok so that s demand fetching 

but you can again anticipate things and do it before time ahead of time you can fetch and this could be this prefetch which means getting the information ahead of time in anticipation 

this could be initiated by the hardware [noise] or by software so there could be hardware prefetching or software prefetching 

um one simple mechanism used for hardware prefetch is that when you are getting one block get another block just the once which is following it hopping that [noise] you you are going to have sequential references 

or it could be software driven where a programmer maybe able to um guess more clearly what is required ahead of time ok 

so you one has to have special instruction which will create which will um actually cause a miss and artificial miss and then force a hardware to get a block ahead of time [noise] 

so now um the well open question here how much ahead can you initiate a prefetch by software 

if you if you do it too much ahead you might do super flows work because it maybe that you are thinking that in in your program flow you will go certain way but if you are doing too much ahead maybe there is a branch point in between and you may go else where and you may not use what you anticipated 

so they they could be wasteful transfer and you you may basically load the memory memory processor bus 

our question is how often you do it do you do it always or do you do it sometime ag again it is a matter of trade off and judgment [noise] 

replacement issue comes um when a new block has to be brought and an old one has to be thrown off ok 

in a direct map cache um there is no choice ok a new block come it has to go to a fixed place so it is trivial 

but when there is some degree of associativity you need to find out which suppose it is a four way associative then which of the four you need to replace ok 

so that that s decided by what is called replacement policy (refer slide time: 00:38:58) 


(slide time: 00:38:58)

and most commonly used policy is LRU or least recently used so you need to keep track of which of the four blocks was most recently used which was least recently used and the one which is least recently used [noise] is the one which is replaced 

alternative strategies are least frequently used instead of counting when was it used last you you keep track of how often it was used from the time it was brought 

or you could use a simply FIFO approach one which is brought in earliest get soon thrown out or you may do it randomly of course which is um um that means you you don't think just arbitrarily replace something 

um all these are some kind of heuristics ok what has been found in fact is LRU is um more suitable ok 


(slide time: 00:40:00)

now this is very important (refer slide time: 00:40:00) and this has very profound influence on the performance 

the the two kinds of write policy which are followed ok 

we we have so far focus our discussion more in terms of read that is you are trying to get data 

what happens when you are trying to write 

um so you can you can follow two approaches one is called write back other is called write through 

in write back what you do is you you keep on writing in the cache ok and um in write through you directly write in the main memory ok 

so now um first lets assume that there is write hit i mean these choice is come when you have write hit or a write miss in both cases first lets examine write hit case 

so write hit means that the the address where you want to write that block is present in the cache ok 

in a write back case you will update that you will write into cache you will not write into main memory and as a as a consequence um main memory is not up to date ok 

when do you up up to date when do you update the main memory when this block which you had modified is about to be thrown away ok before this gets thrown away you must make sure that main memory is updated 

so so um to do it judicially what you need to do is with each block you need to maintain a bit which is called a clean bit or a dirty bit ok 

so um you may get some block only for reading and you may keep it clean but moment you write you call it dirty and um when at the time of replacement if the block which is being replaces dirty then you update the main memory otherwise you don't have to take that trouble 

in write through what you will do is you will actually write in both ok you will write in cache in parallel you will also initiate write in main memory 

um incase of write miss um write back arrangement would require that first you get the block from main memory into the cache serve the miss first ok and then write in cache only 

whereas in write through you have a choice of getting the block or not getting the block ok um and tho those two difference are called write through with write allocate and write thorough without write allocate 

that means now imagine that there is a write miss ok which means that the block where you are trying to write is not there in the cache 

so one possibility is that first you get the data from the memory into the cache and since it is write through you write there in the cache as well as in the main memory ok 

the other possibility is that if the data is not there in the cache it doesn t matter you simply go and write in the main memory ok 

now the the problem with this is that you will save the time although you will save the time of getting data from the cache but you are likely to encounter more misses ok 

where whereas incase of write back um you will definitely bring a block and then you can simply keep on writing in cache 

because writing into mein main memory would be more time consuming [noise] 

in any case what is typically done is that you um do not write into main memory directly actually you write into buffers 

so if you are following write back policy the buffer will um accommodate one or more blocks because you are writing one block at a time 

whereas in write through you are writing one word  at a time ok and there therefore the buffer would allocate buffer would accommodate a couple of words 

so what CPU does is that it will um write either a block or the word as the cases into the buffer which may typically be possible within a single cycle 

and then data could get transferred from this buffer to the main memory in its own due course 

um the this this could cause problem for example suppose you are writing something which is still sitting in the buffer and very soon you required to read it ok 

so now data is sitting in the buffer it doesn t reach main memory and you want to read so from where do you read 

you have to have mechanism which will either let you where till its return till the buffer gets cleared ok or it should be possible to look at the buffer figure out if the data you wanted to read is waiting somewhere in the buffer and read from the buffer itself 

so those complication arise moment you think of any such architectural improvement which tries to improve the performance you have to often look at these complication or side effect which arise and have to make sure that the meaning of the program does not go wrong ok 

the the process should do the job correctly so so all those checks have to be made [student: noise] 

ya wri writing into main memory means that wh what was there in the main memory will get replaced by something new 

i what was that you asking ya [student: noise] 

um ya actually um well ok if you if you think why would you write in memory i mean whatever you have attempted to write in the memory lets say that is getting written in the cache 

but if there was no subsequent use of that it was useless ok you you would read that and print or do something 

so so the last thing has to be um reading of what you have written ok 

if if there is a if the last um lets say last statement is an assignment to some variable ok and that variable has not bean read subsequently then it is does doesn t matter whether you update this in the cache update this in the mein main memory or not 

so um updation of main memory should actually be forced by um reads eventually 

if if you have read and taken action then it doesn t matter it whether goes to main memory or not you are terminating the program anyway [student: noise] ya ya [student: noise] ya [student: noise] um ok

see the the reason for transferring blocks is that it is um more efficient ok so your question is that suppose in one block only one word was modified yes what you are saying is right 

um technically it is possible what you can this dirty flag has to be then with every word and not with the block 

so if you are able to if you are of if you are willing to maintain one bit with every word which is indicating whether it is dirty or not then you can take the trouble of writing only those many words 

but the experience with locality says that um typically you would have dirtied many words it will be more often the case that um in a block it is not it is less likely that only one word is dirty you would have dirtied many more words 

so statistically it it toss out that it will be better to maintain the set block level only ok although there could be a situation where this over done any other question ok 

um finally let me talk of couple of variations in the overall cache organization as such ok 

so so you could um think of um (refer slide time: 00:48:39) either a cache being there just for instructions ok or a cache being there for just the data right or separate cache for instruction and data which is called basically split cache 

unified cache means that the cache is for instructions as well as data right 

um so the the pros and cons of split and unified cache are not very difficult to see 

um split cache allows specializing of each part ok because data may have some behavior cache may have some behaviors or some policy may suit instruction cache different set of policy may suit data cache and you can choose these two separately um if if you have to separate caches in unified you have to have a common policy

split cache also noted here but it also gives you parallelism 

if you have unified cache then they could be in a in a pipelined implementation they would be the source cache ok


(slide time: 00:48:39)

two separate caches means that you can be simultaneously accessing data and instructions 

on the other hand a unified cache will allow best use of the capacity ok 

suppose you you can afford lets say sixteen k bytes on the whole um if you where to split you you may have to make a decision eight byte eight kilo bytes for this eight kilo bytes for that 

and at at some point of time in the program execution your requirement maybe more for data less for instruction at some of the point more requirement for um one and the other ok 

so um if if you are not splitting then you are using entire sixteen kilo bytes and k instruction data will share these as and when to the extend necessary 

so unified cache utilize a capacity better but in incase of split you you may find at sometime you are short of um cache words and instruction sometime you are short in data sometime you are sur surplus in one surplus other ok 

so that that s variation from one point of view 

typically in multi level caches the first level is invariably um split cache ok and second level is most often or unified cache 

then um you can talk of on-chip cache or off-chip cache ok 

cache which is integrate with the processor on the same chip um one once you integrate then you are constraint by the chip area of the processor and um the they maybe constraint on the size but the advantages of speed 

so on-chip cache is typically fast but small off-chip cache can be larger but relatively slower 

and um um in many system you often have multi level cache so you can have one level two level three level and more levels you have more expensive it becomes but it gives you um since they are the order of magnitude difference 

so so you you have you can position multiple caches which are differing slightly in the performance and on the whole you are filling up the spectrum nicely ok 


(slide time: 00:52:09)

so i will close with the summary (refer slide time: 00:52:09) 

so um we we began with looking at the structure of how we map the addresses from main memory to cache we looked at direct mapping associative mapping and something in between which is set associative mapping 

we looked at the expression for performance both for memory access time on the average and the total number of cycles or total time spend for executing a program 

we look at the relationship on the miss penalty depending upon the DRAM organization and we also notice that um page more in DRAM s serves similar purpose as interleaving 

looked at some variety of policies for reading loading fetching replacement and writing and um looked at different possibilities from the point of view of overall organization 

thank you [noise]




Transcription: Shobana

Computer Architecture

by

Prof. Anshul Kumar
Department of Computer Science
and Engineering, IIT Delhi

Lecture # 30

Memory Hierarchy: 
Cache Organization (contd)

we have discussed various aspects of cache organization 

in today's lecture i will give some illustration about how cache accesses are done (refer slide time: 00:01:08) and also we will look into some of numerical problems pertaining to cache performance 


(slide time: 00:01:08)

so this illustration would be essentially like simulation (refer slide time: 00:01:18) 

we will take small example and try to see the effect of various changes in the cache organization 

for example what happen when you change the block size what happen when you change the associativity 


(slide time: 00:01:18)

so in in in a small example we will lo look at some sample references to memory addresses and see how you get a hit or a miss 

and at the individual access level you will be able to see um when a miss gets converted to a hit or hit gets converted to a miss by making a change in the organization [noise] 

so this is one way of looking at cache wh what is typically done is that you look at it grossly in a in a statistical sense um which gives an overall behavior 

but this detailed view detailed illustration is meant to see the exact mechanism which works behind [noise] um cache accesses misses and hits 

so when we will spend little bit of time on trying to see um what is the overhead in a cache organization in terms of the extra bits you required for holding the tags and what are the factors on which it depends that we will see 

then couple of numerical example about cache performance in different context we will see 

(refer slide time: 00:02:39) so um here is [noise] a small cache which we will use for [noise] demo 

so this is a direct map cache with sixteen words 

so we are not worrying about bytes and words lets assume that everything is words 


(slide time: 00:02:39)

so you will get x address sequence from memory or from from from processor trying to access a memory and each access would be examined we will see whether it is a hit or a miss depending upon what the status we see on the board 

so the this is these are sixteen words of a cache numbered from zero to sixteen and um the addresses which we encounter 

so so there is some sequence of address which is stored in this demonstration which will come out and we will record it in miss 

also we will keep track of the total number of accesses made [noise] and total number of hits encountered and misses encountered 

so to begin with we assuming that it s a direct map cache right 

so the the [noise] um the the feature of this kind of cache is that given an address of memory the location in the cache is fixed ok 

so if cache size is sixteen given the memory address you could say address modulo sixteen and you will see where it is going to fit 

so we we will not worry about the memory contents but what we will keep record is essentially which word of memory is sitting in which location ok    


(refer slide time: 00:05:41) um so um you can see that first address which came was address one and naturally it  is supposed to sit here the cache was empty initially and therefore we record this as a miss 


(slide time: 00:05:41)

so here here is the place where i am going to record the addresses and you it will record the misses and hits here 

so first one is a miss and obviously initially we are going to encounter lost of misses and its only when you come to reuse of some address some when address is start repeating then there is some possibility of having hits 

so so initially there will be mostly misses lets proceed further 

um four sits in this position eight sits in that position 

so so just for space aspect the whole thing to two column but assume that it s a it s a single um one dimensional array 

so you have addresses zero to fifteen and um initially numbers are sitting in their natural places ok 

(refer slide time: 00:05:48) now you see what happen 

um address twenty which you take modulo sixteen is same as four ok 


(slide time: 00:05:48)

so four has been replaced four has been thrown off and if there is occurrence of four again although we would have brought this to cache we will not be able to find it next time 


(slide time: 00:06:26)

(slide time: 00:06:26) in seventeen comes and replaces one so here is um the the these are the spot where you can see that perhaps different cache organization could have helped right

nineteen comes in position three fifty six replaces eight (refer slide time: 00:06:56) ok because fifty six modulo sixteen is eight 


(slide time: 00:06:56)

nine eleven forty three so forty three replaces the eleven 

oh i am i am sorry i think it skipped went through couple of steps quickly  

so um four came again and that time it was um a miss um we we can actually [inaudible] le let me come to that point again quickly 

so twenty replaced four i think four comes at this point ya

so four throws twenty back again 

imagine if your program behavior was like this at four and twenty were being accessed repeatedly then they will keep on fighting with each other 

so fortunately five um which was loaded initially in in the in the fourth cycle is still there and so you you get a hit here 

(refer slide time: 00:08:17) six nine nine is already there so nine is another hit and then seventeen is again a hit because seventeen is already there 

so the overall tally is that there was sixteen accesses which were made only three are hits six thirteen are misses 


(slide time: 00:08:17)

now what do we expect if we make changes 

when suppose we change the block size we increase the block size what block size will try to help you with is spatial locality it will capture locality that if you are having references which are close to each other in address in those areas you are likely to have um additional hits ok 

so the next one we look into this um but we we are going to continue with direct mapping 

so it may not help perhaps in the area where um some data was brought in and it was thrown because of cache [noise] ok

(refer slide time: 00:09:21) so next scenario is um same direct map cache with block size equal to four right 

i have just split the cache into four blocks zero one two three these form one block four five six seven form another block and so on 

so now the meaning of this arrangement is that when you make an access um you you are looking at the block level 

tags are actually stored um although i will show all all the location filled up but tags are stored one per block 

and when when you find a miss you getting the entire block from the main memory and filling up in the cache ok 


(slide time: 00:09:21)


(slide time: 00:10:43)

(refer slide time: 00:10:43) so first time when address one was encountered there was a miss but whole block got filled up zero one two and three ok 

then address four was encountered and second block got filled ok 

you now um its it s advantage would be seen if addresses within the block get referred again 

so eight fills another block and now since there is a reference to five um there is a hit because of that right so here you can see some advantage of a larger block size 


(slide time: 00:11:18)

(refer slide time: 00:11:18) twenty replaces this block so so that problem continuous ok four was replaced by twenty now the either block five is also replaced 

um seventeen replaces one what nineteen gives you a hit because nineteen in in the same block as seventeen 

fifty six replaces eight um again since you are referring to nine um that block beginning fifty six is thrown thrown out again (refer slide time: 00:11:46) 

so here is a miss again 

eleven is a hit because we had referred to nine and that block is there 

um so access to four is also a miss here 

(refer slide time: 00:12:11) forty three um the block beginning with eight is again replaced 

um five is a hit because by virtue of four which was accessed [noise] six is also hit (refer slide time: 00:12:17) nine is a miss and seventeen is a hit 

so on the whole there are three additional hits here and the total performance is improved we can we can see which are the new hits which have come its not scrolling 


(slide time: 00:11:46)



(slide time: 00:12:11)

so earlier we had hit for the last five nine and seventeen um nine we have lost ok five and seventeen are still there the additional ones have come here here here and here at these four position you have additional hits 


(slide time: 00:12:17)

so um it is not that whatever is the hit remains a hit ok because um the the total number of blocks is reduced 

so so you you can see some effect of increasing the block size too much so some some of the hits maybe lost whereas other maybe arid 

and one has to see it carefully if you recall i had shown you variation of um miss rate by versus a block size 

so initially um the miss rate decreases ok so that s improvement in performance but if you increase a block size too much you start loosing and miss rate may increase again ok 

(refer slide time: 00:14:05) now lets look at associativity which gives you flexibility of placing blocks at more than one locations and here one has to decide what is the replacement policy  

because each block can be placed at more than one location um when it comes to replacing an old block you need to see which one needs to replace 

so in this example we will follow LRU policy ok 

so the block which is least recently used will get replaced um whenever there is a hit 

so um what i have done is i have repeated those addresses zero to seven here effectively each row for example this three and this three they form a set 




(slide time: 00:14:05)

so so any and all i will take the addresses now modulo eight ok whatever address comes you take it modulo eight and try to place it in in a particular rows 

for example if address modulo eight is three i will see if this is free or that is free it can replaced in one of these 

if both are filled up both are occupied then one has to be replaced and that will be done by um LRU policy 

so there are eight sets and there is a degree of associativity which is two ok 

(refer slide time: 00:16:27) block size is still one right 

so initially everything is empty i i place this one here next is four get gets placed there then eight is placed at this location zero ok 

eight was um the location for eight was this earlier bur now of course both are location zero 

five comes here twenty does not throw four away now so this this is a change which you must observe twenty can be placed here 

twenty modulo eight is four and there is a vacancy in this set so twenty can be placed here

 
(slide time: 00:16:27)

seventeen does not throw away one it has another place nineteen comes here fifty six again does not replace eight 


(slide time: 00:16:59)

(refer slide time: 00:16:59) nine has replaced one ok because now we were falling short of space here and out of one and seventeen which one should have been replaced that you can see 

so you i have to replace seventeen or one but this is more recently used that is less recently used so one gets replaced 

eleven four again so four was not thrown so here is a new hit we have got 


(slide time: 00:17:19)

(refer slide time: 00:17:19) forty three five five is again a hit as was the case earlier six nine and seventeen 

so if you um compare it with the first case ok we we have marginal improvement and and the new hit which you can see has come up here right [noise] ok 

(refer slide time: 00:17:45) now lets extend the degree of associativity further lets make it four way set associative 

so the sixteen block cache sixteen word cache is divided into four sets each with um having four locations 

so we will do it in similar manner 

(refer slide time: 00:19:11) um notice that the sequence of address you are following is same so that we have a fair comparison 

and in this if i remember correctly nothing will be thrown away there is enough room for everything 

so four is a hit five is a hit six is a miss nine is a hit and seventeen is a hit 





(slide time: 00:17:45)


(slide time: 00:19:11)

so um so we we still capture same number of hits ok and um what one could say is that this is a kind of situation point as far as this um scenario is concerned by increasing the degree further we are not really going to get anything better ok 

so now um after having gone through this exercise [noise] 


(slide time: 00:19:50)

(refer slide time: 00:19:50) um we would like to look at different types of misses ok 

there are three types of misses one is compulsory miss 

compulsory misses are those which you encounter initially initially even you are starting with empty cache they compel compulsorily some misses because noting is there in the cache  

so first time anything has to be refer to has to be brought um then they are capacity misses 

capacity misses are because the cache has limited capacity it cannot hold everything which is there in the main memory so something has to be out 

and any misses which are resulting from from that limitation are categorized as capacity misses 

thirdly we have conflict misses conflict miss comes for comes because of specific mapping approach which you have and um more the degree of associativity you have less are the conflict misses 

so conflicts are because many main memory location contain for same position in the cache and one [noise] one word comes and throws something else 

um so now in if you look at previous examples ok lets start with this one um which which of the misses are compulsory misses and which are not [student: noise] 

ya there are lots of compulsory misses lets find out which misses are not compulsory misses [student: noise] 

um wherever we are making second reference ok tha that is either due to conflict or limited capacity 

so for example um second reference to [noise] um this miss is not a compulsory miss because we it was something which could have been saved ok 

and i suppose everything else [student: noise] no um the this five is a hit ok so this is not a miss the this is the one which is not compulsory everything else is fine everything else is a compulsory miss ok 

in the in the next one [noise] you you notice that um compulsory miss misses have been reduces here ok and whatever reduced its a larger block size which are reduced compulsory misses here 

um so five for example which is compulsory miss is no longer a come no longer miss at all 

but we have lost nine so what kind of miss is that 

this is not a compulsory miss [student: noise] um ok now here is a little bit of problem whether you call it capacity miss or conflict miss 

um between capacity miss and conflict miss it its hard to pinpoint for individual miss whether it is conflict or capacity 

so so um this notion is actually at a statistical level you you look at the overall miss rate um so so lets say one percent misses are there ninety nine hits are there so that one could be broken up into lets say point four could be a compulsory misses point um three could be capacity misses point three could be conflict misses 

so so the the way you precisely define um capacity miss is that you take fully associative cache which means they are no conflict misses ok 

so whatever you are getting over and above compulsory misses is um capacity misses 

so fully associative cache has only um compulsory misses and capacity misses 

in a in a cache which is um not fully associative the the additional misses you get at a in terms of average in terms of the total gross behavior 

um they they are the conflict misses because they can be attributed to conflict

otherwise individually it is it its bit of um problem to categorize a miss as a comp as a as a capacity miss or a conflict miss 


(slide time: 00:24:34)

(refer slide time: 00:24:34) alright now um lets um move to the issue of sizes and bits you have cache of certain size certain organization 

how many bits you have to incur as overheads for storing the times ok 

it dep depends upon various parametes

for example suppose the memory address space is address by k bit addresses ok 

so in in in [noise] this example this will be thirty two and suppose cache size is S bytes now we assuming that cache is or the memory is byte addressable and let the S be the size of the cache in bytes 

let B be the size of block so it is B bytes B can also be b would typically be some power of two and let A denote the degree of associativity ok 

so A equal to one means um its direct mapping cache and when A gets its maximum value then it is fully associative 

so how many sets you have 

um S is the total cache size and B is a block size so S upon B actually gives you the number of blocks ok you divide that by A you get the number of sets ok 

so so in a two way set associative the total number of sets is half the num number of blocks and so on 

um how many bits you require to index because um the address is divided into tag part um index part and then there is an offset within the block to access a byte or a word within a block 

so number of bits required to access um one set is naturally log of this ok and um number of bits required to address a byte within a block is log of B 

so the address which is of k bits if you remove if you subtract the number of bi index bits and number of bits required to address a byte the remaining are tag bits ok 

so k minus log of S by A B minus log of B um which you can write as k minus log of S up on A because this B will cancel with that B 

so so these many [noise] um bits are required to define each tag and you can see that as A increases the tag size will increase right 

the total number of tag bits equal to the tag size multiplied by the number of tags and number of tags equal to the number of blocks which which we said was S upon B 

so so this um this way you can actually um calculate the overhead of um cache in terms of the tag bits 

so so actually all these tags together are called wh what is called are tag the cache directory so cache has two parts cache directory and the cache data ok 

now lets move to lets take some numerical example (refer slide time: 00:28:25) and try to relate various performance parameters to each other 

you have seen that effective CPI in terms of a cache is the CPI assuming that there were no cache misses it was ideal situation plus the additional cycle which are coming because of misses 

so these are stall cycles they depend upon the miss rate miss penalty that means additional cycles encountered additional stall cycles encountered per miss ok and the third factor is number of memory access per instruction 

so the product of this gives you the total number of um stall cycles per instruction 

suppose in a given case we have the basic CPI as one point two right um miss rate is zero point five percent and block size is sixteen word how do we find the miss penalty 


(slide time: 00:28:25)

so um we we would need to we we we will need need to find miss penalty to get this so how do we find miss penalty that will come to in a moment 

um for for this example we assume that number of memory access per instruction is one 


(slide time: 00:29:48)

(refer slide time: 00:29:48) the miss penalty will depend upon what kind of memory cache interface you have and we have talked of three interfaces if you recall 

in one case we have memory which is one word wide and you access one word at a time transfer one word at a time 

second case was that you have um memory which is multiple world wide and multiple words can flow on the on the bus simultaneously 

lastly we have um interleaved memory or a page DRAM [noise]

so suppose it takes one cycle to transfer either data or address over the bus right 

so so if we till assuming a bus bus cycle is same as the CPU cycle 

assume that memory latency is ten cycle that means once address is given memory has um delay of ten cycle after ten cycle it will respond with the data 

so given this we can find out miss penalty for case a where one word is accessed at anytime so for each word you spend one cycle sending [noise] a address ten cycle taken by the memory one cycle again by the bus to send the data back 

so total twelve cycles and for a sixteen word block this process will be repeated sixteen time and total of miss penalty is one ninety two 

in case b um lets assume that the memory is four word wide and the bus is also four word wide 

so so that means this cycle of one ten and one this needs to be repeated only four times and therefore we have we encounter forty eight cycles 

in the last case suppose we assume that there is four way interleaving but the bus is one word wide 

that means we will in one transaction will send address take one cycle then memory takes ten cycles 

actually they are four modules each one will be ready with its data and four cycles will be required send it back to the CPU 

so total of fifteen cycles are required to get four words [noise] and that is repeated four times to get a block of sixteen words so that gives you a total miss penalty of sixty ok 

so we will get um three different answers for CPI for these three cases


(slide time: 00:32:21)

so coming back to CPI calculation (refer slide time: 00:32:21) we have um effective CPI as one point two into um point five percent was a miss rate so we put that as a fraction here 

miss penalty we have just calculated we will substitute the values and number of memory access per instruction is one 

so putting the value of one ninety two as miss penalty we get CPI effective as two point one six 

in the second case um miss penalty is forty eight CPI comes out as one point four four third case miss penalty is sixty CPI comes as one point five ok 

so um [noise] you you would notice that um if you look at miss penalty there is so much variation bu but the the factor why which performance varies on the whole is not the same ok 

so so its variation from lets say one point four four to two point one six as you change the miss penalty from four eight to one nine two 

so ultimately um in terms of CPI this is what matters 


(slide time: 00:33:30)

(refer slide time: 00:33:30) alright now um take another example where we try to do comparison of different architectures 

so suppose we have three different cache caches with different organization first two are direct mapped cache and then the third one is set associative cache with two way associative access 

the block size is one word in the first case and four word in the second and third cases 

here we are given instruction misses and data misses separately 

so that means out of hundred instructions we try to access four time we get misses and in the first case 

similarly we make hundred access to the data and there are eight misses 

so so we are given CPI for the first case we need to find CPI for the um remaining cases 

miss penalty as we know depends upon the block size so without going into details lets lets say that miss penalty is given by six plus block size this is in terms of number of cycles 

also we need information about um how many instruction make reference to data so each um each instruction makes one reference for fetching the instruction itself 

and here fifty percent of ins instruction make data reference so they are either load or store 

so so now we can calculate the number of stall cycles um this is the miss penalty in the first case since the word since the block size is one word this is seven multiplied by um the miss rate 

so for instruction it is point zero four for data it is point zero eight but only half the instruction make data access so multiply by half here 

so what this says is that in cache one on the way there are point five six um stall cycles per instruction 

going to cache two the miss penalty is ten because block size is four [noise] right 

um I miss point zero two D miss point zero five as given and half because half the instruction refer to data we have factor of half here and this comes out to be point four five 

in the third case again similar but this factor is reduced 

so so these are the um extra values which have to be arid over basic CPI to get the overall CPI 

what we what we given here is effective CPI two point zero ok 

so so basic CPI can be obtain by looking at this out of two point zero point five six is because of stalls cache stalls and the rest is the basic CPI 

so which can be assume to be same for these cases 

(refer slide time: 00:36:46) so CPI for three cases um it is two point zero here for the second case two point zero minus point five six plus point four five and similarly for the third one 

so the figure we are getting is one point eight nine and one point eight four 

now this is all in terms of number of cycle but as you know we need to take into account the the clocks clock cycle clock period also

and typically more the complex structure you have like you have associative cache the cycle time is likely to be larger because the hardware has to allow multiple comparison at the same time 

so so so it maybe slower and it may pull down the clock period 


 

(slide time: 00:36:46)

so the figure is figure maybe little bit exaggerated here but assume that for direct map caches clock period is two nanoseconds and for the set associative case it is two point four nanoseconds 

so if you multiply the CPI figure with this figure you will get the time spend per instruction ok sorry 

an an and this is a this is a overall indicator of the performance take into account all the aspects 

so so looking at that it is the second one which seems to be the best 

of course if if the increase here this increase is not so much um possibly this could have done better ok 

(refer slide time: 00:38:21) so um we we have to see that by by making it associative by what amount is this miss getting reduced 

so sorry its running away um if if the advantage you are gaining here is not substantial it maybe lost by ba ba by the negative effect you will have on the clock period so that has to be kept in mind ok 

now this example um looks at multi level cache so suppose you have two level caches what what we will do is first we will analyze situation with one cache and see what improvement the second cache brings to the overall performance 



(slide time: 00:38:21)

so suppose we have processor in CPI of one when there is no miss and its running at clock of five hundred mega hertz 

the main memory access time is two hundred nanoseconds and the miss rate is five percent 

so if suppose there was only a single cache we are saying that the miss rate is five percent 

um it is absorbed that by adding L two cache second level cache there is a reduction in the miss rate 

so now suppose the miss rate becomes two percent only it drops from five to two um what is the performance improvement what is the net effect on the CPI 

so we we need access time data about the L two cache and suppose that is twenty nanoseconds main memory is two hundred nanoseconds and L two is twenty nanoseconds 

so um and the access time of L one cache is subsumed in this ok this is when there are no misses 

so so miss penalty um when one as to access memory is um two hundred nanoseconds to convert this into cycles we divided by time period of this which is two nanoseconds 

so two hundred by two or hundred cycle is a miss penalty for making access to main memory ok 

we we are not going into details of how we get this because ultimately dependent upon the bus architectural block size and so on 

but whatever all that is the net result is lets say two hundred nanoseconds 

so if you have just L one cache then what is your effective CPI it is one which is this figure plus five percent that s a miss rate into hundred cycles which is the um penalty 

so this comes out to be six  right

once again we have made an assumption here that number of memory access per instruction is one right 

um there there are no data access are everything is just one 


(slide time: 00:41:26)

(refer slide time: 00:41:26) um suppose we introduce L two [noise] the miss penalty of L two itself is twenty nanoseconds divided by two nanoseconds which means ten cycles 

so now the totals CPI take into account misses at various levels will be the base CPI plus stalls due to misses at L one level ok 

so when when there is miss at L one you make access to L two so there are some stall cycles encountered 

if you have miss at L two level also there are some additional cycles you incur 

so this is this will have to be ten multiplied by the miss rate here this is five this is the hundred multiplied by the miss rate here

so one point zero the base CPI five percent miss at L one level multiplied by ten which is a miss penalty and there is also multiplied by one we assume that one memory reference per instruction otherwise that factor will come here plus two percent is a miss at L two level multiplied by hundred ok 

so so now this first one first term gives us point five second terms gives us two point zero and the total is three point five 

so so what is the ratio what by what factor has performance improved you can simply divide um CPI which we had earlier six point zero and the CPI you got now three point five to one  point seven 

so [noise] um now le let me clarify certain terminology here um we we are saying that when you put L two there is a two percent miss on the whole ok 

so so this is what is called in in a multi level cache terminology this two percent will be consider as global miss that is all cache is put together that as a system is taking care of ninety eight percent of the access and its leaving out only two 

but if you where to focus your attention on [noise] L two cache only L two is receiving some access it is re receiving some requests some of them it is able to serve some it is loosing 

so so there is something called local miss for L two ok that that is not shown up explicitly here right 

um the the there is also a a term called solo miss solo miss is the term suppose now in in this two level cache system you remove L one and L two is the only one left 

um then the misses which you will see at that time are called the solo miss of L two solo means um when L L two is alone 

so is there some relationship between these 

um in in in a specific case there is a very nice relationship between these 

um when you have completely the the data in L two is all inclusive of data in L one ok 

so if we have the situation that anything which is there in L one is there in L two but L two has something more because L two will be typically of larger size than much larger size than L one 

this this property is called inclusion property it it may hold it may not hold ok 

we will come to the point why it may not hold but suppose it holds um then the the solo miss of L two will turn out to be same as the global miss which we are seeing here ok 

wha what it means that if L one L two together are showing you a two percent miss having put L two alone would also have shown you two percent miss then what what purpose L one is serving [student:  noise] L L one is reducing the time 

because you know L one is taking care of some request and reducing the penalty you are incurring ok and also you you you can relate now the the global miss sponsored to the local miss 

so um ok you can look up on it like this that suppose hundred request are made by the processor ninety five are taken care by L one ok the only five are going L two out of those five effectively L two is taking care of three and losing out on two 

so so what is the local miss of L two [student: noise] local miss is forty percent right local miss is forty percent 

so so it it it looks a very large figure but it its because only selected requests are coming to L two and its able to serve some of those and even by doing that it is reducing the overall [noise] miss rate 

um now coming back to the inclusion property the inclusion property may not necessarily hold why because it it may happen that um L one may retain some data which L two throws off for for its own because of its own structure and replacement policy 

so L one and L two could be structured totally independently the degree of associativity could be different the the block size would typically be same but degree of associativity is typically different and other policies like write through write back they could also be different 

so there is a possibility that something which was brought to L one maybe at that time it was there in L two but eventually by the time you have second reference to that L two has mean while thrown it 

so those situations could arrive [noise] and inclusion property may not necessarily hold ok ok

(refer slide time: 00:47:28) so lets conclude by summarizing what we have seen today 

we um we try to simulate um cache for a few simple situations an idea was to focus attention on individual hits and misses to see which factor is affecting what ok 

then we moved on quantify the storage overheads in a cache organization countered the tag bits and the number of tags and so on 


(slide time: 00:47:28)

finally we took some numerical example to um get better insight into um what factors influence the performance in in a in a statistical sense in a in a gross sense um what are the miss rates how do they affect the um miss penalty um stall cycles not miss penalty stall cycle and how ultimately effect the CPI 

we have seen that um what are the variations in these parameters due to change in the cache organization and um as you go from one level two level how things change [noise] i will stop at that thank you  




Educational Technology Services Centre
IIT DELHI

Computer Architecture

Lecture #31
Memory Hierarchy: Virtual Memory


(fig., 01:11)
in the cache hierar- in the memory hierarchy huh after having discussed the cache organization 
we move on to next level which is virtual memory huh we will first try to compare huh virtual memory with the cache memory and try to see what are the similarities which we can carry on and where we need to make changes huh and do things differently 
so we will see essentially this as a problem of mapping virtual addresses to physical addresses
so that s ((how)) t- th- that s how the virtual memory organization is huh made 
[noise]
that you start with virtual addresses and map them to physical addresses
[cough]
this is done using what is called page table so we will describe what is the structure of page tables huh how huh how they operate and where they are located 
huh one major problem which you will have to handle is due to the size of the page tables
[cough] {referring fig., 02:02}
so huh coming back to this picture where three levels of memory are shown namely cache 
primary memory or main memory and back up memory 
so we we have discussed the interface between C P U cache and main huh 
how C P U tries to access cache and which in turn may update itself from main memory 
[throat_clearing]
so huh from in structures at architecture point of view main memory is the one which is typically seen by the programmer and cache is placed in between main memory and huh
processor in a transparent manner
in the sense that programmer may not typically be aware of ((the business)) of cache memory
so cache memory is only a device a magic device which is put there to speed up the whole operation			[Refer Slide Time: 02:48]

(fig., 02:02)
[cough]
huh but when you are looking at instructions huh generally unless there is a uh software driven ((pre-)) you you will not come to huh notice that there is a cache except for performance
on the other hand 
[throat_clearing]
huh back up memory or the virtual memory organization built around that is used to extend size of main memory and again it s a huh done huh typically in a manner huh transparent to the user program huh the the system program which is involved 
but as for as user program or application program is concerned huh it may be totally huh unaware of the fact that there is huh back up memory which is on a disk 
huh one may only get an impression of a large memory 
huh the objective of having virtual memory organization is not just to extend the size of the memory but the huh huh i just noticed 
the main objective of course is to overcome the size limitation of the physical memory 
and it as to be done in a manner which is huh most convenient to the programmer
it huh it is as transparent as possible
so programmer does not have to bother about moving the data and instruction between virtual memory or that huh that disk and the main memory [Refer Slide Time: 04:18]

(fig., 03:45)
and earlier technique which is more primitive huh is called overlays
where uh a programmer would explicitly divide the program and data into portions
and huh take huh care of bringing the right thing in the main memory at right time
and also ((evicting)) the huh old contents and the new contents upto brought in
but virtual memory tries to automate this process tries to keep in the main memory what is required and keep out what is not required
[cough]
apart from this main objective another purpose virtual memory organization serves is to allow multiple programs to share same physical memory 
huh we although there may be a single user in a personal typing type of environment
but there are many processes which huh are serving the purpose of same user
so you have multiple programs or multiple processes which are trying to share the same memory and huh to do it in a manner which provide protection from one another 
[noise] 
is also huh a task combined with the virtual memory organization 
[cough]
an- and thirdly huh it makes it possible to easily reposition or relocate a program in any area of the memory
it it could be that let us say that two program a and b on one particular day a- a comes in first and gets loaded in- into earlier part of the memory and b comes later
but on another occasion b as to be put in the earlier part of the memory and a later or m- may be there is a different state of program 
so you need the flexibility of huh putting a program anywhere within the memory and that s problem of relocation
you call recall that huh if you try to go back to your ((same)) language programming huh there are some addresses which are absolute addresses whereas some which are relative
so for example in b q instruction you always huh going to an instruction in relation to the current instruction ok
so no matter where the program is placed if the offset in the b q is let say hundred 
you are going hundred bytes ahead or hundred words ahead in that (( ))
so so this instruction is re-locatable but if you take a load instruction which takes contents of register and a constant
it ((signed)) to go to a fixed address and if you position your data elsewhere then this will (( )) if you huh shift your program as well as data somewhere else within the memory 
then this instruction will have problem
so huh on- one device which huh is often put in a ((same)) language is to have a base register apart from huh the register which are actually specifying addresses huh in some local context 
so a base register could be set to the datum or the starting point of program plus data
[background noise] {sounds like someone coughing}
whatever space is allocated and by varying that you can relocate the program
bu- but virtual memory organization as as we will see 
[smack] [cough]
also makes the program flexible in terms of where it can be placed or date where it can be placed
[tapping]
huh our focus initially will be on the first aspect namely how do we take care of providing large size 			[Refer Slide Time: 07:45]

(fig., 07:47)
[throat_clearing]
so what actually is virtual memory 
virtual memory is simply an illusion of a memory which is much larger than the physical memory or the main memory 
so huh we have basically a large virtual address space ok
which may be bigger than the memory physically present and huh it- may not have any relation with how much memory is present
so so you see that huh machines are bought with huh two fifty-six M B memory 
five twelve M B memory or or different amounts of memory can be placed within the same processor
so huh but a larger physical a- a- b- huh i mean the total physical space which is available is larger but often it will not be occupied for financial reasons
so you can imagine a larger virtual space where huh programmer huh can place ((his)) program and data huh without worrying about the fact that there is physically smaller amount of memory
[throat_clearing] [smack]
the- virtual address space and the physical address space both are divided into huh chance of equal sizes as we had done with the cache we had blocks 
so here we have huh what is called pages
so virtual memory as well as physical memory both are divided into pages of equal size
and and the mapping huh takes place at the level of page ok
so now out of entire set of virtual pages which f- which constitute the virtual memory
some pages are placed in the physical memory some are not ok
those you need immediately you need currently are kept in the huh physical memory 
[background noise] {sounds like someone coughing}
and this set could change huh you you require huh one such virtual space for each program ok
huh a- as you remember that huh i talked about multiple processes multiple programs ((trying)) to share some memory
so one could give them each a- separate virtual space of of very large size 
now we have talked of dividing both the spaces into huh areas of equal size
huh there is another possibility of doing it huh differently            
[Refer Slide Time: 10:06]
in the sense that you you divide huh more on a logical plane that is huh 
for example if you have functions you could say that each function or may be group of functions or each data structure or group of  da- data structures could be logically organized as what you may call as segment
so the whole program plus data is divided into a few segments which may not necessarily of same size an- an- and then you can huh talk of keeping some segments in the physical memory some out of it right
huh the advantage of this is that you have huh what you have is a complete logical entity so as long as long as you are executing a function huh you have the entire function in the memory or while you are working with one data structure the whole thing is there in the memory 
whereas page is something which is artificial ok
you you are you are taking a program and chopping it off into equal parts 
huh which makes things convenient ok
so organizationally divide into pages equally huh its very convenient and its very efficient 

(fig., 11:18)
[smack] [throat_clearing]
so huh how do you huh implement huh virtual memory
you implement basically by relying on hard disk drive which is huh firstly a non volatile medium and secondly it ((large - capacity)) at a lower cost
so we- we begin by assuming that all virtual pages are huh primarily placed in hard disk ok
in some area you you can place them and huh keep some of them in the main memory 
and this this subset which is in the main memory can be made to change overtime
huh as i need a (( ))
so now for all this to happen the instruction set architecture should support a larger disk space
so huh in in huh (( )) architecture we have discussed example there is an address space of thirty-two bits ok
addresses of thirty-two bits which means four gigabyte of space is there
so we can say that huh a programmer can always imagine a four gigabytes of virtual space and- it might work with smaller amount of memory may be a few megabytes 
so huh you takes care of the rest its huh some hardware support which is in the processor 
which the programmer may not directly see plus software which huh we- huh 
which basically the opening system software or the system software 
[Refer Slide Time: 12:44]

(fig., 12:55)
[background noise] {sounds like someone coughing}
so now for doing all this can we exactly like what we do for the cache
the answer is yes they have similarities the whole idea is basically same 
that from lower level of hierarchy you keep some information at the higher level of hierarchy and huh change it as and when necessary
so as far as that is concerned huh things are similar b- but there are some important differences which needs to be borne in mind while huh organizing the whole thing
[cough]
so the main difference essentially come from the fact that huh speeds are huh different
so when you talk of speed difference between cache and main memory we notice that its only about an order of magnitude difference let let me (( )) huh this figure
{referring fig., 02:02}
so the difference between these two is much smaller difference between these two is very very large ok
so- so the techniques which worked here may not always work here in the same manner
we need to keep this fact in mind
that h d d is much much slower than the main memory several orders of magnitude
[noise]
huh so now a response to a miss when you don't find things in the cache huh what do you do
you basically huh you expect that if few more cycles are required
so you just hold the processor and do the needful and continue execution of the program 
[cough]
so so this huh action as to be handled by the hardware 
you- you cannot have a special software doing this because that will require huh many many cycles 
so hardware can quickly huh get a block from main memory and serve a miss 
[throat_clearing]
so we- we cannot afford to switch context because context switch means that huh instead of waiting you do something else
so that s not possible because that change over may require huh large amount of time 
on the other hand when you are working with virtual memory      
[Refer Slide Time: 14:53]
you find something which you are looking for is not there in the main memory
you need to go to disk 
the times and all are very very large now ok milliseconds 
[smack]
so you cannot keep C P U waiting for that long and you you must switch the context 
so if you if you have mille several milliseconds of gap 
the processor may better do something else
so from one process or one task it switches to something else
[cough]
a- a- and the response to this miss huh can now we handle by software
[cough]
because we have time available 
so we can do it more conveniently by software
huh apart from these huh differences which are coming because of different speeds 
th- th- the terminology also different for more for historical reasons 
things have come up differently 
so we- we are talking of pages instead of blocks
we are talking of huh we talk of page fault instead of miss and huh as you will see later 
there is a page table instead of a cache directory 
so that s a efficient terminology difference

(fig., 16:02)
[throat_clearing]
now huh another implication of the speed difference is that huh miss rate which can be tolerated is much much smaller ok
if if you remember the expression for performance ei- whether you are talking of every memory at this time or you are talking of effective c p i 
you you have a factor huh which is miss rate multiplied by the miss penalty ok
so if miss penalty is small huh basically what you want is a miss rate multiplied by miss penalty should be small enough right so if miss penalty is huh small 
[cough]
then then we huh lets say if it is ten cycles then point one miss rate will make this (( )) as one
huh which is tolerable 
but if miss penalty is lets say huh				[Refer Slide Time: 16:55]
some ten to hundreds of thousands then miss rate as to be accordingly very very small
otherwise you have a very large figure at hand an- and you will lose tremendously in terms of performance huh now this is huh miss miss rate is indeed a small ((hair))
one because of huh large physical memory size as compared to huh small cache size
{faintly}ok
the the miss rate which you see in context of cache is largely determined by how big the cache is larger the cache smaller the miss rate
so huh basically depends upon how much of space you are capturing in in this level of memory 
if this is large miss rate is lower 
so whereas cache could be in terms kilo bytes main memory is in is in terms of megabytes 
so naturally in the first (( )) itself you have a much lower huh miss rate
so which which is good but apart from that we have to organize other things also which helps in keeping the miss rate as low as possible ok
so huh one is that page is to be kept much larger than huh what the block size we have seen right huh 
in cache we cant increase the block size too much because huh it will increase the miss penalty
and it will also huh reduce the number of blocks and therefore the localities you are capturing will be small
so you cannot have two larger block you have basically huh a a few words four words sixteen words 
{faintly}at most sixty-four words
but huh generally not larger than that here we we need to have large page size
huh firstly so that you can capture much larger locality 
[throat_clearing]
secondly when- when you are getting data from disk huh it doesn t make sense to just a few words because once you spend few milliseconds to reach a particular position in the disk you you better transfer substantial amount of data huh so that the huh the- the time to access huh ((time is)) over larger number of words
so for both these reasons you have typically a uh page size which is huh four kilo bytes to sixteen kilo bytes but the trend is to increase the (( )) further to thirty-two or sixty-four kilo bytes
secondly the the mapping in case of cache we have seen that direct there is direct mapping
associative mapping and set associative
the most common is set associative with degree of two four eight or something huh but here we we need to do our at most best
huh that is we we go for fully associative mapping
[background noise]{sounds like someone coughing}
although we don't use associative memory for that but we need complete flexibility in terms of mapping huh so that there is no miss because of the ((co-)) right 
huh how we do that we will see later and huh among the choice of write back and write through huh we need to use write back 
write through doesn t make sense 
((because)) it doesn t make sense to write one word into disk
so so you are going to write always a- a page and which means that you have write back
write back choice also reduces the number of misses ok
the the negative point of write back it caches that	[Refer Slide Time: 20:20]
the time to write back is larger huh time to write a word is smaller 
but the difference is that huh if you are writing a word and write through cache 
you are writing more often
if you are doing write back you are writing less often
so what suits here is a write back approach and you you don't have any virtual memory 
with write through its always write back 
huh then the last point is huh what replacement policy you use
this is also important because if you are throwing back throwing away a huh huh wrong piece of information then you are losing it you you will have a miss later on
huh the ideal thing as for your replacement policy would be ((to be able to see in)) future 
so if you know what references are going to occur in near future it is that data for instruction which you need to retain ok
unfortunately that s what realistic 
wh- what we can is we can only look at the past and then try to figure out what we are likely to use 
so on that basis huh ((l r u)) as been experimentally found to be huh most appropriate policy 
but its not easy to implement in ((concept)) it sounds similar 
that you you pick up the one which as been least recently used but how do you implement 
if you if you are tagging each block or in this case each page with with time 
then this is something this is the quantity which is not bound ok unbounded quantity so its not practical to huh use huh time or the cycle number because you don't you don't want to restrict huh programs shouldn't ((follow)) a limited time
huh you have people doing research who huh put their program for execution for days together and then get the result
so you huh th- that s not really workable huh you you might think of huh may be maintaining huh an order which was huh most recently used keep it number call it number one which was next recently used call it number two and so on
th- then you shuffle the order as accesses take place
huh again th- the problem may be that you may have to make changes in the various huh su- suppose you you are maintaining order of various pages or various cache blocks 
and huh you you change the order of one other as to be shuffled
or you have to use more sophisticate data structures to do that
which which requires that to make one access to one level of memory you huh to do this housekeeping of L R You you may have to make many ((accesses)) 
and the purpose may get lost trying to do this L R U nicely
so huh doing it in hardware is infact more difficult
huh and doing it in software somewhat is easy
but still we don't want to lose efficiency
so in in hardware huh that that means when 
what i mean is when you are handling cache  you may huh you may not worry about doing something which is close to L R U
you may take a more approximate policy huh
but in huh case of virtual memory you try to be
[throat_clearing]
as close to L R U as possible					[Refer Slide Time: 23:42]

(fig., 23:44)
[cough]
so now with this background lets huh try to look at this picture which shows virtual memory on one side and physical memory on the other side 
huh each is divided into pages each page of virtual memory each virtual page huh 
is either mapped to physical memory or to a disk 
huh infact strictly speaking each each page must have its residence in on disk
some of them will also have residence on a physical memory 
so so you need huh mechanism which defines what goes where ok
[background noise] {sounds like someone coughing}
given a page where we place it in the physical memory and huh once we have placed it 
how do we find it later on 

(fig., 24:34)
so this this process is called translation of the address 
huh imagine that you have virtual address 
i am showing thirty-two bits in this case
this could be divided into huh page offset and virtual page number 
so suppose huh page size is four kilo bytes which means a twelve bit number will specify byte within a page 
rest of the address is the page number 			[Refer Slide Time: 25:02]
w- we- we have this twenty bits specifying page number and twelve bits specifying address within a page 
suppose we have huh physical memory present which is huh two (( )) power thirty huh bytes ok
so lets say this is huh four gigabytes and this is one gigabyte 
now physical memory can physical memory address can be 
similarly divided into page number and offset
so the problem now remains is to translate virtual page number into physical page number
so it is a translation process we need to figure out how it as to happen

(fig., 25:48)
huh this is the mechanism using page table 
so page table huh is basically a look up table where you keep one entry for every virtual page so so given the virtual page number you look at appropriate entry here and this will tell you 
huh where this page is located in the physical memory or effectively it will give you physical page number right.
there is a uh bit apart from this physical page number huh which is valid bit and as similar purpose as we have seen in cache it will tell that whether this particular virtual page is present in physical memory or it is not present
so if it is not present in the physical memory we need to know address of this page in the disk because we have to get it from the disk
so either we get physical page number here or we get huh the disk address or pointed to area where disk address would be stored
[throat_clearing]
and huh the location of this table could be obtained from huh a register which is called page table register
so it s a very simple look up process ok
huh in- in terms of
now compare it with cache how we were doing
huh in- in cache we huh we didn t huh in direct map cache 
we looked at the index bits and huh directly went to huh cache where huh we simply make a check whether it is pre- huh it is present or not ok
we we compared with that tag bits huh 
[cough]				[Refer Slide Time: 27:32]
but here now we are before we access the huh higher level of memory which is the physical memory in this case we are going through this table to huh reach the position
whereas in case of cache we directly reached huh the cache memory through that index
but that s the direct memory access where the locations are fixed
in terms of its effect this is same as fully associative cache whe- where in case of cache we were required to make comparison with all the tags present for various blocks 
but in this case we are not making any comparison huh here you have one entry per virtual page ok in cache it is one entry per cache block ok
so so the number of entries in cache directories is equal to the number of huh blocks in high level memory 
here it is huh the number of entry equal to number of entries in the number of pages in the low level memory
ok so that is the difference
{very faintly}you should notice -

(fig., 28:40)

so huh this picture shows the mapping process huh with looking at the huh virtual page number taking it taking it out of the virtual address
we are indexing into this table and this table is telling (( )) either go to physical memory or to disk as the case is
so huh if you if you have a hit it is fine you make an access
if you have a miss then huh this is called page fault and page fault result in huh context switch 
so- so first of all huh the current process which as made this request is suspended ok
you initiate a request to the disk to do the transfer
meanwhile disk is ready several milliseconds are going to elapse
and you can huh you can execute thousand thousand of instruction 
so control is transferred to another process which is huh waiting for execution
so now the question is where is the page table stored ok
what we have said is before you go to physical memory 
you have to go through page table 
you have make an access but where is that located
and the answer huh came ((we)) worked out if we understand how big that table is
or what is the space requirement			[Refer Slide Time: 30:02]

(fig., 29:54)
[throat_clearing]
so su- lets take some typical example suppose virtual address is thirty-two bits
page size is four kilo bytes and each page table entry is four bytes
now what i indicated was that 
page table entry as a valid bit and physical page number
huh actually apart from this it may have other information 
it may have information about memory protection in context of multiple processes
so so lets imagine that you have four bytes of information per entry
if you huh put all this together 
[throat_clearing]
the number of page number of page table entries which are equal to number of pages is two 
(( )) power thirty-two virtual memory size divided by page size are two (( )) power twenty 
one million pages are there in the virtual memory 
and size of the table could be obtained by multiplying this one million by huh size of each entry and you get four megabytes 
[cough]
now there are hundreds and thousands of processes each one as to worry about 
and we have decided to huh allocate same amount of virtual space to each of these
[cough]
that where are we going to put this much of information
huh having a- a- direct separate memory for this is ruled out ok
so- so your attention goes to main memory huh but main memory is also in terms of huh megabytes few hundreds of megabytes (( ))
so trying to stuff it with several huh hundreds of huh page table will simply huh leave no space for other useful things 
[cough]
so what do we do there are various ways you can huh handle it some of these are listed here 
common ones that you realize bound on the page table size exploit that or exploit the sparseness that means the whole huh whole page table may not be really active
you can use multiple level 
or you can use techniques like paging the page table and so on
so lets look at these one by one 				[Refer Slide Time: 32:15]

(fig., 31:55)

(fig., 32:18)
[throat_clearing]
huh so although we are saying that huh you give four g b space 
[background noise]{sounds like someone coughing}
four g b of virtual space to each program but it may not need that
suppose it needs only a few hundreds of megabytes or may be few tens of megabytes 
then we we should actually (( )) only that
so we can huh have a register which keeps track of the bound and huh once you reduce the memory huh virtual memory size the page table size also reduces
so allocate only as much space as necessary and allow it to expand or grow in one direction if need of a program changes then (( ))
but th- this is huh not really does not really matches what is the requirement
typically huh the programs are organized to grow in two directions
one- one ((growing)) area is stand ok
huh which grows or shrinks as call ((them made to)) huh functions
on the other hand there is also what is called heap
wh- which grows and shrinks as memory is randomly allocated reallocated
{faintly}through through pointers 
[Refer Slide Time: 33:28]
so typically these are r- r- huh organized to grow in two opposite directions
suppose you keep keep some space huh stack grows in stack grows in one direction 
keep grows in another direction 
so that huh you can allow them to grow independently
so so its not difficult to accomod- accommodate this by huh thinking of this as two virtual memory segments ok
and have two page tables or you can think of two parts of a page table which can be made to go independently
so this can take care of both stack and heap
huh b- but this is not sufficient it it does not still reduce the memory requirement substantially
huh one thing we are not able to do here is that we are not able to handle sparsesity of the table because huh the the table we are keeping one entry for every virtual page ok
whereas the pages actually present in the physical memory are much fewer
so why not we just keep track of those if if you have a mechanism to just huh 
keep track of only those entries of page table for for for which we huh have current requirement then it will be much better

(fig., 34:52)

so huh one possibility is to go back to cache like approach that is actually have associative memory huh where the number of entries would correspond to number of entries actually there in the physical memory huh so so that will be much smaller
but having even then having associative memory of that size is impractical
so alternative is to use hashing technique ok
so so given a virtual page number you apply suitable hashing function huh
you you go to and you get an index which takes you into huh either a smaller table or or takes you or gives you directly the huh physical page number ok
so huh this this is what is called inverted page table because you- you will have entries corresponding to huh
i mean its becoming something like cache situation where you will keep entries huh organized as per the entries which are there in the physical memory
[throat_clearing]
huh more common technique is to use either two tabl- two level page table or huh paging the page table as i will see huh little later			[Refer Slide Time: 36:11]

(fig., 36:02)
so huh what we are doing here is that we organize the virtual space in terms of segments ok
and segments are divided into pages
so now it- its different from huh segments which i mentioned earlier which correspond to the logical boundaries or function in data structure
so lets imagine that we have segments of equal size 
it is just that huh entire virtual memory is divided into some larger chunks
we- which are again divided into smaller chunks which are called pages
so we can have one page table for each segment and huh then you have flexibility of 
not having to keep all the page tables in the main memory ok you can keep a few page table which are relevant in the main memory and therefore reducing of space requirement 
the segment table will keep track of where the page tables are 
which of them are in main memory which are in the disk right 
so huh this organization huh will try to just track information which is active and which are required 

(fig., 37:24)
so conceptually this is how it is organized
huh you have a number of page table page table zero one two three four page table i page table n minus one ((ok))
[Refer Slide Time: 37:33]
these are much smaller page table 	
effectively if you put all this together they they will form your good old single monolithic page table but now we have divided and all of them need not reside in the main memory 
huh the starting addresses of these page tables is pointed out by another table which we are calling segment table 
so first you make an access to segment table 
now imagine that virtual addresses divided into three parts
segment number page number within a segment and byte number within a page
so using the segment number you pick up one entry in the segment table
this will tell you where the page table is
lets imagine that page- page table you are looking for is in the physical memory
so this will give you the starting address of that page table within this page table you can take this page number and index into it 
this will now tell you huh the physical address or physical page number of the entry you are looking at 
{faintly} you are looking for
and therefore huh by making this two step access
huh you can make you can huh reach the required point in the physical memory
without necessarily having to have this whole thing in the physical memory
now it could be that the page table you are looking for is not in the physical memory
so a page fault will occur at this point and first you will bring this page table into physical memory then make an access to the page table and then make access to huh the the main memory right 
so the- the starting address of the segment table could be in a register 
huh as you can see here there are two points two places where you may encounter page fault you may encounter page fault huh when accessing a page table itself huh secondly you may access you may have the page table but huh you may not have the page ok
so page page fault can be encountered any of those two levels

(fig., 39:47)
huh lastly huh
we can think of keeping the page table itself in the virtual memory ok
instead of imagining page table to be in the physical memory 
[throat_clearing]					[Refer Slide Time: 40:00]
huh and worrying about two levels or other techniques 
suppose you you place the page table itself in virtual memory then automatically some part of it will be kept in physical memory some will not be rest will not be right
so so its only huh you you will bring only that part of the page table as it is needed
as you  make accesses huh few entries will be there rest will not be there ok
[cough]
so n- now huh how do you locate this page table 
huh as you have seen that to access any memory location you need to go through page table
if page table itself in virtual memory you you need some way to find out where it is in the physical memory ok because if if you are keeping page table in the virtual memory
[noise]
how do we get so it it s a kind of (( ))
what is done is that huh we can keep user page table in huh system virtual space ok
you have many user processes 
you have a system process ok
so huh system is also assigned a virtual space huh lets keep user page table in in system virtual space and huh we can also ensure that huh system space table or at least some part of it is always there in the physical memory
so so you can start by that huh find out where your page table is o- or you are not worried about h- having entire page table in the main memory huh you want to find that part of the page table wh- where you are making reference ok
so huh the- the address of huh entry you are looking for is given by the starting address of the page table plus the offset within the table ok
{faintly}lets get back to this
{referring fig., 25:48}
how are you making an access to a particular entry in the page table you need the starting address of the table and you need this offset which is given by virtual page number
so huh if lets say page number if ten that s means you want tenth entry
so in terms of bytes it may be ten into four so you take this address plus four times this number that gives an address huh this is the address of huh normally 
imagine if page table was in the physical memory you will take this address and go to physical memory directly read the page table entry and proceed further 
but now what we are saying is that huh this address which you have it is an address of one page table entry this itself is a virtual address
so first we will have to get a physical counter part by going through huh systems page table ok because this is in system virtual space so we go through system page table which for the moment you have to imagine is in the main memory ok
so you you access that huh get the physical address of huh the page table entry huh hoping that it is present in the main memory you make an access there huh find out whether page you are requiring ultimately is present or not an- and then make an access
so you are need to go through these huh two step process one first step is huh accessing the system page table which hopefully is in the physical memory then accessing huh users page table huh w- which the relevant part hopefully may be in the physical memory
huh otherwise there will be a page fault once you got that then you make access to physical memory 
[Refer Slide Time: 43:48]
so huh it is basically not two totally three steps are involved huh to to read anything from virtual memory huh i will elaborate this little further huh (( ))
so huh you will have a clear idea and then we will also go to performance issues next time

(fig., 44:10)
so let me summarize at this point 
huh we started by virtual memory and cache organization and noticed similarities 
also noticed the differences 
differences are important otherwise things will not work with the efficiency we want 
huh the the key mechanism here is the page table which is used to translate virtual addresses to physical addresses
page tables are tend to be very large which poses a problem
an- and there are many techniques to counter that 
these techniques basically exploit the limited size the sparse sparseness locality and so on 
huh and ultimately the relevant part of page table as to be in the physical memory
there is no separate memory kept for this 
i will stop at this point thank you 			[Refer Slide Time: 45:10]




Educational Technology Services Centre
IIT DELHI

Computer Architecture

Lecture #32
Memory Hierarchy : Virtual Memory (contd.)

we have discussed virtual memory as huh mechanism to give an illusion of a large huh adjustable space to a program 
huh wi- with a with an attempt to keep the performance as good as a physical memory 
today we will continue with that
we have seen that a page table is the key mechanism in organizing a virtual memory
which helps you in translating virtual addresses to physical addresses 

(fig., 01:25)

huh we notice that storage of page tables was a big issue because they are large in size
we looked at that and we will continue huh on that topic little further to see in little more details
huh we will introduce another structure which is present in hardware called T L B 
or translation lookaside buffer
[noise]
whose role is to speed up the access
we will see how T L B works together with cache
[noise]
huh we will also look at the possibility of huh addressing cache by virtual addresses
instead of huh physical addresses
and finally we look at the role of virtual memory organization in memory protection
[throat_clearing]
so we have seen that page table is basically a look up table where given huh virtual page number you look up in the table and you will find out the physical page number corresponding to it 
huh that happens if the page is present in the physical memory but in case page is absent we get page fault and then we expect that this table 		[Refer Slide Time: 02:38]
will give us an indication of where the page is stored on hard disk
so either directly disk track and sector distribute (( )) in the page table or it will point to an area from where these could be picked up

(fig., 02:18)
so huh in concept its very simple
but the problem comes up because of the large size
we- notice that it could be several me- mega bytes and there may be many processes simultaneously active 
you would require a separate page table for each of these and then you don't want to fill up the entire physical memory with just page tables
so huh we discussed a few possibilities and huh we will elaborate on that today
huh two two of the possibilities which i mentioned last time 
one was huh multilevel page table structure that is it its not a simple flat huh array 
but you- you structure it as a hierarchy it could be two level or larger levels 
in- in a simple case of two level we saw that it basically means there are several tables
huh in a tree like structure and that helps in reducing the requirement of the main memory
you also looked at the possibility of huh keeping the page table in virtual space itself
which is much larger and therefore the problem is huh eased out
[background noise]{sounds like someone coughing}	[Refer Slide Time: 04:00]

(fig., 04:06)
yeah so huh coming back to the huh organization of two level page table you have basically
[throat_clearing]
the entire virtual space divided into segments ok 
so for example suppose you have one million pages then you can structure it you can think of it as huh thousand segments 
each segment containing thousand pages
so something of that nature then for each segment you could have a much smaller page table wi- which will point to those thousand pages huh which will have information about those thousand pages
so essentially we will have huh thousand such small small tables and depending upon your requirement you will only have a few of these in the physical memory 
you don't have to have all of them and that s where the saving comes 
so you have these existing somewhere huh but at any time you need to maintain maybe a limited few in the main memory 				
so to keep track of these there as to be another table which you call as segment table and huh the entry in the segment table will point to the starting points of huh these page tables 
so it will also have indication valid bit and so on 
indicate whether that table is actually present in the memory or present in the memory
it may be rest could be lying in the disk 
[smack]
so so this huh in its functionality it is same thing as the page table ok
so you you can call this division as segments and pages or you could call it huh level one and level two ok so idea is same 
the way work it works is that huh the virtual address gets now divided into three parts
huh higher few bits will diff- will decide which segment it is then huh one field will be referral to the page and then huh byte within a page
so using the segment field you address this segment table and basically this leads you to appropriate page table within that page table you index through the page number
and then you get information about the the corresponding physical page of that

(fig., 06:23)
huh this can be seen more clearly with this diagram 
where huh i am trying to show exactly how the accesses would be made
so starting with the huh virtual address 			[Refer Slide Time: 06:32]
looking at the segment field huh you have 
imagine a register which we call segment register which as the starting address of the segment table 
so that starting address with the segment number as an offset you had these two
you you reach huh the required entry of the segment table ok
huh i uh am not showing a multiplying constant multiplying factor here 
huh suppose each segment table entry is four bytes 
then this number will have to be multiplied by four before you add it ok
[background noise]{sounds like someone coughing}
so- conceptually i am just directly putting it here huh the idea here is that segment register contains the address of huh starting point of the segment table and this tells which particular entry you want in that table
so two added with the appropriate weightage here 
huh leads you to the location of segment table which you want to refer to
so so you make an access to the segment table 
we are assuming that segment table would be in the physical memory
it is huh compared to (( )) much smaller ok
lets say if you can manage in few kilo bytes huh its not much of a storage to huh 
dedicate to this
so when you make an access to this entry huh it will either tell you that huh the corresponding page table is not present huh
we will look at that as a page fault or it will tell you where it is present
so now in case of a hit this address will be the starting address of the relevant page table 
and to that we can add the page number again with appropriate weightage here
to get the exact location of the appropriate page table entry ok
so we look up that entry now we are refer to let us say some (( )) page table and 
here again we might either be successful or we might find a page fault
if we are successful then what we are getting here is the physical page number of the word we are looking- looking for or the byte we are looking for
so so this huh would be t- huh well i should say it would be huh ok
yeah i think this is- this is physical page number concatenated with the byte address within the page
you get complete physical address which you can use to access huh instruction or data whatever you are looking for
so so these are the (( )) speaking three steps
one step is this making one access second access and third access ok
the second approach which i mentioned was huh not structuring into multiple level but huh allowing the page table to be fragmented ok
huh think of page table also as consisting of pages after all it s a some storage area some data structure huh and huh it will be spreading over several pages
so huh allow the paging mechanism to work on this itself that means only a few relevant pages or few active pages of the page table are present in the memory the rest are in the disk so effectively we are placing the page table in virtual space and to avoid huh cyclic references we (( ))that this could be in the system virtual space ok not in user virtual space in system virtual space						
[Refer Slide Time: 10:11]

(fig., 09:25)

(fig., 10:32)
huh to to access that we would need access to system page table huh
we would huh we made an assumption that huh either that table would be in the physical memory or at least the relevant part of the table would be in the physical memory
so we will start with that assumption and again look at this mechanism and little more details
so here we have two fields only this is a virtual address and huh in in the parenthesis you will notice i am putting U S or P 
so U stands for users virtual space S stands for system virtual space and P is for physical space ok
so ((- there are)) many addresses flowing in this diagram 
so just to distinguish those i have qualified these with U S or P
so starting point is virtual address which is in usel- users virtual space and it consist of page part page number and the byte number
huh notice that we are not dividing this into two parts
so whole thing represents one of the many many many pages ok
now we need to make a reference to the user page table to know about this
and huh we have the starting address of user page table here in this register
U P T stands for user page table
[Refer Slide Time: 11:40]
so its base address is available in the register ok
but now its not a physical address since you have decided to keep users page table in system virtual space this is an address but its in system virtual space it s a virtual address
so this starting address plus page offset gives us another virtual address in the system space 
which we can again look upon as a page part and a byte part ok
so so this huh sum we are getting is ((its)) again a thirty-two bit number
we we interpret this again as a page number part and a byte number part
so the page number huh is basically saying that it s a page of the system virtual space 
and if we have system page table base address 
together we can find huh pointed to the entry in system page table
now we are assuming that system base system page table base is it physical address ok
so this physical address plus page number offset we get another physical address with which we access memory and this location either tells us that there is a fault huh
what we are looking for is not present or it gives a huh page number ok
physical page number that physical page number with this byte gives us a complete physical address and huh what is this address this is the address of huh users huh page table 
which you are looking for to begin with
so actually this is huh this address which you have got is- is- is the 
[throat_clearing]
physical counter part of this virtual address
[background noise]{sounds like someone coughing}
so this virtual address as been translated by this portion into a physical address here ok
now we make access to users page table ok and get a physical page number or get a page fault
so this huh number together with byte give you another physical address
so this physical address is huh translated part of this virtual address
so you noticed that two address translation going on huh 
one in system space one in user space ok
so this translation means one offset edition and a memory look up
so so that one first one is here this is translation from virtual to physical in system virtual space and we use system page table for that
this translation huh which is huh basically that edition and this look up together are translating this virtual address into a physical address using users page table
ok so once you have that then you make a final access to da- huh data or instruction 
whatever the case is ok
so it it- it- may huh look a bit complicated if you try to look at it at a glance but if you go through it huh gradually and carefully huh 
[background noise]{sounds like someone coughing}
basically they are nothing but two address translation 
are there any questions in this 
this is something which is not clear ok so huh
[cough]
huh first translation is actually in this area ok
here is the huh virtual address in the system space 
w- what is the what is this address of
this is the address of huh user page table entry which we are looking for ok
[Refer Slide Time: 15:30]
huh s- so basically we- we want to take huh the virtual page number of user address 
and user page table base huh but unfortunately this is a virtual address
so so we cant huh use this to access memory 
so we need to translate this into a physical address by this this activity and- and then we are looking up the huh physical memory to know whether the page we are looking for here 
whether this page is present or not it- is indicated here and whether this page is present or not is indicated here ok
so this huh addition and the memory access here huh with concatenation of byte address
this part is one address translation and huh this this and this is forming another address translation ok
so huh but within as- as a- lets say as a (( )) or as a huh next level function 
we have this itself translated before we apply here ok
had had the user page table been in the physical memory then we would simply take this address and look up and proceed but huh because it is not in huh physical space its in system virtual space we need a look up in system space table ok
[throat_clearing]
so now huh suppose we use one of these two mechanism huh then where do we land up 
huh we lets lets look at the hit time ok 

(fig., 17:20)
the the miss penalty here is pretty large as i mentioned because you need to make an access to the disk huh but hopefully with very small huh miss rate or page fault rate 
we can huh reduce the effect of that 
but huh what do we do about the hit time which as i as you will notice here is huh something which is not very acceptable in a in a simple page table structure 
suppose the page table was in physical memory still we will require two physical access two access to physical memory to to read a word or to get an instruction right
so the hit time even if everything is fine huh you have to have two memory accesses made right
so so straight away we have reduce the performance by factor of two by going through this process ok whatever you may completely ((amortize)) the miss penalty ok
but but hit time is going to hit us huh its two times the physical memory access time huh
on the other hand in two level page table as well as paged page table 
you know both both these huh both these picture	[Refer Slide Time: 18:30]
you notice that there are three memory accesses which are made in the best case
{referring fig., 10:32}
one memory access second memory access third memory access 
assuming that there are no page fault ok
here also this is memory access this is memory access and that's memory access
to- to get to read one word you need to make three access
so in reality things are pretty bad huh so what do we do for this huh the solution for this is that we keep a buffer huh keep have a special memory huh as part of the C P U which keeps track of some of the recent address translation which we do ok
so it is like a cache of the page table 
so so some recently accessed entries of the page table can be preserved for future reference
again the idea of locality is what will make this successful and this huh 
this is called a translation lookaside buffer
{very faintly}some (( ))

(fig., 19:44)
ok so huh this buffer is another lookup table T L B stands for translation lookaside buffer
huh and what it as is huh basically 
[throat_clearing]
{very faintly}i think you should 
ok huh look upon this as a huh as a two column table huh given a virtual page number huh we we huh compare it with all huh entries in the first field so 
{faintly}this
this what i am calling as tag here is basically nothing but a V P N virtual page number 
so suppose you have some huh at this translation done recently and this can be captured by huh remembering the virtual page number physical page number pairs ok
so last few pairs if we store huh we we have an associative memory where we can match it with all of them in parallel huh if it is found then we don't need to go any further 
so huh imagine this as a huh associative memory where you have three things
one is a valid bit huh then there is a virtual page number field and a physical page number field
so with this given virtual page number of a of an address
we try looking at huh all these simultaneously 
if it matches somewhere we simply pick up physical page number and make access to physical memory 	[Refer Slide Time: 21:48]
if you don't find it here then of course we go through the user process of translation through page table 
so page table will always help out but huh T L B will have incomplete information but hopefully the the relevant part will be there
[throat_clearing]
so you could have a miss at this level that means you you you are now dividing the process in two steps
first T L B ok you are hopeful and you huh expect that something will be found in the T L B 
if that works out there is nothing like it the the time spent here will be huh typically 
lets say one clock cycle of C P U right 
and huh if you succeed hoping that most of the time you would then the time spent is only one access to physical memory ok 
so we have reduced the hit time from huh three or two huh time memory (( )) to one
and only huh when you fail here then you make an access here again huh this may lead to a success or a failure ok 
so that that we have seen that the failure here should be very very small
{very faintly}what's happening 
[sniffing]
[inaudible background noise]
huh ok yeah you are right 
huh if if you suppose you miss out on the T L B
and you huh go to page table 
you have a hit there
then you will update your T L B right 
so so its like huh its like a cache in principle it s a (( )) like cache 
so imagine that this is your main memory and this is your cache for that 
huh again same questions will come 
when you bringing in- bringing in a new entry in this it s a fully associative cache ok
so you have a question of which entry should be thrown off and replace by a new entry
so again you have same possibilities 
L R U huh you try to use as far as possible
so so this shows huh the overall huh flow 	[Refer Slide Time: 23:48]

(fig., 23:50)
how you will huh use T L B along with cache ok
so you start with virtual address consisting of V P N the page number and the byte 
so for example if huh address was thirty-two bit address the page number would be typically twenty bits if the if the page size is twelve bits and remaining twelve bits specify the byte within a page
so first thing you are doing is huh looking at all the tags huh in this associative memory comparing them with V P N huh so you you also check the valid bit ok
because they may be begin with some of time there may be blank entries in this huh
this d bit is huh is used to indicate dirty bit so huh dirty bit is 
dirty bit or clean bit comes from huh the the right operation 
you remember in cache also we talked about that you huh keep track of huh which cache blocks have been modified 
so that when those blocks are replaced in a (( )) huh they when they are being evicted then the memory can be operated
[inaudible background noise]{sounds like someone coughing}
so d d would mean that there is something which as been modified here ok
but huh i- its not up to date with respect to huh page table
so huh before that entry is thrown page table would need to be updated 
ok so this shows that huh comparison is made simultaneously and if somewhere it s a (( )) it s the hit and if there is a hit we read out this huh physical page number stored here
concatenate that with the byte number to get a physical address
now this physical address would be reinterpreted from the point of view of cache access
we have now huh we we divide differently into fields
there is a tag field and index field and huh number of bytes within a block
so with index field you access the cache right huh what is shown here is a huh direct map cache
but ((other)) organization also possible huh in this particular location we match the tag with a tag as part of the address and if this is the match also ((v-)) bit is set then there is a cache hit ok
so so you can have a hit or a miss at this level then hit or miss at this level
so how do we put all these together
[throat_clearing]
the overall operation where you are looking at three things 	[Refer Slide Time: 26:35]

(fig., 26:40)
there is a T L B ok there is a cache and there is a page table
in in which order do you access them and what is the consequence of huh finding a success or failure at any of these levels
as i mentioned that huh in the previous diagram 
first thing you are doing is you are doing a T L B access ok
T L B access is basically a shortcut to do in huh page table access
so if you are successful at T L B level then you don't need to go to page table
so i am showing two outcomes here huh after T L B access you notice either a success or a failure if it is a success then you can go to cache directly 
if it is a failure then you have to resort to page table access ok
lets see what happens when huh you had a success and you are looking at cache
so at cache level you may again find a hit or a miss huh if you find a hit then your job is done 
huh you you can get the information from cache itself and deliver that to C P U 
so there is no access made to physical memory at all right your your search ends at cache
if there is a miss here huh then you need to make an access to physical memory ok
so now huh what you have you already have a physical address so so you need you can go ahead and make an access to physical memory and the matter ends here 
huh the other event is that you made a T L B access but there was a failure
huh in in this case you need to go to page table huh where you might see a page fault or a hit 
if there is a hit then you get the translation done
basically it is now you are getting physical address and you need to make an access to cache now ok we don't go straight to memory 
so huh we huh go to cache and again face both these possible outcomes right
just not to clutter up i am not connected here but effectively this branch is leading us to same point here huh 
if there is huh fault here then of course we have no choice but to huh service this page fault so context switch will occur and huh page will be brought from the physical memory 
so the that will be a ((-ong)) process here ok
so so now you can see that fastest or the most desirable path is this that you have a hit at both levels no physical memory access is required 
if you come this way one physical memory access ok
if you come this way there are huh because here you are further going here 
so if you suppose have a hit here sorry if a miss here a hit here and a hit here
then you have in the process just done one physical memory access
huh but you might have a failure here and you may have to make another physical memory access so two physical memory access are made here 
huh on the other hand if you go in here then you have made one physical memory access 
then you (( )) page fault and make another memory access after that 
if you look at these three huh tests ok huh together huh each ((throws)) are two possibilities
a hit or a miss right
so total there are eight possibilities huh theoretically ok
you have T L B you have cache you have page table
ea- each will be having a miss or a hit so all put together
there are eight combinations of all this possibilities
huh question is have we covered all or we have not covered all 
just take an moment and see 	[Refer Slide Time: 30:50]
have we covered all eight cases 
[inaudible background noise]
which one um-hm ok something is not in T L B
huh no it could be in the cache an- and we have covered that (( ))
see you you may have miss at T L B level and huh eventually you 
suppose you succeed at P T level then you access cache huh 
we are taking care of both the possibilities
[throat_clearing]
[background noise]{sounds like someone coughing}
huh if you simply count huh w- what are huh
i mean its like a tree 
how many outcomes are there huh one one is leading here one is leading here 
imagine huh repetition of this part here also
so they are they are two leaves there and one leaf here
[tapping]
so we have one two three four five out of eight basically huh if you want to look at it as huh a tree leading to eight outcome we have shown only five
so what is missing here what is missing huh there is nothing missing huh yes yeah 
i- if it is not in the page table huh it is not in the cache
so we are assuming that inclusive property here suppose we assume lets assume inclusive property that huh cache will not anything which is not there in the physical memory ok
what we have in cache is subset of what is there in the physical memory
so with that assumption some huh of those eight get ruled out
that means huh a- a miss at P T and a hit at cache tha- that combination gets ruled out 
so whi- which i actually want two combinations huh miss at P T hit at cache and T L B either a miss or a hit
so tho- those two are huh huh lets say non-existent so that s leaves six huh do we still have something missing huh ((ye- yes)) we should account for that what is missing
yeah see when when there is a huh hit in the T L B huh the- there cant be miss in P T ok 
T L B is also huh eventually subset of huh P T 
so so T L B wont be holding an entry which is not there in huh P T right
[background noise]{sounds like someone coughing}
so so that that combination is also huh ruled out
{faintly}ok so i think we can think little deeper here 
[smack]
huh now so far we assumed that before you hit the cache before you go to the cache 
you would need to translate huh virtual address to physical address
that means huh as far as cache is concerned cache is organized on the basis of physical addresses cache is organized as part of the physical memory and not as a part of the huh not as huh a cache of the virtual memory but huh in principle we can do that also
huh we- we can we can straight away access the cache by virtual address and all that is required is that the tags which you store in- in the cache should come out of the virtual address and not physical address ok
so that s huh quite possible and if that is the case huh the first thing you will access is cache and not the T L B ok
[Refer Slide Time: 36:04]

(fig., 35:50)

str- straight away looking at the virtual address you can make an access to the cache
and only if there is a cache miss huh then you need to go to huh main memory and before you go to main memory you need a physical address from either T L B or if not there from page table
so so order could be reversed huh what do we gain by that huh what- what we gain by that is that the best case path is short only ok
in- in this case huh the the best possibility which we hope will occur most of the time is this whe- where we are going through the sequence of two things
T L B access and cache access
if we on the other hand reverse this and we get cache ((hit)) for the first time huh then there is nothing else we need to do and that s the fastest possible approach
so the- there is huh a merit in trying to do that huh but this comes with some problems
huh of course there are solutions although they huh some of them cost somewhat 
huh but it is possible and it is done
so huh the problem is that of huh aliasing bu- but first of all let me huh mention that huh
to achieve this you need to distinguish huh entries corresponding to different processes
because huh you have same virtual address space huh or lets say it is numbered same way
you have processed a you have processed b
for- for process a you say that virtual address goes from zero to four G B 
for process b also you say that virtual address goes from zero to four G B
so now in cache huh can you have entries corresponding to multiple processes
i- i- if you have it in the straight forward manner there will be confusion
huh ho- how would you know that huh a process which is taking its virtual address 
and trying to access cache is not huh seeing the virtual address not seeing a data of another process huh either we distinguish huh by by extending another field that in- in the cache 
we have data we have tag we also put process I D 
we have another field were we huh distinguish the entries for in the cache corresponding to various processes so that s a extra overhead we need to incur either we do that or we see that when huh context switch occurs 
when you go from one process to other process you start with clearing up the cache 
ok you empty the cache 	[Refer Slide Time: 38:55]
start a fresh and there is no confusion huh at all
the- the only thing you might lose by doing so is that huh if these two processes genuinely shared some some data and huh part part of that data was there in the cache at that time
you will lose it and we will have to load it again
so so that that is the drawback
huh but otherwise if you are willing to incur little more cost spend more bits in the cache and the corresponding hardware you can distinguish the entries corresponding to different process in the cache huh having done that we still have problem of aliasing 
aliasing means that you have huh shared object 
[smack]
which might be looked upon as two different virtual addresses huh lets (( )) shared data structure
in process a virtual space it is lying somewhere process b virtual space it is lying somewhere else and huh depending upon how these processes are running
huh they may get mapped to huh two different areas in the cache and it may happen that two copies exist ok because this virtual address gets translated to some address here
this gets translated to this gets mapped to some other address in the cache
and if two copies are there of the same thing then there is a inconsistency
one process may update one copy other may not come to know of it 
it may read a wrong copy
[throat_clearing]
so huh so this is aliasing (( )) solution of this the huh more (( )) involved solution 
which can eliminate this kind of problem 
the- there is another approach which is uh some somewhat in between virtually address cache and physically address cache and it it tries to get best of both the ((words))
huh we we call it physically tagged and virtually indexed cache that means
{faintly}i think (( )) let me explain (( )) reference to 
this diagram 
{referring fig., 23:50}
[throat_clearing]
huh if you look at this picture huh cache requires two parts of the address
one is used as an index and the second is used as a tag
so huh i- its not difficult to imagine that what you need first is an index ok you- you first need index to get to this point and then you read out and make a comparison
huh now which one of these two is dependent upon the address translation process in- in the in the given picture huh P P N is covering all the tag bits and a few bits of index ok
so we neither know tag nor we know index fully unless we have found out P P N for the given V P N 
so here we cannot proceed with cache access unless we have done the translation 
but suppose the picture was slightly different that is huh this huh huh let us say huh
the the tag was larger or P P N was smaller that is index bits were independent of P P N ok what it means that huh 
huh the the degree of associativity and the number of entries in the cache the page size
all these parameters organized in such a manner that huh the index field does not overlap with P P N field in that case huh index can be picked out from virtual address itself
that is one part 
[Refer Slide Time: 42:42]
see in the address translation this part doesn t through any modification
its being taken straight away 
it is this part V P N part which is getting translated into P P N
so if the the bit the field size are such that huh index can be picked out of the non-changing part then you can start with the cache access immediately
whe- whether there is a T L B access completed or not you can start with the cache access 
and by the time you- are getting this here
hopefully you have done the translation in T L B and you are ready for comparison
so here huh it is physically tagged and virtually indexed cache 
so- so that means the the tag part is coming from the physical address huh 
but the index is coming from the virtual address huh or any infact it huh
it s a bit mis-normal because the index part is same in virtual and physical address
alright its coming we are picking up from virtual address
but huh this part even if picked from the physical address would be unchanged
so- so that that s a huh very huh inexpensive arrangement 
{faintly}and (( )) 
finally let me huh
talk little bit about huh what is the other role in this organization place
ok we we primarily started with using virtual memory for expanding the address space
huh in spite of limited physical memory 

(fig., 44:18)
but huh another thing which is doing is 
is helping in huh putting the protection in place protection means that we we don't want one process to have unauthorized access to another processes huh data area or ins- instruction area 
also huh user processes should be able to access restricted part in the systems area
so now since a process is able to access only those physical pages which are pointed to by its page table huh we have some protection inherently there ok 
a program huh is allowed not- not to go to huh physical memory directly
it as to go through this transition process and therefore it can go to only those areas
huh which huh which- which are actually permitted by the page table and huh user program as no direct control over page table
[Refer Slide Time: 45:15]
page table gets initialized by operating system 
and they get updated automatically
huh huh as- huh pages are brought in or swapped out huh updation of the page table takes place automatically 
user process can do not have explicit access to that and therefore the protection gets maintained
huh so so this requires that huh a processor supports two modes of execution 
the kernel mode or supervisor mode is one which which as all accesses all permissions 
[noise]
and a user mode where a few critical things are not permitted ok
one of the thing you can say is that huh initializing the page table
so t- the will be a special instructions which which are not accessible in user mode
they are accessible only in kernel mode and how does this switching happens
so huh for example huh lets say user program is running huh ok
first of all the- there will be some register some status register in the processor huh
where a bit will be set to indicate whether it is a huh currently a user mode or kernel mode alright and huh naturally user program will not be allowed to set a reset that way
its only (( )) which can be there
so so when let us say when user program is running that bit is reset that means huh its running in user mode huh when it makes a system call 
th- th- that time huh this bit will be set that means wh- when i when you are transiting from user program to huh system call you are going into system area 
you are executing code which is part of a (( )) and then you need to come back
so at these two points huh the bit will flip ok at the end of it before you return to the user
again you will reset this and come back so 
[throat_clearing]
u- user doesn t get control over that particular bit and user cannot change the mode
so huh that- that is ba- that is the basis of huh
{faintly}huh is protection
so ultimately actually huh this is protection or security is a very huh deep huh multilayered mechanism and you can see what is at the bottom of everything bottom of everything is availability of these two modes and restriction on some basic operations
so from that operating system data ((exits)) security huh from that security huh
the application programs are huh communication processes will ((derive)) their security
at the network level and at that 
{faintly}(( )) layers
ok so let me close at this point and to summarize
huh we began today by looking at 
[noise]
huh the techniques to handle the issue of large page table size
we looked at two level structure and paged page table structure 
we we noticed that huh the the page table huh based approach ask us to have at least two memory accesses before you get to huh the physical data 
but huh introducing these strategies makes it huh see memory accesses and to reduce this 
basically the- this three level three memory accesses means your hit time is very high
to improve on that huh the idea of T L B is introduced 
[Refer Slide Time: 49: 02]

(fig., 49:00)
which is a cache for the page table 
then we saw how huh T L B and cache work together 
then we saw everything put together T L B cache page table 
what are the possible events and how one could sequence through these
we also looked at the possibility of accessing cache directly by virtual addresses
and there are advantages and there are problems
finally we saw a memory protection as an additional important role 
which is ((played)) by virtual memory mechanism 
that s all for today thank you 	[Refer Slide Time: 49:40]




Educational Technology Services Centre
IIT DELHI

Computer Architecture

Lecture #33
Input/Output Subsystem: Introduction

huh today we are going to begin with the last topic of this course namely input output subsystem 
we have so far talked about processor and memory 
processor is the central component of the entire system huh where the programs are executed 
memory is essential in order to hold the program and data on which huh some instruction operate 
and huh input output is important to connect the computer to the rest of the world
if you do not any means of input and output no information can enter huh the memory or the processor and no information can come out 
so this is essential to make it useful because huh after all computation involves that you have some data you process and produce the results

(fig., 01:48)
so ultimately you need input and output 
we are going to look at various aspects of (( )) subsystem huh we will talk about performance considerations huh particularly of I O system 
we will talk of peripheral devices which are the extremities of the computer system huh where the data gets transformed into a form which is understandable by huh others could be (( )) could be the environment
all other information is got in the form which is understood outside into the computer 
in form of huh ones and zeroes then we will talk about interfaces and the buses
these are the means to connect the input output huh devices or peripherals to rest of the system 
and finally we will huh look at the operational aspect how you huh input output is carried out 
so our focus today would be on the first two aspects
[clicking][throat_clearing]
huh I O performance huh is contributed to the overall performance but it often gets neglected huh one would talk of huh speeding up the processor executing more and more instruction anytime
[Refer Slide Time: 03:03]

(fig., 02:54)

making access to memory fast but if you neglect I O huh your benefit of performance improvements may not really fully effective
so consider for example 
[throat_clearing]
that you have some benchmark program which takes hundred seconds and all
out of which the C P U consume ninety percent of the time and huh ten percent of the time is taken for I O that means the data is brought in results are sent out and huh we know that by technological innovations huh the speed of processor is continuously improving and huh if we have fifty percent improvement every year 
so over a period of five years we could see that this ninety seconds which huh the C P U  consume would get reduced to sixty forty twenty-seven eighteen and twelve leading to an overall improvement by factor of seven and a half time
huh now on the other hand suppose I O performance remains unchanged then what is the effect of this on the overall performance 
so the total time huh is what we for the C P U and ten seconds of the I O remain unchanged 
((if the)) the total figure would be hundred next state goes to seventy next state goes to fifty and so on and finally we have twenty-two after five years
so now if you look at the ratio of hundred and twenty-two 
its only a four point five times improvement 
so huh not improving I O performance could bring down your expected performance improvements
huh so- so this this was the case where the program is computation bound
computation bound means that majority of the time is taken in
performing computation arithmetic logic decision so on and there is a comparatively much small amount of I O
on the other hand there are situations which are I O bound 
where process- processing is very little
its essentially huh input and output
if you if you look at huh scientific computation huh 
lets say weather prediction or nuclear modeling and so on
[Refer Slide Time: 05:15]
here I O is very little and most of the time it is number crunching or huh heavy computation 
but in business data processing huh or- or database oriented application it 
it s a often most of the time gets spent in input and output
so huh if if the lets say ratios were reversed and its only ten percent part which you improving then obviously there is not much improvement which gets ((ramped))
so it huh its important to take care of I O performance improvement also
huh you know to improve the overall performance 

(fig., 05:55)
huh the next question is 
how do you define I O performance 
what do you mean by I O performance 
huh as we saw in case of C P U depending upon your perspective your requirement
huh the- there may be different definition of what is I O performance 
it could be throughput oriented measure that means huh how much of input output gets done
how much of data transfer gets done per unit time
huh one could talk of amount of data transfer in unit time like kilo bytes per second or megabyte per second 
or you could talk of number of I O operations per unit time ok
each I O operation may involve some transfer could be small could be few bytes 
could be few kilo bytes and we we want to huh 
lets say maximize how many such huh transfers take place
on the other hand uh-huh it may be the response time which is important
so from the time you huh you take some action you give some command and huh
it takes to respond to that 
so the- huh there may be environments where that may be more important
there could also be cases were both are important
ok for example if you take huh reservation system for railways or airlines 
[smack]
so as- as a as a user as an individual user huh you you want your job to be done as fast as possible
once your requirements are fed in you want your system to respond immediately 
so huh response of course would involve computation as well as input output 
on the other hand huh i- if you look at the whole system 	
[Refer Slide Time: 07:25]
then then there is a central database somewhere where huh request are huh being sent in and it is responding 
so at that point at- at the point of the central database huh it is a throughput of I O huh
which might become bottleneck and and you may be huh trying to improve that
so huh at different in different parts of the system or with different perspectives 
the huh consideration for I O performance may be different
f- if if you are interested in
lets say total time the program takes to execute right 
as we had seen for processor then clearly you can divide the total time between the time which huh is taken for execution for computation and the time for taking in the data 
or sending out the data
huh so now ba- based on these considerations suppose you look at a super computing application or typically huh scientific application 

(fig., 08:32)
there huh 
[background noise]{sounds like someone coughing}
you huh it may be typically a computation bound application
but huh they also may be at time lots of results or simulation results which may have to be printed out
so here it is not the response time but again the data throughput which is important
so which may be measured in kilo bytes or megabytes per second
huh transaction processing for example lets say A T M bank A T M
so here huh transaction per second huh is important from the overall system point of view
and again huh for a person when you feed in a request for drawing some money
you want it to respond immediately
so response time and transactions per second both are important
huh take example of a file server 
suppose you have a cluster of computers as we have in our department and huh the- there is one of the machines which is acting as a file server maybe ((or)) homes are there ok
so this this file server would receive request for creating files opening files reading writing and so on
each each such request huh is essentially an I O operation ok
so- your your concern would be how many I O operations 	[Refer Slide Time: 09:48]
the system is able to sustain and that could be huh one thing ((you)) would like to look at
so so depending upon huh situation you you may describe your huh I O consideration 
I O- I O performance in a huh
in terms of some some operations or some ((data transfer)) per second 
or in terms of time it takes to respond to you or some combination of both

(fig., 10:18)
huh while talking of I O performance 
huh one must keep in mind that often there could be a discrepancy in terms of units
i- then we have been talking of memory we- we talk of k b m b and so on
once again here we are talking of kilo bytes per second megabytes per second
huh but typically when you talk of memory size huh you you mean huh one kilo byte 
huh as one zero two four bytes or two is power ten
similarly one megabyte is huh one zero two four kilo bytes or huh one zero four eight five seven six bytes which are basically is a power of two huh tenth power of two and 
{faintly}(( )) twentieth it is twentieth power of two
[throat_clearing]
ok whereas in I O huh you you are talking of thousands and millions in huh in the real sense
so one k b per second here is thousand bytes per second or tens power three and similarly one megabyte per second is thousand kilo bytes per second or one million bytes per second
so huh approximately they are same ok but somewhere if you want to be little more accurate
one must keep in mind that huh there may be a dif- difference 
you know the language one is talking in memory domain or in processor domain
may be different from what you are talking of in I O domain
so huh this is just a word of caution
[throat_clearing][smack]
so now lets huh move towards huh peripheral devices before we do that lets look at the overall huh system how we huh conceive a stored program computed typically
{referring fig., 12:02}
this is a classical block diagram showing five units of a computer huh which probably you would have seen in your school days also
so you have control arithmetic memory input and output right and huh they are connected to each other
[Refer Slide Time: 12:15]

(fig., 12:02)

so control and arithmetic huh are huh the the data path as been mentioned here 
huh these form the central processing unit or the central processor
so we are talking of this part which is input and output alright

(fig., 12:35)
huh elaborating it further huh
this is how the picture looks like
i- am also bringing in huh two levels of memory hierarchy here
there is a cache memory and main memory and huh the interconnection 
i am not showing point to point here -like in the previous diagram 
here we put it as some huh some something in-between 
the- the processor memory and the I O ok
what what is shown at the bottom is the all I O subsystem 
and somehow they are interconnected 
we will elaborate on this huh in subsequent lectures 
[background noise]{sounds like someone coughing}
but notice here that 
[throat_clearing]
[Refer Slide Time: 13:13]
there are two things involved here in I O subsystem 
they are I O devices or peripheral devices ok and they are all I O controllers which huh
which ((sit)) between these devices and rest of the system 
so a- a controller may be taking care of multiple devices also
that possibility is ((also shown)) or a controller may take care of a single device 
so so we need to understand huh what these devices and controllers are 
and we we will have some discussion on this today
and in subsequent lectures we will talk of how these get connected 
so huh we will talk of buses and huh other means of connection and then finally we talk of how information goes across between huh processor and these devices
{faintly}or memory and these devices
[smack]
ok we we will focus our attention on devices and controller
so you you may often not see these as separate things ok
huh physically the controller and device may be all packaged together 
so so roughly speaking that devices the the actual ((- or so to say)) which converts information from one form to other form
ok so lets say printer you send it some bits and bytes at some point its getting converted to a printed page huh b- but there is something between huh the processor and buses
on my hand and huh and this printing mechanism 
so the- there is a controller wh- which takes care of movement of data ((sometime -)) conversion of ((data in some form))

(fig., 15:02)
so there is a wide variety of peripheral devices 
and you can try to classify them in many different ways
so yo- you could huh for example talk about their behavior in terms of whether they 
huh input that means bring information into processor in memory combination 
or take information out or play both the roles ok
so that that is one aspect huh for example keyboard is an input device 
printer is an output device 
huh the second issue is who is the partner 
with whom are these devices trying to communicate
or huh with whom huh 					[Refer Slide Time: 15:38]
do these devices make processor talk to
so huh it could be at the other end it could be a human programmer human operator
or there could be another computer another machine or the devices could let the computer talk to the environment in general not a neither a machine nor huh a human being
or they could be simply huh some kind of internal devices
in the sense that they only store information disk ((drive)) for example right 
so so let me give example of each of these 
huh keyboard is for example for huh human huh operators ok
huh network controller will connect one machine to another machine
huh environment huh that s typically (( )) you could have a sensor for example sensing temperature or huh computer trying to turn a motor
huh storage example is huh a disk drive
then thirdly huh they could be characterized in terms of the kind of data huh
which is moving across
in terms of either its speed some devices very slow some are very fast some could be medium
of course this is very huh (( )) classification
one- one could specify the rate of transfer ok how many bytes or kilo bytes or megabytes getting huh the device is capable of transferring
so huh depending upon how you organize the whole system huh that capability may not be met may be met or may not be met
so suppose huh there is a disk which is capable at the rate of ten megabytes per second
huh you have to organize rest of your system so that this ca- ca- capacity gets utilized 
if you don't huh disk may be ready to transfer data 
{faintly}but you may not be willing to take it
huh ho- how much of data gets transferred in in one chunk one quantum
[background noise]{sounds like someone coughing}
so it could be serial data which means information goes bit by bit ok
as- as it goes on the network for example
or it could be parallel data which which may be eight bits sixteen bits or
{faintly}huh it could be (( )) 
how it is encoded ok
so so you have eight bits going what does huh one group of eight bit means 
so so different devices may have different kind of encoding and huh whole variety exist there
[throat_clearing]
huh now b- before we talk of huh the current situation 
huh let let me recall huh how uh a set of peripheral devices look like huh years ago
so- so this is the story of this is the picture of huh ((I I T Delhi)) which i am trying to huh paint huh when huh i was exposed to computer for the first time
[cough]
so we we had huh not in the present position there was a computer called I C L nineteen ((no)) nine it was somewhere on ((e or f -)) 
and huh the only devices only peripheral device at hand was a paper tape reader and paper tape punch huh
huh the paper tape basically was just a strip of paper with holes punched on it
and huh group of holes form one character 
[Refer Slide Time: 19:08]

(fig., 21:02)

so huh the the way one could input information was to use a device called huh a typewriter ((like)) device 
typewriter like device called (( ))
basically could think of a typewriter wi- with a huh lot of (( )) of tape attached at the end at one end
so as- as you type information could be punched on that and similarly if you run a punch tape it could also print out o- on a sheet of paper
so huh as far as computer is concerned that directly connected peripherals are paper tape reader and paper tape punch
so eve- everything huh everything was in the form of paper tape
the outputting system the compiler huh the the user program the the library 
so everything had to be fed one by one and finally you will
your results will be punched on paper tape and huh so so the larger room huh (( )) of the size and a smaller room 
in the smaller room there were a couple of these typewriter like devices
there you could huh prepare a programs and when results come back you- you- you print them out here 
so huh of course couple of years later the huh ((additional device introduced)) magnetic tape 
where program and data could be stored for repeated use and this was of course much faster 
huh the paper tape was just the reader was a little faster sometimes
three hundred character per second 
(( )) only some ten twenty character per second and a line printer 
so li- line printer huh was something like huh (( )) three hundred lines huh per minute
so it could print one line at a time 
[cough]
so so that make life little easier and subsequently huh tape got replaced by punched cards 
and (( )) at that time the system also moved to the current location
[throat_clearing]
so punch card have a advantage at its like a deck of cards
each card basically is one line of your program ok
[Refer Slide Time: 21:21]
one- one statement 
so if you want to add it you could take out a few cards and you put put back huh new cards
you- you could you could add it to your program like that
huh you cant change a card b- but you in the deck you can take out a few cards
you can insert a few cards
of course editing in paper tape environment huh was huh much much more painful
you you will have to cut the tape and patch it with something another piece 
so obviously its not surprising that people huh some people could (( )) look at the tape and read it
{faintly}look look through the punched holes
huh disk drive was- again added convenience which was of course much faster then huh tape 
the tape goes absolutely serially 
whereas in in disk you can reach a particular track and then 
of course when in a track you are sequential but huh you can huh you don't have to go from one track to other track by going through one track completely
tape is entirely sequential huh
but that disk capacity of those ((things)) were something like huh twenty megabytes was there 
[background noise]{sounds like someone coughing}
was was the big huh (( ))
huh obviously because me- memory was huh thirty-two kilo bytes or sixty-four kilo bytes 
that order 
[background noise]{sounds like someone coughing}

(fig., 22:48)
ok now we fast forward into the modern ((area))
we have whole variety of peripherals and huh l- let me list some of these huh with huh kind of classification we did earlier
so lets look at the peripherals you have for introduction with human beings ok 
human programmer or operator
so the important thing here is to huh i mean you
i am sure you are familiar with all the names which i am putting here
huh but its important at this stage to get a feel of what kind of data rate or throughput rate 
huh these peripherals are dealing with
[Refer Slide Time: 23:25]
(( )) because when you design a complete I O system you should have huh a feel of these things
so huh devices like keyboard and mouse huh are are limited by huh the the rate at which you can type 
so it- its only a fraction of kilo bytes or basically a few bytes per second is- the rate at which you can feed information and and that s what you require 
so uh i mean these are designs to take care of (( )) the fastest huh ((typist))
we don't need anything faster than 
huh you see here voice input and voice output 
so huh the trend is towards huh developing these type of capabilities voice input voice output huh pictorial input pictorial output
huh wh- wh- which are which are more natural to human being
instead of typing if you can talk to computer it is definitely much nicer
but i- its not just a issue of peripheral device 
its also a matter of the recognition of huh what is being spoken or generating speech outputs
huh we are not worrying about we are not concentrating on that part huh but
[throat_clearing]
the the voice input huh voice would be huh digitized and huh brought to the computer in the form of a sequence of samples
so of course the the figures given here huh these are from your textbook are huh point zero two bytes per second or huh twenty bytes per second
huh this this is a huh this is the rate of information huh if if it is compressed
but a typically the speech huh inputs are sampled at least at rate of ten kilo ten sample per second 
{faintly}(( ))sample is huh 
o- one byte so it as to be at least huh ten kilo bytes per second not not twenty bytes per second 
[cough]
similarly huh a plain speech would be against typically sampled eight to ten kilo bytes per second 
huh if you are talking of ((music)) output that is sampled at much higher rate 
so typically fort- in the range of forty fifty huh thousand samples per second
or so many kilo bytes per second
huh then another input device is scanner which can basically scan huh text or image 
whatever is there in ((sheet of)) paper
and huh depending upon the the size and the resolution huh you you may have different data rates 
so huh th- the resolution or the accuracy here measured in terms of dots per inch or D P I
s- so you have scanner which are huh six hundred D P I or twelve hundred D P I
and so on
th- that means huh how closely or how finely the information can be resolvedso if this D P I value is lower then you will get a huh very huh sort of (( )) picture
huh if you have higher D P I then then you get a finer shaper picture
[smack]
huh we have variety of output devices line printers laser printers huh graphic display 
as you would notice graphic display is a highly demanding huh peripheral
so shown as sixty thousand kilo bytes per second or sixty megabytes per second
why is it is it so high because it again depends upon the the resolution
on- on a screen suppose huh roughly speaking 	    [Refer Slide Time: 27:24]
suppose you have thousand by thousand pixels
so it could be more accurately thousand twenty-four by seven sixty or whatever
so so as an approximate calculation suppose you have one million pixels on the screen 
huh then the question is how how much how many bits are required to ((re-)) each pixel
so you you would have seen color settings so huh yo- 
you could use one byte to represent one pixel
so or you have twenty-four bits in in huh if you are going for a good quality
so that means three bytes per pixel ok and huh one million pixel means huh three million bytes 
is- is the information present on screen
and for huh for a persistent display huh this needs to be repeated refreshed
so that you you get a persistent picture 
so the rate of repetition could be something like twenty-five huh frame per second or fifty frame per second or sixty frame per second 
so so if you if you take that into account huh you you realize that it s a really a ((very)) highly demanding huh peripheral device
[throat_clearing]
then you have a variety of huh peripheral devices which connect a computer to a network of computers 
so- so you have various kinds of modems i- it could be fax modem huh cable modem
cable modem connects to huh a T V video cable ok
fax modem connects to a telephone line

(fig., 28:54)
[background noise]{sounds like someone coughing}
A D S L modem also connects to a telephone line huh which is capable of carrying digital information 
huh on the other hand you could have huh LAN adapters
local area network adapters wired or wireless
so so these could be huh the- these are typically input and output both ok
and the data rates huh for LANs are higher huh depending upon whether it s a huh ten megabytes per second LAN or hundred megabytes per second LAN or one gigabytes per second LAN
so typi- these are three common standards			[Refer Slide Time: 29:44]
huh so so many bits per second can be easily translated into bytes per second by reducing one order for example 
[throat_clearing]
huh we similarly you have modems huh in a wide variety of speeds 
huh th- the the slowest one would be a fax modem which works on telephone lines
so typically they are fifty-six kilo bauds is fifty-six kilo bits per second
huh you might have heard the term baud b a u d 
so baud is same thing as bits per second huh it is a unit used in terms of communication huh specifying the rates in terms of bits per second 
so fifty-six kilo bauds are fifty-six kilo bits per second
[throat_clearing]
example of huh peripheral devices for storage huh floppy disk drive  optical disk or C D ROM and D V Ds ok their storage is in the form of optical 
so huh on- on a disk you have portions which are huh opaque which are huh
portions which are transparent and depending upon how finely you can resolve 
how closely you can space huh the the huh opaque region and transparent regions
you you will get more bits per huh unit area
and huh that will also determine how how much- of data can be read out as huh
the medium moves
((migrated)) tape huh as i mentioned earlier we have sequential devices so information is stored from one end to other end in serial form and they are for archival purpose for long time storage where you are not reading and writing very often

(fig., 31:33)
huh among these disks are the fastest ok
as you can see that the data rates could be huh varying from something like hundred kilo bytes per second of floppy drive to huh as much as ten thousand kilo bytes per second or ten megabytes per second of huh hard disk drive
[cough]
huh another huh (( )) you can add to this list is huh is ((these)) flash memories or or the pen drive U S B drive the kind which on which i am carrying my lecture and plugged into huh the U S B port 
so so these are there is no moving part in that which basically huh a semiconductor memory
[Refer Slide Time: 32:15]
which is non-volatile type called flash memory and huh i- it is comparatively 
{faintly}what would be the transfer rate 
{faintly}i think it will be 
huh i- i- it s a its slightly slower than 
{faintly}magnetic disk
{faintly}i do not have the exact figure
huh lets look little bit deeper into some of the peripherals
huh so this is a schematic of a hard disk drive 
huh so basically it s a it s a group of rotating (( )) platters as they are called
huh this picture shows two platters ok
so these these are rotating disks where on the surface information is stored
huh each platter typically is capable of storing information in both sides ok

(fig., 33:00)
so you can 
the- there is some coating which is magnetically sensitive 
and it is on that huh one one zero gets stored in the form of polarity or magnetization 
so there is a huh huh there is a set of read write heads which are mounted on an arm huh
here it is pivoted o- on this huh and it it rotates around this 
[background noise]{sounds like someone coughing}
by that rotation this end which is the head can move closer to the center or move towards the circumference towards the peripheral
huh you you have circular tracks on the disk ok
[background noise]{sounds like someone coughing}
very closely ((spaced)) circular tracks and huh these heads get positioned over huh some tracks
so all these heads move together so so at one point of time
lets say huh all the heads are positioned on track one of huh that particular huh platter
then you you move them they could be positioned on
all of them could be positioned on their track number ten and so on
so so there is a movement of head from track to track
[background noise]{sounds like someone coughing}
and the movement of disk is rotary
so if you need to reach some point in the huh some some information which is stored somewhere here						[Refer Slide Time: 34:48]
then this part as to rotate and come in contact with the head and head also 
effectively this moves ((radically)) and it as to get positioned at that - that point 
so what's that is done by information can be recorded or read out 

(fig., 35:15)

so this this is the real picture of a huh hard disk drive huh
so so here you can see four platters ok
so there will be eight surfaces eight recording surfaces and huh this is the head 
this is the head assembly and the huh this is sorry
this is the arm and the head is actually close to the ((tip))
so huh it is huh pivoted around this point and so- so this head will move outside outwards or inwards move from track to track
the rotational speeds are typically huh the- they- they follow certain standards
so huh standards these keep changing huh thirty-six hundred revolution per minute
or from that it change to fifty-four hundred revolution per minute
or seventy-two hundred revolution per minute 

(fig., 36:11)
this example is of fifteen point three thousand 
or fif- fifteen thousand three hundred revolutions per second 
[Refer Slide Time: 36:18]
so example of huh disk from sea gate
so huh fifteen point fifteen k is the huh highest speed you find these days commercially
and huh that- that decides how fast you can get the data ok
huh it inferences both the time it takes to reach the initial point and the time to transfer 
(( )) when the disk is rotating huh what's your position ahead at the right point 
huh the rate at which it rotates determines the rate at which you are encountering the data
and that s the rate of transfer
huh this as total capacity of huh seventy-three G B speed is fifteen k r p m
huh seek time seek time the the time to reach the desired point 
which could be more or which could be small
so if you are already close to that it may take less timeif you sitting far away then it may take more time
so three point six is average
huh the the interface also huh follows certain standards and this standard is called ultra three two zero S C S I 
we will talk of these little more little more later and this is characterized by the rate of transfer three twenty megabytes per second
so so this is the rate sustained by the interface huh th- that interface on that interface depending upon what disk you connect and what speed it is rotating at huh
you you will have actual data generated at a different rate
so so this is not a example of huh very high capacitors but a fast disk

(fig., 38:00)
the next one is huh
somewhat slower about half the speed huh but capacity is larger so capacity
{faintly}i think it will come on the next part
{referring fig., 38:15}
this is the huh two fifty G B disk ok
[rustling]
eight M B huh buffer size
which means huh the the data doesn t go to huh the processor or memory directly 
its first brought into a local memory which is of size eight megabytes ok
and then from there it is transferred
average latency huh this is the rotational latency 
so huh the the the time to reach a particular point is determined by huh two things
one is the time taken for rotation and second is time for the head to go from one track to other track right 
so huh the the rotational latency in this case four point two millisecond and huh this can be determined if you take the R P M figured
how many revolutions per minute 
you can find the time for one revolution and if you take half of that 
that is typically taken as average huh rotational latency ok
so if you do that arithmetic you will get four point two milliseconds huh the seek time is huh ok we can look at this here
track to track seek time is moving from one track to other track is two milliseconds 
and huh ((this is)) if you move one track only
but on the average ((there is some average figure)) given
it is for reading eight point nine milliseconds and writing ten point nine milliseconds
so so basically a huh the time involved to to reach the first piece of data is a (( )) several milliseconds ok and huh the time to transfer the data is much faster
once you reach that point then things are faster 
huh so so this is the buffer to disk transfer time between the disk and the buffer
huh this is seven thirty-seven mega bits per second maximum 
this is dependant upon the rotational speed ok and buffer to host huh here it s a different interface E I D E 
huh in the previous there was a different interface 
this takes hundred megabyte per second
so here huh the the peek of the transfer between disk and buffer is much higher
huh but what happens is that huh the the disk is not always busy in transferring
it is at time busy in seeking also
so- so therefore this is this figure is ok 
o- initially one might think that disk is faster but you have arranged for a slower interface 
but but disk cannot be continuously seeking it as to s- 
this is continuously transferring 
it huh in-between it spends time in moving to huh the desired point 
[Refer Slide Time: 41:04]

(fig., 38:15)
so continuing huh with that huh there are some parameters which define the storage layout
huh cylinder is actually huh collection of all the tracks which are huh sort of coinciding on different platters
so suppose you have four platters on huh you talk of tenth track on each of these
so all this together will form huh what is called a cylinder
so of- often huh you do not write in terms of number of tracks you talk of total number of cylinders so- this is one figure 
the total number of heads huh each surface is accessed by different head
huh then number of each track is divided into number of sectors
well this is from the western digital internet site
but i think there is a there is something wrong in this figure because it doesn t tell you the rest huh number of sectors per track typically are few hundreds
[cough]
huh total capacity is two fifty G B 
huh bytes per sector is five hundred twelve so half kilo byte is each sector
[throat_clearing]

(fig., 42:25)
huh lets huh
so so this was one of very important peripheral device that disk huh
well sometime it is not considered as an I O device 
huh some people try to characterize this as a storage device 
ok but all the same the the way huh a processor memory have to communicate with disk is something like an I O device
[background noise]{sounds like someone coughing}
huh this is an example of an L C D monitor huh from Samsung 
{faintly}different views are shown
huh interesting thing is this view which shows how thin it is unlike huh 
well this is also L C D monitor 
huh i- you have C R T monitors were which are very bulky
[throat_clearing]
so you can get some idea of the resolution here 
so (( )) the picture is really very high resolution
[Refer Slide Time: 43:24]

(fig., 43:25)
nineteen twenty into twelve hundred and huh the viewing angle ok
so often L C D monitors huh may have limited viewing angle
if you look from side you may have problem in seeing but this one is one seventy degree so you can see almost one eighty
[background noise]{sounds like someone coughing}
then huh these are wireless LAN adapters ok

(fig., 43:57)
huh so so this one is huh one which connects to huh U S B 
U S B is a serial bus
we- we will talk of these term little more later and th- so this this connects ((external into)) the computer this goes inside the computer this is this connects to a P C I bus
again we will talk of this term little later 
{referring fig., 44:24}
huh graphics card so th- these are the cards which drive really high resolution monitors and can huh they are able to show you huh changing scenes very conveniently 
{referring fig., 44:40}
scanners scanners are characterized by their resolution in D P I as i mentioned
(( )) D P I huh the number of colors they can resolve  	[Refer Slide Time: 44:51]

(fig., 44:24)

(fig., 44:40)
so forty-eight bit color six bytes 
huh the- these scanners have a document feeder attached
that means you can put a bunch of papers and they can huh automatically feed like a photocopy machine if you have seen (( )) they are feeded
{referring fig., 45:11}
huh inkjet printers 
they are very cost effective and huh quality is continuously improving
huh they also characterize by the number of dots per inch
that that is the printing quality and the number of colors they can resolve
huh i don't have those figures here 
huh the- these have huh direct network interface so so that it can be shared over 
{faintly}in network
{referring fig., 45:50}
huh well sorry for this picture
my idea was to show you a little close up of an inkjet printing mech- huh printer mechanism 
huh if you can see here in this one huh
the paper gets set from the top and goes through this area where printing gets done
[Refer Slide Time: 46:11]

(fig., 45:11)

(fig., 45:50)
and then moves out here
huh the- they are ink cartridges here and the the key thing here is this ink cartridge ok
the the crucial mechanism (( )) here
[noise]
rest of it is essentially paper feed and huh movement of this and interface
but huh th- the new technological breakthrough which as happened over last couple of years are in this
[noise][background noise]{sounds like someone coughing}
so how huh ink ve- very fine drops of ink can be huh 
huh thrown on paper to form huh high quality pictures 
{referring fig., 46:55}
so they are typically 
[noise]
there are two techniques 
one is huh piezoelectric mechanism and other is huh ((thermal)) system 
so to understand this we will just quickly take a minute here
s- so this is ink cartridge where in this chamber there is ink
[Refer Slide Time: 47:10]

(fig., 46:55)
and here is a huh plate (( )) plate which is piezo- piezoelectric and it can vibrate and by huh that vibration it can throw out a very tiny very fine finely controlled droplet of a huh 
so so this vibration would be controlled by huh the electrical signal which it receives depending upon the information which is to be printed
an- another view is that again same thing
this is the chamber where ink is and a droplet gets thrown by this huh vibration of piezoelectric huh disk here

(fig., 47:52)

this is a uh thermal mechanism where huh the ink droplet is thrown out by a small
by very localized hitting process
so here huh there is a hitting register which gets huh 
[background noise]{sounds like someone coughing}
which gets hitted by electrical current and huh its very controlled hitting
so by hitting there is an expansion and a drop goes out
so huh you remember we are talking of resolution of nature of twelve hundred dots per inch
so you can imagine the the amount of 
the the the level of control which you have to have on this ink droplet to make ((it printed))
[Refer Slide Time: 48:34]
so huh just ((con-)) with the the technology of peripheral devices which existed couple of decades back and where we are now 

(fig., 49:02)
ok let me close at this point 
huh just to summarize we talked of huh I O performance consideration
they are important
briefly we talked about throughput type of I O
[noise]
definition I O performance 
definition or response time type of definition 
huh we looked at variety of peripherals tried to classify from different counts 
huh tired to get a feel of their data transfer rates and we looked at a few examples of peripherals in little more detail
in the next few lecture we will talk of how they get interfaced to (( )) 
thank you 
[Refer Slide Time: 49:20]






Educational Technology Services Centre
IIT DELHI

Computer Architecture

Lecture #34
Input/Output Subsystem: Interfaces and buses

in the first lecture on input output subsystem we talked mostly about huh peripheral devices which are in some sense trans- carrying information from huh processor of memory to the outside world and vice versa 
and transforming the information into suitable forms in the process 
[throat_clearing]
today we will how these get interfaced to huh the processor in memory and become part of the whole system 

(fig., 01:44)
[throat_clearing]
so we will begin by looking at different types of alternatives which are available to interconnect various subsystems and then we will focus on buses as huh the most common of these 
we will look at different types of buses 
how buses can be classified 
what is their role 
and huh the information which flows in the buses can follow different types of protocol
it could be synchronous or asynchronous 
synchronous means it will be timed to the clock 
and asynchronous means it will be huh not synchronized with any clock
they will be uh-huh signals which will carry the event sequence information
[throat_clearing]
and we will then take some example trying to compare performance of huh two different protocols
[cough]							[Refer Slide Time: 02:25]
so huh beginning with this diagram which we had seen last time

(fig., 02:27)
[cough]
that we have huh processor memory 
cache is also shown with the processor is typically the case
and on the other hand you have huh number of I O controllers connecting to huh I O devices
so a controller may connect to one or more I O devices 
then we left huh this open that there is something in-between which connects these
so information needs to flow between huh processor cache combination and memory
between these combination and devices and between memory and the devices
of course there is unlikely that information goes from one device to another device directly
huh it it comes from this side to this side or vice versa
[throat_clearing]
so what are the different alternatives which exist or which can be conceived of 
for connecting huh all these subsystems 
[smack]

(fig., 03:25)
this is one huh straight forward huh answer that you have point to point connection 
that is if you if you want memory to talk to huh this device 
so establish a connection 
[background noise]{sounds like someone coughing}	[Refer Slide Time: 03:38]
if you want huh cache or processor to talk to this device establish a connection 
also connection between cache and main memory
so whichever pairs of devices or subsystem need to talk to each other 
huh th- there is a dedicated connection 

(fig., 04:00)
[throat_clearing][clicking]
and alternative to this is a huh shared set of wires which is called bus
so bus is nothing but a shared communication link
where huh different types of information flow
so it is a uh it s a collection of huh basically wires and signals which are carried on these wires
which will connect huh two or more of subsystems
so same wires run through everything
these wires would carry different types of information could be huh address which would be used to select to whom you want to talk to huh the data which needs to flow in some direction and control which will tell huh what is going on the bus huh
so- it does the timing 
it also indicates huh the nature of information which is flowing 
[throat_clearing]
so the huh the design of a bus is determined by how you organize these signals
what is the number of wires assigned to each and huh in terms of time sequence how this information is carried the basic idea here is that huh you simply allow one set of wires to connect everything and obviously ((alimentation)) of this is that only one com- huh one pair can be communicating on this anytime ok
huh i- if the more than one pairs which need to talk to each other huh they the- the- conversation as to be sequenced but it- it s a very convenient and cost effective 
[throat_clearing]
yet another alternative could be huh to put a some sort of exchange
like you have a telephone exchange
(( )) cross bar switch huh to which we connect everything 
and huh here huh responsibility of this box in-between is to huh make establish connections 
[background noise]{sounds like someone coughing}
so so if huh wants to talk to this unit
then these lines will be connected and huh it would be	[Refer Slide Time: 06:00]


(fig., 05:38)
[throat_clearing]
in- in general it should be possible to connect not just one pair but many many pairs
so- huh th- this can allow multiple conversations to take place 
the same way as you will have in a point to point scenario
so how do these three alternatives compare with each other 
[cough]

(fig., 06:18)
huh so i am showing it here
point to point connection bus and cross bar
in terms of cost bus offers you a solution at a very low cost because its just one set of wires
whereas other two are huh much more expensive
the throughput is low in case of bus where its much higher in case of other two solutions 
[cough]
huh point to point you also require multiple ports 
if you go back to the diagram 
{referring fig., 03:25}
huh if I O controller sitting here needs talk to huh processor as well as memory then it requires two 							[Refer Slide Time: 07:00]
two separate ports because we are having distinct isolated connections
and huh memory needs to talk to one two three four five parties 
so there are five ports 
so having multiple port is an expensive affair
so the the cost here is not just huh th- the cost of the links but huh this also is a contributed to the cost
so the huh issue of multiple port does not exist in other two cases
the next question is expansion that huh what's your designed system
you want to add more huh pieces of hardware to that	
its much easier in case of the bus ok
because th- there is a uniform interface you can plug-in more and more things 
huh but in case of point to point or cross bar you you would need to create more links ok
or increase the capacity of the cross bar 
i- if you have more things to be connected
[throat_clearing]
now- now these are only huh three specific possibilities 
the- they are the- th- are many many more variations which are possible
for example you could have multiple buses not just one bus huh the limitation of one bus is having one conversation at a time
[background noise]{sounds like someone coughing}
you can provide multiple buses and then more can go on
so so there is a huh spectrum 
you form a completely low cost low (( )) solution 		[Refer Slide Time: 08:32]
you can slightly improve things by introducing more buses you incur more cost 
but you also increase performance
on the other hand huh instead of having a monolithic single cross bar switch which connects every everyone 
you could also divide into huh a fewer but smaller cross bar switches and huh that would be meaningful if everyone need not talk to everyone ok
for example in- in our situation we don't necessarily want one peripheral device to talk to another peripheral device
so that that complete huh cross bar capability is not required
so- so we can organize huh it in terms of huh multiple smaller switches and effectively huh reduce the cost 
[smack]
so what i mean here is that if you huh lets say you have n units ok
a cross bar will make it possible for anyone to talk to anyone huh which means you have n into n minus one by two pairs ok
huh i- if the number of communicating pair is much then then you can huh
incur less lower cost
[throat_clearing]
{faintly}((that s what i mean))				   [Refer Slide Time: 09:42]
ok so now huh from this point onwards huh lets focus on the buses which are most commonly used for interconnecting subsystem in the computer system 
huh the- are the- are several types of buses 
or huh there are few it s a clear definitions which are possible [Refer Slide Time: 10:10]

(fig., 09:55)
huh once again the picture could get blurred if one mixes is characteristics of these things
so you can have on my hand what is called processor memory bus 
whose responsibility is to connect processor and memory
huh wh- when i say processor it would typically mean processor and cache together
[background noise]{sounds like someone coughing}
so such buses are required to be extremely high speed buses and huh the- they are short in length that means the- they don't spread out physically in the system
the the design of these is proprietary they are not standard huh
standardization is required when you need to connect a large variety of subsystems
huh as you would see a case in another huh other two scenarios 
so so these are defined for a specific case
so let us say you have a system based on huh Intel's Pentium four
so huh depending upon the signals which are there in Pentium 
huh you need to connect that to huh memory and a bus could be designed specifically to keep in mind the the Pentium's behavior 
whereas if you connecting ((spark)) processor to huh memory
then the whole design will be oriented towards that and it is huh this is a proprietary design 
[cough]
in the sense that huh its not poss- its not huh possible to just pick up any arbitrary subsystem and plug-in to it
huh of course huh somewhere this would need to be connected to peripheral
so so somewhere there as to be path to the peripherals 
but primarily it connects processor and memory
[throat_clearing]
the- the second in our list is I O bus
where huh the main purpose is to connect I O devices to rest of the system 
[throat_clearing]
so so number of I O devices can be ((hung)) on the bus
and huh this bus is com- carrying comparatively 
huh lower huh speed of traffic 
and this could be lengthy because this devices and the controller could be physically spread out
they could be larger in number 				     [Refer Slide Time: 12:12]
and they could be ph- physically spread out
this necessarily as to be huh standard because it is expected that huh there may be different devices which may be manufactured by huh different manufacturers
and unless there is a common definition of huh the bus operation the bus signals huh
the bus width data rate and so on
its not possible for huh multiple parties to design their system which are compatible
so so we want interoperability and that (( )) standard are required
we will talk of some examples of standard buses in the subsequent lectures huh but but this issue of huh standard here is very very important that must be noticed
[cough]
thirdly we have what is called a backplane bus and huh this is more versatile 
in the sense that this huh may connect everything
this may connect processor memory I O peripherals
every- everything could be actually hanging
so- so logically huh this is what we have shown in the previous diagram 
so if you look at this 
we- if we really have a bus like this
{referring fig., 06:55}
this would be a backpanel back- back- backplane bus
a question is why why this term backplane
this as historical reasons because earlier huh the way system was designed was huh
there would be a ((cabinet)) or a box 
where huh each subsystem would be a separate card and
and they will all plug into a connector which are huh connecting a single bus
and that would be typically in the backside on the front front side you would have huh a dis- huh L E D displays on the backside you have a bus running on which they are connectors on which huh processor could be processor would be a single board or multiple boards not a single chip 
memory could be one or more boards and each peripheral could be typically one board
so i- it s a it was typically a bus connecting everything and physically located the backside of the huh cabinet
so no- now if you look at a huh system like huh we have huh Pentium four huh
there is a concept of what is called m- motherboard 
and there is a bus running on that 
[throat_clearing]
so which- which huh would be something like huh backplane bus ok
but physically its not a not a backplane bus but in- logical sense it could be backplane bus
on which many boards could be plugged in huh wi- with a difference that you 
you don't have huh processor board and memory board plug-in to that
so  it s a more for input output
[throat_clearing]
let me huh illustrate some of these ideas here is an example of backplane bus where you have processor memory and various devices all connecting to the same bus
so here i am not showing the the device and the controller separately
so it s a co- complete subsystem which is huh shown as a single box here
[Refer Slide Time: 15:40]

(fig., 15:25)
but huh as i as i described last time that there is a huh there is a controller huh which may be physically integral part of the device 
and and the- there is a device which actually transforms information 
huh between ones and zeroes which C P U or memory will understand
and an information which is more readable
for example huh in case huh lets say a laser printer
huh what you need is huh characters of pictures in the printed form
whereas what comes from memory process would be all huh binary encoded
[throat_clearing]
this is an example where you can see a processor memory bus

(fig., 16:28)
so this bus connecting a processor with the memory 
huh but of course these two cannot be isolated 
we have to have input output somewhere and input output is organized in terms of I O buses 
so the- there are multiple I O buses here
also notice that although processor memory bus and backplane bus would be typically one in a system 
I O buses could be multiple 
so this I O bus takes care of these devices 			[Refer Slide Time: 17:00]
this takes care of these devices and so on
and these I O buses are further connected to processor memory bus through some adapters or interfaces ok
now huh why is it that we don't simply connect these
why why would you separate
the- the reason is that the requirement here
the behavior you see here and the behavior you see there would be quite different 
the the wo- the width of the data could be different 
you may be transferring thirty-two bit or sixty-four bit data here
this could be for example just eight bit (( )) you you are transferring character by character or byte by byte
the speeds here could be slower the speed here is fast huh 
[throat_clearing]
here it is primarily catering for processor memory and huh the- they occupy the bus mainly
huh the- the it is only the the free slots which may be passed on to I O for its usage
so huh this would be more expensive in terms of fits huh design- cost 
and this could be comparatively cheaper 
so so there are huh various reasons why one needs to seperate 
infact multiple huh these different I O buses which you are seeing here 
could also be different in characteristics
one I O bus example could take care of the storage devices disk and huh C D ROM and so on
[background noise]{sounds like someone coughing}
there could be other huh which could take care of huh serial peripheral lets say huh
this could be U S B serial bus where you may you may connect your scanner and maybe printer and so on
[background noise]{sounds like someone coughing}
so so multiple buses again are for huh grouping peripheral devices according to their needs and characteristics

(fig., 18:48)
huh finally this is a picture which shows huh 
all the three types of bus 
so we have a processor memory bus in the same manner
huh there are multiple I O buses 				[Refer Slide Time: 19:02]
but huh these I O buses don't hook on to processor memory bus directly
there there is a backplane bus which is connecting huh
now logically this is the one processor memory and I O ok
so everything is meeting here and huh looking at in more details 
they are bus adapters which connect backplane bus to the processor memory bus
and different kinds of adapters which connect this to I O bus
so each of these adapter as to understand the the definition of signals 
the protocols the speeds at the two ends and make them match right 
huh so each adapter would be a different design although i put same name on each 
so- so infact huh a typical sophisticated system will be quite like this
that it as multiple buses and huh infact huh our Pentium four is also following this approach
so what i said is huh a bus on the motherboard huh could be a backplane bus 
[throat_clearing]
[inaudible background noise]
huh yes here huh we- we are huh grouping peripheral devices which are somewhat similar in nature ok
which which would be 
[throat_clearing]
huh the- there would be a common interface here and huh the- the controllers within each device huh
[background noise]{sounds like someone coughing}
would make sure that that th- the device is able to talk to that interface
{faintly}ok
so so you are right that huh multiple devices will have their own variations
and those variations will be taken care of here
[throat_clearing]
so some adaptation is necessary but that s all built in here 
[inaudible background noise]
yeah ok no- now lets say huh 
it s a matter of how you allow sharing of information or sharing of that bus by multiple devices
so huh we will we will look at that 
huh this is this is a very crucial issue that if if you are connecting multiple devices huh
how is it that you are you are getting huh
we have multiple buses are- are we getting some parallelism here or not
that- that's the question which we need to understand
[throat_clearing]
so so firstly as huh ((was this -)) the concern was that huh would multiple devices (( )) onto the same bus be able to communicate concurrently or not that s one question 
and then- then this whole system together are- are multiple transactions possible
so huh now ultimately if many devices simultaneously speak to memory 
that means the huh there is a 
[throat_clearing]
lets say huh data is being transferred from a disk to memory
some file is being read
at the same time something is being printed in a printer
information is being brought from the memory		[Refer Slide Time: 22:08]
so now huh at the memory end huh everything as to be sequentialized ok
because we we are assuming it to be a single port memory and at any given time it could be either reading a word or writing a word
so how- how is that huh concurrency is possible huh concurrency would be possible in in lets say localized area so so let us say the disk was here and printer was here 
so huh the- the disk data could be flowing on this bus at the same time as printer data is flowing on this bus but here they are coming on the on a common bus
so here they will get sequentialized 
but huh what we need to understand is that the the rate at which data flows would be different ok
so so suppose the the data is flowing at the rate of five megabytes per second here 
and say for printer its flowing at one megabyte per second here 
and if suppose this capacity of this backplane bus is say twenty megabytes per second
so huh here we have two different streams of data parallel
but here they get sort of inter- ok
a- and if you have designed this bus to take care of the traffic ok
so its like huh you have smaller lanes on which some traffic is coming and here on a bigger road it merges huh and from here it is going on to even a faster highway
so on- on the whole there is (( )) at the system level (( )) 
this kind of structure huh brings in some concurrency
huh a- huh at some point things may have to be sequentialized 
but as long as we have been able to keep huh the disk and the printer busy huh
we are utilized there huh capacity huh in the best manner
so so this is how huh one could visualize it does it answer your question 
[inaudible background noise]
yeah yeah yeah huh it- its managed in terms of time sequence
so huh as i was saying lets say coming back to same figure suppose backplane bus is able to carry the data at the rate of twenty megabytes per second 
so so so just for the sake of argument
lets say each of these is one  byte wide which means that huh every fifty nano second huh 
e- e- each data each byte of data occupies fifty nano second sorry
huh twent- yeah fifty nano seconds right
and huh each byte of data occupies this bus for huh i said how much
five megabytes or two hundred nano seconds right 
so so this adapter huh will will take this data coming one byte every two hundred nano second 
and try to accommodate it somewhere here 
this this is where you have lets say one megabyte per second 
so huh every huh thousand nano second or microsecond data is coming
and which needs to be accommodated here 
so so these things could be in someway sequenced right 
huh rather it huh these are inter- at the byte level 
or they are inter- at huh at a packet level or group of byte level 
huh tha- that depends upon huh how you organize this bus ok
it may be that huh this adapter may collect several bytes and put them as a ((bust)) on this
huh this one also collecting its own bytes			         
[Refer Slide Time: 25:50]
and when it gets a chance it puts its own huh own packet on the bus
so huh the the key thing here is that huh if bus as higher capacity 
it should be possible to put things in sequence inter- them in some manner

(fig., 26:11)
[throat_clearing]
so now huh what actually goes on on a bus
so here is a illustration huh considering huh that a processor memory and 
[background noise]{sounds like someone coughing}
some devices like disk are sitting on the same common bus ok
so so huh these ideas could be send- to various bus organization
huh but lets look at the issue of huh input and output transactions on a bus
and once again this would be one possible protocol huh and lot of variations exist
so here we will assume that there are bus as two types of lines
they are control lines which tells you what happens in the bus and when it happens
the other data lines apart from data they also carry address 
since the- they are multiple devices huh there is a need to address them 
and also since memory is sitting on the bus
there is a need to address the memory words or memory bytes
so huh we are assuming here that the address and data 
flow actually on the same physical lines
[throat_clearing]
so now we are trying to show an output transaction that means huh something is to be read from memory and sent to a device
so huh we will try to shade huh memory or the device reflecting whether its reading or writing
so huh if right half is shaded it indicates reading and if left half is shaded it indicates writing
so to begin with huh address is sent to the memory ok
and control lines will indicate that now address is being sent 
it is not the data so me- memory huh would use that address and try to huh internally pick up the word or byte which is being addressed and bring out the data
then the data will flow on this data lines 
and huh this disk is supposed to take the data and absorb it write it
so a- a- at this point of time huh				[Refer Slide Time: 28:33]
we also need to tell the disk drive that is here is data which is flowing out
and it is meant for you to write
so huh the- the- there is a what what we are trying to show here is that there is a sequence there is some protocol 
there is some sequence of huh information flowing back and forth which is involved
so so memory needs to be given address and (( )) responsible data
or memory would need to be given huh address it need to be given data which needs to be huh written and then it will take time in huh record-

(fig., 29:11)
this picture shows the opposite operation where 
huh you are reading from the disk and writing into the memory 
so again memory is given the address and told that is there will be data following which you need to write and huh disk is told to read data
this data goes out and memory supposed to w- write that
so now ho- how this happens exactly what is the nature of signals 
we will huh we will take up huh shortly
there are two ways in which huh this information could be organized or this transaction could be organized huh asynchronous and synchronous	 
in asynchronous case there is no clock involved to synchronize huh the events
basically the the party which involved will tell each other huh what needs to be done next ok
so lets huh lets look at a case of reading from the memory
so the party which is trying to read from memory will huh send a signal 
so it s a control signal which i am calling as read request
so when the signal is activated 
the meaning of this which is understood by the memory is that huh 
some something as to be read 
memory as to do read operation and send out data
huh the data bus huh while the signal is active is carrying the addressso it it as to be understood huh by both the parties that huh
at- at this time what data bus is carrying is actually the address
so the the the processor which is sending the address and the memory which is receiving the address they should understand what it means			
[Refer Slide Time: 30:55]

(fig., 35:18)
in response to this change of read request zero to one 
[background noise]{sounds like someone coughing}
huh memory would send an acknowledgement that yes huh it as seen the read request
[throat_clearing]
now huh in asynchronous huh transaction we all that we want is that this acknowledgement should come in response to read request
we are not saying that it should come after a specific time
so it could it could come after one nano second it could come after ten nano seconds
huh what is important is that huh this carries ((a meaning))
this acknowledgement from zero to one indicates that memory as huh noticed that read request as come
[throat_clearing]
and it ad picked up the address 
so when the huh this read request could have actually been originated from processor
or it could be from huh I O device 
so huh whosoever as sent this request will keep looking at acknowledge signal
and when acknowledge signal becomes one then this can be lowered
[throat_clearing]
so so this is a confirmation this this change or this event is a confirmation that this event as been noticed ok
so huh there is a sequencing shown by arrow one another sequencing shown by arrow two 
so these these have to be huh done these events have to occur in this particular order
[cough]
an another response to this change is that huh since data as been read it need not be kept on the bus so it can be removed from the bus and bus becomes free and 
now what is this huh signal shape indicating
[cough]
huh thi- this basically indicating that huh there is a change occurring here huh let me 
[throat_clearing]
see see generally when when you talking of huh an individual signal
you can show it going from zero to one or one to zero                  
[Refer Slide Time: 33:06]
but when you are talking of a bunch of signals 
some may be going from zero to one 
some may be going from one to zero 
so you you will often show it like this 
so this this is the indication that here is the transition point where some change is occurring 
we were not particularly worried about huh what change is occurring huh this indicates that some bits may be going from zero to one and some bits may be going from one to zero
so this is the change of same nature huh but you notice here a third level 
so this this third level is huh level indicating that bus is free there is nothing on it ok
huh these buses are organized as huh what is called a tri-state bus 
so it could be in there are three possible states zero one and a high impedance state 
huh high impedance state means that e- every every one which is connected to the bus is infact huh a disk connected
its nei- neither trying to put a zero nor trying to put a one
so if everyone isolates from the bus the bus is in a huh floating state 
there is neither huh low impedance path to huh ground or zero level
nor huh low impedance path to power supply or one level so so it s a floating state 
and symbolically this is shown as an intermediate level
so what we are indicating is that data bus here upto this point was floating ok
it was an high impedance state here some bits go to one some bits go to zero and this state continues 
at this point the data is removed huh not the data but the address is removed from the data bus by the device which was driving it and bus floats again right 
[cough]
huh now huh this sequence actually needs to be completed by huh lowering the acknowledge signal 
so memory will huh after raising acknowledge signal it will keep monitoring this and when this is gone down huh when it as noticed that this as gone down this will also go down 
so these are the four events one two three four which complete huh cycle
which is called handshaking
((why)) now after memory as seen the address huh it it does its homework 
and huh gets back with data at some point of time
and when it is ready with the data it will put the data in the bus and activate the signal which says that now data is ready alright
otherwise the device which is expecting data as no other means to know when the data is available from memory
so this is this signal which indicates the data is available and now it is huh turn of that device to acknowledge 
so this change i- is is a recognition of a fact that the device as seen that data is ready
it as read the data and it as confirmed that is i have read the data huh having seen that memory will huh remove the data from the bus and also lower the signal 
so which is an acknowledgement of the fact that huh
th- the data as been read by the huh it as seen that data as been read by the device
and then huh device will lower the acknowledge signal saying that the cycle is completed 
that it as re- recognized that data (( )) as gone down [Refer Slide Time: 36:45]
so again there is a huh sequence of four huh events which occur in a particular order in a in a lock lock manner is there any question about this
this is a huh from from a bus to bus there could be variations in terms of nomenclature of the signal or whether huh it you get a positive pulse or negative pulse huh
and the exact sequence in which it occur but the basic idea is that of ((hashing))
so you see no clock here its just that one event is timing the other ok
so one event triggers the other that event that event triggers next one and so on
and that s how huh transaction completes
[clicking]
so i wonder if it is readable can you read the ((texture))

(fig., 37:44)

can you i will- i will- huh sort of read it out aloud huh now for for this handshaking to occur huh
internally huh the I O device and memory have to go through a sequence of huh states ok
so the- there is a controller within each of these 
which as to huh step through a sequence of states and huh
in each state it response to some changes which are observed and huh 
{faintly}huh noticed
[throat_clearing]
so so so the cycle this is the state diagram for the device and huh this is for the memory
so we will start here in this state huh the requesting device is putting address on data lines
and asserts read request signal ok
huh whereas on the other hand memory is in this state and it is waiting for read request to become one 
so while read request is zero it keeps on cycling in this state
when read request becomes one it comes to this state where it huh 
it observes the data lines which are carrying the address 
it remembers the address and it asserts the acknowledgement line
so where on the other hand after having done this huh
it will be watching the acknowledgement signal
as long as acknowledgement signal is zero 
it remains here in this state 					      
[Refer Slide Time: 39:18]
as soon as the acknowledgement signal is seen huh this proceeds to this next state 
and in this state it will release the data lines and reassert read request 
so whereas on the other hand huh after having sent acknowledgement signal huh 
memory would be waiting for read request to go down and it will remain in read request state as long as 
it will remain in state one as long as read request is one
so huh you would notice that huh th- the transitions in these two state diagrams
are occurring one by one 
so one transition occurs here then one occurs here one occurs here one occurs here and so on
so you could follow these diagrams huh and see that the behavior which we had described in the previous huh wave form is captured here
so so this is this basically huh this is indicating 
how huh this action will be carried out inside the- memory and inside the I O device 
so the- they would have to have states like this which huh observe occurrences on the bus
and accordingly huh carry out state changes inside

(fig., 40:30)
now lets look at the synchronous bus 
huh in a synchronous bus there is a clock which is timing everything
in a in a complete system if there are multiple buses and multiple synchronous buses
they may have their independent clocks
so one bus may have twenty megahertz clock 
other may have five megahertz clock
other may have hundred megahertz clock
they could be different depending upon the data rate they need to carry
so so in this case all events will get timed by huh the clock so i- i am trying to show huh 
lets say raising edge of the clock is active edge all changes will occur here 
huh somewhere there as to be an indication that this is the first cycle where transaction begins
so i am showing it with a signal called start and huh there will be address first of all put on the bus huh it could be available for one cycle or two cycle or mu- multiple integral number of cycle most common is for one cycle 
suppose again we have similar situation that a device is trying to read memory
so this address goes to memory 				[Refer Slide Time: 41:50]
memory does its homework and it is expected to come up with data after six number of cycles
so in this case we are assuming that with a gap of two clock cycles huh
the data will be available 
so th- the device can make this assumption that bus address as been given without waiting to check whether memory as seen the address or not seen the address
huh no confirmation is ((sort after)) and huh
you simply wait for two clock cycles and then huh pick up data from the bus ok
it is assumed that huh memory understands this 
and everyone else sitting on the device huh sitting on the bus understand that 
[smack]
so huh all all this assumptions are there huh in terms of what happens in which cycle right
so huh you don't ((sit)) to any wave form there huh everything happens at a fixed clock cycle 

(fig., 42:36)
so now huh wh-
how do these alternatives compare and huh how throughput could be determined huh depending upon the protocol so- so lets take an example 
suppose we look at huh asynchronous bus which is running at fifty megahertz 
and huh suppose it takes huh the address needs to be put on the bus for at least fifty nano seconds which means one cycle 
memory takes let us say two hundred nano seconds to respond after bus huh 
after address is given which means we we take it as a fixed four clock cycles
and suppose huh fifty nano seconds are required to send the data 
again we interpret this as one clock cycle so so now the huh entire sequence is very clear 
one cycle you send the address then huh four cycle you do nothing wait
and then huh have another cycle to huh transfer the data
so on the whole you require six cycles or three hundred nano seconds 
[background noise]{sounds like someone coughing}
for huh transferring one word lets assume that the bus width was four bytes ok
so in one transaction like this four bytes are transferred and therefore the peak bandwidth 
i- if bus was continuously doing this then it could be four bytes divided by three hundred (( )) nano second which is thirteen point three megabytes per second
so bus bandwidth or bus throughput would be huh characterized in terms of huh megabytes per second and this how it could be related to the basic operations
lets take example of a similar speed asynchronous bus	[Refer Slide Time: 44:20]

(fig., 44:22)
and suppose huh
here there is huh no constraint on timing in term in terms of huh
when one event is coming in response to other event what is the interval
it could be very small it could be very large
huh but lets for the sake of calculation lets assume that each response huh is taking minimum of forty nano seconds right 
so every time we are ((chaining)) two events 
we have to ((allow)) for forty nano seconds and this 
if this is the minimum time huh we will get the maximum throughput by this assumption
so now ((the)) step one two three four five etcetera huh these pertain to this diagram ok

(fig., 45:06)
huh th- this is a chain of events one two then three four five and six followed by seven 
{referring fig., 44:22}
so huh first step huh takes forty nano seconds
its huh basically delay one event and the other event
huh if you look at two three and four huh which means huh this delay this interval this interval and this interval effectively huh the interval from this point acknowledgement to the (( )) availability of data
[Refer Slide Time: 45:45]
huh minimum as for as bus protocol is concerned is forty plus forty plus forty 120 nano seconds 
but this is the time memory also as to be allowed to get (( )) its data 
so we are seeing that it is max of these two quantities three times forty and two hundred nano second which in this case happens to be two hundred nano seconds
and then there are three more steps five six and seven 
each requiring forty nano seconds 
so another one twenty nano second goes for that
adding all this huh the total time for all one to seven adds upto three sixty nano seconds
and once again assuming that bus is huh one word wide 
we get total bandwidth of eleven point one megabyte per second 
[throat_clearing]
huh well huh what this example is indicating is synchronous bus is slightly faster 
which indeed is the case of course huh we we have picked up huh
if instead of forty we take thirty ((things)) may be different
huh th- the relative values are quite realistic 
in the sense that huh synchronous buses are generally faster because huh you you are not waiting for huh sensing the signals and acknowledging them 
you make assumption and simply go by that
huh what are the factors which can be used to increase bus bandwidth 

(fig., 47:21)
you could increase the bus width itself ok
instead of thirty-two bit wide bus you could have sixty-four bit wide or one twenty-eight bit wide 
as we have seen earlier when you talking of huh block transfer between cache and main memory 
[background noise]{sounds like someone coughing}
you could separate data and address lines alright
so so that huh huh wherever possible th- the two could be carried 
two different thing at the same time
you you could transfer huh multi block words rather than talking of one word at a time
you could transfer multiple words 
as we have seen earlier and we will quantify it further in a moment 
huh you could also have what is called split transaction 
split transaction means that huh between initiation of a transfer and completion of that
huh you would have noticed that buses lie unutilized	    [Refer Slide Time: 48:08]
huh the interval here or
[smack]
the interval here
so huh if this delay is large we can do something on the bus
we we can initiate another transaction 
so a transaction gets splits in- two parts initiation part and the ending part
and there could be something else which could sit here 
i will not going into details of at the moment 		

(fig., 48:38)
and huh close by just looking at this example
[throat_clearing]
huh le- lets huh look at two scenarios of a synchronous bus where 
what you are ((varying)) is the huh size of a block which you are trying for transfer 
so now we are not looking at just one word but multiple words which need to be transferred
so huh suppose we like to talk of block size varying from four to sixteen words 
huh it say sixty-four bits synchronous bus frequency is two hundred megahertz 
takes one clock to send either data or address 		
two clocks between huh each bus operation 
so one transaction then you have to have a rest of two clock cycle
so this is a protocol which as been defined for this bus
huh memory access takes huh two hundred nano seconds to get first four words
but subsequent words could be huh faster 
lets say subsequent words come at two twenty nano seconds each 
so this could be by inter-leaving or huh page mode in a (( )) and huh bus transfer and reading next data overlap 
that means lets say address was given 
you- you are getting huh one data which is getting transferred in the bus
meanwhile memory is working on the next word ok		
so that twenty nano seconds ((or)) the next word are overlapping with the transfer of the previous data
so so with this scenario huh we can we find the bandwidth and latency for transferring two fifty-six words ok
total we need to transfer two fifty-six words which huh              [Refer Slide Time: 50:20]
we will see what happens if you make blocks of size four or blocks of size sixteen 
so with the block size equal to four words 

(fig., 50:30)

huh you will require sixty-four transactions to to get two fifty-six words 
and we can count the cycle one for sending address huh forty for memory access 
because two hundred nano second is the access time five nano second is the clock period
huh we send the data over two clocks and then there are two cycles of idle time
so adding this all together huh the total cycles are forty-five 
this is one plus forty plus these four
so overall total number of cycles is sixty-four multiplied by forty-five 
these many cycles you will require for entire two fifty-six words
and huh each cycle being five nano second 			
the total latency in terms of time is huh one four four zero zero nano seconds 
and the transaction rate 
how many transactions huh per second we are carrying out is that 
we have sixty-four transaction those many huh nano seconds
or effectively the bus bandwidth is
[throat_clearing]
so many words or so many bytes over that much of time
so seventy-one point one megabytes per second
so this is a huh four word blocks same exercise we can do with sixteen word blocks
the number of transactions reduces 
{very faintly}(( ))						[Refer Slide Time: 51:58]
huh this is a mistake so this this should be huh sixteen this should be sixteen 
this should also be sixteen and huh this this remains same this remains same 
here we have to do it four times ok
because we- we are talking of sixteen sixteen words huh so this total adds up to fifty-seven 
the total number of cycles is sixteen huh which would- would have been number of transaction into fifty-seven latency this number multiplied by five nano seconds 
[throat_clearing]
transaction rate sixteen transactions over this much time               
[Refer Slide Time: 52:40]
that means three point five one ((million))			
and bandwidth is huh these many bytes over these many nano seconds
so two twenty-four point five six megabyte per second
so you could see a tremendous increase in the huh bandwidth ok 
let me huh stop at this point huh and summarize huh

(fig., 53:05)
we we looked at different interconnection alternatives 
huh bus was the most huh most popular among those 
we talked about three different types of buses namely 
backplane bus processor memory bus and I O bus
and we noticed that I O buses necessarily have to be huh standard buses
standard which are applicable across multiple organizations
whereas processor memory buses are proprietary 
and backplane buses could be either of the two 
the two types of protocols of the bus we have seen synchronous and asynchronous 
synchronous is generally faster 
huh we have also seen the effect of huh choosing a suitable bla- block size
in terms of how it improves the bus bandwidth 
i will stop at that 
thank you 							[Refer Slide Time: 53:50]




Educational Technology Services Centre
IIT DELHI

Computer Architecture

Lecture #35
Input/Output Subsystem: Interfaces and buses (Contd.)

so far we have seen that the input output subsystem consist of huh peripheral devices along with their controllers which are connected to huh rest of the system through buses
so buses form typically the medium of huh communication between peripherals processor and memory 
[noise]
huh last time we saw that there are different types of buses and huh performance of the buses in terms of their throughput or bandwidth was an important issue
we will continue on that discussion huh talk about performance

(fig., 01:32)
we will begin with the example 
[background noise]{sounds like someone coughing}
which we huh covered in the end of the last lecture 
we will repeat that and huh talk about huh issue of bus arbitration when there are multiple devices which need to communicate on the same bus 
huh we will we will take specific example of how buses are organized in a modern P C system and we will also look at some standard buses 
{faintly}as- a- as an example 
[cough]
so we huh while talking of huh performance of the bus 
we noticed that there are several factors which huh can be huh exploited to increase the performance of the bus 
so something which is very obvious is the width of the bus that means how many bits or bytes it can carry at any time
so so there are huh buses on one extreme which are serial buses [Refer Slide Time: 02:25]

(fig., 02:05)
that means one bit of data huh is carried at any given time 
then there are eight bit buses sixteen bit buses thirty-two bit buses sixty-four and even higher
[throat_clearing]
so huh the- the rate at which data moves or the throughput is directly proportional to width of the bus which is very obvious
then different things on a bus maybe multiplexed fo-
[background noise]{sounds like someone coughing}
for example typically address and data lines are multiplexed that means at any given point of time either is address is being communicated on the bus or data is being communicated
so huh if you can huh provide separate lines which means additional cost of the bus 
then there is an improvement in performance 
we also looked at two different types of protocols synchronous and asynchronous 
synchronous protocol implies that huh everything is controlled by huh clock 
so all events occur huh at active edge of clock and all times are measured in terms of clock cycles whereas in asynchronous bus huh
th- there is interlocked response huh to each others request from huh the device and the other party which could be memory or processor
so huh one event occurs huh which triggers another event that triggers another event and so on 
so the events get chained or interlocked one after another 
now because of huh the need to huh allow arbitrary amount of time huh from one event to other event these protocols tend to be slower 
they are flexible in the sense that huh slow and fast devices can be mixed on the same bus
huh but the throughput is comparatively lower as compared to synchronous bus 
where you do not have to sense signals going up or down 
you need to wait for a fixed amount of time and assume that something would have happened 
[noise]
so th- the higher speed buses typically follow huh synchronous approach 
we huh in the last example in the previous lecture we were seeing the effect of huh the block size 
i- if you huh are able to transfer larger blocks chunks of larger blocks
[background noise]{sounds like someone coughing}
then huh the overall throughput is faster 			[Refer Slide Time: 04:45]
we will huh go through that once again and finally huh if you can make use of the bus huh in the idle period typically the bus is occupied when you are starting a transaction then it is unoccupied and then its required again when you are closing it
so in-between there is a unutilized period and we can use it to interleave transactions or initiate other transactions that s called a- split transaction protocol
so that s another device another mechanism to improve the bus throughput
so lets get back to huh that example of block size and its effect on bus performance

(fig., 05:30)

(fig., 05:38)
huh just for your recollection
here is the asynchronous handshaking protocol 
one example we had seen and a synchronous protocol 
[throat_clearing]
{referring fig., 05:40}
so huh we are talking of huh a bus which is a sixty-four bit synchronous bus 
[background noise]{sounds like someone coughing}
and frequency is two hundred megahertz 
and we will see effect of huh varying the block size from four to sixteen 
so now the protocol of the bus is as follows 
that it takes one clock cycle to send either data or address 	      [Refer Slide Time: 05:55]

(fig., 05:40)
and we require two clock cycles between huh each bus operation 
so so these are characteristics of the bus and memory which is being accessed is capable of huh sending first four words huh at huh within two hundred nano seconds and then each additional word huh requires twenty nano seconds
so so it as huh a mechanism within it that huh 
[smack]
you don't need to huh repeat the whole transaction it it can actually get additional words within huh twenty nano seconds
huh we also seen that bus transfer and reading of next data overlap that means when huh one data which is fetched from memory is being transferred on the bus
the the memory is memory could be busy reading the next data ok
so now our requirement is to transfer two fifty-six words on the whole 
either in groups of four words or in groups of sixteen words 

(fig., 07:18)
[noise]								
so here is the calculation of finding the latency that means how long it takes to transfer or the band width that means the total huh rate at which data gets transferred 
so we are considering two possibilities the block size is either four words or sixteen words ok
[background noise]{sounds like someone coughing}	[Refer Slide Time: 07:30]
so i am just denoting that by n 
the number of huh transfer that transaction required for two fifty-six words is 
two fifty-six by n so huh 
when you are transferring four words at a time you require this to be done sixty-four times 
when you are transferring sixteen words at a time you require this to be done sixteen  times 
ok to make a total of two fifty-six words 
now the the cycles each transaction takes huh involves sending an address which takes one clock then allowing memory to access the data which takes forty clocks ok
because two hundred (( )) for the time given and two hundred megahertz means five nano seconds is the clock period 
so forty clock periods huh 
[throat_clearing]
the data requires two s- two clock cycles to be sent because memory is fetching four words at a time and the bus is sixty-four bit wide ok
lets huh get back to this specification
{referring fig., 05:40}
huh memory access is four words at a time ok
huh first time it takes two hundred nano seconds and next time it takes twenty nano seconds
and the bus is two word wide 
so so it requires two cycles to send this data and then huh between huh one transfer of data and the next one the bus needs an idle time of two clock cycles 
so now it is huh these figures which have been put here one plus forty plus this two into two which will take care of huh sending huh four words ok 
and huh if n is larger then this part will be repeated 
so so in this case when the block size is four words only huh th- this happens only once 
when the block size is sixteen this happens four times ok
[background noise]{sounds like someone coughing}
so we are multiplying this part by n by four which is huh the block size in words
n is the block size in words 
[throat_clearing]
so it takes forty-five word- forty-five cycles here and in this cycle in this time we have sent four words 
here it takes fifty-seven cycles and we have sent sixteen words in that time
so the total number of cycles huh is this c cycle per transaction multiplied by m 
which is the number of transactions to make up two fifty-six words
so forty-five multiplied by sixty-four 
product of these two numbers is the total number of cycles 
product of the- these two numbers is the total number of cycles here ok
so now this is in terms of cycles 
we can huh convert this into nano seconds by multiplying it with five nano seconds ok
so huh the total latency is this multiplied by five 
so many nano seconds or nine twelve multiplied by five four five six zero nano seconds
so ((if)) this is the total time required to transfer two fifty-six to read two fifty-six words from memory 							
huh we can also talk of transaction rate how many transactions are being done per second ok a transaction means different thing here 			[Refer Slide Time: 11:00]
transaction here means sending four words 
transaction here means sending sixteen words 
so number of transaction ((in)) million transactions per second is huh thousand times and the number of transactions over the time it takes ok
so its huh four point four four million transactions per second and three point five one million transactions per second here
the bandwidth is the total number of bytes being transferred per second ok
[background noise]{sounds like someone coughing}
bandwidth or throughput is huh lets say measured in megabytes per second 
so we have two fifty-six words multiplied by four which means which gives you bytes 
so many bytes in time t gives you the throughput 
so seventy one point one megabytes per second is the throughput here and 
two twenty-four point six is the megabytes per second is the throughput there
so- so there is a huh ((marked)) difference in terms of the huh throughput of these two buses

(fig., 12:18)
[throat_clearing][rustling]
huh the other thing we talked of was split transaction 
so- if you look along the time axis 
so suppose one transaction huh begins here
[background noise]{sounds like someone coughing}
ok for example sending the address all right and sending a request to memory that you want to read and then huh memory take some time 		
so in-between huh the- the device which had sent this request is not using the bus 
so bus could be released and made available to somebody else and when memory signals that data is ready or huh 
[background noise]{sounds like someone coughing}
if it is synchronous you wait for ((right)) of cycles and then you can come back on the bus and read the data 
now in-between you can allow another transaction to begin 
[background noise]{sounds like someone coughing}
which might end later on ok					[Refer Slide Time: 13:10]
so so now huh you- you have to properly link the beginnings and the ends 
so- so that means the device which send a request here huh should know exactly when it as to huh pick up the data ((at requested)) and similarly the one which requested here needs to pick up at appropriate time ok
huh s- bu- but as it is very obvious that utilization of the bus is much better here
[noise]
ok now we have assumed that you can have huh many huh parties connected to a single bus
ok they could be processor memory I O devices all could be in general sitting on a single bus 
so huh there are many conversations or many transactions between different pairs which can go on

(fig., 14:06)
huh so we have typically a concept of master and slave on the bus
master is the one which initiates a transaction ok
so it will initiate a request for read or write and slave is the one which will respond to this request
so so typically huh let us say lets imagine three different situation processor talking to memory 
so processor wants to huh get a block of data containing instructions ok
or- wants to write a block of data to the memory 
so huh in in this memory huh conversation processor would be the master and memory would be the slave 
another scenario is that huh a- disk drive wants to huh write into memory or read from memory 
so in this case the disk drive controller would be the master memory would be the slave 
huh there could be another scenario that processor wants to initiate a transfer 
so processor wants to instruct the disk drive that from track number so and so huh set number so and so send one thousand bytes of data to memory ok
so that is the huh initiation process and there processor would be the master and the peripheral controller would be slave
[background noise]{sounds like someone coughing}
so huh among masters ((i will -)) processors or peripherals and slaves peripherals or memory 
so peripherals could be slaves while talking to processor 
would be master while talking to memory 
and processors are always masters memories are always slaves ok
now with mul- possible of multiple masters on a bus
[smack]								
how do we coordinate among them ok			[Refer Slide Time: 15:40]
what happens if huh multiple masters have a need to transact on the bus simultaneously 
so what is the discipline which as to be followed huh
so- so it s a its an issue of getting access to the bus or getting control of the bus and using it and then releasing it ok
so huh after bus as been released by one master another master can use it 
huh the- there as to be typically an uh arrangement of priorities 
so huh somebody may have higher priority somebody may have lower priority
[background noise]{sounds like someone coughing}
and huh this priority could be used to resolve the conflict 
when multiple request are there simultaneously one with higher priority needs to be given
and this priorities huh would depend upon huh wh- what is urgency
[throat_clearing]
so there may be some transactions which cannot wait which have to be done on a ((urgent)) basis 
at the same time fairness is es- essential 
huh whether huh a particular party as low priority or higher priority 
it should eventually gets its chance 
so the the usage should be huh reasonably distributed and everyone should get chance
we look at huh four different mechanisms called daisy chaining centralized parallel arbitration 
distributed arbitration and arbitration by collision detection 	    
so which- which are used huh different mechanisms are used in different buses and we will see how these work 

(fig., 25:18)
[clicking]
huh so here is a scenario showing huh the first approach called daisy chain for huh [background noise]{sounds like someone coughing}
resolving the huh accessed issued to the bus 
so here we assume that ((they)) are multiple devices huh
i am using the term device here in a generic sense
[background noise]{sounds like someone coughing}
one of these could be processor ok
so these are all potential masters who wants to access the bus huh we are not showing the bus completely 						
bus will have data lines and address line and so on		[Refer Slide Time: 17:45]
we are not showing those 
we are only showing a few signals which will huh define the discipline of transfer of control 
of bus from one master to other master 
we assume that there is huh a block here called bus arbiter huh which will actually coordinate the whole thing
[cough]
so the devices are arranged in huh decreasing order of priority 
highest priority is sitting closest to the arbiter and the one with lowest priority sits farthest away from the arbiter 
there are huh bus request signals and bus release signals 
so so this is a there is a common signal on which every device can send a request 
and there is another signal on which bus huh device which had the bus can- can indicates that s it doesn t need the bus any longer
[smack]
then the interesting part here is that huh a bus grant signal which comes from the arbiter is chained through all devices in a huh in a manner which is called daisy chain
so grant signal goes to device one which is the highest priority device
it may use it or it may pass it on to the next one which may block it or pass it on to the next one and so on						
so huh let us say a request comes from some device say device number two 
in response to that the arbiter will send a grant signal ok
[background noise]{sounds like someone coughing}
and huh device one doesn t need it so it will allow the signal to pass through to device two
device two needs this so it will block it and huh all the devices further down will not see huh grant signal 
so its a same signal huh which is trying to propagate through from one end to other end and at some point it gets blocked 
if the- are multiple devices requesting for the bus the one which is closest to the arbiter will block it first and the one which is huh down stream will not be able to see it
so th- that s how priorities are managed
so the exact sequence of events which will go on is as follows 
the device one o- one or more devices they will send a request ok and request is sent by huh let us say raising this line bus request line to one
huh one could have an opposite convention but huh just for the sake of explanation i am assuming that huh the request line is normally zero and it will go to one indicating that some device is requesting 
in response to this huh if the bus is free the huh arbiter will activate the grant signal 
and huh assuming that the the device which needs the bus gets the signal 
it will then lower the request ok and then start using the bus right 
so it will use the bus for certain interval of time and then activate release signal 
so i am assuming that release signal was also zero initially and now it as become one right 
huh when the bus arbiter huh sees that bus as been released it will lower the grant signal ok
and in response to that the device will also lower its release signal 
so so this is huh a- a- 
[background noise]{sounds like someone coughing}	[Refer Slide Time: 21:31]
complete transaction of getting acquiring the bus and releasing the bus 
now lets try to understand what will happen if the- are multiple devices which are requesting the bus 
[smack]
so at any point of time lets say bus was free and the bus arbiter notices that there is a request for the bus 
now huh you can imagine that imagine that this both these signals release and request are wired 
or in the sense that huh each device may send its individual request but effectively 
what you see on the bus what you see on this line is or of all those ok
so a one here means that huh one or more devices are requesting for the bus
you don't know which one 
all you know is that there is at least one device which is requesting for the bus 
[background noise]{sounds like someone coughing}
bus arbiter doesn t worry about who is requesting ((had)) it simply huh activates the grant line
indicating that yes huh among all of you which one a- whosoever as the highest priority can now use the bus ok
so huh naturally the device which is low- down in the chain will wait
the one which is higher up in the chain will get the bus 	
huh wh- when the highest priority device completes the cycle ok 
it will huh it would have low- down its own request 
but- but huh this line will stay high because huh there is a lower priority device which is still persisting right and huh eventually when the bus huh grant as been lowered down 
release as been lowered down huh the bus arbiter will still see that there is a request right 
and it will give the grant once again which will be huh now seen by a lower priority device 
so huh eventually huh if multiple devices had requested for the bus huh simultaneously
[background noise]{sounds like someone coughing}
or in overlapping manner then they will get the bus grant one by one 
huh one device gets served then huh when it is through another device in the chain will get served then next device will get served and so on
huh what would happen if huh a- a- device is getting served and a higher priority device comes up with a request now 
so huh if a higher priority device comes huh it will find that grant signal is one ok
and it might think that huh the grant as been made one for huh ((its)) own request 
bu- but that could be uh a disaster situation because huh the high priority device will hijack away the bus huh while low priority device was still using it
to avoid that huh we- we follow a rule that huh th- the device as to look at a transition on the grant signal 
it should not just assume that a one level on the grant signal is sufficient to ensure that it as the bus granted 
it as to see the grant signal going from zero to one 
so- so now with- with this assumption if you have a high priority device coming 
in-between when bus was already being used by a low priority device huh it will find grant signal one but it will not see a transition zero to one on the bus on the on the on the bus grant
and therefore the situation which i mentioned will get avoided
ok i- i have already mentioned raising edge on the grant signal is significant and not the level
th- the- there is another point here 	[Refer Slide Time: 25:23]
huh one may question that why do we need huh release signal why not we simply manage with a grant signal and a request signal 
huh if you if you do not have a release signal then the bus arbiter will not have a clear indication of when the tran- when the usage of bus is over
because huh a device which is using the bus would have removed its request but since other request are still persisting you wont know huh this change
so you- you need another huh signal to indicate that bus is being released ok
[background noise]{sounds like someone coughing}
{faintly}so this- this is important
huh now this arrangement is huh very simple and inexpensive ok the arbiter is very simple 

(fig., 26:10)
it doesn t have to worry about who as what priority 
it simply looks at the request and release and activates grant or 
{very faintly} deactivates grants (( ))
huh the the problem with this that huh the speed could be limited here if the chain is very long 
if the chain is long as we have seen in ((carr-)) propagation huh the- there is a delay 
so huh we have to allow for the maximum delay before we can actually huh decide whether the signal as changed from zero to one 
{faintly}or one to zero 
so if chain is long huh the operation will get slow- down 
[throat_clearing]
the the other problem is that huh in this we have not ensured that a low priority device does not st- 
what could happen is that huh suppose
{faintly}we will just go back to previous diagram 
huh it could happen that huh high priority device can keep on huh ((shuttling)) the bus 
in-between them and huh a low priority device can always keep waiting right 
{referring fig., 25:18}
so s- suppose huh device one and two lets say one two and all had requested 
so one gets served then two gets its chance but before two finishes one as another request ok
so as soon as huh two releases grant is given again device one gets it ok
and huh meanwhile request from two could come and so on       [Refer Slide Time: 27:40]
so bus could keep huh getting passed on between high priority devices and some devices at low priority and could get st- st-
{referring fig., 26:10}
so the solution to that huh could be as follows that a device that has just used the bus huh can be should be disallowed to reacquire it until it sees the request line go low ok
the meaning of this is that huh if if you have used the bus huh there may be other request pending so so request line will continue to be one huh till everyone down the line as serve as been served ok
the request line goes low only when huh no device is requesting 
that means everyone who as requested some point of time huh is served
so huh a- a device if if a device does not make a second request before everyone else who was in the queue as been served then this problem will go away 
[smack]
ok the second solution which ((are)) mentioned about arbitration is the centralized parallel arbitration	 

(fig., 28:50)
[background noise]{sounds like someone coughing}
huh here we have again huh many devices which can request and there is a central arbiter which can send grants 
now we assume that there is a separate request line for each device and separate grant line for each device 
so huh this is the request line for device one and a grant line
request line for two grant line for two 
request line for n grant line for n 
now th- the whole logic is contained in this arbiter
arbiter huh is supposed to look at all the request and issue them grants individually 
so huh then the- priorities of the devices huh could be hard coded into this arbiter 
or could be defined ((dynamically)) and th- this could in a fair manner huh assign the bus or give the grant signal to various devices huh turn by turn alright
huh so here huh th- the priority need not be defined by by the position the way we did earlier 
although in the picture i have shown  this is the highest priority this is the lowest priority 
but- but- huh th- the position where you are located huh is not necessarily huh the priority because there is no chaining here 	[Refer Slide Time: 30:13]
so so you can arbitrarily define priorities of various devices and huh the the resolution of multiple request will take place here
so the kind of huh mechanism which is being followed in the previous case in somewhat a distributed manner would now get ((concentrated)) huh in a single arbiter 
so arbiter here is more complex then huh what we assumed in the previous case 

(fig., 30:40)

huh we can distribute this task of arbitration among the devices themselves
so huh it s a more huh democratic kind of system where all all the devices sort of negotiate among themselves or huh collectively decide who gets the bus
so you- you have huh you have several control lines on which devices can send their request and also huh put their identity on the on some huh specific lines
so ea- each one can see who are all requesting at any given time and huh the identities are all available 
so so let us say huh device two sees that device one is requesting and device five is requesting and huh then it if it understands that it among these three among one two and five it is not the highest priority 
it it would stay back and allow the one which is highest priority among all these two go through
so huh the priorities of various devices are understood by each other and huh in a in a honest manner all devices supposed to look at what is going on the bus
what request are there on the table and huh make a choice accordingly 
{referring fig., 32:02}
[throat_clearing]
huh then lastly we have arbitration by collision detection 
so th- example of this is Ethernet huh where you have 
similar to the previous case you have huh shared medium over which all devices are connected and here huh you you notice each device simply sees that if the bus is free 
you try to huh initiate your transaction and if multiple huh devices happen to do so huh then there there is a need to figure out that whether collision occurred or not ok
so huh what you do is you first check if the bus is free huh then try to start and then check if huh what you sent huh was huh did collide with something else or not
if no collision takes place that s means wh- what you see on the bus is what you wrote ok
[Refer Slide Time: 32:55]

(fig., 32:02)
huh then that means there is no collision and it means you have the access to the bus and you can continue
on the other hand if you if you huh collide that means you try to write something but because somebody else also wrote something on the bus huh
what you saw was a combination which is different from what you wrote 
that s an indication of collision 
so so you back of and try again after some time 
huh to ensure that huh both devices which collided don't huh try again at the same time 
this huh delay is huh modified randomly ok
so huh one device may try after huh five microseconds other may try after fifteen microseconds and therefore unlikely that they will clash again 
for some reason there is a clash again you just repeat the process 
[background noise]{sounds like someone coughing}
so its again very simple but obviously time gets lost in huh because of collision 
so huh if the density of the traffic 
if if the usage requirement for the bus is not very heavy huh the the collision will not be too many and therefore 
{faintly}it will work efficiently
[inaudible background noise]
yeah huh yes it could get starved ok
i- i- if the request keep on coming from huh high priority devices and priorities are fixed huh and we we don't follow a rule of the kind i mentioned earlier huh then starvation could occur 
so so you you could either have somewhat restrict- rules so that repeated huh re- request actually huh get held back 
or you could also talk of huh modifying the priorities dynamically the priorities could rotate ok
for example huh a device which lets say use the bus huh kind of goes back to the queue end of the queue and stand in the queue again 
[background noise]{sounds like someone coughing}
so so that could be the lowest priority and and therefore the priority could dynamically modify 
ok now we talked of different types of buses and huh one scenario which i had shown where we had three types of buses 	[Refer Slide Time: 35:11]

(fig., 35:12)
huh i am showing it again here 
you have a processor memory bus typically a proprietary bus connects processor memory and through an adapter you have a backplane bus (( ))
so backplane huh bus is the one where I O bus directly or indirectly and processor memory directly or indirectly all connected
the I O multiple I O buses could be there connected through different adapters to the backplane bus 
so for example huh one I O bus could take care of huh disk drives C D ROM and so on
either could take care of huh maybe huh printers scanners and so on 
huh ty- typically huh the I O buses are standard processor memory buses are proprietary backplane buses could be either
more often they are also more and more becoming huh standard 
huh the the speeds obviously are highest at processor memory bus level and lowest at I O bus level 
huh the P M buses will tend to be synchronous I O buses huh would tend to be asynchronous 
but huh there are examples of both lines backplane buses are also now generally synchronous 
huh this this is a this is a kind of huh oversimplified situation if you look at a real system huh thing may be somewhat different	[Refer Slide Time: 36:40]

(fig., 36:48)
so i am showing an example of a typical huh Pentium four huh type of architecture where
[throat_clearing]
you have huh basically this is the processor and you have two huh controllers or two hubs ok
instead of huh adapters we have these device 
actually these two form what is huh commonly called a ((chip set)) ok
so huh you have a ((chip set)) around which a motherboard is built 
you have a processor and you have ((chip set)) huh which interface huh most of the other things and these actually characterize a particular motherboard 
so the- are two complex chips here 
this is called M C H or G M C H 
M C H stands for memory control hub or G M C H graphics memory control hub 
I C H is I O control hub
so huh this is on this side it connects to memory modules 
on this side it connects to the display modules	
it could be huh the C R T monitor L C D monitor or it could be simply a video out
[background noise]{sounds like someone coughing}
and huh this connects to huh variety of peripherals including huh a P P C I bus here which is a which is a backplane bus and the- there are several I O buses 
for example this A ((T)) A for disk drive U S B for variety of devices and so on LAN ok
huh i- in ((-ser)) sometimes some people call this as a north bridge and this as south bridge 
right just because of the way they are typically placed in the diagram 
so huh you you would notice huh in some cases of frequencies are also given 
huh this is a bus which is connecting huh the hub here and the processor and it would typically run ((at)) five thirty-three or eight hundred megahertz right 
this is called the front side bus and huh this is designed to connect to memory modules which will run at huh three thirty-three megahertz or four hundred megahertz or five thirty-three megahertz 
huh ok so so huh i think rest of these huh this is U S B s ((eighty)) a ((eighty)) a 
these are these are basically disk drive huh interfaces huh this is audio interface 
this is S I O which is another kind of serial I O 	
P C I P C I express once again another derivative of P C I and this is LAN
so now in which way is this different from the diagram we had drawn earlier

                                                            (fig., 39:46) 	[Refer Slide Time: 39:48]
huh here we have what you would have noticed is that we have hubs rather then adapters
{referring fig., 35:12}
we we talked of adapters trying to connect two different types of buses 
{referring fig., 36:48}
but hubs are somewhat more complex we are connecting multiple buses ok
huh and huh device high speed devices like graphic display are connecting directly to memory hub and not to the I O hub 
{referring fig., 35:12}
s- so this this is a here we huh had an impression that all the I O devices huh are connecting 
are coming through I O bus to the backplane bus and then to the memory 
memory processor bus but here huh particularly these huh display devices are are not connected to I O control hub 
they are connected to ((the)) memory hub
the the the reason for this is that huh the huh the transfer demand the bandwidth demand here is extremely high 
a- as compared to all these this is highest if you recall huh the first lecture on I O 
we looked at a variety of peripherals and tried to see get a feel of the speeds or the throughput 
requirement of various devices and we noticed that the the display as the highest throughput requirement and therefore huh this connects directly here
then huh we talked of a processor memory bus here but actually speaking although it didn t show up in this diagram there are huh two buses 
one is called front side bus and other is called huh backside bus 
front side bus is the one which connects the main memory ok
huh through the memory control hub and there is also a backside bus which connects L two cache which is a faster bus right huh a- and- that s not accessible outside on the motherboard 
its huh within the processor because L two cache here is on the ((chip set)) 
[throat_clearing]
so huh the- the- are numerous varieties which are possible when it comes down to huh real system 
although conceptually we have seen two three different possibilities 
{faintly}the- are huh lots of variations which are possible 
ok i have been mentioning the term standard bus 	[Refer Slide Time: 42:12]

(fig., 42:15)
huh what what actually why we need standards and what these standards are 
huh standardization is required so that huh subsystem from different manufacturers could talk to each other huh
if if that is not the if that were not the case if the- were no standard buses huh we would expect entire system to be built by one manufacturer so that huh compatibility is ensured 
but once you d- define a standard 
it s a common interface so huh one company can build a processor one can build memory 
one can build different huh I O peripherals 
so huh you you can get huh one each single company may not necessarily specialize in all the areas and therefore huh it s a better situation huh that you- you can allow multiple parties to build different things in which they are good at
huh now huh but a- as technology develops huh the- these huh speeds change and huh  various requirements huh change 
on the other hand when you say something as been standardized 
you you are freezing all the specification so so these are two contradictory requirements huh on one hand you you want things to improve huh
for example when you saying that you have defined a bus with two hundred megahertz so so you are freezing huh everything at that 
you are saying that huh if if i make one device it should be compatible- two hundred megahertz 
you make you also make it compatible two hundred megahertz 
and we have sort of agreed and frozen it at that 
but huh suppose i i make my devices better and i would like to ((run at)) three hundred megahertz you also want to run ((at)) maybe three fifty megahertz 
so huh by standardizing you are sort of freezing and huh arresting the growth whereas the technology will like to an- an- commercial pressure will like to push it in the other direction 
so what you need to do is you need to keep on revising and refining your standards ok
so you- you have standard which is version one then you get version two 
which is where some of the things get re- redefined 
all the performance specs go up an- and this process as to be huh a continuous process
huh so you- you have to have huh formal mechanisms of huh defining these standards 
and typically these are done by huh either groups of industries which are formed
industries and other bodies could also be there 
so you have con- formed which take the responsibility of collectively defining standards 
[background noise]{sounds like someone coughing}
or you have professional bodies such as I triple E or huh I C U and so on I T U huh 
they they have again representation from various huh organizations and they define the standard huh in a manner which are acceptable to larger huh community 
sometimes what happens is that huh a proprietary interface or proprietary huh mechanism which becomes very popular gets adapted as a standard
so other- see benefit in huh following what what huh a popular person is doing 
now what exactly is standard 
standard as far as buses are concerned huh is defined at various level
at physical level at electrical level and at the logical level 
at physical level you need to define exactly the shape size dimension of the connectors ok huh or the cables 	[Refer Slide Time: 45:55]
at electrical level you need to define huh the voltage and current levels the impedances 
and at logical level you need to define the meaning of each signal and the sequence in which signals change and the events take place 
so huh a bus standard is a is a complex definition ((sp-)) from physical level to the logical level 
huh lets look at a few examples of huh various kinds of buses 

(fig., 46:29)

so huh in in P C domain huh in- in early stages we used to have I S A bus which is which is transformed Industry Standard Architecture huh
so that later on got extended to E I S A or extended I S A bus then huh further down the line huh 	
now I S A and E I S A huh were trying to connect everything ok
but then later it was felt that huh processor memory could be connected through a faster link and peripherals need to be connected on a slower link
so huh V L B or huh V E S A local bus 
V E S A stands for video electronics standards association 
a local bus is defined and I S A or E I S A could get huh linked to this then subsequently huh P C I bus was defined stands for peripheral component interconnect bus 
it s a backplane bus huh and this also as several as seen several revisions
i will come come to this in a moment and A G P stands for accelerated graphics port huh on which the display devices get connected 

                                                              (fig., 47:40)	[Refer Slide Time: 47:37]
so here is the comparison of some of these buses which i as mentioned huh I S A bus was earlier eight bit later on became sixteen bit 
E I S A is thirty-two bits V L B and P C I also thirty-two bits but later versions of P C I are sixty-four bits and huh A G P is a thirty-two bit bus 
the the frequency huh for I S A and E I S A it is eight point three megahertz 
V L B and P C I thirty-three megahertz later versions of P C I huh sixty-six 
A G P as gone from sixty-six to double of that and quadruple of that and one A G P 
{faintly}((- eight x )) is of (( )) 
so huh depending upon the frequency and the width huh the the throughput is given in megabytes per second 
so here huh one megabyte is huh meant to represent two- power twenty and not tens power six ok
that s why you will see some some discrepancy but approximately huh you can get this figure by huh combining this and this 

(fig., 49:00)
so now huh a P C I huh 
lets spend huh a couple of minutes on P C I bus which is huh invariably there in- all the P C system now 
the basic P C I was huh thirty-three megahertz synchronous bus 
width was thirty-two bits and accordingly huh peak transfer rate is one thirty-three megabyte 
per second
so this is the peak transfer rate that means (( )) the bus is continuously transferring data
it could transfer at that rate but actually huh because of protocol delays and- idle time
the transfer rate would be much less 
the address which flows on this is thirty-two bits huh 
{faintly}which can address four gigabytes of memory 
{faintly}in terms of voltage it is 
{faintly}it could have three point three volt or five volt 
{faintly}the- are two ((variations)) are possible
huh then later development on P C I lead to huh P C I version two point two which is sixty-four bit wide sixty-six megahertz ok
and therefore huh the overall performance ((is roughly four times)) 
both both these factor get doubled 	[Refer Slide Time: 50:02]
then there is P C I x version also huh wh- which is huh having a data rate of huh one thirty-three megahertz so bandwidth is twice that right
and huh it as a two sixty-six version two sixty-six megahertz and bandwidth is huh more than two gigabytes per second 
the other variations like mini P C I or compact P C I and huh there is also serial version 
P C I express ok it s a serial bus which follows signaling like P C I 
we are not going into details of huh what signals are there 
it- it s a huh that itself will take several hours if we have to go through that in detail huh b- but that s just a serial version 
serial buses are typically huh cheaper because they have to carry very few wires alright
and therefore the cables are cheaper the connectors are cheaper and on the whole cost is lower
but ob- obviously huh if you are sending one bit at a time the total data rate gets reduced [throat_clearing]
so huh here is the comparison of some of I O buses huh
[background noise]{sounds like someone coughing}

(fig., 51:25)
you have serial port ok huh 
on which sometime you connect huh external modems 
huh parallel port on which you typically connect printer ok
[background noise]{sounds like someone coughing}
this is some extended parallel port 
U S B on which i have connected the this huh flash memory device 
you can connect devices like cameras printers ((standard)) and so on
huh then there are huh other high performance serial de- serial ports like fire wire fiber channel 
you can see that there is a wide range in terms of throughput rate huh starting from huh a fraction of megabytes per second going all the way upto huh something like four hundred megabytes per second 
then these are the some of the I O buses to which disk disk drives huh C D ROM drives 
D V Ds they connect 
{referring fig., 52:25}
so I D E is a very old one ultra I D E huh S C S I which stands for small computer system interface huh it as again 
you can see how the standards have evolved 			      [Refer Slide Time: 52:36]
from S C S I one to two to three ((they are)) ultra wide fast 
you know all these prefixes keep on getting added and the standards improve 
so starting with the five megabytes per second all the way upto one sixty megabyte per second 
so there are various huh steps in-between  	

(fig., 52:25)

(fig., 53:02)
huh so so this is not all 
these are only some of the standard buses huh which we have seen 
i will close at point and in summary what we have seen today is huh we had seen the factors which infer- performance of a bus in particular in detail 
we saw how variation of block size huh would huh per- huh would change the performance we saw it quantitatively 
we huh looked at different methods of arbitration of bus when the- are multiple masters 
(( )) to get hold of the bus and huh the requirement there is that it should be huh mechanism which can support priorities huh but at the same time it should not lead to starvation 
they should be a fairness 
we huh looked at the organization of buses within P Cs and huh we- we huh very huh briefly looked at some examples of the buses at the b- backplane level and at I O level huh within I O we saw huh series of buses which are for hard disk drive or huh C D ROM drive or D V Ds 
and we have seen that as huh time progresses the buses have to be huh refined and redefined 
the standards have to keep on changing to huh keep pace with the technology 
thank you 
[Refer Slide Time: 54:30]




Educational Technology Services Centre
IIT DELHI

Computer Architecture

Lecture #36
Input/Output Subsystem: I/O Operations

continuing our discussion on I O subsystem we will today talk of I O operations 
so far we have seen huh peripheral devices and the buses which interconnect them 
now we will try to see how the whole thing works together 
so we will try to view the whole thing from a software point of view as seen by a programmer how the entire activity appears

(fig., 01:21)

huh we look at the issue of how I O devices are addressed and huh one crucial thing in 
I O operation is checking the status or identifying when certain events are occurring and huh finally we will talk of huh mechanism which allows memory and I O devices to talk to each other 
initially we will see from processor point of view 
that s what i mean by saying huh talk from software point of view 
so we will see how processor participates in this huh activity and finally how memory directly interacts with the devices 
[throat_clearing]
{referring fig., 02:02}
so as seen by the software huh a high level language programmer 
[noise]
huh tries to see the I O activity in terms of some abstract statements 
for example we have statements like printf and scanf in C and huh your view of I O as a high level language program would be in terms of what these statements can do 
[noise]
so you you can huh you- you can send the data out 		[Refer Slide Time: 02:29]
you can get the data in through these statements and these are statements and 

(fig., 02:02)
these are (( )) high levels of abstraction 
so huh once you huh look at these at the low level at the machine language level 
the- are lots of details which need to be taken care of 
for example you need to worry about huh the formatting of data 
so huh when you are using printf statement you may huh you look at the data as huh high level language program sees it in terms of huh arrays and structures and huh these structures could consist of huh various types of uh primitive elements integers floating point strings and so on 
so how all these as to be converted into raw form of data huh th- that formatting as to be done 
the values have to be encoded in the form in which devices can understand and each device will have its own way of representing information
for example huh display device understands things  in terms of huh matrix or pixels huh a- character level device such as a keyboard huh ((talk)) understands things in terms of ASCII characters and so on
so huh the information when it is coming in as to be brought in from the form which huh peripheral device is sending into a form which programmer would like to see and vice versa
then huh the the I O operations at the low level have to deal with the interconnection structure 
between the device and the processor or memory what kind of hardware is there what kind of bus is there 
so how to huh get the data through huh these paths or these structure as to be seen 
and then huh timing characteristics of the devices also as to be taken into account 
for example if you are trying to read from a particular disk track it takes time to have the head move from that huh from the current position to that position 
so there are certain timings involved which have to be huh taken care of 
so all these details have to be taken care by the system software and huh a- a programmer gets an abstract view 
[throat_clearing]
so huh to be more precise huh what is the role of system software in this whole operation 
{referring fig., 04:50}
so you you have format conversions examples huh
so you have I O library which when- when you compile a program huh there is a lot of huh
[Refer Slide Time: 04:55]

(fig., 04:50)

there are lots of library function library routines which get linked up into your object code 
so so they take care of converting some of the abstract operations into low level operation so things like format conversion would be handled at that level 
then huh there are complex details of the device itself 
the the issue of huh dots or the pixels in the pixel matrix or the the question of huh tracks and sectors of a disk drive
these details are known to huh what is called device driver 
so so device drivers look at huh specific peripheral devices or a family of peripheral devices and understand th- details of how these work 
how the information is to be handled and so on
huh then you you require to work with interrupts 
so so the events which occur in the I O devices or the- their timings 
they have to be handled through interrupts or exceptions and huh as we have seen earlier that huh these are handled by exception routine or interrupt handling routine which are part of O S kernel 
so huh there is an interrupt system huh the the number of interrupts which are possible for huh given processor and they are handled by huh a portion of O S kernel
[throat_clearing]
huh then there is also a question of multiple programs or multiple processes huh sharing different peripheral devices 
so huh several programs or processors trying to share a disk or share a printer or display on
display you have multiple windows coming up due to huh multiple tasks
so th- the sharing also as to be looked after and that is huh the role 
{faintly}one of the roles which was placed 
huh then in devices like base or huh hard disk drives pro- properties drives C D ROMs and so on 
huh the information is organized in terms of files
so you have certain logical view of files huh as you see huh in a high level language program 
but the- these files have to be mapped to huh physical devices in certain manner ((ok))
so huh you- you see some records in the file at the physical level they may be tracks and sectors
so how this mapping is done 
is taken care of by the file system part of huh operating system [Refer Slide Time: 07:23] 

(fig., 07:44)

so they are- all these different aspects huh which huh take lot of ((drudgery)) of I O operations away from huh a high level language programmer 
[throat_clearing]
huh now the requirement of different devices huh could vary quite vastly depending upon nature of the device the kind of speed it has huh th- th- the- could be huh the- the demand which these devices place on the system ((may be)) quite different
if you consider for example slow devices like keyboard or mouse huh then the- these huh are producing information which is governed by the rate at which a human operator can enter for example keyboard is limited by the speed at which you would type ok
so huh th- th- the demand is that you are transferring a- at times only one character one byte at a time or maybe a small groups of bytes at a time an- and the rate at which this happens could be huh just a few bytes per second 
same thing you could say about mou- mouse huh th- the mouse basically is tracking huh the position and as you move the mouse from one place to other place huh essentially huh what is being sensed is the movement
whether it is in plus x direction minus x direction plus y or minus y or a combination of it 
so huh from one position to other position the- the movement huh is huh slow huh  imagine that processor is working at the rate of huh several gigahertz ok
and huh th- the mouse movement is in terms of probably a fraction of second 
so huh the the rate at which the information is being generated is pretty small
huh so th- th- you are handling small volume at a time maybe just a character or a few character and at a small rate 
on the other hand if you look at huh network as a peripheral huh the- the rate could be ((fairly)) fast and you also look at huh a block of information 
you you are not talking of sending a byte on a network or sending ten bytes on network 
[rustling]
i- its always in terms of huh big blocks or packets 
so the- the way in (( )) you would like to see huh network operation is that you- you have a block of data in memory ok
you- you want it to be sent out on the network or you have a block of data coming from huh network (( )) 						[Refer Slide Time: 10:00]
you want it to be placed in a particular position of memory 	
so huh the the rate could be huh for example the- the- network standards talk of ten megabits per second or hundred megabits per second or one gigabits per second 
now (( )) going- to ten gigabits per second 
so so which would mean typically a few hundreds of huh few few huh tens or hundreds of megabytes per second ok
now lets turn our attention to graphic display huh 
in graphic display what you want is huh some some picture or some text or some combination of these huh which is steadily displayed on the screen 
so that means the the information on the screen as to be refreshed so that huh a human eye sees it as a constant image
so you need to refresh it huh several times a second could be huh twenty-five huh times a second or thirty times a second and
huh what is done is that you maintain an image of huh what- what you want on the screen in memory 
so it huh would be called video memory and this memory would be huh within the address space of the processor 
so so basically huh any any change you want to do on the screen 
you- you- you modify certain area in the memory huh which is directly reflected on the screen so so there as to be huh some activity which takes huh matrix of pixels as stored in the memory and huh refresh certain number of times per second on the screen 
so huh this is how most of the modern systems work huh 
in earlier days the the graphic display or the ((al-)) display used to be a peripheral sep- separately huh which was linked through a serial port 
so huh it would mean that huh sequence of characters or sequence of bytes would be sent reflecting what you want to display or the change which you want to make and- and then huh some intelligence in- in the device would huh take that sequence of characters and huh modify the display accordingly 
so huh there huh the chan- the rate at which you can change things would be limited by the rate at which you can encode the changed information and send it over huh serial port
huh but now in ((in)) P Cs and huh all workstations huh the information is directly picked up from a buffer in the memory and and display it
so you can continuously refresh ((in there)) huh look at disk drive 
in the disk drive typical operation you may like is that huh be- before you do the data transfer you you want the head read write head to be positioned on certain track and certain sector 
[throat_clearing]
so so here is an operation which does not involve huh bulk of data transfer huh
i- it is it is a ((pr-)) action huh which is required 
so now huh looking at huh these four example which i have taken 
you can see large variety in terms of the the rate at which data gets transferred 
the quantum of data which is handled and huh the the the the mechanism of data transfer 
for example ((if)) data automatically being picked up from some area or it is being explicitly transferred as and when required
ok so graphic display was an example where huh automatically the the data keeps getting transferred whereas huh in network or in huh mouse or keyboard example 
we are doing it on demand 					[Refer Slide Time: 13:55]
so- as some keys get huh depressed on the keyboard then the information gets transferred or huh in- in network for example when something comes from outside on the network then a transfer occurs 
in in a in graphic display huh you you may change that huh pixel matrix as and when you want but huh transfer from pixel matrix huh to the display device is a continuous activity huh now for huh any of these operations huh you need to address huh the- the- are large number of devices 
you need to talk to them huh the processor needs to address them huh before it could talk to them huh so each device or strictly speaking the controller of the device appears as a set of registers to the processor ok
huh you you can view them as registers or view them ports huh in the sense that they 
they are actually allowing you to talk to the outside world 
{faintly}so you can also call them ports 

(fig., 14:45)
so huh what do you do with this ports 
you can for example huh give commands to a peripheral device huh 
you so so there would be some register huh in which command could be given
so you load some huh code in a particular register 
it will be treated as command and the device would be expected to work on that command 
commands may also involve some parameters
for example a command to seek the disk m- you may have to specify the track number and sect number 
you can read the status of the device 
so basically huh certain register would be designated as status register and you can read huh that register and check specific bits to find out if certain things have occurred or not ok for example huh if key as been depressed on the keyboard or huh a- a packet as come from the network or or so on
also there could be registers through which actual data could be read or written 
so huh all these huh each de- depending on the requirement of the device each device would be seen through a collection of registers and huh these registers must have address so that at any given time huh a program running in C P U can read or write a register 
[smack] 							
so the- are two ways addresses can be assigned to this 	[Refer Slide Time: 16:30]
you can have a memory mapped I O which means that huh in the entire memory space memory address space you can leave some area which is (( )) for input output 
so you might say for example huh at the higher end of the address space you leaving huh one one k area which is not to be utilized for memory and it is ((dedicated)) for I O
so last one thousand addresses are reserved for register
so- or it could be first few or w- wh- whatever a particular architecture huh as (( ))
so huh you you are basically reducing the memory space by a small amount which may not really be huh very critical and huh tha- that area is available for I O 
the other alternative is have a separate address space for I Os 

(fig., 17:25)
so lets look at each of these 
huh as i- just mentioned in memory mapped I O part of memory address space is reserved for I O and huh you can use the usual load and store instructions for reading or writing the register which are actually located in that space
so huh memory is designed to ignore huh the addresses which are following in that ((range)) so let us say you are using last one k addresses for I O in- in a given address space 
so if huh if a if a load occurs if a load instruction is good ((it)) and address is found to be following within the ((train)) the memory doesn t respond
but appropriate I O device huh to whom the address as been assigned will respond ok
so so th- th- the data which would be sent to the C P U will not be ((by)) memory but it will be huh the particular I O register which will send the data and similarly 
when you are doing a store instruction huh depending upon huh where the address falls 
address falls in memory range or I O range the the data will go to memory or I O accordingly 
huh in in the second approach you have to have different huh address space for input output 
and it has- it need not be as large as huh what you have ((f-)) memory it is much smaller 
it could be huh you could have huh lets say huh ten to fifteen bits trying to address I O devices
so it need not be very large address space 
huh and when huh processor is trying to read from memory or read from I O device 
the distinction would now be made by not by the address value but by some control signals which will be flowing on the bus			
so i- in the in the first case huh depending upon the address value you can determine whether address is meant for I O or for memory but in this case 
[background noise]{sounds like someone coughing}	[Refer Slide Time: 19:23]
same address could fall in both the ((spaces)) an- and therefore you need something additional to distinguish whether address is meant for I O or memory
so the- there are additional control lines which are included in the bus and they sp- they indicate whether huh memory is being addressed or I O is being addressed and huh you need different instructions to work with I O in such a case 
so huh the- could be additional instruction similar to load and store right
but huh different op-codes which will indicate that these are for huh transferring data to and from I O devices

(fig., 20:06)
huh before huh transferring data 
huh a program would often need to check whether a device is ready for transferring data ok so for example huh take- take the case of keyboard huh the- the processor needs to determine if key as been depressed or not
so it- needs to periodically check whether huh key as been depressed and its code is available or not 
so so this as to be done periodically and huh the the frequency at which you sample this as to be huh enough so that you don't miss out anything ok
so if keys are being depressed at certain ((rate)) lets say huh at the rate of five key per second huh and you are checking only once in a second then you are going to miss out
so you have to check frequent enough frequently enough
[background noise]{sounds like someone coughing}
so that nothing is missed out 
this approach is called polling wh- where you are periodically sampling and huh trying to see huh trying to huh you would read a status register check particular bit and figure out if what you are looking for is there or not 
an alternative is that huh processor doesn t get tied up in checking the status 
it- it its busy with something else ok
huh it could be doing some other useful task and- whenever huh there is a need for device to invoke action from C P U it will send an exception or interrupt ok
[background noise]{sounds like someone coughing}	[Refer Slide Time: 21:45]
so huh the device will intimates its readiness by raising an exception in huh 
it basically means huh that a signal which is going to which is going as input to the processor is activated and an- processor huh would have huh a hardware which will huh transfer its control to exception routine wherever that signal gets activated
so huh in general a processor may have huh multiple interrupt- signal coming from outside devices 

(fig., 22:20)
in terms of huh time huh 
you you can visualize the polling activity like this that 
let us say huh the- some status is changing at this particular instant 
[background noise]{sounds like someone coughing}
an- and you are polling at certain rate ok
you polled here spent some time in reading the status and figuring out if huh device is 
[throat_clearing]
ready or not 
you found it is not ready ((and)) do something else come back check it again found it not ready 
come back here find that status is ready then you huh carry out the process-

(fig., 22:54)
[cough] [noise]
lets look at an example huh and put some numerical value 
see huh what would be the implication of such an activity 	[Refer Slide Time: 23:02]
suppose there is a processor running at the rate of five hundred megahertz 
{faintly}(( )) five hundred megahertz clock is there 
[smack]
and huh every time you need to go and poll and take a decision 
suppose it consumes four hundred cycles 
[throat_clearing]
then we want to see how much is the polling overhead for different types of devices with different speeds 
so lets say first example is mouse huh which needs to be polled let us say thirty times a second 
ok an- an- this figure is huh keeping in mind how fast one would typically move the mouse
second case is you have floppy disk drive which transfers the data at the rate of fifty kilo bytes per second and huh each each time every time you get sixteen bits of data or two bytes 
ok then thirdly you have hard disk drive which transfers data at four megabytes per second 
and the unit of data of data which comes out is four words at a time right 
so now in th- these three cases huh the- there is a huh vast variation in terms of speed and you- your polling as to be faster and faster accordingly
huh as we will see that huh when you have to poll very frequently you are you are wasting lots of time and C P U gets tied up huh to a significant extent in this polling activity
so in the first case we are we are trying to poll at the rate of thirty times a second 
[breathing]
and every time we huh poll we are wasting ((- consume-  )) 
let me not say wasting we are consuming four hundred cycles
so thirty times four hundred is the number of cycles consumed in polling every second right and huh C P U is running huh lets say five hundred cycles in one microsecond or five hundred million cycles per second 
so as a fraction or as a percentage huh what is the fraction of the cycles which actually are lost in this activity and not in executing other computation instruction 
so so we we divide this huh this is the number of cycles per second divided by this cycle per second we get a ratio huh convert that to percentage you get point zero zero two percent
so huh say very small fraction of the time which is spent and huh we we will ((settling out mind -)) doing huh polling of mouse in this particular manner ok huh very little overhead 
as you go to floppy disk drive huh th- the rate of sampling or rate of polling is higher so we have to poll huh at least at this rate ok
huh every every data huh we- we should every time its coming at this rate ok
sixteen bits at a time so so basically huh we are getting two bytes at a time 
so so number of times we are actually transferring is fifty divided by two huh into tens power three 
so this is fifty kilo bytes per second taken half of it because ((divid- may lead to)) two bytes at a time 
so we must poll at least these many times per second alright  huh and again similar factors 
four hundred cycles ((lost)) every time we poll divided by the processor clock 
we have two percent 	
that means huh only two percent of the cycles huh are dedicated for this 
so so this is huh not as comfortable as huh mouse but still tolerable 
the only problem is if we had several of such devices then these percentages would add up and we would have lost sufficient time 	[Refer Slide Time: 27:12]
but but huh with the lets say this is an isolation 
it still a tolerable device if you do not have too much of other activities 
finally huh we move to a fast even faster device four megabytes per second and four words at a time 
so this four megabyte per second figure is divided by sixteen because we are trying to transfer four words or sixteen bytes at a time and huh each time we transfer we spend four hundred cycles divides by the clock rate 
we get a figure of twenty percent wh- which is huh which is clearly ((undesi-)) 
you cannot spend twenty percent of the time huh just polling a particular device right 
so we need to do something better than that and huh solution for that is interrupt driven input output 

(fig., 28:10)
huh here what we do is 
we huh allow a device to inform the processor as about its readiness 
so huh processor will huh probably instruct the device and huh ask it to huh send an interrupt or send an exception as and when huh it needs processor attention 
so processor attention is given to the device by transferring control to exception handling routine and huh exception handling routine will do the needful so th- the way you handle exceptions here coming from external word of input output 
the mechanism is same as what you for internal exceptions which may come because of arithmetic ((overthrow)) or illegal instruction or page fault and so on
huh the the difference here is that these exceptions which are coming from outside 
they are asynchronous with respect to the instruction
whereas huh internal exceptions will come at specific time for example arithmetic overflow would be detected in a specific cycle of an instruction 
but huh exception or interrupt from I O device could come at come in any cycle
it as no links with a specific instruction and huh therefore huh what you would typically do is that huh at the end of instruction execution you will see if there is an exception from external word and then huh respond to that 
huh these exceptions are given comparatively lower priority as compared to internal exceptions 
among these external exceptions huh the priority may be according to the speed 
so devices which are faster need early attention and a- therefore there are given priority 
slow- devices are given lower priority 	[Refer Slide Time: 30:00]
so huh you might have several exceptions occurring from I O devices at the same time huh an- and you will serve them according to the priority 
similar to internal exceptions which we discussed earlier the the (( )) could be vector interrupt or non vector interrupt  
vector interrupt means that huh the control directly gets transferred to an exception handler which is meant for a specific exception and in non vector case you you huh set the calls of exception in a particular register but transfer of control takes place to a specific location and then- then you are going to run some code 
check that ((calls)) register and branch of to one of the points 
so huh both the possibilities exist here also 

(fig., 30:53)
[smack]
now lets huh get back to huh similar scenario and try to quantify the overheads of interrupt driven transfer 
so we have same processor working at five hundred megahertz huh
now let us say that overhead per interrupt is little more than the overhead of ((polling)) 
so suppose it is five hundred cycles huh and we want to see how it is going to work in case of hard disk drive 
now huh if disk drive was always active then things are going to be as bad as polling ok
so so you you you will get continuous interrupt 
infact it would be worse because every time there is an interrupt the overhead is larger huh but th- but the reality is that device or huh hard disk drive in this particular case will not necessarily be always active 
so suppose its active in in entire huh duration of program execution only five percent of the time 
so so wh- when it is busy then data is coming at a fast rate when it is not busy there is no activity 
so in such a case we will definitely have huh saving of time here because the the device would inform huh only when it needs the attention 
when- when it is inactive nothing will happen and the- are no interrupts 
but in a in a polling based approach you you have to continuously keep polling ok
so now huh we have huh the overhead if the device was continuously busy huh 
we have same expression four megabytes per second or one by sixteen 
because we are transferring sixteen bytes at a time huh this is the overhead 
[Refer Slide Time: 32:40]
huh number of cycles per interrupt and divided by the clock sequence 
you get twenty-five percent 
so instead of twenty percent we have twenty-five percent but since huh we- assuming that the device is only busy or active five percent of the time
we take five percent of this and the real overhead will be one point two five percent ok
so th- that s where it will score 
now huh at times even one point two five percent huh overhead may also not be ((desi-)) particularly when- when you have number of such devices huh hard disk drive in a system is only one
you have display you have network and you have other things   
so huh if all such overheads are added then lot of huh process time will be consumed 
so a- solution for that is huh not to involve processor in transfer of every word or every group of words 

(fig., 33:55)

what you could do is you could allow the device to deposit data directly into the memory or read data directly from the memory and that operation is called direct memory access
so the processors role in direct memory access would be only to initiate huh the trans- or setup things to tell the device that huh this is the amount of data you need to get 
lets say if it is just huh from track number so and so sect number so and so huh tra- huh get so much of data and huh it also specifies that this data as to go to this particular area in the memory 
so once huh that initialization is done the processor can get busy with something else 
and as the words or bytes come from huh disk drive they get transferred directly to the memory and processor doesn t come to know of that 
the processor of course needs to be ((informed)) when the entire job is done and th- that time processor can see huh check huh the transfer as taken place correctly or not huh for example if in-between huh you you found a bad sector or something ((then)) some (( )) occurred then status would be ((sent)) accordingly and processor interrupted so the processor as to see at the end whether I O operation end- successfully or there was an error 
huh this this entire job actually is entrusted to huh special controller which is a special piece of hardware its called D M A controller 	[Refer Slide Time: 35:15]
so it is in the D M A controller you would huh specify memory addresses 
amount of huh word amount of data to be transferred 
the direction of transfer and what is to be done at the end of the transfer 
a- a- D M A controller could actually take care of not just one peripheral device but multiple peripheral devices which huh are required to transfer data directly to the memory
so it could be a two channel D M A or four channel D M A huh 
four channel D M A for example would allow four high speed devices to transfer data to huh memory or read data from memory ok
so so basically it s a you could say its an extension of the processor logic or ((augmentation)) of the processor logic which looks after this ((row))
so the sequential address in the memory huh will be provided by this D M A controller 
it will start from the starting address and as and when data comes huh it will keep on updating the memory addresses and ((detect)) when the whole transfer is done 

	(fig., 36:18)		      
now lets huh le- lets try to quantify the overheads in a D M A like situation 
so we again have same processor huh running at five hundred megahertz 
suppose huh initiation of D M A requires thousand cycles 
so all everything initially is setup and huh th- that s one time when processor gets involved 
huh next the processor gets involved when huh end of D M A interrupt comes and let us say five hundred cycles get ((consumed)) in that 
so so in the in in one huh D M A activity 
basically the C P U is spending thousand cycle plus five hundred cycle 
lets imagine that the block size huh the chunk of data which gets transferred in one D M A 
is eight kilo bytes 
so huh by by spending this one thousand five hundred cycles 
the processor will achieve transfer of eight kilo bytes 
and for next eight kilo bytes you may have to repeat the process
so huh we we can now even look at situation when hard disk drive of the kind we discussed 
in previous example is always active 
so suppose it needs to transfer four megabytes per second continuously alright
and huh the overhead then will be 
you have huh four megabytes per second transfer rate and huh since each time you are each D M A operation requires eight kilo bytes 	[Refer Slide Time:38:06]
the ratio of these two will give how many D M As per second are being done ok
huh and each time you do D M A you are spending these many cycles
this in the beginning this in the end and- divided by the clock frequency huh will give you the fraction of time spent on the D M A by the processor
so of course huh D M A controller as to do lots of work
the work as been taken of ((an)) outsource by the processor here 
so so this comes out to be huh ok
{faintly}you can see that huh the the first factor the ratio of these two is five hundred huh 
{faintly}this is three into tens power minus six so it comes out to be point one five percent
which is quit- quite an ((-table)) figure alright 
as- as we have seen from huh polled transfer- to interrupt transfer- to D M A transfer- depending upon huh requirement of the device 
we- we can chose a suitable mechanism for transfer and huh as you go along this progression there is higher and higher performance huh but but the there is a cost issue also 
D M A controller means huh additional cost 
similarly interrupt also requires certain mechanism huh 
so there is extra hardware which also adds to the cost
huh one step further from D M A is the concept of I O processor huh a- a- D M A can be huh initialized to do one one block transfer 
but huh an I O processor could actually huh be asked to do a sequence of many D M A like operations 

(fig., 39:39)
so so it could be huh asked to do lets say huh 
read a block from the disk that s operation number one 
send a block of different size from a different area in the memory to the the network controller
that could be second operation 
third could be huh send a block of data to a printer and so on     
so a sequence of operations each typically involving a D M A of certain huh block size huh could be defined and huh I O processor could go through this sequence 
so fo- for an I O processor huh each appears like an instruction or an I O operation ok
so huh the- these instructions could be stored as a small I O program somewhere either in huh a dedicated memory for the I O processor or somewhere in the main memory 
so e- each I O operation i- is a major activity and huh I O processor will sequence from one to other
{faintly}in this particular ((manner))	[Refer Slide Time: 40:55]
now huh when you are talking of huh D M A huh essentially there is a direct communication 
between memory and huh one or more I O devices 
now how do you view this in in light of the fact that a memory is not something flat huh 
there is a memory hierarchy 
you could have cache one or more levels and you also have virtual memory 
so huh how does huh this whole thing work when memory hierarchy is present 
so huh wha- what are the problems caused by presence of virtual memory 

(fig., 41:18)
the the programs which are running in the processor 
they look upon memory in terms of virtual addresses ok	
whereas huh th- the D M A devices will typically work with physical addresses 
so- so there is a discrepancy between these huh what it may mean is that huh you 
you need to do huh translation of the address 
so huh when you are initiating a D M A ok 
you you would need to specify a physical address for the D M A controller 
so you need to tell D M A controller that transfer lets say eight k bytes of data to this physical address huh an- and the program would be working in huh in terms of virtual addresses 
so huh if you have created an array you you know its virtual address
so there as to be an explicit conversion of that virtual address to get to the physical address
now huh remember that the virtual addresses do not map linearly to the physical address ok
so so in virtual address you have lets say one page and then next page huh contiguous but they they may map to huh two pages in the memory which are not contiguous which may be at arbitrary addresses 
so it huh it may not be sufficient for this translation to be done once 
huh one of the huh one of the ways to handle this is that huh you restrict huh each D M A activity to at most a page ok
so suppose a page size is four kilo bytes and you want to transfer eight kilo bytes huh then there is a problem that for you- your data is coming in two pages 
you need to specify huh the the physical address for each page ok
you you cant specify huh one physical address and say that transfer eight kilo bytes of data to this because it it may not go to contiguous areas in the physical pages 
you you want them huh in contiguous area in virtual space 
but in physical space it is not contiguous 	[Refer Slide Time: 43:41]
huh i- if you ((are)) limiting if you limit your data size or D M As data size to one page then this problem is taken care of ok
[smack]
so basically huh if you want to transfer larger piece of data you have to break it up and huh in- the overhead of multiple D M As 
[smack]
huh what about cache when you have cache huh what problems it can possibly bring in huh there is essentially a problem of consistency because C P U (( )) is directly talking to cache and not to the memory ok 
whereas huh D M A device will typically have no access to the cache 
it it write to the main memory or read from main memory 
so th- there is a problem of huh consistency 
one one one of the two memories may have more recent information as compared to other
so for example if there is a huh D M A input happening 
the information coming from devices getting put in the memory and huh suppose it gets put in the block for which copy exist in the cache then what you have in the cache is out of date huh the information in the main memory as been updated by the device and what you are seeing in the cache what processor is seeing in the cache is huh not (( )) information 
it is not huh up to data 	
huh you can also have reverse happening huh that huh C P U as put something in a cache block and huh it is expected to be sent out to a huh device through D M A 
but D M A- D M A control will pick up information from memory 
which is not ((upto date)) particularly with write back cache where there is a huh very you are not updating main memory immediately so huh in write back cache if you recall huh data in the memory gets updated only when a block is getting evicted 
so you keep on making updates into cache blocks and bu- but if huh corresponding block from main memory was being ((returned)) to a D M A device then you have a problem
so huh th- th- this actually similar problem occurs in a multiprocessor cases also
huh imagine that you have multiple processors which are trying to share same memory huh they would typically have their own caches ok 
so just imagine two processors their own cache but a common shared memory 
so huh once again there is a data which huh processor one as to communicate to huh processor two through shared memory huh th- the two caches may be inconsistent 
so huh the solution for this lies in actually what is called cache ((co-)) protocol so
[inaudible background noise]
huh you are talking of the first problem virtual memory
yeah if you are limiting your data size to a page fo- for huh D M A transfer then you are definitely increasing the overhead 
so suppose you want to transfer huh sixteen k bytes of data huh in D M A 
and your page size is four kilo bytes huh i- i- if this virtual memory problem did not exist you could have incurred the D M A overhead only once 
but now you will have to break it up into four D M As 
each time you supply appropriate physical address and huh D D M A occurs huh without any problem but huh the overhead has occurred four times
so you increase the overhead ok the the problem of huh cache ((in-)) is solved by what is called cache ((co-)) protocol	[Refer Slide Time: 47:30]
so so these protocols are huh additional actions which huh cache controllers have to take when multiple copies of data exist in multiple caches and memory 
so huh there could be one or more cache in memory which is cause of having multiple copies of data and something is getting updated something is not getting updated which is the problem of coherence 
so huh these protocols huh i will not go into details of this 
are designed so that huh coherence is maintained for at- at least huh i- if if two copies are going out of sync huh everyone comes to know that they are out of sync and one ((way)) still updation occurs
so huh this is also means additional overhead work which ((is)) to be done ok
so i will stop at this point and summarize 

(fig., 48:30)
[noise]
we started by looking at the role of huh system software in huh I O operation and we noticed that huh ((a)) software is required 
a system software particularly as to handle huh device specific details 
it as to handle interrupts 
it as to take care of sharing of device by multiple processor huh
look at the file organization and so on 
huh we saw that devices are viewed as set of registers by the processor and they can be addressed either by using part of the memory address space or defining huh separate address space 
we saw two mechanism of checking the d- device status huh polling and interrupts
huh polling as higher overhead particularly huh ((it)) become undesirable for fast devices
and interrupts ((is)) used 
huh and then they are (( )) interrupts also huh not acceptable 
you have to have huh direct communication between memory and the device 
that s called direct memory access 
(( )) which is huh which is a complex mechanism but it is essential for fast devices huh like network controller disk drive and so on
i will stop at that thank you 
[Refer Slide Time: 49:45]




Educational Technology Services Centre
IIT DELHI

Computer Architecture

Lecture #37
Input/Output Subsystem: Designing I/O Systems

in the previous two lectures we have talked about individual aspects of I O subsystem talked about peripherals and the buses which interconnect them huh
we also have huh given some thought to how software looks at the whole operation 
[background noise]{sounds like someone coughing}
today we look at the overall system design issues and primarily huh the discussion will be in the form of examples where we will see that huh 
given certain building blocks and certain requirements how we put them together to achieve huh certain performance objectives 

(fig., 01:34)
[throat_clearing]
so huh wh- when you are designing the system huh you have to keep performance in mind 
and- as we discussed earlier the performance could focus on the latency aspect or the throughput aspects or- or a combination of these 
[background noise]{sounds like someone coughing}
the latency is important in terms of huh achieving certain response time 
so huh when- when some event occurs outside huh 
the system as to respond and huh the response of I O subsystem is very important in that case 
[throat_clearing]
so huh you have data which moves from external ((world)) into the system 
it processed and then maybe some response comes back 
so huh as the data moves through various stages all the delays get added up
the the delay may be huh in in terms of the rate at which the data gets generated at the peripheral 
[background noise]{sounds like someone coughing}	[Refer Slide Time: 02:38]
huh the way it huh crosses through controller buses and adapters 
and huh finally it goes through huh processing and follows huh somewhat similar path
[cough]
huh on the other hand huh there is a throughput issue huh where essentially the whole thing may actually like a pipeline and th- the slowest link in the whole system huh would basically ((dictate)) the throughput 
[throat_clearing]
so huh as far as throughput is concerned its comparatively huh easier huh
at least if you have to find the average
so you can image a ((sturdy)) state with data flowing and if you if you understand the capability at each stage ((about)) at what rate the data can flow 
the- then you can get (( )) estimate of the throughput huh
but latency could often be ((a)) more complicated issue 
for example the delay huh a block of data and counters going through for example a bus huh would also depend upon how much the load is being carried on the bus
so huh you need to look at complex interaction between various pieces of information which flow through huh various components of the system and huh
at times (( )) may not be very easy and the last resort may be that you try to simulate the entire behavior 
[background noise]{sounds like someone coughing}
and huh then- then see in a very accurate manner huh what kind of latenc- huh you can obtain
[throat_clearing]
so huh in the example you will consider today we will focus more on the throughput aspects
so we we will imagine huh ((sturdy)) state flow of data and huh li- li- (( )) flowing through various pipes 
we will try to see what is the throughput 
what is the rate at which data can flow and- that will give as idea of throughput

(fig., 10:44)
[throat_clearing]
so first example huh we will consider is huh one ((where))
[cough]
huh there is an application where huh huge amount of data as to be huh read out
read from various disks
so huh or it could be the reverse that- there is huh s- lots of data which have to be stored on the disk 							[Refer Slide Time: 04:44]
so huh functionally it s a very simple application that you- you have huh processor memory and lots of disk huh
so we we want to huh attain certain 
we want to maximize our throughput huh 
that is huh as much data we can huh pull out of the disk huh 
for for a given capacity of processor memory bus and other components ((in)) the system 
[background noise]{sounds like someone coughing}
we will try to achieve that 
so the architecture you see here as huh a processor memory bus on which huh certain number of adapters buffer after (( )) each provides an I O bus 
so in this case we consider (( )) two I O bus huh which is supporting multiple disks ((ok))
so so each can have huh one or more disks and huh the the ((disks)) read the data send the blocks which huh ((travels)) through this bus go through the adapter and ((ul-)) ultimately go to the memory 
huh the processor for each block of data which huh gets read 
there is some certain amount of processing involved huh from both point of view to organize the I O as well as to actually upon the data 
[smack]
so huh lets look at some numerical parameter associated with various components huh
the the processor as an instruction execution rate of three hundred million instruction per second ok
so that quantifies the capacity of processor to perform certain tasks 
the bus which connects processor and memory is capable of handling a throughput of hundred megabytes per second ok
so data which come from various sources is huh con- on this and 
when it is b- deposited in the memory one as to keep in mind that there is a certain throughput limit here and how how we arrive at huh bus throughput limit 
we have discussed depends upon the protocol the- the size of block which is being transferred and so on
so suppose that analysis was carried out separately and one comes- comes up with a figure of hundred megabytes performance second 
[cough]
then these (( )) two buses ok huh each has a capacity of twenty megabytes per second 
so so these buses are huh carrying the data from the disk towards the memory 
each (( )) controller is capable of handling upto seven disk drives ok
and each disk drive huh requires ten milliseconds to access 
that is huh seek time plus rotation latency plus controller delay 
and then rate at which data gets huh read out data gets transferred is five megabytes performance second ok
[background noise]{sounds like someone coughing}
[throat_clearing]
huh now what let also define what each I O operation is 
[background noise]{sounds like someone coughing}
each I I O operation involves sixty-four kilo bytes of data to be read from the disk
so let us say it as to be read from some contiguous area on the disk from (( )) a single track but eve- every time you read a block of data 		[Refer Slide Time: 08:15]
it may be a different track 
[cough]
so so every time we have to huh (( )) ten milliseconds of delay before we start reading the next block and once we huh have incurred the ten milliseconds delay then huh sixty-four kilo bytes will come continuously at the rate of huh five megabytes per second 
now as far as huh the huh S C S I bus is concerned huh 
when one disk is busy in seeking or huh its rotating to get to the position huh other disk could be transferring the data
so so seeking and transfer huh among different disk could overlap
[throat_clearing]
ok now huh each I O operation of huh sixty-four kilo bytes transfer requires huh some work on the part of the processor
so huh one hundred thousand instructions are executed
[background noise]{sounds like someone coughing}
by the user program and fifty thousand instructions are executed by operating system 
so huh imagine that huh each transfer takes place using D M A huh process 
which means that huh th- the processor will initially setup huh the process 
huh it will instruct the D M A controller huh 
we are not showing so lets say that all the huh (( )) control and D M A control is lumped within the bus adapters 
[smack]
so it as to initially instruct the D M A controller to read from a particular track and particular sector huh from a particular disk and huh it also supplies the address in the memory where things have to be deposited
huh so that s the initiation path and when the transfer is over it needs to be informed with the help of an interrupt 
so again some instruction get executed 
so on the whole O S goes through fifty thousand instructions huh
all put together beginning and end and th- the data huh is- processed by the the C P U
huh it requires huh hundred k instructions of the user program 
so what is that we need to do 
what is the problem
problem is to huh find out what is the maximum input output rate
that huh this system can sustain 
so the fixed components here are 
there is one single processor one single memory and a single processor memory bus
the the flexibilities in terms of huh putting as many disks as you can -fully put in the system 
and depending upon the the number of disk you have you will require certain number of S C S I buses and corresponding controllers
so the problem involves first finding out what is the maximum I O rate
that means how many I O operations as defined here can be carried out ((ok))
what at what rate can you (( )) and to achieve that how many disks and ((sp-)) controllers are required ok
so now huh to solve this fo- huh first lets analyze what are the limits of individual huh component and huh the the limit will come from the fixed component 
that is the processor and the memory and corresponding bus      [Refer Slide Time: 11:42]

(fig., 11:27)
so huh th- the processor as to execute hundred plus fifty 
that is one fifty thousand instruction per I O block and huh this execution happens at the rate of three hundred mille three hundred million instructions per second 
so huh maximum I O rate as far as processor is concerned is huh three hundred million divided by one fifty k ok
so many I O operations per second is possible 
so so this turns out to be two thousand I O operations per second 
so if if processor if rest of the system was capable of huh matching this rate 
the the processor is ready to do two thousand operations as defined here per second 

(fig., 12:35)
the the next huh object is processor memory bus 
so what is the limit as far as this is concerned 
huh the bus has a bandwidth of hundred megabytes per second 
each I O block huh it may come from some disk and some (( )) controller 
ultimately it will land up on the same bus 
and the the bus will have to carry ((out)) transfer of sixty-four kilo bytes 
so now from these two figures you can figure out huh hundred million bytes per second is the capability 
sixty-four k bytes you want to transfer 			[Refer Slide Time: 13:02]
so basically you get huh one five six two I O operations per second 
now out of these two figures two thousand of C P U and one five six two of huh P M bus
[background noise]{sounds like someone coughing}
obviously this is smaller figure and therefore that s the bottle neck 
so processor as certain (( )) and this will dictate the rate 
so huh out first answer is that overall maximum I O rate is one five six two I O operations per second 
now the question is how do we achieve this huh
how many disk we need to put to huh do this 

(fig., 13:44)
huh so so lets look at one disk at a time
huh the seek plus rotational latency is ten milliseconds 
and huh with five megabytes per second of transfer rate 
and sixty-four kilo bytes of block size 
the transfer time would be sixty-four kilo bytes divided by five megabytes per second 
it comes out to be twelve point eight milliseconds ok
so- so now huh for each I O operation basically disk as to spend ten milliseconds followed by twelve point eight milliseconds
and therefore on the whole each disk huh 
when it gets involved in I O is busy for huh the sum of these two figures
that is ten milliseconds latency plus twelve point eight milliseconds transfer time
[background noise]{sounds like someone coughing}
((are)) total of twenty-two point eight milliseconds 
so if disk was to be continuously asked to seek and then transfer 
seek and transfer
the rate at which huh this can (( )) blocks is reciprocal of twenty-two point eight milliseconds or forty-three point nine I O operation per second ok
[throat_clearing]
so the- the throughput huh the throughput which actually disk is huh sustaining is sixty-four kilo bytes have been generated in a time of twenty-two point eight milliseconds
so its huh although transfer rate was five megabytes per second 
effectively we are getting two point eight megabytes per second 
i- is the demand each disk is placing on the (( )) controller         [Refer Slide Time: 15:18] 
so that much of data (( )) it is pumping on the (( )) bus and huh (( )) bus will con- from various disks and send further 
so so now huh to begin with lets-
we want to use this figure to get the number of disks 

(fig., 15:36)

huh we have seen that maximum I O rate of the system is one five six two I O operations and I O rate of the hard disk drive is forty-three point nine 
so the total number of disk drives which you require to saturate the P M bus is the ratio of these two fifteen sixty-two divided by forty-three point nine 
[background noise]{sounds like someone coughing}
so rounded of to integer you get thirty-six disks
now the question is of how do you distribute this thirty-six disks to huh 
[throat_clearing]
huh various (( )) controllers 
the the number of controllers you require at least is thirty-six by seven (( )) of that
because each huh controller is able to at most sustain at most control seven of these
so so now this limit of seven huh is from the point of view of its huh addressing capability ok
we we also have to check huh whether huh the data generated by these disks will be handled by the (( )) controller 
so the number of controller huh by this ratio comes out to be six ok
and huh accordingly the number of drives which each controller is assigned if you distribute them uniformly huh
it comes to be six so six thirty-six drives are grouped into six groups 
and huh each controller will see a throughput of six times two point eight ok
{referring fig., 13:44}
we have we have seen earlier that each disk is actually 
[background noise]{sounds like someone coughing}
pumping data at the rate of two point eight megabytes per second 
so which is sixteen point eight megabytes and therefore this is within the limit of huh twenty megabytes of S C S I bus 
so so therefore answer huh which we have got here is fine
that you need thirty-six disks and six controllers 		[Refer Slide Time: 17:27]
so so that that was one exercise where huh the the parameter- were number of disks and number of controllers and we were able to analyze and de- huh 
how this is figure- out 
[throat_clearing]
huh before i go to next example is there any question 
so huh th- the very simple calculation very simple rules ((where)) no complicate- formula 
you- you just have to see how huh things are put together 
and huh th- th- there is lot of actually commonsense which is behind this
once you understand the operation of how things are happening huh rest huh 

(fig., 17:44)
{faintly}is very straight forward 
ok the second problem we take is again huh design problem huh
what we are trying to do is 
[smack]
huh we want to design a multi-function unit 
huh particularly a something which can do photocopy and huh send and receive faxes
and the way ((we)) want to do this is 
we we take a low cost scanner a low cost printer huh and a modem and connect them through some inexpensive processor and huh suitable memory ok
so so put these peripheral devices together 
and huh we we have certain performance objectives
i will come to that in a moment 
so functionally huh these will take care of this huh 
that is huh you you can scan a document and fax 
you can huh take an incoming fax and print it 
you can also huh scan and print so which effectively like a photocopy operation 
[throat_clearing]
so huh this is what i already said 
modem plus printer function as fax receiver
scanner plus modem function as fax sender
and scanner plus printer actually as photocopier
so of course in-between you have a processor and memory sitting in the background
huh n- now we also assume for our further calculation that huh [Refer Slide Time: 19:29]
every photocopy operation may require certain number of copies 
that you- you scan and print a couple of copies 
let us say the average number of copies you print huh 
for every document you scan is huh three ok

(fig., 19:45)
[throat_clearing]
then there are certain operating requirements 
in what kind of scenario you want this to work 
so lets say huh there is a heavy huh fax load
so huh continuously fax are coming and going 
and huh we we will work with the assumption- that 
huh the telephone line to which this is connecting is continuously kept busy
so ha- huh lets say half the time its huh (( ))
incoming and outgoing faxes are balanced that means al- almost equal number of fax is coming and equal number a- as to be sent out 
and huh each fax message is a single page
we we are not we don't want to- worry about complication of sending multiple pages
so just imagine that each 
we are only dealing with A four size pages huh
so- both printer and scanner have to work with that
and huh after satisfying the fax load huh 
whatever is the spare capacity of scanner and printer is to be utilized for photocopying
and we want to get a feel of 
we want to know how how many maximum 
at what rate we can do photocopying process 
so we want to find eventually the throughput for each type of operation 
that means huh what is the rate of fax message- that would be sustained and huh
what is the rate of photocopying that could be achieved 
so this is a this is a block diagram of the whole system we have in mind huh
this sim- we want to make it low cost huh setup 
so there is processor memory bus and a single I O bus 
so the- there is a bus adapter sitting here 
huh a scanner printer modem which hooks to a telephone line [Refer Slide Time: 21:35]

(fig., 21:18)
and maybe we might need a disk drive
so that s a question mark although it doesn t come directly from the specification
so huh we we want to huh build it as a sort of dedicated system 
huh and huh you know once it is designed and all assembled 
we want it to be just doing this huh work as a huh fax machine plus a photocopy 
[throat_clearing]
so- whatever program is there could be possibly put in huh non-volatile memory and you can 
if disk is not required we wont put that
{faintly}huh it will save maybe little bit of money
[throat_clearing]
so of course huh these huh am ignoring the details that these devices huh will eventually ((perhaps)) connect to different type of port or a busfor example modem may go to huh serial port huh these may connect to U S B port or
[background noise]{sounds like someone coughing}
printer may go to parallel port and so on
so huh we we will ignore that last point detail and assume that there is a single bus huh from which eventually all these interface will be hooked up
so so now lets look at the building blocks which are available 
so huh something will be sort of fixed and somewhere there will be choice and we will figure out what what choices we make 
so lets say printer and scanner are available to us 
we will take a low or medium resolution print- huh machines three hundred D P I 
we don't- need a six hundred or twelve hundred D P I huh
the page size fixed page size as to be handled and huh this (( )) says its eleven inch by eight inch A four size 
[background noise]{sounds like someone coughing}
and- it can both these can work in two modes 
one mode is when- when you huh simply work huh with black and white ok so huh 
[throat_clearing]
the- are no gray levels huh 
each each pixel is either black or white ok
and- in second mode you have two fifty-six gray level	[Refer Slide Time: 23:45]
which you can represent with a byte
so for fax purpose we will- use mode one and for- photocopy purpose we will use mode two 
huh lets say this scanner works at the speed of four pages per minute and printer works at the speed of eight pages per minute ok

(fig., 24:08)
huh continuing with the building blocks 
huh we have a modem available huh 
we are not taking top of the line fifty-six K B P S modem 
we lets take huh thirty-three kilo bits per second modem
and huh it also lets say there is a ten percent overhead 
that is whatever data is to be sent 
a few bits as part of the protocol as to be added by the modem and lets say its ten percent extra data which gets added 
huh there is a disk available huh which as a latency of ten milliseconds and transfer time of five megabytes per second

(fig., 24:50)
we we want to use again a low cost processor huh
which is available in two options hundred megahertz or two hundred megahertz 
so its not a gigahertz type of processor and its not pipeline 
it as a C P I of four huh 					[Refer Slide Time: 25:02]
[noise]
it doesn t have cache so directly accesses memory 
huh the the program which will be running on this to carry out this operation 
huh the fax requires certain encoding and huh decoding ok
so- you you have the scan- image of the page 
huh that that needs to be huh encoded in certain form before you send 
a- and this encoding actually compresses the size of the data
similarly the fax which is received as to be decoded and put back into a pixel matrix so {faintly}huh i will maybe i think the number is mentioned later 
so we will assume that there is a huh compression or decompression of factor of ten is to one which is happening 
[cough][background noise]{sounds like someone coughing}
and huh this process of encoding or decoding involves hundred instructions per byte of raw data
so so lets say you have scanned the image 
huh if the image is huh ten kilo bytes 
you will require hundred instructions to be executed performance byte
or if it is one megabyte accordingly 
so so the algorithm of encoding or decoding either way as two ((incur cum)) computational load
which is spent out here 
also huh each I O operation 
i am assuming here a flat rate of huh hundred k instructions performance I O operation 
so for example huh sending a fax would involve scanning and huh sending to the modem
so so each as to be setup huh in a D M A manner and i am assuming that each will require huh each will consume hundred ((k)) instructions ((per)) processor
huh and on the average the program huh would have 
lets say thirty percent of the instructions which are of load store ((kind))
which will make additional access to the memory 

(fig., 27:06)
[background noise]{sounds like someone coughing}
huh the- memory is available in two sizes 
sixty-four megabytes or one twenty-eight megabytes 	[Refer Slide Time: 27:18]
and two speeds huh forty million words per second or sixty million words per second
and there is an appropriate synchronous bus which will connect to the processor 
huh the I O bus- huh is to be used 
there is no choice here
its ten megabytes per second 
so now with all this huh we can proceed and huh try to put things together 
see what kind of rates we get and what choices we ((made)) wh- choice could be made for processor and memory 

(fig., 27:44)
so- lets first huh try to understand huh
what information each page carries huh
we have seen that resolution of the page is three hundred D P I 
and with page size of eleven inches by eight inches 
[background noise]{sounds like someone coughing}
we get total number of pixels as eleven into eight into three hundred into three hundred which is seven point nine two million 
so those many pixels are there 
now depending upon the mode whether it is black and white mode or a gray scale mode 
you will require certain number of bits per pixel 
so mode one is one bit per pixel and mode two is eight bits per pixel 
so you you get huh for seven point nine two mega pixels 
you get seven point nine two megabytes or point nine nine megabytes ok
huh so this is these are bits we have converted to bytes
[throat_clearing]
now huh the compression in fax and coding was ten is to one so huh
when you have to send or receive a page of these many megabytes 
actually what goes in the fax line huh will- be one tenth of that
so point zero nine nine megabytes ok
ok now huh memory and disk size
lets have a look into that
[background noise]{sounds like someone coughing}
so lets assume that the code huh or the software will take something less than twenty megabytes
so huh sixty-four megabytes of memory will be adequate	[Refer Slide Time: 29:29]
in m- in the sense that huh we don't have to put one twenty-eight 

(fig., 29:45)
sixty-four megabytes will accommodate easily the program 
and it can also buffer several pages which a- been scanned or which are waiting for printing ok
each each of these requires something like eight megabytes 
so sixty-four megabytes gives us reasonable cushion for accommodating a couple of scan pages
[throat_clearing][background noise]{sounds like someone coughing}
and huh since everything can be accommodated in memory 
we can huh eliminate the use of disk 
we can have the memory itself as a huh nonvolatile memory 
o- or we could have huh volatile memory huh augmented with flash memory
because flash memory is slower
so you will not like to execute program directly from flash huh 
instead of disk you can put a huh flash memory 

(fig., 30:29)
[throat_clearing]
ok now huh lets go to the huh fax process 
so modem speed is thirty-three kilo bytes per second or in terms of bytes huh you can divide by eight and get four point one two five kilo bytes per second 
[throat_clearing]						[Refer Slide Time: 30:44]
and huh the compressed page or encoded page is point zero nine nine megabytes 
huh which is same as ninety-nine kilo bytes 
and with ten percent of overhead we can find out transmission or re- ((recession)) time
right so ninety-nine multiplied by one point one because its overhead 
divided by the kilo bytes per second rate 
huh we get twenty-six point four seconds
so each page huh takes this much time to re- or to be received 
plus huh now since huh this happens over telephone line huh th- there is huh time taken to establish a call ok
either there is an incoming call and modem will respond to that establish connection or huh when it is to be sent number would be dialed and connection would be established
so lets say twenty seconds either way get used up for this purpose fo- for establishing a call 
huh so the two put together twenty-six point four and twenty is total time which is actually taken 
fo- for which the modem and line are busy (( )) forty-six point four seconds 
and therefore fax rate 
the rate at which you can be huh handling faxes huh incoming and outgoing all together
is reciprocal of this which is point zero two two per second or 
you can have a more convenient figure in- per minute huh one point three two per minute 
so- so basically huh this is one of the responses th- th- setup 
huh here are the things actually limited more by huh th- the modem speed ok
{faintly}t- the rate at which you can send fax 
and its not so much influenced by the other components wh- which are in the picture 
so this this is the rate which modem can sustain and rest of the system has to actually work with this 

(fig., 32:44)
now lets turn out attention to scanner and printer
so scanning rate is four P P M or- in terms of pages per second huh 
it is point zero six seven so many huh pages per second 
huh the outgoing fax rate huh point zero two two 
we divide into point zero one one incoming and point zero one one outgoing 
so now th- the scanner as to match the outgoing faxes 
and therefore huh if so many faxes are going out
so many pages have to be scanned alright 			[Refer Slide Time: 33:25]
so th- the capacity of scanner available for photocopy- is the difference of these two ok
it as total capacity of point zero six seven 
if you want to continuously scan pages 
so many pages per second could be scanned 
and huh out of that so many will be huh faxed out 
rest could be used for photocopying purpose 
huh on the other hand printing rate is eight P P M or point one three three P P S page per second 
incoming fax rate is point zero one one 
the capacity available for photocopying is point one two two pages per second
now huh so th- these are spare capacity and- huh photocopy operations we can do
since huh if you recall i mentioned that 
for each page you scan you will print on the average three copies
so printer as a slightly higher load and huh this figure is certainly less than three times that
so so basically huh printer would dictate the photocopying rate and we say that we will be able to huh churn out so many photocopy pages per second and that means
one third of that we need to huh scan on the average
so point zero four zero seven pages per second are scanned and point one two two pages per second are printed huh in the photocopy process ok

(fig., 34:50)
now huh we go to the I O bus requirement 
so so we have looked at each peripheral 
[background noise]{sounds like someone coughing}
we have looked at the modem scanner and huh printer
we have huh gotten away with the disk ok
so now I O bus as to basically sustain the throughput which is being generated by these huh so wh- when you are receiving a fax 
you have a data coming from modem and it goes to the memory ok
it passes through I O bus 
and huh similarly for outgoing fax it comes from memory and goes to the modem
so it passes through the I O bus
similarly huh from memory to printer or scanner to memory will happen at huh this rate
for for huh each page per page 
((its)) point zero nine nine megabytes per page 
this is the modem traffic 					[Refer Slide Time: 35:44]
point nine nine megabytes per page is the printer scanner traffic 
so so the data rate huh which huh the I O bus sees for the fax operation is 
point zero two two faxes per second multiply by huh
so that means so many pages per second and- these many megabytes per page
so the the product gives you point zero two four megabytes per second
so that s the demand placed on the I O bus by the faxing operation 
for photocopy operation huh there is a traffic between memory 
[noise]
huh to printer or scanner to memory which is seven point nine two megabytes per page ok and huh the rate at which we are doing photocopy 
we are scanning point zero four zero seven pages per second huh that sends out this much of data and point one two two pages per second we are printing 
so that again demands that much data 
so- sum of this is one point two eight nine megabytes per second ok
so so that is the total demand which is placed by photocopying operation 
when it is running to its peak on the I O bus
so sum of these two huh point zero two four and one point two eight nine megabytes per second is- one point three one three megabytes per second is the total I O demand
and the bus which as been provided to us is ten megabytes per second
so we are very comfortable with that and we can -cept that solution 

(fig., 37:33)
now lets see how the processor is doing 
[background noise]{sounds like someone coughing}
the instruction which it needs to execute per fax message huh is- huh
i mentioned that the- are hundred instructions per byte ok
so point nine nine megabytes huh is is the page huh data 
so this product gives you the number of instructions which will be required for either encoding or decoding a page ok and huh each fax in or fax out involves two operation 
so ei- either huh reading from the modem or writing to the modem and on the other hand huh either scanning or printing
so so each will require hundred k instructions
so so total of instructions which have to be executed is huh
these many for either encoding or decoding 			[Refer Slide Time: 38:29]
these many for huh either sending or receiving 
so this turns out to be 
actually this is a very small component as you can see 
so bu- bulk of the ((computation power)) is going in encoding or decoding 
the I O part is much smaller 
so this is a ninety-nine point two million instructions huh per fax incoming or outgoing
ok the the instructions executed per photocopy huh now 
{faintly}huh i ((have made a)) mistake here 
it should have been four 
so huh on the average you are reading you are you are doing one scan and you are doing three prints
so it should have been four here huh
so each requires hundred k instructions 
so total point four M ok
[background noise]{sounds like someone coughing}
the total instructions throughput required is this much plus point four 
[cough]
sorry this into huh the fax rate and this into the scan rate 
huh so so together you will get two point one nine 
[background noise]{sounds like someone coughing}
th- this (( )) is a small so small factor huh it makes minor difference here
so lets not worry about that 
the the total huh instruction rate required on on the part of processor is two point one nine million instruction per second 
and huh we talked of hundred megahertz or two hundred megahertz C P U 
huh with four C P I 
so so basically hundred megahertz C P U will require 
we- we will actually deliver twenty-five MIPS which is huh very comfortable to handle that ok
so we don't need to go for two hundred megahertz processor 
we can work with hundred megahertz process- 
infact if there was a fifty megahertz version available huh (( )) 
we would have picked that 
so so (( )) case out of the choice given this is quite adequate      [Refer Slide Time: 40:33]

(fig., 40:33)
[throat_clearing]
huh the memory bandwidth huh now
we- we are not ((requiring)) the processor to be full capacity but suppose huh 
after have seen that processor is under utilize maybe we might put in some other tra- that suppose processor was to run at full capacity
it was not to stroll for lack of no work or lack of work
[background noise]{sounds like someone coughing}
huh twenty-five MIPS would require twenty-five million instruction words to be fetched per second ok
and huh load store we noticed earlier was thirty percent of instructions are of load store type \
so additional demand would be point three into two five 
[background noise]{sounds like someone coughing}
((or)) seven point five million words per second 
[noise]
huh so this this is the huh traffic which processor will generate for the memory ok
twenty-five million words per second and seven point five words per second 
instruction traffic and data traffic load store traffic 
the I O traffic is also showing up on the memory bus 
so which is huh very small one point three one three megabytes per second 
so all all put together the mem- huh memory bandwidth needs to be capable of handling this
twenty-five plus seven point five plus one point three one ((four/three)) divided by four to get million words
this was million bytes ((right))
so that s only thirty-two point nine million words per second 
and huh so we can pick up the memory which as huh out of the forty and sixty t- possibilities which were shown 
we can pick up the one which as forty million per second capability
so that is adequate for our purpose 
[background noise]{sounds like someone coughing}
so huh so so basically huh a- as a end result 
you can see that we have huh
we can get certain huh throughput rate for fax and huh photocopy operation 
and we are able to sustain that with huh the the simpler of the processors and slower of the memories
so that is all as for as this example is concerned 
if you have any questions huh i hope these huh
[throat_clearing]
these calculations are fairly ((s-)) for you 
you wont need to proceed huh in certain manner and huh th- this 
there is no fixed huh rule or procedure to solve such problems
you know you each problem may be of its own kind
[background noise]{sounds like someone coughing}
and huh we- we all along we should understand what are the assumptions we are making in all the ((scenarios))
we assuming that huh everything is in a steady state ok	[Refer Slide Time: 43:38]
so so the- are no huh no accumulations of data or huh nothing is huh getting st- for huh data
so huh th- are no huh congestions or no starving huh 
all traffic all data traffic or instruction traffic is flowing smoothly 
and there is a sturdy state 
so if you have huh multiple buses converging to one bus huh 
you simply add the traffic coming out of huh multiple de- devices 
which come on the same bus 
[throat_clearing]
so huh we- we- we have not analyze- here the delays that that is huh 
once a fax comes how long it takes to print ok
so that was the latency issue or huh from the time huh 
[background noise]{sounds like someone coughing}
you press a scan button ok
how long it takes to get three copies printed out 
so so that we have not analyzed 
that is somewhat more complicated and huh often you need to use huh sim-
you you may not be able to simply add the delays because huh as- as you are huh
these operations are all ((interleaved)) a- as you- huh receiving and sending faxes 
photocopying is taking place ok
so huh these are interleaved and huh th- the- may be sometime queuing delays ok
so which- which we have not worried about in huh bandwidth calculation
but when you look at the latency you you would need to 
huh for example a page has been scanned and it needs to print it 
so how much is the delay between the two 
huh wh- when- when will the processor execute those instructions required 
when will the bus be free 
when will the printer be free ((ok))
so huh that analysis may require tools like
[background noise]{sounds like someone coughing}
queuing theory for example 
or sometimes huh if even if that is adequate you would need to actually stimulate the whole huh scenario and huh then- then study h- how much latency is involved ok
[noise]
that s all for the moment 
any questions
[inaudible background noise]
yeah that is the rate at which huh disk is able to transfer data
[background noise]{sounds like someone coughing}
once you reach th appropriate point 
[inaudible background noise]
{referring fig., 10:44}
huh yeah yeah so so th- that s where things will get buffered in the controller
ok so so lets say huh what you are saying is very realistic and it- its going to happen 
it may not be simultaneous but lets say huh one- one disk huh as huh started transferring 
maybe its halfway through
another disk is ready ok
so now huh what will happen 					[Refer Slide Time: 46:44]
it it may be
there will be some buffering at the disk end ok
there will be buffering at the huh at the controller end right 
so huh also- we would huh need to understand huh 
how actually the two ((streams)) of data huh get put on the on the bus 
do- do you allow them to be interleaved or not 
so lets say they- they are not interleaved ok
[background noise]{sounds like someone coughing}
[smack]
so huh the data which is taking huh twelve point eight milliseconds to come out of the 
as it is coming out of the disk huh the disk is active for twelve point eight milliseconds ok
but huh the rate at which it goes on the SCSI bus is twenty megabytes per second 
so so let us say huh there is no interleaving of huh the blocks from two different sources
and huh the- therefore the SCSI bus will be occupied huh with this much of data

(fig., 47:45)
for huh how much time 
(( )) sixty-four kilo bits of data over huh lets see 
i- it will be one fourth of that actually 
so this since this is a five megabytes per second 
so some three point two milliseconds 
so- basically huh what will happen is that huh 
if you are not interleaved- leaving huh you 
it will better to buffer this sixty-four kilo bytes somewhere ok
either at the disk controller end or at the SCSI controller end and huh the- then 
put it for three point two milliseconds on the bus right 
let- let me try to huh 
[noise]
{faintly}show you (( ))
[writing on a paper]
so one disk lets say spent- 
[background noise]{sounds like someone coughing}
twelve point eight milliseconds and huh
lets imagine this data is going to some buffer 		[Refer Slide Time: 49:00]
an- and then over a period of three point two millisecond 
it it huh it is occupying a SCSI bus
and huh over a shorter period huh 
how much was the memory bus
i think hundred megabytes 
yeah so huh one fifth of that 
so a fraction of milliseconds huh it will be huh present on the processor memory bus 
now in-between here although i am i am showing it contiguously 
but the- could be gap here 
so in actual practice we should be open to have situation like this
twelve point eight then some gap 
somewhere down the line three point two milliseconds it spends on th- SCSI bus
somewhere later with i don't know how much break 
it will spend three point two by huh five 
so many milliseconds on the P M bus 
so now each each transaction you can imagine as this three parts 
and you have another one such thing coming from huh a different disk
[writing on a paper]
so it may be its twelve point eight was happening here ok 
and its three point two may get ((just placed)) after that 
so so you have huh basically these streams coming from different disk 
they get buffered and one after another they will be placed on the SCSI bus
similarly the the adapter which is sitting on the main huh 
when it is put on the processor memory bus again they will be sequentialized
[background noise]}{sounds like someone coughing}
put one after another
so lets say it gives a thin ((bus-)) on huh P M bus
so suitable bus- at various end will take care of this 
does it answer your question 
any other question 
ok thank you 
[Refer Slide Time: 51:00]




Educational Technology Services Centre
IIT DELHI

Computer Architecture

Lecture #38
Concluding Remarks

this is the last lecture on computer architecture and we conclude huh series of lectures 
so today we will go back to what we had set out to do in this course and see where do we stand 
so lets- try to review what have we learnt in this course 
huh as it was spelt out in the first lecture huh

(fig., 01:44)
[noise]
we had listed a few course objectives huh
the objective was try to learn how computer work 
what are the basic principles 
how we analyze the- performance 
how we design and build the computers 
and huh how do we huh improve performance
what are the issues which have to be kept in mind
in the modern processors issues like caches or memory hierarchy pipelining and so on
[throat_clearing]{referring fig., 02:32}
so in summary huh we have covered various topics
we talked about how performance is defined 
we took a specific instruction set and huh discussed some code instruction 
[background noise]{sounds like someone coughing}
then we went out to build an A L U huh which will carry out those instructions and then we constructed a complete processor to huh execute these instructions
we saw the concept of pipelining to improve the performance 
huh we saw that memory is hierarchical not just a flat array of huh words
and finally to complete the picture we talked about input output 
[Refer Slide Time: 02:32]

(fig., 02:32)

(fig., 02:44)

huh we began by looking at how the technology as been moving over several decades and huh this was the table which we had looked at on the first lecture 
a- a- added one more entry here the last one 
so you- would notice that there as been an exponential growth in performance 
[background noise]{sounds like someone coughing}
what you can get out of a single monetary unit 
so performance per rupee or per dollar huh 
the the figures here are of course relative values
so so taking what you get from a vacuum tube system -as one 
other figure as been projected
so there as been an exponential growth 
this as also divided huh the computers historical into generations 
{referring fig., 03:29}
so our huh focus huh while talking of architecture as been on the fo- on the interface of hardware and software 
where the hardware and software meet which is essentially huh machine instructions 
as seen from the program end 				[Refer Slide Time: 03:40]
and huh the building blocks like huh A L Us adders registers multiplexers buses and so on
as seen from the hardware end

(fig., 03:29)

(fig., 03:55)
the small set of instruction where we really focused is shown here
these are for ((mis))processor and huh it s a processor of what is called huh risk variety reduce ((instructs)) th- computer 
huh on the whole the- are huh very few instructions and simple instruction 
each instruction- tr- tries to do one particular task 
[background noise]{sounds like someone coughing}
and not mix many things 
the advantage of such ((or)) choice huh is in huh building a fast processor huh
ease of doing pipelining and also ease of generating code
so huh th- the- are three formats only in this R I and J and the instruction we talked about huh nicely fall into these three categories
[throat_clearing]
the performance was an important issue which we discussed in the beginning
and huh lot of discussion was huh pertaining to how to improve performance 
of course the- are other huh aspects when you design a system 
huh we have not gone into all of those 			[Refer Slide Time: 05:00]

(fig., 06:18)
particularly huh power consumption is very important and huh
unless that is checked the the power being produced or power being dissipated in the processor chip per lets say square centimeters increasing tremendously 
and you need elaborate arrangement for (( )) 
{faintly}otherwise system cannot ((work))
so from- from this point of view we looked at two possible directions 
execution time and throughput
huh throughput is a measure which is at a gross level 
when you look at collectively huh several tasks being executed by a processor
but for an individual user huh execution time is important
which can be related to three basic huh parameters 
the instructions executed in a program the instruction count 
cycles per instruction on an average and the clock period or clock rate
the factor which influence these are- varied 
the compiler w- generates a code 
[background noise]{sounds like someone coughing}
instruction set architecture 
what instruction you have chosen 
micro-architecture that means how you implement those instruction and ((under-)) technology 
[throat_clearing][tapping]
so huh wh- when we try to improve performance 
its important that we look at what is the common case
and huh try to make that fast ok
if if you cannot make everything fast at least make the common case fast
{referring fig., 06:44}
so now having studied a few instructions from programming point of view and huh
developing some understanding of what performance means
we had set out to design huh the A L U first and then the complete processor
so A L U is is a heart of the processor which carries out all arithmetic and logical operations 
{referring fig., 07:00}						[Refer Slide Time: 07:08]
and huh its design was done by looking at one cell which can be repeated for every bit
and the construction was huh done by simply looking at various functions it as to perform

(fig., 06:44)

(fig., 07:00)
huh put hardware in place for each of these
and then have a multiplexer to select one of the outcomes
so effectively it- it does all the operations simultaneously 
but you look at the result of only one of these 		[Refer Slide Time: 07:20]

(fig., 07:25)
and this is repeated huh thirty-two times
[background noise]{sounds like someone coughing}
the- ((is)) additional logic shown here which huh does the equality comparison 
so- so this is the A L U we had worked with huh while doing so we had ignored somewhat complex operations like multiplier and divider huh which 
for which we primarily discussed iterative algorithm which take huh thirty-two steps and in each step (( )) produce one result of the- one bit of the result 

(fig., 08:02)
so this is a multiplier design this is one of the divider design- we had and huh you can notice similarity of the structure here huh we had lot of variations 
for example division was done in a restoring manner non-restoring manner
so so this is a non-restor- non-restoring division 
[throat_clearing] [noise]
huh its usually possible to merge the multiplier hardware and divider hardware
if you if you look at the commonality 
there is lot in common huh the key differen- is that huh the big register at the bottom shifts right in one case and left in other case
so if you make it possible to shift it both ways and- organize the control- appropriately 
huh the then make this huh this unit capable of doing addition or subtraction both

					(fig., 08:55)		[Refer Slide Time: 08:50]
[noise] 
you can have huh a single unit which does multiplication or division depending upon how you ((exercise)) the control 

(fig., 09:00)
so this huh design huh we saw how we could speed it up
huh you- you could for a speeding up addition which you dis- earlier 
you could use huh carry look ahead technique ok
huh w- which takes from huh time of ((the order of)) n to O log n and 
that s a tremendous improvement 
to speed up multiplication we looked at huh 
[background noise]{sounds like someone coughing}
possibility using huh carry save adders in an array form
so instead of doing iteratively huh you you expand it out into huh adders
which add all the (( ))
and use carry save additions
so that improves the delay by a significant factor

(fig., 09:50)
then huh noticing the limitation of huh integer operation 
we talked about floating point operation 			[Refer Slide Time: 10:02]	
huh the- are varieties of ways in which you can represent numbers 
and our focus was on a standard method which is commonly followed now
that s I triple E seven five four standard 
huh there are issues of huh how you detect overflow and underflow
so underflow is an additional feature which occurs in floating point 
we saw how these operations are performed 
huh we also look- at the need for accurate arithmetic with the help of g- round and sticky bits 
[throat_clearing]
the notation provides for some special number such as huh ((de-normalize)) numbers infinity and something which is ((indeterminate)) and so on 
and we had huh a brief look at 
[throat_clearing]
how floating point operations are done 
in (( )) what are the instructions which support this type of arithmetic 
[throat_clearing][background noise]{sounds like someone coughing}
then huh having understood how basic operations are done 
we looked at the entire process of instruction executions starting from fetch ((or)) fetching of instructions 
picking up the operands
doing the operation and then putting the result back in place

(fig., 11:11)
[noise]
so huh the first design we looked at was a single cycle design 
where the entire instruction execution is finished in a single cycle 
[throat_clearing]
and huh the control here is a simple combinational circuit 
which looks at opcode a- and associated function bits and tries to generate control of various components within the ((data path))
so huh what you are seeing in huh black and white is the huh data path 
and what you are seeing in orange is the control
so so th- there is a when you design you have a clear distinction made between the data and the control
[background noise]{sounds like someone coughing}
wh- which ((is this the t-)) substantially			[Refer Slide Time: 11:45]
so having designed this we notice the limitations of such a design 
huh various points of view and went on to a design which is multi-cycle design

(fig., 12:00)
so- so that s improve- the performance huh 
[background noise]{sounds like someone coughing}
helps in huh sharing the resources and also huh 
makes it possible to implement much larger variety of instructions 
[background noise]{sounds like someone coughing}
so in this the control although not shown here 
i- is a sequential circuit ok 
which steps through various states and huh takes the instruction through various cycles

(fig., 12:25)
so th- this flow chart actually indicates how various instructions are done in- multiple cycles
each box represents activity which is done in one clock cycle 
so so now here its very crucial to see how instruction execution is divided into cycle
because that determines that influences both the C P I ok
so so C P I is very obvious for example if- if there is an instruction 
which is following lets say this path ok
it takes five cycles ok
you have five you go through five boxes 			[Refer Slide Time: 13:02]
huh but what you have put in these boxes determines how wide the clock as to be
[background noise]{sounds like someone coughing}
so clock period as to accommodate huh the slowest of the activity 
which you have put in this box
so- you have divide these things
distribute these things in a uniform manner as far as possible

(fig., 13:25)
[throat_clearing]
now going further huh we looked at huh pipelining of the design 
which where each individual instruction take multiple cycle but huh 
instructions get issued huh hopefully every cycle 
[throat_clearing]
and huh this design was obtained in a very simple manner by looking at the single cycle design 
and introducing registers which separate pipeline stages 
[smack]
so- in terms of concept it is very simple but huh
we immediately notice some hazards huh
structure hazards data hazards and control hazards
so structure hazards are relatively simpler to handle 
you try to provide enough resources 
so that there are no conflicts occur
but data and control hazards have quite inherent huh in- 
partly in the hardware which you design and pa- partly in the way huh
program logic is there
[throat_clearing]
so huh one needs to provide for huh extra control to take care of these things 
so huh for example for huh flushing out instructions huh
wh- which are fetched when you are trying to take a bl- decision ok 
so the wrong instruction may fetched and you have to flush them later on
huh for handling data hazards huh you try to reduce the delay by forwarding data huh 
so that the instruction which is dependant and looking for some in- some data 
gets it as early as possible 
[Refer Slide Time: 14:55]
so the data forwarding paths are not shown here
b- but they are huh a number of possible paths 
so that huh data flow from instruction to instruction can be done as fast as possible

(fig., 15:08)
huh after having huh spent several lectures on processor design 
we went on to discussion on memory which was huh seen to be as a hierarchical structure as ((in)) a flat structure 
and the basic idea there was to huh try to get a- advantage of huh fast (( ))
and try to get the bulk of huh inexpensive and- dense ((de-)) at- at one end
and also huh taking it further get the advantage of size of what you can get on a disk at a- huh in in inexpens- manner 
so huh a typical cache organization is- shown here 
huh this one in particular shows a four way set associative cache ((ok))
we- we looked at three organizations 
direct map cache 
[background noise]{sounds like someone coughing}
set associative cache
and fully associative cache
so most commonly what is used in p- is set associative cache 
with varying degree of huh associative- 
this one is four way associative huh 
in the sense that a given block of data ((or)) instructions can be placed in 
one of the four alternative slots
and huh this number four gives you the kind of flexibility you have in placing a block
more more is larger the number larger the flexibility
(( )) a larger number also implies huh more hardware cost ok
because you need to huh look at more places in an associative manner or in
by doing parallel comparison 
huh but this huh parallel comparison and associative huh search 
also as an effect on the clock periods
so typically as we increase the degree of associative- 
the clock period becomes larger and larger which means 
{faintly}re is an adv- impact on performance 		[Refer Slide Time: 17:02]
huh the positive impact on performance is that 
by giving more flexibility you reduce the number of misses
you are unlikely to huh throw away huh data
which you would have brought earlier but ((are)) throwing away because of conflict
because that space is required by something else
huh the o- other level which is commonly used
other level of hierarchy is virtual memory
[throat_clearing]
where huh main memory form- one level and the the disk stores huh the next level

(fig., 17:44)
[throat_clearing]
so mapping here is done through a page table 
which also is in the memory huh
either in physical or in virtual huh
to to speed up this process of accessing huh virtual memory
we have an associative buffer called T L B or translation lookaside buffer

(fig., 18:08)
[throat_clearing]
ok finally we moved on to huh I O 
so huh putting I O together you form the complete system 	[Refer Slide Time: 18:18]
and- this is a diagram which are projected in the first huh lecture 
[smack]
so- you you have various parts here
there is a processor 
there is a memory
there is a bus and 
there is a I O subsystem 
so physically you can relate it to huh what you see if you open huh a computer 
so this is a processor ok with the (( )) fan 
you don't actually see the processor 
[background noise]{sounds like someone coughing}
its covered with (( )) and a fan on top of it 
huh these are memory modules 
these are huh P C I slot which is a which is a bus we had seen and huh
some of these slots have been plugged in huh
this probably the network card and i think this takes care of huh 
i am not sure maybe keyboard and mouse or- ok
so so this huh this one actually not on P C I 
this is a huh graphics display card

(fig., 20:11)
[throat_clearing]
huh so some of these things could of- often be huh on the board ok
so this is not a very huh very recent board 
{faintly}it- it will be some two three years 
huh so what you are seeing here as a separate card
could also be part of motherboard
huh but as we have seen that 
the the graphics display huh is the most demanding huh in terms of performance 
and- it s a connected as close as possible to the the processor memory interconnection
whereas huh other things might sit on huh backplane bus or ((an)) I O bus
[throat_clearing]
so huh these are I O devices 
this is disk drive 						[Refer Slide Time: 20:11]

(fig., 20:18)
[throat_clearing]
and this is a complete motherboard 

(fig., 20:23)
ok now why have we studied all this[noise]
huh so these are the points of motivation which we had looked at
huh so ideal would be that huh 
you get an opportunity to design huh a new architecture 
new instruction set
build the processor and so on
but that that s practically speaking that would be a very rare opportunity
[background noise]{sounds like someone coughing}
huh not many new processor design take place
[throat_clearing]
more common more likely huh situation is that 
you might design or build a new version of an ((old)) architecture ok
or huh from software point of view this knowledge could be useful huh
to write better software ok
huh e- either ap- application level 				[Refer Slide Time: 21:11]
if you have an understanding of huh
what architectural huh com- are you- you can actually write better software
or if- if you are in compiler operating system area again
then of course a deep understanding of architecture 
{faintly}is very very importanteven if you are not huh doing a technical development of hardware software huh
understanding of architecture and performance issues is important 
when you are for example purchasing a computer
so now al- al- all these huh points were with desktop computing 
or general purpose computing in mind 
but if you look at huh embedded computers huh
then then you  might be doing much more of this
th- th- are much more many more opportunities
and huh it could be basically designing 
huh huh you might end up in designing a new processor 
or actually huh adapt huh building a complete solution 
out of huh a processor used as a component 
[throat_clearing]

(fig., 22:25)
[smack]
so it- its interesting at this stage to look at the huh statistics here 
about huh how many processors of various kinds typically gets sold every year
so there are three classes of systems are put here in this table 
servers huh desktops and embedded processors
so- the figures are in millions and there are given year wise
over last huh ov- over five cons- years
so you would huh the- are several observations you can make
[throat_clearing]
one observation is that huh the the volume is is varying by orders
as you go from server to desktop to embedded ok
the the numbers here are many more
the ((second)) observation is that 
while servers a- are growing very very slowly 		[Refer Slide Time: 23:11]
desktop appear to be almost saturating huh but embedded are huh 
if you look at this only as an (( ))
but if you broadly look at the curve 
th- there is a huh very fast growth alright 
[throat_clearing]
so- wh- when you talk of huh desktops a- and 
you talk of huh availability of these machines per person 
huh you talk of a very small numbers ok
for for example huh i- if you if you look at huh our own huh com- here
huh the the number would be either one or 
maybe little less than one 
or little more than one huh per person
but embedded could be much much larger you you have as a 
as i mentioned earlier you could have embedded processors in various huh
other devices various appliances
your mobile phone and so on
so huh num- number you can con- of per person could be huh
easily go to double digits

(fig., 24:25)
so what are these embedded computers 
basically here you have 
these are processors
wh- which- which are not seen as a general purpose computing device
so they do some huh intelligent function 
but but you you don't huh 
you you don't have user sitting there and programming the-
so they typically do fix- function or a fix- set of functions
they operate in real time 
interact with ((the)) environment
and huh the the design process is more involved
we have not discussed that huh
some some of the portions which you might do as (( )) later on
[Refer Slide Time: 24:55]
would address that issue
huh these typically require one to address the hardware and software design issues
simultaneously as a combined hardware software co-design ((problem))
and huh a design as to be de- 
these are very size rate power and cost sensitive
and therefore they have to be highly customized
so huh i i see tremendous opportunities of huh 
do- doing some interesting work in this area
huh where huh the knowledge of this course 
{faintly}would definitely be (( ))

(fig., 25:32)
huh th- the question is why huh different processors 
why do you need different processors huh fo- for different types of things ok
we talked of huh servers desktop embedded huh
what is the difference between processors used in desktops laptops mobile phones and appliances like washing machines etcetera 
so so the difference comes in various aspects in performance or speed
difference in power consumption cost			[Refer Slide Time: 26:11]
and whether processor is for a general purpose huh operation or it as to do a special purpose 

(fig., 26:11)
huh this is a picture which (( )) from earlier ((show)) the Pentium processor
and a a micro-controller huh this is an eight bit processor huh
its actually processor plus memory ok
huh th- the program could be stored in the memory which is on the- and it is a 
you you would typically store a (( )) program and- huh it could ge-
repeatedly just execute that

(fig., 26:44)
[throat_clearing]
huh so huh i i looked at the third edition of the ((textbook)) we have been following
and- huh i- i- it talks of huh lots of huh real life example where huh computers are used 
huh in most cases these are embedded computers huh
sometimes they are huh ((augmented)) by general purpose computing also
[throat_clearing]
so i- i ((we)) will look some of these briefly and huh
i will encourage you to see these interesting huh possibilities
so huh i will go through some of these and- towards end add a few huh
from my own personal experience ok
so le- lets look at some of these				[Refer Slide Time: 27:44]
huh so this talks of an attempt to huh take computers to (( ))

(fig., 27:55)
so so basically huh a low cost P C huh 
which was huh which as been designed by huh group of people for huh villages
the idea here was to make it huh
[throat_clearing]
((portable)) in low cost 
so that affordability for people with huh small means could be there 
it as no moving parts 
and it s a- its ((re-)) huh no no delicate things like C R T (( )) hard disk
huh it uses flash memory instead of hard disk
L C D screen a- and a and an inexpensive Intel four eight six processor
wh- which is huh couple of generations behind
but huh since those are available in cheap huh in
with very low cost that was used
powered by a car battery which can be charged by huh a bicycle
wireless internet cards which connect to a solar powered hilltop relay station
[throat_clearing]
and huh it connects to huh the telephone system and also to- to the internet
the O S is Linux and huh it- its being localized in the sense that huh 
in the local language its communication is in local language so that 
people who don't know English can also use it 
another attempt ((to)) do something similar huh is from huh Bangalore
you must have heard of Simputer
how many have heard of Simputer ok

(fig., 29:44)
so so y- you would huh recognize ((this ))
it s a- again an attempt to build a small huh low cost computer
and huh this was actually initiated by a team in I I S C Bangalore
a- a startup which came out of I I S C Bangalore
the processor used here is strong arm huh
which as low power consumption 
huh again it is flash based 
there is no hard disk no ((moving part))
this plays L C D 						[Refer Slide Time: 30:02]
it as a modem Linux O S and- input output 
there is no keyboard ok
so you you can 
this is a touch pad 
so you can either work through icons
or it also as huh text to speech capability 
so that huh i- it can 
there is no speech input but speech output is there
so e- even if you cant read text for an illiterate person that could be huh 
{faintly}a reasonable medium of communication so huh 
[throat_clearing]
track- the information coming from computer can be huh spoken out
and- the input could be by touching the icons
y- you can huh look at the pictures 
{faintly}and ((convey)) the input 
{faintly}as you can see what is happening here
[throat_clearing]
huh this is an example of where huh 
computers and modern technology can help in carrying out huh some scientific experiments

(fig., 31:06)
so this is huh 
what you are seeing here is a redwood tree and huh
some scientist wanted to study huh th- th- the biological phenomenon here 
huh for example how these tree becomes so tall
these are these are joint (( )) joint trees and huh they wanted to study 
whether they these tree absorb moisture from huh atmosphere from fog
so initial attempts were huh to place lot of heavy (( )) on this 
with lot of cables running around 
but huh then subsequently it was replaced by huh
some simple huh sensors 
miniature battery driven wireless sensor which ((were)) placed on huh trees and huh because they are wireless the scientist could move around with laptop near the base and- t- the data which is collected could be 			[Refer Slide Time: 32:00]
simply communicated without having to climb 
[throat_clearing]
or without running huh huge cables

(fig., 32:11)
[tapping]
this is an example of huh computers trying to bring safety into huh 
automobiles or huh in ((transporting)) system 
so the example is of huh French T G V train system which is the fast- system and the key to safety here is the signaling system which is all computer based
huh what it tries to do is that 
the entire huh track system is divided into segments or blocks of huh size
one point five kilometer
so at- at places they are even shorter and essentially huh
sensors and computers and the signaling mechanism try to ensure that in each block
or in each section there is at most one train ok
(( )) more than one would (( )) a disaster
[cough]
so an- an the system issues commands to huh the the driver huh
who is supposed to huh continue or stop and so on
but huh there is also a computer there 
w- which can huh take the commands and act if necessary 
if if the human operator ((fails to listen))
so le- let me go back to this list and 
[throat_clearing]
huh ((mention)) some of the other examples
i am not putting the details here huh
so this is huh an example of a- news medium 
wh- which is huh which is different from traditional huh T V or newspaper medium
[throat_clearing]
so huh basically huh 
from huh web sources huh the the news could be collected and published automatically
and huh th- that crosses huh th- the typical 
[cough]							[Refer Slide Time: 34:11]
((excuse)) me
regional boundaries and regional (( )) ok
so so it could be huh a totally different nontraditional nonlocal source of news
huh the next one is about huh 
how computers can help in huh preserving the old huh pieces of art
by using huh image processing techniques 
and the last one is use of computers in huh diagnosis through huh M R I and such techniques 
[throat_clearing]
ok i want to add huh at the end huh
some interesting huh applications we have been involved in

(fig., 35:02)
in developing huh together with the startup we have
you must have heard of ((critical)) solutions
{faintly}which is in our (( ))
so so these are picture of huh device for scanning 
underbelly of a car or any vehicle in general for security
so huh what huh the security is becoming an increasingly important concern with the
{faintly}because of terrorism the- are 
so so one would like to huh prevent huh some dangerous thing being attached to the bottom of a car 
and then the car being brought into a premise and it might explode later on
so so you need to huh scan the underbelly of the car
the traditional approach is to use (( )) and a mirror
you might have seen that huh security people would carry a mirror and then huh try to do that
the problem th- are that the illumination is very poor its generally dark 
there is not ((i mean)) one goes through the drill but doesn t really huh help you much
a- and at any given time you see only a very small part of a car
so exhaustive checking is never possible 
so the idea here is to have basically a camera base- system 
you have wh- what you are seeing in this picture are four units
each unit huh these bright dots you see are huh L E D displays 
huh L E D sources which emit light ok
and in-between you are not seeing very clearly 		[Refer Slide Time: 36:36]
but there is a digital camera video camera sitting there 
[throat_clearing]
so these lights illuminate th- this is a device huh which is placed in a pit like this ok
and as vehicle rides over it the the bottom gets illuminated and cameras take picture 
huh but what happens is that 
b- because the gap between the camera and the bottom of the car is not too much
each camera gets only a limited huh field of view ok
its sees only part of the car 
[noise]
so as car is passing a camera would see only a small part 
and- each camera actually captures a video of huh a strip of the car
[background noise]{sounds like someone coughing}
so huh now these video signals 
you can see some cabling here
these are sent to a computer which is huh somewhere in the side in a control room
[smack]
and huh each video first of all 
each frame of the video huh is taken together and huh 
you- generate a single picture ok
you you stitch these together 
it requires sophisticated huh (( )) techniques to form huh a strip of huh image
and then these strips are put together to form a complete image
so what you get is like this and- if you care 
if you see carefully you might be able to see a boundary of (( )) huh
although i have shown here but this image was taken with three 
(( )) the whole image is divided into three strips actually 
but huh the stitching is done nicely so that you don't really 
unless you are careful huh you don't see the boundaries 
so so you could see the role of huh computer 
it- it- it s a it s a dedicated application 
where a computer is performing huh single but ((fairly)) sophisticated task of creating this image out of three videos 				[Refer Slide Time: 38:35]

(fig., 38:40)
[throat_clearing]
ok huh another example is huh to automate the kind of process you would have seen on the entry exit gates huh
one like ((I I T)) so here if you huh those who have driven cars across the gate- know that huh the car- which are authorized 
[throat_clearing]
which are registered with our security 
they they carry a sticker on the windscreen and huh
the person who is driving huh is supposed to carry a card
and the se- as you as you pass through the gate 
the security man as to check the card huh
look at the huh identification number in the card
identification number on that sticker which you have and 
see if the right person is driving the right car ok
so now it s a workable system in principle but what happens in practice is that huh
its impossible to do it thoroughly 
so huh you know on- once you start this 
you put this kind of process and p- its works right
huh nicely for a few days and then after that the security ((may -)) say ok go huh 
t- th- there is no time no patience in people who are driving to wait and have it checked 
and also no patience huh in the security people to really go through it thoroughly 
[throat_clearing]
so here is an attempt to huh do this automatically 
huh what is required is that you have huh a device installed in the car
huh which carries huh small huh computer
huh so- so infact it s a it s a variation of huh what i showed you here
{referring fig., 26:11}
huh not in this packaging but in a different packaging 
huh this this as huh what is called an E PROM huh
erasable and programmable read only memory
huh we huh we have used a version of this which uses flash 
[throat_clearing]
but basically same processor ((- instruction same))
so a- processor is sitting there huh
which communicates with huh this pair of devices which is installed 
o- on a on a pole near the gate 
[throat_clearing]
so so huh this is connected to huh a P C 
[background noise]{sounds like someone coughing}
and basically huh th- this th- the main thing here is an infrared source
infrared- huh sender and receiver 
this also is capable of huh sending and receiving infrared signals
[throat_clearing]
and huh instead of huh a- a card where the number is printed huh 
which carries my identity as a driver
i would be carrying huh a- smart card ok			[Refer Slide Time: 41:32]
[noise]
(( )) i would probably have (( )) in my pocket
i will show you how it looks like
[throat_clearing]

(fig., 41:55)
so so this is a smart card and what you see huh
this golden thing is basically a chip
wh- which as a tiny processor and some memory in it ok
[throat_clearing]
huh rest of it just plastic to make it a credit card size thing 
so huh basically th- this carries the identity of the person who is driving
huh well th- these cards are used in variety of applications and- and huh
so so they could carry for example electronic cash also
huh electron- electronic cash is essentially nothing but huh 
a set of bits huh ones and zeroes 
so in this case this carries identity of the person who is driving the car
and huh this identity is also known to huh the the control computer 
which is connected to the gate
[throat_clearing]
{referring fig., 38:40}
the identity of the car is- actually huh embedded in the microprocessor 
which is there inside this device 
so so this- this as to be inserted there and huh 
before you pass through the gate you- you insert this 
and as you are passing through the gate huh this device which is on the gate will (( )) 
and seek the identity 
so identity of the car and the driver would be encrypted and communicated to this 
and then huh if if the two the- the two will be checked against a database which you have 
i- in a computer and huh then computer can be huh used to either turn a light huh green or red or open a- a barrier 
[throat_clearing]
so huh th- the- are interesting design issues which come up here
huh basically since this is a battery driven device huh 	[Refer Slide Time: 43:48]
power consumption reducing the power consumption 
becomes ex- extremely important 
so so you have to have this operation organized such that 
power consumption is minimized
(( )) i cannot go into details huh
but of course since these are the last (( )) of in-house development
and those who are huh interested can always contact us and find out more
ok so i will- like to close at this note 

(fig., 44:24)
that there are numerous other areas huh
where specially designed computer based system could touch our lives
so th- are very interesting areas very challenging and huh 
th- the are lot of excitement which could be lying ahead of you
thank you 
[throat_clearing]
[Refer Slide Time: 44:37]


