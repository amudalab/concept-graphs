Productivity experts say that breakthroughs come by thinking  nonlinearly.  In
this chapter, we discuss one of the most important nonlinear data structures in
computing trees. Tree structures are indeed a breakthrough in data organization,
for they allow us to implement a host of algorithms much faster than when using
linear data structures, such as array-based lists or linked lists. Trees also provide a
natural organization for data, and consequently have become ubiquitous structures
in file systems, graphical user interfaces, databases, Web sites, and other computer
systems.
It is not always clear what productivity experts mean by  nonlinear  thinking,
but when we say that trees are  nonlinear,  we are referring to an organizational
relationship that is richer than the simple  before  and  after  relationships between
objects in sequences. The relationships in a tree are hierarchical, with some
objects being  above  and some  below  others. Actually, the main terminology
for tree data structures comes from family trees, with the terms  parent,   child, 
 ancestor,  and  descendant  being the most common words used to describe relationships.
We show an example of a family tree in Figure 8.1.
8.1. General Trees 
8.1.1 Tree Definitions and Properties
A tree is an abstract data type that stores elements hierarchically. With the exception
of the top element, each element in a tree has a parent element and zero or
more children elements. A tree is usually visualized by placing elements inside
ovals or rectangles, and by drawing the connections between parents and children
with straight lines. (See Figure 8.2.) We typically call the top element the root
of the tree, but it is drawn as the highest element, with the other elements being
connected below (just the opposite of a botanical tree).
Africa Europe Asia Australia
Canada S. America Overseas
Domestic International TV CD Tuner
R&D Sales Purchasing Manufacturing
Electronics R Us
Figure 8.2: A tree with 17 nodes representing the organization of a fictitious corporation.
The root stores Electronics R Us. The children of the root store R&D,
Sales, Purchasing, and Manufacturing. The internal nodes store Sales, International,
Overseas, Electronics R Us, and Manufacturing.
Formal Tree Definition
Formally, we define a tree T as a set of nodes storing elements such that the nodes
have a parent-child relationship that satisfies the following properties:
  If T is nonempty, it has a special node, called the root of T, that has no parent.
  Each node v of T different from the root has a unique parent node w; every
node with parent w is a child of w.
Note that according to our definition, a tree can be empty, meaning that it does not
have any nodes. This convention also allows us to define a tree recursively such
that a tree T is either empty or consists of a node r, called the root of T, and a
(possibly empty) set of subtrees whose roots are the children of r.
 
302 Chapter 8. Trees
Other Node Relationships
Two nodes that are children of the same parent are siblings. A node v is external
if v has no children. A node v is internal if it has one or more children. External
nodes are also known as leaves.
Example 8.1: In Section 4.1.4, we discussed the hierarchical relationship between
files and directories in a computer s file system, although at the time we
did not emphasize the nomenclature of a file system as a tree. In Figure 8.3, we
revisit an earlier example. We see that the internal nodes of the tree are associated
with directories and the leaves are associated with regular files. In the UNIX
and Linux operating systems, the root of the tree is appropriately called the  root
directory,  and is represented by the symbol  /. 
/user/rt/courses/
cs016/ cs252/
homeworks/ programs/ projects/
papers/ demos/
hw1 hw2 hw3 pr1 pr2 pr3
grades
buylow sellhigh market
grades
Figure 8.3: Tree representing a portion of a file system.
A node u is an ancestor of a node v if u = v or u is an ancestor of the parent
of v. Conversely, we say that a node v is a descendant of a node u if u is an ancestor
of v. For example, in Figure 8.3, cs252/ is an ancestor of papers/, and pr3 is a
descendant of cs016/. The subtree of T rooted at a node v is the tree consisting of
all the descendants of v in T (including v itself). In Figure 8.3, the subtree rooted at
cs016/ consists of the nodes cs016/, grades, homeworks/, programs/, hw1, hw2,
hw3, pr1, pr2, and pr3.
Edges and Paths in Trees
An edge of tree T is a pair of nodes (u,v) such that u is the parent of v, or vice
versa. A path of T is a sequence of nodes such that any two consecutive nodes in
the sequence form an edge. For example, the tree in Figure 8.3 contains the path
(cs252/, projects/, demos/, market).
 
8.1. General Trees 303
Example 8.2: The inheritance relation between classes in a Python program forms
a tree when single inheritance is used. For example, in Section 2.4 we provided a
summary of the hierarchy for Python s exception types, as portrayed in Figure 8.4
(originally Figure 2.5). The BaseException class is the root of that hierarchy, while
all user-defined exception classes should conventionally be declared as descendants
of the more specific Exception class. (See, for example, the Empty class we introduced
in Code Fragment 6.1 of Chapter 6.)
Figure 8.4: A portion of Python s hierarchy of exception types.
In Python, all classes are organized into a single hierarchy, as there exists a
built-in class named object as the ultimate base class. It is a direct or indirect base
class of all other types in Python (even if not declared as such when defining a new
class). Therefore, the hierarchy pictured in Figure 8.4 is only a portion of Python s
complete class hierarchy.
As a preview of the remainder of this chapter, Figure 8.5 portrays our own
hierarchy of classes for representing various forms of a tree.
ArrayBinaryTree LinkedBinaryTree
Tree
BinaryTree LinkedTree
Figure 8.5: Our own inheritance hierarchy for modeling various abstractions and
implementations of tree data structures. In the remainder of this chapter, we provide
implementations of Tree, BinaryTree, and LinkedBinaryTree classes, and highlevel
sketches for how LinkedTree and ArrayBinaryTree might be designed.
 
304 Chapter 8. Trees
Ordered Trees
A tree is ordered if there is a meaningful linear order among the children of each
node; that is, we purposefully identify the children of a node as being the first,
second, third, and so on. Such an order is usually visualized by arranging siblings
left to right, according to their order.
Example 8.3: The components of a structured document, such as a book, are hierarchically
organized as a tree whose internal nodes are parts, chapters, and sections,
and whose leaves are paragraphs, tables, figures, and so on. (See Figure 8.6.) The
root of the tree corresponds to the book itself. We could, in fact, consider expanding
the tree further to show paragraphs consisting of sentences, sentences consisting of
words, and words consisting of characters. Such a tree is an example of an ordered
tree, because there is a well-defined order among the children of each node.
Figure 8.6: An ordered tree associated with a book.
Let s look back at the other examples of trees that we have described thus far,
and consider whether the order of children is significant. A family tree that describes
generational relationships, as in Figure 8.1, is often modeled as an ordered
tree, with siblings ordered according to their birth.
In contrast, an organizational chart for a company, as in Figure 8.2, is typically
considered an unordered tree. Likewise, when using a tree to describe an inheritance
hierarchy, as in Figure 8.4, there is no particular significance to the order
among the subclasses of a parent class. Finally, we consider the use of a tree in
modeling a computer s file system, as in Figure 8.3. Although an operating system
often displays entries of a directory in a particular order (e.g., alphabetical, chronological),
such an order is not typically inherent to the file system s representation.
 
8.1. General Trees 305
8.1.2 The Tree Abstract Data Type
As we did with positional lists in Section 7.4, we define a tree ADT using the
concept of a position as an abstraction for a node of a tree. An element is stored
at each position, and positions satisfy parent-child relationships that define the tree
structure. A position object for a tree supports the method:
p.element( ): Return the element stored at position p.
The tree ADT then supports the following accessor methods, allowing a user to
navigate the various positions of a tree:
T.root(): Return the position of the root of tree T,
or None if T is empty.
T.is root(p): Return True if position p is the root of Tree T.
T.parent(p): Return the position of the parent of position p,
or None if p is the root of T.
T.num children(p): Return the number of children of position p.
T.children(p): Generate an iteration of the children of position p.
T.is leaf(p): Return True if position p does not have any children.
len(T): Return the number of positions (and hence elements) that
are contained in tree T.
T.is empty( ): Return True if tree T does not contain any positions.
T.positions( ): Generate an iteration of all positions of tree T.
iter(T): Generate an iteration of all elements stored within tree T.
Any of the above methods that accepts a position as an argument should generate a
ValueError if that position is invalid for T.
If a tree T is ordered, then T.children(p) reports the children of p in the natural
order. If p is a leaf, then T.children(p) generates an empty iteration. In similar
regard, if tree T is empty, then both T.positions( ) and iter(T) generate empty iterations.
We will discuss general means for iterating through all positions of a tree in
Sections 8.4.
We do not define any methods for creating or modifying trees at this point.
We prefer to describe different tree update methods in conjunction with specific
implementations of the tree interface, and specific applications of trees.
 
306 Chapter 8. Trees
A Tree Abstract Base Class in Python
In discussing the object-oriented design principle of abstraction in Section 2.1.2, we
noted that a public interface for an abstract data type is often managed in Python via
duck typing. For example, we defined the notion of the public interface for a queue
ADT in Section 6.2, and have since presented several classes that implement the
queue interface (e.g., ArrayQueue in Section 6.2.2, LinkedQueue in Section 7.1.2,
CircularQueue in Section 7.2.2). However, we never gave any formal definition of
the queue ADT in Python; all of the concrete implementations were self-contained
classes that just happen to adhere to the same public interface. A more formal
mechanism to designate the relationships between different implementations of the
same abstraction is through the definition of one class that serves as an abstract
base class, via inheritance, for one or more concrete classes. (See Section 2.4.3.)
We choose to define a Tree class, in Code Fragment 8.1, that serves as an abstract
base class corresponding to the tree ADT. Our reason for doing so is that there
is quite a bit of useful code that we can provide, even at this level of abstraction, allowing
greater code reuse in the concrete tree implementations we later define. The
Tree class provides a definition of a nested Position class (which is also abstract),
and declarations of many of the accessor methods included in the tree ADT.
However, our Tree class does not define any internal representation for storing
a tree, and five of the methods given in that code fragment remain abstract
(root, parent, num children, children, and len ); each of these methods raises a
NotImplementedError. (A more formal approach for defining abstract base classes
and abstract methods, using Python s abc module, is described in Section 2.4.3.)
The subclasses are responsible for overriding abstract methods, such as children, to
provide a working implementation for each behavior, based on their chosen internal
representation.
Although the Tree class is an abstract base class, it includes several concrete
methods with implementations that rely on calls to the abstract methods of the class.
In defining the tree ADT in the previous section, we declare ten accessor methods.
Five of those are the ones we left as abstract, in Code Fragment 8.1. The other five
can be implemented based on the former. Code Fragment 8.2 provides concrete
implementations for methods is root, is leaf, and is empty. In Section 8.4, we will
explore general algorithms for traversing a tree that can be used to provide concrete
implementations of the positions and iter methods within the Tree class. The
beauty of this design is that the concrete methods defined within the Tree abstract
base class will be inherited by all subclasses. This promotes greater code reuse, as
there will be no need for those subclasses to reimplement such behaviors.
We note that, with the Tree class being abstract, there is no reason to create a
direct instance of it, nor would such an instance be useful. The class exists to serve
as a base for inheritance, and users will create instances of concrete subclasses.
 
.
8.1.3 Computing Depth and Height
Let p be the position of a node of a tree T. The depth of p is the number of
ancestors of p, excluding p itself. For example, in the tree of Figure 8.2, the node
storing International has depth 2. Note that this definition implies that the depth of
the root of T is 0. The depth of p can also be recursively defined as follows:
  If p is the root, then the depth of p is 0.
  Otherwise, the depth of p is one plus the depth of the parent of p.
Based on this definition, we present a simple, recursive algorithm, depth, in Code
Fragment 8.3, for computing the depth of a position p in Tree T. This method calls
itself recursively on the parent of p, and adds 1 to the value returned.
52 def depth(self, p):
53    Return the number of levels separating Position p from the root.   
54 if self.is root(p):
55 return 0
56 else:
57 return 1 + self.depth(self.parent(p))
Code Fragment 8.3: Method depth of the Tree class.
The running time of T.depth(p) for position p is O(dp +1), where dp denotes
the depth of p in the tree T, because the algorithm performs a constant-time recursive
step for each ancestor of p. Thus, algorithm T.depth(p) runs in O(n) worstcase
time, where n is the total number of positions of T, because a position of T
may have depth n 1 if all nodes form a single branch. Although such a running
time is a function of the input size, it is more informative to characterize the running
time in terms of the parameter dp, as this parameter may be much smaller than n.
 
8.1. General Trees 309
Height
The height of a position p in a tree T is also defined recursively:
  If p is a leaf, then the height of p is 0.
  Otherwise, the height of p is one more than the maximum of the heights of
p s children.
The height of a nonempty tree T is the height of the root of T. For example, the
tree of Figure 8.2 has height 4. In addition, height can also be viewed as follows.
Proposition 8.4: The height of a nonempty tree T is equal to the maximum of
the depths of its leaf positions.
We leave the justification of this fact to an exercise (R-8.3). We present an
algorithm, height1, implemented in Code Fragment 8.4 as a nonpublic method
height1 of the Tree class. It computes the height of a nonempty tree T based on
Proposition 8.4 and the algorithm depth from Code Fragment 8.3.
Unfortunately, algorithm height1 is not very efficient. We have not yet defined
the positions( ) method; we will see that it can be implemented to run in O(n) time,
where n is the number of positions of T. Because height1 calls algorithm depth(p)
on each leaf of T, its running time is O(n+ p L(dp +1)), where L is the set of
leaf positions of T. In the worst case, the sum  p L(dp +1) is proportional to n2.
(See Exercise C-8.33.) Thus, algorithm height1 runs in O(n2) worst-case time.
We can compute the height of a tree more efficiently, in O(n) worst-case time,
by relying instead on the original recursive definition. To do this, we will parameterize
a function based on a position within the tree, and calculate the height of
the subtree rooted at that position. 

310 Chapter 8. Trees
It is important to understand why algorithm height2 is more efficient than
height1. The algorithm is recursive, and it progresses in a top-down fashion. If
the method is initially called on the root of T, it will eventually be called once for
each position of T. This is because the root eventually invokes the recursion on
each of its children, which in turn invokes the recursion on each of their children,
and so on.
We can determine the running time of the height2 algorithm by summing, over
all the positions, the amount of time spent on the nonrecursive part of each call.
(Review Section 4.2 for analyses of recursive processes.) In our implementation,
there is a constant amount of work per position, plus the overhead of computing the
maximum over the iteration of children. Although we do not yet have a concrete
implementation of children(p), we assume that such an iteration is generated in
O(cp +1) time, where cp denotes the number of children of p. Algorithm height2
spends O(cp+1) time at each position p to compute the maximum, and its overall
running time is O( p(cp +1)) = O(n+ p cp). In order to complete the analysis,
we make use of the following property.
Proposition 8.5: Let T be a tree with n positions, and let cp denote the number of
children of a position p of T. Then, summing over the positions of T,  p cp =n 1.
Justification: Each position of T, with the exception of the root, is a child of
another position, and thus contributes one unit to the above sum.
By Proposition 8.5, the running time of algorithm height2, when called on the
root of T, is O(n), where n is the number of positions of T.
Revisiting the public interface for our Tree class, the ability to compute heights
of subtrees is beneficial, but a user might expect to be able to compute the height
of the entire tree without explicitly designating the tree root. We can wrap the nonpublic
height2 in our implementation with a public height method that provides
a default interpretation when invoked on tree T with syntax T.height(). Such an
implementation is given in Code Fragment 8.6.

8.2. Binary Trees 
8.2 Binary Trees
A binary tree is an ordered tree with the following properties:
1. Every node has at most two children.
2. Each child node is labeled as being either a left child or a right child.
3. A left child precedes a right child in the order of children of a node.
The subtree rooted at a left or right child of an internal node v is called a left subtree
or right subtree, respectively, of v. A binary tree is proper if each node has either
zero or two children. Some people also refer to such trees as being full binary
trees. Thus, in a proper binary tree, every internal node has exactly two children.
A binary tree that is not proper is improper.
Example 8.6: An important class of binary trees arises in contexts where we wish
to represent a number of different outcomes that can result from answering a series
of yes-or-no questions. Each internal node is associated with a question. Starting at
the root, we go to the left or right child of the current node, depending on whether
the answer to the question is  Yes  or  No.  With each decision, we follow an
edge from a parent to a child, eventually tracing a path in the tree from the root
to a leaf. Such binary trees are known as decision trees, because a leaf position p
in such a tree represents a decision of what to do if the questions associated with
p s ancestors are answered in a way that leads to p. A decision tree is a proper
binary tree. Figure 8.7 illustrates a decision tree that provides recommendations to
a prospective investor.
Chapter 8. Trees
Example 8.7: An arithmetic expression can be represented by a binary treewhose
leaves are associated with variables or constants, and whose internal nodes are
associated with one of the operators +,  ,  , and /. (See Figure 8.8.) Each node
in such a tree has a value associated with it.
  If a node is leaf, then its value is that of its variable or constant.
  If a node is internal, then its value is defined by applying its operation to the
values of its children.
An arithmetic expression tree is a proper binary tree, since each operator +,  ,  ,
and / takes exactly two operands. Of course, if we were to allow unary operators,
like negation ( ), as in   x,  then we could have an improper binary tree.
Figure 8.8: A binary tree representing an arithmetic expression. This tree represents
the expression ((((3 +1) 3)/((9  5)+2)) ((3 (7 4)) +6)). The value
associated with the internal node labeled  /  is 2.
A Recursive Binary Tree Definition
Incidentally, we can also define a binary tree in a recursive way such that a binary
tree is either empty or consists of:
  A node r, called the root of T, that stores an element
  A binary tree (possibly empty), called the left subtree of T
  A binary tree (possibly empty), called the right subtree of T
 
8.2. Binary Trees 313
8.2.1 The Binary Tree Abstract Data Type
As an abstract data type, a binary tree is a specialization of a tree that supports three
additional accessor methods:
T.left(p): Return the position that represents the left child of p,
or None if p has no left child.
T.right(p): Return the position that represents the right child of p,
or None if p has no right child.
T.sibling(p): Return the position that represents the sibling of p,
or None if p has no sibling.
Just as in Section 8.1.2 for the tree ADT, we do not define specialized update methods
for binary trees here. Instead, we will consider some possible update methods
when we describe specific implementations and applications of binary trees.
The BinaryTree Abstract Base Class in Python
Just as Tree was defined as an abstract base class in Section 8.1.2, we define a
new BinaryTree class associated with the binary tree ADT. We rely on inheritance
to define the BinaryTree class based upon the existing Tree class. However, our
BinaryTree class remains abstract, as we still do not provide complete specifications
for how such a structure will be represented internally, nor implementations
for some necessary behaviors.
Our Python implementation of the BinaryTree class is given in Code Fragment
8.7. By using inheritance, a binary tree supports all the functionality that was
defined for general trees (e.g., parent, is leaf, root). Our new class also inherits the
nested Position class that was originally defined within the Tree class definition.
In addition, the new class provides declarations for new abstract methods left and
right that should be supported by concrete subclasses of BinaryTree.
Our new class also provides two concrete implementations of methods. The
new sibling method is derived from the combination of left, right, and parent. Typically,
we identify the sibling of a position p as the  other  child of p s parent.
However, if p is the root, it has no parent, and thus no sibling. Also, p may be the
only child of its parent, and thus does not have a sibling.
Finally, Code Fragment 8.7 provides a concrete implementation of the children
method; this method is abstract in the Tree class. Although we have still not specified
how the children of a node will be stored, we derive a generator for the ordered
children based upon the implied behavior of abstract methods left and right.
 
314 Chapter 8. Trees

8.2. Binary Trees 315
8.2.2 Properties of Binary Trees
Binary trees have several interesting properties dealing with relationships between
their heights and number of nodes. We denote the set of all nodes of a tree T at the
same depth d as level d of T. In a binary tree, level 0 has at most one node (the
root), level 1 has at most two nodes (the children of the root), level 2 has at most
four nodes, and so on. (See Figure 8.9.) In general, level d has at most 2d nodes.
Figure 8.9: Maximum number of nodes in the levels of a binary tree.
We can see that the maximum number of nodes on the levels of a binary tree
grows exponentially as we go down the tree. From this simple observation, we can
derive the following properties relating the height of a binary tree T with its number
of nodes. A detailed justification of these properties is left as Exercise R-8.8.
Proposition 8.8: Let T be a nonempty binary tree, and let n, nE, nI and h denote
the number of nodes, number of external nodes, number of internal nodes, and
height of T, respectively. 
316 Chapter 8. Trees
Relating Internal Nodes to External Nodes in a Proper Binary Tree
In addition to the earlier binary tree properties, the following relationship exists
between the number of internal nodes and external nodes in a proper binary tree.
Proposition 8.9: In a nonempty proper binary tree T, with nE external nodes and
nI internal nodes, we have nE = nI +1.
Justification: We justify this proposition by removing nodes from T and dividing
them up into two  piles,  an internal-node pile and an external-node pile, until
T becomes empty. The piles are initially empty. By the end, we will show that the
external-node pile has one more node than the internal-node pile. We consider two
cases:
Case 1: If T has only one node v, we remove v and place it on the external-node
pile. Thus, the external-node pile has one node and the internal-node pile is
empty.
Case 2: Otherwise (T has more than one node), we remove from T an (arbitrary)
external node w and its parent v, which is an internal node. We place w on
the external-node pile and v on the internal-node pile. If v has a parent u,
then we reconnect u with the former sibling z of w, as shown in Figure 8.10.
This operation, removes one internal node and one external node, and leaves
the tree being a proper binary tree.
Repeating this operation, we eventually are left with a final tree consisting
of a single node. Note that the same number of external and internal nodes
have been removed and placed on their respective piles by the sequence of
operations leading to this final tree. Now, we remove the node of the final
tree and we place it on the external-node pile. Thus, the the external-node
pile has one more node than the internal-node pile.
Figure 8.10: Operation that removes an external node and its parent node, used in
the justification of Proposition 8.9.
Note that the above relationship does not hold, in general, for improper binary
trees and nonbinary trees, although there are other interesting relationships that do
hold. (See Exercises C-8.32 through C-8.34.)
 
8.3. Implementing Trees 317
8.3 Implementing Trees
The Tree and BinaryTree classes that we have defined thus far in this chapter are
both formally abstract base classes. Although they provide a great deal of support,
neither of them can be directly instantiated. We have not yet defined key implementation
details for how a tree will be represented internally, and how we can
effectively navigate between parents and children. Specifically, a concrete implementation
of a tree must provide methods root, parent, num children, children,
len , and in the case of BinaryTree, the additional accessors left and right.
There are several choices for the internal representation of trees. We describe
the most common representations in this section. We begin with the case of a
binary tree, since its shape is more narrowly defined.
8.3.1 Linked Structure for Binary Trees
A natural way to realize a binary tree T is to use a linked structure, with a node
(see Figure 8.11a) that maintains references to the element stored at a position p
and to the nodes associated with the children and parent of p. If p is the root of
T, then the parent field of p is None. Likewise, if p does not have a left child
(respectively, right child), the associated field is None. The tree itself maintains an
instance variable storing a reference to the root node (if any), and a variable, called
size, that represents the overall number of nodes of T. We show such a linked
structure representation of a binary tree in Figure 8.11b.
Figure 8.11: A linked structure for representing: (a) a single node; (b) a binary tree.
 
318 Chapter 8. Trees
Python Implementation of a Linked Binary Tree Structure
In this section, we define a concrete LinkedBinaryTree class that implements the
binary tree ADT by subclassing the BinaryTree class. Our general approach is very
similar to what we used when developing the PositionalList in Section 7.4: We
define a simple, nonpublic Node class to represent a node, and a public Position
class that wraps a node. We provide a validate utility for robustly checking the
validity of a given position instance when unwrapping it, and a make position
utility for wrapping a node as a position to return to a caller.
Those definitions are provided in Code Fragment 8.8. As a formality, the new
Position class is declared to inherit immediately from BinaryTree.Position. Technically,
the BinaryTree class definition (see Code Fragment 8.7) does not formally
declare such a nested class; it trivially inherits it from Tree.Position. A minor benefit
from this design is that our position class inherits the ne special method
so that syntax p != q is derived appropriately relative to eq .
Our class definition continues, in Code Fragment 8.9, with a constructor and
with concrete implementations for the methods that remain abstract in the Tree and
BinaryTree classes. The constructor creates an empty tree by initializing root to
None and size to zero. These accessor methods are implemented with careful use
of the validate and make position utilities to safeguard against boundary cases.
Operations for Updating a Linked Binary Tree
Thus far, we have provided functionality for examining an existing binary tree.
However, the constructor for our LinkedBinaryTree class results in an empty tree
and we have not provided any means for changing the structure or content of a tree.
We chose not to declare update methods as part of the Tree or BinaryTree abstract
base classes for several reasons. First, although the principle of encapsulation
suggests that the outward behaviors of a class need not depend on the internal
representation, the efficiency of the operations depends greatly upon the representation.
We prefer to have each concrete implementation of a tree class offer the most
suitable options for updating a tree.
The second reason we do not provide update methods in the base class is that
we may not want such update methods to be part of a public interface. There are
many applications of trees, and some forms of update operations that are suitable
for one application may be unacceptable in another. However, if we place an update
method in a base class, any class that inherits from that base will inherit the update
method. Consider, for example, the possibility of a method T.replace(p, e) that
replaces the element stored at position p with another element e. Such a general
method may be unacceptable in the context of an arithmetic expression tree (see
Example 8.7 on page 312, and a later case study in Section 8.5), because we may
want to enforce that internal nodes store only operators as elements.
 
8.3. Implementing Trees 319
For linked binary trees, a reasonable set of update methods to support for general
usage are the following:
T.add root(e): Create a root for an empty tree, storing e as the element,
and return the position of that root; an error occurs if the
tree is not empty.
T.add left(p, e): Create a new node storing element e, link the node as the
left child of position p, and return the resulting position;
an error occurs if p already has a left child.
T.add right(p, e): Create a new node storing element e, link the node as the
right child of position p, and return the resulting position;
an error occurs if p already has a right child.
T.replace(p, e): Replace the element stored at position p with element e,
and return the previously stored element.
T.delete(p): Remove the node at position p, replacing it with its child,
if any, and return the element that had been stored at p;
an error occurs if p has two children.
T.attach(p, T1, T2): Attach the internal structure of trees T1 and T2, respectively,
as the left and right subtrees of leaf position p of
T, and reset T1 and T2 to empty trees; an error condition
occurs if p is not a leaf.
We have specifically chosen this collection of operations because each can be
implemented in O(1) worst-case time with our linked representation. The most
complex of these are delete and attach, due to the case analyses involving the
various parent-child relationships and boundary conditions, yet there remains only
a constant number of operations to perform. (The implementation of both methods
could be greatly simplified if we used a tree representation with a sentinel node,
akin to our treatment of positional lists; see Exercise C-8.40).
To avoid the problem of undesirable update methods being inherited by subclasses
of LinkedBinaryTree, we have chosen an implementation in which none
of the above methods are publicly supported. Instead, we provide nonpublic versions
of each, for example, providing the underscored delete in lieu of a public
delete. Our implementations of these six update methods are provided in Code
Fragments 8.10 and 8.11.
In particular applications, subclasses of LinkedBinaryTree can invoke the nonpublic
methods internally, while preserving a public interface that is appropriate
for the application. A subclass may also choose to wrap one or more of the nonpublic
update methods with a public method to expose it to the user. We leave as
an exercise (R-8.15), the task of defining a MutableLinkedBinaryTree subclass that
provides public methods wrapping each of these six update methods.
 
8.3. Implementing Trees 321
 Chapter 8. Trees
Performance of the Linked Binary Tree Implementation
To summarize the efficiencies of the linked structure representation, we analyze the
running times of the LinkedBinaryTree methods, including derived methods that
are inherited from the Tree and BinaryTree classes:
  The len method, implemented in LinkedBinaryTree, uses an instance variable
storing the number of nodes of T and takes O(1) time. Method is empty,
inherited from Tree, relies on a single call to len and thus takes O(1) time.
  The accessor methods root, left, right, parent, and num children are implemented
directly in LinkedBinaryTree and take O(1) time. The sibling and
children methods are derived in BinaryTree based on a constant number of
calls to these other accessors, so they run in O(1) time as well.
  The is root and is leaf methods, from the Tree class, both run in O(1) time,
as is root calls root and then relies on equivalence testing of positions, while
is leaf calls left and right and verifies that None is returned by both.
  Methods depth and height were each analyzed in Section 8.1.3. The depth
method at position p runs in O(dp+1) time where dp is its depth; the height
method on the root of the tree runs in O(n) time.
  The various update methods add root, add left, add right, replace, delete,
and attach (that is, their nonpublic implementations) each run in O(1) time,
as they involve relinking only a constant number of nodes per operation.
Table 8.1 summarizes the performance of the linked structure implementation of a
binary tree.
Operation Running Time
len, is empty O(1)
root, parent, left, right, sibling, children, num children O(1)
is root, is leaf O(1)
depth(p) O(dp+1)
height O(n)
add root, add left, add right, replace, delete, attach O(1)
Table 8.1: Running times for the methods of an n-node binary tree implemented
with a linked structure. The space usage is O(n).
 
8.3. Implementing Trees 325
8.3.2 Array-Based Representation of a Binary Tree
An alternative representation of a binary tree T is based on a way of numbering the
positions of T. For every position p of T, let f (p) be the integer defined as follows.
  If p is the root of T, then f (p) = 0.
  If p is the left child of position q, then f (p) = 2 f (q)+1.
  If p is the right child of position q, then f (p) = 2 f (q)+2.
The numbering function f is known as a level numbering of the positions in a
binary tree T, for it numbers the positions on each level of T in increasing order
from left to right. (See Figure 8.12.) Note well that the level numbering is based
on potential positions within the tree, not actual positions of a given tree, so they
are not necessarily consecutive. For example, in Figure 8.12(b), there are no nodes
with level numbering 13 or 14, because the node with level numbering 6 has no
children.

326 Chapter 8. Trees
The level numbering function f suggests a representation of a binary tree T
by means of an array-based structure A (such as a Python list), with the element
at position p of T stored at index f (p) of the array. We show an example of an
array-based representation of a binary tree in Figure 8.13.
Figure 8.13: Representation of a binary tree by means of an array.
One advantage of an array-based representation of a binary tree is that a position
p can be represented by the single integer f (p), and that position-based methods
such as root, parent, left, and right can be implemented using simple arithmetic
operations on the number f (p). Based on our formula for the level numbering, the
left child of p has index 2 f (p)+1, the right child of p has index 2 f (p)+2, and
the parent of p has index _( f (p) 1)/2_. We leave the details of a complete implementation
as an exercise (R-8.18).
The space usage of an array-based representation depends greatly on the shape
of the tree. Let n be the number of nodes of T, and let fM be the maximum value
of f (p) over all the nodes of T. The array A requires length N = 1+ fM, since
elements range from A[0] to A[ fM]. Note that A may have a number of empty cells
that do not refer to existing nodes of T. In fact, in the worst case, N = 2n  1,
the justification of which is left as an exercise (R-8.16). In Section 9.3, we will
see a class of binary trees, called  heaps  for which N = n. Thus, in spite of the
worst-case space usage, there are applications for which the array representation
of a binary tree is space efficient. Still, for general binary trees, the exponential
worst-case space requirement of this representation is prohibitive.
Another drawback of an array representation is that some update operations for
trees cannot be efficiently supported. For example, deleting a node and promoting
its child takes O(n) time because it is not just the child that moves locations within
the array, but all descendants of that child.
8.3. Implementing Trees 327
8.3.3 Linked Structure for General Trees
When representing a binary tree with a linked structure, each node explicitly maintains
fields left and right as references to individual children. For a general tree,
there is no a priori limit on the number of children that a node may have. A natural
way to realize a general tree T as a linked structure is to have each node store a
single container of references to its children. For example, a children field of a
node can be a Python list of references to the children of the node (if any). Such a
linked representation .Figure 8.14: The linked structure for a general tree: (a) the structure of a node; (b) a
larger portion of the data structure associated with a node and its children.
Table 8.2 summarizes the performance of the implementation of a general tree
using a linked structure. The analysis is left as an exercise (R-8.14), but we note
that, by using a collection to store the children of each position p, we can implement
children(p) by simply iterating that collection.
Operation Running Time
len, is empty O(1)
root, parent, is root, is leaf O(1)
children(p) O(cp +1)
depth(p) O(dp +1)
height O(n)
Table 8.2: Running times of the accessor methods of an n-node general tree implemented
with a linked structure. We let cp denote the number of children of a
position p. The space usage is O(n).
328 Chapter 8. Trees
8.4 Tree Traversal Algorithms
A traversal of a tree T is a systematic way of accessing, or  visiting,  all the positions
of T. The specific action associated with the  visit  of a position p depends
on the application of this traversal, and could involve anything from incrementing
a counter to performing some complex computation for p. In this section, we
describe several common traversal schemes for trees, implement them in the context
of our various tree classes, and discuss several common applications of tree
traversals.
8.4.1 Preorder and Postorder Traversals of General Trees
In a preorder traversal of a tree T, the root of T is visited first and then the subtrees
rooted at its children are traversed recursively. If the tree is ordered, then
the subtrees are traversed according to the order of the children. The pseudo-code
for the preorder traversal of the subtree rooted at a position p is shown in Code
Fragment 8.12.
Algorithm preorder(T, p):
perform the  visit  action for position p
for each child c in T.children(p) do
preorder(T, c) {recursively traverse the subtree rooted at c}
Code Fragment 8.12: Algorithm preorder for performing the preorder traversal of a
subtree rooted at position p of a tree T.
Figure 8.15 portrays the order in which positions of a sample tree are visited
during an application of the preorder traversal algorithm.
Paper
Title Abstract   1   2   3 References
  1.1   1.2   2.1   2.2   2.3   3.1   3.2
Figure 8.15: Preorder traversal of an ordered tree, where the children of each position
are ordered from left to right.

8.4. Tree Traversal Algorithms 
Postorder Traversal
Another important tree traversal algorithm is the postorder traversal. In some
sense, this algorithm can be viewed as the opposite of the preorder traversal, because
it recursively traverses the subtrees rooted at the children of the root first, and
then visits the root (hence, the name  postorder ). Pseudo-code for the postorder
traversal is given in Code Fragment 8.13, and an example of a postorder traversal
is portrayed in Figure 8.16.
Algorithm postorder(T, p):
for each child c in T.children(p) do
postorder(T, c) {recursively traverse the subtree rooted at c}
perform the  visit  action for position p
Code Fragment 8.13: Algorithm postorder for performing the postorder traversal of
a subtree rooted at position p of a tree T.
Paper
Title Abstract   1   2   3 References
  1.1   1.2   2.1   2.2   2.3   3.1   3.2
Figure 8.16: Postorder traversal of the ordered tree of Figure 8.15.
Running-Time Analysis
Both preorder and postorder traversal algorithms are efficient ways to access all the
positions of a tree. The analysis of either of these traversal algorithms is similar to
that of algorithm height2, given in Code Fragment 8.5 of Section 8.1.3. At each
position p, the nonrecursive part of the traversal algorithm requires time O(cp+1),
where cp is the number of children of p, under the assumption that the  visit  itself
takes O(1) time. By Proposition 8.5, the overall running time for the traversal of
tree T is O(n), where n is the number of positions in the tree. This running time is
asymptotically optimal since the traversal must visit all the n positions of the tree.
 
330 Chapter 8. Trees
8.4.2 Breadth-First Tree Traversal
Although the preorder and postorder traversals are common ways of visiting the
positions of a tree, another common approach is to traverse a tree so that we visit
all the positions at depth d before we visit the positions at depth d +1. Such an
algorithm is known as a breadth-first traversal.
A breadth-first traversal is a common approach used in software for playing
games. A game tree represents the possible choices of moves that might be made
by a player (or computer) during a game, with the root of the tree being the initial
configuration for the game. For example, Figure 8.17 displays a partial game tree
for Tic-Tac-Toe.

Figure 8.17: Partial game tree for Tic-Tac-Toe, with annotations displaying the order
in which positions are visited in a breadth-first traversal.
A breadth-first traversal of such a game tree is often performed because a computer
may be unable to explore a complete game tree in a limited amount of time. So the
computer will consider all moves, then responses to those moves, going as deep as
computational time allows.
Pseudo-code for a breadth-first traversal is given in Code Fragment 8.14. The
process is not recursive, since we are not traversing entire subtrees at once. We use
a queue to produce a FIFO (i.e., first-in first-out) semantics for the order in which
we visit nodes. The overall running time is O(n), due to the n calls to enqueue and
n calls to dequeue.
Algorithm breadthfirst(T):
Initialize queue Q to contain T.root( )
while Q not empty do
p = Q.dequeue( ) {p is the oldest entry in the queue}
perform the  visit  action for position p
for each child c in T.children(p) do
Q.enqueue(c) {add p s children to the end of the queue for later visits}
Code Fragment 8.14: Algorithm for performing a breadth-first traversal of a tree.
 
8.4. Tree Traversal Algorithms 331
8.4.3 Inorder Traversal of a Binary Tree
The standard preorder, postorder, and breadth-first traversals that were introduced
for general trees, can be directly applied to binary trees. In this section, we introduce
another common traversal algorithm specifically for a binary tree.
During an inorder traversal, we visit a position between the recursive traversals
of its left and right subtrees. The inorder traversal of a binary tree T can be
informally viewed as visiting the nodes of T  from left to right.  Indeed, for every
position p, the inorder traversal visits p after all the positions in the left subtree of
p and before all the positions in the right subtree of p. Pseudo-code for the inorder
traversal algorithm is given in Code Fragment 8.15, and an example of an inorder
traversal is portrayed in Figure 8.18.
Algorithm inorder(p):
if p has a left child lc then
inorder(lc) {recursively traverse the left subtree of p}
perform the  visit  action for position p
if p has a right child rc then
inorder(rc) {recursively traverse the right subtree of p}
Code Fragment 8.15: Algorithm inorder for performing an inorder traversal of a
subtree rooted at position p of a binary tree.
3 1 9 5 7 4
  3   2 3  
      6
   
 
Figure 8.18: Inorder traversal of a binary tree.
The inorder traversal algorithm has several important applications. When using
a binary tree to represent an arithmetic expression, as in Figure 8.18, the inorder
traversal visits positions in a consistent order with the standard representation of
the expression, as in 3+1 3/9 5+2. . . (albeit without parentheses).
 
332 Chapter 8. Trees
Binary Search Trees
An important application of the inorder traversal algorithm arises when we store an
ordered sequence of elements in a binary tree, defining a structure we call a binary
search tree. Let S be a set whose unique elements have an order relation. For
example, S could be a set of integers. A binary search tree for S is a binary tree T
such that, for each position p of T:
  Position p stores an element of S, denoted as e(p).
  Elements stored in the left subtree of p (if any) are less than e(p).
  Elements stored in the right subtree of p (if any) are greater than e(p).
An example of a binary search tree is shown in Figure 8.19. The above properties
assure that an inorder traversal of a binary search tree T visits the elements in
nondecreasing order.
36
25
31
42
12
62
75
58
90
Figure 8.19: A binary search tree storing integers. The solid path is traversed when
searching (successfully) for 36. The dashed path is traversed when searching (unsuccessfully)
for 70.
We can use a binary search tree T for set S to find whether a given search
value v is in S, by traversing a path down the tree T, starting at the root. At each
internal position p encountered, we compare our search value v with the element
e(p) stored at p. If v < e(p), then the search continues in the left subtree of p.
If v = e(p), then the search terminates successfully. If v > e(p), then the search
continues in the right subtree of p. Finally, if we reach an empty subtree, the search
terminates unsuccessfully. In other words, a binary search tree can be viewed as a
binary decision tree (recall Example 8.6), where the question asked at each internal
node is whether the element at that node is less than, equal to, or larger than the
element being searched for. We illustrate several examples of the search operation
in Figure 8.19.
Note that the running time of searching in a binary search tree T is proportional
to the height of T. Recall from Proposition 8.8 that the height of a binary tree with
n nodes can be as small as log(n+1) 1 or as large as n 1. Thus, binary search
trees are most efficient when they have small height. Chapter 11 is devoted to the
study of search trees.
 
8.4. Tree Traversal Algorithms 333
8.4.4 Implementing Tree Traversals in Python
When first defining the tree ADT in Section 8.1.2, we stated that tree T should
include support for the following methods:
T.positions( ): Generate an iteration of all positions of tree T.
iter(T): Generate an iteration of all elements stored within tree T.
At that time, we did not make any assumption about the order in which these
iterations report their results. In this section, we demonstrate how any of the tree
traversal algorithms we have introduced could be used to produce these iterations.
To begin, we note that it is easy to produce an iteration of all elements of a
tree, if we rely on a presumed iteration of all positions. Therefore, support for
the iter(T) syntax can be formally provided by a concrete implementation of the
special method iter within the abstract base class Tree. We rely on Python s
generator syntax as the mechanism for producing iterations. (See Section 1.8.) Our
implementation of Tree. iter is given in Code Fragment 8.16.
75 def iter (self):
76    Generate an iteration of the tree s elements.   
77 for p in self.positions( ): # use same order as positions()
78 yield p.element( ) # but yield each element
Code Fragment 8.16: Iterating all elements of a Tree instance, based upon an iteration
of the positions of the tree. This code should be included in the body of the
Tree class.
To implement the positions method, we have a choice of tree traversal algorithms.
Given that there are advantages to each of those traversal orders, we will
provide independent implementations of each strategy that can be called directly
by a user of our class. We can then trivially adapt one of those as a default order
for the positions method of the tree ADT.
Preorder Traversal
We begin by considering the preorder traversal algorithm. We will support a public
method with calling signature T.preorder( ) for tree T, which generates a preorder
iteration of all positions within the tree. However, the recursive algorithm for generating
a preorder traversal, as originally described in Code Fragment 8.12, must
be parameterized by a specific position within the tree that serves as the root of a
subtree to traverse. A standard solution for such a circumstance is to define a nonpublic
utility method with the desired recursive parameterization, and then to have
the public method preorder invoke the nonpublic method upon the root of the tree.
Our implementation of such a design is given in Code Fragment 8.17.
 
334 Chapter 8. Trees
79 def preorder(self):
80    Generate a preorder iteration of positions in the tree.   
81 if not self.is empty( ):
82 for p in self. subtree preorder(self.root( )): # start recursion
83 yield p
84
85 def subtree preorder(self, p):
86    Generate a preorder iteration of positions in subtree rooted at p.   
87 yield p # visit p before its subtrees
88 for c in self.children(p): # for each child c
89 for other in self. subtree preorder(c): # do preorder of c s subtree
90 yield other # yielding each to our caller
Code Fragment 8.17: Support for performing a preorder traversal of a tree. This
code should be included in the body of the Tree class.
Formally, both preorder and the utility subtree preorder are generators. Rather
than perform a  visit  action from within this code, we yield each position to the
caller and let the caller decide what action to perform at that position.
The subtree preorder method is the recursive one. However, because we are
relying on generators rather than traditional functions, the recursion has a slightly
different form. In order to yield all positions within the subtree of child c, we loop
over the positions yielded by the recursive call self. subtree preorder(c), and reyield
each position in the outer context. Note that if p is a leaf, the for loop over
self.children(p) is trivial (this is the base case for our recursion).
We rely on a similar technique in the public preorder method to re-yield all
positions that are generated by the recursive process starting at the root of the tree;
if the tree is empty, nothing is yielded. At this point, we have provided full support
for the preorder generator. A user of the class can therefore write code such as
for p in T.preorder( ):
#  visit  position p
The official tree ADT requires that all trees support a positions method as well. To
use a preorder traversal as the default order of iteration, we include the definition
shown in Code Fragment 8.18 within our Tree class. Rather than loop over the
results returned by the preorder call, we return the entire iteration as an object.
91 def positions(self):
92    Generate an iteration of the tree s positions.   
93 return self.preorder( ) # return entire preorder iteration
Code Fragment 8.18: An implementation of the positions method for the Tree class
that relies on a preorder traversal to generate the results.
 
8.4. Tree Traversal Algorithms 335
Postorder Traversal
We can implement a postorder traversal using very similar technique as with a
preorder traversal. The only difference is that within the recursive utility for a postorder
we wait to yield position p until after we have recursively yield the positions
in its subtrees. An implementation is given in Code Fragment 8.19.
94 def postorder(self):
95    Generate a postorder iteration of positions in the tree.   
96 if not self.is empty( ):
97 for p in self. subtree postorder(self.root( )): # start recursion
98 yield p
99
100 def subtree postorder(self, p):
101    Generate a postorder iteration of positions in subtree rooted at p.   
102 for c in self.children(p): # for each child c
103 for other in self. subtree postorder(c): # do postorder of c s subtree
104 yield other # yielding each to our caller
105 yield p # visit p after its subtrees
Code Fragment 8.19: Support for performing a postorder traversal of a tree. This
code should be included in the body of the Tree class.
Breadth-First Traversal
In Code Fragment 8.20, we provide an implementation of the breadth-first traversal
algorithm in the context of our Tree class. Recall that the breadth-first traversal
algorithm is not recursive; it relies on a queue of positions to manage the traversal
process. Our implementation uses the LinkedQueue class from Section 7.1.2,
although any implementation of the queue ADT would suffice.
Inorder Traversal for Binary Trees
The preorder, postorder, and breadth-first traversal algorithms are applicable to
all trees, and so we include their implementations within the Tree abstract base
class. Those methods are inherited by the abstract BinaryTree class, the concrete
LinkedBinaryTree class, and any other dependent tree classes we might develop.
The inorder traversal algorithm, because it explicitly relies on the notion of a
left and right child of a node, only applies to binary trees. We therefore include its
definition within the body of the BinaryTree class. We use a similar technique to
implement an inorder traversal (Code Fragment 8.21) as we did with preorder and
postorder traversals.
 
336 Chapter 8. Trees
106 def breadthfirst(self):
107    Generate a breadth-first iteration of the positions of the tree.   
108 if not self.is empty( ):
109 fringe = LinkedQueue( ) # known positions not yet yielded
110 fringe.enqueue(self.root( )) # starting with the root
111 while not fringe.is empty( ):
112 p = fringe.dequeue( ) # remove from front of the queue
113 yield p # report this position
114 for c in self.children(p):
115 fringe.enqueue(c) # add children to back of queue
Code Fragment 8.20: An implementation of a breadth-first traversal of a tree. This
code should be included in the body of the Tree class.
37 def inorder(self):
38    Generate an inorder iteration of positions in the tree.   
39 if not self.is empty( ):
40 for p in self. subtree inorder(self.root( )):
41 yield p
42
43 def subtree inorder(self, p):
44    Generate an inorder iteration of positions in subtree rooted at p.   
45 if self.left(p) is not None: # if left child exists, traverse its subtree
46 for other in self. subtree inorder(self.left(p)):
47 yield other
48 yield p # visit p between its subtrees
49 if self.right(p) is not None: # if right child exists, traverse its subtree
50 for other in self. subtree inorder(self.right(p)):
51 yield other
Code Fragment 8.21: Support for performing an inorder traversal of a binary tree.
This code should be included in the BinaryTree class (given in Code Fragment 8.7).
For many applications of binary trees, an inorder traversal provides a natural
iteration. We could make it the default for the BinaryTree class by overriding the
positions method that was inherited from the Tree class (see Code Fragment 8.22).
52 # override inherited version to make inorder the default
53 def positions(self):
54    Generate an iteration of the tree s positions.   
55 return self.inorder( ) # make inorder the default
Code Fragment 8.22: Defining the BinaryTree.position method so that positions are
reported using inorder traversal.
 
8.4. Tree Traversal Algorithms 337
8.4.5 Applications of Tree Traversals
In this section, we demonstrate several representative applications of tree traversals,
including some customizations of the standard traversal algorithms.
Table of Contents
When using a tree to represent the hierarchical structure of a document, a preorder
traversal of the tree can naturally be used to produce a table of contents for the document.
For example, the table of contents associated with the tree from Figure 8.15
is displayed in Figure 8.20. Part (a) of that figure gives a simple presentation with
one element per line; part (b) shows a more attractive presentation produced by
indenting each element based on its depth within the tree. A similar presentation
could be used to display the contents of a computer s file system, based on its tree
representation (as in Figure 8.3).
Paper Paper
Title Title
Abstract Abstract
1 1
1.1 1.1
1.2 1.2
2 2
2.1 2.1
... ...
(a) (b)
Figure 8.20: Table of contents for a document represented by the tree in Figure 8.15:
(a) without indentation; (b) with indentation based on depth within the tree.
The unindented version of the table of contents, given a tree T, can be produced
with the following code:
for p in T.preorder( ):
print(p.element( ))
To produce the presentation of Figure 8.20(b), we indent each element with a
number of spaces equal to twice the element s depth in the tree (hence, the root element
was unindented). Although we could replace the body of the above loop with
the statement print(2 T.depth(p) + str(p.element())),
such an approach is
unnecessarily inefficient. Although the work to produce the preorder traversal runs
in O(n) time, based on the analysis of Section 8.4.1, the calls to depth incur a hidden
cost. Making a call to depth from every position of the tree results in O(n2)
worst-case time, as noted when analyzing the algorithm height1 in Section 8.1.3.
 
338 Chapter 8. Trees
A preferred approach to producing an indented table of contents is to redesign
a top-down recursion that includes the current depth as an additional parameter.
Such an implementation is provided in Code Fragment 8.23. This implementation
runs in worst-case O(n) time (except, technically, the time it takes to print strings
of increasing lengths).
1 def preorder indent(T, p, d):
2    Print preorder representation of subtree of T rooted at p at depth d.   
3 print(2 d + str(p.element( ))) # use depth for indentation
4 for c in T.children(p):
5 preorder indent(T, c, d+1) # child depth is d+1
Code Fragment 8.23: Efficient recursion for printing indented version of a preorder
traversal. On a complete tree T, the recursion should be started with form
preorder indent(T, T.root( ), 0).
In the example of Figure 8.20, we were fortunate in that the numbering was
embedded within the elements of the tree. More generally, we might be interested
in using a preorder traversal to display the structure of a tree, with indentation and
also explicit numbering that was not present in the tree. For example, we might
display the tree from Figure 8.2 beginning as:
Electronics R Us
1 R&D
2 Sales
2.1 Domestic
2.2 International
2.2.1 Canada
2.2.2 S. America
This is more challenging, because the numbers used as labels are implicit in
the structure of the tree. A label depends on the index of each position, relative to
its siblings, along the path from the root to the current position. To accomplish the
task, we add a representation of that path as an additional parameter to the recursive
signature. Specifically, we use a list of zero-indexed numbers, one for each position
along the downward path, other than the root. (We convert those numbers to oneindexed
form when printing.)
At the implementation level, we wish to avoid the inefficiency of duplicating
such lists when sending a new parameter from one level of the recursion to the next.
A standard solution is to share the same list instance throughout the recursion. At
one level of the recursion, a new entry is temporarily added to the end of the list
before making further recursive calls. In order to  leave no trace,  that same block
of code must remove the extraneous entry from the list before completing its task.
An implementation based on this approach is given in Code Fragment 8.24.
 
8.4. Tree Traversal Algorithms 339
1 def preorder label(T, p, d, path):
2    Print labeled representation of subtree of T rooted at p at depth d.   
3 label = . .join(str(j+1) for j in path) # displayed labels are one-indexed
4 print(2 d + label, p.element())
5 path.append(0) # path entries are zero-indexed
6 for c in T.children(p):
7 preorder label(T, c, d+1, path) # child depth is d+1
8 path[ 1] += 1
9 path.pop()
Code Fragment 8.24: Efficient recursion for printing an indented and labeled presentation
of a preorder traversal.
Parenthetic Representations of a Tree
It is not possible to reconstruct a general tree, given only the preorder sequence
of elements, as in Figure 8.20(a). Some additional context is necessary for the
structure of the tree to be well defined. The use of indentation or numbered labels
provides such context, with a very human-friendly presentation. However, there
are more concise string representations of trees that are computer-friendly.
In this section, we explore one such representation. The parenthetic string
representation P(T) of tree T is recursively defined as follows. If T consists of a
single position p, then
P(T) = str(p.element()).
Otherwise, it is defined recursively as,
P(T) = str(p.element())+ ( +P(T1)+ , +       + , +P(Tk)+ )
where p is the root of T and T1,T2, . . . ,Tk are the subtrees rooted at the children
of p, which are given in order if T is an ordered tree. We are using  +  here to
denote string concatenation. As an example, the parenthetic representation of the
tree of Figure 8.2 would appear as follows (line breaks are cosmetic):
Electronics R Us (R&D, Sales (Domestic, International (Canada,
S. America, Overseas (Africa, Europe, Asia, Australia))),
Purchasing, Manufacturing (TV, CD, Tuner))
Although the parenthetic representation is essentially a preorder traversal, we
cannot easily produce the additional punctuation using the formal implementation
of preorder, as given in Code Fragment 8.17. The opening parenthesis must be
produced just before the loop over a position s children and the closing parenthesis
must be produced just after that loop. Furthermore, the separating commas must
be produced. The Python function parenthesize, shown in Code Fragment 8.25, is
a custom traversal that prints such a parenthetic string representation of a tree T.
 
340 Chapter 8. Trees
1 def parenthesize(T, p):
2    Print parenthesized representation of subtree of T rooted at p.   
3 print(p.element( ), end= ) # use of end avoids trailing newline
4 if not T.is leaf(p):
5 first time = True
6 for c in T.children(p):
7 sep = ( if first time else , # determine proper separator
8 print(sep, end= )
9 first time = False # any future passes will not be the first
10 parenthesize(T, c) # recur on child
11 print( ) , end= ) # include closing parenthesis
Code Fragment 8.25: Function that prints parenthetic string representation of a tree.
Computing Disk Space
In Example 8.1, we considered the use of a tree as a model for a file-system structure,
with internal positions representing directories and leaves representing files.
In fact, when introducing the use of recursion back in Chapter 4, we specifically
examined the topic of file systems (see Section 4.1.4). Although we did not explicitly
model it as a tree at that time, we gave an implementation of an algorithm for
computing the disk usage (Code Fragment 4.5).
The recursive computation of disk space is emblematic of a postorder traversal,
as we cannot effectively compute the total space used by a directory until after we
know the space that is used by its children directories. Unfortunately, the formal
implementation of postorder, as given in Code Fragment 8.19 does not suffice for
this purpose. As it visits the position of a directory, there is no easy way to discern
which of the previous positions represent children of that directory, nor how much
recursive disk space was allocated.
We would like to have a mechanism for children to return information to the
parent as part of the traversal process. A custom solution to the disk space problem,
with each level of recursion providing a return value to the (parent) caller, is
provided in Code Fragment 8.26.
1 def disk space(T, p):
2    Return total disk space for subtree of T rooted at p.   
3 subtotal = p.element( ).space( ) # space used at position p
4 for c in T.children(p):
5 subtotal += disk space(T, c) # add child s space to subtotal
6 return subtotal
Code Fragment 8.26: Recursive computation of disk space for a tree. We assume
that a space( ) method of each tree element reports the local space used at that
position.
 
8.4. Tree Traversal Algorithms 341
8.4.6 Euler Tours and the Template Method Pattern _
The various applications described in Section 8.4.5 demonstrate the great power
of recursive tree traversals. Unfortunately, they also show that the specific implementations
of the preorder and postorder methods of our Tree class, or the inorder
method of the BinaryTree class, are not general enough to capture the range of
computations we desire. In some cases, we need more of a blending of the approaches,
with initial work performed before recurring on subtrees, additional work
performed after those recursions, and in the case of a binary tree, work performed
between the two possible recursions. Furthermore, in some contexts it was important
to know the depth of a position, or the complete path from the root to that
position, or to return information from one level of the recursion to another. For
each of the previous applications, we were able to develop a custom implementation
to properly adapt the recursive ideas, but the great principles of object-oriented
programming introduced in Section 2.1.1 include adaptability and reusability.
In this section, we develop a more general framework for implementing tree
traversals based on a concept known as an Euler tour traversal. The Euler tour
traversal of a general tree T can be informally defined as a  walk  around T, where
we start by going from the root toward its leftmost child, viewing the edges of T as
being  walls  that we always keep to our left. (See Figure 8.21.)
3 1 9 5 7 4
  3   2 3  
      6
   
 
Figure 8.21: Euler tour traversal of a tree.
The complexity of the walk is O(n), because it progresses exactly two times
along each of the n 1 edges of the tree once going downward along the edge, and
later going upward along the edge. To unify the concept of preorder and postorder
traversals, we can think of there being two notable  visits  to each position p:
  A  pre visit  occurs when first reaching the position, that is, when the walk
passes immediately left of the node in our visualization.
  A  post visit  occurs when the walk later proceeds upward from that position,
that is, when the walk passes to the right of the node in our visualization.
 
342 Chapter 8. Trees
The process of an Euler tour can easily be viewed recursively. In between the
 pre visit  and  post visit  of a given position will be a recursive tour of each of
its subtrees. Looking at Figure 8.21 as an example, there is a contiguous portion
of the entire tour that is itself an Euler tour of the subtree of the node with element
 / . That tour contains two contiguous subtours, one traversing that position s left
subtree and another traversing the right subtree. The pseudo-code for an Euler tour
traversal of a subtree rooted at a position p is shown in Code Fragment 8.27.
Algorithm eulertour(T, p):
perform the  pre visit  action for position p
for each child c in T.children(p) do
eulertour(T, c) {recursively tour the subtree rooted at c}
perform the  post visit  action for position p
Code Fragment 8.27: Algorithm eulertour for performing an Euler tour traversal of
a subtree rooted at position p of a tree.
The Template Method Pattern
To provide a framework that is reusable and adaptable, we rely on an interesting
object-oriented software design pattern, the template method pattern. The template
method pattern describes a generic computation mechanism that can be specialized
for a particular application by redefining certain steps. To allow customization, the
primary algorithm calls auxiliary functions known as hooks at designated steps of
the process.
In the context of an Euler tour traversal, we define two separate hooks, a previsit
hook that is called before the subtrees are traversed, and a postvisit hook that is
called after the completion of the subtree traversals. Our implementation will take
the form of an EulerTour class that manages the process, and defines trivial definitions
for the hooks that do nothing. The traversal can be customized by defining
a subclass of EulerTour and overriding one or both hooks to provide specialized
behavior.
Python Implementation
Our implementation of an EulerTour class is provided in Code Fragment 8.28. The
primary recursive process is defined in the nonpublic tour method. A tour instance
is created by sending a reference to a specific tree to the constructor, and then by
calling the public execute method, which beings the tour and returns a final result
of the computation.
 
8.4. Tree Traversal Algorithms 343
1 class EulerTour:
2    Abstract base class for performing Euler tour of a tree.
3
4 hook previsit and hook postvisit may be overridden by subclasses.
5    
6 def init (self, tree):
7    Prepare an Euler tour template for given tree.   
8 self. tree = tree
9
10 def tree(self):
11    Return reference to the tree being traversed.   
12 return self. tree
13
14 def execute(self):
15    Perform the tour and return any result from post visit of root.   
16 if len(self. tree) > 0:
17 return self. tour(self. tree.root( ), 0, [ ]) # start the recursion
18
19 def tour(self, p, d, path):
20    Perform tour of subtree rooted at Position p.
21
22 p Position of current node being visited
23 d depth of p in the tree
24 path list of indices of children on path from root to p
25    
26 self. hook previsit(p, d, path) #  pre visit  p
27 results = [ ]
28 path.append(0) # add new index to end of path before recursion
29 for c in self. tree.children(p):
30 results.append(self. tour(c, d+1, path)) # recur on child s subtree
31 path[ 1] += 1 # increment index
32 path.pop( ) # remove extraneous index from end of path
33 answer = self. hook postvisit(p, d, path, results) #  post visit  p
34 return answer
35
36 def hook previsit(self, p, d, path): # can be overridden
37 pass
38
39 def hook postvisit(self, p, d, path, results): # can be overridden
40 pass
Code Fragment 8.28: An EulerTour base class providing a framework for performing
Euler tour traversals of a tree.
 
344 Chapter 8. Trees
Based on our experience of customizing traversals for sample applications Section
8.4.5, we build support into the primary EulerTour for maintaining the recursive
depth and the representation of the recursive path through a tree, using the
approach that we introduced in Code Fragment 8.24. We also provide a mechanism
for one recursive level to return a value to another when post-processing. Formally,
our framework relies on the following two hooks that can be specialized:
  method hook previsit(p, d, path)
This function is called once for each position, immediately before its subtrees
(if any) are traversed. Parameter p is a position in the tree, d is the depth of
that position, and path is a list of indices, using the convention described in
the discussion of Code Fragment 8.24. No return value is expected from this
function.
  method hook postvisit(p, d, path, results)
This function is called once for each position, immediately after its subtrees
(if any) are traversed. The first three parameters use the same convention as
did hook previsit. The final parameter is a list of objects that were provided
as return values from the post visits of the respective subtrees of p. Any value
returned by this call will be available to the parent of p during its postvisit.
For more complex tasks, subclasses of EulerTour may also choose to initialize
and maintain additional state in the form of instance variables that can be accessed
within the bodies of the hooks.
Using the Euler Tour Framework
To demonstrate the flexibility of our Euler tour framework, we revisit the sample
applications from Section 8.4.5. As a simple example, an indented preorder traversal,
akin to that originally produced by Code Fragment 8.23, can be generated with
the simple subclass given in Code Fragment 8.29.
1 class PreorderPrintIndentedTour(EulerTour):
2 def hook previsit(self, p, d, path):
3 print(2 d + str(p.element( )))
Code Fragment 8.29: A subclass of EulerTour that produces an indented preorder
list of a tree s elements.
Such a tour would be started by creating an instance of the subclass for a given
tree T, and invoking its execute method. This could be expressed as follows:
tour = PreorderPrintIndentedTour(T)
tour.execute( )
 
8.4. Tree Traversal Algorithms 345
A labeled version of an indented, preorder presentation, akin to Code Fragment
8.24, could be generated by the new subclass of EulerTour shown in Code
Fragment 8.30.
1 class PreorderPrintIndentedLabeledTour(EulerTour):
2 def hook previsit(self, p, d, path):
3 label = . .join(str(j+1) for j in path) # labels are one-indexed
4 print(2 d + label, p.element())
Code Fragment 8.30: A subclass of EulerTour that produces a labeled and indented,
preorder list of a tree s elements.
To produce the parenthetic string representation, originally achieved with Code
Fragment 8.25, we define a subclass that overrides both the previsit and postvisit
hooks. Our new implementation is given in Code Fragment 8.31.
1 class ParenthesizeTour(EulerTour):
2 def hook previsit(self, p, d, path):
3 if path and path[ 1] > 0: # p follows a sibling
4 print( , , end= ) # so preface with comma
5 print(p.element( ), end= ) # then print element
6 if not self.tree( ).is leaf(p): # if p has children
7 print( ( , end= ) # print opening parenthesis
8
9 def hook postvisit(self, p, d, path, results):
10 if not self.tree( ).is leaf(p): # if p has children
11 print( ) , end= ) # print closing parenthesis
Code Fragment 8.31: A subclass of EulerTour that prints a parenthetic string representation
of a tree.
Notice that in this implementation, we need to invoke a method on the tree instance
that is being traversed from within the hooks. The public tree() method of the
EulerTour class serves as an accessor for that tree.
Finally, the task of computing disk space, as originally implemented in Code
Fragment 8.26, can be performed quite easily with the EulerTour subclass shown
in Code Fragment 8.32. The postvisit result of the root will be returned by the call
to execute( ).
1 class DiskSpaceTour(EulerTour):
2 def hook postvisit(self, p, d, path, results):
3 # we simply add space associated with p to that of its subtrees
4 return p.element( ).space( ) + sum(results)
Code Fragment 8.32: A subclass of EulerTour that computes disk space for a tree.
 
346 Chapter 8. Trees
The Euler Tour Traversal of a Binary Tree
In Section 8.4.6, we introduced the concept of an Euler tour traversal of a general
graph, using the template method pattern in designing the EulerTour class. That
class provided methods hook previsit and hook postvisit that could be overridden
to customize a tour. In Code Fragment 8.33 we provide a BinaryEulerTour
specialization that includes an additional hook invisit that is called once for each
position after its left subtree is traversed, but before its right subtree is traversed.
Our implementation of BinaryEulerTour replaces the original tour utility to
specialize to the case in which a node has at most two children. If a node has only
one child, a tour differentiates between whether that is a left child or a right child,
with the  in visit  taking place after the visit of a sole left child, but before the visit
of a sole right child. In the case of a leaf, the three hooks are called in succession.
1 class BinaryEulerTour(EulerTour):
2    Abstract base class for performing Euler tour of a binary tree.
3
4 This version includes an additional hook invisit that is called after the tour
5 of the left subtree (if any), yet before the tour of the right subtree (if any).
6
7 Note: Right child is always assigned index 1 in path, even if no left sibling.
8    
9 def tour(self, p, d, path):
10 results = [None, None] # will update with results of recursions
11 self. hook previsit(p, d, path) #  pre visit  for p
12 if self. tree.left(p) is not None: # consider left child
13 path.append(0)
14 results[0] = self. tour(self. tree.left(p), d+1, path)
15 path.pop( )
16 self. hook invisit(p, d, path) #  in visit  for p
17 if self. tree.right(p) is not None: # consider right child
18 path.append(1)
19 results[1] = self. tour(self. tree.right(p), d+1, path)
20 path.pop( )
21 answer = self. hook postvisit(p, d, path, results) #  post visit  p
22 return answer
23
24 def hook invisit(self, p, d, path): pass # can be overridden
Code Fragment 8.33: A BinaryEulerTour base class providing a specialized tour for
binary trees. The original EulerTour base class was given in Code Fragment 8.28.
 
8.4. Tree Traversal Algorithms 347
3
2
1
0
0 1 2 3 4 5 6 7 8 9 10 11 12
4
Figure 8.22: An inorder drawing of a binary tree.
To demonstrate use of the BinaryEulerTour framework, we develop a subclass
that computes a graphical layout of a binary tree, as shown in Figure 8.22. The
geometry is determined by an algorithm that assigns x- and y-coordinates to each
position p of a binary tree T using the following two rules:
  x(p) is the number of positions visited before p in an inorder traversal of T.
  y(p) is the depth of p in T.
In this application, we take the convention common in computer graphics that xcoordinates
increase left to right and y-coordinates increase top to bottom. So the
origin is in the upper left corner of the computer screen.
Code Fragment 8.34 provides an implementation of a BinaryLayout subclass
that implements the above algorithm for assigning (x,y) coordinates to the element
stored at each position of a binary tree. We adapt the BinaryEulerTour framework
by introducing additional state in the form of a count instance variable that represents
the number of  in visits  that we have performed. The x-coordinate for each
position is set according to that counter.
1 class BinaryLayout(BinaryEulerTour):
2    Class for computing (x,y) coordinates for each node of a binary tree.   
3 def init (self, tree):
4 super(). init (tree) # must call the parent constructor
5 self. count = 0 # initialize count of processed nodes
6
7 def hook invisit(self, p, d, path):
8 p.element( ).setX(self. count) # x-coordinate serialized by count
9 p.element( ).setY(d) # y-coordinate is depth
10 self. count += 1 # advance count of processed nodes
Code Fragment 8.34: A BinaryLayout class that computes coordinates at which to
draw positions of a binary tree. We assume that the element type for the original
tree supports setX and setY methods.
 
348 Chapter 8. Trees
8.5 Case Study: An Expression Tree
In Example 8.7, we introduced the use of a binary tree to represent the structure of
an arithmetic expression. In this section, we define a new ExpressionTree class that
provides support for constructing such trees, and for displaying and evaluating the
arithmetic expression that such a tree represents. Our ExpressionTree class is defined
as a subclass of LinkedBinaryTree, and we rely on the nonpublic mutators to
construct such trees. Each internal node must store a string that defines a binary operator
(e.g., + ), and each leaf must store a numeric value (or a string representing
a numeric value).
Our eventual goal is to build arbitrarily complex expression trees for compound
arithmetic expressions such as (((3+1) 4)/((9 5)+2)). However, it suffices
for the ExpressionTree class to support two basic forms of initialization:
ExpressionTree(value): Create a tree storing the given value at the root.
ExpressionTree(op,E1,E2): Create a tree storing string op at the root (e.g., +),
and with the structures of existing ExpressionTree
instances E1 and E2 as the left and right subtrees of
the root, respectively.
Such a constructor for the ExpressionTree class is given in Code Fragment 8.35.
The class formally inherits from LinkedBinaryTree, so it has access to all the nonpublic
update methods that were defined in Section 8.3.1. We use add root to create
an initial root of the tree storing the token provided as the first parameter. Then
we perform run-time checking of the parameters to determine whether the caller
invoked the one-parameter version of the constructor (in which case, we are done),
or the three-parameter form. In that case, we use the inherited attach method to
incorporate the structure of the existing trees as subtrees of the root.
Composing a Parenthesized String Representation
A string representation of an existing expression tree instance, for example, as
(((3+1)x4)/((9-5)+2)) , can be produced by displaying tree elements using
an inorder traversal, but with opening and closing parentheses inserted with
a preorder and postorder step, respectively. In the context of an ExpressionTree
class, we support a special str method (see Section 2.3.2) that returns the
appropriate string. Because it is more efficient to first build a sequence of individual
strings to be joined together (see discussion of  Composing Strings  in Section
5.4.2), the implementation of str relies on a nonpublic, recursive method
named parenthesize recur that appends a series of strings to a list. These methods
are included in Code 8.35.
 
8.5. Case Study: An Expression Tree 349
1 class ExpressionTree(LinkedBinaryTree):
2    An arithmetic expression tree.   
3
4 def init (self, token, left=None, right=None):
5    Create an expression tree.
6
7 In a single parameter form, token should be a leaf value (e.g., 42 ),
8 and the expression tree will have that value at an isolated node.
9
10 In a three-parameter version, token should be an operator,
11 and left and right should be existing ExpressionTree instances
12 that become the operands for the binary operator.
13    
14 super(). init ( ) # LinkedBinaryTree initialization
15 if not isinstance(token, str):
16 raise TypeError( Token must be a string )
17 self. add root(token) # use inherited, nonpublic method
18 if left is not None: # presumably three-parameter form
19 if token not in +-*x/ :
20 raise ValueError( token must be valid operator )
21 self. attach(self.root( ), left, right) # use inherited, nonpublic method
22
23 def str (self):
24    Return string representation of the expression.   
25 pieces = [ ] # sequence of piecewise strings to compose
26 self. parenthesize recur(self.root( ), pieces)
27 return .join(pieces)
28
29 def parenthesize recur(self, p, result):
30    Append piecewise representation of p s subtree to resulting list.   
31 if self.is leaf(p):
32 result.append(str(p.element( ))) # leaf value as a string
33 else:
34 result.append( ( ) # opening parenthesis
35 self. parenthesize recur(self.left(p), result) # left subtree
36 result.append(p.element( )) # operator
37 self. parenthesize recur(self.right(p), result) # right subtree
38 result.append( ) ) # closing parenthesis
Code Fragment 8.35: The beginning of an ExpressionTree class.
 
350 Chapter 8. Trees
Expression Tree Evaluation
The numeric evaluation of an expression tree can be accomplished with a simple
application of a postorder traversal. If we know the values represented by the two
subtrees of an internal position, we can calculate the result of the computation that
position designates. Pseudo-code for the recursive evaluation of the value represented
by a subtree rooted at position p is given in Code Fragment 8.36.
Algorithm evaluate recur(p):
if p is a leaf then
return the value stored at p
else
let   be the operator stored at p
x = evaluate recur(left(p))
y = evaluate recur(right(p))
return x   y
Code Fragment 8.36: Algorithm evaluate recur for evaluating the expression represented
by a subtree of an arithmetic expression tree rooted at position p.
To implement this algorithm in the context of a Python ExpressionTree class,
we provide a public evaluate method that is invoked on instance T as T.evaluate( ).
Code Fragment 8.37 provides such an implementation, relying on a nonpublic
evaluate recur method that computes the value of a designated subtree.
39 def evaluate(self):
40    Return the numeric result of the expression.   
41 return self. evaluate recur(self.root( ))
42
43 def evaluate recur(self, p):
44    Return the numeric result of subtree rooted at p.   
45 if self.is leaf(p):
46 return float(p.element( )) # we assume element is numeric
47 else:
48 op = p.element( )
49 left val = self. evaluate recur(self.left(p))
50 right val = self. evaluate recur(self.right(p))
51 if op == + : return left val + right val
52 elif op == - : return left val   right val
53 elif op == / : return left val / right val
54 else: return left val right val # treat x or as multiplication
Code Fragment 8.37: Support for evaluating an ExpressionTree instance.
 
8.5. Case Study: An Expression Tree 351
Building an Expression Tree
The constructor for the ExpressionTree class, from Code Fragment 8.35, provides
basic functionality for combining existing trees to build larger expression trees.
However, the question still remains how to construct a tree that represents an expression
for a given string, such as (((3+1)x4)/((9-5)+2)) .
To automate this process, we rely on a bottom-up construction algorithm, assuming
that a string can first be tokenized so that multidigit numbers are treated
atomically (see Exercise R-8.30), and that the expression is fully parenthesized.
The algorithm uses a stack S while scanning tokens of the input expression E to
find values, operators, and right parentheses. (Left parentheses are ignored.)
  When we see an operator  , we push that string on the stack.
  When we see a literal value v, we create a single-node expression tree T
storing v, and push T on the stack.
  When we see a right parenthesis, ) , we pop the top three items from the
stack S, which represent a subexpression (E1   E2). We then construct a
tree T using trees for E1 and E2 as subtrees of the root storing  , and push
the resulting tree T back on the stack.
We repeat this until the expression E has been processed, at which time the top
element on the stack is the expression tree for E. The total running time is O(n).
An implementation of this algorithm is given in Code Fragment 8.38 in the form
of a stand-alone function named build expression tree, which produces and returns
an appropriate ExpressionTree instance, assuming the input has been tokenized.
1 def build expression tree(tokens):
2    Returns an ExpressionTree based upon by a tokenized expression.   
3 S = [ ] # we use Python list as stack
4 for t in tokens:
5 if t in +-x*/ : # t is an operator symbol
6 S.append(t) # push the operator symbol
In Chapter 8 we introduced the tree data structure and demonstrated a variety of
applications. One important use is as a search tree (as described on page 332). In
this chapter, we use a search tree structure to efficiently implement a sorted map.
The three most fundamental methods of a map M (see Section 10.1.1) are:
M[k]: Return the value v associated with key k in map M, if one exists;
otherwise raise a KeyError; implemented with getitem method.
M[k] = v: Associate value v with key k in map M, replacing the existing value
if the map already contains an item with key equal to k; implemented
with setitem method.
del M[k]: Remove from map M the item with key equal to k; if M has no such
item, then raise a KeyError; implemented with delitem method.
The sorted map ADT includes additional functionality (see Section 10.3), guaranteeing
that an iteration reports keys in sorted order, and supporting additional
searches such as find gt(k) and find range(start, stop).
Binary trees are an excellent data structure for storing items of a map, assuming
we have an order relation defined on the keys. In this context, a binary search tree
is a binary tree T with each position p storing a key-value pair (k,v) such that:
  Keys stored in the left subtree of p are less than k.
  Keys stored in the right subtree of p are greater than k.
An example of such a binary search tree is given in Figure 11.1. As a matter of
convenience, we will not diagram the values associated with keys in this chapter,
since those values do not affect the placement of items within a search tree.
Figure 11.1: A binary search tree with integer keys. We omit the display of associated
values in this chapter, since they are not relevant to the order of items within a
search tree.
. Binary Search Trees 
Navigating a Binary Search Tree
We begin by demonstrating that a binary search tree hierarchically represents the
sorted order of its keys. In particular, the structural property regarding the placement
of keys within a binary search tree assures the following important consequence
regarding an inorder traversal (Section 8.4.3) of the tree.
Proposition 11.1: An inorder traversal of a binary search tree visits positions in
increasing order of their keys.
Justification: We prove this by induction on the size of a subtree. If a subtree
has at most one item, its keys are trivially visited in order. More generally, an
inorder traversal of a (sub)tree consists of a recursive traversal of the (possibly
empty) left subtree, followed by a visit of the root, and then a recursive traversal of
the (possibly empty) right subtree. By induction, a recursive inorder traversal of the
left subtree will produce an iteration of the keys in that subtree in increasing order.
Furthermore, by the binary search tree property, all keys in the left subtree have
keys strictly smaller than that of the root. Therefore, visiting the root just after that
subtree extends the increasing order of keys. Finally, by the search tree property,
all keys in the right subtree are strictly greater than the root, and by induction, an
inorder traversal of that subtree will visit those keys in increasing order.
Since an inorder traversal can be executed in linear time, a consequence of this
proposition is that we can produce a sorted iteration of the keys of a map in linear
time, when represented as a binary search tree.
Although an inorder traversal is typically expressed using a top-down recursion,
we can provide nonrecursive descriptions of operations that allow more finegrained
navigation among the positions of a binary search relative to the order of
their keys. Our generic binary tree ADT from Chapter 8 is defined as a positional
structure, allowing direct navigation using methods such as parent(p), left(p), and
right(p). With a binary search tree, we can provide additional navigation based on
the natural order of the keys stored in the tree. In particular, we can support the
following methods, akin to those provided by a PositionalList (Section 7.4.1).
first( ): Return the position containing the least key, or None if the tree is empty.
last( ): Return the position containing the greatest key, or None if empty tree.
before(p): Return the position containing the greatest key that is less than that of
position p (i.e., the position that would be visited immediately before p
in an inorder traversal), or None if p is the first position.
after(p): Return the position containing the least key that is greater than that of
position p (i.e., the position that would be visited immediately after p
in an inorder traversal), or None if p is the last position.
Chapter 11. Search Trees
The  first  position of a binary search tree can be located by starting a walk at
the root and continuing to the left child, as long as a left child exists. By symmetry,
the last position is reached by repeated steps rightward starting at the root.
The successor of a position, after(p), is determined by the following algorithm.
Algorithm after(p):
if right(p) is not None then {successor is leftmost position in p s right subtree}
Code Fragment 11.1: Computing the successor of a position in a binary search tree.
The rationale for this process is based purely on the workings of an inorder
traversal, given the correspondence of Proposition 11.1. If p has a right subtree,
that right subtree is recursively traversed immediately after p is visited, and so the
first position to be visited after p is the leftmost position within the right subtree.
If p does not have a right subtree, then the flow of control of an inorder traversal
returns to p s parent. If p were in the right subtree of that parent, then the parent s
subtree traversal is complete and the flow of control progresses to its parent and
so on. Once an ancestor is reached in which the recursion is returning from its
left subtree, then that ancestor becomes the next position visited by the inorder
traversal, and thus is the successor of p. Notice that the only case in which no such
ancestor is found is when p was the rightmost (last) position of the full tree, in
which case there is no successor.
A symmetric algorithm can be defined to determine the predecessor of a position,
before(p). At this point, we note that the running time of single call to
after(p) or before(p) is bounded by the height h of the full tree, because it is found
after either a single downward walk or a single upward walk. While the worst-case
running time is O(h), we note that either of these methods run in O(1) amortized
time, in that series of n calls to after(p) starting at the first position will execute in a
total of O(n) time. We leave a formal justification of this fact to Exercise C-11.34,
but intuitively the upward and downward paths mimic steps of the inorder traversal
(a related argument was made in the justification of Proposition 9.3).
 
11.1. Binary Search Trees 463
11.1.2 Searches
The most important consequence of the structural property of a binary search tree
is its namesake search algorithm. We can attempt to locate a particular key in a
binary search tree by viewing it as a decision tree (recall Figure 8.7). In this case,
the question asked at each position p is whether the desired key k is less than, equal
to, or greater than the key stored at position p, which we denote as p.key( ). If the
answer is  less than,  then the search continues in the left subtree. If the answer
is  equal,  then the search terminates successfully. If the answer is  greater than, 
then the search continues in the right subtree. Finally, if we reach an empty subtree,
then the search terminates unsuccessfully. (See Figure 11.2.)
Figure 11.2: (a) A successful search for key 65 in a binary search tree; (b) an
unsuccessful search for key 68 that terminates because there is no subtree to the
left of the key 76.
We describe this approach in Code Fragment 11.2. If key k occurs in a subtree
rooted at p, a call to TreeSearch(T, p, k) results in the position at which the key
is found; in this case, the getitem map operation would return the associated
value at that position. In the event of an unsuccessful search, the TreeSearch algorithm
returns the final position explored on the search path (which we will later
make use of when determining where to insert a new item in a search tree).
464 Chapter 11. Search Trees
Analysis of Binary Tree Searching
The analysis of the worst-case running time of searching in a binary search tree
T is simple. Algorithm TreeSearch is recursive and executes a constant number
of primitive operations for each recursive call. Each recursive call of TreeSearch
is made on a child of the previous position. That is, TreeSearch is called on the
positions of a path of T that starts at the root and goes down one level at a time.
Thus, the number of such positions is bounded by h+1, where h is the height of T.
In other words, since we spend O(1) time per position encountered in the search,
the overall search runs in O(h) time, where h is the height of the binary search
tree T. (See Figure 11.3.)
Figure 11.3: Illustrating the running time of searching in a binary search tree. The
figure uses standard caricature of a binary search tree as a big triangle and a path
from the root as a zig-zag line.
In the context of the sorted map ADT, the search will be used as a subroutine
for implementing the getitem method, as well as for the setitem and
delitem methods, since each of these begins by trying to locate an existing
item with a given key. To implement sorted map operations such as find lt and
find gt, we will combine this search with traversal methods before and after. All
of these operations will run in worst-case O(h) time for a tree with height h. We
can use a variation of this technique to implement the find range method in time
O(s+h), where s is the number of items reported (see Exercise C-11.34).
Admittedly, the height h of T can be as large as the number of entries, n, but we
expect that it is usually much smaller. Indeed, later in this chapter we show various
strategies to maintain an upper bound of O(logn) on the height of a search tree T.
11.1. Binary Search Trees 465
11.1.3 Insertions and Deletions
Algorithms for inserting or deleting entries of a binary search tree are fairly straightforward,
although not trivial.
Insertion
The map command M[k] = v, as supported by the setitem method, begins
with a search for key k (assuming the map is nonempty). If found, that item s
existing value is reassigned. Otherwise, a node for the new item can be inserted
into the underlying tree T in place of the empty subtree that was reached at the end
of the failed search. The binary search tree property is sustained by that placement
(note that it is placed exactly where a search would expect it). Pseudo-code for
such a TreeInsert algorithm is given in in Code Fragment 11.3.
Algorithm TreeInsert(T, k, v):
Input: A search key k to be associated with value v
p = TreeSearch(T,T.root(),k)
if k == p.key() then
Set p s value to v
else if k < p.key() then
add node with item (k,v) as left child of p
else
add node with item (k,v) as right child of p
Code Fragment 11.3: Algorithm for inserting a key-value pair into a map that is
represented as a binary search tree.
An example of insertion into a binary search tree is shown in Figure 11.4.
Figure 11.4: Insertion of an item with key 68 into the search tree of Figure 11.2.
Finding the position to insert is shown in (a), and the resulting tree is shown in (b).
466 Chapter 11. Search Trees
Deletion
Deleting an item from a binary search tree T is a bit more complex than inserting
a new item because the location of the deletion might be anywhere in the tree. (In
contrast, insertions are always enacted at the bottom of a path.) To delete an item
with key k, we begin by calling TreeSearch(T, T.root( ), k) to find the position p
of T storing an item with key equal to k. If the search is successful, we distinguish
between two cases (of increasing difficulty):
  If p has at most one child, the deletion of the node at position p is easily
implemented. When introducing update methods for the LinkedBinaryTree
class in Section 8.3.1, we declared a nonpublic utility, delete(p), that deletes
a node at position p and replaces it with its child (if any), presuming that p has
at most one child. That is precisely the desired behavior. It removes the item
with key k from the map while maintaining all other ancestor-descendant
relationships in the tree, thereby assuring the upkeep of the binary search
tree property. (See Figure 11.5.)
  If position p has two children, we cannot simply remove the node from T
since this would create a  hole  and two orphaned children. Instead, we
proceed as follows (see Figure 11.6):
  We locate position r containing the item having the greatest key that is
strictly less than that of position p, that is,r = before(p) by the notation
of Section 11.1.1. Because p has two children, its predecessor is the
rightmost position of the left subtree of p.
  We use r s item as a replacement for the one being deleted at position p.
Because r has the immediately preceding key in the map, any items in
p s right subtree will have keys greater than r and any other items in p s
left subtree will have keys less than r. Therefore, the binary search tree
property is satisfied after the replacement.
  Having used r s as a replacement for p, we instead delete the node at
position r from the tree. Fortunately, since r was located as the rightmost
position in a subtree, r does not have a right child. Therefore, its
deletion can be performed using the first (and simpler) approach.
As with searching and insertion, this algorithm for a deletion involves the
traversal of a single path downward from the root, possibly moving an item between
two positions of this path, and removing a node from that path and promoting its
child. Therefore, it executes in time O(h) where h is the height of the tree.
 
11.1. Binary Search Trees 467
Figure 11.5: Deletion from the binary search tree of Figure 11.4b, where the item
to delete (with key 32) is stored at a position p with one child r : (a) before the
deletion; (b) after the deletion.
Figure 11.6: Deletion from the binary search tree of Figure 11.5b, where the item
to delete (with key 88) is stored at a position p with two children, and replaced by
its predecessor r : (a) before the deletion; (b) after the deletion.
468 Chapter 11. Search Trees
11.1.4 Python Implementation
In Code Fragments 11.4 through 11.8 we define a TreeMap class that implements
the sorted map ADT using a binary search tree. In fact, our implementation is more
general. We support all of the standard map operations (Section 10.1.1), all additional
sorted map operations (Section 10.3), and positional operations including
first( ), last( ), find position(k), before(p), after(p), and delete(p).
Our TreeMap class takes advantage of multiple inheritance for code reuse,
inheriting from the LinkedBinaryTree class of Section 8.3.1 for our representation
as a positional binary tree, and from the MapBase class from Code Fragment 10.2
of Section 10.1.4 to provide us with the key-value composite item and the concrete
behaviors from the collections.MutableMapping abstract base class. We subclass
the nested Position class to support more specific p.key( ) and p.value( ) accessors
for our map, rather than the p.element( ) syntax inherited from the tree ADT.
We define several nonpublic utilities, most notably a subtree search(p, k)
method that corresponds to the TreeSearch algorithm of Code Fragment 11.2. That
returns a position, ideally one that contains the key k, or otherwise the last position
that is visited on the search path. We rely on the fact that the final position during
an unsuccessful search is either the nearest key less than k or the nearest key
greater than k. This search utility becomes the basis for the public find position(k)
method, and also for internal use when searching, inserting, or deleting items from
a map, as well as for the robust searches of the sorted map ADT.
When making structural modifications to the tree, we rely on nonpublic update
methods, such as add right, that are inherited from the LinkedBinaryTree class
(see Section 8.3.1). It is important that these inherited methods remain nonpublic,
as the search tree property could be violated through misuse of such operations.
Finally, we note that our code is peppered with calls to presumed methods
named rebalance insert, rebalance delete, and rebalance access. These methods
serve as hooks for future use when balancing search trees; we discuss them in
Section 11.2. We conclude with a brief guide to the organization of our code.
Code Fragment 11.4: Beginning of TreeMap class including redefined Position
class and nonpublic search utilities.
Code Fragment 11.5: Positional methods first( ), last( ), before(p), after(p),
and find position(p) accessor.
Code Fragment 11.6: Selected methods of the sorted map ADT: find min( ),
find ge(k), and find range(start, stop); related methods
are omitted for the sake of brevity.
Code Fragment 11.7: getitem (k), setitem (k, v), and iter ().
Code Fragment 11.8: Deletion either by position, as delete(p), or by key, as
delitem (k).
11.1.5 Performance of a Binary Search Tree
An analysis of the operations of our TreeMap class is given in Table 11.1. Almost
all operations have a worst-case running time that depends on h, where h is the
height of the current tree. This is because most operations rely on a constant amount
of work for each node along a particular path of the tree, and the maximum path
length within a tree is proportional to the height of the tree. Most notably, our
implementations of map operations getitem , setitem , and delitem
each begin with a call to the subtree search utility which traces a path downward
from the root of the tree, using O(1) time at each node to determine how to continue
the search. Similar paths are traced when looking for a replacement during a
deletion, or when computing a position s inorder predecessor or successor. We note
that although a single call to the after method has worst-case running time of O(h),
the n successive calls made during a call to iter require a total of O(n) time,
since each edge is traced at most twice; in a sense, those calls have O(1) amortized
time bounds. A similar argument can be used to prove the O(s+h) worst-case
bound for a call to find range that reports s results (see Exercise C-11.34).
474 Chapter 11. Search Trees
Table 11.1: Worst-case running times of the operations for a TreeMap T. We denote
the current height of the tree with h, and the number of items reported by find range
as s. The space usage is O(n), where n is the number of items stored in the map.
A binary search tree T is therefore an efficient implementation of a map with n
entries only if its height is small. In the best case, T has height h=log(n+1)_ 1,
which yields logarithmic-time performance for all the map operations. In the worst
case, however, T has height n, in which case it would look and feel like an ordered
list implementation of a map. Such a worst-case configuration arises, for example,
if we insert items with keys in increasing or decreasing order. 
Figure 11.7: Example of a binary search tree with linear height, obtained by inserting
entries with keys in increasing order.
We can nevertheless take comfort that, on average, a binary search tree with
n keys generated from a random series of insertions and removals of keys has expected
height O(logn); the justification of this statement is beyond the scope of the
book, requiring careful mathematical language to precisely define what we mean
by a random series of insertions and removals, and sophisticated probability theory.
In applications where one cannot guarantee the random nature of updates, it
is better to rely on variations of search trees, presented in the remainder of this
chapter, that guarantee a worst-case height of O(logn), and thus O(logn) worstcase
time for searches, insertions, and deletions.
11.2 Balanced Search Trees
In the closing of the previous section, we noted that if we could assume a random
series of insertions and removals, the standard binary search tree supports O(logn)
expected running times for the basic map operations. However, we may only claim
O(n) worst-case time, because some sequences of operations may lead to an unbalanced
tree with height proportional to n.
In the remainder of this chapter, we explore four search tree algorithms that
provide stronger performance guarantees. Three of the four data structures (AVL
trees, splay trees, and red-black trees) are based on augmenting a standard binary
search tree with occasional operations to reshape the tree and reduce its height.
The primary operation to rebalance a binary search tree is known as a rotation.
During a rotation, we  rotate  a child to be above its parent, as diagrammed in
Figure 11.8: A rotation operation in a binary search tree. A rotation can be performed
to transform the left formation into the right, or the right formation into the
left. Note that all keys in subtree T1 have keys less than that of position x, all keys
in subtree T2 have keys that are between those of positions x and y, and all keys in
subtree T3 have keys that are greater than that of position y.
To maintain the binary search tree property through a rotation, we note that
if position x was a left child of position y prior to a rotation (and therefore the
key of x is less than the key of y), then y becomes the right child of x after the
rotation, and vice versa. Furthermore, we must relink the subtree of items with
keys that lie between the keys of the two positions that are being rotated. For
example, in Figure 11.8 the subtree labeled T2 represents items with keys that are
known to be greater than that of position x and less than that of position y. In the
first configuration of that figure, T2 is the right subtree of position x; in the second
configuration, it is the left subtree of position y.
Because a single rotation modifies a constant number of parent-child relationships,
it can be implemented in O(1) time with a linked binary tree representation.
 
476 Chapter 11. Search Trees
In the context of a tree-balancing algorithm, a rotation allows the shape of a
tree to be modified while maintaining the search tree property. If used wisely, this
operation can be performed to avoid highly unbalanced tree configurations. For
example, a rightward rotation from the first formation of Figure 11.8 to the second
reduces the depth of each node in subtree T1 by one, while increasing the depth
of each node in subtree T3 by one. (Note that the depth of nodes in subtree T2 are
unaffected by the rotation.)
One or more rotations can be combined to provide broader rebalancing within a
tree. One such compound operation we consider is a trinode restructuring. For this
manipulation, we consider a position x, its parent y, and its grandparent z. The goal
is to restructure the subtree rooted at z in order to reduce the overall path length
to x and its subtrees. Pseudo-code for a restructure(x) method is given in Code
Fragment 11.9 and illustrated in Figure 11.9. In describing a trinode restructuring,
we temporarily rename the positions x, y, and z as a, b, and c, so that a precedes b
and b precedes c in an inorder traversal of T. There are four possible orientations
mapping x, y, and z to a, b, and c, as shown in Figure 11.9, which are unified
into one case by our relabeling. The trinode restructuring replaces z with the node
identified as b, makes the children of this node be a and c, and makes the children
of a and c be the four previous children of x, y, and z (other than x and y), while
maintaining the inorder relationships of all the nodes in T.
Algorithm restructure(x):
Input: A position x of a binary search tree T that has both a parent y and a
grandparent z
Output: Tree T after a trinode restructuring (which corresponds to a single or
double rotation) involving positions x, y, and z
1: Let (a, b, c) be a left-to-right (inorder) listing of the positions x, y, and z, and
let (T1,T2,T3,T4) be a left-to-right (inorder) listing of the four subtrees of x,
y, and z not rooted at x, y, or z.
2: Replace the subtree rooted at z with a new subtree rooted at b.
3: Let a be the left child of b and let T1 and T2 be the left and right subtrees of a,
respectively.
4: Let c be the right child of b and let T3 and T4 be the left and right subtrees of
c, respectively.
Code Fragment 11.9: The trinode restructuring operation in a binary search tree.
In practice, the modification of a tree T caused by a trinode restructuring operation
can be implemented through case analysis either as a single rotation (as in
Figure 11.9a and b) or as a double rotation (as in Figure 11.9c and d). The double
rotation arises when position x has the middle of the three relevant keys and is first
rotated above its parent, and then above what was originally its grandparent. In any
of the cases, the trinode restructuring is completed with O(1) running time.
11.2. Balanced Search Trees 477
Figure 11.9: Schematic illustration of a trinode restructuring operation: (a and b)
require a single rotation; (c and d) require a double rotation.
 
478 Chapter 11. Search Trees
11.2.1 Python Framework for Balancing Search Trees
Our TreeMap class, introduced in Section 11.1.4, is a concrete map implementation
that does not perform any explicit balancing operations. However, we designed
that class to also serve as a base class for other subclasses that implement more
advanced tree-balancing algorithms. A summary of our inheritance hierarchy is
shown in Figure 11.10.
Hooks for Rebalancing Operations
Our implementation of the basic map operations in Section 11.1.4 includes strategic
calls to three nonpublic methods that serve as hooks for rebalancing algorithms:
  A call to rebalance insert(p) is made from within the setitem method
immediately after a new node is added to the tree at position p.
  A call to rebalance delete(p) is made each time a node has been deleted
from the tree, with position p identifying the parent of the node that has just
been removed. Formally, this hook is called from within the public delete(p)
method, which is indirectly invoked by the public delitem (k) behavior.
  We also provide a hook, rebalance access(p), that is called when an item at
position p of a tree is accessed through a public method such as getitem .
This hook is used by the splay tree structure (see Section 11.4) to restructure
a tree so that more frequently accessed items are brought closer to the root.
We provide trivial declarations of these three methods, in Code Fragment 11.10,
having bodies that do nothing (using the pass statement). A subclass of TreeMap
may override any of these methods to implement a nontrivial action to rebalance
a tree. This is another example of the template method design pattern, as seen in
11.2. Balanced Search Trees 479
174 def rebalance insert(self, p): pass
175 def rebalance delete(self, p): pass
176 def rebalance access(self, p): pass
Code Fragment 11.10: Additional code for the TreeMap class (continued from Code
Fragment 11.8), providing stubs for the rebalancing hooks.
Nonpublic Methods for Rotating and Restructuring
A second form of support for balanced search trees is our inclusion of nonpublic
utility methods rotate and restructure that, respectively, implement a single
rotation and a trinode restructuring (described at the beginning of Section 11.2).
Although these methods are not invoked by the public TreeMap operations, we
promote code reuse by providing these implementation in this class so that they are
inherited by all balanced-tree subclasses.
Our implementations are provided in Code Fragment 11.11. To simplify the
code, we define an additional relink utility that properly links parent and child
nodes to each other, including the special case in which a  child  is a None reference.
The focus of the rotate method then becomes redefining the relationship
between the parent and child, relinking a rotated node directly to its original grandparent,
and shifting the  middle  subtree (that labeled as T2 in Figure 11.8) between
the rotated nodes. For the trinode restructuring, we determine whether to perform
a single or double rotation, as originally described in Figure 11.9.
Factory for Creating Tree Nodes
We draw attention to an important subtlety in the design of both our TreeMap class
and the original LinkedBinaryTree subclass. The low-level definition of a node is
provided by the nested Node class within LinkedBinaryTree. Yet, several of our
tree-balancing strategies require that auxiliary information be stored at each node
to guide the balancing process. Those classes will override the nested Node class
to provide storage for an additional field.
Whenever we add a new node to the tree, as within the add right method of
the LinkedBinaryTree (originally given in Code Fragment 8.10), we intentionally
instantiate the node using the syntax self. Node, rather than the qualified name
LinkedBinaryTree. Node. This is vital to our framework! When the expression
self. Node is applied to an instance of a tree (sub)class, Python s name resolution
follows the inheritance structure (as described in Section 2.5.2). If a subclass has
overridden the definition for the Node class, instantiation of self. Node relies on
the newly defined node class. This technique is an example of the factory method
design pattern, as we provide a subclass the means to control the type of node that
is created within methods of the parent class.
Chapter 11. Search Trees
Code Fragment 11.11: Additional code for the TreeMap class (continued from Code
Fragment 11.10), to provide nonpublic utilities for balanced search tree subclasses.
11.3. AVL Trees 481
11.3 AVL Trees
The TreeMap class, which uses a standard binary search tree as its data structure,
should be an efficient map data structure, but its worst-case performance for the
various operations is linear time, because it is possible that a series of operations
results in a tree with linear height. In this section, we describe a simple balancing
strategy that guarantees worst-case logarithmic running time for all the fundamental
map operations.
Definition of an AVL Tree
The simple correction is to add a rule to the binary search tree definition that will
maintain a logarithmic height for the tree. Although we originally defined the
height of a subtree rooted at position p of a tree to be the number of edges on
the longest path from p to a leaf (see Section 8.1.3), it is easier for explanation in
this section to consider the height to be the number of nodes on such a longest path.
By this definition, a leaf position has height 1, while we trivially define the height
of a  null  child to be 0.
In this section, we consider the following height-balance property, which characterizes
the structure of a binary search tree T in terms of the heights of its nodes.
Height-Balance Property: For every position p of T, the heights of the children
of p differ by atmost 1.
Any binary search tree T that satisfies the height-balance property is said to be an
AVL tree, named after the initials of its inventors: Adel son-Vel skii and Landis.
An example of an AVL tree is shown in Figure 11.11.
Figure 11.11: An example of an AVL tree. The keys of the items are shown inside
the nodes, and the heights of the nodes are shown above the nodes (with empty
subtrees having height 0).
Chapter 11. Search Trees
An immediate consequence of the height-balance property is that a subtree of an
AVL tree is itself an AVL tree. The height-balance property has also the important
consequence of keeping the height small, as shown in the following proposition.
Proposition 11.2: The height of an AVL tree storing n entries is O(logn).
Justification: Instead of trying to find an upper bound on the height of an AVL
tree directly, it turns out to be easier to work on the  inverse problem  of finding a
lower bound on the minimum number of nodes n(h) of an AVL tree with height h.
We will show that n(h) grows at least exponentially. From this, it will be an easy
step to derive that the height of an AVL tree storing n entries is O(logn).
We begin by noting that n(1) = 1 and n(2) = 2, because an AVL tree of height
1 must have exactly one node and an AVL tree of height 2 must have at least two
nodes. Now, an AVL tree with the minimum number of nodes having height h for
h   3, is such that both its subtrees are AVL trees with the minimum number of
nodes: one with height h 1 and the other with height h 2. Taking the root into
account, we obtain the following formula that relates n(h) to n(h 1) and n(h 2),
for h   3:
n(h) = 1+n(h 1)+n(h 2). (11.1)
At this point, the reader familiar with the properties of Fibonacci progressions (Section
1.8 and Exercise C-3.49) will already see that n(h) is a function exponential
in h. To formalize that observation, we proceed as follows.
Formula 11.1 implies that n(h) is a strictly increasing function of h. Thus, we
know that n(h 1) > n(h 2). Replacing n(h 1) with n(h 2) in Formula 11.1
and dropping the 1, we get, for h   3,
n(h) > 2   n(h 2). (11.2)
Formula 11.2 indicates that n(h) at least doubles each time h increases by 2, which
intuitively means that n(h) grows exponentially. To show this fact in a formal way,
we apply Formula 11.2 repeatedly, yielding the following series of inequalities:

11.3. AVL Trees
By substituting the above value of i in Formula 11.3, we obtain, for h   3,
from which we get
h < 2log(n(h))+2, (11.5)
which implies that an AVL tree storing n entries has height at most 2logn+2.
By Proposition 11.2 and the analysis of binary search trees given in Section 11.1,
the operation getitem , in a map implemented with an AVL tree, runs in time
O(logn), where n is the number of items in the map. Of course, we still have to
show how to maintain the height-balance property after an insertion or deletion.
11.3.1 Update Operations
Given a binary search tree T, we say that a position is balanced if the absolute
value of the difference between the heights of its children is at most 1, and we say
that it is unbalanced otherwise. Thus, the height-balance property characterizing
AVL trees is equivalent to saying that every position is balanced.
The insertion and deletion operations for AVL trees begin similarly to the corresponding
operations for (standard) binary search trees, but with post-processing for
each operation to restore the balance of any portions of the tree that are adversely
affected by the change.
Insertion
Suppose that tree T satisfies the height-balance property, and hence is an AVL tree,
prior to the insertion of a new item. An insertion of a new item in a binary search
tree, as described in Section 11.1.3, results in a new node at a leaf position p. This
action may violate the height-balance property (see, for example, Figure 11.12a),
yet the only positions that may become unbalanced are ancestors of p, because
those are the only positions whose subtrees have changed. Therefore, let us describe
how to restructure T to fix any unbalance that may have occurred.
Chapter 11. Search Trees
Figure 11.12: An example insertion of an item with key 54 in the AVL tree of
Figure 11.11: (a) after adding a new node for key 54, the nodes storing keys 78
and 44 become unbalanced; (b) a trinode restructuring restores the height-balance
property. We show the heights of nodes above them, and we identify the nodes x,
y, and z and subtrees T1, T2, T3, and T4 participating in the trinode restructuring.
We restore the balance of the nodes in the binary search tree T by a simple
 search-and-repair  strategy. In particular, let z be the first position we encounter in
going up from p toward the root of T such that z is unbalanced (see Figure 11.12a.)
Also, let y denote the child of z with higher height (and note that y must be an
ancestor of p). Finally, let x be the child of y with higher height (there cannot be a
tie and position x must also be an ancestor of p, possibly p itself ). We rebalance
the subtree rooted at z by calling the trinode restructuring method, restructure(x),
originally described in Section 11.2. An example of such a restructuring in the
context of an AVL insertion is portrayed in Figure 11.12.
To formally argue the correctness of this process in reestablishing the AVL
height-balance property, we consider the implication of z being the nearest ancestor
of p that became unbalanced after the insertion of p. It must be that the height
of y increased by one due to the insertion and that it is now 2 greater than its
sibling. Since y remains balanced, it must be that it formerly had subtrees with
equal heights, and that the subtree containing x has increased its height by one.
That subtree increased either because x = p, and thus its height changed from 0
to 1, or because x previously had equal-height subtrees and the height of the one
containing p has increased by 1. Letting h   0 denote the height of the tallest child
of x, this scenario might be portrayed as in Figure 11.13.
After the trinode restructuring, we see that each of x, y, and z has become
balanced. Furthermore, the node that becomes the root of the subtree after the
restructuring has height h+2, which is precisely the height that z had before the
insertion of the new item. Therefore, any ancestor of z that became temporarily
unbalanced becomes balanced again, and this one restructuring restores the heightbalance
property globally.
Figure 11.13: Rebalancing of a subtree during a typical insertion into an AVL tree:
(a) before the insertion; (b) after an insertion in subtree T3 causes imbalance at z;
(c) after restoring balance with trinode restructuring. Notice that the overall height
of the subtree after the insertion is the same as before the insertion.
Chapter 11. Search Trees
Deletion
Recall that a deletion from a regular binary search tree results in the structural
removal of a node having either zero or one children. Such a change may violate
the height-balance property in an AVL tree. In particular, if position p represents
the parent of the removed node in tree T, there may be an unbalanced node on the
path from p to the root of T. In fact, there can be at most one
such unbalanced node. 
Figure 11.14: Deletion of the item with key 32 from the AVL tree of Figure 11.12b:
(a) after removing the node storing key 32, the root becomes unbalanced; (b) a
(single) rotation restores the height-balance property.
As with insertion, we use trinode restructuring to restore balance in the tree T.
In particular, let z be the first unbalanced position encountered going up from p
toward the root of T. Also, let y be the child of z with larger height (note that
position y is the child of z that is not an ancestor of p), and let x be the child of y
defined as follows: If one of the children of y is taller than the other, let x be the
taller child of y; else (both children of y have the same height), let x be the child of
y on the same side as y (that is, if y is the left child of z, let x be the left child of
y, else let x be the right child of y). In any case, we then perform a restructure(x)
operation. (See Figure 11.14b.)
The restructured subtree is rooted at the middle position denoted as b in the
description of the trinode restructuring operation. The height-balance property is
guaranteed to be locally restored within the subtree of b. (See Exercises R-11.11
and R-11.12). Unfortunately, this trinode restructuring may reduce the height of the
subtree rooted at b by 1, which may cause an ancestor of b to become unbalanced.
So, after rebalancing z, we continue walking up T looking for unbalanced positions.
If we find another, we perform a restructure operation to restore its balance, and
continue marching up T looking for more, all the way to the root. Still, since the
height of T is O(logn), where n is the number of entries, by Proposition 11.2,
O(logn) trinode restructurings are sufficient to restore the height-balance property.
Performance of AVL Trees
By Proposition 11.2, the height of an AVL tree with n items is guaranteed to be
O(logn). Because the standard binary search tree operation had running times
bounded by the height (see Table 11.1), and because the additional work in maintaining
balance factors and restructuring an AVL tree can be bounded by the length
of a path in the tree, the traditional map operations run in worst-case logarithmic
time with an AVL tree. We summarize these results in Table 11.2, and illustrate
this performance in Figure
In this section, we consider a data structure known as a (2,4) tree. It is a particular
example of a more general structure known as a multiway search tree, in which
internal nodes may have more than two children. Other forms of multiway search
trees will be discussed in Section 15.3.
11.5.1 Multiway Search Trees
Recall that general trees are defined so that internal nodes may have many children.
In this section, we discuss how general trees can be used as multiway search trees.
Map items stored in a search tree are pairs of the form (k,v), where k is the key and
v is the value associated with the key.
Definition of a Multiway Search Tree
Let w be a node of an ordered tree. We say that w is a d-node if w has d children.
We define a multiway search tree to be an ordered tree T that has the following
properties, which are illustrated in Figure 11.23a:
  Each internal node of T has at least two children. That is, each internal node
is a d-node such that d   2.
  Each internal d-node w of T with children c1, . . . ,cd stores an ordered set of
d 1 key-value pairs (k1,v1), . . ., (kd 1,vd 1), where k1           kd 1.
  Let us conventionally define k0 =    and kd =+ . For each item (k,v)
stored at a node in the subtree of w rooted at ci, i = 1, . . . ,d, we have that
ki 1   k   ki.
That is, if we think of the set of keys stored at w as including the special fictitious
keys k0 =    and kd =+ , then a key k stored in the subtree of T rooted at a
child node ci must be  in between  two keys stored at w. This simple viewpoint
gives rise to the rule that a d-node stores d  1 regular keys, and it also forms the
basis of the algorithm for searching in a multiway search tree.
By the above definition, the external nodes of a multiway search do not store
any data and serve only as  placeholders.  These external nodes can be efficiently
represented by None references, as has been our convention with binary search
trees (Section 11.1). However, for the sake of exposition, we will discuss these
as actual nodes that do not store anything. Based on this definition, there is an
interesting relationship between the number of key-value pairs and the number of
external nodes in a multiway search tree.
Proposition 11.7: An n-item multiway search tree has n+1 external nodes.
We leave the justification of this proposition as an exercise (C-11.52).
(2,4) Trees 503
25
11 13
3 4 6 8 14 23 24 27
5 10
22
17
(a)
6 8
5 10
22
25
11 13 17
3 4 14 23 24 27
(b)
23 24
17
3 4 6 8 27
25
11 13
14
5 10
22
(c)
Figure 11.23: (a) A multiway search tree T; (b) search path in T for key 12 (unsuccessful
search); (c) search path in T for key 24 (successful search).
 
504 Chapter 11. Search Trees
Searching in a Multiway Tree
Searching for an item with key k in a multiway search tree T is simple. We perform
such a search by tracing a path in T starting at the root. (See Figure 11.23b and c.)
When we are at a d-node w during this search, we compare the key k with the keys
k1, . . . ,kd 1 stored at w. If k = ki for some i, the search is successfully completed.
Otherwise, we continue the search in the child ci of w such that ki 1 < k < ki.
(Recall that we conventionally define k0 =    and kd = + .) If we reach an
external node, then we know that there is no item with key k in T, and the search
terminates unsuccessfully.
Data Structures for Representing Multiway Search Trees
In Section 8.3.3, we discuss a linked data structure for representing a general tree.
This representation can also be used for a multiway search tree. When using a
general tree to implement a multiway search tree, we must store at each node one
or more key-value pairs associated with that node. That is, we need to store with w
a reference to some collection that stores the items for w.
During a search for key k in a multiway search tree, the primary operation
needed when navigating a node is finding the smallest key at that node that is greater
than or equal to k. For this reason, it is natural to model the information at a
node itself as a sorted map, allowing use of the find ge(k) method. We say such
a map serves as a secondary data structure to support the primary data structure
represented by the entire multiway search tree. This reasoning may at first seem
like a circular argument, since we need a representation of a (secondary) ordered
map to represent a (primary) ordered map. We can avoid any circular dependence,
however, by using the bootstrapping technique, where we use a simple solution to
a problem to create a new, more advanced solution.
In the context of a multiway search tree, a natural choice for the secondary
structure at each node is the SortedTableMap of Section 10.3.1. Because we want
to determine the associated value in case of a match for key k, and otherwise the
corresponding child ci such that ki 1 < k < ki, we recommend having each key
ki in the secondary structure map to the pair (vi,ci). With such a realization of a
multiway search tree T, processing a d-node w while searching for an item of T
with key k can be performed using a binary search operation in O(logd) time. Let
dmax denote the maximum number of children of any node of T, and let h denote the
height of T. The search time in a multiway search tree is therefore O(hlog dmax).
If dmax is a constant, the running time for performing a search is O(h).
The primary efficiency goal for a multiway search tree is to keep the height as
small as possible. We next discuss a strategy that caps dmax at 4 while guaranteeing
a height h that is logarithmic in n, the total number of items stored in the map.
 
11.5. (2,4) Trees 505
11.5.2 (2,4)-Tree Operations
A multiway search tree that keeps the secondary data structures stored at each node
small and also keeps the primary multiway tree balanced is the (2,4) tree, which is
sometimes called a 2-4 tree or 2-3-4 tree. This data structure achieves these goals
by maintaining two simple properties (see Figure 11.24):
Size Property: Every internal node has at most four children.
Depth Property: All the external nodes have the same depth.
12
3 4 6 7 8 11 17
5 10 15
13 14
Figure 11.24: A (2,4) tree.
Again, we assume that external nodes are empty and, for the sake of simplicity,
we describe our search and update methods assuming that external nodes are real
nodes, although this latter requirement is not strictly needed.
Enforcing the size property for (2,4) trees keeps the nodes in the multiway
search tree simple. It also gives rise to the alternative name  2-3-4 tree,  since it
implies that each internal node in the tree has 2, 3, or 4 children. Another implication
of this rule is that we can represent the secondary map stored at each internal
node using an unordered list or an ordered array, and still achieve O(1)-time performance
for all operations (since dmax = 4). The depth property, on the other hand,
enforces an important bound on the height of a (2,4) tree.
Proposition 11.8: The height of a (2,4) tree storing n items is O(logn).
Justification: Let h be the height of a (2,4) tree T storing n items. We justify
the proposition by showing the claim
1
2
log(n+1)   h   log(n+1). (11.9)
To justify this claim note first that, by the size property, we can have at most
4 nodes at depth 1, at most 42 nodes at depth 2, and so on. Thus, the number of
external nodes in T is at most 4h. Likewise, by the depth property and the definition
 
506 Chapter 11. Search Trees
of a (2,4) tree, we must have at least 2 nodes at depth 1, at least 22 nodes at depth
2, and so on. Thus, the number of external nodes in T is at least 2h. In addition, by
Proposition 11.7, the number of external nodes in T is n+1. Therefore, we obtain
2h   n+1   4h.
Taking the logarithm in base 2 of the terms for the above inequalities, we get that
h   log(n+1)   2h,
which justifies our claim (Formula 11.9) when terms are rearranged.
Proposition 11.8 states that the size and depth properties are sufficient for keeping
a multiway tree balanced. Moreover, this proposition implies that performing
a search in a (2,4) tree takes O(logn) time and that the specific realization of the
secondary structures at the nodes is not a crucial design choice, since the maximum
number of children dmax is a constant.
Maintaining the size and depth properties requires some effort after performing
insertions and deletions in a (2,4) tree, however. We discuss these operations next.
Insertion
To insert a new item (k,v), with key k, into a (2,4) tree T, we first perform a search
for k. Assuming that T has no item with key k, this search terminates unsuccessfully
at an external node z. Let w be the parent of z. We insert the new item into node w
and add a new child y (an external node) to w on the left of z.
Our insertion method preserves the depth property, since we add a new external
node at the same level as existing external nodes. Nevertheless, it may violate the
size property. Indeed, if a node w was previously a 4-node, then it would become
a 5-node after the insertion, which causes the tree T to no longer be a (2,4) tree.
This type of violation of the size property is called an overflow at node w, and it
must be resolved in order to restore the properties of a (2,4) tree. Let c1, . . . ,c5 be
the children of w, and let k1, . . . ,k4 be the keys stored at w. To remedy the overflow
at node w, we perform a split operation on w as follows (see Figure 11.25):
  Replace w with two nodes w_ and w__, where
  w_ is a 3-node with children c1,c2,c3 storing keys k1 and k2
  w__ is a 2-node with children c4,c5 storing key k4.
  If w is the root of T, create a new root node u; else, let u be the parent of w.
  Insert key k3 into u and make w_ and w__ children of u, so that if w was child
i of u, then w_ and w__ become children i and i+1 of u, respectively.
As a consequence of a split operation on node w, a new overflow may occur at the
parent u of w. If such an overflow occurs, it triggers in turn a split at node u. (See
Figure 11.26.) A split operation either eliminates the overflow or propagates it into
the parent of the current node. We show a sequence of insertions in a (2,4) tree in
Figure 11.27.
Figure 11.26: An insertion in a (2,4) tree that causes a cascading split: (a) before
the insertion; (b) insertion of 17, causing an overflow; (c) a split; (d) after the split
a new overflow occurs; (e) another split, creating a new root node; (f ) final tree.
 
508 Chapter 11. Search Trees

(k) (l)
Figure 11.27: A sequence of insertions into a (2,4) tree: (a) initial tree with one
item; (b) insertion of 6; (c) insertion of 12; (d) insertion of 15, which causes an
overflow; (e) split, which causes the creation of a new root node; (f ) after the split;
(g) insertion of 3; (h) insertion of 5, which causes an overflow; (i) split; (j) after the
split; (k) insertion of 10; (l) insertion of 8.
 
11.5. (2,4) Trees 509
Analysis of Insertion in a (2,4) Tree
Because dmax is at most 4, the original search for the placement of new key k uses
O(1) time at each level, and thus O(logn) time overall, since the height of the tree
is O(log n) by Proposition 11.8.
The modifications to a single node to insert a new key and child can be implemented
to run in O(1) time, as can a single split operation. The number of
cascading split operations is bounded by the height of the tree, and so that phase of
the insertion process also runs inO(log n) time. Therefore, the total time to perform
an insertion in a (2,4) tree is O(logn).
Deletion
Let us now consider the removal of an item with key k from a (2,4) tree T. Webegin
such an operation by performing a search in T for an item with key k. Removing
an item from a (2,4) tree can always be reduced to the case where the item to be
removed is stored at a node w whose children are external nodes. Suppose, for
instance, that the item with key k that we wish to remove is stored in the ith item
(ki,vi) at a node z that has only internal-node children. In this case, we swap the
item (ki,vi) with an appropriate item that is stored at a node w with external-node
children as follows (see Figure 11.28d):
1. We find the rightmost internal node w in the subtree rooted at the ith child of
z, noting that the children of node w are all external nodes.
2. We swap the item (ki,vi) at z with the last item of w.
Once we ensure that the item to remove is stored at a node w with only externalnode
children (because either it was already at w or we swapped it into w), we
simply remove the item from w and remove the ith external node of w.
Removing an item (and a child) from a node w as described above preserves the
depth property, for we always remove an external child from a node w with only
external children. However, in removing such an external node, we may violate
the size property at w. Indeed, if w was previously a 2-node, then it becomes a
1-node with no items after the removal (Figure 11.28a and d), which is not allowed
in a (2,4) tree. This type of violation of the size property is called an underflow
at node w. To remedy an underflow, we check whether an immediate sibling of w
is a 3-node or a 4-node. If we find such a sibling s, then we perform a transfer
operation, in which we move a child of s to w, a key of s to the parent u of w and s,
and a key of u to w. (See Figure 11.28b and c.) If w has only one sibling, or if both
immediate siblings of w are 2-nodes, then we perform a fusion operation, in which
we merge w with a sibling, creating a new node w_, and move a key from the parent
u of w to w_. (See Figure 11.28e and f.)
 
510 Chapter 11. Search Trees
Figure 11.28: A sequence of removals from a (2,4) tree: (a) removal of 4, causing
an underflow; (b) a transfer operation; (c) after the transfer operation; (d) removal
of 12, causing an underflow; (e) a fusion operation; (f ) after the fusion operation;
(g) removal of 13; (h) after removing 13.
 
11.5. (2,4) Trees 511
A fusion operation at node w may cause a new underflow to occur at the parent
u of w, which in turn triggers a transfer or fusion at u. (See Figure 11.29.) Hence,
the number of fusion operations is bounded by the height of the tree, which is
O(logn) by Proposition 11.8. If an underflow propagates all the way up to the root,
then the root is simply deleted. (See Figure 11.29c and d.)
11
8 10 17
6
5
14
15 6
w
8 10 17
11
u
5
15
(a) (b)
8 10
6
u
w
5 15 17
11
5 8 10 17
6 11
15
(c) (d)
Figure 11.29: A propagating sequence of fusions in a (2,4) tree: (a) removal of 14,
which causes an underflow; (b) fusion, which causes another underflow; (c) second
fusion operation, which causes the root to be removed; (d) final tree.
Performance of (2,4) Trees
The asymptotic performance of a (2,4) tree is identical to that of an AVL tree (see
Table 11.2) in terms of the sorted map ADT, with guaranteed logarithmic bounds
for most operations. The time complexity analysis for a (2,4) tree having n keyvalue
pairs is based on the following:
  The height of a (2,4) tree storing n entries is O(logn), by Proposition 11.8.
  A split, transfer, or fusion operation takes O(1) time.
  A search, insertion, or removal of an entry visits O(logn) nodes.
Thus, (2,4) trees provide for fast map search and update operations. (2,4) trees
also have an interesting relationship to the data structure we discuss next.
 
512 Chapter 11. Search Trees
11.6 Red-Black Trees
Although AVL trees and (2,4) trees have a number of nice properties, they also
have some disadvantages. For instance, AVL trees may require many restructure
operations (rotations) to be performed after a deletion, and (2,4) trees may require
many split or fusing operations to be performed after an insertion or removal. The
data structure we discuss in this section, the red-black tree, does not have these
drawbacks; it uses O(1) structural changes after an update in order to stay balanced.
Formally, a red-black tree is a binary search tree (see Section 11.1) with nodes
colored red and black in a way that satisfies the following properties:
Root Property: The root is black.
Red Property: The children of a red node (if any) are black.
Depth Property: All nodes with zero or one children have the same black depth,
defined as the number of black ancestors. (Recall that a node is its own
ancestor).
An example of a red-black tree is shown in Figure 11.30.
10 13 17
15
12
5
3
4
6 8
7 11 14
Figure 11.30: An example of a red-black tree, with  red  nodes drawn in white. The
common black depth for this tree is 3.
We can make the red-black tree definition more intuitive by noting an interesting
correspondence between red-black trees and (2,4) trees (excluding their trivial
external nodes). Namely, given a red-black tree, we can construct a corresponding
(2,4) tree by merging every red node w into its parent, storing the entry from w at
its parent, and with the children of w becoming ordered children of the parent. For
example, the red-black tree in Figure 11.30 corresponds to the (2,4) tree from Figure
11.24, as illustrated in Figure 11.31. The depth property of the red-black tree
corresponds to the depth property of the (2,4) tree since exactly one black node of
the red-black tree contributes to each node of the corresponding (2,4) tree.
Conversely, we can transform any (2,4) tree into a corresponding red-black tree
by coloring each node w black and then performing the following transformations,
as illustrated in Figure 11.32.
 
11.6. Red-Black Trees 513
11
5
14
3 13 17
15
12
4
6 8
7
10
Figure 11.31: An illustration that the red-black tree of Figure 11.30 corresponds to
the (2,4) tree of Figure 11.24, based on the highlighted grouping of red nodes with
their black parents.
  If w is a 2-node, then keep the (black) children of w as is.
  If w is a 3-node, then create a new red node y, give w s last two (black)
children to y, and make the first child of w and y be the two children of w.
  If w is a 4-node, then create two new red nodes y and z, give w s first two
(black) children to y, give w s last two (black) children to z, and make y and
z be the two children of w.
Notice that a red node always has a black parent in this construction.
Proposition 11.9: The height of a red-black tree storing n entries is O(logn).
15
  
15
(a)
13 14
  
13
14
14
13
or
(b)
6 7 8
  
7
6 8
(c)
Figure 11.32: Correspondence between nodes of a (2,4) tree and a red-black tree:
(a) 2-node; (b) 3-node; (c) 4-node.
 
514 Chapter 11. Search Trees
Justification: Let T be a red-black tree storing n entries, and let h be the height
of T. We justify this proposition by establishing the following fact:
log(n+1) 1   h   2log(n+1) 2.
Let d be the common black depth of all nodes of T having zero or one children.
Let T_ be the (2,4) tree associated with T, and let h_ be the height of T_ (excluding
trivial leaves). Because of the correspondence between red-black trees and (2,4)
trees, we know that h_ =d. Hence, by Proposition 11.8, d =h_  log(n+1) 1. By
the red property, h 2d. Thus, we obtain h 2log(n+1) 2. The other inequality,
log(n+1) 1   h, follows from Proposition 8.8 and the fact that T has n nodes.
11.6.1 Red-Black Tree Operations
The algorithm for searching in a red-black tree T is the same as that for a standard
binary search tree (Section 11.1). Thus, searching in a red-black tree takes time
proportional to the height of the tree, which is O(logn) by Proposition 11.9.
The correspondence between (2,4) trees and red-black trees provides important
intuition that we will use in our discussion of how to perform updates in red-black
trees; in fact, the update algorithms for red-black trees can seem mysteriously complex
without this intuition. Split and fuse operations of a (2,4) tree will be effectively
mimicked by recoloring neighboring red-black tree nodes. A rotation within
a red-black tree will be used to change orientations of a 3-node between the two
forms shown in Figure 11.32(b).
Insertion
Now consider the insertion of a key-value pair (k,v) into a red-black tree T. The
algorithm initially proceeds as in a standard binary search tree (Section 11.1.3).
Namely, we search for k in T until we reach a null subtree, and we introduce a new
leaf x at that position, storing the item. In the special case that x is the only node
of T, and thus the root, we color it black. In all other cases, we color x red. This
action corresponds to inserting (k,v) into a node of the (2,4) tree T_ with external
children. The insertion preserves the root and depth properties of T, but it may
violate the red property. Indeed, if x is not the root of T and the parent y of x is
red, then we have a parent and a child (namely, y and x) that are both red. Note that
by the root property, y cannot be the root of T, and by the red property (which was
previously satisfied), the parent z of y must be black. Since x and its parent are red,
but x s grandparent z is black, we call this violation of the red property a double
red at node x. To remedy a double red, we consider two cases.
 
11.6. Red-Black Trees 
Case 1: The Sibling s of y Is Black (or None). (See Figure 11.33.) In this case,
the double red denotes the fact that we have added the new node to a corresponding
3-node of the (2,4) tree T_, effectively creating a malformed
4-node. This formation has one red node (y) that is the parent of another
red node (x), while we want it to have the two red nodes as siblings instead.
To fix this problem, we perform a trinode restructuring of T. The trinode
restructuring is done by the operation restructure(x), which consists of the
following steps (see again Figure 11.33; this operation is also discussed in
Section 11.2):
  Take node x, its parent y, and grandparent z, and temporarily relabel
them as a, b, and c, in left-to-right order, so that a, b, and c will be
visited in this order by an inorder tree traversal.
  Replace the grandparent z with the node labeled b, and make nodes a
and c the children of b, keeping inorder relationships unchanged.
After performing the restructure(x) operation, we color b black and we color
a and c red. Thus, the restructuring eliminates the double-red problem. Notice
that the portion of any path through the restructured part of the tree is
incident to exactly one black node, both before and after the trinode restructuring.
Therefore, the black depth of the tree is unaffected.

Figure 11.33: Restructuring a red-black tree to remedy a double red: (a) the four
configurations for x, y, and z before restructuring; (b) after restructuring.
 
516 Chapter 11. Search Trees
Case 2: The Sibling s of y Is Red. (See Figure 11.34.) In this case, the double red
denotes an overflow in the corresponding (2,4) tree T_. To fix the problem,
we perform the equivalent of a split operation. Namely, we do a recoloring:
we color y and s black and their parent z red (unless z is the root, in which
case, it remains black). Notice that unless z is the root, the portion of any
path through the affected part of the tree is incident to exactly one black
node, both before and after the recoloring. Therefore, the black depth of the
tree is unaffected by the recoloring unless z is the root, in which case it is
increased by one.
However, it is possible that the double-red problem reappears after such a
recoloring, albeit higher up in the tree T, since z may have a red parent. If
the double-red problem reappears at z, then we repeat the consideration of the
two cases at z. Thus, a recoloring either eliminates the double-red problem
at node x, or propagates it to the grandparent z of x. We continue going
up T performing recolorings until we finally resolve the double-red problem
(with either a final recoloring or a trinode restructuring). Thus, the number
of recolorings caused by an insertion is no more than half the height of tree
T, that is, O(logn) by Proposition 11.9.

(b)
Figure 11.34: Recoloring to remedy the double-red problem: (a) before recoloring
and the corresponding 5-node in the associated (2,4) tree before the split; (b) after
recoloring and the corresponding nodes in the associated (2,4) tree after the split.
As further examples, Figures 11.35 and 11.36 show a sequence of insertion
operations in a red-black tree.
 
11.6. Red-Black Trees 517

Figure 11.35: A sequence of insertions in a red-black tree: (a) initial tree; (b) insertion
of 7; (c) insertion of 12, which causes a double red; (d) after restructuring; (e)
insertion of 15, which causes a double red; (f ) after recoloring (the root remains
black); (g) insertion of 3; (h) insertion of 5; (i) insertion of 14, which causes a
double red; (j) after restructuring; (k) insertion of 18, which causes a double red;
(l) after recoloring. (Continues in Figure 11.36.)
 
518 Chapter 11. Search Trees

(q)
Figure 11.36: A sequence of insertions in a red-black tree: (m) insertion of 16,
which causes a double red; (n) after restructuring; (o) insertion of 17, which causes
a double red; (p) after recoloring there is again a double red, to be handled by a
restructuring; (q) after restructuring. (Continued from Figure 11.35.)
 
11.6. Red-Black Trees 519
Deletion
Deleting an item with key k from a red-black tree T initially proceeds as for a binary
search tree (Section 11.1.3). Structurally, the process results in the removal a node
that has at most one child (either that originally containing key k or its inorder
predecessor) and the promotion of its remaining child (if any).
If the removed node was red, this structural change does not affect the black
depths of any paths in the tree, nor introduce any red violations, and so the resulting
tree remains a valid red-black tree. In the corresponding (2,4) tree T_, this case
denotes the shrinking of a 3-node or 4-node. If the removed node was black, then
it either had zero children or it had one child that was a red leaf (because the null
subtree of the removed node has black height 0). In the latter case, the removed
node represents the black part of a corresponding 3-node, and we restore the redblack
properties by recoloring the promoted child to black.
The more complex case is when a (nonroot) black leaf is removed. In the corresponding
(2,4) tree, this denotes the removal of an item from a 2-node. Without
rebalancing, such a change results in a deficit of one for the black depth along the
path leading to the deleted item. By necessity, the removed node must have a sibling
whose subtree has black height 1 (given that this was a valid red-black tree
prior to the deletion of the black leaf ).
To remedy this scenario, we consider a more general setting with a node z that
is known to have two subtrees, Theavy and Tlight, such that the root of Tlight (if any) is
black and such that the black depth of Theavy is exactly one more than that of Tlight,
as portrayed in Figure 11.37. In the case of a removed black leaf, z is the parent of
that leaf and Tlight is trivially the empty subtree that remains after the deletion. We
describe the more general case of a deficit because our algorithm for rebalancing
the tree will, in some cases, push the deficit higher in the tree (just as the resolution
of a deletion in a (2,4) tree sometimes cascades upward). We let y denote the root
of Theavy. (Such a node exists because Theavy has black height at least one.)
y
Theavy
Tlight
z
Figure 11.37: Portrayal of a deficit between the black heights of subtrees of node z.
The gray color in illustrating y and z denotes the fact that these nodes may be
colored either black or red.
 
520 Chapter 11. Search Trees
We consider three possible cases to remedy a deficit.
Case 1: Node y Is Black and Has a Red Child x. (See Figure 11.38.)
We perform a trinode restructuring, as originally described in Section 11.2.
The operation restructure(x) takes the node x, its parent y, and grandparent
z, labels them temporarily left to right as a, b, and c, and replaces z with the
node labeled b, making it the parent of the other two. We color a and c black,
and give b the former color of z.
Notice that the path to Tlight in the result includes one additional black node
after the restructure, thereby resolving its deficit. In contrast, the number
of black nodes on paths to any of the other three subtrees illustrated in Figure
11.38 remains unchanged.
Resolving this case corresponds to a transfer operation in the (2,4) tree T_
between the two children of the node with z. The fact that y has a red child
assures us that it represents either a 3-node or a 4-node. In effect, the item
previously stored at z is demoted to become a new 2-node to resolve the
deficiency, while an item stored at y or its child is promoted to take the place
of the item previously stored at z.

Figure 11.38: Resolving a black deficit in Tlight by performing a trinode restructuring
as restructure(x). Two possible configurations are shown (two other configurations
are symmetric). The gray color of z in the left figures denotes the fact that this node
may be colored either red or black. The root of the restructured portion is given
that same color, while the children of that node are both colored black in the result.
 
11.6. Red-Black Trees 521
Case 2: Node y Is Black and Both Children of y Are Black (or None).
Resolving this case corresponds to a fusion operation in the corresponding
(2,4) tree T_, as y must represent a 2-node. We do a recoloring; we color
y red, and, if z is red, we color it black. (See Figure 11.39). This does not
introduce any red violation, because y does not have a red child.
In the case that z was originally red, and thus the parent in the corresponding
(2,4) tree is a 3-node or 4-node, this recoloring resolves the deficit. (See
Figure 11.39a.) The path leading to Tlight includes one additional black node
in the result, while the recoloring did not affect the number of black nodes
on the path to the subtrees of Theavy.
In the case that z was originally black, and thus the parent in the corresponding
(2,4) tree is a 2-node, the recoloring has not increased the number of
black nodes on the path to Tlight; in fact, it has reduced the number of black
nodes on the path to Theavy. (See Figure 11.39b.) After this step, the two children
of z will have the same black height. However, the entire tree rooted at
z has become deficient, thereby propogating the problem higher in the tree;
we must repeat consideration of all three cases at the parent of z as a remedy.

Figure 11.39: Resolving a black deficit in Tlight by a recoloring operation: (a) when
z is originally red, reversing the colors of y and z resolves the black deficit in Tlight,
ending the process; (b) when z is originally black, recoloring y causes the entire
subtree of z to have a black deficit, requiring a cascading remedy.
 
522 Chapter 11. Search Trees
Case 3: Node y Is Red. (See Figure 11.40.)
Because y is red and Theavy has black depth at least 1, z must be black and the
two subtrees of y must each have a black root and a black depth equal to that
of Theavy. In this case, we perform a rotation about y and z, and then recolor y
black and z red. This denotes a reorientation of a 3-node in the corresponding
(2,4) tree T_.
This does not immediately resolve the deficit, as the new subtree of z is an old
subtree of y with black root y_ and black height equal to that of the original
Theavy. We reapply the algorithm to resolve the deficit at z, knowing that the
new child y_, that is the root of Theavy is now black, and therefore that either
Case 1 applies or Case 2 applies. Furthermore, the next application will be
the last, because Case 1 is always terminal and Case 2 will be terminal given
that z is red.

Figure 11.40: A rotation and recoloring about red node y and black node z, assuming
a black deficit at z. This amounts to a change of orientation in the corresponding
3-node of a (2,4) tree. This operation does not affect the black depth of any paths
through this portion of the tree. Furthermore, because y was originally red, the
new subtree of z must have a black root y_ and must have black height equal to the
original Theavy. Therefore, a black deficit remains at node z after the transformation.
In Figure 11.41, we show a sequence of deletions on a red-black tree. A dashed
edge in those figures, such as to the right of 7 in part (c), represents a branch with a
black deficiency that has not yet been resolved. We illustrate a Case 1 restructuring
in parts (c) and (d). We illustrate a Case 2 recoloring in parts (f ) and (g). Finally,
we show an example of a Case 3 rotation between parts (i) and (j), concluding with
a Case 2 recoloring in part (k).
 
11.6. Red-Black Trees 523

Figure 11.41: A sequence of deletions from a red-black tree: (a) initial tree; (b) removal
of 3; (c) removal of 12, causing a black deficit to the right of 7 (handled by
restructuring); (d) after restructuring; (e) removal of 17; (f ) removal of 18, causing
a black deficit to the right of 16 (handled by recoloring); (g) after recoloring; (h) removal
of 15; (i) removal of 16, causing a black deficit to the right of 14 (handled
initially by a rotation); (j) after the rotation the black deficit needs to be handled by
a recoloring; (k) after the recoloring.
 
524 Chapter 11. Search Trees
Performance of Red-Black Trees
The asymptotic performance of a red-black tree is identical to that of an AVL tree
or a (2,4) tree in terms of the sorted map ADT, with guaranteed logarithmic time
bounds for most operations. (See Table 11.2 for a summary of the AVL performance.)
The primary advantage of a red-black tree is that an insertion or deletion
requires only a constant number of restructuring operations. (This is in contrast
to AVL trees and (2,4) trees, both of which require a logarithmic number of structural
changes per map operation in the worst case.) That is, an insertion or deletion
in a red-black tree requires logarithmic time for a search, and may require a logarithmic
number of recoloring operations that cascade upward. Yet we show, in the
following propositions, that there are a constant number of rotations or restructure
operations for a single map operation.
Proposition 11.10: The insertion of an item in a red-black tree storing n items
can be done in O(logn) time and requires O(logn) recolorings and at most one
trinode restructuring.
Justification: Recall that an insertion begins with a downward search, the creation
of a new leaf node, and then a potential upward effort to remedy a double-red
violation. There may be logarithmically many recoloring operations due to an upward
cascading of Case 2 applications, but a single application of the Case 1 action
eliminates the double-red problem with a trinode restructuring. Therefore, at most
one restructuring operation is needed for a red-black tree insertion.
Proposition 11.11: The algorithm for deleting an item from a red-black tree with
n items takes O(log n) time and performs O(log n) recolorings and at most two
restructuring operations.
Justification: A deletion begins with the standard binary search tree deletion
algorithm, which requires time proportional to the height of the tree; for red-black
trees, that height is O(logn). The subsequent rebalancing takes place along an
upward path from the parent of a deleted node.
We considered three cases to remedy a resulting black deficit. Case 1 requires a
trinode restructuring operation, yet completes the process, so this case is applied at
most once. Case 2 may be applied logarithmically many times, but it only involves
a recoloring of up to two nodes per application. Case 3 requires a rotation, but this
case can only apply once, because if the rotation does not resolve the problem, the
very next action will be a terminal application of either Case 1 or Case 2.
In the worst case, there will be O(logn) recolorings from Case 2, a single rotation
from Case 3, and a trinode restructuring from Case 1.
 
11.6. Red-Black Trees 525
11.6.2 Python Implementation
A complete implementation of a RedBlackTreeMap class is provided in Code Fragments
11.15 through 11.17. It inherits from the standard TreeMap class and relies
on the balancing framework described in Section 11.2.1.
We begin, in Code Fragment 11.15, by overriding the definition of the nested
Node class to introduce an additional Boolean field to denote the current color
of a node. Our constructor intentionally sets the color of a new node to red to
be consistent with our approach for inserting items. We define several additional
utility functions, at the top of Code Fragment 11.16, that aid in setting the color of
nodes and querying various conditions.
When an element has been inserted as a leaf in the tree, the rebalance insert
hook is called, allowing us the opportunity to modify the tree. The new node is
red by default, so we need only look for the special case of the new node being
the root (in which case it should be colored black), or the possibility that we have
a double-red violation because the new node s parent is also red. To remedy such
violations, we closely follow the case analysis described in Section 11.6.1.
The rebalancing after a deletion also follows the case analysis described in
Section 11.6.1. An additional challenge is that by the time the rebalance hook is
called, the old node has already been removed from the tree. That hook is invoked
on the parent of the removed node. Some of the case analysis depends on knowing
about the properties of the removed node. Fortunately, we can reverse engineer that
information by relying on the red-black tree properties. In particular, if p denotes
the parent of the removed node, it must be that:
  If p has no children, the removed node was a red leaf. (Exercise R-11.26.)
  If p has one child, the removed node was a black leaf, causing a deficit,
unless that one remaining child is a red leaf. (Exercise R-11.27.)
  If p has two children, the removed node was a black node with one red child,
which was promoted.
