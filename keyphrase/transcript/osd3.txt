In Chapter 5, we showed how the CPU can be shared by a set of processes. As
a result of CPU scheduling, we can improve both the utilization of the CPU and
the speed of the computer's response to its users. To realize this increase in
performance, however, we must keep several processes in memory; that is, we
must share memory.
In this chapter, we discuss various ways to manage memory. The memorymanagement
algorithms vary from a primitive bare-machine approach to
paging and segmentation strategies. Each approach has its own advantages
and disadvantages. Selection of a memory-management method for a specific
system depends on many factors, especially on the hardware design of the
system. As we shall see, many algorithms require hardware support, although
recent designs have closely integrated the hardware and operating system.
To provide a detailed description of various ways of organizing memory
hardware.
To discuss various memory-management techniques, including paging
and segmentation.
To provide a detailed description of the Intel Pentium, which supports both
pure segmentation and segmentation with paging.
As we saw in Chapter 1, memory is central to the operation of a modern
computer system. Memory consists of a large array of words or bytes, each
with its own address. The CPU fetches instructions from memory according
to the value of the program counter. These instructions may cause additional
loading from and storing to specific memory addresses.
A typical instruction-execution cycle, for example, first fetches an instruction
from memory. The instruction is then decoded and may cause operands
to be fetched from memory. After the instruction has been executed on the
315
316 Chapter 8
operands, results may be stored back in memory. The mernory unit sees only a
stream of memory addresses; it does not know how they are generated (by the
instruction counter, indexing, indirection, literal addresses, and so on) or what
they are for (instructions or data). Accordingly, we can ignore hozu a program
generates a memory address. We are interested only in the sequence of memory
addresses generated by the running program.
We begin our discussion by covering several issues that are pertinent to the
various techniques for managing memory. This coverage includes an overview
of basic hardware issues, the binding of symbolic memory addresses to actual
physical addresses, and the distinction between logical and physical addresses.
We conclude the section with a discussion of dynamically loading and linking
code and shared libraries.
8.1.1 Basic Hardware
Main memory and the registers built into the processor itself are the only
storage that the CPU can access directly. There are machine instructions that take
memory addresses as arguments, but none that take disk addresses. Therefore,
any instructions in execution, and any data being used by the instructions,
must be in one of these direct-access storage devices. If the data are not in
memory, they must be moved there before the CPU can operate on them.
Registers that are built into the CPU are generally accessible within one
cycle of the CPU clock. Most CPUs can decode instructions and perform simple
operations on register contents at the rate of one or more operations per
clock tick The same cannot be said of main memory, which is accessed via
a transaction on the memory bus. Completing a memory access may take
many cycles of the CPU clock. In such cases, the processor normally needs
to stall, since it does not have the data required to complete the instruction
that it is executing. This situation is intolerable because of the frequency of
memory accesses. The remedy is to add fast memory between the CPU and
0   
operating
system
    
256000
process
300040 i soa( LJ.o   I
process base
420940 I 120!1GO I I     .
limit
process
880000
1024000
Figure 8.1 A base and a limit register define a logical address space.
8.1 317
main memory. A memory buffer used to accommodate a speed differential,
called a is described in Section 1.8.3.
Not only are we concerned with the relative speed of accessing physical
memory, but we also must ensure correct operation to protect the operating
system from access by user processes and, in addition, to protect user processes
from one another. This protection must be provided by the hardware. It can be
implemented in several ways, as we shall see throughout the chapter. In this
section, we outline one possible implementation.
We first need to make sure that each process has a separate memory space.
To do this, we need the ability to determine the range of legal addresses that
the process may access and to ensure that the process can access only these
legal addresses. We can provide this protection by using two registers, usually
a base and a limit, as illustrated in Figure 8.1. The base holds the
smallest legal physical memory address; the specifies the size of
the range. For example, if the base register holds 300040 and the limit register is
120900, then the program can legally access all addresses from 300040 through
420939 (inclusive).
Protection of memory space is accomplished by having the CPU hardware
compare every address generated in user mode with the registers. Any attempt
by a program executing in user mode to access operating-system memory or
other users' memory results in a trap to the operating system, which treats the
attempt as a fatal error (Figure 8.2). This scheme prevents a user program from
(accidentally or deliberately) modifying the code or data structures of either
the operating system or other users.
The base and limit registers can be loaded only by the operating system,
which uses a special privileged instruction. Since privileged instructions can
be executed only in kernel mode, and since only the operating system executes
in kernel mode, only the operating system can load the base and limit registers.
This scheme allows the operating system to change the value of the registers
but prevents user programs from changing the registers' contents.
The operating system, executing in kernel mode, is given unrestricted
access to both operating system memory and users' memory. This provision
allows the operating system to load users' programs into users' memory, to
yes
no
trap to operating system
monitor-addressing error memory
Figure 8.2 Hardware address protection with base and limit registers.
318 Chapter 8
dump out those programs in case of errors, to access and modify parameters
of system calls, and so on.
8.1.2 Address Binding
Usually, a program resides on a disk as a binary executable file. To be executed,
the program must be brought into memory and placed within a process.
Depending on the memory management in use, the process may be moved
between disk and memory during its execution. The processes on the disk that
are waiting to be brought into memory for execution form the
The normal procedure is to select one of the processes in the input queue
and to load that process into memory. As the process is executed, it accesses
instructions and data from memory. Eventually, the process terminates, and its
memory space is declared available.
Most systems allow a user process to reside in any part of the physical
memory. Thus, although the address space of the computer starts at 00000,
the first address of the user process need not be 00000. This approach affects
the addresses that the user program can use. In most cases, a user program
will go through several steps-some of which may be optional-before bein.g
executed (Figure 8.3). Addresses may be represented in different ways during
these steps. Addresses in the source program are generally symbolic (such as
count). A compiler will typically bind these symbolic addresses to relocatable
addresses (such as   14 bytes from the beginning of this module  ). The lin.kage
editor or loader will in turn bind the relocatable addresses to absolute addresses
(such as 74014). Each binding is a mapping from one address space to another.
Classically, the binding of instructions and data to memory addresses can
be done at any step along the way:
Compile time. If you know at compile time where the process will reside
in memory, then can be generated. For example, if you krww
that a user process will reside starting at location R, then the generated
compiler code will start at that location and extend up from there. If, at
some later time, the starting location changes, then it will be necessary
to recompile this code. The MS-DOS .COM-format programs are bound at
compile time.
Load time. If it is not known at compile time where the process will reside
in memory, then the compiler must generate In this case,
final binding is delayed until load time. If the starting address changes, we
need only reload the user code to incorporate this changed value.
Execution time. If the process can be moved during its execution from
one memory segment to another, then binding must be delayed until run
time. Special hardware must be available for this scheme to work, as will
be discussed in Section 8.1.3. Most general-purpose operating systems 11se
this method.
A major portion of this chapter is devoted to showing how these various
bindings can be implemented effectively in a computer system and to
discussing appropriate hardware support.
8.1
compile
time
load
time
}
execution
time (run
time)
Figure 8.3 Multistep processing of a user program.
8.1.3 Logical versus Physical Address Space
An address generated by the CPU is commonly referred to as a
319
whereas an address seen by the memory unit-that is, the one loaded into
the of the memory-is commonly referred to as a
The compile-time and load-time address-binding methods generate identical
logical and physical addresses. However, the execution-time addressbinding
scheme results in differing logical and addresses. In this case,
we usually refer to the logical address as a We use logical address
and virtual address interchangeably in this text. The set of all logical addresses
generated by a program is a logical the set of all physical
addresses corresponding to these logical addresses is a physical
Thus, in_ the execution-time address-binding scheme, the logical and physical
address spaces differ.
The run-time mapping from virtual to physical addresses is done by a
hardware device called the We can choose
from many different methods to accomplish such mapping, as we discuss in
320 Chapter 8
Figure 8.4 Dynamic relocation using a relocation register.
Sections 8.3 through 8.7. For the time being, we illustrate this mapping with
a simple MMU scheme that is a generalization of the base-register scheme
described in Section 8.1.1. The base register is now called a
The value in the relocation register is added to every address generated by a user
process at the time the address is sent to memory (see Figure 8.4). For example,
if the base is at 14000, then an attempt by the user to address location 0 is
dynamically relocated to location 14000; an access to location 346 is mapped
to location 14346. The MS-DOS operating system running on the Intel 80x86
family of processors used four relocation registers when loading and running
processes.
The user program never sees the real physical addresses. The program can
create a pointer to location 346, store it in memory, manipulate it, and compare it
with other addresses-all as the number 346. Only when it is used as a memory
address (in an indirect load or store, perhaps) is it relocated relative to the base
register. The user program deals with logical addresses. The memory-mapping
hardware converts logical addresses into physical addresses. This form of
execution-time binding was discussed in Section 8.1.2. The final location of
a referenced memory address is not determined until the reference is made.
We now have two different types of addresses: logical addresses (in the
range 0 to max) and physical addresses (in the rangeR+ 0 toR+ max for a base
valueR). The user generates only logical addresses and thinks that the process
runs in locations 0 to max. The user program generates only logical addresses
and thinks that the process runs in locations 0 to max. However, these logical
addresses must be mapped to physical addresses before they are used.
The concept of a logical address space that is bound to a separate physical
address space is central to proper memory management.
8.1.4 Dynamic Loading
In our discussion so far, it has been necessary for the entire program and all
data of a process to be in physical memory for the process to execute. The size
of a process has thus been limited to the size of physical memory. To obtain
better memory-space utilization, we can use dynamic With dynancic
8.1 321
loading, a routine is not loaded until it is called. All routines are kept on disk
in a relocatable load format. The main program is loaded into memory and
is executed. When a routine needs to call another routine, the calling routine
first checks to see whether the other routine has been loaded. If it has not, the
relocatable linking loader is called to load the desired routine into menwry and
to update the program's address tables to reflect this change. Then control is
passed to the newly loaded routine.
The advantage of dynamic loading is that an unused routine is never
loaded. This method is particularly useful when large amounts of code are
needed to handle infrequently occurring cases, such as error routines. In this
case, although the total program size may be large, the portion that is used
(and hence loaded) may be much smaller.
Dynamic loading does not require special support from the operating
system. It is the responsibility of the users to design their programs to take
advantage of such a method. Operating systems may help the programmer,
however, by providing library routines to implement dynamic loading.
8.1.5 Dynamic Linking and Shared Libraries
Figure 8.3 also shows Some operating systems
support only linking, in system language libraries are treated
like any other object module and are combined by the loader into the binary
program image. Dynamic linking, in contrast, is similar to dynamic loading.
Here, though, linking, rather than loading, is postponed until execution time.
This feature is usually used with system libraries, such as language subroutine
libraries. Without this facility, each program on a system must include a copy
of its language library (or at least the routines referenced by the program) in the
executable image. This requirement wastes both disk space and main memory.
With dynamic linking, a stub is included in the image for each libraryroutine
reference. The stub is a small piece of code that indicates how to locate
the appropriate memory-resident library routine or how to load the library if
the routine is not already present. When the stub is executed, it checks to see
whether the needed routine is already in memory. If it is not, the program loads
the routine into memory. Either way, the stub replaces itself with the address
of the routine and executes the routine. Thus, the next time that particular
code segment is reached, the library routine is executed directly, incurring no
cost for dynamic linking. Under this scheme, all processes that use a language
library execute only one copy of the library code.
This feature can be extended to library updates (such as bug fixes). A library
may be replaced by a new version, and all programs that reference the library
will automatically use the new version. Without dynamic linking, all such
programs would need to be relinked to gain access to the new library. So that
programs will not accidentally execute new, incompatible versions of libraries,
version information is included in both the program and the library. More than
one version of a library may be loaded into memory, and each program uses its
version information to decide which copy of the library to use. Versions with
minor changes retain the same version number, whereas versions with major
changes increment the number. Thus, only programs that are compiled with
the new library version are affected by any incompatible changes incorporated
322 Chapter 8
8.2
in it. Other programs linked before the new library was installed will continue
using the older library. This system is also known as   'H   -  ='    
Unlike dynamic loading, dynamic linking generally requires help from the
operating system. If the processes in memory are protected from one another,
then the operating system is the only entity that can check to see whether the
needed routine is in another process's memory space or that can allow multiple
processes to access the same memory addresses. We elaborate on this concept
when we discuss paging in Section 8.4.4.
A process must be in memory to be executed. A process, however, can be
temporarily out of memory to a and then brought
into memory for continued execution. For example, assume a multiprogramming
environment with a round-robin CPU-scheduling algorithm. When
a quantum expires, the memory manager will start to swap out the process that
just finished and to swap another process into the memory space that has been
freed (Figure 8.5). In the meantime, the CPU scheduler will allocate a time slice
to some other process in memory. When each process finishes its quantum, it
will be swapped with another process. Ideally, the memory manager can swap
processes fast enough that some processes will be in memory, ready to execute,
when the CPU scheduler wants to reschedule the CPU. In addition, the quantum
must be large enough to allow reasonable amounts of computing to be done
between swaps.
A variant of this swapping policy is used for priority-based scheduling
algorithms. If a higher-priority process arrives and wants service, the memory
manager can swap out the lower-priority process and then load and execute
the higher-priority process. When the higher-priority process finishes, the
@swap out
@swap in
backing store
main memory
Figure 8.5 Swapping of two processes using a disk as a backing store.
8.2 323
lower-priority process can be swapped back in and continued. This variant
of swapping is sometimes called roll
Normally, a process that is swapped out will be swapped back into the
same memory space it occupied previously. This restriction is dictated by the
method of address binding. If binding is done at assembly or load time, then
the process cannot be easily moved to a different location. If execution-time
binding is being used, however, then a process can be swapped into a different
memory space, because the physical addresses are computed during execution
time.
Swapping requires a backing store. The backing store is commonly a fast
disk. It must be large enough to accommodate copies of all memory images
for all users, and it must provide direct access to these memory images. The
system maintains a consisting of all processes whose memory
images are on the backing store or in memory and are ready to run. Whenever
the CPU scheduler decides to execute a process, it calls the dispatcher. The
dispatcher checks to see whether the next process in the queue is in memory.
If it is not, and if there is no free memory region, the dispatcher swaps out a
process currently in memory and swaps in the desired process. It then reloads
registers and transfers control to the selected process.
The context-switch time in such a swapping system is fairly high. To get
an idea of the context-switch time, let us assume that the user process is 100
MB in size and the backing store is a standard hard disk with a transfer rate of
50MB per second. The actual transfer of the 100-MB process to or from main
memory takes
100MB/50MB per second= 2 seconds.
Assuming an average latency of 8 milliseconds, the swap time is 2008
milliseconds. Since we must both swap out and swap in, the total swap time is
about 4016 milliseconds.
Notice that the major part of the swap time is transfer time. The total
transfer time is directly proportional to the amount of memory swapped. If we
have a computer system with 4 GB of main memory and a resident operating
system taking 1 GB, the maximum size of the user process is 3GB. However,
many user processes may be much smaller than this-say, 100 MB. A 100-MB
process could be swapped out in 2 seconds, compared with the 60 seconds
required for swapping 3 GB. Clearly, it would be useful to know exactly how
much memory a user process is using, not simply how much it might be using.
Then we would need to swap only what is actually used, reducing swap time.
For this method to be effective, the user must keep the system informed of
any changes in memory requirements. Thus, a process with dynamic memory
requirements will need to issue system calls (request memory and release
memory) to inform the operating system of its changing memory needs.
Swapping is constrained by other factors as well. If we want to swap
a process, we must be sure that it is completely idle. Of particular concern
is any pending I/0. A process may be waiting for an I/0 operation when
we want to swap that process to free up memory. However, if the I/0 is
asynchronously accessing the user memory for I/0 buffers, then the process
cannot be swapped. Assume that the I/0 operation is queued because the
device is busy. If we were to swap out process P1 and swap in process P2, the
324 Chapter 8
8.3
I/0 operation might then attempt to use memory that now belongs to process
P2 . There are two main solutions to this problem: never swap a process with
pending I/0, or execute I/0 operations only into operating-system buffers.
Transfers between operating-system buffers and process memory then occur
only when the process is swapped in.
The assumption, mentioned earlier, that swapping requires few, if any,
head seeks needs further explanation. We postpone discussing this issue until
Chapter 12, where secondary-storage structure is covered. Generally, swap
space is allocated as a chunk of disk, separate from the file system, so that its
use is as fast as possible.
Currently, standard swapping is used in few systems. It requires too
much swapping time and provides too little execution time to be a reasonable
memory-management solution. Modified versions of swapping, however, are
found on many systems.
A modification of swapping is used in many versions of UNIX. Swapping is
normally disabled but will start if many processes are running and are using a
threshold amount of memory. Swapping is again halted when the load on the
system is reduced. Memory management in UNIX is described fully in Sections
21.7 and A.6.
Early PCs-which lacked the sophistication to implement more advanced
memory-management methods-ran multiple large processes by using a
modified version of swapping. A prime example is the Microsoft Windows
3.1 operating system, which supports concurrent execution of processes in
memory. If a new process is loaded and there is insufficient main memory,
an old process is swapped to disk This operating system does not provide
full swapping, however, because the user, rather than the scheduler, decides
when it is time to preempt one process for another. Any swapped-out process
remains swapped out (and not executing) until the user selects that process to
run. Subsequent versions of Microsoft operating systems take advantage of the
advanced MMU features now found in PCs. We explore such features in Section
8.4 and in Chapter 9, where we cover virtual memory.
The main memory must accommodate both the operating system and the
various user processes. We therefore need to allocate main menlOry in the most
efficient way possible. This section explains one common method, contiguous
memory allocation.
The memory is usually divided into two partitions: one for the resident
operating system and one for the user processes. We can place the operating
system in either low memory or high memory. The major factor affecting this
decision is the location of the interrupt vector. Since the interrupt vector is
often in low memory, programmers usually place the operating system in low
memory as well. Thus, in this text, we discuss only the situation in which
the operating system resides in low memory. The development of the other
situation is similar.
We usually want several user processes to reside in memory at the same
time. We therefore need to consider how to allocate available memory to the
processes that are in the input queue waiting to be brought into memory.
8.3 325
In. contiguous memory allocation, each process is contained in a single
contiguous section of memory.
8.3.1 Memory Mapping and Protection
Before discussing memory allocation further, we must discuss the issue of
memory mapping and protection. We can provide these features by using a
relocation register, as discussed in Section 8.1.3, together with a limit register,
as discussed in Section 8.1.1. The relocation register contaiTlS the value of
the smallest physical address; the limit register contains the range of logical
addresses (for example, relocation= 100040 and limit= 74600). With relocation
and limit registers, each logical address must be less than the limit register; the
MMU maps the logical address dynamically by adding the value in the relocation
register. This mapped address is sent to memory (Figure 8.6).
When the CPU scheduler selects a process for execution, the dispatcher
loads the relocation and limit registers with the correct values as part of the
context switch. Because every address generated by a CPU is checked against
these registers, we can protect both the operating system and the other users'
programs and data from being modified by this running process.
The relocation-register scheme provides an effective way to allow the
operating system's size to change dynamically. This flexibility is desirable in
many situations. For example, the operating system contains code and buffer
space for device drivers. If a device driver (or other operating-system service)
is not commonly used, we do not want to keep the code and data in memory, as
we might be able to use that space for other purposes. Such code is sometimes
called transient operating-system code; it comes and goes as needed. Thus,
using this code changes the size of the operating system during program
execution.
8.3.2 Memory Allocation
Now we are ready to turn to memory allocation. One of the simplest
methods for allocating memory is to divide memory into several fixed-sized
Each partition may contain exactly one process. Thus, the degree
no
trap: addressing error
Figure 8.6 Hardware supportfor relocation and limit registers.
326 Chapter 8
of multiprogramming is bound by the number of partitions. In this
when a partition is free, a process is selected from the input
queue and is loaded into the free partition. When the process terminates, the
partition becomes available for another process. This method was originally
used by the IBM OS/360 operating system (called MFT); it is no longer in use.
The method described next is a generalization of the fixed-partition scheme
(called MVT); it is used primarily in batch environments. Many of the ideas
presented here are also applicable to a time-sharing environment in which
pure segmentation is used for memory management (Section 8.6).
In the scheme, the operating system keeps a table
indicating which parts of memory are available and which are occupied.
Initially, all memory is available for user processes and is considered one
large block of available memory a Eventually as you will see, memory
contains a set of holes of various sizes.
As processes enter the system, they are put into an input queue. The
operating system takes into account the memory requirements of each process
and the amount of available memory space in determining which processes are
allocated memory. When a process is allocated space, it is loaded into memory,
and it can then compete for CPU time. When a process terminates, it releases its
memory which the operating system may then fill with another process from
the input queue.
At any given time, then, we have a list of available block sizes and an
input queue. The operating system can order the input queue according to
a scheduling algorithm. Memory is allocated to processes untit finally, the
memory requirements of the next process cannot be satisfied -that is, no
available block of memory (or hole) is large enough to hold that process. The
operating system can then wait until a large enough block is available, or it can
skip down the input queue to see whether the smaller memory requirements
of some other process can be met.
In generat as mentioned, the memory blocks available comprise a set of
holes of various sizes scattered throughout memory. When a process arrives
and needs memory, the system searches the set for a hole that is large enough
for this process. If the hole is too large, it is split into two parts. One part is
allocated to the arriving process; the other is returned to the set of holes. When
a process terminates, it releases its block of memory, which is then placed back
in the set of holes. If the new hole is adjacent to other holes, these adjacent holes
are merged to form one larger hole. At this point, the system may need to check
whether there are processes waiting for memory and whether this newly freed
and recombined memory could satisfy the demands of any of these waiting
processes.
This procedure is a particular instance of the general
which concerns how to satisfy a request of size n from a
There are many solutions to this problem. The
and strategies are the ones most commonly used to select a free hole
from the set of available holes.
First fit. Allocate the first hole that is big enough. Searching can start either
at the beginning of the set of holes or at the location where the previous
first-fit search ended. We can stop searching as soon as we find a free hole
that is large enough.
8.3 327
Best fit. Allocate the smallest hole that is big enough. We must search the
entire list, unless the list is ordered by size. This strategy produces the
smallest leftover hole.
Worst fit. Allocate the largest hole. Again, we must search the entire list,
unless it is sorted by size. This strategy produces the largest leftover hole,
which may be more useful than the smaller leftover hole from a best-fit
approach.
Simulations have shown that both first fit and best fit are better than worst
fit in terms of decreasing time and storage utilization. Neither first fit nor best
fit is clearly better than the other in terms of storage utilization, but first fit is
generally faster.
8.3.3 Fragmentation
Both the first-fit and best-fit strategies for memory allocation suffer from
external As processes are loaded and removed from memory,
the free memory space is broken into little pieces. External fragmentation exists
when there is enough total memory space to satisfy a request but the available
spaces are not contiguous; storage is fragmented into a large number of small
holes. This fragmentation problem can be severe. In the worst case, we could
have a block of free (or wasted) memory between every two processes. If all
these small pieces of memory were in one big free block instead, we might be
able to run several more processes.
Whether we are using the first-fit or best-fit strategy can affect the amount
of fragmentation. (First fit is better for some systems, whereas best fit is better
for others.) Another factor is which end of a free block is allocated. (Which is
the leftover piece-the one on the top or the one on the bottom ) No matter
which algorithm is used, however, external fragmentation will be a problem.
Depending on the total amount of memory storage and the average process
size, external fragmentation may be a minor or a major problem. Statistical
analysis of first fit, for instance, reveals that, even with some optimization,
given N allocated blocks, another 0.5 N blocks will be lost to fragmentation.
That is, one-third of memory may be unusable! This property is known as the
Memory fragmentation can be internal as well as external. Consider a
multiple-partition allocation scheme with a hole of 18,464 bytes. Suppose that
the next process requests 18,462 bytes. If we allocate exactly the requested block,
we are left with a hole of 2 bytes. The overhead to keep track of this hole will be
substantially larger than the hole itself. The general approach to avoiding this
problem is to break the physical memory into fixed-sized blocks and allocate
memory in units based on block size. With this approach, the memory allocated
to a process may be slightly larger than the requested memory. The difference
between these two numbers is internal memory that
is internal to a partition.
One solution to the problem of external fragmentation is The
goal is to shuffle the memory contents so as to place all free n'lemory together
in one large block. Compaction is not always possible, however. If relocation
is static and is done at assembly or load time, compaction cannot be done;
compaction is possible only if relocation is dynamic and is done at execution
328 Chapter 8
8.4
time. If addresses are relocated dynamically, relocation requires only moving
the program and data and then changing the base register to reflect the new
base address. When compaction is possible, we must determine its cost. The
simplest compaction algorithm is to move all processes toward one end of
memory; all holes move in the other direction, producing one large hole of
available memory. This scheme can be expensive.
Another possible solution to the external-fragmentation problem is to
permit the logical address space of the processes to be noncontiguous, thus
allowing a process to be allocated physical memory wherever such memory
is available. Two complementary techniques achieve this solution: paging
(Section 8.4) and segmentation (Section 8.6). These techniques can also be
combined (Section 8.7).
is a memory-management scheme that permits the physical address
space a process to be noncontiguous. Paging avoids external fragmentation
and the need for compaction. It also solves the considerable problem of
fitting memory chunks of varying sizes onto the backin.g store; most memorymanagement
schemes used before the introduction of paging suffered from
this problem. The problem arises because, when some code fragments or data
residing in main memory need to be swapped out, space must be fmmd on
the backing store. The backing store has the same fragmentation problems
discussed in connection with main memory, but access is much slower, so
compaction is impossible. Because of its advantages over earlier methods,
paging in its various forms is used in most operating systems.
physical
address fOOOO  .. 0000
f1111 ... 1111
page table
Figure 8.7 Paging hardware.
1---------1
physical
memory
8.4 329
Traditionally, support for paging has been handled by hardware. However,
recent designs have implemented paging by closely integrating the hardware
and operating system, especially on 64-bit microprocessors.
8.4.1 Basic Method
The basic method for implementing paging involves breaking physical memory
into fixed-sized blocks called harnes and breaking logical memory into
blocks of the same size called When a process is to be executed, its
pages are loaded into any available memory frames from their source (a file
system or the backing store). The backing store is divided into fixed-sized
blocks that are of the san1.e size as the memory frames.
The hardware support for paging is illustrated in Figure 8.7. Every address
generated the CPU is divided into two parts: a {p) and a
. The page number is used as an index into a The
page table contains the base address of each page in physical memory. This
base address is combined with the page offset to define the physical memory
address that is sent to the memory unit. The paging model of memory is shown
in Figure 8.8.
The page size (like the frame size) is defined by the hardware. The size
of a page is typically a power of 2, varying between 512 bytes and 16 MB per
page, depending on the computer architecture. The selection of a power of 2 as
a page size makes the translation of a logical address into a page number and
page offset particularly easy. If the size of the logical address space is 2m, and
a page size is 271 addressing units (bytes or wordst then the high-order m- n
bits of a logical address designate the page number, and the n low-order bits
designate the page offset. Thus, the logical address is as follows:
logical
memory
~w page table
frame
number
physical
memory
Figure 8.8 Paging model of logical and physical memory.
330 Chapter 8
page number page offset
d
m -n n
where p is an index into the page table and d is the displacement within the
page.
As a concrete (although minuscule) example, consider the memory in
Figure 8.9. Here, in the logical address, n= 2 and m = 4. Using a page size
of 4 bytes and a physical memory of 32 bytes (8 pages), we show how the
user's view of memory can be mapped into physical memory. Logical address
0 is page 0, offset 0. Indexing into the page table, we find that page 0 is in frame
5. Thus, logical address 0 maps to physical address 20 [= (5 x 4) + 0]. Logical
address 3 (page 0, offset 3) maps to physical address 23 [ = (5 x 4) + 3]. Logical
address 4 is page 1, offset 0; according to the page table, page 1 is mapped to
frame 6. Thus, logical address 4 maps to physical address 24 [ = ( 6 x 4) + O].
Logical address 13 maps to physical address 9.
You may have noticed that paging itself is a form of dynamic relocation.
Every logical address is bound by the paging hardware to some physical
address. Using paging is similar to using a table of base (or relocation) registers,
one for each frame of memory.
~m6 2 1
3 2
page table
logical memory
physical memory
Figure 8.9 Paging example for a 32-byte memory with 4-byte pages.
8.4 331
When we use a paging scheme, we have no external fragmentation: any free
frame can be allocated to a process that needs it. However, we may have some
internal fragmentation. Notice that frames are allocated as units. If the memory
requirements of a process do not happen to coincide with page boundaries,
the last frame allocated may not be completely full. For example, if page size
is 2,048 bytes, a process of 72,766 bytes will need 35 pages plus 1,086 bytes. It
will be allocated 36 frames, resulting in internal fragmentation of 2,048 - 1,086
= 962 bytes. In the worst case, a process would need 11 pages plus 1 byte. It
would be allocated 11 + 1 frames, resulting in internal fragmentation of almost
an entire frame.
If process size is independent of page size, we expect internal fragmentation
to average one-half page per process. This consideration suggests that small
page sizes are desirable. However, overhead is involved in each page-table
entry, and this overhead is reduced as the size of the pages increases. Also,
disk I/0 is more efficient when the amount data being transferred is larger
(Chapter 12). Generally, page sizes have grown over time as processes, data
sets, and main memory have become larger. Today, pages typically are between
4 KB and 8 KB in size, and some systems support even larger page sizes. Some
CPUs and kernels even support multiple page sizes. For instance, Solaris uses
page sizes of 8 KB and 4 MB, depending on the data stored by the pages.
Researchers are now developing support for variable on-the-fly page size.
Usually, each page-table entry is 4 bytes long, but that size can vary as well.
A 32-bit entry can point to one of 232 physical page frames. If frame size is 4 KB,
then a system with 4-byte entries can address 244 bytes (or 16 TB) of physical
memory.
When a process arrives in the system to be executed, its size, expressed
in pages, is examined. Each page of the process needs one frame. Thus, if the
process requires 11 pages, at least 11 frames must be available in memory. If n
frames are available, they are allocated to this arriving process. The first page
of the process is loaded inJo one of the allocated frames, and the frame number
is put in the page table for this process. The next page is loaded into another
frame, its frame number is put into the page table, and so on (Figure 8.10).
An important aspect of paging is the clear separation between the user's
view of memory and the actual physical memory. The user program views
memory as one single space, containing only this one program. In fact, the user
program is scattered throughout physical memory, which also holds other
programs. The difference between the user's view of memory and the actual
physical memory is reconciled by the address-translation hardware. The logical
addresses are translated into physical addresses. This mapping is hidden from
the user and is controlled by the operating system. Notice that the user process
by definition is unable to access memory it does not own. It has no way of
addressing memory outside of its page table, and the table includes only those
pages that the process owns.
Since the operating system is managing physical memory, it must be aware
of the allocation details of physical memory-which frames are allocated,
which frames are available, how many total frames there are, and so on. This
information is generally kept in a data structure called a frame The frame
table has one entry for each physical page frame, indicating whether the latter
is free or allocated and, if it is allocated, to which page of which process or
processes.
332 Chapter 8
free-frame list free-frame list
14 13 15 13 13
18
20 14 14
15
15 15
16 16
17 17
18 18
19 01 19
1 13
20 2 18 20
3.20
21 new-process page table 21
(a) (b)
Figure 8.10 Free frames (a) before allocation and (b) after allocation.
In addition, the operating system must be aware that user processes operate
in user space, and all logical addresses must be mapped to produce physical
addresses. If a user makes a system call (to do I/0, for example) and provides
an address as a parameter (a buffe1~ for instance), that address must be mapped
to produce the correct physical address. The operating system maintains a copy
of the page table for each process, just as it maintains a copy of the instruction
counter and register contents. This copy is used to translate logical addresses to
physical addresses whenever the operating system must map a logical address
to a physical address manually. It is also used by the CPU dispatcher to define
the hardware page table when a process is to be allocated the CPU. Paging
therefore increases the context-switch time.
8.4.2 Hardware Support
Each operating system has its own methods for storing page tables. Most
allocate a page table for each process. A pointer to the page table is stored with
the other register values (like the instruction counter) in the process control
block. When the dispatcher is told to start a process, it must reload the user
registers and define the correct hardware page-table values from the stored
user page table.
The hardware implementation of the page table can be done in several
In the simplest case, the page table is implemented as a set of dedicated
These registers should be built with very high-speed logic to make the
paging-address translation efficient. Every access to memory nlust go through
the paging map, so efficiency is a major consideration. The CPU dispatcher
reloads these registers, just as it reloads the other registers. Instructions to load
or modify the page-table registers are, of course, privileged, so that only the
operating system can change the memory map. The DEC PDP-11 is an example
of such an architecture. The address consists of 16 bits, and the page size is 8
KB. The page table thus consists of eight entries that are kept in fast registers.
8.4 333
The use of registers for the page table is satisfactory if the page table is
reasonably sncall (for example, 256 entries). Most contemporary computers,
however, allow the page table to be very large (for example, 1 million entries).
For these machines, the use of fast registers to implement the page table is
not feasible. Rather, the page table is kept in main memory, and a
points to the page table. Changing page tables requires
changing only this one register, substantially reducing context-switch time.
The problem with this approach is the time required to access a user
memory location. If we want to access location i, we must first index into
the page table, using the value in the PTBR offset by the page number fori. This
task requires a memory access. It provides us with the frame number, which
is combined with the page offset to produce the actual address. We can then
access the desired place in memory. With this scheme, two memory accesses are
needed to access a byte (one for the page-table entry, one for the byte). Thus,
memory access is slowed by a factor of 2. This delay would be intolerable under
most circumstances. We might as well resort to swapping!
The standard solution to this problem is to use a special, small, fastlookup
hardware cache, called a bc.1Her The TLB
is associative, high-speed memory. Each entry in the TLB consists of two parts:
a key (or tag) and a value. When the associative memory is presented with an
item, the item is compared with all keys simultaneously. If the item is found,
the corresponding value field is returned. The search is fast; the hardware,
however, is expensive. Typically, the number of entries in a TLB is small, often
numbering between 64 and 1,024.
The TLB is used with page tables in the following way. The TLB contains
only a few of the page-table entries. When a logical address is generated by
the CPU, its page number is presented to the TLB. If the page number is found,
its frame number is immediately available and is used to access memory. The
whole task may take less than 10 percent longer than it would if an unmapped
memory reference were used.
If the page number is not in the TLB (known as a a memory
reference to the page table must be made. When the frame number is obtained,
we can use it to access memory (Figure 8.11). In addition, we add the page
number and frame number to the TLB, so that they will be found quickly on the
next reference. If the TLB is already full of entries, the operating system must
select one for replacement. Replacement policies range from least recently
used (LRU) to random. Furthermore, some TLBs allow certain entries to be
meaning that they cannot be removed from the TLB. Typically,
TLB entries for kernel code are wired down.
Some TLBs store in each TLB entry. An
ASID uniquely identifies each process and is used to provide address-space
protection for that process. When the TLB attempts to resolve virtual page
numbers, it ensures that the ASID for the currently running process matches the
ASID associated with the virtual page. If the ASIDs do not match, the attempt is
treated as a TLB miss. In addition to providing address-space protection, an ASID
allows the TLB to contain entries for several different processes simultaneously.
If the TLB does not support separate ASIDs, then every time a new table
is selected (for instance, with each context switch), the TLB must
(or erased) to ensure that the next executing process does not use the wrong
translation information. Otherwise, the TLB could include old entries that
334 Chapter 8
TLB hit
TLB
p
TLB miss
page table
Figure 8.11 Paging hardware with TLB.
physical
memory
contain valid virtual addresses but have incorrect or invalid physical addresses
left over from the previous process.
The percentage of times that a particular page number is found in the TLB
is called the An 80-percent hit ratio, for example, means that we
find the desired page number in the TLB 80 percent of the time. If it takes 20
nanoseconds to search the TLB and 100 nanoseconds to access memory, then
a mapped-memory access takes 120 nanoseconds when the page number is
in the TLB. If we fail to find the page number in the TLB (20 nanoseconds),
then we must first access memory for the page table and frame number (100
nanoseconds) and then access the desired byte in memory (100 nanoseconds),
for a total of 220 nanoseconds. To find the effective we
weight the case by its probability:
effective access time = 0.80 x 120 + 0.20 x 220
= 140 nanoseconds.
In this example, we suffer a 40-percent slowdown in memory-access time (from
100 to 140 nanoseconds).
For a 98-percent hit ratio, we have
effective access time = 0.98 x 120 + 0.02 x 220
= 122 nanoseconds.
This increased hit rate produces only a 22 percent slowdown in access time.
We will further explore the impact of the hit ratio on the TLB in Chapter 9.
8.4 335
8.4.3 Protection
Memory protection in a paged environment is accomplished by protection bits
associated with each frame. Normally, these bits are kept in the page table.
One bit can define a page to be read-write or read-only. Every reference
to memory goes through the page table to find the correct frame nuncber. At
the same time that the physical address is being computed, the protection bits
can be checked to verify that no writes are being made to a read-only page. An
attempt to write to a read-only page causes a hardware trap to the operating
system (or memory-protection violation).
We can easily expand this approach to provide a finer level of protection.
We can create hardware to provide read-only, read-write, or execute-only
protection; or, by providing separate protection bits for each kind of access, we
can allow any combination of these accesses. Illegal attempts will be trapped
to the operating system.
One additional bit is generally attached to each entry in the page table: a
bit. When this bit is set to   valid,   the associated page is in the
process's logical address space and is thus a legal (or valid) page. When the bit
is set to  invalid,   the page is not in the process's logical address space. Illegal
addresses are trapped by use of the valid -invalid bit. The operating system
sets this bit for each page to allow or disallow access to the page.
Suppose, for example, that in a system with a 14-bit address space (0 to
16383), we have a program that should use only addresses 0 to 10468. Given
a page size of 2 KB, we have the situation shown in Figure 8.12. Addresses in
0
frame number j valid-invalid bit
0
10,468
1 2,287 '-----'--'--'-'
page n
Figure 8. i 2 Valid (v) or invalid (i) bit in a page table.
336 Chapter 8
pages 0, 1, 2, 3, 4, and 5 are mapped normally through the page table. Any
attempt to generate an address in pages 6 or 7, however, will find that the
valid -invalid bit is set to invalid, and the computer will trap to flee operating
system (invalid page reference).
Notice that this scheme has created a problem. Because the program
extends only to address 10468, any reference beyond that address is illegal.
Howeve1~ references to page 5 are classified as valid, so accesses to addresses
up to 12287 are valid. Only the addresses from 12288 to 16383 are invalid. This
problem is a result of the 2-KB page size and reflects the internal fragmentation
of paging.
Rarely does a process use all its address range. In fact many processes
use only a small fraction of the address space available to them. It would be
wasteful in these cases to create a page table with entries for every page in the
address range. Most of this table would be unused but would take up valuable
memory space. Some systems provide hardware, in the form of a
length to indicate the size of the page table. value is
checked against every logical address to verify that the address is in the valid
range for the process. Failure of this test causes an error trap to the operating
system.
8.4.4 Shared Pages
An advantage of paging is the possibility of sharing common code. This consideration
is particularly important in a time-sharing environment. Consider a
system that supports 40 users, each of whom executes a text editor. If the text
editor consists of 150 KB of code and 50 KB of data space, we need 8,000 KB to
support the 40 users. If the code is (or pure however, it
can be shared, as shown in Figure 8.13. Here we see a three-page editor-each
page 50 KB in size (the large page size is used to simplify the figure)-being
shared among three processes. Each process has its own data page.
Reentrant code is non-self-modifying code: it never changes during execution.
Thus, two or more processes can execute the same code at the same time.
Each process has its own copy of registers and data storage to hold the data for
the process's execution. The data for two different processes wilt of course, be
different.
Only one copy of the editor need be kept in physical memory. Each user's
page table maps onto the same physical copy of the editor, but data pages are
mapped onto different frames. Thus, to support 40 users, we need only one
copy of the editor (150 KB), plus 40 copies of the 50 KB of data space per user.
The total space required is now 2)50 KB instead of 8,000 KB-a significant
savings.
Other heavily used programs can also be shared -compilers, window
systems, run-time libraries, database systems, and so on. To be sharable, the
code must be reentrant. The read-only nature of shared code should not be
left to the correctness of the code; the operating system should enforce this
property.
The sharing of memory among processes on a system is similar to the
sharing of the address space of a task by threads, described in Chapter 4.
Furthermore, recall that in Chapter 3 we described shared memory as a method
8.5
ed 1
..
ed 2
ed 3
data .1
process P1
process P3
page table
for P1
page table
for P3
8.5
ed 1
ed 2
ed 3
data 2
process P2
0
data 1
2 data 3
3 ed 1
ed 2
ed 3 [ 4
5
6
data 2
page table
for P2
7
8
9
10
11
Figure 8.13 Sharing of code in a paging environment.
337
of interprocess corrununication. Some operating systems implement shared
memory using shared pages.
Organizing memory according to pages provides numerous benefits in
addition to allowing several processes to share the same physical pages. We
cover several other benefits in Chapter 9.
In this section, we explore some of the most common techniques for structuring
the page table.
8.5.1 Hierarchical Paging
Most modern computer systems support a large logical address space
(232 to 264). In such an environment, the page table itself becomes excessively
large. For example, consider a system with a 32-bit logical address space. If
the page size in such a system is 4 KB (212), then a page table may consist of
up to 1 million entries (232 /212). Assuming that each entry consists of 4 bytes,
each process may need up to 4MB of physical address space for the page table
alone. Clearly, we would not want to allocate the page table contiguously in
main memory. One simple solution to this problem is to divide the page table
into smaller pieces. We can accomplish this division in several ways.
One way is to use a two-level paging algorithm, in which the page table
itself is also paged (Figure 8.14). For example, consider again the system with
338 Chapter 8
0
page table
memory
Figure 8.14 A two-level page-table scheme.
a 32-bit logical address space and a page size of 4 KB. A logical address is
divided into a page number consisting of 20 bits and a page offset consisting
of 12 bits. Because we page the page table, the page number is further divided
into a 10-bit page number and a 10-bit page offset. Thus, a logical address is as
follows:
page number page offset
d
10 10 12
where p1 is an index into the outer page table and P2 is the displacement
within the page of the outer page table. The address-translation method for this
architecture is shown in Figure 8.15. Because address translation works from
the outer page table inward, this scheme is also known as a
The VAX architecture supports a variation of two-level paging. The VAX is
a 32-bit machine with a page size of 512 bytes. The logical address space of a
process is divided into four equal sections, each of which consists of 230 bytes.
Each section represents a different part of the logical address space of a process.
The first 2 high-order bits of the logical address designate the appropriate
section. The next 21 bits represent the logical page number of that section, and
the final 9 bits represent an offset in the desired page. By partitioning the page
outer page
table
8.5
Figure 8.  15 Address translation for a two-level 32-bit paging architecture.
339
table in this manner, the operating system can leave partitions unused until a
process needs them. An address on the VAX architecture is as follows:
section page offset
s p d
2 21 9
where s designates the section number, p is an index into the page table, and d
is the displacement within the page. Even when this scheme is used, the size
of a one-level page table for a VAX process using one section is 221 bits    4
bytes per entry= 8MB. To further reduce main-memory use, the VAX pages the
user-process page tables.
For a system with a 64-bit logical address space, a two-level paging scheme
is no longer appropriate. To illustrate this point, let us suppose that the page
size in such a system is 4 KB (212). In this case, the page table consists of up
to 252 entries. If we use a two-level paging scheme, then the iml.er page tables
can conveniently be one page long, or contain 210 4-byte entries. The addresses
look like this:
outer page inner page offset
I .. Pl   ..   I P2  . I d
42 10 12
The outer page table consists of 242 entries, or 244 bytes. The obvious way to
avoid such a large table is to divide the outer page table into smaller pieces.
(This approach is also used on some 32-bit processors for added flexibility and
efficiency.)
We can divide the outer page table in various ways. We can page the outer
page table, giving us a three-level paging scheme. Suppose that the outer page
table is made up of standard-size pages (210 entries, or 212 bytes). In this case,
a 64-bit address space is still daunting:
2nd outer page outer page inner page offset
I Pr   .  ) P2 I P3 I d
32 10 10 12
The outer page table is sti11234 bytes in size.
340 Chapter 8
The next step would be a four-level paging scheme, where the second-level
outer page table itself is also paged, and so forth. The 64-bit UltraSPARC would
require seven levels of paging-a prohibitive number of memory accessesto
translate each logical address. You can see from this example why, for 64-bit
architectures, hierarchical page tables are generally considered inappropriate.
8.5.2 Hashed Page Tables
A common approach for handling address spaces larger than 32 bits is to use
a with the hash value being the virtual page number. Each
entry in the hash table contains a linked list of elements that hash to the same
location (to handle collisions). Each element consists of three fields: (1) the
virtual page number, (2) the value of the mapped page frame, and (3) a pointer
to the next element in the linked list.
The algorithm works as follows: The virtual page number in the virtual
address is hashed into the hash table. The virtual page number is compared
with field 1 in the first element in the linked list. If there is a match, the
corresponding page frame (field 2) is used to form the desired physical address.
If there is no match, subsequent entries in the linked list are searched for a
matching virtual page number. This scheme is shown in Figure 8.16.
A variation of this scheme that is favorable for 64-bit address spaces has
been proposed. This variation uses which are similar to
hashed page tables except that each entry in the hash table refers to several
pages (such as 16) rather than a single page. Therefore, a single page-table
entry can store the mappings for multiple physical-page frames. Clustered
page tables are particularly useful for address spaces, where memory
references are noncontiguous and scattered throughout the address space.
8.5.3 Inverted Page Tables
Usually, each process has an associated page table. The page table has one
entry for each page that the process is using (or one slot for each virtual
hash table
Figure 8.16 Hashed page table.
physical
address
physical
memory
8.5 341
address, regardless of the latter's validity). This table representation is a natural
one, since processes reference pages through the pages' virtual addresses. The
operating system must then translate this reference into a physical memory
address. Since the table is sorted by virtual address, the operating system is
able to calculate where in the table the associated physical address entry is
located and to use that value directly. One of the drawbacks of this method
is that each page table may consist of millions of entries. These tables may
consume large amounts of physical memory just to keep track of how other
physical memory is being used.
To solve this problem, we can use an page An inverted
page table has one entry for each real page (or frame) of memory. Each entry
consists of the virtual address of the page stored in that real memory location,
with information about the process that owns the page. Thus, only one page
table is in the system, and it has only one entry for each page of physical
memory. Figure 8.17 shows the operation of an inverted page table. Compare
it with Figure 8.7, which depicts a standard page table in operation. Inverted
page tables often require that an address-space identifier (Section 8.4.2) be
stored in each entry of the page table, since the table usually contains several
different address spaces mapping physical memory. Storing the address-space
identifier ensures that a logical page for a particular process is mapped to the
corresponding physical page frame. Examples of systems using inverted page
tables include the 64-bit UltraSPARC and PowerPC.
To illustrate this method, we describe a simplified version of the i11verted
page table used in the IBM RT. Each virtual address in the system consists of a
triple:
  process-id, page-number, offset  .
Each inverted page-table entry is a pair   process-id, page-number   where the
process-id assumes the role of the address-space identifier. When a memory
page table
physical
address
Figure 8.17 Inverted page table.
physical
memory
342 Chapter 8
8.6
reference occurs, part of the virtual address, consisting of   process-id, pagenumber  ,
is presented to the memory subsystem. The inverted page table
is then searched for a match. If a match is found-say, at entry i-then the
physical address   i, offset   is generated. If no match is found, then an illegal
address access has been attempted.
Although this scheme decreases the amount of memory needed to store
each page table, it increases the amount of time needed to search the table when
a page reference occurs. Because the inverted page table is sorted by physical
address, but lookups occur on virtual addresses, the whole table might need to
be searched for a match. This search would take far too long. To alleviate this
problem, we use a hash table, as described in Section 8.5.2, to limit the search
to one-or at most a few-page-table entries. Of course, each access to the
hash table adds a memory reference to the procedure, so one virtual memory
reference requires at least two real memory reads-one for the hash-table entry
and one for the page table. (Recall that the TLB is searched first, before the hash
table is consulted, offering some performance improvement.)
Systems that use inverted page tables have difficulty implementing shared
memory. Shared memory is usually implemented as multiple virtual addresses
(one for each process sharing the memory) that are mapped to one physical
address. This standard method cannot be used with inverted page tables;
because there is only one virtual page entry for every physical page, one
physical page cannot have two (or more) shared virtual addresses. A simple
technique for addressing this issue is to allow the page table to contain only
one mapping of a virtual address to the shared physical address. This means
that references to virtual addresses that are not mapped result in page faults.
An. important aspect of memory management that became unavoidable with
paging is the separation of the user's view of memory from the actual physical
memory. As we have already seen, the user's view of memory is not the
same as the actual physical memory. The user's view is mapped onto physical
memory. This mapping allows differentiation between logical memory and
physical memory.
8.6.1 Basic Method
Do users think of memory as a linear array of bytes, some containing
instructions and others containing data  Most people would say no. Rather,
users prefer to view memory as a collection of variable-sized segments, with
no necessary ordering among segments (Figure 8.18).
Consider how you think of a program when you are writing it. You think
of it as a main program with a set of methods, procedures, or functions. It
may also include various data structures: objects, arrays, stacks, variables, and
so on. Each of these modules or data elements is referred to by name. You
talk about   the stack,     the math library,     the n1.ain program,   without caring
what addresses in memory these elements occupy. You are not concerned
with whether the stack is stored before or after the Sqrt () function. Each
of these segments is of variable length; the length is intrinsically defined by
subroutine
symbol
table
 .
main
program
logical address
8.6
Figure 8.18 User's view of a program.
343
the purpose of the segment in the program. Elements within a segment are
identified by their offset from the begim1.ing of the segment: the first statement
of the program, the seventh stack frame entry in the stack, the fifth instruction
of the Sqrt (), and so on.
is a memory-management scheme that supports this user
view of memory. A logical address space is a collection of segments. Each
segment has a name and a length. The addresses specify both the segment name
and the offset within the segment. The user therefore specifies each address
by two quantities: a segment name and an offset. (Contrast this scheme with
the paging scheme, in which the user specifies only a single address, which is
partitioned by the hardware into a page number and an offset, all invisible to
the programmer.)
For simplicity of implementation, segments are numbered and are referred
to by a segn  lent number, rather than by a segment name. Thus, a logical address
consists of a two tuple:
  segment-number, offset  .
Normally, the user program is compiled, and the compiler automatically
constructs segments reflecting the input program.
A C compiler might create separate segments for the following:
The code
Global variables
The heap, from which memory is allocated
The stacks used by each thread
The standard C library
344 Chapter 8
  
no
segment
table
yes
trap: addressing error
+
Figure 8.19 Segmentation hardware.
physical memory
Libraries that are linked in during compile time might be assign.ed separate
segments. The loader would take all these segments and assign them segment
numbers.
8.6.2 Hardware
Although the user can now refer to objects in the program by a two-dimensional
address, the actual physical memory is still, of course, a one-dimensional
sequence of bytes. Thus, we must define an implementation to map twodimensional
user-defined addresses into one-dimensional physical addresses.
This mapping is effected by a Each entry in the segment table
has a segment base and a segment limit. The segment base contains the startilcg
physical address where the segment resides in memory, and the segment limit
specifies the length of the segment.
The use of a segment table is illustrated in Figure 8.19. A logical address
consists of two parts: a segment number, s, and an offset into that segment, d.
The segment number is used as an index to the segment table. The offset d of
the logical address must be between 0 and the segment limit. If it is not, we trap
to the operating system (logical addressing attempt beyond end of segment).
When an offset is legal, it is added to the segment base to produce the address
in physical memory of the desired byte. The segment table is thus essentially
an array of base-limit register pairs.
As an example, consider the situation shown in Figure 8.20. We have five
segments numbered from 0 through 4. The segments are stored in physical
memory as shown. The segment table has a separate entry for each segment,
giving the beginning address of the segment in physical memory (or base) and
the length of that segment (or limit). For example, segment 2 is 400 bytes long
and begins at location 4300. Thus, a reference to byte 53 of segment 2 is mapped
8.7
subroutine
segment o
segment1
symbol
table
.  segment 4
main
program
segment 2
logical address space
8.7
0
2
3
4
limit base
1000 1400
400 6300
400 4300
1100 3200
1000 4700
segment table
Figure 8.20 Example of segmentation.
14001---1
segment o
2400
3200 1-----1
segment 3
4300 1--~--1
4700 segment 2
segment 4
5700 f--------1
6300 . .
s~gt\1e!it 1
6700
physical memory
345
onto location 4300 +53= 4353. A reference to segment 3, byte 852, is mapped to
3200 (the base of segment 3) + 852 = 4052. A reference to byte 1222 of segment
0 would result in a trap to the operating system, as this segment is only tOOO
bytes long.
Both paging and segmentation have advantages and disadvantages. In fact
some architectures provide both. In this section, we discuss the Intel Pentium
architecture, which supports both pure segmentation and segmentation with
paging. We do not give a complete description of the memory-management
structure of the Pentium in this text. Rather, we present the major ideas on
which it is based. We conclude our discussion with an overview of Linux
address translation on Pentium systems.
In Pentium systems, the CPU generates logical addresses, which are given
to the segmentation unit. The segmentation unit produces a linear address for
each logical address. The linear address is then given to the paging unit, which
in turn generates the physical address in main memory. Thus, the segmentation
and paging units form the equivalent of the memory-management unit (MMU).
This scheme is shown in Figure 8.21.
8.7.1 Pentium Segmentation
The Pentium architecture allows a segment to be as large as 4 GB, and the
maximum number of segments per process is 16 K. The logical-address space
346 Chapter 8
I CPU I
Figure 8.21 Logical to physical address translation in the Pentium.
of a process is divided into two partitions. The first partition consists of up to
8 K segments that are private to that process. The second partition consists of
up to 8 K segments that are shared all the processes. Information about
the first partition is kept in the information about
the second partition is kept in the Each entry
in the LDT and GDT consists of an 8-byte segment descriptor with detailed
information about a particular segment, including the base location and limit
of that segment.
The logical address is a pair (selector, offset), where the selector is a 16-bit
number:
g p
13 2
in which s designates the segment number, g indicates whether the segment is
in the GDT or LDT, and p deals with protection. The offset is a 32-bit number
specifying the location of the byte (or word) within the segment in question.
The machine has six segment registers, allowing six segments to be
addressed at any one time by a process. It also has six 8-byte microprogram
registers to hold the corresponding descriptors from either the LDT or GDT.
This cache lets the Pentium avoid having to read the descriptor from memory
for every memory reference.
The linear address on the Pentium is 32 bits long and is formed as follows.
The segment register points to the appropriate entry in the LDT or GDT. The
base and limit information about the segment in question is used to generate
a First, the limit is used to check for address validity. If the
address is not valid, a memory fault is generated, resulting in a trap to the
operating system. If it is valid, then the value of the offset is added to the value
of the base, resulting in a 32-bit linear address. This is shown in Figure 8.22. In
the following section, we discuss how the paging unit turns this linear address
into a physical address.
8.7.2 Pentium Paging
The Pentium architecture allows a page size of either 4 KB or 4 MB. For 4-KB
pages, the Pentium uses a two-level paging schence in which the division of
the 32-bit linear address is as follows:
page number page offset
d
10 10 12
The address-translation scheme for this architecture is similar to the scheme
shown in Figure 8.15. The Intel Pentium address translation is shown in more
8.7 347
logical address offset
+
32-bit linear address
Figure 8.22 Intel Pentium segmentation.
detail in Figure 8.23. The 10 high-order bits reference an entry in the outern'lost
page table, which the Pentium terms the page directory. (The CR3 register
points to the page directory for the current process.) The page directory entry
points to an inner page table that is indexed by the contents of the innermost
10 bits in the linear address. Finally, the low-order bits 0-11 refer to the offset
in the 4-KB page pointed to in the page table.
One entry in the page directory is the Page Size flag, which-if setindicates
that the size of the page frame is 4 MB and not the standard 4 KB.
If this flag is set, the page directory points directly to the 4-MB page frame,
bypassing the inner page table; and the 22 low-order bits in the linear address
refer to the offset in the 4-MB page frame.
31
CR3-  registe
r
page directory
page
directory
page directory
(logical address)
page table
22 21 l 1211
page
table -
I
offset
31 22 21
offset
j
4-KB
page
4-MB
page
Figure 8.23 Paging in the Pentium architecture.
0
0
3L!8 Chapter 8
To improve the efficiency of physical memory use, Intel Pentium page
tables can be swapped to disk. In this case, an invalid bit is used in the page
directory entry to indicate whether the table to which the entry is pointing is
in memory or on disk. If the table is on disk, the operating system can use
the other 31 bits to specify the disk location of the table; the table then can be
brought into memory on demand.
8.7.3 Linux on Pentium Systems
As an illustration, consider the Linux operating system running on the Intel
Pentium architecture. Because Linux is designed to run on a variety of processors-
many of which may provide only limited support for segmentationLinux
does not rely on segmentation and uses it minimally. On the Pentium,
Linux uses only six segments:
A segment for kernel code
A segment for kernel data
A segment for user code
A segment for user data
A task-state segment (TSS)
1i A default LDT segment
The segments for user code and user data are shared by all processes
running in user mode. This is possible because all processes use the same logical
address space and all segment descriptors are stored in the global descriptor
table (GDT). Furthermore, each process has its own task-state segment (TSS),
and the descriptor for this segment is stored in the GDT. The TSS is used to store
the hardware context of each process during context switches. The default LDT
segment is normally shared by all processes and is usually not used. However,
if a process requires its own LDT, it can create one and use that instead of the
default LDT.
As noted, each segment selector includes a 2-bit field for protection. Thus,
the Pentium allows four levels of protection. Of these four levels, LimlX only
recognizes two: user mode and kernel mode.
Although the Pentium uses a two-level paging model, Linux is designed
to run on a variety of hardware platforms, many of which are 64-bit platforms
where two-level paging is not plausible. Therefore, Linux has adopted a threelevel
paging strategy that works well for both 32-bit and 64-bit architectures.
The linear address in Linux is broken into the following four parts:
global
directory
middle
directory
page
table
Figure 8.24 highlights the three-level paging model in Linux.
The number of bits in each part of the linear address varies according
to architecture. However, as described earlier in this section, the Pentium
architecture only uses a two-level paging model. How, then, does Linux apply
8.8
Lglobal directory
global
directory
CR3 __,.c__ ___ __l
register
8.8
(linear address)
middle directory
Figure 8.24 Three-level paging in Linux.
offset
page
frame
349
its three-level model on the Pentium  In this situation, the size of the middle
directory is zero bits, effectively bypassing the middle directory.
Each task in Linux has its own set of page tables and -just as in Figure 8.23
-the CR3 register points to the global directory for the task currently executing.
During a context switch, the value of the CR3 register is saved and restored in
the TSS segments of the tasks involved in the context switch.
Memory-management algorithms for multiprogrammed operating systems
range from the simple single-user system approach to paged segmentation.
The most important determinant of the method used in a particular system is
the hardware provided. Every memory address generated by the CPU must be
checked for legality and possibly mapped to a physical address. The checking
cannot be implemented (efficiently) in software. Hence, we are constrained by
the hardware available.
The various memory-management algorithms (contiguous allocation, paging,
segmentation, and combinations of paging and segmentation) differ in
many aspects. In comparing different memory-management strategies, we use
the following considerations:
Hardware support. A simple base register or a base-limit register pair is
sufficient for the single- and multiple-partition schemes, whereas paging
and segmentation need mapping tables to define the address map.
Performance. As the memory-management algorithm becomes more
complex, the time required to map a logical address to a physical address
increases. For the simple systems, we need only compare or add to the
logical address-operations that are fast. Paging and segmentation can be
as fast if the mapping table is implemented in fast registers. If the table is
350 Chapter 8
in memory, however, user memory accesses can be degraded substantially.
A TLB can reduce the performance degradation to an acceptable level.
Fragmentation. A multiprogrammed system will generally perform more
efficiently if it has a higher level of multiprogramming. For a given
set of processes, we can increase the multiprogramming level only by
packing more processes into memory. To accomplish this task, we must
reduce memory waste, or fragmentation. Systems with fixed-sized allocation
units, such as the single-partition scheme and paging, suffer from
internal fragmentation. Systems with variable-sized allocation units, such
as the multiple-partition scheme and segmentation, suffer from external
fragmentation.
Relocation. One solution to the external-fragmentation problem is compaction.
Compaction involves shifting a program in memory in such a
way that the program does not notice the change. This consideration
requires that logical addresses be relocated dynamically, at execution time.
If addresses are relocated only at load time, we cannot compact storage.
Swapping. Swapping can be added to any algorithm. At intervals determined
by the operating system, usually dictated by CPU-scheduling policies,
processes are copied from main memory to a backing store and later
are copied back to main memory. This scheme allows more processes to be
run than can be fit into memory at one time.
Sharing. Another means of increasing the multiprogramming level is to
share code and data among different users. Sharing generally requires
that either paging or segmentation be used to provide small packets of
information (pages or segments) that can be shared. Sharing is a means
of running many processes with a limited amount of memory, but shared
programs and data must be designed carefully.
Protection. If paging or segmentation is provided, different sections of a
user program can be declared execute-only, read -only, or read-write. This
restriction is necessary with shared code or data and is generally useful
in any case to provide simple run-time checks for common programming
errors.
8.1 Explain the difference between internal and external fragmentation.
8.2 Compare the memory organization schemes of contiguous memory
allocation, pure segmentation, and pure paging with respect to the
following issues:
a. External fragmentation
b. Internal fragmentation
c. Ability to share code across processes
351
8.3 Why are segmentation and paging sometimes combined into one
scheme 
8.4 Most systems allow a program to allocate more memory to its address
space during execution. Allocation of data in the heap segments of
programs is an example of such allocated memory. What is required
to support dynamic memory allocation in the following schemes 
a. Contiguous memory allocation
b. Pure segmentation
c. Pure paging
8.5 Consider the Intel address-translation scheme shown in Figure 8.22.
a. Describe all the steps taken by the Intel Pentium in translatil  g a
logical address into a physical address.
b. What are the advantages to the operating system of hardware that
provides such complicated memory translation 
c. Are there any disadvantages to this address-translation system  If
so, what are they  If not, why is this scheme not used by every
manufacturer 
8.6 What is the purpose of paging the page tables 
8.7 Explain why sharil  g a reentrant module is easier when segmentation is
used than when pure paging is used.
8.8 On a system with paging, a process cannot access memory that it does
not own. Why  How could the operating system allow access to other
memory  Why should it or should it not 
8.9 Compare the segmented pagil  g scheme with the hashed page table
scheme for handling large address spaces. Under what circumstances is
one scheme preferable to the other 
8.10 Consider a paging system with the page table stored in memory.
a. If a memory reference takes 200 nanoseconds, how long does a
paged memory reference take 
b. If we add TLBs, and 75 percent of all page-table references are found
in the TLBs, what is the effective memory reference time  (Assume
that finding a page-table entry in the TLBs takes zero time, if the
entry is there.)
352 Chapter 8
8.11 Compare paging with segmentation with respect to the amount of
memory required by the address translation structures in order to
convert virtual addresses to physical addresses.
8.12 Consider a system in which a program can be separated into two
parts: code and data. The CPU knows whether it wants an instruction
(instruction fetch) or data (data fetch or store). Therefore, two baselimit
register pairs are provided: one for instructions and one for data.
The instruction base-limit register pair is automatically read-only, so
programs can be shared among different users. Discuss the advantages
and disadvantages of this scheme.
8.13 Consider the following process for generating binaries. A compiler is
used to generate the object code for individual modules, and a linkage
editor is used to combine multiple object modules into a single program
bilcary. How does the linkage editor change the bindmg of instructions
and data to memory addresses  What information needs to be passed
from the compiler to the linkage editor to facilitate the memory-binding
tasks of the linkage editor 
8.14 Consider a logical address space of 64 pages of 1,024 words each, mapped
onto a physical memory of 32 frames.
a. How many bits are there in the logical address 
b. How many bits are there in the physical address 
8.15 Consider the hierarchical paging scheme used by the VAX architecture.
How many memory operations are performed when a user program
executes a memory-load operation 
8.16 Given five memory partitions of 100 KB, 500 KB, 200 KB, 300 KB, and 600
KB (ill order), how would the first-fit, best-fit, and worst-fit algorithms
place processes of 212 KB, 417 KB, 112 KB, and 426 KB (in order)  Which
algorithm makes the most efficient use of memory 
8.17 Describe a mechanism by which one segment could belong to the address
space of two different processes.
8.18 Consider a computer system with a 32-bit logical address and 4-KB page
size. The system supports up to 512MB of physical memory. How many
entries are there in each of the following 
a. A conventional single-level page table
b. An inverted page table
353
8.19 Assuming a 1-KB page size, what are the page numbers and offsets for
the following address references (provided as decimal numbers):
a. 2375
b. 19366
c. 30000
d. 256
e. 16385
8.20 Program binaries in many systems are typically structured as follows.
Code is stored starting with a small, fixed virtual address, such as 0. The
code segment is followed by the data segment that is used for storing
the program variables. When the program starts executing, the stack is
allocated at the other end of the virtual address space and is allowed
to grow toward lower virtual addresses. What is the significance of this
structure for the following schemes 
a. Contiguous memory allocation
b. Pure segmentation
c. Pure paging
8.21 Consider the following segment table:
Segment Base Length
0 219 600
1 2300 14
2
90 100
3 1327 580
4 1952 96
What are the physical addresses for the following logical addresses 
a. 0,430
b. 1,10
c. 2,500
d. 3,400
e. 4,112
8.22 Consider a logical address space of 32 pages with 1,024 words per page,
mapped onto a physical memory of 16 frames.
a. How many bits are required in. the logical address 
b. How many bits are required in the physical address 
354 Chapter 8
8.23 Sharing segments among processes without requiring that they have the
same segment number is possible in a dynamically linked segmentation
system.
a. Define a system that allows static linking and sharing of segments
without requiring that the segment numbers be the same.
b. Describe a paging scheme that allows pages to be shared without
requiring that the page numbers be the same.
8.24 Assume that a system has a 32-bit virtual address with a 4-KB page size.
Write a C program that is passed a virtual address (in decincal) on the
command line and have it output the page number and offset for the
given address. As an example, your program would run as follows:
./a.out 19986
Your program would output:
The address 19986 contains:
page number = 4
offset = 3602
Writing this program will require using the appropriate data type to
store 32 bits. We encourage you to use unsigned data types as well.
Dynamic storage allocation was discussed by Knuth [1973] (Section 2.5), who
found through simulation results that first fit is generally superior to best fit.
Knuth [1973] also discussed the 50-percent rule.
The concept of paging can be credited to the designers of the Atlas system,
which has been described by Kilburn et al. [1961] and by Howarth et al.
[1961]. The concept of segmentation was first discussed by Dennis [1965].
Paged segmentation was first supported in the GE 645, on which MULTICS was
originally implemented (Organick [1972] and Daley and Dennis [1967]).
Inverted page tables are discussed in an article about the IBM RT storage
manager by Chang and Mergen [1988].
Address translation in software is covered in Jacob and Mudge [1997].
Hennessy and Patterson [2002] explains the hardware aspects of TLBs,
caches, and MMUs. Talluri et al. [1995] discusses page tables for 64-bit address
spaces. Alternative approaches to enforcing memory protection are proposed
and studied in Wahbe et al. [1993a], Chase et al. [1994], Bershad et al. [1995],
and Thorn [1997]. Dougan et al. [1999] and Jacob and Mudge [2001] discuss
355
tedmiques for managing the TLB. Fang et al. [2001] evaluate support for large
pages.
Tanenbaum [2001] discusses Intel80386 paging. Memory management for
several architectures-such as the Pentiunl II, PowerPC, and UltraSPARCare
described by Jacob and Mudge [1998a]. Segmentation on Lim1X systems is
presented in Bovet and Cesati [2002].

9.1
c ER
In Chapter 8, we discussed various memory-management strategies used in
computer systems. All these strategies have the same goal: to keep many
processes in memory simultaneously to allow multiprogramming. However,
they tend to require that an entire process be in memory before it can execute.
Virtual memory is a tecrucique that allows the execution of processes
that are not completely in memory. One major advantage of this scheme is
that programs can be larger than physical memory. Further, virtual memory
abstracts main memory into an extremely large, uniform array of storage,
separating logical memory as viewed by the user from physical memory.
This technique frees programmers from the concerns of memory-storage
limitations. Virtual memory also allows processes to share files easily and
to implement shared memory. In addition, it provides an efficient mechanism
for process creation. Virtual memory is not easy to implement, however, and
may substantially decrease performance if it is used carelessly. In this chapter,
we discuss virtual memory in the form of demand paging and examine its
complexity and cost.
To describe the benefits of a virtual memory system.
To explain the concepts of demand paging, page-replacement algorithms,
and allocation of page frames.
To discuss the principles of the working-set model.
The memory-management algorithms outlined in Chapter 8 are necessary
because of one basic requirement: The instructions being executed must be
in physical memory. The first approach to meeting this requirement is to place
the entire logical address space in physical memory. Dynamic loading can help
to ease this restriction, but it generally requires special precautions and extra
work by the programmer.
357
358 Chapter 9
The requirement that instructions m.ust be in physical memory to be
executed seems both necessary and reasonable; but it is also unfortunate, since
it limits the size of a program to the size of physical memory. In fact, an
examination of real programs shows us that, in many cases, the entire program
is not needed. For instance, consider the following:
Programs often have code to handle unusual error conditions. Since these
errors seldom, if ever, occur in practice, this code is almost never executed.
Arrays,lists, and tables are often allocated more memory than they actually
need. An array may be declared 100 by 100 elements, even though it is
seldom larger than 10 by 10 elements. An assembler symbol table may
have room for 3,000 symbols, although the average program has less than
200 symbols.
Certain options and features of a program may be used rarely. For instance,
the routines on U.S. government computers that balance the budget have
not been used in many years.
Even in those cases where the entire program is needed, it may not all be
needed at the same time.
The ability to execute a program that is only partially in memory would
confer many benefits:
A program would no longer be constrained by the amount of physical
memory that is available. Users would be able to write programs for an
extremely large virtual address space, simplifying the programming task.
page 0
page 1
page 2
page v
virtual
memory
memory
map
physical
memory
Figure 9.1 Diagram showing virtual memory that is larger than physical memory.
9.1 359
Because each user program could take less physical memory, more
programs could be run at the sance time, with a corresponding increase in
CPU utilization and throughput but with no increase in response time or
turnaround time.
Less I/O would be needed to load or swap user programs into memory, so
each user program would run faster.
Thus, running a program that is not entirely in memory would benefit both
the system and the user.
involves the separation of logical memory as perceived
by users from physical memory. This separation allows an extremely large
virtual memory to be provided for programmers when only a smaller physical
memory is available (Figure 9.1). Virtual memory makes the task of programming
much easier, because the programmer no longer needs to worry about
the amount of physical memory available; she can concentrate instead on the
problem to be programmed.
The address space of a process refers to the logical (or virtual) view
of how a process is stored in memory. Typically, this view is that a process
begins at a certain logical address-say, address 0-and exists in contiguous
memory, as shown in Figure 9.2. Recall from Chapter 8, though, that in fact
physical memory may be organized in page frames and that the physical page
frames assigned to a process may not be contiguous. It is up to the memorymanagement
unit (MMU) to map logical pages to physical page frames in
memory.
Note in Figure 9.2 that we allow for the heap to grow upward in memory
as it is used for dynamic memory allocation. Similarly, we allow for the stack to
grow downward in memory through successive function calls. The large blank
space (or hole) between the heap and the stack is part of the virtual address
Figure 9.2 Virtual address space.
360 Chapter 9
space but will require actual physical pages only if the heap or stack grows.
Virtual address spaces that include holes are known as sparse address spaces.
Using a sparse address space is beneficial because the holes can be filled as the
stack or heap segments grow or if we wish to dynam.ically link libraries (or
possibly other shared objects) during program execution.
In addition to separating logical memory from physical memory, virtual
memory allows files and memory to be shared by two or more processes
through page sharing (Section 8.4.4). This leads to the following benefits:
System libraries can be shared by several processes through mapping
of the shared object into a virtual address space. Although each process
considers the shared libraries to be part of its virtual address space, the
actual pages where the libraries reside in physical memory are shared by
all the processes (Figure 9.3). Typically, a library is mapped read-only into
the space of each process that is linked with it.
Similarly, virtual memory enables processes to share memory. Recall from
Chapter 3 that two or more processes can communicate through the use
of shared memory. Virtual memory allows one process to create a region
of memory that it can share with another process. Processes sharing this
region consider it part of their virtual address space, yet the actual physical
pages of memory are shared, much as is illustrated in Figure 9.3.
Virtual memory can allow pages to be shared during process creation with
the fork() system calt thus speeding up process creation.
We further explore these-and other-benefits of virtual memory later in
this chapter. First though, we discuss implementing virtual memory through
demand paging.
shared library
shared
pages shared library
Figure 9.3 Shared library using virtual memory.
9.2
9.2 361
Consider how an executable program might be loaded from disk into n'lemory.
One option is to load the entire program in physical memory at program
execution time. However, a problent with this approach is that we may not
initially need the entire program in memory. Suppose a program starts with
a list of available options from which the user is to select. Loading the entire
program into memory results in loading the executable code for all options,
regardless of whether an option is ultimately selected by the user or not. An
alternative strategy is to load pages only as they are needed. This technique is
known as paging and is commonly used in virtual memory systems.
With demand-paged virtual memory, pages are only loaded when they are
demanded during program execution; pages that are never accessed are thus
never loaded into physical memory.
A demand-paging system is similar to a paging system with swapping
(Figure 9.4) where processes reside in secondary memory (usually a disk).
When we want to execute a process, we swap it into memory. Rather than
swapping the entire process into memory, however, we use a A
lazy swapper never swaps a page into memory unless that page will be needed.
Since we are now viewing a process as a sequence of pages, rather than as one
large contiguous address space, use of the term swapper is technically incorrect.
A swapper manipulates entire processes, whereas a is concerned with
the individual pages of a process. We thus use pager, rather than swapper, in
connection with demand paging.
program
A
program
B
main
memory
swap out
so 90100110
120130140150
swap in 16017
Figure 9.4 Transfer of a paged memory to contiguous disk space.
362 Chapter 9
9.2.1 Basic Concepts
When a process is to be swapped in, the pager guesses which pages will be
used before the process is swapped out again. Instead of swapping in a whole
process, the pager brings only those pages into memory. Thus, it avoids reading
into memory pages that will not be used anyway, decreasing the swap time
and the amount of physical memory needed.
With this scheme, we need some form of hardware support to distinguish
between the pages that are in memory and the pages that are on the disk.
The valid -invalid bit scheme described in Section 8.4.3 can be used for this
purpose. This time, however, when this bit is set to   valid/' the associated page
is both legal and in n1.emory. If the bit is set to   invalid/' the page either is not
valid (that is, not in the logical address space of the process) or is valid but
is currently on the disk. The page-table entry for a page that is brought into
memory is set as usuat but the page-table entry for a page that is not currently
in memory is either simply marked invalid or contains the address of the page
on disk. This situation is depicted in Figure 9.5.
Notice that marking a page invalid will have no effect if the process never
attempts to access that page. Hence, if we guess right and page in all and only
those pages that are actually needed, the process will run exactly as though we
had brought in all pages. While the process executes and accesses pages that
are execution proceeds normally.
0
2
3
4
5
6
7
valid-invalid
frame bit '\. I
0 4 v
logical
memory
physical memory
DOD
D
[1J
[.@JtB]
ODD
Figure 9.5 Page table when some pages are not in main memory.
operating
system
reference
(,;\, page is on
\.:V backing store
 
trap
restart
instruction
page table
 
reset page
table
physical
memory
9.2
0
bring in
missing page
Figure 9.6 Steps in handling a page fault.
363
But what happens if the process tries to access a page that was not brought
into memory  Access to a page marked invalid causes a The paging
hardware, in translating the address through the page table, will notice that
the invalid bit is set, causing a trap to the operating system. This trap is the
result of the operating system's failure to bring the desired page into memory.
The procedure for handling this page fault is straightforward (Figure 9.6):
We check an internal table (usually kept with the process control block)
for this process to determine whether the reference was a valid or an
invalid memory access.
If the reference was invalid, we terminate the process. If it was valid, but
we have not yet brought in that page, we now page it in.
We find a free frame (by taking one from the free-frame list, for example).
We schedule a disk operation to read the desired page into the newly
allocated frame.
When the disk read is complete, we modify the internal table kept with
the process and the page table to indicate that the page is now in memory.
We restart the instruction that was interrupted by the trap. The process
can now access the page as though it had always been in memory.
In the extreme case, we can start executing a process with no pages in
memory. When the operating system sets the instruction pointer to the first
364 Chapter 9
instruction of the process, which is on a non-memory-resident page, the process
immediately faults for the page. After this page is brought into memory, the
process continues to execute, faulting as necessary until every page that it
needs is in memory. At that it can execute with no more faults. This
scheme is never bring a page into memory until it is
required.
Theoretically, some programs could access several new pages of memory
with each instruction execution (one page for the instruction and many for
data), possibly causing multiple page faults per instruction. This situation
would result in unacceptable system performance. Fortunately, analysis of
running processes shows that this behavior is exceedingly unlikely. Programs
tend to have described in Section 9.6.1, which results in
reasonable performance from demand paging.
The hardware to support demand paging is the same as the hardware for
paging and swapping:
Page table. This table has the ability to mark an entry invalid through a
valid -invalid bit or a special value of protection bits.
Secondary memory. This memory holds those pages that are not present
in main memory. The secondary memory is usually a high-speed disk. It is
known as the swap device, and the section of disk used for this purpose is
known as Swap-space allocation is discussed in Chapter 12.
A crucial requirement for demand paging is the ability to restart any
instruction after a page fault. Because we save the state (registers, condition
code, instruction counter) of the interrupted process when the page fault
occurs, we must be able to restart the process in exactly the same place and
state, except that the desired page is now in memory and is accessible. In most
cases, this requirement is easy to meet. A page fault may occur at any memory
reference. If the page fault occurs on the instruction fetch, we can restart by
fetching the instruction again. If a page fault occurs while we are fetching an
operand, we must fetch and decode the instruction again and then fetch the
operand.
As a worst-case example, consider a three-address instruction such as ADD
the content of A to B, placing the result in C. These are the steps to execute this
instruction:
Fetch and decode the instruction (ADD).
Fetch A
Fetch B.
Add A and B.
Store the sum in C.
If we fault when we try to store inC (because C is in a page not currently
in memory), we will have to get the desired page, bring it in, correct the
page table, and restart the instruction. The restart will require fetching the
instruction again, decoding it again, fetching the two operands again, and
9.2 365
then adding again. However, there is not much repeated work (less than one
complete instruction), and the repetition is necessary only when a page fault
occurs.
The major difficulty arises when one instruction may modify several
different locations. For example, consider the IBM System 360/370 MVC (move
character) instruction, which can ncove up to 256 bytes from one location to
another (possibly overlapping) location. If either block (source or destination)
straddles a page boundary, a page fault might occur after the move is partially
done. In addition, if the source and destination blocks overlap, the source
block may have been modified, in which case we cannot simply restart the
instruction.
This problem can be solved in two different ways. In one solution, the
microcode computes and attempts to access both ends of both blocks. If a page
fault is going to occm~ it will happen at this step, before anything is modified.
The move can then take place; we know that no page fault can occur, since all
the relevant pages are in memory. The other solution uses temporary registers
to hold the values of overwritten locations. If there is a page fault, all the old
values are written back into memory before the trap occurs. This action restores
memory to its state before the instruction was started, so that the instruction
can be repeated.
This is by no means the only architectural problem resulting from adding
paging to an existing architecture to allow demand paging, but it illustrates
some of the difficulties involved. Paging is added between the CPU and the
memory in a computer system. It should be entirely transparent to the user
process. Thus, people often assume that paging can be added to any system.
Although this assumption is true for a non-demand-paging environment,
where a page fault represents a fatal errm~ it is not true where a page fault
means only that an additional page must be brought into memory and the
process restarted.
9.2.2 Performance of Demand Paging
Demand paging can significantly affect the performance of a computer system.
To see why, let's compute the effective access time for a demand-paged
memory. For most computer systems, the memory-access time, denoted ma,
ranges from 10 to 200 nanoseconds. As long as we have no page faults, the
effective access time is equal to the memory access time. If, howeve1~ a page
fault occurs, we must first read the relevant page from disk and then access the
desired word.
Let p be the probability of a page fault (0 :::; p :::; 1). We would expect p to
be close to zero-that is, we would expect to have only a few page faults. The
    t'tP  r'! nrr-   access is then
effective access time= (1 - p) x ma + p x page fault time.
To compute the effective access time, we must know how much time is
needed to service a page fault. A page fault causes the following sequence to
occur:
Trap to the operating system.
Save the user registers and process state.
366 Chapter 9
Deterncine that the interrupt was a page fault.
Check that the page reference was legal and determine the location of the
page on the disk
Issue a read from the disk to a free frame:
a. Wait in a queue for this device until the read request is serviced.
b. Wait for the device seek and/ or latency time.
c. Begin the transfer of the page to a free frame.
While waiting, allocate the CPU to some other user (CPU scheduling,
optional).
Receive an interrupt from the disk I/0 subsystem (I/0 completed).
Save the registers and process state for the other user (if step 6 is executed).
Determine that the interrupt was from the disk
Correct the page table and other tables to show that the desired page is
now in memory.
Wait for the CPU to be allocated to this process again.
Restore the user registers, process state, and new page table, and then
resume the interrupted instruction.
Not all of these steps are necessary in every case. For example, we are assuming
that, in step 6, the CPU is allocated to another process while the I/O occurs.
This arrangement allows multiprogramming to maintain CPU utilization but
requires additional time to resume the page-fault service routine when the I/0
transfer is complete.
In any case, we are faced with tlu ee major components of the page-fault
service time:
Service the page-fault interrupt.
Read in the page.
Restart the process.
The first and third tasks can be reduced, with careful coding, to several
hundred instructions. These tasks may take from 1 to 100 microseconds each.
The page-switch time, however, will probably be close to 8 milliseconds.
(A typical hard disk has an average latency of 3 milliseconds, a seek of
5 milliseconds, and a transfer time of 0.05 milliseconds. Thus, the total
paging time is about 8 milliseconds, including hardware and software time.)
Remember also that we are looking at only the device-service time. If a queue
of processes is waiting for the device, we have to add device-queueing time as
we wait for the paging device to be free to service our request, increasing even
more the time to swap.
With an average page-fault service time of 8 milliseconds and a memoryaccess
time of 200 nanoseconds, the effective access time in nanoseconds is
3
9.3
effective access time= (1 - p) x (200) + p (8 milliseconds)
= (1 p) X 200 + p X 8,000,000
= 200 + 7,999,800 X p.
367
We see, then, that the effective access time is directly proportional to the
If one access out of 1,000 causes a page fault, the effective
access time is 8.2 microseconds. The computer will be slowed down by a factor
of 40 because of demand paging! If we want performance degradation to be
less than 10 percent, we need
220    200 + 7,999,800 X p,
20    7,999,800 X p,
p    0.0000025.
That is, to keep the slowdown due to paging at a reasonable level, we can
allow fewer than one memory access out of 399,990 to page-fault. In sum,
it is important to keep the page-fault rate low in a demand-paging system.
Otherwise, the effective access time increases, slowing process execution
dramatically.
An additional aspect of demand paging is the handling and overall use
of swap space. Disk I/0 to swap space is generally faster than that to the file
system. It is faster because swap space is allocated in much larger blocks, and
file lookups and indirect allocation methods are not used (Chapter 12). The
system can therefore gain better paging throughput by copying an entire file
image into the swap space at process startup and then performing demand
paging from the swap space. Another option is to demand pages from the file
system initially but to write the pages to swap space as they are replaced. This
approach will ensure that only needed pages are read from the file system but
that all subsequent paging is done from swap space.
Some systems attempt to limit the amount of swap space used through
demand paging of binary files. Demand pages for such files are brought directly
from the file system. However, when page replacement is called for, these
frames can simply be overwritten (because they are never modified), and the
pages can be read in from the file system again if needed. Using this approach,
the file system itself serves as the backing store. Howeve1~ swap space must
still be used for pages not associated with a file; these pages include the stack
and heap for a process. This method appears to be a good compromise and is
used in several systems, including Solaris and BSD UNIX.
In Section 9 .2, we illustrated how a process can start quickly by merely demandpaging
in the page containing the first instruction. However, process creation
using the fork() system call may initially bypass the need for demand paging
by using a technique similar to page sharing (covered in Section 8.4.4). This
technique provides for rapid process creation and minimizes the number of
new pages that must be allocated to the newly created process.
368 Chapter 9
physical
Figure 9.7 Before process I modifies page C.
Recall thatthe fork() system call creates a child process that is a duplicate
of its parent. Traditionally, fork() worked by creating a copy of the parent's
address space for the child, duplicating the pages belonging to the parent.
However, considering that many child processes invoke the exec() system
call immediately after creation, the copying of the parent's address space may
be unnecessary. Instead, we can use a technique known as
which works by allowing the parent and child processes initially to share the
same pages. These shared pages are marked as copy-on-write pages, meaning
that if either process writes to a shared page, a copy of the shared page is
created. Copy-on-write is illustrated in Figures 9.7 and Figure 9.8, which show
the contents of the physical memory before and after process 1 modifies page
c.
For example, assume that the child process attempts to modify a page
containing portions of the stack, with the pages set to be copy-on-write. The
operating system will create a copy of this page, nl.apping it to the address space
of the child process. The child process will then modify its copied page and not
the page belonging to the parent process. Obviously, when the copy-on-write
technique is used, only the pages that are modified by either process are copied;
all unmodified pages can be shared by the parent and child processes. Note, too,
process1
physical
memory
Figure 9.8 After process 1 modifies page C.
process2
9.4
9.4 369
that only pages that can be nwdified need be m~arked as copy-on-write. Pages
that cannot be modified (pages containing executable code) can be shared by
the parent and child. Copy-on-write is a common technique used by several
operating systems, including Windows XP, Linux, and Solaris.
When it is determined that a page is going to be duplicated using copyon-
write, it is important to note the location from which the free page will
be allocated. Many operating systems provide a of free pages for such
requests. These free pages are typically allocated when the stack or heap for a
process must expand or when there are copy-on-write pages to be managed.
Operating systems typically allocate these pages using a technique known as
zem-fHl-on-den:1and. Zero-fill-on-demand pages have been zeroed-out before
being allocated, thus erasing the previous contents.
Several versions of UNIX (including Solaris and Linux) provide a variation
ofthe fork() system call-vfork() (for fori()- that operates
differently from fork() with copy-on-write. With vfork(), the parent process
is suspended, and the child process uses the address space of the parent.
Because vfork() does not use copy-on-write, if the child process changes
any pages of the parent's address space, the altered pages will be visible to the
parent once it resumes. Therefore, vf ork () must be used with caution to ensure
that the child process does not modify the address space of the parent. vf or k ()
is intended to be used when the child process calls exec() immediately after
creation. Because no copying of pages takes place, vf ork () is an extremely
efficient method of process creation and is sometimes used to implement UNIX
command-line shell interfaces.
In our earlier discussion of the page-fault rate, we assumed that each page
faults at most once, when it is first referenced. This representation is not strictly
accurate, however. If a process of ten pages actually uses only half of them, then
demand paging saves the I/0 necessary to load the five pages that are never
used. We could also increase our degree of multiprogramming by running
twice as many processes. Thus, if we had forty frames, we could run eight
processes, rather than the four that could run if each required ten frames (five
of which were never used).
If we increase our degree of multiprogramming, we are
memory. If we run six processes, each of which is ten pages in size but
uses only five pages, we have higher CPU utilization and throughput,
ten frames to spare. It is possible, however, that each of these processes, for a
particular data set, may suddenly try to use all ten of its pages, resulting in a
need for sixty frames when only forty are available.
Further, consider that system memory is not used only for holding program
pages. Buffers for I/ 0 also consume a considerable amount of memory. This use
can increase the strain on memory-placement algorithms. Deciding how much
memory to allocate to I/0 and how much to program pages is a significant
challenge. Some systems allocate a fixed percentage of memory for I/0 buffers,
whereas others allow both user processes and the I/0 subsystem to compete
for all system memory.
370 Chapter 9
valid-invalid
PC--::  -_='-~~==: !came f il
logical memory
for user 1
page table
for user 1
valid-invalid
0
frame ~bi~
r---~ v
v
~-------'--'
2
3
logical memory
for user 2
page table
for user 2
0 monitor
2
3
4
5 J
6 A
7 E
physical
memory
Figure 9.9 Need for page replacement
Over-allocation of memory manifests itself as follows. While a user process
is executing, a page fault occurs. The operating system determines where the
desired page is residing on the disk but then finds that there are no free frames
on the free-frame list; all memory is in use (Figure 9.9).
The operating system has several options at this point. It could terminate
the user process. However, demand paging is the operating system's attempt to
improve the computer system's utilization and throughput. Users should not
be aware that their processes are running on a paged system-paging should
be logically transparent to the user. So this option is not the best choice.
The operating system could instead swap out a process, freeing all its
frames and reducing the level of multiprogramming. This option is a good one
in certain circumstances, and we consider it further in Section 9.6. Here, we
discuss the most common solution:
9.4.1 Basic Page Replacement
Page replacement takes the following approach. If no frame is free, we find
one that is not currently being used and free it. We can free a frame by writing
its contents to swap space and changing the page table (and all other tables) to
indicate that the page is no longer in memory (Figure 9.10). We can now use
the freed frame to hold the page for which the process faulted. We modify the
page-fault service routine to include page replacement:
Find the location of the desired page on the disk.
Find a free frame:
a. If there is a free frame, use it.
9.4 371
b. If there is no free frame, use a page-replacement algorithnc to select
a
c. Write the victim frame to the disk; change the page and frame tables
accordingly.
Read the desired page into the newly freed frame; change the page and
frame tables.
Restart the user process.
Notice that, if no frames are free, two page transfers (one out and one in) are
required. This situation effectively doubles the page-fault service time and
increases the effective access time accordingly.
We can reduce this overhead by using a (or When this
scheme is used, each page or frame has a modify bit associated with it in the
hardware. The modify bit for a page is set by the hardware whenever any word
or byte in the page is written into, indicating that the page has been modified.
When we select a page for replacement, we examine its modify bit. If the bit
is set, we know that the page has been modified since it was read in from the
disk. In this case, we must write the page to the disk. If the modify bit is not set,
however, the page has not been modified since it was read into memory. In this
case, we need not write the memory page to the disk: it is already there. This
technique also applies to read-only pages (for example, pages of binary code).
Such pages cannot be modified; thus, they may be discarded when desired.
This scheme can significantly reduce the time required to service a page fault,
since it reduces I/O time by one-half if the page has not been modified.
frame valid-invalid bit
'\. /
physical
memory
Figure 9.10 Page replacement
372 Chapter 9
Page replacement is basic to demand paging. It completes the separation
between logical memory and physical memory. With this mechanism, an
enormous virtual memory can be provided for programn'lers on a smaller
physical memory. With no demand paging, user addresses are mapped into
physical addresses, so the two sets of addresses can be different. All the pages of
a process still must be in physical memory, however. With demand paging, the
size of the logical address space is no longer constrained by physical memory.
If we have a user process of twenty pages, we can execute it in ten frames
simply by using demand paging and using a replacement algorithm to find
a free frame whenever necessary. If a page that has been modified is to be
replaced, its contents are copied to the disk. A later reference to that page will
cause a page fault. At that time, the page will be brought back into memory,
perhaps replacing some other page in the process.
We must solve two major problems to implement demand
develop a algorithm and a '  ''          -  '  '    l  tcemE~lU ~~F  '-'~~  ''H'  
That is, if we have multiple processes in memory, we must decide how many
frames to allocate to each process; and when page replacement is required,
we must select the frames that are to be replaced. Designing appropriate
algorithms to solve these problems is an important task, because disk I/0
is so expensive. Even slight improvements in demand-paging methods yield
large gains in system performance.
There are many different page-replacement algorithms. Every operating
system probably has its own replacement scheme. How do we select a
particular replacement algorithm  In general, we want the one with the lowest
page-fault rate.
We evaluate an algorithm by running it on a particular string of memory
references and computing the number of page faults. The string of memory
references is called a reference We can generate reference strings
artificially (by using a random-number generator, for example), or we can trace
a given system and record the address of each memory reference. The latter
choice produces a large number of data (on the order of 1 million addresses
per second). To reduce the number of data, we use two facts.
First, for a given page size (and the page size is generally fixed by the
hardware or system), we need to consider only the page number, rather than
the entire address. Second, if we have a reference to a page p, then any references
to page p that immediately follow will never cause a page fault. Page p will be in
memory after the first reference, so the immediately following references will
not fault.
For example, if we trace a particular process, we might record the following
address sequence:
0100,0432,0101,0612,0102,0103,0104,0101,0611,0102,0103,
0104,0101,0610,0102,0103,0104,0101,0609,0102,0105
At 100 bytes per page, this sequence is reduced to the following reference
string:
1, 4, 1, 6, 1, 6, 1, 6, 1, 6, 1
9.4 373
16
g) 14
:::J
.;2 12
Q)
Ol cO 10
0..
0 8
'-
Q)
..0 6 E
:::J c 4
2
2 3 4 5 6
number of frames
Figure 9.1 i Graph of page faults versus number of frames.
To determine the number of page faults for a particular reference string and
page-replacement algorithm, we also need to know the number of page frames
available. Obviously, as the number of frames available increases, the number
of page faults decreases. For the reference stril'lg considered previously, for
example, if we had three or more frames, we would have only three faultsone
fault for the first reference to each page. In contrast, with only one frame
available, we would have a replacement with every reference, resulting in
eleven faults. In general, we expect a curve such as that in Figure 9.11. As the
number of frames increases, the number of page faults drops to some minimal
level. Of course, adding physical memory increases the number of frames.
We next illustrate several page-replacement algorithms. In doing so, we
use the reference string
for a memory with three frames.
9.4.2 FIFO Page Replacement
The simplest page-replacement algorithm is a first-in, first-out (FIFO) algorithm.
A FIFO replacement algorithm associates with each page the time when that
page was brought into memory. When a page must be replaced, the oldest
page is chosen. Notice that it is not strictly necessary to record the time when
a page is brought in. We can create a FIFO queue to hold all pages in memory.
We replace the page at the head of the queue. When a page is brought into
memory, we insert it at the tail of the queue.
For our example reference string, our three frames are initially empty. The
first three references (7, 0, 1) cause page faults and are brought into these empty
frames. The next reference (2) replaces page 7, because page 7 was brought in
first. Since 0 is the next reference and 0 is already in memory, we have no fault
for this reference. The first reference to 3 results in replacement of page 0, since
374 Chapter 9
reference string
7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0
page frames
Figure 9.12 FIFO page-replacement algorithm.
it is now first in line. Because of this replacement, the next reference, to 0, will
fault. Page 1 is then replaced by page 0. This process continues as shown in
Figure 9.12. Every time a fault occurs, we show which pages are in our three
frames. There are fifteen faults altogether.
The FIFO page-replacement algorithm is easy to Lmderstand and program.
However, its performance is not always good. On the one hand, the page
replaced may be an initialization module that was used a long time ago and is
no longer needed. On the other hand, it could contain a heavily used variable
that was initialized early and is in constant use.
Notice that, even if we select for replacement a page that is in active use,
everything still works correctly. After we replace an active page with a new
one, a fault occurs almost immediately to retrieve the active page. Some other
page must be replaced to bring the active page back into memory. Thus, a bad
replacement choice increases the page-fault rate and slows process execution.
It does not, however, cause incorrect execution.
To illustrate the problems that are possible with a FIFO page-replacement
algorithm, we consider the following reference string:
1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5
Figure 9.13 shows the curve of page faults for this reference string versus the
number of available frames. Notice that the number of faults for four frames
(ten) is greater than the number of faults for three frames (nine)! This most
unexpected result is known as . for some page-replacement
algorithms, the page-fault rate may increase as the number of allocated frames
increases. We would expect that giving more memory to a process would
improve its performance. In some early research, investigators noticed that
this assumption was not always true. Belady's anomaly was discovered as a
result.
9.4.3 Optimal Page Replacement
of Belady's anomaly was the search for an
which has the lowest page-fault rate of all
algorithms and will never suffer from Belady's anomaly. Such an algorithm
does exist and has been called OPT or MIN. It is simply this:
Replace the page that will not be used
for the longest period of time.
9.4 375
16
~
:::5
2 12
CJ)
mOJ 10
0..
0 8
CJ _o E 6
:::5 c 4
2
number of frames
Figure 9.13 Page-fault curve for FIFO replacement on a reference string.
Use of this page-replacement algorithm guarantees the lowest possible pagefault
rate for a fixed number of frames.
For example, on our sample reference string, the optimal page-replacement
algorithm would yield nine page faults, as shown in Figure 9.14. The first three
references cause faults that fill the three empty frames. The reference to page
2 replaces page 7, because page 7 will not be used until reference 18, whereas
page 0 will be used at 5, and page 1 at 14. The reference to page 3 replaces
page 1, as page 1 will be the last of the three pages in memory to be referenced
again. With only nine page faults, optimal replacement is much better than
a FIFO algorithm, which results in fifteen faults. (If we ignore the first three,
which all algorithms must suffer, then optimal replacement is twice as good as
FIFO replacement.) Irt fact, no replacement algorithm can process this reference
string in three frames with fewer than nine faults.
Unfortunately, the optimal page-replacement algorithm is difficult to
implement, because it requires future knowledge of the reference string. (We
encountered a similar situation with the SJF CPU-schedulin.g algorithm in
Section 5.3.2.) As a result, the optimal algorithm is used mainly for comparison
studies. For instance, it may be useful to know that, although a new algorithm
reference string
7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0
page frames
Figure 9.14 Optimal page-replacement algorithm.
376 Chapter 9
is not optimat it is within 12.3 percent of optimal at worst and within 4.7
percent on average.
9.4.4 LRU Page Replacement
lf the optimal algorithm is not feasible, perhaps an approximation of the
optimal algorithm is possible. The key distinction between the FIFO and OPT
algorithms (other than looking backward versus forward in time) is that the
FIFO algorithm uses the time when a page was brought into memory, whereas
the OPT algorithm uses the time when a page is to be used. If we use the
recent past as an approximation of the near future, then we can replace the
that has not been used for the longest period of time. This approach is the
LRU replacement associates with each page the time of that page's last use.
When a page must be replaced, LRU chooses the page that has not been used
for the longest period of time. We can think of this strategy as the optimal
page-replacement algorithm looking backward in time, rather than forward.
(Strangely, if we let sR be the reverse of a reference stringS, then the page-fault
rate for the OPT algorithm on Sis the same as the page-fault rate for the OPT
algorithm on SR. Similarly, the page-fault rate for the LRU algorithm on Sis the
same as the page-fault rate for the LRU algorithm on sR.)
The result of applying LRU replacement to our example reference string is
shown in Figure 9.15. The LRU algorithm produces twelve faults. Notice that
the first five faults are the same as those for optimal replacement. When the
reference to page 4 occurs, however, LRU replacement sees that, of the three
frames in memory, page 2 was used least recently. Thus, the LRU algorithm
replaces page 2, not knowing that page 2 is about to be used. When it then faults
for page 2, the LRU algorithm replaces page 3, since it is now the least recently
used of the three pages in memory. Despite these problems, LRU replacement
with twelve faults is much better than FIFO replacement with fifteen.
The LRU policy is often used as a page-replacement algorithm and
is considered to be good. The major problem is how to implement LRU
replacement. An LRU page-replacement algorithm may require substantial
hardware assistance. The problem is to determine an order for the frames
defined by the time of last use. Two implementations are feasible:
Counters. In the simplest case, we associate with each page-table entry a
time-of-use field and add to the CPU a logical clock or counter. The clock is
reference string
7 0 2 0 3 0 4 2 3 0 3 2 2 0 7 0
page frames
Figure 9.15 LRU page-replacement algorithm.
9.4 377
incremented for every memory reference. Whenever a reference to a page
is made, the contents of the clock register are copied to the ti1ne-of-use
field in the page-table entry for that page. In this way, we always have
the   time   of the last reference to each page. We replace the page with the
smallest time value. This scheme requires a search of the page table to find
the LRU page and a write to memory (to the time-of-use field in the page
table) for each memory access. The times must also be m~aintained when
page tables are changed (due to CPU scheduling). Overflow of the clock
must be considered.
Stack Another approach to implementing LRU replacement is to keep
a stack of page numbers. Whenever a page is referenced, it is removed
from the stack and put on the top. In this way, the most recently used
page is always at the top of the stack and the least recently used page is
always at the bottom (Figure 9.16). Because entries must be removed from
the middle of the stack, it is best to implement this approach by using a
doubly linked list with a head pointer and a tail pointer. Removing a page
and putting it on the top of the stack then requires changing six pointers
at worst. Each update is a little more expensive, but there is no search for
a replacement; the tail pointer points to the bottom of the stack, which is
the LRU page. This approach is particularly appropriate for software or
microcode implementations of LRU replacement.
Like optimal replacement, LRU replacement does not suffer from Belady's
Both belong to a class of page-replacement algorithms, called si:ack
that can never exhibit Belady's anomaly. A stack algorithm is
an algorithm for which it can be shown that the set of pages in memory for n
frames is always a subset of the set of pages that would be in memory with n + 1
frames. For LRU replacement, the set of pages in memory would be the n most
recently referenced pages. If the number of frames is increased, these n pages
will still be the most recently referenced and so will still be in memory.
Note that neither implementation of LRU would be conceivable without
hardware assistance beyond the standard TLB registers. The updating of the
reference string
4 7 0 7
stack
before
a
0 2
stack
after
b
2 7 2
i l
a b
Figure 9.16 Use of a stack to record the most recent page references.
378 Chapter 9
clock fields or stack must be done for every memory reference. If we were to
use an interrupt for every reference to allow software to update such data
structures, it would slow every memory reference by a factor of at least ten,
hence slowing every user process by a factor of ten. Few systems could tolerate
that level of overhead for memory management.
9.4.5 LRU-Approximation Page Replacement
Few computer systems provide sufficient hardware support for true LRU page
replacement. Some systems provide no hardware support, and other pagereplacement
algorithms (such as a FIFO algorithm) must be used. Many systems
provide some help, however, in the form of a The reference bit
for a page is set by the hardware whenever that page is referenced (either a
read or a write to any byte in the page). Reference bits are associated with each
entry in the page table.
Initially, all bits are cleared (to 0) by the operating system. As a user process
executes, the bit associated with each page referenced is set (to 1) by the
hardware. After some time, we can determine which pages have been used and
which have not been used by examining the reference bits, although we do not
know the order of use. This information is the basis for many page-replacement
algorithms that approximate LRU replacement.
9.4.5.1 Additional-Reference-Bits Algorithm
We can gain additional ordering information by recording the reference bits at
regular intervals. We can keep an 8-bit byte for each page in a table in memory.
At regular intervals (say, every 100 milliseconds), a timer interrupt transfers
control to the operating system. The operating system shifts the reference bit
for each page into the high-order bit of its 8-bit byte, shifting the other bits right
by 1 bit and discarding the low-order bit. These 8-bit shift registers contain the
history of page use for the last eight time periods. If the shift register contains
00000000, for example, then the page has not been used for eight time periods;
a page that is used at least once in each period has a shift register value of
11111111. A page with a history register value of 11000100 has been used more
recently than one with a value of 01110111. If we interpret these 8-bit bytes
as unsigned integers, the page with the lowest number is the LRU page, and
it can be replaced. Notice that the numbers are not guaranteed to be unique,
however. We can either replace (swap out) all pages with the smallest value or
use the FIFO method to choose among them.
The number of bits of history included in the shift register can be varied,
of course, and is selected (depending on the hardware available) to make
the updating as fast as possible. In the extreme case, the number can be
reduced to zero, leaving only the reference bit itself. This algorithm is called
the
9.4.5.2 Second-Chance Algorithm
The basic algorithm of second-chance replacement is a FIFO replacement
algorithm. When a page has been selected, however, we inspect its reference
bit. If the value is 0, we proceed to replace this page; but if the reference bit
is set to 1, we give the page a second chance and move on to select the next
next
victim
9.4
reference pages reference pages
bits bits
circular queue of pages circular queue of pages
(a) (b)
Figure 9.17 Second-chance (clock) page-replacement algorithm.
379
FIFO page. When a page gets a second chance, its reference bit is cleared, and
its arrival time is reset to the current time. Thus, a page that is given a second
chance will not be replaced until all other pages have been replaced (or given
second chances). In addition, if a page is used often enough to keep its reference
bit set, it will never be replaced.
One way to implement the second-chance algorithm (sometimes referred
to as the clock algorithm) is as a circular queue. A poi11ter (that is, a hand on
the clock) indicates which page is to be replaced next. When a frame is needed,
the pointer advances until it finds a page with a 0 reference bit. As it advances,
it clears the reference bits (Figure 9.17). Once a victim page is found, the page
is replaced, and the new page is inserted in the circular queue in that position.
Notice that, in the worst case, when all bits are set, the pointer cycles through
the whole queue, giving each page a second chance. It clears all the reference
bits before selecting the next page for replacement. Second-chance replacement
degenerates to FIFO replacement if all bits are set.
9.4.5.3 Enhanced Second-Chance Algorithm
We can enhance the second-chance algorithm by considering the reference bit
and the modify bit (described in Section 9.4.1) as an ordered pair. With these
two bits, we have the following four possible classes:
(0, 0) neither recently used nor modified -best page to replace
380 Chapter 9
(0, 1) not recently used hut modified-not quite as good, because the
page will need to be written out before replacement
(1, 0) recently used but clean-probably will be used again soon
(1, 1) recently used and modified -probably will be used again soon, and
the page will be need to be written out to disk before it can be replaced
Each page is in one of these four classes. When page replacement is called for,
we use the same scheme as in the clock algorithm; but instead of examining
whether the page to which we are pointing has the reference bit set to 1,
we examine the class to which that page belongs. We replace the first page
encountered in the lowest nonempty class. Notice that we may have to scan
the circular queue several times before we find a page to be replaced.
The major difference between this algorithm and the simpler clock algorithm
is that here we give preference to those pages that have been modified
to reduce the number of I/Os required.
9.4.6 Counting-Based Page Replacement
There are many other algorithms that can be used for page replacement. For
example, we can keep a counter of the number of references that have been
made to each page and develop the following two schemes.
The least frequently used (LFU) page-replacement algorithm requires
that the page with the smallest count be replaced. The reason for this
selection is that an actively used page should have a large reference count.
A problem arises, however, when a page is used heavily during the initial
phase of a process but then is never used again. Since it was used heavily,
it has a large count and remains in memory even though it is no longer
needed. One solution is to shift the counts right by 1 bit at regular intervals,
forming an exponentially decaying average usage count.
The most frequently used (MFU) page-replacement algorithm is based
on the argument that the page with the smallest count was probably just
brought in and has yet to be used.
As you might expect, neither MFU nor LFU replacement is common. The
implementation of these algorithms is expensive, and they do not approxin'late
OPT replacement well.
9.4.7 Page-Buffering Algorithms
Other procedures are often used in addition to a specific page-replacement
algorithm. For example, systems commonly keep a pool of free frames. When
a page fault occurs, a victim frame is chosen as before. However, the desired
page is read into a free frame from the pool before the victim is written out. This
procedure allows the process to restart as soon as possible, without waiting
for the victim page to be written out. When the victim is later written out, its
frame is added to the free-frame pool.
9.4 381
An expansion of this idea is to maintain a list of modified pages. Whenever
the paging device is idle, a modified page is selected and is written to the disk.
Its modify bit is then reset. This scheme increases the probability that a page
will be clean when it is selected for replacement and will not need to be written
out.
Another modification is to keep a pool of free frames but to remember
which page was in each frame. Since the frame contents are not modified when
a frame is written to the disk, the old page can be reused directly fronc the
free-frame pool if it is needed before that frame is reused. No I/O is needed in
this case. When a page fault occurs, we first check whether the desired page is
in the free-frame pool. If it is not, we must select a free frame and read into it.
This technique is used in the VAX/VMS system along with a FIFO replacement
algorithm. When the FIFO replacement algorithm mistakenly replaces a
page that is still in active use, that page is quickly retrieved from the free-frame
pool, and no I/O is necessary. The free-frame buffer provides protection against
the relatively poor, but sirnple, FIFO replacement algorithm. This method is
necessary because the early versions of VAX did not implement the reference
bit correctly.
Some versions of the UNIX system use this method in conjunction with
the second-chance algorithm. It can be a useful augmentation to any pagereplacement
algorithm, to reduce the penalty incurred if the wrong victim
page is selected.
9.4.8 Applications and Page Replacement
In certain cases, applications accessing data through the operating system's
virtual memory perform worse than if the operating system provided no
buffering at all. A typical example is a database, which provides its own
memory management and I/0 buffering. Applications like this understand
their memory use and disk use better than does an operating system that is
implementing algorithms for general-purpose use. If the operating system is
buffering I/0, and the application is doing so as well, then twice the memory
is being used for a set of I/0.
In another example, data warehouses frequently perform massive sequential
disk reads, followed by computations and writes. The LRU algorithm would
be removing old pages and preserving new ones, while the application would
more likely be reading older pages than newer ones (as it starts its sequential
reads again). Here, MFU would actually be more efficient than LRU.
Because of such problems, some operating systems give special programs
the ability to use a disk partition as a large sequential array of logical blocks,
without any file-system data structures. This array is sometimes called the raw
disk, and I/O to this array is termed raw I/0. Raw I/0 bypasses all the filesystem
services, such as file I/0 demand paging, file locking, prefetching, space
allocation, file names, and directories. Note that although certain applications
are more efficient when implementing their own special-purpose storage
services on a raw partition, most applications perform better when they use
the regular file-system services.
382 Chapter 9
9.5
We turn next to the issue of allocation. How do we allocate the fixed amount
of free memory among the various processes  If we have 93 free frames and
two processes, how many frames does each process get 
The simplest case is the single-user system. Consider a single-user system
with 128 KB of memory composed of pages 1 KB in size. This system has 128
frames. The operating system may take 35 KB, leaving 93 frames for the user
process. Under pure demand paging, all 93 frames would initially be put on
the free-frame list. When a user process started execution, it would generate a
sequence of page faults. The first 93 page faults would all get free frames from
the free-frame list. When the free-frame list was exhausted, a page-replacement
algorithm would be used to select one of the 93 in-memory pages to be replaced
with the 94th, and so on. When the process terminated, the 93 frames would
once again be placed on the free-frame list.
There are many variations on this simple strategy. We can require that the
operating system allocate all its buffer and table space from the free-frame list.
When this space is not in use by the operating system, it can be used to support
user paging. We can try to keep three free frames reserved on the free-frame list
at all times. Thus, when a page fault occurs, there is a free frame available to
page into. While the page swap is taking place, a replacement can be selected,
which is then written to the disk as the user process continues to execute.
Other variants are also possible, but the basic strategy is clear: the user process
is allocated any free frame.
9.5.1 Minimum Number of Frames
Our strategies for the allocation of frames are constrained in various ways. We
cannot, for example, allocate more than the total number of available frames
(unless there is page sharing). We must also allocate at least a minimum number
of frames. Here, we look more closely at the latter requirement.
One reason for allocating at least a minimum number of frames involves
performance. Obviously, as the number of frames allocated to each process
decreases, the page-fault rate increases, slowing process execution. In addition,
remember that when a page fault occurs before an executing ilcstruction
is complete, the instruction must be restarted. Consequently. we must have
enough frames to hold all the different pages that any single ilcstruction can
reference.
For example, consider a machine in which all memory-reference instructions
may reference only one memory address. In this case, we need at least one
frame for the instruction and one frame for the mernory reference. In addition,
if one-level indirect addressing is allowed (for example, a load instruction on
page 16 can refer to an address on page 0, which is an indirect reference to page
23), then paging requires at least three frames per process. Think about what
might happen if a process had only two frames.
The minimum number of frames is defined by the computer architecture.
For example, the move instruction for the PDP-11 includes more than one word
for some addressing modes, and thus the ilcstruction itself may straddle two
pages. In addition, each of its two operands may be indirect references, for a
total of six frames. Another example is the IBM 370 MVC instruction. Since the
9.5 383
instruction is from storage location to storage location, it takes 6 bytes and can
straddle two pages. The block of characters to move and the area to which it
is to be m.oved can each also straddle two pages. This situation would require
six frames. The worst case occurs when the MVC instruction is the operand of
an EXECUTE instruction that straddles a page boundary; in this case, we need
eight frames.
The worst-case scenario occurs in computer architectures that allow
multiple levels of indirection (for example, each 16-bit word could contain
a 15-bit address plus a 1-bit indirect indicator). Theoretically, a simple load
instruction could reference an indirect address that could reference an indirect
address (on another page) that could also reference an indirect address (on yet
another page), and so on, until every page in virtual memory had been touched.
Thus, in the worst case, the entire virtual memory must be in physical memory.
To overcome this difficulty, we must place a limit on the levels of indirection (for
example, limit an instruction to at most 16levels of indirection). When the first
indirection occurs, a counter is set to 16; the counter is then decremented for
each successive irtdirection for this instruction. If the counter is decremented to
0, a trap occurs (excessive indirection). This limitation reduces the maximum
number of memory references per instruction to 17, requiring the same number
of frames.
Whereas the minimum number of frames per process is defined by the
architecture, the maximum number is defined by the amount of available
physical memory. In between, we are still left with significant choice in frame
allocation.
9.5.2 Allocation Algorithms
The easiest way to split m frames among n processes is to give everyone an
equal share, m/n frames. For instance, if there are 93 frames and five processes,
each process will get 18 frames. The three leftover frames can be used as a
free-frame buffer pool. This scheme is called
An alternative is to recognize that various processes will need differing
amounts of memory. Consider a system with a 1-KB frame size. If a small
student process of 10 KB and an interactive database of 127 KB are the only
two processes running in a system with 62 free frames, it does not make much
sense to give each process 31 frames. The student process does not need more
than 10 frames, so the other 21 are, strictly speaking, wasted.
To solve this problem, we can use in which we
allocate available memory to each process according to its size. Let the size of
the virtual memory for process p; be s;, and define
S=  Ls;.
Then, if the total number of available frames is m, we allocate a; frames to
process p;, where a; is approximately
a;= s;/S x m.
384 Chapter 9
Of course, we must adjust each ai to be an integer that is greater than the
ncinimum number of frames required by tl1e instruction set, with a sum not
exceeding m.
With proportional allocation, we would split 62 frames between two
processes, one of 10 pages and one of 127 pages, by allocating 4 frames and 57
frames, respectively, since
10/137 x 62 ~ 4, and
127/137 X 62 ~57.
In this way, both processes share the available frames according to their
  needs,   rather than equally.
In both equal and proportional allocation, of course, the allocation may
vary according to the multiprogramming level. If the multiprogramming level
is increased, each process will lose some frames to provide the memory needed
for the new process. Conversely, if the multiprogramming level decreases, the
frames that were allocated to the departed process can be spread over the
remaining processes.
Notice that, with either equal or proportional allocation, a high-priority
process is treated the same as a low-priority process. By its definition, however,
we may want to give the high-priority process more memory to speed its
execution, to the detriment of low-priority processes. One solution is to use
a proportional allocation scheme wherein the ratio of frames depends not on
the relative sizes of processes but rather on the priorities of processes or on a
combination of size and priority.
9.5.3 Global versus Local Allocation
Another important factor in the way frames are allocated to the various
processes is page replacement. With multiple processes competing for frames,
we can classify page-replacement algorithms into two broad categories:
;.no' '-c'u~''   and local Global replacement allows a process to
a replacement frame from the set of all frames, even if that frame is
currently allocated to some other process; that is, one process can take a frame
from another. Local replacement requires that each process select from only its
own set of allocated frames.
For example, consider an allocation scheme wherein we allow high-priority
processes to select frames from low-priority processes for replacement. A
process can select a replacement from among its own frames or the frames
of any lower-priority process. This approach allows a high-priority process to
increase its frame allocation at the expense of a low-priority process. With a
local replacement strategy, the number of frames allocated to a process does not
change. With global replacement, a process may happen to select only frames
allocated to other processes, thus increasing the number of frames allocated to
it (assuming that other processes do not choose its frames for replacement).
One problem with a global replacement algorithm is that a process cannot
control its own page-fault rate. The set of pages in memory for a process
depends not only on the paging behavior of that process but also on the paging
behavior of other processes. Therefore, the same process may perform quite
9.5 385
differently (for example, taking 0.5 seconds for one execution and 10.3 seconds
for the next execution) because of totally external circuntstances. Such is not
the case with a local replacement algorithm. Under local replacement, the
set of pages in memory for a process is affected by the paging behavior of
only that process. Local replacement might hinder a process, however, by
not making available to it other, less used pages of memory. Thus, global
replacement generally results in greater system throughput and is therefore
the more common method.
9.5.4 Non-Uniform Memory Access
Thus far in our coverage of virtual memory, we have assumed that all main
memory is created equal-or at least that it is accessed equally. On many
computer systems, that is not the case. Often, in systems with multiple CPUs
(Section 1.3.2), a given CPU can access some sections of main memory faster
than it can access others. These performance differences are caused by how
CPUs and memory are interconnected in the system. Frequently, such a system
is made up of several system boards, each containing multiple CPUs and some
memory. The system boards are interconnected in various ways, ranging from
system busses to high-speed network connections like InfiniBand. As you
might expect, the CPUs on a particular board can access the memory on that
board with less delay than they can access memory on other boards in the
system. Systems in which memory access times vary significantly are known
collectively as systems, and without
exception, they are slower than systems in which memory and CPUs are located
on the same motherboard.
Managing which page frames are stored at which locations can significantly
affect performance in NUMA systems. If we treat memory as uniform in such
a system, CPUs may wait significantly longer for memory access than if we
modify memory allocation algorithms to take NUMA into account. Similar
changes must be rnade to the scheduling system. The goal of these changes is
to have memory frames allocated   as close as possible   to the CPU on which
the process is running. The definition of   close   is   with minimum latency,  
which typically means on the same system board as the CPU.
The algorithmic changes consist of having the scheduler track the last CPU
on which each process ran. If the scheduler tries to schedule each process onto
its previous CPU, and the memory-management system tries to allocate frames
for the process close to the CPU on which it is being scheduled, then improved
cache hits and decreased memory access times will result.
The picture is more complicated once threads are added. For example, a
process with many running threads may end up with those threads scheduled
on many different system boards. How is the memory to be allocated in this
case  Solaris solves the problem by creating an entity in the kernel. Each
lgroup gathers together close CPUs and memory. In fact, there is a hierarchy
of lgroups based on the amount of latency between the groups. Solaris tries to
schedule all threads of a process and allocate all memory of a process within
an lgroup. If that is not possible, it picks nearby lgroups for the rest of the
resources needed. In this manner, overall memory latency is minimized, and
CPU cache hit rates are maximized.
386 Chapter 9
9.6
If the number of frames allocated to a low-priority process falls below the
minimum number required by the computer architecture, we must suspend
that process's execution. We should then page out its remaining pages, freeing
all its allocated frames. This provision introduces a swap-in, swap-out level of
intermediate CPU scheduling.
In fact, look at any process that does not have   enough   frames. If the
process does not have the num.ber of frames it needs to support pages in
active use, it will quickly page-fault. At this point, it must replace some page.
However, since all its pages are in active use, it must replace a page that will
be needed again right away. Consequently, it quickly faults again, and again,
and again, replacing pages that it must back in immediately.
This high paging activity is called A process is thrashing if it is
spending more time paging than executing.
9.6.1 Cause of Thrashing
Thrashing results in severe performance problems. Consider the following
scenario, which is based on the actual behavior of early paging systems.
The operating system monitors CPU utilization. If CPU utilization is too low,
we increase the degree of multiprogramming by introducing a new process
to the system. A global page-replacement algorithm is used; it replaces pages
without regard to the process to which they belong. Now suppose that a process
enters a new phase in its execution and needs more frames. It starts faulting and
taking frames away from other processes. These processes need those pages,
however, and so they also fault, taking frames from other processes. These
faulting processes must use the pagin.g device to swap pages in and out. As
they queue up for the paging device, the ready queue empties. As processes
wait for the paging device, CPU utilization decreases.
The CPU scheduler sees the decreasing CPU utilization and increases the
degree of multiprogramming as a result. The new process tries to get started
by taking frames from running processes, causing more page faults and a longer
queue for the paging device. As a result, CPU utilization drops even further,
and the CPU scheduler tries to increase the degree of multiprogramming even
more. Thrashing has occurred, and system throughput plunges. The pagefault
rate increases tremendously. As a result, the effective m.emory-access
time increases. No work is getting done, because the processes are spending
all their time paging.
This phenomenon is illustrated in Figure 9.18, in which CPU utilization
is plotted against the degree of multiprogramming. As the degree of multiprogramming
increases, CPU utilization also ilccreases, although more slowly,
until a maximum is reached. If the degree of multiprogramming is increased
even further, thrashing sets in, and CPU utilization drops sharply. At this point,
to increase CPU utilization and stop thrashing, we must decrease the degree of
multiprogramming.
We can limit the effects of thrashing by using a
(or With local replacement, if one process
starts thrashing, it cannot frames from another process and cause the latter
to thrash as well. However, the problem is not entirely solved. If processes are
9.6 387
degree of multiprogramming
Figure 9.18 Thrashing.
thrashing, they will be in the queue for the paging device most of the time. The
average service time for a page fault will increase because of the longer average
queue for the paging device. Thus, the effective access time will increase even
for a process that is not thrashing.
To prevent thTashing, we must provide a process with as many frames as
it needs. But how do we know how many frames it   needs    There are several
teclmiques. The working-set strategy (Section 9.6.2) starts by looking at how
frames a process is actually using. This approach defines the locality
of process execution.
The locality model states that, as a process executes, it moves from locality
to locality. A locality is a set of pages that are actively used together (Figure
9.19). A program is generally composed of several different localities, which
may overlap.
For example, when a function is called, it defines a new locality. In this
locality, memory references are made to the instructions of the function call, its
local variables, and a subset of the global variables. When we exit the function,
the process leaves this locality, since the local variables and instructions of the
function are no longer in active use. We may return to this locality later.
Thus, we see that localities are defined by the program structure and its
data structures. The locality model states that all programs will exhibit this
basic memory reference structure. Note that the locality model is the unstated
principle behind the caching discussions so far in this book If accesses to any
types of data were random rather than patterned, caching would be useless.
Suppose we allocate enough frames to a process to accommodate its current
locality. It will fault for the pages in its locality until all these pages are in
memory; then, it will not fault again until it changes localities. If we do not
allocate enough frames to accommodate the size of the current locality, the
process will thrash, since it cannot keep in memory all the pages that it is
actively using.
9.6.2 Working-Set Model
As mentioned, the is based on the assumption of locality.
This model uses a paramete1~ /':,, to define the vrindovv. The idea
388 Chapter 9
32~~----~~==~~~~~WL~~#-~~--~~~-
\jjl:jlli111
28
(j)
(j)
(!:  
    0
    0
(lj 26 I' c
0 I  
E I
(lJ
E
execution time -------..
Figure 9.19 Locality in a memory-reference pattern.
is to examine the most recent 6 references. The set of pages in the most
recent 6 page references is the (Figure 9.20). If a page is in active
use, it will be in the working set. If it is no longer being used, it will drop from
the working set 6 time units after its last reference. Thus, the working set is an
approximation of the program's locality.
For example, given the sequence of memory references shown in Figure
9.20, if 6 = 10 memory references, then the working set at time t1 is {1, 2, 5,
6, 7}. By time t2, the working set has changed to {3, 4}.
The accuracy of the working set depends on the selection of 6. If 6 is too
small, it will not encompass the entire locality; if 6 is too large, it may overlap
9.6
page reference table
. . . 2 6 1 5 7 7 7 7 5 1 6 2 3 4 1 2 3 4 4 4 3 4 3 4 4 4 1 3 2 3 4 4 4 3 4 4 4 .
~ ~r ~ r
t1
WS(t1) = {1 ,2,5,6,7}
Figure 9.20 Working-set model.
389
several localities. In the extrem.e, if L. is infinite, the working set is the set of
pages touched during the process execution.
The most important property of the working set, then, is its size. If we
compute the working-set size, WSS;, for each process in the system, we can
then consider that
where Dis the total demand for frames. Each process is actively using the pages
in its working set. Thus, process i needs WSS; frames. If the total demand is
greater than the total number of available frames (D    m), thrashing will occur,
because some processes will not have enough frames.
Once L. has been selected, use of the working-set model is simple. The
operating system monitors the working set of each process and allocates to
that working set enough frames to provide it with its working-set size. If there
are enough extra frames, another process can be initiated. If the sum of the
working-set sizes increases, exceeding the total number of available frames,
the operating system selects a process to suspend. The process's pages are
written out (swapped), and its frames are reallocated to other processes. The
suspended process can be restarted later.
This working-set strategy prevents thrashing while keeping the degree of
multiprogramming as high as possible. Thus, it optimizes CPU utilization.
The difficulty with the working-set model is keeping track of the working
set. The working-set window is a moving window. At each memory reference,
a new reference appears at one end and the oldest reference drops off the other
end. A page is in the working set if it is referenced anywhere in the working-set
window.
We can approximate the working-set model with a fixed-interval timer
interrupt and a reference bit. For example, assum.e that L. equals 10,000
references and that we can cause a timer interrupt every 5,000 references.
When we get a timer interrupt, we copy and clear the reference-bit values for
each page. Thus, if a page fault occurs, we can examine the current reference
bit and two in-memory bits to determine whether a page was used within the
last 10,000 to 15,000 references. If it was used, at least one of these bits will be
on. If it has not been used, these bits will be off. Those pages with at least one
bit on will be considered to be in the working set. Note that this arrangement
is not entirely accurate, because we cannot tell where, within an interval of
5,000, a reference occurred. We can reduce the uncertainty by increasing the
number of history bits and the frequency of interrupts (for example, 10 bits
and interrupts every 1,000 references). However, the cost to service these more
frequent interrupts will be correspondingly higher.
390 Chapter 9
9.7
number of frames
Figure 9.21 Page-fault frequency.
9.6.3 Page-Fault Frequency
The working-set model is successful, and knowledge of the working set can
be useful for prepaging (Section 9.9.1), but it seems a clumsy way to control
thrashilcg. A strategy that uses the takes a more
direct approach.
The specific problem is how to prevent thrashilcg. Thrashing has a high
page-fault rate. Thus, we want to control the page-fault rate. When it is too
high, we know that the process needs more frames. Conversely, if the page-fault
rate is too low, then the process may have too many frames. We can establish
upper and lower bounds on the desired page-fault rate (Figure 9.21). If the
actual page-fault rate exceeds the upper limit, we allocate the process another
frame; if the page-fault rate falls below the lower limit, we remove a frame
from the process. Thus, we can directly measure and control the page-fault
rate to prevent thrashing.
As with the working-set strategy, we may have to suspend a process. If the
page-fault rate ilccreases and no free frames are available, we must select some
process and suspend it. The freed frames are then distributed to processes with
high page-fault rates.
Consider a sequential read of a file on disk using the standard system calls
open (),read (), and write (). Each file access requires a system call and disk
access. Alternatively, we can use the virtual memory techniques discussed
so far to treat file I/0 as routine memory accesses. This approach, known as
a file, allows a part of the virtual address space to be logically
associated with the file. As we shall see, this can lead to significant performance
increases when performing I/0.
9.7 391
WORKING SETS AND PAGE FAULTRATES
There is a directrelationship between the working set of a process and its
page-fault rate. Typically as shown in Figure 9.20, the working set ofa process
changes pver time as references to. data and code sections move from one
locality to another. Assuming there is sufficient memory to store the working
set of .a process (that is, the processis 11.ot thrashing), tbe page-fault rate of
the process will transition between peaks and valleys over time. This general
behavior is shown in Figure 9.22.
page
fault
rate
working set
time
Figure 9.22 Page fault rate over time.
A peak in the page-fault rate occurs when we begin demand-paging a new
locality. However, once the working set of this new locality is in memory,
the page-fault rate falls. When the process moves to a new working set, the
page..:fault rate rises toward a peak once again, returning to a lower rate once
the new working set is loaded into memory. The span oftime between the
start of one peak and the start of thenext peak represents the transition from
one working set to another.
9.7.1 Basic Mechanism
Memory mapping a file is accomplished by mapping a disk block to a page (or
pages) in memory. Initial access to the file proceeds through ordinary demand
paging, resulting in a page fault. However, a page-sized portion of the file
is read from the file system into a physical page (some systems may opt
to read in more than a page-sized chunk of memory at a time). Subsequent
reads and writes to the file are handled as routine memory accesses, thereby
simplifying file access and usage by allowing the system to manipulate files
through memory rather than incurring the overhead of using the read () and
write() system calls. Similarly, as file l/0 is done in memory- as opposed
to using system calls that involve disk I/0 - file access is much faster as well.
Note that writes to the file mapped in memory are not necessarily
imm.ediate (synchronous) writes to the file on disk. Some systems may choose
to update the physical file when the operating system periodically checks
392 Chapter 9
whether the page in memory has been modified. When the file is closed, all the
memory-mapped data are written back to disk and ren  loved from the virtual
memory of the process.
Some operating systems provide memory mapping only through a specific
system call and use the standard system calls to perform all other file I/0.
However, some systems choose to memory-map a file regardless of whether
the file was specified as memory-mapped. Let's take Solaris as an example. If
a file is specified as memory-mapped (using the mmap () system call), Solaris
maps the file into the address space of the process. If a file is opened and
accessed using ordinary system calls, such as open(), read(), and write(),
Solaris still memory-maps the file; however, the file is mapped to the kernel
address space. Regardless of how the file is opened, then, Solaris treats all
file I/0 as memory-mapped, allowing file access to take place via the efficient
memory subsystem.
Multiple processes may be allowed to map the same file concurrently,
to allow sharing of data. Writes by any of the processes modify the data in
virtual memory and can be seen by all others that map the same section of
the file. Given our earlier discussions of virtual memory, it should be clear
how the sharing of memory-mapped sections of memory is implemented:
the virtual memory map of each sharing process points to the same page of
physical memory-the page that holds a copy of the disk block This memory
sharing is illustrated in Figure 9.23. The memory-mapping system calls can
also support copy-on-write functionality, allowing processes to share a file in
read-only mode but to have their own copies of any data they modify. So that
r---
I
I I
1 - r - - ;
I I 1- 1
I -1- II
I I I I I
J---r' -rL..-r
I I I I -r-' I I I I
  1 -1 I I 1- _,.. I I
I I I f-+-=-.:.....c.c~..-'---r~-, I I
- .L-   I   J I I
I 1 I
I I I I I I
I I I L_ ~ I
process A 1 1 1
virtual memory: ~ 1 -
disk file
Figure 9.23 Memory-mapped files.
process B
virtual memory
9.7
memory-mapped
file
Figure 9.24 Shared memory in Windows using memory-mapped 1/0.
393
access to the shared data is coordinated, the processes involved might use one
of the mechanisms for achieving mutual exclusion described in Chapter 6.
In many ways, the sharing of memory-mapped files is similar to shared
memory as described in Section 3.4.1. Not all systems use the same mechanism
for both; on UNIX and Linux systems, for example, memory mapping is
accomplished with the mmap () system call, whereas shared memory is achieved
with the POSIX-compliant shmget () and shmat () systems calls (Section
3.5.1). On Windows NT, 2000, and XP systems, howeve1~ shared memory is
accomplished by memory mapping files. On these systems, processes can
communicate using shared memory by having the communicating processes
memory-map the same file into their virtual address spaces. The memorymapped
file serves as the region of shared memory between the communicating
processes (Figure 9.24). In the following section, we illustrate support in the
Win32 API for shared memory using memory-mapped files.
9.7.2 Shared Memory in the Win32 API
The general outline for creating a region of shared memory using memorymapped
files in the Win32 API involves first creating a file mapping for the file
to be mapped and then establishing a view of the mapped file in a process's
virtual address space. A second process can then open and create a view of
the mapped file in its virtual address space. The mapped file represents the
shared-menwry object that will enable communication to take place between
the processes.
We next illustrate these steps in more detail. In this example, a producer
process first creates a shared-memory object using the memory-mapping
features available in the Win32 API. The producer then writes a message
to shared m.emory. After that, a consumer process opens a mapping to the
shared-memory object and reads the message written by the consum.er.
To establish a memory-mapped file, a process first opens the file to be
mapped with the CreateFile () function, which returns a HANDLE to the
opened file. The process then creates a mapping of this file HANDLE using
the CreateFileMapping() function. Once the file mapping is established, the
process then establishes a view of the mapped file in its virtual address space
with the MapViewDfFile () function. The view of the mapped file represents
the portion of the file being mapped in the virtual address space of the process
394 Chapter 9
#include   windows.h  
#include   stdio.h  
int main(int argc, char   argv[])
{
}
HANDLE hFile, hMapFile;
LPVOID lpMapAddress;
hFile = CreateFile(  temp.txt  , //file name
GENERICJREAD I GENERIC_WRITE, // read/write access
0, II no sharing of the file
NULL, //default security
OPEN_ALWAYS, //open new or existing file
FILE_ATTRIBUTE_NORMAL, //routine file attributes
NULL); //no file template
hMapFile = CreateFileMapping(hFile, //file handle
NULL, //default security
PAGEJREADWRITE, //read/write access to mapped pages
0, II map entire file
0,
TEXT(  SharedObject  )); //named shared memory object
lpMapAddress = MapViewDfFile(hMapFile, //mapped object handle
FILEJMAP_ALL_ACCESS, //read/write access
0, II mapped view of entire file
0,
0);
II write to shared memory
sprintf(lpMapAddress,  Shared memory message  );
UnmapViewOfFile(lpMapAddress);
CloseHandle(hFile);
CloseHandle(hMapFile);
Figure 9.25 Producer writing to shared memory using the Win32 API.
-the entire file or only a portion of it may be mapped. We illustrate this
sequence in the program shown in Figure 9 .25. (We eliminate much of the error
checking for code brevity.)
The call to CreateFileMapping() creates a named shared-memory object
called SharedObj ect. The consumer process will communicate using this
shared-memory segment by creating a mapping to the same named object.
The producer then creates a view of the memory-mapped file in its virtual
address space. By passing the last three parameters the value 0, it indicates
that the mapped view is the entire file. It could instead have passed values
specifying an offset and size, thus creating a view containing only a subsection
of the file. (It is important to note that the entire mapping may not be loaded
#include   windows.h  
#include   stdio.h  
int main(int argc, char   argv[])
{
HANDLE hMapFile;
LPVOID lpMapAddress;
9.7 395
hMapFile = OpenFileMapping(FILE_MAP_ALL_ACCESS, // R/W access
FALSE, //no inheritance
}
TEXT(  SharedObject  )); //name of mapped file object
lpMapAddress = MapViewOfFile(hMapFile, //mapped object handle
FILEJMAP_ALL_ACCESS, //read/write access
0, II mapped view of entire file
0,
0);
II read from shared memory
printf(  Read message %s  , lpMapAddress);
UnmapViewOfFile(lpMapAddress);
CloseHandle(hMapFile);
Figure 9.26 Consumer reading from shared memory using the Win32 API.
into memory when the mapping is established. Rather, the mapped file may be
demand-paged, thus bringing pages into memory only as they are accessed.)
The MapViewOfFile () fm1ction returns a pointer to the shared-memory object;
any accesses to this memory location are thus accesses to the memory-mapped
file. In this ii1stance, the producer process writes the message   Shared memory
message   to shared memory.
A program illustrating how the consumer process establishes a view of
the named shared-memory object is shown in Figure 9.26. This program is
somewhat simpler than the one shown in Figure 9.25, as all that is necessary
is for the process to create a mapping to the existii1g named shared-memory
object. The consumer process must also create a view of the mapped file, just
as the producer process did ii1 the program in Figure 9.25. The consumer then
reads from shared memory the message   Shared memory message   thatwas
written by the producer process.
Finally, both processes remove the view of the mapped file with a call to
UnmapViewOfFile (). We provide a programming exercise at the end of this
chapter using shared memory with memory mapping in the Win32 API.
9.7.3 Memory-Mapped i/0
In the case of I/0, as mentioned in Section 1.2.1, each I/0 controller includes
registers to hold commands and the data being transferred. Usually, special I/0
instructions allow data transfers between these registers and system memory.
396 Chapter 9
9.8
To allow more convenient access to I/0 devices1 many computer architectures
provide In this case/ ranges of memory addresses are
set aside and are mapped to the device registers. Reads and writes to these
memory addresses cause the data to be transferred to and from the device
registers. This method is appropriate for devices that have fast response times/
such as video controllers. In the IBM PC each location on the screen is mapped
to a n1.emory location. Displaying text on the screen is almost as easy as writing
the text into the appropriate memory-mapped locations.
Memory-mapped I/O is also convenient for other devices/ such as the serial
and parallel ports used to connect modems and printers to a computer. The
CPU transfers data through these kinds of devices by reading and writing a few
device registers/ called an I/0 To send out a long string of bytes through a
memory-mapped serial port1 the CPU writes one data byte to the data register
and sets a bit in the control register to signal that the byte is available. The device
takes the data byte and then clears the bit in the control register to signal that
it is ready for the next byte. Then the CPU can transfer the next byte. If the
CPU uses polling to watch the control bit/ constantly looping to see whether
the device is ready/ this method of operation is called
If the CPU does not poll the control bit/ but instead receives an interrupt when
the device is ready for the next byte/ the data transfer is said to be
When a process running in user rnode requests additional memory/ pages
are allocated from the list of free page frames maintained by the kernel.
This list is typically populated using a page-replacement algorithm such as
those discussed in Section 9.4 and most likely contains free pages scattered
throughout physical memory/ as explained earlier. Remember/ too/ that if a
user process requests a single byte of memory/ internal fragmentation will
result/ as the process will be granted an entire page frame.
Kernel memory/ however1 is often allocated from a free-memory pool
different from the list used to satisfy ordinary user-mode processes. There
are two primary reasons for this:
The kernel requests memory for data structures of varying sizes, some of
which are less than a page in size. As a result1 the kernel must use memory
conservatively and attempt to minimize waste due to fragmentation. This
is especially important because many operating systems do not subject
kernel code or data to the paging system.
2. Pages allocated to user-mode processes do not necessarily have to be in
contiguous physical memory. However/ certain hardware devices interact
directly with physical memory-without the benefit of a virtual memory
interface-and consequently may require memory residing in physically
contiguous pages.
In the following sections/ we examine two strategies for managing free memory
that is assigned to kernel processes: the   buddy system.   and slab allocation.
9.8 397
9.8.1 Buddy System
Tbe buddy system allocates memory from a fixed-size segment consisting of
physically contiguous pages. Memory is allocated from this segment using a
power-of-2 allocator, which satisfies requests in units sized as a power of 2
(4 KB, 8 KB, 16 KB, and so forth). A request in units not appropriately sized is
rounded up to the next highest power of 2. For example, if a request for 11 KB
is made, it is satisfied with a 16-KB segment.
Let's consider a simple example. Assume the size of a memory segment
is initially 256 KB and the kernel requests 21 KB of memory. The segment is
initially divided into two buddies-which we will call AL and AR -each 128
KB in size. One of these buddies is further divided into two 64-KB buddiesBLand
BR- However, the next-highest power of 2 from 21 KB is 32 KB so either
Bt or BR is again divided into two 32-KB buddies, CL and CR. One of these
buddies is used to satisfy the 21-KB request. This scheme is illustrated in Figure
9.27, where CL is the segment allocated to the 21 KB request.
An advantage of the buddy system is how quickly adjacent buddies can be
combined to form larger segments using a teclmique known as coalescing. In
Figure 9.27, for example, when the kernel releases the CL unit it was allocated,
the system can coalesce C L and C R into a 64-KB segment. This segment, B L, can
in turn be coalesced with its buddy B R to form a 128-KB segment. Ultimately,
we can end up with the original256-KB segment.
The obvious drawback to the buddy system is that rounding up to the
next highest power of 2 is very likely to cause fragmentation within allocated
segments. For example, a 33-KB request can only be satisfied with a 64-
KB segment. In fact, we cannot guarantee that less than 50 percent of the
allocated unit will be wasted due to internal fragmentation. In the following
section, we explore a memory allocation scheme where no space is lost due to
fragmentation.
physically contiguous pages
256 KB
Figure 9.27 Buddy system allocation.
398 Chapter 9
9.8.2 Slab Allocation
A second strategy for allocating kernel memory is known as A
is made up of one or nwre physically contiguous pages. A consists of
one or more slabs. There is a single cache for each unique kernel data structure
-for example, a separate cache for the data structure representing process
descriptors, a separate cache for file objects, a separate cache for semaphores,
and so forth. Each cache is populated with that are instantiations of the
kernel data structure the cache represents. For example, the cache representing
semaphores stores instances of semaphore objects, the cache representing
process descriptors stores instances of process descriptor objects, and so forth.
The relationship between slabs, caches, and objects is shown in Figure 9.28.
The figure shows two kernel objects 3 KB in size and three objects 7 KB in size.
These objects are stored in their respective caches.
The slab-allocation algorithm uses caches to store kernel objects. When a
cache is created, a number of objects-which are initially marked as free-are
allocated to the cache. The number of objects in the cache depends on the size
of the associated slab. For example, a 12-KB slab (made up of three continguous
4-KB pages) could store six 2-KB objects. Initially, all objects in the cache are
marked as free. When a new object for a kernel data structure is needed, the
allocator can assign any free object from the cache to satisfy the request. The
object assigned from the cache is marked as used.
Let's consider a scenario in which the kernel requests memory from the
slab allocator for an object representing a process descriptor. In Linux systems,
a process descriptor is of the type struct task_struct, which requires
approximately 1.7 KB of memory. When the Linux kernel creates a new task,
it requests the necessary memory for the struct task_struct object from its
cache. The cache will fulfill the request using a struct task_struct object
that has already been allocated in a slab and is marked as free.
In Linux, a slab may be in one of three possible states:
kernel objects slabs
3-KB
objects
7-KB
objects
Figure 9.28 Slab allocation.
physically
contiguous
pages
9.9
9.9
Full. All objects in the slab are marked as used.
Empty. All objects in the slab are marked as free.
Partial. The slab consists of both used and free objects.
399
The slab allocator first attempts to satisfy the request with a free object in a
partial slab. If none exist, a free object is assigned from an empty slab. If no
empty slabs are available, a new slab is allocated from contiguous physical
pages and assigned to a cache; memory for the object is allocated from this
slab.
The slab allocator provides two main benefits:
No memory is wasted due to fragmentation. Fragn  entation is not an
issue because each unique kernel data structure has an associated cache,
and each cache is made up of one or more slabs that are divided into
chunks the size of the objects being represented. Thus, when the kernel
requests memory for an object, the slab allocator returns the exact amount
of memory required to represent the object.
Memory requests can be satisfied quickly. The slab allocation scheme
is thus particularly effective for mm  aging memory when objects are
frequently allocated and deallocated, as is often the case with requests
from the kernel. The act of allocating-and releasing-memory can be
a time-consuming process. However, objects are created in advance and
thus can be quickly allocated from the cache. Furthermore, when the
kernel has finished with an object and releases it, it is marked as free and
returned to its cache, thus making it immediately available for subsequent
requests fi om the kernel.
The slab allocator first appeared in the Solaris 2.4 kernel. Because of its
general-purpose nature, this allocator is now also used for certain user-mode
memory requests in Solaris. Linux originally used the buddy system; however,
beginning with Version 2.2, the Linux kernel adopted the slab allocator.
The major decisions that we make for a paging system are the selections of
a replacement algorithm and an allocation policy, which we discussed earlier
in this chapter. There are many other considerations as well, and we discuss
several of them here.
9.9.1 Prepaging
An obvious property of pure demand paging is the large number of page faults
that occur when a process is started. This situation results from trying to get the
initial locality into memory. The same situation may arise at other times. For
instance, when a swapped-out process is restarted, all its are on the disk,
and each must be brought in by its own page fault. is an attempt to
prevent this high level of initial paging. The strategy is to bring into memory at
400 Chapter 9
one tin1.e all the pages that will be needed. Some operating systerns-notably
Solaris-prepage the page frames for small files.
In a system using the working-set model, for example, we keep with each
process a list of the pages in its working set. If we must suspend a process
(due to an I/0 wait or a lack of free frames), we remember the working set for
that process. When the process is to be resumed (because I/0 has finished or
enough free frames have become available), we automatically bring back into
memory its entire working set before restarting the process.
Prepaging may offer an advantage in some cases. The question is simply
whether the cost of using prepaging is less than the cost of servicing the
corresponding page faults. It may well be the case that many of the pages
brought back into memory by prepaging will not be used.
Assume that s pages are prepaged and a fraction a of these s pages is
actually used (0 :'::: a :'::: 1). The question is whether the cost of the s   .a saved
page faults is greater or less than the cost of prepaging s   . (1 - a) unnecessary
pages. If a is close to 0, prepaging loses; if a is close to 1, prepaging wins.
9.9.2 Page Size
The designers of an operating system for an existing machine seldom have
a choice concerning the page size. However, when new machines are being
designed, a decision regarding the best page size must be made. As you might
expect, there is no single best page size. Rather, there is a set of factors that
support various sizes. Page sizes are invariably powers of 2, generally ranging
from 4,096 (212) to 4,194,304 (222) bytes.
How do we select a page size  One concern is the size of the page table. For
a given virtual memory space, decreasing the page size increases the number
of pages and hence the size of the page table. For a virtual memory of 4 MB
(222), for example, there would be 4,096 pages of 1,024 bytes but only 512 pages
of 8,192 bytes. Because each active process must have its own copy of the page
table, a large page size is desirable.
Memory is better utilized with smaller pages, however. If a process is
allocated memory starting at location 00000 and continuing until it has as much
as it needs, it probably will not end exactly on a page boundary. Thus, a part
of the final page must be allocated (because pages are the units of allocation)
but will be unused (creating internal fragmentation). Assuming independence
of process size and page size, we can expect that, on the average, half of the
final page of each process will be wasted. This loss is only 256 bytes for a page
of 512 bytes but is 4,096 bytes for a page of 8,192 bytes. To minimize internal
fragmentation, then, we need a small page size.
Another problem is the time required to read or write a page. I/0 time is
composed of seek, latency, and transfer times. Transfer time is proportional
to the amount transferred (that is, the page size)-a fact that would seem
to argue for a small page size. Howeve1~ as we shall see in Section 12.1.1,
latency and seek time normally dwarf transfer time. At a transfer rate of 2
MB per second, it takes only 0.2 milliseconds to transfer 512 bytes. Latency
time, though, is perhaps 8 milliseconds and seek time 20 milliseconds. Of
the total I/0 time (28.2 milliseconds), therefore, only 1 percent is attributable
to the actual transfer. Doubling the page size increases I/0 time to only 28.4
milliseconds. It takes 28.4 milliseconds to read a single page of 1,024 bytes but
9.9 401
56.4 milliseconds to read the sam.e amount as two pages of 512 bytes each.
Thus, a desire to minimize 1/0 time argues for a larger page size.
With a smaller page size, though, to tall /0 should be reduced, since locality
will be improved. A smaller page size allows each page to match program
locality more accurately. For example, consider a process 200 KB in size, of
which only half (100 KB) is actually used in an execution. If we have only one
large page, we must bring in the entire page, a total of 200 KB transferred and
allocated. If instead we had pages of only 1 byte, then we could bring in only
the 100 KB that are actually used, resulting in only 100 KB transferred and
allocated. With a smaller page size, we have better allowing us to
isolate only the memory that is actually needed. With a larger page size, we
must allocate and transfer not only what is needed but also anything else that
happens to be in the page, whether it is needed or not. Thus, a smaller page
size should result in less I/0 and less total allocated memory.
But did you notice that with a page size of 1 byte, we would have a page
fault for each byte  A process of 200 KB that used only half of that memory
would generate only one page fault with a page size of 200 KB but 102,400 page
faults with a page size of 1 byte. Each page fault generates the large amount
of overhead needed for processing the interrupt, saving registers, replacing a
page, queueing for the paging device, and updating tables. To minimize the
number of page faults, we need to have a large page size.
Other factors must be considered as well (such as the relationship between
page size and sector size on the paging device). The problem has no best
answer. As we have seen, some factors (internal fragmentation, locality) argue
for a small page size, whereas others (table size, I/0 time) argue for a large
page size. However, the historical trend is toward larger page sizes. Indeed, the
first edition of Operating System Concepts (1983) used 4,096 bytes as the upper
bound on page sizes, and this value was the most common page size in 1990.
Modern systems may now use much larger page sizes, as we will see in the
following section.
9.9.3 TLB Reach
In Chapter 8, we introduced the of the TLB. Recall that the hit ratio
for the TLB refers to the percentage of virtual address translations that are
resolved in the TLB rather than the page table. Clearly, the hit ratio is related
to the number of entries in the TLB, and the way to increase the hit ratio is
by increasing the number of entries in the TLB. This, however, does not come
cheaply, as the associative memory used to construct the TLB is both expensive
and power hungry.
Related to the hit ratio is a similar metric: the The TLB reach refers
to the amount of memory accessible from the TLB and is simply the number
of entries multiplied by the page size. Ideally, the working set for a process is
stored in the TLB. If it is not, the process will spend a considerable amount of
time resolving memory references in the page table rather than the TLB. If we
double the number of entries in the TLB, we double the TLB reach. However,
for some memory-intensive applications, this may still prove insufficient for
storing the working set.
Another approacl1 for increasing the TLB reach is to either increase the size
of the page or provide multiple page sizes. If we increase the page size-say,
402 Chapter 9
from 8 KB to 32 KB-we quadruple the TLB reach. However, this may lead to
an increase in fragmentation for some applications that do not require such
a large page size as 32 KB. Alternatively, an operating system may provide
several different page sizes. For example, the UltraSPARC supports page sizes
of 8 KB, 64 KB, 512 KB, and 4MB. Of these available pages sizes, Solaris uses
both 8-KB and 4-MB page sizes. And with a 64-entry TLB, the TLB reach for
Solaris ranges from 512 KB with 8-KB pages to 256MB with 4-MB pages. For the
majority of applications, the 8-KB page size is sufficient, although Solaris maps
the first 4 MB of kernel code and data with two 4-MB pages. Solaris also allows
applications-such as databases-to take advantage of the large 4-MB page
size.
Providing support for multiple page sizes requires the operating system
-not hardware-to manage the TLB. For example, one of the fields in a
TLB entry must indicate the size of the page frame corresponding to the
TLB entry. Managing the TLB in software and not hardware comes at a cost
in performance. Howeve1~ the increased hit ratio and TLB reach offset the
performance costs. Indeed, recent trends indicate a move toward softwaremanaged
TLBs and operating-system support for multiple page sizes. The
UltraSPARC, MIPS, and Alpha architectures employ software-managed TLBs.
The PowerPC and Pentium manage the TLB in hardware.
9.9.4 Inverted Page Tables
Section 8.5.3 introduced the concept of the inverted page table. The purpose
of this form of page management is to reduce the amount of physical memory
needed to track virtual-to-physical address translations. We accomplish this
savings by creating a table that has one entry per page of physical memory,
indexed by the pair   process-id, page-number  .
Because they keep information about which virtual memory page is stored
in each physical frame, inverted page tables reduce the amount of physical
memory needed to store this information. However, the inverted page table
no longer contains complete information about the logical address space of a
process, and that information is required if a referenced page is not currently
in memory. Demand paging requires this information to process page faults.
For the information to be available, an external page table (one per process)
must be kept. Each such table looks like the traditional per-process page table
and contains information on where each virtual page is located.
But do external page tables negate the utility of inverted page tables  Since
these tables are referenced only when a page fault occurs, they do not need to
be available quickly. Instead, they are themselves paged in and out of memory
as necessary. Unfortunately, a page fault may now cause the virtual memory
n1.anager to generate another page fault as it pages in the external page table it
needs to locate the virtual page on the backing store. This special case requires
careful handling in the kernel and a delay in the page-lookup processing.
9.9.5 Program Structure
Demand paging is designed to be transparent to the user program. In many
cases, the user is completely unaware of the paged nature of memory. In other
cases, however, system performance can be improved if the user (or compiler)
has an awareness of the underlying demand paging.
9.9 403
Let's look at a contrived but informative example. Assume that pages are
128 words in size. Consider a C program whose function is to initialize to 0
each element of a 128-by-128 array. The following code is typical:
inti, j;
int [128J [128J data;
for (j = 0; j    128; j++)
for (i = 0; i    128; i++)
data[iJ[jJ = 0;
Notice that the array is stored row major; that is, the array is stored
data [OJ [OJ, data [OJ [1J,      , data [OJ [127J, data [1J [OJ, data [1J [1J,      ,
data[127J [127J. For pages of 128 words, each row takes one page. Thus,
the preceding code zeros one word in each page, then another word in each
page, and so on. If the operating system allocates fewer than 128 frames to the
entire program, then its execution will result in 128 x 128 = 16,384 page faults.
In contrast, suppose we change the code to
inti, j;
int [128J [128J data;
for (i = 0; i    128; i++)
for (j = 0; j    128; j++)
data[iJ[jJ = 0;
This code zeros all the words on one page before starting the next page,
reducing the number of page faults to 128.
Careful selection of data structures and programming structures can
increase locality and hence lower the page-fault rate and the number of pages in
the working set. For example, a stack has good locality, since access is always
made to the top. A hash table, in contrast, is designed to scatter references,
producing bad locality. Of course, locality of reference is just one measure of
the efficiency of the use of a data structure. Other heavily weighted factors
include search speed, total number of memory references, and total number of
pages touched.
At a later stage, the compiler and loader can have a sigicificant effect on
paging. Separating code and data and generating reentrant code means that
code pages can be read-only and hence will never be modified. Clean pages
do not have to be paged out to be replaced. The loader can avoid placing
routines across page boundaries, keeping each routine completely in one page.
Routines that call each other many times can be packed into the same page.
This packaging is a variant of the bin-packing problem of operations research:
try to pack the variable-sized load segments into the fixed-sized pages so that
interpage references are minimized. Such an approach is particularly useful
for large page sizes.
The choice of programming language can affect paging as well. For
example, C and C++ use pointers frequently, and pointers tend to randomize
access to memory, thereby potentially diminishing a process's locality. Some
studies have shown that object-oriented programs also tend to have a poor
locality of reference.
404 Chapter 9
9.9.6 1/0 Interlock
When demand paging is used, we sometimes need to allow some of the pages
to be in n  emory. One such situation occurs when I/0 is done to or from
user (virtual) memory. l/0 is often implemented by a separate I/0 processor.
For example, a controller for a USB storage device is generally given the number
of bytes to transfer and a memory address for the buffer (Figure 9.29). When
the transfer is complete, the CPU is interrupted.
We must be sure the following sequence of events does not occur: A process
issues an I/0 request and is put in a queue for that I/O device. Meanwhile, the
CPU is given to other processes. These processes cause page faults; and one of
them, using a global replacement algorithm, replaces the page containing the
memory buffer for the waiting process. The pages are paged out. Some time
later, when the I/O request advances to the head of the device queue, the I/O
occurs to the specified address. However, this frame is now being used for a
different page belonging to another process.
There are two common solutions to this problem. One solution is never to
execute I/0 to user memory. Instead, data are always copied between system
memory and user memory. I/0 takes place only between system memory
and the I/0 device. To write a block on tape, we first copy the block to system
memory and then write it to tape. This extra copying may result in unacceptably
high overhead.
Another solution is to allow pages to be locked into memory. Here, a lock
bit is associated with every frame. If the frame is locked, it cannot be selected
for replacement. Under this approach, to write a block on tape, we lock into
memory the pages containing the block. The system can then continue as
usual. Locked pages cannot be replaced. When the I/O is complete, the pages
are unlocked.
Figure 9.29 The reason why frames used for 1/0 must be in memory.
9.10
9.10 405
Lock bits are used in various situations. Frequently, some or all of the
operating-system kernel is locked into memory, as many operating systems
cannot tolerate a page fault caused by the kernel.
Another use for a lock bit involves normal page replacement. Consider
the following sequence of events: A low-priority process faults. Selecting a
replacement frame, the paging system reads the necessary page into memory.
Ready to continue, the low-priority process enters the ready queue and waits
for the CPU. Since it is a low-priority process, it may not be selected by the
CPU scheduler for a time. While the low-priority process waits, a high-priority
process faults. Looking for a replacement, the paging system sees a page that
is in memory but has not been referenced or modified: it is the page that the
low-priority process just brought in. This page looks like a perfect replacement:
it is clean and will not need to be written out, and it apparently has not been
used for a long time.
Whether the high-priority process should be able to replace the low-priority
process is a policy decision. After all, we are simply delaying the low-priority
process for the benefit of the high-priority process. However, we are wasting
the effort spent to bring in the page for the low-priority process. If we decide
to prevent replacement of a newly brought-in page until it can be used at least
once, then we can use the lock bit to implement this mechanism. When a page
is selected for replacement, its lock bit is turned on; it remains on until the
faulting process is again dispatched.
Using a lock bit can be dangerous: The lock bit may get turned on but
never turned off. Should this situation occur (because of a bug in the operating
system, for example), the locked frame becomes unusable. On a single-user
system, the overuse of locking would hurt only the user doing the locking.
Multiuser systems must be less trusting of users. For instance, Solaris allows
locking   hints,   but it is free to disregard these hints if the free-frame pool
becomes too small or if an individual process requests that too many pages be
locked in memory.
In this section, we describe how Windows XP and Solaris implement virtual
memory.
9.10.1 Windows XP
Windows XP implements virtual memory using demand paging with
Clustering handles page faults by bringing in not only the faultil1.g
page also several pages following the faulting page. When a process is first
created, it is assigned a working-set minimum and maximum. The
is the minimum number of pages the process is guaranteed to
in memory. If sufficient memory is available, a process may be assigned as
many pages as its For most applications, the value
of working-set minimum and working-set maximum is 50 and 345 pages,
respectively. (In some circumstances, a process may be allowed to exceed its
working-set maximum.) The virtual memory manager maintains a list of free
page frames. Associated with this list is a threshold value that is used to
406 Chapter 9
indicate whether sufficient free memory is available. If a page fault occurs for a
process that is below its working-set maximum, the virtual memory manager
allocates a page from this list of free pages. If a process that is at its working-set
rnaximum incurs a page fault, it must select a page for replacement using a
local page-replacement policy.
When the amount of free memory falls below the threshold, the virtual
memory manager uses a tactic known as to
restore the value above the threshold. Automatic working-set trimming works
by evaluating the number of pages allocated to processes. If a process has
been allocated more pages than its working-set minimum, the virtual memory
manager removes pages until the process reaches its working-set minimum. A
process that is at its working-set minimum may be allocated pages from the
free-page-frame list once sufficient free memory is available.
The algorithm used to determine which page to remove from a working set
depends on the type of processor. On single-processor 80x86 systems, Windows
XP uses a variation of the clock algorithm discussed in Section 9.4.5.2. On
Alpha and multiprocessor x86 systems, clearing the reference bit may require
invalidatil  g the entry in the translation look-aside buffer on other processors.
Rather than incurring this overhead, Windows XP uses a variation on the FIFO
algorithm discussed in Section 9.4.2.
9.10.2 Solaris
In Solaris, when a thread incurs a page fault, the kernel assigns a page to
the faulting thread from the list of free pages it maintains. Therefore, it is
imperative that the kernel keep a sufficient amount of free memory available.
Associated with this list of free pages is a parameter-Zotsfree-that represents
a threshold to begin paging. The lotsfree parameter is typically set to 1/64 the
size of the physical memory. Four times per second, the kernel checks whether
the amount of free memory is less than lotsfree. If the number of free pages falls
below lotsfree, a process known as a pageout starts up. The pageout process is
similar to the second-chance algorithm described in Section 9.4.5.2, except that
it uses two hands while scanning pages, rather than one. The pageout process
works as follows: The front hand of the clock scans all pages in memory, setting
the reference bit to 0. Later, the back hand of the clock examines the reference
bit for the pages in memory, appending each page whose reference bit is still set
to 0 to the free list and writing to disk its contents if modified. Solaris maintains
a cache list of pages that have been   freed   but have not yet been overwritten.
The free list contains frames that have invalid contents. Pages can be reclaimed
from the cache list if they are accessed before being moved to the free list.
The pageout algorithm uses several parameters to control the rate at which
pages are scam  ed (known as the scanrate). The scanrate is expressed in pages
per second and ranges from slowscan to fastscan. When free memory falls
below lotsfree, scanning occurs at slowscan pages per second and progresses
to fastscan, depending on the amount of free memory available. The default
value of slowscan is 100 pages per second; fasts can is typically set to the value
(total physical pages)/2 pages per second, with a maximum of 8,192 pages per
second. This is shown in Figure 9.30 (withfastscan set to the maximum).
The distance (in pages) between the hands of the clock is determil  ed by
a system parameter, handspread. The amount of time between the front hand's
9.11
8192
fastscan
Cll
7 
c
(1j
u
en
100
slowscan
minfree desfree
amount of free memory
Figure 9.30 Solaris page scanner.
9.11 407
lotsfree
clearing a bit and the back hand's investigating its value depends on the scanrate
and the handspread. If scam-ate is 100 pages per second and handspread is 1,024
pages, 10 seconds can pass between the time a bit is set by the front hand
and the time it is checked by the back hand. However, because of the demands
placed on the memory system, a scanrate of several thousand is not uncommon.
This means that the amount of time between clearing and investigating a bit is
often a few seconds.
As mentioned above, the pageout process checks memory four times per
second. However, if free memory falls below desfree (Figure 9.30), pageout
will nm 100 times per second with the intention of keeping at least desfree
free memory available. If the pageout process is unable to keep the amount
of free memory at desfree for a 30-second average, the kernel begins swapping
processes, thereby freeing all pages allocated to swapped processes. In general,
the kernel looks for processes that have been idle for long periods of time. If
the system is unable to maintain the amount of free memory at minfree, the
pageout process is called for every request for a new page.
Recent releases of the Solaris kernel have provided enhancements of
the paging algorithm. One such enhancement involves recognizing pages
from shared libraries. Pages belonging to libraries that are being shared by
several processes-even if they are eligible to be claimed by the scannerare
skipped during the page-scanning process. Another enhancement concerns
distinguishing pages that have been allocated to processes from pages allocated
to regularfiles. This is known as and is covered in Section 11.6.2.
It is desirable to be able to execute a process whose logical address space is
larger than the available physical address space. Virtual memory is a technique
408 Chapter 9
that enables us to map a large logical address space onto a smaller physical
menlOry. Virtual memory allows us to run extremely large processes and to
raise the degree of multiprogramming, increasing CPU utilization. Further, it
frees application programmers from worrying about memory availability. In
addition, with virtual memory, several processes can share system libraries
and memory. Virtual memory also enables us to use an efficient type of process
creation known as copy-on-write, wherein parent and child processes share
actual pages of memory.
Virtual memory is commonly implemented by demand paging. Pure
demand paging never brings in a page until that page is referenced. The first
reference causes a page fault to the operating system. The operating-system
kernel consults an internal table to determine where the page is located on the
backing store. It then finds a free frame and reads the page in from the backing
store. The page table is updated to reflect this change, and the instruction that
caused the page fault is restarted. This approach allows a process to run even
though its entire memory image is not in main memory at once. As long as the
page-fault rate is reasonably low, performance is acceptable.
We can use demand paging to reduce the number of frames allocated to
a process. This arrangement can increase the degree of multiprogramming
(allowing more processes to be available for execution at one time) and-in
theory, at least-the CPU utilization of the system. It also allows processes
to be run even though their memory requirements exceed the total available
physical memory. Such processes run in virtual memory.
If total memory requirements exceed the capacity of physical memory,
then it may be necessary to replace pages from memory to free frames for
new pages. Various page-replacement algorithms are used. FIFO page replacement
is easy to program but suffers from Belady's anomaly. Optimal page
replacement requires future knowledge. LRU replacement is an approximation
of optimal page replacement, but even it may be difficult to implement.
Most page-replacement algorithms, such as the second-chance algorithm, are
approximations of LRU replacement.
In addition to a page-replacement algorithm, a frame-allocation policy
is needed. Allocation can be fixed, suggesting local page replacement, or
dynamic, suggesting global replacement. The working-set model assumes that
processes execute in localities. The working set is the set of pages in the current
locality. Accordingly, each process should be allocated enough frames for its
current working set. If a process does not have enough memory for its working
set, it will thrash. Providing enough frames to each process to avoid thrashing
may require process swapping and schedulil  g.
Most operating systems provide features for memory mappil1g files, thus
allowing file I/0 to be treated as routine memory access. The Win32 API
implements shared memory through memory mappil1g files.
Kernel processes typically req1.1ire memory to be allocated using pages
that are physically contiguous. The buddy system allocates memory to kernel
processes in units sized according to a power of 2, which often results in
fragmentation. Slab allocators assign kernel data structures to caches associated
with slabs, which are made up of one or more physically contiguous pages.
With slab allocation, no memory is wasted due to fragmentation, and memory
requests can be satisfied quickly.
409
In addition to reqmnng that we solve the major problems of page
replacement and frame allocation, the proper design of a paging systern
requires that we consider prep aging, page size, TLB reach, inverted page tables,
program structure, I/0 interlock, and other issues.
9.1 Assume there is a 1,024-KB segment where memory is allocated using
the buddy system. Using Figure 9.27 as a guide, draw a tree illustrating
how the following memory requests are allocated:
Request 240 bytes
Request 120 bytes
Request 60 bytes
Request 130 bytes
Next modify the tree for the followilcg releases of memory. Perform
coalescing whenever possible:
Release 240 bytes
Release 60 bytes
Release 120 bytes
9.2 Consider the page table for a system with 12-bit virtual and physical
addresses with 256-byte pages. The list of free page frames is D, E, F
(that is, Dis at the head of the list E is second, and F is last).
410 Chapter 9
Convert the following virtual addresses to their equivalent physical
addresses in hexadecimal. All numbers are given in hexadecimal. (A
dash for a page frame indicates that the page is not in memory.)
9EF
111
700
OFF
9.3 A page-replacement algorithm should minimize the number of page
faults. We can achieve this minimization by distributing heavily used
pages evenly over all of memory, rather than having them compete for
a small number of page frames. We can associate with each page frame
a counter of the number of pages associated with that frame. Then,
to replace a page, we can search for the page frame with the smallest
counter.
a. Define a page-replacement algorithm using this basic idea. Specifically
address these problems:
i. What is the initial value of the counters 
ii. When are counters increased 
iii. When are counters decreased 
1v. How is the page to be replaced selected 
b. How many page faults occur for your algorithm for the following
reference string with four page frames 
1, 2, 3, 4, 5, 3, 4, 1, 6, 7, 8, 7, 8, 9, 7, 8, 9, 5, 4, 5, 4, 2.
c. What is the minimum number of page faults for an optimal pagereplacement
strategy for the reference string in part b with four
page frames 
9.4 Consider a demand-paging system with the following time-measured
utilizations:
CPU utilization
Paging disk
Other I/0 devices
20%
97.7%
5%
For each of the following, say whether it will (or is likely to) improve
CPU utilization. Explain your answers.
a. Install a faster CPU.
b. Install a bigger paging disk.
c. Increase the degree of multiprogramming.
d. Decrease the degree of multiprogramming.
411
e. Install more main n1.enl0ry.
f. Install a faster hard disk or multiple controllers with multiple hard
disks.
g. Add prepaging to the page-fetch algorithms.
h. Increase the page size.
9.5 Consider a demand-paged computer system where the degree of multiprogramming
is currently fixed at four. The system was recently
measured to determine utilization of the CPU and the paging disk.
The results are one of the following alternatives. For each case, what
is happening  Can the degree of multiprogramming be increased to
increase the CPU utilization  Is the paging helping 
a. CPU utilization 13 percent; disk utilization 97 percent
b. CPU utilization 87 percent; disk utilization 3 percent
c. CPU utilization 13 percent; disk utilization 3 percent
9.6 Consider a demand-paging system with a paging disk that has an
average access and transfer time of 20 milliseconds. Addresses are
translated through a page table in main memory, with an access time of 1
microsecond per memory access. Thus, each memory reference through
the page table takes two accesses. To improve this time, we have added
an associative memory that reduces access time to one memory reference
if the page-table entry is in the associative memory.
Assume that 80 percent of the accesses are in the associative memory
and that, of those remaining, 10 percent (or 2 percent of the total) cause
page faults. What is the effective memory access time 
9.7 A simplified view of thread states is Ready, Running, and Blocked,
where a thread is either ready and waiting to be scheduled, is running
on the processor, or is blocked (i.e. is waiting for I/0.) This is illustrated
in Figure 9.31. Assuming a thread is in the Running state, answer the
following questions: (Be sure to explain your answer.)
a. Will the thread change state if it incurs a page fault  If so, to what
new state 
Figure 9.31 Thread state diagram for Exercise 9.7.
412 Chapter 9
b. Will the thread change state if it generates a TLB miss that is resolved
in the page table  If so, to what new state 
c. Will the thread change state if an address reference is resolved in
the page table  If so, to what new state 
9.8 Discuss the hardware support required to support demand paging.
9.9 Consider the following page reference string:
1, 2, 3, 4, 2, 1, 5, 6, 2, 1, 2, 3, 7, 6, 3, 2, 1, 2, 3, 6.
How many page faults would occur for the following replacement
algorithms, assuming one, two, three, four, five, six, and seven frames 
Remember that all frames are initially empty, so your first unique pages
will cost one fault each.
LRU replacement
FIFO replacement
Optimal replacement
9.10 Consider a system that allocates pages of different sizes to its processes.
What are the advantages of such a paging scheme  What modifications
to the virtual memory system provide this functionality 
9.11 Discuss situations in which the most frequently used page-replacement
algorithm generates fewer page faults than the least recently used
page-replacement algorithm. Also discuss under what circumstances
the opposite holds.
9.12 Under what circumstances do page faults occur  Describe the actions
taken by the operating system when a page fault occurs.
9.13 Suppose that a machine provides instructions that can access memory
locations using the one-level indirect addressing scheme. What sequence
of page faults is ilccurred when all of the pages of a program are
currently nonresident and the first instruction of the program is an
indirect memory-load operation  What happens when the operating
system is using a per-process frame allocation technique and only two
pages are allocated to this process 
9.14 Consider a system that provides support for user-level and kernellevel
threads. The mapping in this system is one to one (there is a
corresponding kernel thread for each user thread). Does a multithreaded
process consist of (a) a working set for the entire process or (b) a working
set for each thread  Explain.
413
9.15 What is the copy-on-write feature, and under what circumstances is it
beneficial to use this feature  What hardware support is required to
implement this feature 
9.16 Consider the two-dimensional array A:
int A[] [] = new int [100] [100] ;
where A [OJ [OJ is at location 200 in a paged memory system with pages
of size 200. A small process that manipulates the matrix resides in page
0 (locations 0 to 199). Thus, every instruction fetch will be from page 0.
For three page frames, how many page faults are generated by
the following array-initialization loops, using LRU replacement and
assuming that page frame 1 contains the process and the other two
are initially empty 
a. for (int j = 0; j    100; j++)
for (int i = 0; i    100; i++)
A[i] [j] = 0;
b. for (int i = 0; i    100; i++)
for (int j = 0; j    100; j++)
A [i] [j] = 0;
9.17 Discuss situations in which the least frequently used page-replacement
algorithm generates fewer page faults than the least recently used
page-replacement algorithm. Also discuss under what circumstances
the opposite holds.
9.18 What is the cause of thrashing  How does the system detect thrashing 
Once it detects thrashing, what can the system do to eliminate this
problem 
9.19 Assume that you are monitoring the rate at which the pointer in the
clock algorithm (which indicates the candidate page for replacement)
moves. What can you say about the system if you notice the following
behavior:
a. Pointer is moving fast.
b. Pointer is moving slow.
9.20 The VAX/VMS system uses a FIFO replacement algorithm for resident
pages and a free-frame pool of recently used pages. Assume that the
free-frame pool is managed using the least recently used replacement
policy. Answer the following questions:
a. If a page fault occurs and if the page does not exist in the free-frame
pool, how is free space generated for the newly requested page 
414 Chapter 9
b. If a page fault occurs and if the page exists in the free-frame pool,
how is the resident page set and the free-france pool managed to
make space for the requested page 
c. What does the system degenerate to if the number of resident pages
is set to one 
d. What does the system degenerate to if the number of pages in the
free-frame pool is zero 
9.21 The slab-allocation algorithm uses a separate cache for each different
object type. Assuming there is one cache per object type, explain why
this scheme doesn't scale well with multiple CPUs. What could be done
to address this scalability issue 
9.22 Assume that we have a demand-paged memory. The page table is held in
registers. It takes 8 milliseconds to service a page fault if an empty frame
is available or if the replaced page is not modified and 20 milliseconds if
the replaced page is modified. Memory-access time is 100 nanoseconds.
Assume that the page to be replaced is modified 70 percent of the
time. What is the maximum acceptable page-fault rate for an effective
access time of no more than 200 nanoseconds 
9.23 Segmentation is similar to paging but uses variable-sized   pages.   Define
two segment-replacement algorithms based on FIFO and LRU pagereplacement
schemes. Remember that since segments are not the same
size, the segment that is chosen to be replaced may not be big enough
to leave enough consecutive locations for the needed segment. Consider
strategies for systems where segments cam  ot be relocated and strategies
for systems where they can.
9.24 Which of the following programming techniques and structures are
  good   for a demand-paged environment   Which are   not good   
Explain your answers.
a. Stack
b. Hashed symbol table
c. Sequential search
d. Binary search
e. Pure code
f. Vector operations
a Indirection b  
9.25 When a page fault occurs, the process requesting the page must block
while waiting for the page to be brought from disk into physical memory.
Assume that there exists a process with five user-level threads and that
the mapping of user threads to kernel threads is many to one. If one user
thread incurs a page fault while accessing its stack, would the other user
user threads belonging to the same process also be affected by the page
fault-that is, would they also have to wait for the faulting page to be
brought into memory  Explain.
415
9.26 Consider a system that uses pure demand paging.
a. When a process first starts execution, how would you characterize
the page fault rate 
b. Once the working set for a process is loaded into memory, how
would you characterize the page fault rate 
c. Assume that a process changes its locality and the size of the new
working set is too large to be stored in available free memory.
Identify some options system designers could choose from to
handle this situation.
9.27 Assume that a program has just referenced an address in virtual memory.
Describe a scenario in which each of the following can occur. (If no such
scenario can occur, explain why.)
TLB miss with no page fault
TLB miss and page fault
TLB hit and no page fault
TLB hit and page fault
9.28 A certain computer provides its users with a virtual memory space of
232 bytes. The computer has 218 bytes of physical memory. The virtual
memory is implemented by paging, and the page size is 4,096 bytes.
A user process generates the virtual address 11123456. Explain how
the system establishes the corresponding physical location. Distinguish
between software and hardware operations.
9.29 When virtual memory is implemented in a computing system, there are
certain costs associated with the technique and certain benefits. List the
costs and the benefits. Is it possible for the costs to exceed the benefits 
If it is, what measures can be taken to ensure that this does not happen 
9.30 Give an example that illustrates the problem with restarting the move
character instruction (MVC) on the IBM 360/370 when the source and
destination regions are overlapping.
9.31 Consider the parameter 6. used to define the working-set window in
the working-set model. What is the effect of setting 6. to a small value
on the page-fault frequency and the number of active (nonsuspended)
processes currently executing in the system  What is the effect when 6.
is set to a very high value 
9.32 Is it possible for a process to have two working sets, one representing
data and another representing code  Explain.
9.33 Suppose that your replacement policy (in a paged system) is to examine
each page regularly and to discard that page if it has not been used since
the last examination. What would you gain and what would you lose
by using this policy rather than LRU or second-chance replacement 
416 Chapter 9
9.34 Write a program that implements the FIFO and LRU page-replacement
algorithms presented in this chapter. First, generate a random pagereference
string where page numbers range from 0 to 9. Apply the
random page-reference string to each algorithm, and record the number
of page faults incurred by each algorithm. Implement the replacement
algorithms so that the number of page frames can vary from 1 to 7.
Assume that demand paging is used.
9.35 The Catalan numbers are an integer sequence C11 that appear in treeenumeration
problems. The first Catalan numbers for n = 1, 2, 3, ... are
1, 2, 5, 14, 42, 132, .... A formula generating C11 is
1 (2n) (2n)!
ell = (n + 1) --;; = (n + 1)!n!
Design two programs that communicate with shared memory using
the Win32 API as outlined in Section 9.7.2. The producer process will
generate the Catalan sequence and write it to a shared memory object.
The consumer process will then read and output the sequence from
shared memory.
In this instance, the producer process will be passed an integer
parameter on the command line specifying how many Catalan numbers
to produce (for example, providing 5 on the command line means the
producer process will generate the first five Catalan numbers).
Demand paging was first used iil the Atlas system, implemented on the
Manchester University MUSE computer around 1960 (Kilburn et al. [1961]).
Another early demand-paging system was MULTICS, implemented on the GE
645 system (Organick [1972]).
Belady et al. [1969] were the first researchers to observe that the FIFO
replacement strategy may produce the anomaly that bears Belady's name.
Mattson et al. [1970] demonstrated that stack algorithms are not subject to
Belady's anomaly.
The optimal replacement algorithm was presented by Belady [1966] and
was proved to be optimal by Mattson et al. [1970]. Belady' s optimal algorithm is
for a fixed allocation; Prieve and Fabry [1976] presented an optimal algorithm
for situations in which the allocation can vary.
The enl  lanced clock algorithm was discussed by Carr and Hennessy [1981].
The working-set model was developed by Denning [1968]. Discussions
concerning the working-set model were presented by Denning [1980].
The scheme for monitoring the page-fault rate was developed by Wulf
[1969], who successfully applied this technique to the Burroughs BSSOO computer
system.
Wilson et al. [1995] presented several algoritluns for dynamic memory allocation.
Jolmstone and Wilson [1998] described various memory-fragmentation
417
issues. Buddy system memory allocators were described in Knowlton [1965L
Peterson and Norman [1977], and Purdom, Jr. and Stigler [1970]. Bonwick
[1994] discussed the slab allocator, and Bonwick and Adams [2001] extended
the discussion to multiple processors. Other memory-fitting algorithms can
be found in Stephenson [1983], Bays [1977], and Brent [1989]. A survey of
memory-allocation strategies can be found in Wilson et al. [1995].
Solomon and Russinovich [2000] and Russinovich and Solomon [2005]
described how Windows implements virtual memory. McDougall and Mauro
[2007] discussed virtual memory in Solaris. Virtual memory techniques in
Linux and BSD were described by Bovet and Cesati [2002] and McKusick
et al. [1996], respectively. Ganapathy and Schimmel [1998] and Navarro et al.
[2002] discussed operating system support for multiple page sizes. Ortiz [2001]
described virtual memory used in a real-time embedded operating system.
Jacob and Mudge [1998b] compared implementations of virtual memory in
the MIPS, PowerPC, and Pentium architectures. A companion article (Jacob and
Mudge [1998a]) described the hardware support necessary for implementation
of virtual memory in six different architectures, including the UltraSPARC.

Part Five
Since main memory is usually too small to accommodate all the data and
programs permanently, the computer system must provide secondary
storage to back up main memory. Modern computer systems use disks
as the primary on-line storage medium for information (both programs
and data). The file system provides the mechanism for on-line storage
of and access to both data and programs residing on the disks. A file
is a collection of related information defined by its creator. The files are
mapped by the operating system onto physical devices. Files are normally
organized into directories for ease of use.
The devices that attach to a computer vary in many aspects. Some
devices transfer a character or a block of characters at a time. Some
can be accessed only sequentially, others randomly. Some transfer
data synchronously, others asynchronously. Some are dedicated, some
shared. They can be read-only or read-write. They vary greatly in speed.
In many ways, they are also the slowest major component of the
computer.
Because of all this device variation, the operating system needs to
provide a wide range of functionality to applications, to allow them to
control all aspects of the devices. One key goal of an operating system's
1/0 subsystem is to provide the simplest interface possible to the rest of
the system. Because devices are a performance bottleneck, another key
is to optimize 1/0 for maximum concurrency.

