COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
LECTURE# 21   
LOCAL INTERNETWORKING
(Reference Time: 00:43)
Good day. Today we will be talking about Local Internetworking (Refer Slide Time: 00:51 ? 01:13)

What is Internetworking? Internetworking is the connection of different networks. Everybody is aware of the term internet today. Internet comes from this term Internetworking, and by Internetworking we mean connecting different networks. (Refer Slide Time: 01.13 ? 01:35)


What is a network?  As we discussed in the last lecture, the interconnected nodes in the same broadcast domain form a network, and they are usually connected through routers. But for local internetworking, we may not need a router. We may need something called a bridge. Thus, a network is the set of nodes, which are in the same broadcast domain. This broadcast is advantageous for some applications, but for the operation of the network, a crucial requirement is the broadcast. That is used to discover the MAC addresses of the different computers. You can send an ARP message and get the MAC, which will give the IP address. That means you are basically finding out the MAC address of the machine whose IP address is given. The machine will then reply with its own MAC address. That MAC address has to be put in the destination address of the data link layer frame. This is how it operates in the single network. If you have two networks which are connected to each other, internetworking is required for so many reasons: (Refer Slide Time: 2:47-2:51)




two or more networks can be managed as a single network. That could be one advantage of interconnection. Similar type of computers can communicate with another; that is electronic mail, etc., going across the networks. This is another advantage. (Refer Slide Time: 2.52-04.18)


If you have two networks connected, there may be multiple routes between nodes, which help to create alternative communication routes, when links are either not operating or they are busy. The one important reason for local internetworking is to have the capacity to isolate traffic from other networks. As we have seen, if you want to find the MAC address, you send an ARP broadcast. But if the network is large, many people would be broadcasting and the broadcasting load on the entire network will become very heavy. So as the network grows, as more and more nodes and more and more computers get connected to the network, at some point of time, the performance will be getting degraded because so many nodes are sending broadcast messages. The network has to be broken, although it may be in same building. So the scope of broadcast becomes limited. (Refer Slide Time: 04:19-06:50)


To have the capacity to isolate traffic from other networks ? here we are referring to broadcast traffic ? and to access the information on remote sites are the advantages of connecting networks. Look at the problem, which we had glimpsed earlier. Suppose there is this node A, which wants to send a message to a node B. A knows B?s IP address but they are in two different LANs which are separated. Nevertheless, they are connected through a router to have some physical connection. A router is some kind of network device that enables two different LANs to communicate with each other. We will see what actually this router does and whether the same thing applies to the bridges. When we come to bridges we will discuss the basics of a router and what it contains and other details when we discuss the network layer. But right now, we will just talk about the ARP and the MAC layers. So let us say that there are two ARP tables in router 
 for this LAN and another one for that LAN. So there are two ARP tables in the router, one for each IP network. In routing table at source host, it has to find the router. First of all, the host must find the router. For finding the router it once again must know the MAC address, which is given over here as something like E5 or E9 etc., which does not make too much sense to human beings. It is just a bunch of bytes, 6 bytes actually. So the host, that is, A, has to know the router in order to send it to B; or in order to even know the address of B, A must somehow communicate with the router; and for communicating with the router it must know the router?s IP address. But finally, these two hardware adapters should communicate. So they have to know each other?s hardware address or MAC address. It will find the MAC address in the ARP table at source. If it has already communicated with the router, the address may already be in its ARP table. It will do an ARP and find out the MAC address of the adapter of the router. (Refer Slide Time: 06.51-07.22)


Then A  creates a datagram with source A and destination B. Here source A and destination B mean the IP address of B and IP address of A, i.e., the network layer address. This should all be in the packets, which are coming down from the network layer. A uses ARP to get R?s MAC address for this. A creates link layer frame with R?s MAC address as destination; the frame contains A to B IP datagram; A?s data link layer sends the frame. (Refer Slide Time: 07.23-08.09)


R?s data link layer will receive this frame on that particular adaptor. R removes IP datagram from the Ethernet frame. Now it is stripped of the data link layer header and trailer and the router sees that this is destined to B. By looking at the IP address, the router would know that it is not in LAN 1 but in LAN 2. So R uses ARP to get B?s physical layer address. Now through the other adapter, R must communicate with LAN 2 and specifically to B. For that, it has to know B?s address and for that, it does an ARP in LAN 2. Then R creates a frame containing A to B IP datagram, and sends it to B. (Refer Slide Time: 08.10-09.10)



In this way, A communicates with B and the original packet contains the source IP address as this may be the IP address of A and the IP address of B. This is just for example, and these numbers don?t matter. There is a 4-byte IP address and that datagram is sent with A?s source MAC address to the router. Router finds the MAC address as destination B, does the ARP in the LAN 2 and finds out the MAC address, which is 49B-D. Then R forms the net frame, which contains the original packet which was sent by A?s network layer. But now it has the source as the MAC address of the router and it sends it to the MAC address of B through the LAN. This is how the whole scheme works. (Refer Slide Time: 09.11-10.18)

We will see just a couple more slides for finishing this ARP. When sending an ARP request the sender includes its own binding; that means, its own IP address and MAC address. All machines in the local network can extract the bindings from the ARP traffic and store it in its cache. Remember all other nodes don?t have anything to do with this ARP, but still they listen to this ARP traffic, which is going on and whatever bindings it can, it extracts and puts it in its cache, so that if this particular machine wants to communicate with any one of those, it can bypass the ARP and get it straightaway from the ARP cache, rather than doing a broadcast on the network again. A system can notify others of its address by sending an ARP when it boots, because it includes its own binding in the ARP message. (Refer Slide Time: 10.19-10.56)



ARP is a low-level protocol that hides the underlying network?s physical addressing, permitting one to assign arbitrary IP address to every machine. We think of ARP as a part of the physical network system and not as part of IP. So, ARP has to do more with the data link layer rather than the network layer. The other protocol, which is predominately related to the IP address, will be discussed later. To accommodate various systems, ARP uses variable length packets because these ARPs may be used in different networks (Refer Slide Time: 10.57-12.07)


So this is just a part of ARP message format; there are other parts of it. The format used is, Hardware type: 2  bytes. (For example for Ethernet you will get a value of 1), Protocol type: 2  bytes (0800 for IP), Hardware address length: the ARP may be used in various networks and various networks may have different classes of hardware addresses, which are of varying length. If it is just the Ethernet address, there will be 6 bytes but for other kinds of networks it may be something different. So the hardware address length mentioned here is 1 byte; protocol address length is mentioned in 1 byte; but operation is 2 bytes of ARP; i.e., it could be ARP request/response, RARP request/response, etc. (Refer Slide Time: 12.08-14.50)


What is RARP? RARP stands for reverse ARP. What is ARP?  ARP is used to find the MAC address of the machine having a particular IP address and RARP is the reverse. What is the IP address of the machine having this particular MAC address? RARP is required to know one?s own IP address, when one can?t store one?s IP address locally and specifically. One place where it is quite often used is in diskless machines. Nowadays, there is a concept of diskless machines and they are also known as thin clients, which use the computational power and the disk storage, etc., of a central server. These thin clients, since they have minimal functionality, are easier to maintain. It may be a little cheaper to upgrade them, as it is easier to upgrade one central server rather than a whole bunch of PCs. So, if you have a thin client and a server in a network, they communicate with each other in an IP network. But for communicating, for being an entity in the IP network, you must have an IP address. How is this IP address to be stored in this diskless machine, which does not have any disk, to store anything permanently? Whenever you switch it off, all of it becomes volatile, except for the small ROM. So when it boots up, it has small program in its ROM, which can do some elementary processing. It can find out its own MAC address because it is just local and give that MAC address with an RARP, asking for its own IP address and the server will assign an IP address to it. So RARP is used by diskless machines to get its IP address, which may be in a server. Like ARP, RARP is also sent in the data portion of the frame. The frame type contains 8035 and the data portion contains 28 octets. There may be primary and back-up RARP servers. There are two other protocols like BOOTP and DHCP, which are successors of RARP, and they have to do more with the network layer. We will talk about BOOTP and DHCP later, when we talk about the other TCP/IP protocol suites. (Refer Slide Time: 14.51-16.51)


Till now, we have seen what internetworking is; it means connecting two different networks together. If this internetworking is local, i.e., may be  in the same organization or may be in the same building or in nearby buildings, in the same campus, etc., you may not require the full power of a router for the networking part. You may do it with a data link layer device, called a bridge. We will be discussing about bridges today. Bridges can connect different networks. As a matter of fact, they can connect different networks of different types. For example, a bridge can connect 802.x to 802.y.  These x and y may have the same value; i.e., both of them may be 3, that is, both of them may be Ethernets or one may be an Ethernet and another may be a Token Ring, something like locally connected small LANs. If a LAN is big, bridges can?t handle it any longer. It is a data link layer device and it follows a protocol from IEEE 802.1, which is a spanning tree of bridges. If you remember how this IEEE 802 protocols are organized, 802.1 is put at the top as it gives an overview and a few things which are generally applicable to all the layers. Then we have 802.2 for link layer, etc., and 802.3 is Ethernet 4 for token bus, token ring, etc. So we will look at some more 802 protocols when we discuss wireless networks. A bridge may connect different types of networks and that is why it is put in 802.1; this uses spanning tree protocol. We will discuss the spanning tree protocol presently. (Refer Slide Time: 16.52-17.20)



A local internetwork picture would look something like this. Suppose there are four LANs: LAN 1, LAN 2, LAN 3 and LAN 4, which are connected by two bridges. Bridge  has two ports ? one connecting to LAN 1 and the other to LAN 2. Bridge  has three ports, each one connecting to LAN 2, 3 and 4. Now, A can communicate with H through two bridges. (Refer Slide Time: 17.21-18.18)

Let us see the salient points of a bridge. First of all, it is a link layer device. It stores and forwards Ethernet frames, which means it is related to the MAC address rather than the IP address. These bridges handle the hardware addresses. It examines frame headers and selectively forwards frames, based on MAC destination address. This means that (in the previous diagram) when  gets some frame from here, it will look at the hardware address and decide whether to send it to LAN 4 or to LAN 3. It would selectively send it to one of them. When the frame is to be forwarded on segment, it uses CSMA/CD to access the segment. So that is the protocol. (Refer Slide Time: 18.19-19.46)



Bridges are transparent, that is, hosts are unaware of the presence of bridges. To the host the whole thing might look like one single network. Since it is transparent and plug-and-play device, it will do some self-learning and start operating. In the beginning of the self-learning phase, it may be little inefficient but later on its efficiency will improve as it learns more. Bridges need not be configured, which is another advantage. A bridge separates LANs if we want each group?s traffic to remain within its own LAN. There are other reasons like security issues to maintain the traffic within one?s own LAN. (Refer Slide Time: 19.47-20.12)



At the physical level, the bridge boosts the signal strength like a repeater or completely regenerates the signal. Just like a hub or a switch, before forwarding the signal, it will boost up the signal, i.e., the signal is being regenerated, which is highly advantageous. Bridges usually use the same protocol on either side; for example Ethernet-to-Ethernet or Token Ring-to-Token Ring. (Refer Slide Time: 20.13-20.25)



They also convert between protocols. For example, Ethernet-to-Token Ring Protocol Conversion is possible in bridges. Also bridges are fine for medium-sized organization but are totally inadequate for large installations. They would require routers (Refer Slide Time: 20.26-21.27)


A bridge stores the hardware addresses observed from the frames received by each interface and uses this information to learn which frames need to be forwarded by the bridge. It will maintain a table in itself and for each of these interfaces and in that table it will store all the hardware addresses it has seen in the segment. So it will know the location of the machines in the different segments. If it gets a packet from LAN A, which has a hardware address for the machine in LAN B, it will send it to LAN B. Each bridge has physical interfaces, the data link layer, the address table and the filter table. How is it filtered before being forwarded? Using the information from the address table it learns which frame needs to be forwarded by the bridge. (Refer Slide Time: 21.28-21.31)




This is just an example in the form of a picture (Refer Slide Time: 21.32-22.58)



So bridges forward information only in the form of packets to the segment where the destination host is connected to. Bridges can construct (learn) forwarding table from the source address of the packets which have recently been forwarded to it. So whenever there is an ARP on that side of the LAN, the port of the bridge that is connected to it will also get that ARP request. Then it finds some of the bindings and it quickly learns and fills up its table. What will happen if a host is moved to another segment or if a new host is connected to a segment? If a new host is connected to a segment, this learning becomes a continuous process. If a system is moved from LAN 1 to LAN 2, the network interface card, which is present in it, also goes along with the machine from LAN 1 to LAN 2. Since the particular MAC address has moved from LAN1 to LAN 2 the table entries of the system moved will get erased and the data remain fresh and relevant. (Refer Slide Time: 22.59-23.39)




In the above slide, the green ones are the different LAN segments and ,  till  are seven bridges and they are connecting multiple LANs. You can connect multiple LANs like this; but it has a problem of looping and now we will discuss how to avoid that. (Refer Slide Time: 23.40-24.49)

For increased reliability, it is desirable to have redundant alternative paths from source to destination. With the multiple paths, cycles result and so bridges may multiply and forward frames forever. The frame may go on and on without a frame getting dropped, because in the bridge and in the data link layer there is no concept of a particular frame moving around for a long time. Since there is no way to handle that, we do it in IP layer. We will discuss about that later. Once the frame starts circulating, that means, going in a cycle, it will go on and on and such frames may actually increase in numbers and then bring down the whole network, which is not acceptable. The solution for this is to organize bridges in a spanning tree by disabling subset of interfaces willfully, in the sense that we don?t use them. (Refer Slide Time: 24.50-25.12)


Suppose you have a graph as shown above, there could be cycles over here but if you disable these two interfaces of the bridge, you no longer have a cycle. So with these three paths shown you can?t have a cycle. (Refer Slide Time:  25.13-26.37)




We can think of the extended LAN as a graph. Its nodes are the LAN segments and bridges. Edges are bridge-to-segment links. This is how a graph should be constructed. Now construct a tree from the original graph keeping all segment nodes and removing some bridge nodes and edges. The individual LAN segments are the nodes and for the bridge to the LAN connection, we form an edge. This graph may have cycles. We remove the cycles minimally by still keeping the graph connected. If the original graph is not connected, i.e., if it is not possible to go from one node to another, then in the diagram there is no physical connection and hence you can?t do anything about it. But if the original one was connected and when we disable some of the links to avoid cycles, we must keep it minimally connected and so all the nodes are retained. That is why it is called a Spanning Tree. So, finally what we want is to have a Spanning Tree with some of the edges disabled to avoid cycles. (Refer Slide Time: 26.38-26.59)



The above slide is an example graph with lots of cycles. We remove some of the edges so that the graph still remains connected and it is still possible to go from any node to any other node without a cycle in this graph. This  is the Spanning Tree in which all the  nodes are still connected but there is no cycle. (Refer Slide Time: 27.00-27.34)


For this, we use the spanning tree algorithm from IEEE 802.1. The basic idea is that each bridge decides which ports it should forward packets to, so that the resulting network is acyclic and the resulting network interconnects all segments. Assume each bridge has a unique ID and each one knows its own ID. By the algorithm, we assume that the bridge with the smallest ID is the root bridge and this root bridge forwards packets to all its ports. (Refer Slide Time: 27.35-29.03)




How do non-root bridges compute the shortest path to root? Some algorithms are distributed algorithms, i.e., each node does some computation using the locally available knowledge. But this locally available knowledge may not be consistent with the global picture as the global picture is not known. Since the local things are known, we go for distributed algorithm instead of a centralized algorithm. We do use centralized algorithm, gather all the information together in one place, and then do the computation. Centralized or traditional algorithms are obviously easier but it is more difficult to write distributed algorithms. But you don?t have any option. You have to write a distributed algorithm. A node or a bridge in this particular case can do the computation only based on what it knows locally. It does not have the global picture. It tries to form a global picture and that is the task of the algorithm, to form that global picture. Each LAN has a single designated bridge closest to root and the tie-breaker is the minimum bridge. All packets of a LAN are forwarded only to that LAN?s designated bridge. We will look at this algorithm in more detail. (Refer Slide Time: 29.04-30.04)




Bridges exchange configuration messages to determine spanning tree in a distributed manner. The configuration message (CM) consists of three things, S, R and H, where S is the bridge ID of the message sender, R is the bridge ID for the assumed root. Whoever is sending this CM, assigns the best value for the root; that is R. H is the distance in hops from message sender to the assumed root, so H is the distance from R to S, as is known to this particular bridge. So CM gives these three things: bridge ID, root ID, and the distance from the root to the particular sender. (Refer Slide Time: 30.05-31.26)


When there are two CMs, we say CM-1 is better than CM-2, under three conditions. When CM-1 identifies root with smaller bridge ID; when both the CMs give the same root ID you can?t say which one is better based on this but CM-1 is closer to root; when both CMs identify same root and distance to the root, but CM-1?s sender has smaller bridge ID. In these cases, CM-1 is preferred. Initially all bridges assume that they are roots and generate CMs. As the algorithm starts, all the bridges assume themselves to be root and send initiating CMs to all its neighbor nodes. (Refer Slide Time: 31.27-32.23)



Each bridge remembers the best CM it has received or sent. The best CM is the value of the smallest root it has received. If more than 1 value of the root is same, the best CM is the one in which it can reach the root with the smallest number of hops. Bridges use the best CM to determine true root and to compute the distance to root. A bridge stops generating CMs when it realizes that it is not the root. After that point, it simply forwards all CMs it receives. (Refer Slide Time: 32.24-33.10)



A bridge stops forwarding CMs to a segment when it receives better CM from that segment. Suppose a bridge is sending a CM with some particular root; and through some other segment a CM has come with a better root and the better root and the path to the better root are through the original segment, then that segment will not get CMs forwarded from this particular bridge. Then where does this algorithm converge to? (Refer Slide Time: 33.11-35.37)



Let us say  will send CMs to , ,  and . When  gets a message from , it knows that  is on this side and so it will stop sending on this segment.  will send about  to  but this is two hops away so  will not only know that  is there but it will know the shortest route also.  will get the message from either  or through , whichever comes first. So it will find its hop to the root . Then among  and ,  is smaller assuming , , , , , ,  are in the lexical order. So it will connect to . This is how they all will come to know about the root very quickly. Although each bridge may initially think of itself as a root, at some particular point of time, for example,  originally assumes that it is the root but when it gets a CM from , it will know that there is something smaller than itself and  must be the root. So it will latch on to .  will of course by that time have got the CM from  and knows  is the root. So it will forward this CM with  as the root and with its own distance to  and with it to . Now  will know that  is the root. In this way, all the bridges will come to know about the root very quickly and they will latch on to the path or the root, which goes through the smallest number possible error. For example,  could latch through  or  but it will choose  and in this way we will finally have a tree. (Refer Slide Time: 35.38-36.12) 



Now the algorithm will converge to a tree that connects all segments. What if the root fails or what if the designated bridge of a LAN fails? To identify these you have to run this spanning tree algorithm from time to time. (Refer Slide Time: 36.13-37.04)




One of the main uses of the spanning tree algorithm is to isolate the traffic, specifically broadcast traffic. So bridge installation breaks LAN into LAN segments and bridges filter packets. Same LAN segment frames are not usually forwarded to other LAN segments. Hence segments become separate collision domains and any broadcast over here is just limited to the particular segment. So this is the full LAN 1 IP network divided into LAN segments which are bridged. There may be hubs, nodes, etc. (Refer Slide Time: 37.05-37.19)



How does the bridge know which LAN segment to forward the message? When a bridge gets the frame how does it know where to forward it? (Refer Slide Time: 37.20-38.09)

	
A bridge has a bridge table and entry in the bridge table is of node LAN Address, bridge interface and time stamp. The node LAN address is the MAC address, the bridge interface is the group to which it belongs. If the entry in the table becomes too old and is to be dropped from the table, such stale entries in the table can be dropped after a particular prerequisite time known as time stamp, which can be configured (60 min for TTL). So a configured bridge knows which hosts can be reached through which interface without traffic. When a frame is received, the bridge learns the location of the sender, i.e., incoming LAN segment, and records sender location pair in bridge table. (Refer Slide Time:  38.10-39.57)


When a bridge receives a frame, it forms an index bridge table using MAC destination address. If an entry is found for destination and if the destination is on the segment from which the frame had arrived, it drops the frame, because the frame is already in the particular LAN. If an entry is found for destination and if it has to go to some other LAN, it will forward it. This is known as selective forwarding. If this packet frame is meant for some other LAN, the bridge will forward it to that particular interface. Sometimes a new machine is connected to the LAN, etc. or the bridge is newly connected and its bridge table may not have been constructed fully and so it may not have an entry for this particular MAC address. In such a case it will flood. Flooding means forwarding the frame to all interfaces except the interface on which the frame arrived. When a particular frame arrives from some interface for some destination MAC address and the bridge does not know to which LAN segment this MAC address belongs, it does not have a corresponding entry in the bridge table. So it will simply flood, i.e., put in a copy of the frame to each of the other interfaces. If a bridge is newly put in a network, in the beginning it will be inefficient as we have already seen, flooding many packets to many segments. But as it slowly learns, it will become more and more efficient. (Refer Slide Time: 39.58-41.25)


Suppose C sends a frame to D and D replies back with a frame to C and the bridge has this address table and C is on interface 1. Let A and B be with port 1, E in port 2, H and J in port 3. The bridge doesn?t know about either C or D. So when the bridge receives a frame from C, it notes in bridge table that C is on interface 1. This table will get updated because C is the sender MAC address, which it will put in its MAC table. Still the bridge does not know where D is and it will send the frames into interfaces 2 and 3 and not to 1, as we have seen earlier. The frame gets copied on to other ports, 2 and 3. In interface 3, D is not present. So the host will ignore that frame in 3. But interface 2 has D in it and so D will receive it and will try to reply back to C. (Refer Slide Time: 41.26-41.56)


Now D generates a frame for C and bridge receives the frame. One notes in bridge table that D is on the interface 2. Bridge already knows that C is on interface 1. So it selectively sends the frame to interface 1 and does not give to interface 3 any longer. (Refer Slide Time: 41.57-43.04)




We have LANs and when they get bigger and bigger they can?t really exist as 1 single collision domain. So we have to segment it, i.e. break it up into segments and bridge is 1 way to do it. One way of segmenting some of the LANs is shown above. There are three hubs with each hub having a work group. We can connect them through bridges but this is not recommended for two reasons: First of all single point of failure at computer science hub. If this hub goes the other two can?t communicate with each other and all traffic between EE (electrical engineering) and SE (system engineering) can?t pass through CS, which is not good. (Refer Slide Time: 43.05-43.36)




So the recommended configuration would be something as shown above. A bridge or a switch here connects all the hubs. And this bridge or switch acts as the backbone. If EE wants to communicate with SE, it goes through the backbone. (Refer Slide Time: 43.37-44.25)


Let?s see some of the features of bridges. A bridge has the features similar to a switch. It isolates collision domains resulting in higher total maximum throughput. Because in a collision domain with a lot of broadcast traffic the net throughput of the network will go down. So if you can make it smaller and make the frames to travel from one segment to the other, the overall throughput of the network increases. Also it supports limitless number of nodes and geographical coverage. Bridges can connect different network types. It is transparent (i.e. plug and play) and does ?self-learning?. So no configuration is necessary for its operation. (Refer Slide Time: 44.26-46.15)



Now let us compare  a bridge with a router. Both are store-and-forward devices. A router is a network layer device (examine network layer headers) and is used for connecting two different networks globally. A bridge is a link layer device and is used for connecting two different networks in local internetworking rather than global internetworking. Routers maintain routing tables and implement routing algorithms. But bridges maintain bridge tables and implement filtering, learning and spanning tree algorithms. Bridges maintain bridge tables consisting of MAC addresses but routers maintain routing table consisting of IP addresses. (Refer Slide Time: 46.16-47.09)

These are the layers in the protocol stack of the host and from the higher layer, i.e. layer 5 a frame is coming down to 4,3,2,1. 1 is the physical layer and from the physical layer it travels to the bridge. Since the bridge is the layer 2 device, it goes only up to layer 2. Then it encounters a router in the next hop and router will take it up to layer 3, and again bring it back and then send it to the host. The frame will again go to layer 5. So a bridge is the layer 2 device and router is the layer 3 device. (Refer Slide Time: 47.10-48.19)



Let us see the advantages and disadvantages of a bridge. The advantages are:
- bridge operation is simpler requiring less packet processing
- bridge tables are self-learning
- no configuration is necessary and 
- all traffic confined to spanning tree even when alternative bandwidth is available
The disadvantages of bridges are, 
- As we disable some of the links while we run the spanning tree algorithm, at a time, only some of the links are used while the rest remain idle. So we are not using the total bandwidth that is available to its fullest and bridges do not offer protection from broadcast storms. (Refer Slide Time: 48.20-50.02)



Let us now look into the advantages and disadvantages of routers. The advantages are - arbitrary topologies can be supported. Cycles are supported in bridges, i.e., a frame may go on and on forever. But in the network layer, the packet will have a counter and if a packet starts cycling, the protocol is such that at each hop the counter will be decremented and if some router finds that this count has become 0 it will simply drop the packet. So there is a definite time for a packet to circulate and so it can?t circulate indefinitely. The counters used are TTL counters. It provides good routing protocols such as limiting the cycles etc. The network becomes better. The router is capable of providing protection against broadcast storms. The disadvantages are - it requires IP address configuration which means that it is not a plug-and-play device. It requires some manual configuration and higher packet processing. So it is costlier. (Refer Slide Time: 50.03-50.35)


Bridges work well in small networks, i.e. with few hundreds of hosts while routers are used in large networks, i.e. with thousands of hosts. Also for similar networks like Ethernet modems, switches can be configured to do some bridging functions. As we have seen already, whatever functionality is available in the bridge is available in modem switches also. Now let?s compare switches with bridges and routers respectively. (Refer Slide Time: 50.35-51.56)


Switches are very fast but routers are slow, i.e., switches are doing just switching so they are very fast whereas routers have to do some computation and so they are slow. Switches are inexpensive but routers are expensive. Switches don?t give the benefit of alternative routing whereas benefits of alternative routing are available in routers. There is no hierarchical addressing in bridges, but hierarchical addressing is possible with routers. Hierarchical addressing will be discussed later. When we are connected to the wide area networks, i.e., to the whole wide world, then router is a must because other people can also connect through routers and the routers will talk to each other. But switches can?t talk. If you are trying to do local internetworking, a bridge or a switch which now-a-days gives all the bridging functions, may be a good, cheap and efficient alternative. (Refer Slide Time: 51.57-53.34)


Let us finally summarize the comparison between hubs, bridges, switches and routers. 
* Traffic isolation - Bridges, routers, switches provide traffic isolation where as it is not possible in hubs. Because a hub is just a shared medium that does not give any isolation at all.
* Plug-and-play - Hubs, switches and bridges are plug-and-play devices. But routers need some configuration so it is not a plug-and-play device.
* Optimal routing - Hubs, bridges and switches do not know about routing. But routers can find the optimal route at any particular point of time.
* Cut through - Cut through means you start transmitting as the bits arrive. A hub can cut through because it is a replacement of a passive shared medium, so whatever bit comes, gets transmitted. So cut through is possible in switches also. But bridges and routers have to wait for the whole frame and then inspect it through some routing table or bridging table so they are not cut through. (Refer Slide Time: 53.35-54.58)



Finally let us see about virtual LANs. It is partition of an extended LAN to logically separate LANs (VLANs). Each VLAN is assigned a color identifier and packets are forwarded only to VLANs of the same color. So we can use a bridge or a switch. The different ports of a switch could be different VLANs. The ports belonging to the same particular VLAN may physically be two different LAN segments. But logically they are in the same VLAN. Suppose in one building, computer science is in three different floors and the same floors are also shared by say, the electrical engineering department. Each floor has a switch. The computer science floors 1, 2, 3 are in same VLAN or they are in one logical group. Electrical engineering in these floors forms another VLAN. This needs manual configuration but this is possible. (Refer Slide Time: 54.59)


Why are VLANs so popular today? Scalability is possible because broadcasts are now getting limited. Security and network management is better in VLAN. Network management is decoupling physical topology from the logical topology. As we have seen, two different LAN segments could be in same VLAN. For example, a LAN segment at CCB is to be switched from COC administration to ECE administration. We have finished our discussion on local internetworking. Next, we will see another emerging technology, which is becoming very important in the retailing business. Let us see about Wireless Technology in the next lecture. Thank you. (Refer Time Slide: 55:52 - 55:55)


LECTURE # 22 
CELLULAR NETWORKS



We will start our discussion on terrestrial wireless networks. We have already seen 1 kind of wireless communication which is through satellite. It is a microwave repeater. There are 2 very important and rapidly expanding fields in networking. They are terrestrial wireless networking and wireless LAN. We will have 2 lectures on this. The first lecture is on cellular networks and in the next lecture we will talk about wireless LANs and a little of wireless MANs. Today we will discuss about cellular networks. (Refer Time Slide: 56:52-58:47)







The cell phones have become ubiquitous nowadays.  What is a cell? In the cellular network, the network is organized in the form of some cells and each cell covers a geographical region.  It has base station (BS) analogous to 802.11 AP. AP stands for Access Point.  802.11 is the wireless LAN technology which will be dealt in the next lecture. There is a base station and it will have an antenna and some transmitters and recivers and they are connected to the backbone through a line.  It is a wireless line but usually it would be a fibre optic line. Take this particular base station shown. All the mobile stations or mobile users in the certain geographical location around this base station will communicate with this base station and through this base station to the rest of the network. So mobile users attach to network through BS and air interface is the physical and link layer protocol between mobile and BS. All the base stations are connected to the mobile switching center (MSC). The switching is essentially done here.  The MSC connects cells to Wide Area Network.
COMPUTER NETWORKS
PROF. Sujay Ghosh
Department Of Computer Science and Engineering
IIT Kharagpur
Lecture Name #22
Cellular Networks
(Refer slide time - 0:55)

Good day. So, now we will start our discussion on terrestrial wireless networks. We have already seen one kind of wireless communication, which is through satellites. So it is microwave repeater and we know that. But there are two very important and rapidly expanding field in networking, which is terrestrial wireless networking. We will have two lectures on this. In the first lecture, we will discuss cellular networks. The cell phone, which has become ubiquitous nowadays and in the next lecture, we will talk about Wireless LANs: Wireless LANs are may be little bit of Wireless MANs also. So, today we will discuss about cellular networks. (Refer slide time: 01 :49-04 :29)

So just right away, let us learn some jargons. What is a cell? The cellular network is organized in the form of some cells and it covers a geographical region. It has base station analogous to 802.11 AP. AP is for access point. 802.11 is the wireless LAN technology. We will discuss about this in the next lecture. Anyway, the point is that there is a base station and it will have an antenna, some transmitters and receivers. It will be connected to the backbone through a line. May be this could also be a wireless line, but usually this could be a fibre optic line. In a circluar geographical location around this base station, mobiles will communicate with this base station and through this base station to the rest of the network. So mobile users attach to network through BS. And air interface is the physical and link layer protocol between mobile and BS. That is called air interface between the mobile and base station. Now all these base stations are connected to the mobile switching center (MSC). The switching is essentially done over here. The MSC connects cells to Wide Area Network. The mobile switching center will be referred to as MSC, Base station as BS ,mobile switching center as MSC, MS, by the way, is a mobile station. MSC connects cells to Wide Area Network and manages call setup. More about that later and these MSCs will be connected to each other for a particular service provider. They will also connect to public telephone network and the internet etc. So one service provider, their network would be  connected to another service provider?s network. So somebody from this network can call the other network and so on. So we have the cells; we have the base stations; we have the mobile stations or MSs. We have the mobile switching center, MSC, and of course the PSTN at the back of it all. This part is usually wired.(Refer slide time: 04:30- 06:20)

The first hop ? we are now talking about the air interface between the mobile station and the base station. There are two techniques for sharing mobile to BS radio spectrum. There is certain radio spectrum, which is allocated to the base station and to the particular region that it has to be shared. Now, this is a question of multiple access and two techniques that we talked about earlier are FDMA, if you remember this is frequency division multiple access, and TDMA, time division multiple access. In cellular technology, what we usually do is that we combine FDMA and TDMA. So divide the spectrum in frequency channels and divide each channel into time slots. If you say that these are the different frequency channels and each channel may be divided into number of time slots. We will do FDMA as well as TDMA on this. That is one kind of scheme ? the so-called GSM utilizes this. We will be talking about how FDMA and TDMA are combined. The other technology is CDMA, which was designed by a company called VOLCOM in USA, which uses code division multiple access. We have already seen what code division multiple access is. So we will not get into the details of CDMA systems here. CDMA is another popular way of transporting data to the mobile devices. Both GSM and CDMA are used in many countries. For example, in our country also some service providers provide CDMA services, and few offer GSM services, some provide both, and so on. (Refer slide time: 06:24-07:19)

We will now discuss cellular standards as they stand today. Historically, we had only the cellular system that came from some amps in USA. Previously, there was only one cell from the analog system. From analog system, it came down to digital system in deamps and then we have these two systems of the 2G system ? one is the GSM system and the other is the CDMA version. 2G systems are voice channels: IS-136 is TDMA, combined with FDMA, which is used in North America. GSM, which is more popular of these schemes, is the global system for mobile communications. It has combined FDMA/TDMA, which is most widely deployed. IS -95 is the code for CDMA systems, which use code division multiple access. So these are the 2G or second generation systems. (Refer slide time: 07:20-08:23)

There are 2.5G systems. These were introduced because the 3G was promised quite some time back but the service providers really could not deliver it or crank it up to that extent. But, there was lot of demand for it. Voice was alright with 2G, but then the demand for data and other kind of multimedia services etc., was increasing. So people had to be given some data services. Instead of going all the way to 3G, people went to 2.5G. 2.5G systems have both voice and data channels. So, for those who cannot wait for 3G services, there are 2G extensions. One is GPRS. This is General Packet Radio Services evolved from GSM and the data is sent on multiple channels if available. It has an enhanced data rate for global evolution edge; also evolved from GSM using enhanced modulation data rates up to 384k.(Refer slide time:08 :24-09 :11)

CDMA has its own version called CDMA 2000, that was phase 1; then there was phase 2 also. So data rates up to 144k evolved from IS-95, which is the CDMA system. 3G system includes both voice and data: one service it provides is UMTS. This is the name of the Standard Universal Mobile Telecommunications Service (UMTS). GSM is the next step, but using CDMA 2000. So, all these merge into the 3G systems. How exactly this merging will take place and how it will actually be deployed and become popular remains to be seen; but today you can get these data services on your cell phones, etc. (Refer slide time: 09:12-09:46)

The protocol layering for cells is a little different. We will not go too much in to this. One is of course, the physical layer, which has to do with the physical channels. Then there is MAC, medium access control. We will talk about it, at least for GSM. So, there are these logical channels, transport channels, and then there is Radio Resource Control layer. That is the layer 3 particularly. This you might say is a protocol, but this does not go all the way that the OSI 7 layer. This is just for the cellular systems. (Refer slide time: 09:47-12:04)

 Our idea is that we have some base stations and each base station will cover some geographical area like it has been shown here. Different BS would be connected through a backbone network or through MSCs. We are trying to get the basic idea of the cell. The point is that, nowadays cell phones have become very popular. Its rate of penetration is much faster than the original telephones, and it is much faster than PCs also. So, cellular phones have become very popular, which means a lot of people want to use it and lot of people have cell phones. Many of them would want to talk at the same time. But how do you accommodate all these people talking at the same time? We do multiple access. But then, there is a limit to what you can do using same frequency spectrum. The idea was to do some kind of space division multiple access. In the sense that within one particular geographical area, we use a particular frequency band and then in another geographical area, which is far removed from there, so that these two do not interfere with each other, we use the same set of frequency band at the same time for a different set of users. The point is that these powers have to be controlled because, if they are very powerful, they will start interfering with each other. But if this power is controlled, then within that cell, that power is enough. But, it is not enough to interfere with each other. So, two different groups of users can use the same frequency band at the same time. This is the basic idea of breaking up a region into cells so that you can increase the number of people, who would be using this system. That is the basic concept of a cell. (Refer slide time: 12:06-13:03)

In practice, cells are may be of arbitrary shape. But they will be close to a circle because usually the kind of antenna used in base stations is omni directional antenna, in the sense that it gives the same power on all sides. It has the same sensitivity on all sides. If that is so, the area of influence would be a circle. But when many circles are put together they are pulling and they will intersect with each other. To solve this problem, we can use a tessellation. There are only three types of tessellations, which are possible ? equilateral triangles, squares or regular hexagons. Out of these three, the regular hexagon is the closest to a circle. That is why usually the regular hexagons are used to represent a cellular structure. A hexagonal cell, the closest approximation to a circle, is used traditionally for system design. (Refer slide time: 13: 04-14:27)

This is how a big geographical area may have been divided into a large number of cells ? it looks like a beehive. If you notice carefully some of the cells are dark and these cells are marked as A B C D E F G. So, these are seven. There are seven hexagons like this and these are actually different frequency ranges. These frequencies are again reused. For example, you have another A B C D E F G  over here. This B and this B ? although they use the same frequency ranges ? are far apart. So, different groups of people can use it at the same. Once it gets into the base station, we usually take it to the fiber optic domain, where a large number of calls, simultaneous calls can be handled. This really shows you the frequency reuse. A, the set of frequency bands, which are associated with A, will also be reused here, here, and there and so on. That is how a hexagonal cellular structure is constructed and we do this frequency reuse. (Refer slide time: 14:28-14:54)

Co-channel reuse ratio is given by DL/RL = ?3N, where DL is the distance between co-channel cells, that means those who share the same channel. RL is cell radius; N is the cluster size. The number of cells in a cluster N determines the amount of co-channel interference and the number of frequency channels available per cell. This really comes from geometry. (Refer slide time: 14:55-16:29)

When the number of subscribers in a given area increases, allocation of more channels covered by that cell is necessary. What happens is that in one area, say a small town, one base station could satisfy people, who had these cellular phones or mobile phones. Now what happens is that, the number of people who wanted to use mobile phones kept on increasing and now we cannot serve them any longer. The number of requests, which are denied, keeps on increasing. How can we increase? May be break it up into two cells and then break it up into four cells and break it up into many more cells, depending on the clusters of users and the cells. Now, the same area has been divided into smaller cells. May be in the BS, you decrease the transmitter power so that they do not interfere with each other. So when the number of subscribers in a given area increases, allocation of more channels covered by that cell is necessary. This is done by cell splitting. A single small cell midway between two co-channel cells may be introduced. (Refer slide time: 16:30-16:53)

These are the small adhoc solutions to the problem. For example over here, you had a large number of  cells. We created a small cell over here using A, which uses the same frequency bands as the already existing ones. You cannot use E, F, C, or B. But you can use A with other cells. So that is called cell splitting. These are ad hoc solutions, when a particular area has more number of users. (Refer slide time: 16:54-17:14)

We  now have a cellular hierarchy, the needs of which are: extending the coverage to the areas that are difficult to cover by a large cell; increasing the capacity of the network for those areas that have a high density of users; increasing the number of wireless devices and the communication between them. (Refer slide time: 17:15- 18:04)

So, you have a large number of cellular hierarchies. One set of them are called Femto cells. These are the smallest unit in the hierarchy. So these cells need to cover only a few meters, where all devices are in the physical range of the users. This is also called Personal Area Networking. So I have something in my left pocket, something in my right pocket and something in my hand. These might communicate with each other. So, that is Personal Area Networking. Femtocells are small cells. Then we have Picocells, the size of their network is in the range of a few tens of meters. So, you can think of a small building as Picocell. For example, WLAN (Wireless LAN). (Refer slide time: 18:05-18:34)

Micro cells cover a range of hundreds of meters; for example, in urban areas to support PCS or other technologies. PCS is another kind of mobile technology. Macro cells cover areas in the order of several kilometers, for example, a metropolitan area, or may be a small town. Mega cells cover nationwide areas. So, mega cells possibly are being serviced by a satellite. (Refer slide time: 18:35-19:04)

This is the picture of satellite, which may service a mega cell. Then, we have macro cell from this tower. Then we have pico cells, which have some access points etc inside a building and so on. Microcells for covering communication. So these ways of different kinds of technologies may be deployed for these different ranges of cells. (Refer slide time: 19:05-19:32)

Frequency reuse: We have already talked about this. Radio spectrum is one of the scarcest resources available; because, there is so much demand for it for so many applications. So, employ architectures that can support as many uses as possible (theoretically) with the available spectrum. Same spectrum can support multiple users separated by a distance and thus efficiently be using the spectrum. (Refer slide time: 19:33-20:46)

Frequency reuse has its foundations in the attenuation of the signal strength of EM waves with distance. So, if two points are at a distance from each other, this signal gets attenuated and does not interfere significantly with this one, although they are using the same frequency band. Usually, what will happen is that the service provider will be given some band of frequencies.  Now, he has to use that and cannot stray from there, as that is the license agreement. So, what he will do is that, is depending on where his users are, and what the distribution is, what the density is like, he has to develop or plan a cellular infrastructure in this fashion using and reusing this frequency, the same frequency band, here and there to give the maximum amount of service. The distance separating the transmitters of this frequency reuse should be sufficiently large. Of course, this has to do with a transmitter?s power. Transmit power should be reasonably small. The cellular concept is an intelligent means of employing frequency reuse. (Refer slide time: 20:41-21:21)

So what we have been talking about is something like a fixed channel allocation. That means for a particular cell, the channels, that means, the frequency band associated with the cell is fixed. So, total number of channels is Nc = W/B, where W is the bandwidth of the available for spectrum. B is the bandwidth needed by each channel. The total number of channels per cell is Cc = Nc/N, where N is the cluster size. (Refer slide time: 21:22-21:53)

Adjacent radio frequency bands are assigned to different cells as shown. In analog each channel corresponds to one user while in digital each RF channel carries several time slots or codes. So, you are doing either TDMA or CDMA. So, if you are doing this, the naturally FDMA TDMA combine or CDMA uses spread spectrum technology. So, it?s simple to implement. So, fixed channel allocation is simple to implement if traffic is uniform. (Refer slide time: 21:54-23:24)

But then, sometimes traffics are not uniform ? there may be two cells, which are side by side. So, this has been given one band. one has been given another band of frequencies. They do not interfere with each other. But, we find for each cell.  Let us say, to start with, we are given with equal bandwidth to each of the cells. Now,  I find that in one particular cell, the user density is much higher, whereas in adjacent cell, the user density is lower. So, I could use some more bandwidth in this cell and I could do with a little less bandwidth here. So, what could do is that, a part of this frequency band  can borrow from the adjacent cell. So, that is called Channel Borrowing technique.  High traffic cells borrows channel frequencies from low traffic cells. Temporary channel borrowing and static channel borrowing. This could be a permanent feature or this could be the feature of a day. For example, in the central business district, It might become very busy during the day time. So, it may borrow channels from the side, whereas after the evening the use may fall drastically. In that case, one cell can give out channels to others. Not only sort of giving channels to other people. So, this could be static as well as it could be temporary. (Refer slide time: 23:25-26:26)

This is suitably complex picture of GSM, i.e. the Global System for Mobile communications. So, this just to show you how these TDMA and FDMA are combined. So, you see there are 124 simplex channels in the GSM system. Now, each of the simplex channels actually carries a series of TDMA frames and each of the frames is divided into 8 parts, that is how a large number of channels can be given. There are two parts: one is the uplinking and another is the downlinking, that means, from BS to MS ? Base Station to the Mobile Station or from the Mobile Station to the Base Station. So you have two different frequency bands for these ? one band for this BS to MS communication and another band for MS to BS communication. In each band, there are a number of frequency channels and each frequency channel is again divided into so many slots: eight slots for simultaneous communication. So this one and this one are same channels, but this and this are two different channels for a particular mobile station. So from base station, it may be using this particular time slot in this particular channel, that is, from the base station to the mobile station. The same one, from the mobile station to the base station, will be using another channel and actually another time slot, because there is some technical problem in giving the same time slot in this channel as well as the other channel. So, you give it a different time slot over here. So, in this particular time slot of this particular channel, the mobile station is communicating with its base station. So, that is how it goes. GSM uses 124 frequency channels, each of which uses an 8-slot TDM system. And there is a frequency band at which it operates; this is also fixed. This is in the 959.8 MHZ range. You need not remember these figures, but this is the general scheme of the way TDMA and FDMA are combined together. (Refer slide time: 26:27-27:15)

Suppose this is S1(t) and  this is the signal which comes from source 1 and this is the signal from source n, Sn(t). This is some other source. What is happening is that in the Tm slots, the first slot ? S1 ? gets the first slot and Sn gets the nth slot. So, they are pushed into this in the same frequency band, and as time progresses, they function just like in a TDMA system. So, this is the TDMA part. So, GSM = FDMA 200 KHZ; that is the GSM system (Refer slide time: 27:16- 27:46)

This is a portion of the GSM framing structure. So, how they are framed? Actually, this is a somewhat complicated scheme. Some of these frames are used for control purpose and others for communication: one group for base to mobile and other from mobile to base, etc. So, there is a scheme for this. We will not go into the details of this. (Refer slide time: 27:47-28:17)


A GSM system has 124 pairs of simplex channels. They are in pairs because one goes from BS to MS and the other from BS to MS. Each of these is 200 kilo hertz wide and supports 8 separate connections on it, using TDM. So, each active station is assigned to one time slot on one channel pair. 992 channels can be supported in each cell, but many of them are not available to avoid frequency conflicts with neighboring cells. (Refer slide time: 28:18-30:16)

Transmitting and receiving does not happen in the same time slot because the GSM radios cannot transmit and receive at the same time and it takes time to switch from one to another. That is why different time slots are given. A data frame is transmitted in 547 microseconds, but a transmitter is only allowed to send one data frame every 4.615 milliseconds, since it is sharing the channel with seven other stations. The gross rate of each channel is about 270 or about 271 kbps divided among eight users. This gives about 33 or 34 kbps gross. CC i.e., control channels are used to manage the system if somebody is getting only 33 or 34 kbps. Previously, we have been talking about voice channel requiring 64 kbps. Now, the 64 kbps happens to be if you are doing a plain vanilla PCM. That means we have explained, how it is encoded by sampling it at eight samples and eight levels for each sample ? that gives us 64 kbps. The point is that, it is not the only coding scheme. Actually, there are more advanced coding schemes. We did not find time to discuss those coding schemes. Using those coding schemes, good quality voice transmission can be achieved, using a much lower bandwidth. This 33.854 kbps is actually enough, if you are doing your coding in a smart fashion. (Refer slide time: 30:17-32:19)


As I said, apart from the user channels, there are some control channels. CC is used to manage the system. The Broadcast Control Channel (BCC) is a continuous stream of output from the base station containing the BS?s identity and the channel status. All mobile stations monitor their signal strength to see when they moved into a new cell. The point is that the mobile station, when it gets these broadcasts from BS, by just sensing how much transmitter power it is getting, it can identify whether it is near this particular BCC, what this particular BS is, or what its identity is, or whether it is near some other BS. In some systems like CDMA, this power is very crucial even for decoding purposes. That is one thing which is being broadcast and to listened by all the MS. The dedicated control channel is used for location updating, registration, and call setup; in particular, each BS maintains a database of mobile stations, which are in its area. So, information needed to maintain this database is sent on the dedicated control channel. So, the point is that these mobile stations are moving. They move from one cell to another, from the vicinity of one base station to the vicinity of another base station. So, the set of MS, which are now currently under this, that information in some schemes is collected on the side from time to time and there is a database, which is associated with the BS. This is centrally communicated. That is important for locating a person. We come to that later on. (Refer slide time: 32:20-33:33)

And then there is a common control channel which has got three logical sub channels. That is the paging sub channel, paging channel, in which the BS uses to announce incoming calls. Each MS monitors it continuously to watch for the call it should answer. The point is that, if there is a call,  and MS is in the area of some BS and then somebody wants to call to this MS, that one particular MS has to be alerted. So, there is a paging for that MS from the BS and the MS is always listening to it. So, whenever it hears the page for itself, it gets alerted. So, the other is the  random access channel. This allows users to request a slot on the dedicated control channel. If two requests collide, they are garbled and have to be retried later on. So, this is the part of the call set-up. So, it is the first part of call set-up. It tries to put a request in the random access channel for a slot in the dedicated control channel. When it gets a slot in the dedicated control channel, it can go away with the further steps of call set-up. Next is the access grant channel. (Refer slide time: 33:34-33:51)
 
In GSM the channel multiplexing is FDM + with eight TDM slots. Uplink is this much and channel bandwidth is200 KHz. So, DCS has certain frequency range etc. (Refer slide time: 33:52-34:03)

Channels are broadcast and the channel rate is 13 kbps.  (Refer slide time: 34:04-34:42)

We have already seen what CDMA is ? it is based on DS spread spectrum, that is, the Direct Sequence Spread Spectrum. It has two frequency bands, one for forward channel and one for reverse channel and one frequency band, a wide band actually that is shared. That means it uses orthogonal codes by a number of handsets or number of mobile stations. So, CDMA allows use of same spectrum over all cells. It also gives net capacity improvement. Although which system is better ? CDMA or GSM ? is still not clear. (Refer slide time: 34:43-35:40)

There are certain issues in the cellular infrastructure, which have to be handled.  We will quickly discuss each of them. The most important one is handoff. Because you may be talking on your mobile phone while moving, may be moving in a car. So, what will happen is that, the car eventually will pass out of the range of one base station and move in to the range of another base station. So, you have to hand off. That means previously all communication from this mobile station was being handled by this particular base station, as it moves in to the area of another base station, this call has to be handed off from one BS to the other BS. Handoff changes of radio connection from one base station to another will happen. But this not such a simple scheme and we will see why. There are two types of handoff: hard handoff and soft handoff. (Refer slide time: 35:41-36:33)

This handoff has to be managed. In order to manage the handoff, we have to detect that handoff requirement has arisen because the mobile station has moved and then you have to execute the handoff, in the sense that you have to do the channel assignment and you may have to do some path rerouting and there may be problems in this section also. For example, when you move into a new cell, all the channels over there may be busy and so you may not have any extra channel, which has to be given to this ongoing call. So, there are various schemes for handling ? may be you drop this. That is the simplest thing to do, that you do not allow it or maybe you keep some guard channels specifically for these kinds of cases. But this detection of handoff requirement is a troublesome affair. (Refer slide time: 36:34-37:04)

As I said, there are two types of handoff: hard handoff and soft handoff. Hard handoff is break before make. This is used in GSM system ? that means you break this connection and set up the new connection with the BS in whose area you are moving. MS connects to base station 2 after link with base station 1 breaks, and this is the region where the handoff will take place. (Refer slide time: 37:05-37:57)

The difficulties in handoff detection are the following. The signal strength fluctuates. This is a very challenging area of mobile system design ? the signal area fluctuates due to various reasons: scattering, reflection and diffraction results in fading. There are fast fading and slow fading of the receiving signal. There are false handoff requirements at the boundary; there is a ping pong effect at the boundary. That means what might happen is that it may hand off from BS 1 to BS 2, then again from BS 2 to BS 1, again from BS 1to BS 2. This kind of ping pong might be going on. So, the number of unnecessary handoffs must be reduced because handoffs have a price paid, actually, keeping both channels busy on both sides. There are other kinds of overheads to this. (Refer slide time: 37:59-38:41)

Let us look at a very simple model. Actually the situation is much more complex because, there are number of base stations ? maybe three base stations and you may be equidistant from all the three at a particular point. So, you may have an even more difficult problem. But let us look at a simple problem. Suppose D is the distance between two base stations. So, ideally we would like that the signal strength from BS1 is following like this and the signal strength from BS2 is going like this. As you move from BS1 to BS2, as the MS is moving at the cell boundary, it just switches from BS1 to BS2. But the actual picture is something like this. (Refer slide time: 38:42-39:44)

This is the point is that the signal strength from BS1 is varying, very fast. Why does it vary? I will just tell you. I had just mentioned it, but I will tell it again. This is varying like this. The signal strength from BS2 is also varying like this. So this has been plotted from, let us say,  800 to 1200 region. So, there is a solid region at least from 950 to 1050. There is a region of 100 m, where you really do not know who is stronger. So the signal is varying all the time. What might happen is that you might now decide to move from BS1 to BS2, and then you find that BS1 has become much stronger and BS2 has become much weaker. So, it might switch from BS2 to BS1 and this might go on as a ping pong effect. So, this is a very difficult problem. (Refer slide time: 39:45 -41:03)

And as I said, why is it that it varies in this fashion? There are various reasons for this: one is that you are moving. This mobile station is actually moving.  Now whatever signal it gets, it may get some direct signal, it may get some reflected signal, it may get some scattered signal and all these signals may start interfering with each other. So, actually what might happen is what is called multipath fading. That means, the same signal may have arrived from source to destination through two different paths ? may be through two reflections ? and they may be out of phase because of the different distances, which may be allowed. If they are precisely out of say, 180? out of phase, then you are going to have distractive interference or they may be in phase; they may strengthen each other. So, in a very short span of time, as the mobile is moving, you may find a very largely fluctuating signal. There are other reasons for this fading etc. We will not go into the detail of this. (Refer slide time:41:04-42:21)

So, there is a problem of handoff. So, for the handoff decision there is various algorithms which have been  proposed. I will just mention them. One is relative signal strength, which is the simplest first thing. You will think that whichever is weaker, we leave that, and whichever is stronger it will chose that one. So, choose BS2 if signal from BS2 is greater than the signal from BS 1. But as we saw, with just this, there may be lot of ping pong effect and lot of unnecessary handoffs. You can use this same RSS, that is, received signal strength, and some threshold base. That means we choose BS2 if the signal from BS2 is greater than the signal from BS1 and the signal from BS1 is less than a threshold, which means that although BS2 is stronger, if BS1 is above the threshold, which is still working, then we do not do a handoff. Another thing is RSS plus hysteresis, that is, received signal strength. Just being greater is not enough; it has to be greater by a certain amount of hysteresis. The hysteresis means base1 persists as the BS2 is becoming stronger in base 1 and then there are other kinds of other combinations. People have tried for getting a good handoff decision. (Refer slide time: 42:22-42:50)


As I said, this hard handoff is used in GSM, whereas in CDMA system, they use soft handoff. This is ?make before break? ? that means you make a connection to that coming in the next base station, before you release the connection with the previous base station. So, MS connects to BS2 before connection to BS1 breaks. This is called soft handoff. (Refer slide time: 42:51-43:14)

We will now discuss the merits and demerits of soft handoff. Merits are: mobile station does not loose contact during handoff; the effects of ping pong are reduced; and it is easy to implement for CDMA systems. The demerits are: it is a complex process. So, hardware requirement is more and that means your hardware cost may also go up; and it utilizes extra resource during handoff. (Refer slide time: 43:16-44:22)

[43:16]Now, we come to the question of mobility management.  How do you manage mobility in a local in a MS?  That is because, one of the very fascinating thing about mobile connection.  I have called somebody on the mobile, who I assumed is just local. That is who is in the same area as i am. It just so happens, that he is in a place far away. He is visiting some place.  May be Rajasthan or something. ,as he is very far away. Now  I will expect that the system would somehow locate him in Rajasthan and then allow me to talk to him. So, that is again a non- trivial problem. There are various approaches to this problem. We will just once again touch on this.   So, this is called mobility management. One is location management access point of a mobile station changes as it moves around the network coverage area and important for effective delivery of incoming cells and other is handoff management. We have already talked about it. (Refer slide time: 44:23-45:21)

Now, for location management, one approach through location updates. That means messages are sent by MS that is a mobile station regarding its changing points of access to the fixed network. So, that is to the fixed network, that is sort of time of time it tells that. Ok.  This is where some central database is updated. Each time the MS makes an update to its location a database in the fixed part of the network has to be updated to reflect the new location information. So, that for a particular MS, if you go to the data base and find out what is the last point, where he said, that he was. Of course what he might have done is that he might have switched off his   mobile and then moved to somewhere else and then put it on or something. .  So, put it on again. So,   that is not still solved all the problem 100%.  But this is one approach to solve it. (Refer slide time:45:22-46:26)

The other this thing is the paging. You know what is paging? Paging means that  sort of broadcast it. Well broadcast means broadcast it everywhere. We do not want to broadcast it everywhere. So, you broadcast it only to certain places. So what we do is that, we broadcast that there is a call for such. So, that is what we page and if that paging is being done in a cell, where the MS is actually present and that the MS  will respond, that is what will happen. So, that is another scheme  required to deliver an incoming message to the MS.  Response from the paged terminal enables the network to locate the MS. The other thing about location management is location information dissemination. Procedures to store and distribute the location information related to MS are serviced by the network. that is the issues over. That I am not going detail in any of these, as no time.(Refer slide time:46:27-48:03)

And for the location update, you may do static location update. That means initiation of location update is decided by the topology of the network and location area based location updates which is commonly used.  what a location area is and distance based, which performs location update after crossing certain number of cells or timer- based, performs location update after a certain time has elapsed. So, the question is that how frequently do you update this? Because, if collecting all the data all the time and just updating it all the time, that will consume an enormous amount of resources. You have to optimize somehow.  That this is the point is that if we not doing frequently enough, your data in the database going to stay and then when you want to actually search for somebody, then you might have to search around a large area. Ideally what would you liked is that when a call is there for somebody, we know that exactly in that particular cell that mobile station is there. So, we go and page over there. He responds ok.  That is the idea. But this idea will never work.  Because, you cannot keep it updated on all the time and you cannot collect and keep all the information. So, you have to do some kind of optimization over there, which means that you actually looking for one particular mobile. You have to search not in just one cell, but may be in several cells. So, that is where location area comes in that location. Area is collection of cells, we will come to that. (Refer slide time: 48:04- 49:02)

The other is dynamic location update. It uses mobility of the user and call pattern for location update. So, if you know the mobility of the user and some call pattern etc, you may be able to predict that where this particular user may be. It is more likely that he will be there.  So, it state-based: performs location update, based on the current state information such as distance  travelled, the number of LAs crossed, etc or user profile based, which is more difficult,   not exactly used at the moment maintains. A list of LAs that is location areas that the MS located in at different points of that time usually.  So, usually during office hours I will be found in my office.   In that particular area. So, that may be known and that may be a guess.  But then gathering this information and keeping this information for all users, this is not a mean task. (Refer slide time:49:03-49:58)

So,this is the location area based  location update. So, as you see that a bunch of cells together form a location area.  So, this is the location area 1 containing 1 2 3 4 5 cells. This location area 2 containing 7 cells and so on. So, assign a location area identifier to a group of cells LA 1, LA2.  BS broadcasts periodically LA identifier. So, it is enough to trying to fill a particular cell. You are trying to fill it down to a location area. So, BS broadcast it, whichever location it is in MS is required to listen for LA identifier and make an update to the location if necessary. Drawback is once again,  there may be ping pong effect.  This fellow is moving like this.  So, it going from location area 1 to 2,1 to 2 etc.  That is always the thing. You cannot eliminate this completely. (Refer slide time: 49:59-50:22)

 Location update in GSM. LA identity, that is it takes the location based approach. Identity is used for location updates. LA consists of a group of cells controlled by BSC and MS performs location update under 3 (1).circumstances upon power up, compares previous LA identity with the one currently being broadcast- if different, performs update. (Refer slide time: 50:22-50:52)

(2)When MS crosses LA boundary, performs update. (3)After a predetermined period of time, performs update to ensure MS is available. So that you do all the three things simultaneously. So that would make sort of judgment about, what is this time interval, after which it will go automatically or of course, other two are simple. (Refer slide time: 50:53-51:21))

And then in for paging schemes, you can do blanket paging, that means when you know  the location you just page everything.  All the cells paging in all cells within an LA simultaneously. If the LA update is current, MS responds immediately. Advantage is minimum delay in getting paging response and disadvantage is    it needs paging in all the cells within LA equidistant from the current cells. A timer is used to declare the MS is unreachable. (Refer slide time: 51:22-51:39)

All it could be that closest cells approach. First page the cell where MS was last seen. If not successful, page subsequent rings of cells that are. So, this is all trying to reduce the overhead and give the maximum response time etc.  So,all these different schemes are there. (Refer slide time:51:40-52:52)

And finally we will not go in to the details of these. as i said that, now everybody  want news on their handsets. Not only news they want to access, to the internet through the handsets, which means we will have to give some data service and that is why service providers also move from [Noise]  2G to 2.5G systems wherein from the GSM family.  It,the general packet radio system GPRS and  CDMA to CDMA2000.  So, GPRS is a really ?packet overlay? network that means on the same network, there  is a  packet service, which is going on available frequency bands.  Network on top of the existing GSM digital circuit switched voice based network. So, it is TCP/ IP based. The protocol based  is same as the TCP IP which we will learn later. It allows data packets to be conveyed across the mobile network using packet switching and it is ?Always on?. ? Always connected? type of thing and after initial ?log-on?, user is permanently connected to the IP services, that is the GPRS.   (Refer slide time: 52:54-53:22)

Instant access, no further log on and usually the rates also gives a flat rate. User perceived performance: fluctuates (as GPRS users defer to voice users). So, voice users have a preference. So, because data may delay that is may be acceptable to a maximum of [Noise] 50kbps. Network resources only used when information ready to be exchanged bandwidth on demand.  So, more utilization of air time that is the GPRS. (Refer slide time: 53:23-53:52)

So, this provides high speed frequency.. So, uplink is on the particular frequency band and downlink is on particular frequency band and these are all packet services which provide high speed packet data access. This uses modified GSM hardware (different phones or cards) are required. That is you have particular kind of set that handle GPRS. Several time slots can be dynamically allocated to transmit a block of data. So, if the packet is large, so several time slots may be used for that. (Refer slide time: 53:53-54:25)

The uplink channel is shared by a number of mobiles, and its use is allocated by a BSC, base station stream. The downlink is of course fully controlled by the serving BSC and random access is not needed in the uplink. Of course multiple access will still because, so many people want to send the request for data. The MS requests use of the channel in ?a packet random access message?. The BSC allocates an unused channel to the mobile and sends a ?packet access grant message? etc. (Refer slide time: 54:26-54:49)

CDMA 2000 is once again is the CDMA version of it. Increasing voice capacity. Once again this is always on peak packet data rate of 153 kbps which is quite high. Connectivity to ANSI -41and GSM-MAP, which we need not to bother. Various bands and bandwidths of operation in support of different operator needs. (Refer slide time: 54:50- 55:58)

It is expected that actually that, this  CDMA 2000 1X RTT is backward compatible with CDMA1 system, which was the previous original CDMA system. Improved service multiplexing and QOS management and variable transmission rates and it is expected that in future what is going to happen is that, as data demand is definitely going to grow, so these will sort of move from these interim 2.5G system to the 3G systems. I. CDMA is already being employed.  This is part of our big network architecture.  This converged network architecture, which is slowly emerging and in the next lecture, what we are going to do is that we are going to discuss Wireless networking in the LAN setup.   In the sense purely in this, we are talking about voice and voice added to data. Next, we are going to talk about data and may be data plus wires that is a separate issue. Thank you. 
Computer Networks
Faculty Name
Prof. Sujoy Ghosh
Dept. of Computer Science & Engineering I.I.T, Kharagpur
Lecture Name # 23
Wireless Network
(Refer slide time: 00:39)
Good day. In the last lecture we had discussed about the cellular network and that end of wireless networking; today we will talk more specifically about data networking and wireless LAN, and may be wireless MAN and things like that. Actually there has been an explosive interest in wireless technology in just the last few years, and a number of different systems have come up. It is not known at this moment what will finally stabilize, but the number of systems have come up and some of them of are on drawing board, some of them are on actual deployment. So we will talk about just a few of them, the more important ones today. (Refer slide time: 01:29) 

Today we will talk about wireless networks, (Refer slide time: 01:29 - 03:36)

and specifically if I may say, wireless LAN. A LAN means a local area network that works without wires, which means you do not have to wire up the whole place; you do not have to have a wire coming into your system; you can walk into a room with a laptop and you are already on the network. But this has some peculiar problems; we will discuss them. This is not as easy since signals are of limited range. Unlike wired LAN, if A can hear B and B can hear C, it is not necessarily true that A can hear C. So this is a problem which we have to handle; secondly in many of the cases, these wireless LANs use unlicensed frequencies and low power. Low power is important because you want to have a small-sized cell so that in the another part of the building there may be another cell just giving services to another group of users. As we know that this way, by doing space division multiplexing, we can increase the number of users who are on the network. One of the most important LAN standards today, wireless LAN standard, is 802.11 and there are various versions of 802.11. The speed varies from 2 mbps to 54 mbps.  We will also talk a little bit about Bluetooth, which is a personal area network. We will talk a little bit about wireless MAN, which is 802.16 and just mention of few other emerging technologies. (Refer slide time: 03:07 - 04:28)	   

There are two modes of operation in a LAN ? in the presence of a control module or a CM often called a base station; just as we have a base station in case of a mobile, similarly here we can have a base station  which, in 802.11 parlance, is called an access point or AP. So we can have a base station access point or AP; so we can have a base station or a control module and a number of users. That is one mode of operation. The other mode of operation is a rather ad hoc network; that means, we just have some peers. There is a peer to peer connectivity and there is no central module. So applications could be LAN extensions, cross building interconnect, or nomadic access; that means some body just moves in and gets immediate access to ad hoc networking. (Refer slide time: 04:30 - 04:42) 

These are the two modes of operation ? in one we have a base station, which is controlling them. This is slightly easier to handle than complete peer to peer ad hoc network. (Refer slide time: 04:43 - 05:23) 

What happens is this control module or this access point in the case of 802.11 could be connected to a wired network so that all those stations, which are connected to the control module via wireless link, get connected to the entire network so they may connect straightaway to individual PCs or they may connect to some network hub or switch. They may connect to a server and a number of LANs. So this is the picture of a single cell, wireless LAN single cell ? W LAN ? and we can have multiple cells of W LAN. (Refer slide time: 05:25 - 05:50)

In each of the cells we will have a control module, which will serve some of the user modules. You may note that there may be a region where it is possible to connect to either of the CMs. Also that is something the user module will have to handle.  (Refer slide time: 05:51 - 07:33)    

Now we will look at W LAN requirement. This is some kind of a wish list actually ? what all we would want from wireless LAN. Good use of bandwidth is we want ? high throughput ? every body uses a number of nodes; it should be large, may be in the hundreds. A good connection to LAN backbone is required because nowadays just a local network by itself is of limited utility since every body is getting use to be connected to the entire network meaning the internet even all the time.   so the  backbone connectivity is also important; good service coverage; ok   I mean wherever I am I would like to be  connected so this to be a good service coverage or range; minimal battery power consumption this an important issue in any kind of mobile system because if the battery consumption becomes high, either you have to carry  heavier batteries or you have to  charge them often so that is not good so we want minimal battery power consumption; transmission security and robustness ? this may be an issue in many cases ? because you know  so in a wireless system   the medium is of course open to every body alright including snoopers if any so but you would like your communication to remain somewhat private or protected  and in some cases that may even become crucial  so we want security and robustness ok and some colocated network operation. (Refer slide time: 07:35 - 09:15)  

License free operation: this is another important issue. For example, the ISM band consists of industrial, scientific and medical bands of frequencies which are free; there is no license on it. That means operation with the unlicensed band is important because then whoever can develop a good system can go ahead and compete in it, and that way the world technology improves fast. Then people also get cheaper and better quality service. That is why ISM band is generally preferred; but it is not that in a wireless network, we always stick to ISM band. Cell hand-off and network roaming: this is another thing we would like to have. This is some kind of a wish list; that is, not all of them are achieved 100% today, but these are the kinds of things we would like to have, like cell hand-off and network roaming. Just as in voice network we can roam from one cell to another and our call remains online, similarly in network connection, we would like them to remain online when we move from one cell to another. So we require dynamic management, adaptive MAC address management, dynamic and automated addition, deletion, relocation of end systems without disruption. And then we require a choice of physical solutions; for example infrared spread spectrum, narrow band microwave, etc. (Refer slide time: 09:16 - 10:22)

As I said there are a number of standards that came up; here I am just showing some standards in the 802.11 family. Then there is an 802.15 family; 802.16; and so on. This is just one of them. 802.11 originally was a 2.4 GHz ISM band and used FHSS, which is frequency hopping spread spectrum, with 1?2 mbps speed or direct sequence spread spectrum, DSSS had 1 or 2 mbps, and then slowly it graduated and then it fell over to three standards: 802.11b, which is the most earliest and the most common one; it was followed by 802.11a; and 802.11g. These two are in the 2.4 GHz ISM band, whereas this one is in the 5 GHz ISM band. (Refer slide time: 10:23 - 11:14) 

We will not go in to the details of all these and what exactly are their differences, etc. Today what we are trying to do is that we are just trying to get a general idea, because there are too many standards. 802.11 LAN architecture: by now we know that we will have an access point, which is connected to a hub or a switch or a router. This is in one cell; this is another cell. So cells may be called a basic service set, also known as cell. In infrastructural mode, it contains wireless hosts ? so these are the wireless hosts. It contains an access point. In an ad hoc mode, there will not be any access station, so they will all be connecting to each other in a peer-to-peer module. (Refer slide time: 11:15 - 11:46) 

And in the physical layer in 802.11 family itself you see that there are so many techniques that are used ? FHSS, which is frequency hopping spread spectrum; direct sequence spread spectrum; orthogonal frequency division multiplexing (OFDM); HRDSSS is another one; OFDM and so on. Above this we have the data link layer, that is, the LLC and the MAC sub-layer, and above that we have the upper layers. (Refer slide time: 11:47 - 12:59) 

So we do not have the time to go into the details of the physical layer technologies, like, how exactly the multiplexing and multiple access is done, but this is just a very broad and high level view of the system. We have the input data, which is encoded. So that is a channel encoder; it uses either FSK, that is, frequency shift keying, or phase shift keying, FSK or PSK. There are other variations of this. We will just get a feel of this. This feeds into a modulator and then there is a pseudo random pattern generator on the receiver side. This is on the sender side, similarly there is a pseudo random pattern generator on the receiver side and these two are synchronized. So this modifies the main carrier, and it then goes into the air or whatever the medium; then it arises at the other end, plus some noise is also added to it, where it is demodulated and it is decoded and then we get the output data. (Refer slide time: 13:00 - 14:58)


Now how do we ? glossing over the physical layer ? how do we handle the multiple access of an 802.11? It avoids collisions; that means, we know that when two or more nodes are transmitting at the same time, their signals will	collide, and we will have a collision. So 802.11 tries to avoid collision. It does CSMA ? if you remember CSMA is carrier sense multiple access. So it does some carrier sensing; it senses the channel before transmitting. Of course it does not collide with the ongoing transmissions by other node, but it does not do any collision detection. And the reason it does not do any collision detection is that if it has to do collision detection, first of all what would happen is that not all traffic is apparent to all the nodes in the network. Due to various reasons it could happen. So that is one reason that even if there is a collision and if you are doing collision detection, you may not be able to detect it at all so that is why the stress here is not to do collision detection like you do in a wired LAN like Ethernet, but to avoid the collision. alright so it is difficult to receive sense collision when transmitting due to weak received signal and fading etc and it can not sense all collisions in any case ? hidden terminal fading so goal is to avoid collisions so this is called CSMA CA. instead of CSMA CD we have CSMA CA that is CSMA with collision avoidance.  (Refer slide time: 15:00 - 15:58)

So this is a diagram which shows you this problem about hidden terminal. For example, we have A, B and C. Now B can listen to C; A can listen to B, that means, A B can communicate with each other; B C can communicate with each other; but between A and C there is some kind of an obstacle. So A and C can not communicate with each other. Even if there is no obstacle like this, the situation could be something like this. Suppose this is A; this is B; and this is C. Now at the point B, A and B have fairly high signal, whereas at C, A signal strength is very low; similarly at A, C signal strength is very low. Some of the terminals may be hidden from some other terminals. This is a problem; that is why our MAC protocol is designed to handle (Refer slide time: 16:00 ? 18:29) 

situations like these. This is a MAC overview; we have a number of boxes here. I will not go in to all the details like radio management; power management; management information base; this is for network management; there is an addressing; there is a security part, like shared key and association management; similarly there is a fragmentation of large frames and so on. We will not look into all this; we will just mention that for addressing we use the similar 48-bit MAC address, which is Ethernet compliant. You remember that the Ethernet address or the hardware address or the MAC address that we talked about when we discussed Ethernet, is a 6-byte or 48-bit address and 48 is of course a very large address space; that means 248 is 256 trillion, which is a very large number. So there is no shortage of addresses. So a bunch of addresses may be given to these. The same kind of 48-bit addresses are used for this also. Making it Ethernet compliant has its advantage because Ethernet is just another ubiquitous kind of network. Another point is that we have an acknowledgement request kind of a system, where some frames and some fragments, etc. are acknowledged. So if the acknowledgement does not come we have retransmission. We also have some error correction; and radio link security; data authentication; data encryption; simple scrambling; or peer-to-peer, etc. We will not be discussing these as do not have the time. In the radio link, there is a question of quality of service. There is this CSMA CA ? we will look into this channel access mechanism in some more detail. Dedicated real time support systems are also there; they are with PCF. So there are actually two mechanisms, which may be simultaneously present in the same system ? DCF and PCF ? we will be talking about these. The standard provides two modes of (Refer slide time: 18:36 - 19:33)

Operation: DCF, which is mandatory. That means every 802.11 system has to be following DCF at least. So it is a best effort service that uses CSMA CA; that is, CSMA with collision avoidance. And there is another mode, which may optional, which is PCF. This is a base station. This is a distributed control function and this is a point control function. This base station controls access to the medium and uses a polling mechanism with higher priority access to the medium. So actually, if PCF is there, what PCF can do is that actually PCF can take precision. So for DCF, it can give some guaranteed kind of service or quality of service to some users. There are three different types of frames: data frames, control frames, and management frames. (Refer slide time: 19:35 - 20:09)

So one is the point coordination function which is PCF the other is the distributed coordination function which is the DCF. ok so  and how I mean which one you are using may be you are not using PCF at all so that would be a network administrator?s choice.  so the if you are using PCF that would give you some contention free service whereas if you are using DCF  you are using  a service where there may be contention. of course you can use PCF and DCF at the same time. alright (Refer slide time: 20:11 - 22:10) 

So how does the protocol work? From the sender?s side it senses if the channel is idle for DIFS. DIFS is the period of time which can be configured; it then transmits the entire frame. So it just can not send some thing as soon as the channel is idle; it has to wait for at least DIFS amount of time. And it does not do any collision detection. How does it know that there will not be any collision? Just because you have waited for DIFS amount of time does not mean that there will not be any collision; there may still be collision because somebody else may also be listening to the channel waiting for DIFS amount of time and start transmitting, and you are not doing any collision detection. The point is that you will not get an acknowledgement. Unlike the Ethernet system, where there is no acknowledgement, this is an acknowledgement based system. So you will get an acknowledgement; if you get the acknowledgement you know that there is a collision and if you do not get an acknowledgement you know that there is a problem, so you retransmit. If, on the other hand, you sense the channel to be busy, then you start some random back-off time, similar to Ethernet where you do binary exponential back-off, etc. We start random back-off time. The timer counts down while the channel is idle, transmits when timer expires. If there is no acknowledgement, we increase random back-off interval and repeat step 2. This is the way system works: if there is no acknowledgement, it means that it has not succeeded. So you increase the back-off time and repeat. Another reason why collision detection is not done in wireless network is that for many of the radio systems, it is difficult to do transmission and reception at the same time. So collision detection means you keep on doing collision detection while you are transmitting. You just keep on listening whether it is going through or there is some garbled message in the medium. But that is difficult to do in many systems; so that is another reason why CD is not done. In the receiver it is simple; if it gets the frame then it returns the acknowledgement. (Refer slide time: 22:51 - 23:23) 

SIFS is another time interval, which is defined. So after some time, it will send the acknowledgement. Acknowledgement is needed for the hidden terminal problem. So there is the sender; there is the receiver; and suppose after DIFS amount of time the sender has sent some data, the receiver has received it. After SIFS amount of time, it sends back the acknowledgement. (Refer slide time: 23:26 - 24:48)


There is another scheme which uses this RTS CTS exchange. Suppose A wants to communicate ? this is the AP and this is B ? so A wants to communicate and let say B also wants to communicate. So A sends a request for transmission ? it is just a reservation request. B also sends the reservation request at the same time, and there is a collision. Since there is a collision, none of them will get it. Actually they will get it by some signal from AP as we will see. After some time A may be sending the request again and may be it is has gone through; so once it goes through AP will issue a CTS that now CTS A can be sent. And please note that CTS A not only reaches A it also reaches B. And since now B knows that it has been reserved by A, B will back off or defer for a considerable period of time. B will defer for quite a bit of time and A will send its data, then A will get its acknowledgement. This is an RTS CTS based scheme. (Refer slide time: 24:49 - 25:53) 

This is another example. A wants to send data; so it sends an RTS and gets a CTS from B. A can now send the data. By the way, this RTS and CTS have been detected by C and D also, so what they do is that now they know that somebody is communicating so this NAV or network allocation vector. It automatically puts itself off. This is a very polite kind of system so it automatically puts itself off till it gets the acknowledgement. A wants to talk to B, C is in range of A, but D is in the range of B. That is why the NAV of C starts here, whereas when B sends a CTS meant for A, then D catches it and starts it own NAV. That means it starts its own blocking time; this is called virtual channel sensing. (Refer slide time: 25:55 - 27:17)

Now just to mention how this point coordination function and distributed coordination function work at the same time ? the PFC and DFC ? and why we use DIFS and SIFS ? these two periods of time. In DIFS there are actually three time intervals, which are configured. This is DIFS; this is PIFS for point control function; and this is SIFS. Please note that when the medium is busy, after that if somebody wants to send, he can not send immediately. He has to wait for DIFS amount of time. If PCF is also operating at the same time and PCF wants to send something, PCF has to start doing that within this PIFS amount of time. So somebody wanted to send and is waiting for DIFS; when he gets this, when the PCF grabs the channel, then this other node will defer for a longer time and there is an SIFS, after which acknowledgements are sent. And after this DIFS, there is a contention window where there may be a random back-off and the next frame is sent. (Refer slide time: 27:19 - 28:17)

Suppose this was the previous frame. Within SIFS, the acknowledgement and the control frame and next fragment may be sent here; so either acknowledgement or control frame or next fragment is sent here. PCF: frames may be sent here. That means after PIFS amount of time, the PCF or point coordination function will grab the channel. If PCF has not done that, then after DIFS amount of time, it can be distributed ? that means anybody can try to send anything. There is another time which is called EIFS, for bad frame recovery. So SIFS is for short inter frame spacing; PIFS is for PCF inter frame spacing; DIFS is for DCF inter frame spacing; and EIFS for extended inter frame spacing. These are the different kinds of spacing. This way this PCF and DCF can work at the same time. Another point is that if you have a very large frame, there may be a problem in the sense that  (Refer slide time: 28:38 - 29:45) 

if a large frame becomes garbled, a large frame has a larger window, where it puts off every body. So for better throughput, it may be a good thing to break up a large frame into smaller fragments. After an RTS CTS, it may send as one small fragment; then acknowledgement fragment to an acknowledgement; and so on. The other thing is that a large frame is more likely to beget errors and if you just do the calculation, you will find that if you break it up, there may be orders of magnitude difference between the error probability of a large frame and a small frame. So overall, your throughput may be much better and in an especially noisy situation, your throughput may be much better if you send smaller fragments. For smaller fragments, first of all you can do some error handling locally, and you can handle it ? that is one thing. Secondly, for a large frame, the probability of error is much higher. (Refer slide time: 29:46 - 32:09) 

We will discuss just a little bit about the 802.11 frame addressing. If you remember, in an Ethernet, we had two addresses: the source address and the destination address. Here, actually very surprisingly, we have four addresses. And just to show you why, address 1 is the MAC address of wireless host or AP to receive this frame. So this is the destination, immediate wireless destination, that is, wherever you want to land up on this wireless link. Address 2 is the MAC address or wireless host or AP transmitting this frame. This is the source address, so to say. Now the point is that, after all, quite often what you want to do is that you are not always interested in the technology used for this wireless transmission. You are trying to connect to a network, which is in the outside world. So this is what will happen ? this access point will connect to a router, or it may connect to a LAN and that LAN may be connected to a router. So basically what you have to do for going out of this network altogether ? that means not only this wireless part of the LAN or the wire part of the LAN ? you will have to know the MAC address of that particular port of the router, which you want to reach as a next stop. Then the router will decide to go next so the MAC address of that router must also come from the source itself. So address 3 is for that MAC address of router interface, to which AP is attached and AP may be attached to a LAN and it may have multiple addresses. Address 4 is used only in ad hoc mode; we will not discuss it there. These are the four addresses; yet another thing is the payload. The payload is from 0 to 2 kb. These are all in bytes: all MAC addresses are in 6 bytes; MAC addresses are 18 bytes just for 3 addresses; and for 4 addresses it is 24 bytes. (Refer slide time: 32:10 - 32:33)

So this is the picture ? originally we had a just the routers, MAC address and AP MAC address. These two ? destination and source address ? when you are sending from a wireless host the AP MAC address, the host MAC address, the router MAC address are address 1, address 2 and address 3. (Refer slide time: 32:36 - 33:25) 

Let us now talk about the other fields ? there is a duration of reserved transmission time in the RTS CTS system that we were talking about, and that we showed you. There is a duration of reserved time. Then there is a sequence control ? this is the frame sequence number for reliable ARQ. Since you are doing acknowledgement with retransmission request, you require a 6-sum sequence number for that window. We have discussed this and now we have a sequence control number over here. Then of course, there are other fields. It could be frame type; it could be RTS type; CTS type; or the acknowledgement data subtype. We need not go into all of these. (Refer slide time: 33:27 - 34:23) 

And then we talk a little bit about the mobility within this, because whenever we are in wireless we want to be mobile. If we are going from one from under one AP to under another AP, that means from one BSS to another BSS there is the basic service set that is from one cell to another, so what the mobile host we will do is that it will sense whoever is stronger and he will connect there so there is some chance of confusion in this. But since he handles it, as he moves he will connect from AP 1 to AP 2. But this is assuming that these two APs are in the same network. If these two APs are in different networks, then the situation is a little more difficult and we can not handle it at this layer (Refer slide time: 34:25 - 37:00)

directly. We have quickly covered 802.11, which is the most common kind of wireless LAN that we see today. In many places, we have 802.11; actually in some places they are also called hot spots. That means this is under some AP, so that if a person is in that hot spot he can connect to the network and there are some campuses, at least some places, where a large number of APs have been deployed so that you are continuously ? wherever you are in that whole campus ? always in the network. That is one kind of system, that is, 802.11. Next we come to another kind of wireless systems, namely wireless MAN; that means wireless metropolitan area network. What we want to do is that we want to connect an entire metropolis with this; obviously this 802.11 is no longer sufficient. First of all, the power is low. Actually in 802.11, in order to handle more number of users, we keep the power low so that we have smaller sized cells, etc., but in this metropolitan area network there will be many users in the same cell; working in that 2.4 GHz ISM band will not be sufficient any more. So we have to go for a much wider range of frequencies, and for this, we need to go to a higher frequency in the so-called millimeter wave region. Millimeter wave means when the wavelength is at the millimeter order. There is a standard for this wireless MAN; this is called 802.16. What we might do is that we might have a large tower because these millimeter waves usually travel in straight lines. So what we have to do is that we have to have a line of sight to the base station. We have to have a large tower so that everybody can be on the line of sight, and these different base stations may be connected through a wired network, or this base station would be connected to the general network through may be a fiber optic line or something. (Refer slide time: 37:02 - 37:44)	

This is the 802.16 protocol stack; this is orthogonal phase shift keying or quadrature amplitude modulation QM 16 or QM 64. We are not going into these or the different kinds of modulation techniques, which are used. There is a transmission convergence sub-layer; that means how to handle it from here ? once again we do not bother about this. We will just talk a little bit about the MAC sub-layer common part and the service specific convergence layer. We will just talk a little bit about it. As I said, so many systems are coming up these days that it will not be possible to handle all of them. This is (Refer slide time: 37:46 - 39:03)

just the frame format. There may be a data frame and the control frame. The control frame is the bandwidth request frame.  There is a connection ID; this binds the end points to the system. There is a connection ID, through which any particular system would get a chance to communicate. The first bit defines if it is a data or a control frame; if it is a data frame, the first bit is 0, if it is the control frame, the first bit is 1. Then it says whether the payload is encrypted or not; 1 or 0 type is the type of the frame. There are management frames and things like that; so C1 is the check sum. There is a check sum indication key that is used. Once again, we need not go in to all the details but it uses CRC error correction. So header is the header portion; for the header portion there is a CRC and the data connection ID etc. Basically the access to the medium is controlled through this connection ID. (Refer slide time: 39:04 - 39:00)

There are different service classes, which are defined in this: one is the constant bit rate service for voice real time; variable bit rate service (this is a VBR); RTVBR or a non-real time variable bit rate service, that means, NRTVBR, for high quality data; and for ordinary data, email, http, etc., this is the best efforts service. There are different service classes in 802.16, and all these are possible because here the MAC is simply controlled by the base station. The key architectural principle (Refer slide time: 39:53 - 41:09) 

is traffic control by the base station. The base station controls the traffic totally. It creates frames of time slots and allocates timeslots to connection IDs. So time slots are allocated by service class. This means that if there is a constant bit rate service, what the base station would do is that in every frame it is going to allot one or more slots to this constant bit rate service, so that it gets constant rate updates. So whoever reserves or requests that constant bit rate service, if he is not using it at any particular point of time, then it is going empty. That is why he has to pay higher for this constant bit rate service. Similarly there are variable bit rate services, and finally, with just a slightly higher priority than the non-real time one and just the available bit rate service, whatever else is left may be given to those connection IDs, which are only getting available bit rate service. These are just (Refer slide time: 41:11 - 42:07) 

pictures showing frames and each of the frames will have some slot. There are some guard times between the frames. Some of the slots are reserved for upstream traffic, whereas some of the other slots are for the downstream traffic. Quite often, what happens is that in this metropolitan area networks or in many networks, the downstream traffic turns out to be much higher than the upstream traffic. We all know, for example, if you are surfing the net, which is a very popular activity, you just send one request, which is a very small thing, and in response to your http request, a large page with a lot of graphics, etc., may be downloaded. So the downstream traffic turns to be much larger. There is a lot of asymmetry here; that is why there are a few upstream slots and a lot of downstream slots. (Refer slide time: 42:08 - 42:55) 

So frames and time slots are for time division multiplexing or duplexing. Actually it is for time division duplexing, because both upstream and downstream traffics are given some slots. Duplexing means it is going in both directions. Base station allocates time slots in frames to connections. There are some connection IDs, and to a particular connection ID, the base station may allocate time slots. CBR as I said, constant bit rate services, are of the highest priority. RTVBR, that is, real time variable bit rate has the next highest priority; NRTVBR has the third highest priority. This can be delayed and anything remaining is the best efforts contention based access of unallocated timeslots to other kinds of traffic. This is how it is done. Next we move on to (Refer slide time: 42:56 - 44:55) 

another end of the spectrum. First we talked about  wireless LANs; that means 802.11, and as I mentioned, it has a lot of variations like 802.11 a, b, g, etc. So 802.11 is the LAN side. Then we talked about MAN, metropolitan area network, and now we are going to the other end of the scale, which means very small networks, let us say, personal area networks. That means a small area is covered under a network ? between whatever I have in this pocket and this pocket, and my in my hand. We will talk about one thing, which is quite popular, namely Bluetooth, which is 802.15. So 802.15 is for the personal area network group; 802.11 is for local area network; .16 is for metropolitan area network; and .15 is for personal area network. This is less than 10 m in diameter; so you see this is very small. It has a replacement for cables, mouse, keyboard, headphones, etc. So whatever I am using may be replaced by wireless links ? that was the idea. This is ad hoc; that means there no infrastructure is necessary. This works with the idea of master and slaves; that means slaves request permission to send to master, and master grants the request. So in any such cell or radius of coverage, there will be a master and then there will be some slaves. M is the master device, S are the slave devices, and P are the parked devices. That means these are inactive at the moment, so they are called parked devices in Bluetooth. (Refer slide time: 44:57 - 46:52)

So Bluetooth and 802.15 are almost the same. There are some small differences, but this not very important. It operates in the 2.4 GHz industrial scientific, that is, ISM band, and is unlicensed, packet switched, and 1 milliwatt. This is a very important issue ? that this uses a very small amount of power as opposed to, let?s say, 500 milliwatt for a cell phone. This is low cost; that means up to 10 m to 100 m range and uses frequency hop spread spectrum; so FHSS is used.  We will see what kind of an FHSS, which divides a frequency band into a number of hop channels, is used. During connection, devices hop from one channel to another 1600 times per second; so you see it is hopping the frequencies very fast. So that is one good thing because if some part of the frequency band has noise, it has got better noise immunity because it is hopping such a large number of times. There are a large number of channels ? why a large number of channels? We are talking about a personal area network; but nowadays I may be using so many different gadgets, etc., when I am using my PC, I may be having a cell phone; I may be having a laptop; I may also have a desktop in front of me. Each of them will have a mouse and all these peripherals ? let?s say it has a monitor and all these peripherals may be connected in some way through wireless. So there may be so many things; altogether about 79 channels are possible in Bluetooth and so they go on hopping in the frequency. (Refer slide time: 46:53 - 47:28) 

Bandwidth is 1?2 mbps; we are not looking for a very large bandwidth over here, but this is just more for control and function rather than downloading files. It supports up to eight devices in a piconet. What is a piconet? Two or more bluetooth units sharing a channel is called a piconet. It has some built-in security line of sight transmission through walls and briefcases because of the frequency band which it uses. It uses integration of TCP/IP for networking. (Refer slide time: 47:30 - 47:54)

So piconet is a small area network. It is ad hoc, which means that a network with no predefined structure.  There is no predefined structure; it is based on available nodes and their locations; it is formed and changed in real time. As you can see, these networks are being formed and being changed in real time; so they may be changing all the time. (Refer slide time: 47:56 - 48:38)  

The basic connectivity is point-to-point; that means from the master to the slave. Piconet is point-to-multipoint master multiple slaves; two or more piconets can be connected to form a scatternet. By the way, how does a piconet start? Anybody can start it and claim himself to be a master, and the other devices, which are coming in later, will become the slaves. So anybody can start and become a master. This is one piconet; this is another piconet; and they may be connected. The two piconets may be connected to a scatternet. If you want to have a scatternet, then we have to have a bridge from this piconet to this other piconet. So we have a bridge slave. (Refer slide time: 48:39 - 49:26)

So 802.15 version of the Bluetooth protocol architecture ? these two are slightly different, but we will not bother about it. We have the application profiles; then we have the physical radio base band and the link manager. Link manager means the radio link manager, and then there is a middle layer, which is the service discovery, telephony, RF communication, and so on. Once again, we do not have the time to go into the details of this. The idea is that you can switch from one kind of service to another kind of service, depending on the context and situation. (Refer slide time: 49:27 - 49:38)

This is a more detailed picture of the different kinds of protocol. Let us skip this also. (Refer slide time: 49:39 - 50:29) 

In the physical layer, it has 79 channels, each 1 MHz, using frequency shift keying with 1 bit per symbol. So it comes out to about 1 mbps per channel. Of course, this is 1 mbps. The individual devices finally do not get a 1 mbps throughput for the payload part, because the efficiency is quite low. Much of the 1 mbps is taken up with protocol overheads caused by the frequency hopping. So this takes about 250 to 260 micro seconds needed to stabilize the radio after the hop. So this leaves about 366 bits for actual data, of which 126 bits are headers, leaving only 240 bits for data per slot. So what was supposed to be 1000 bits has become 240 bits. But for small devices, which are getting locally connected to each other, even 240 bits per second kind of speed may be more than enough. But so many different channels are possible; 79 channels are possible. This is just a little bit about the Bluetooth frame structure; we have the access code, and then the header, and then the data. (Refer slide time: 50:49 - 51:16)   

And the header will contain the address type, etc., and some flags and some checksum; also it is an 18-bit header. It is repeated three times for a total of 54 bits. The access code identifies the master to its slaves; one master and upto seven active slaves. Some slaves are in parked mode. (Refer slide time: 51:17 - 51:28)

So these are the system blocks: we have a Bluetooth radio; a Bluetooth link controller; and a Bluetooth link manager.  (Refer slide time: 51:29 - 52:00)

So two physical link types have been defined: one is synchronous connection oriented link ? between the master and a single slave for audio and data ? and the asynchronous connectionless ACL link, point-to-multipoint between the master and all those slaves on the piconet for data only. This is for data and this is used for others. If some voice channel is there, you can get a synchronous connection oriented link there so that you get acceptable quality of service. (Refer slide time: 52:08 - 52:24)

Multiple access scheme is based on FH CDMA, that is, frequency hopping CDMA. High speed of hops and code division multiple access offers the best properties for ad hoc radio systems. As I said, 79 hop carriers have been defined at a 1 MHz spacing. (Refer slide time: 52:26 - 52:58)

The Bluetooth has been designed to allow a large number of independent channels, each channel having only a limited number of participants. Theoretically, the spectrum with 79 carriers can support 79 mbps, but as we have seen, the efficiency may be something of the order of 25%. So it will be much less than that. Different channels have different masters and therefore, they also have different hopping sequences and phases. (Refer slide time: 52:59 - 53:38)

By definition, the unit that establishes the piconet becomes the master. As I said, anybody can start a piconet and become his master. In Bluetooth, the master implements centralized control; once again we do not try to do any distributed control. It is a small system so we do a centralized control by the master. Communication is possible only between the master and one or more slaves, which means that the slaves do not communicate with each other directly. It has to go through by the master. The master unit schedules the traffic in both the uplink and the downlink. (Refer slide time: 53:39 - 54:08)

There are various types of error corrections, which are possible. This includes both FEC and packet retransmission schemes at the base band level. Bluetooth makes use of three types of error correction schemes: one-third rate FEC, sending three copies of each bit with a majority logic; two-third rate FEC, a form of some kind of hamming code; or automatic repeat request or ARQ. So this is the error correction scheme that is used. (Refer slide time: 54:10 - 55:54)

Just now we have talked about three different ends of the spectrum. I will just mention one or two more, just to show that there are all kinds of other possibilities. For example, after this Bluetooth became somewhat popular, there was a group who wondered why not make the radius of operation of this even smaller. But here the main emphasis would be on long battery life, so that you put a small battery in a small device and it will just work till the battery?s shelf life is over. It will have a very low power. We have the 802.15.4. Similarly, there is an 802.15.2 and 802.15.3. We are not covering any of them; this is just to give you a feeling of the kinds of things, which are going on. 802.15.4 is a simple packet data protocol for low rate; it has no quality of service; has wireless personal area network; is a low power, low cost, device. So low power and low cost are the most important things. Naturally you will get low rate also, but for many applications, this may be quite fine. The channel access is via carrier sense multiple access with collision avoidance, and optional time slotting. It has message acknowledgement and an optional beacon structure ? beacon means the signal, which may be sent centrally to synchronize other systems. So three bands and 27 channels are specified: 2.4 GHz and 16 channels; 868.3 MHz and so on. (Refer slide time: 55:56 - 56:37)

It works well for long battery life; it has selectable latency for controllers, sensors, remote monitoring, and portable electronics. For example, a sensor just stays there; it is supposed to do its work, which is sensing, and may be send little bits of data from time to time. So it has a low rate, no quality of service guaranty, etc. is required, but low power and low cost are very important. That is the focus of this particular group. It is configured for maximum battery life; has the potential to last as long as the shelf life of most batteries. (Refer slide time: 56:40 - 56:59)

So MAC uses 64-bit IEEE or 16-bit short addresses, using local addressing. That means, if it is just locally, you can have your own 16-bit or you can use the full 64-bit  IEEE address; it has a simple frame structure, reliable delivery of data, etc.  (Refer slide time: 57:01 - 57:29)

So as I said, there are two channel access mechanisms: one is the non-beacon type, where it uses a standard ALOHA with CSMA CA, that is collision avoidance, and positive acknowledgement; or we can have a beacon enabled network, where it is synchronized. It has a super frame structure for dedicated bandwidth and low latency setup by network coordinator to transmit beacons at predetermined intervals. (Refer slide time: 57:30 - 58:14)

Let us now compare quickly between 802.15.4, which is the low rate and low power one, and 15.1, which is the standard Bluetooth: it transmits smaller packets over large network and larger packets over small network. They are mostly static networks with many infrequently used devices. This is an ad hoc network, which is more dynamic. You can do things like file transfer here, which you do not look forward to doing here. This may be used for home automation, toys, remote control sensing, etc. This may be used for screen graphics, pictures, hands-free, etc. So this is a somewhat different niche of application, which is the two groups, but both use wireless with different emphases. (Refer slide time: 58:15 - 58:55)

As I said, there are many standards ? I just listed some of them. There are many more, which I have not put over here: 802.11 b, which gives 11 mbps; a, which gives 54 mbps; g, which gives 54 mbps, but this is backward compatible with b, because b was the one which was most widely deployed in the beginning. 802.16 is for a MAN; Bluetooth has about 30-feet radius; we have talked about GSM GPRS when we talked about cell phones; it is going to 3G. People also talk about of 4G, but nobody knows when even 3G will actually get widely deployed. We have just seen UWB, and there are so many others. (Refer slide time: 58:59 - 59:06)

The one last point is that if you have a wireless LAN, you would want to have a bridge for connecting the TCP/IP stack, etc. We will talk about TCP/IP later on. To transmit from one to another, we require a bridge in-between. This WLAN may be a plain wireless LAN extension and the application will sit on top of this. We require a seamless support for this bridge; that is very important. There are a large number of such protocols, because there is a lot of interest over there. Some of these protocols, etc., will tie up some of them and naturally become very widely used and this is one of the most important areas of networking today. Thank you. 

 COMPUTER NETWORKS
Prof. Sujoy Ghosh
Dept of Computer Science & Engineering Department
 IIT Kharagpur

Lecture No. 24
ATM: Asynchronous transfer mode
(Refer Slide Time: 00:44)

In this lecture we will start our discussion on another very important technology, namely, Asynchronous Transfer Mode or ATM. (Refer Slide Time: 00:57 - 00:59) 

Slide Time: 02:25 - 03:35
The ATM was originally envisioned as a technology, which will solve all problems. It is present in the LAN, WAN and it gives very good quality of service. When introduced, it was considered a very ambitious plan. Unfortunately, it did not work out that way because the standardisation took a lot of time. When people from computer world and people from communication world start discussing things and come up with a standard, it becomes very complex and also the cost becomes quite high. Although the ATM made its debut in some local area networks, slowly it has been replaced in most of the local area networks. But still it is a very strong and important technology in the wide area network. So ATM remains in operation in a lot of places today and we will look into this ATM (Asynchronous Transfer Mode) now. (Refer Slide Time: 02:25 - 03:35) 

Slide Time: 03:04 - 03:35

Why ATM networks? It is driven by the integration of services, i.e., wires, data, and everything else are integrated into the same kind of network. This is the vision of the performance requirements of both telephony and data networking. This was called broadband integrated service vision or B ISDN. Telephone networks support a single quality of service and are expensive to boot. Internet supports no quality of service; but it is flexible and cheap. The ATM was developed to utilize the best of both. (Refer Slide Time: 03:04 - 03:35)   

Slide Time: 03:37 - 04:45
ATM networks were meant to support a range of service qualities at a reasonable cost and hence intended to subsume both telephone network and the internet. But the cost and complexity turned out to be high and now IP-based technology is going to fill the above role. But as we have seen, in many service providers, ATM is still present and people are deploying ATM networks even today. So ATM will remain in existence for quite some time. (Refer Slide Time: 03:37 - 04:45) 
 

Slide Time: 04:47 - 05:15
Let us see in brief the history of ATM. In the 1980s, a packet research began and in 1986, it adopted the B ISDN approach. In 1989, a 53-byte cell was permitted, which was a rather small value. The communication experts wanted a small value, but computer experts wanted a large value and there was a dispute. In 1991, the ATM forum was set up. In 1992, the ATM forum issued its first spec and added user committees. In 1996, it approved the anchorage that occurred. From then on, death of ATM in the enterprise and rollouts in the carrier networks occurred. But it is still important today. (Refer Slide Time: 04:47 - 05:15)  



Slide Time: 06:27-06:45
The basic points of ATM are that it transmits all information in small, fixed-size packets called cells. Since they are of fixed size, the switch design becomes easier. Cells are transmitted asynchronously at high speeds. This is basically statistical multiplexing. In TDM we saw that one of the advantages of packet networks is that it was more efficient in terms of bandwidth utilization. The circuit switch network was less efficient because when there is no communication, the circuit is remaining idle and bandwidth is wasted. So the ATM tried to address that using statistical multiplexing. That means, the size of cell is fixed and these cells can be pushed in any order by the end stations, but the network will be connection-oriented. This was necessary in order to accommodate the quality of service that everybody wanted. This is asynchronous unlike SDH, which is a synchronous system. Nevertheless, each cell is 53 bytes long with 5 bytes of header and 48 bytes of payload. (Refer Slide Time: 06:27-06:45)  



Slide Time: 07:49 - 08:05
To make an ATM call, a message is to be sent first to set up a connection. Subsequently all cells follow the same path to the destination. So this is just like circuit switching. First of all, you have to set up a connection where the connection is not physical but they are virtual circuits. That means the path should be found first and then along the path all the ATM switches, etc., would reserve some resources for the connection. This reservation of the resources in all the nodes along the way constitutes the virtual circuit. All the cells would flow through this same path. This is a connection-oriented system, but cells are switched for better efficiency. It can handle both constant rate traffic and variable rate traffic. Thus it can carry multiple types of traffic with end-to-end quality of service. (Refer Slide Time: 07:49 - 08:05)




Slide Time: 09:14 - 10:01
ATM is independent of transmission medium; we will see later about the different transmission media that are possible. It does not prescribe any particular rule for transmission medium. They may be sent on a wire or fibre by themselves or they may also be packaged inside the payload of other carrier systems. This is a very interesting situation. For example, when you are carrying just ATM traffic and in-between you have a WAN segment which has SDH, the ATM cells can be pushed into some SDH container and sent along correctly. Reversal of this process is also possible. Suppose there is some SDH traffic and in-between there is an ATM link, the VCs can take some constant bit rate service on an ATM link. So ATM transmitted on SDH is possible and SDH transmitted on ATM is also possible. ATM by itself can be used as a transport network. So the carriers or the service providers have employed a lot of ATM in their backbone.  So ATM is still prevalent, although it is not present in LAN or enterprise network. (Refer Slide Time: 09:14 - 10:01) 




The delivery of packets is not guaranteed but the order is. As this is circuit-oriented or is a set of virtual circuits, the order of the cells will remain the same, so that the higher layer cells need not be changed. For example, IBM suggested 25 Mbps for ATM NICs for taking it to the desktop, but it did not survive. One of the reasons for this was the cost.  The ATM NICs were more costly. The ATM switches are costlier than the ordinary (Ethernet) switches. There were few or no software for ATM as most of the software developed was based on IP. People were also not ready to move all the software into ATM, which would involve high cost and since the market did not expand as expected, the cost remained high for quite some time. So this was the difficulty of this approach.  Much of the ATM devices operated at 155 Mbps (OC 3) or, even higher, 622 Mbps (OC 12) speeds. The standardization process took too long and the resulting technology was too complex (costly) to remain in the cutting edge. The basic ATM concepts are: virtual circuits, fixed-size packets (cells), which allow fast hardware switching, small packet size, statistical multiplexing and integrated services. That means different qualities of services can coexist at the same time with good management and traffic engineering features. Scalability in speed and network sizes is possible. We will look at a few of these in the next lecture. (Refer Slide Time: 11:03 ? 11:35) 

Slide Time: 11:03 ? 11:35
ATM applications could be ATM deployments in frame relay backbones. Frame relay is one kind of wide area network connectivity, which is now slowly going out, but ATMs give connectivity to backbones. It is also deployed in internet backbones and aggregating residential broadband networks (cable, DSL, ISDN, etc). They can feed into an ATM switch and then get transported. Carrier infrastructure for the telephone and private line networks deploys ATM, and this is one area where it is still going strong. (Refer Slide Time: 10:36 - 12:23) 


Slide Time: 10:36 - 12:23

The failed market tests of ATM were the ATM workgroup and campus networks, ATM enterprise network consolidation, and end-to-end ATM. These did not happen because of software and these did not take off because of cost. (Refer Slide Time: 12:25 - 13:36) 

Slide Time: 12:25 - 13:36
Now we will compare the synchronous, i.e., telephone networks and ATM. Telephone networks are synchronous and we know it because 125 ?s is the frame rate, which is very sacrosanct in this world. ATM is asynchronous transfer mode. Phone networks use circuit switching, whereas ATM networks use packet or cell switching with virtual circuits. In a telephone network, cells from a particular source or information or payload from a particular source will come periodically as shown, whereas in ATM they can come any time if the line is free. (Refer Slide Time: 12:48 - 13:26)



Slide Time: 12:48 - 13:26
 

In telephone networks all rates are in multiples of 64 kbps, but with ATM service you can get any rate and you can vary the rate with time by programming for the required rate. If you require a constant bit rate service of 10 kbps in a data service, it is possible to have a virtual circuit where the reservation of resources would be in that fashion. This kind of service is used with current phone networks and all high speed circuits are manually set up. ATM allows dialling at any speed and rapid provisioning since this is done through software and signalling; ATM allows this. As far as telephone networks are concerned, there are lots of advantages of using ATM. (Refer Slide Time: 14:09 - 14:33)


Slide Time: 14:09 - 14:33

Now let us compare ATM with data networks. ATM is ?virtual circuit? based. The path (and optionally resources on the path) is reserved before transmission. IP on the other hand is connectionless and end-to-end resource reservation is not possible directly. Indirectly people are still trying to do that because quality of service remains an important issue. RSVP is a new signalling protocol in this IP domain in the internet, which tries to reserve resources. There are other ways to do this and one way which has become quite popular is MPLS; and we will talk about MPLS in a different lecture. (Refer Slide Time: 14:39 - 15:14)


Slide Time: 14:39 - 15:14
 
ATM cells are fixed and are small in size and there is a trade-off between voice and data. But IP packets are in variable sizes. ATM provides QoS routing coupled to signalling called PNNI. Internet provides ?best-effort? service, aiming only for connectivity. (Refer Slide Time: 15:15-15:41) 


Slide Time: 15:15-15:41
For addressing, ATM uses a 20-byte global NSAP addresses for signalling and 32-bit locally assigned labels in cells. Actually when we are talking about ATM, there are two kinds of addresses that are referred. One is the ATM address, which is 20-byte (160 bits) long. It is a huge address and it requires a large address space; this address space is divided in a particular way, which will be discussed later. There are different schemes of addresses, which people tried to subsume in this ATM addressing; this is one kind of ATM addressing. For setting of a path, this 20-byte address is required but once a path has been set up this address is not required any longer because the path has been set up. The only thing is that since this is a virtual path in-between when a node gets a cell, it must know to which particular virtual circuit this belongs. So some kind of virtual circuit identifier is required and it is a much smaller address. This is another kind of addressing. But IP uses 32-bit global address in all packets. ATM offers sophisticated traffic management and this is one of the strong points of ATM and still remains strong in it. But in TCP/IP, congestion control is packet-loss based. Whether the packet is lost or not, ATM gives much more sophisticated QoS. (Refer Slide Time: 15:43 - 17:26) 



Slide Time: 15:43 - 17:26

Let us see the pros and cons of fixed-size packets. Pros are that it uses simpler buffer hardware, i.e. packet arrival and departure requires us to manage fixed buffer sizes, simpler line scheduling, i.e. each cell takes a constant chunk of bandwidth to transmit, and it is easier to build large parallel packet switches. (Refer Slide Time: 17:29 - 17:55) 


Slide Time: 17:29 - 17:55
The disadvantage is the overhead, i.e. for sending small amounts of data at the same time for each cell you have to have this 5-byte header on only 48 bytes. So 10% is already gone on the header and if a large amount of data (may be several megabytes) is to be sent, you need a lot of cells. Hence overhead becomes important. Large frames, which are to be sent, have to be broken up into small cells. All these cells are to be put together to form the original large frame. This segmentation is on one side and the reassembly is on the other side. The overhead and the cost will also come in and the last unfilled cells, after segmentation, waste the bandwidth. This is not very important. (Refer Slide Time: 17:56 - 18:56) 

Slide Time: 17:56 - 18:56
When the cell is smaller, an endpoint has to wait for lesser time to fill it. So there is low packetization delay. When the packet is smaller, the header overhead is larger. Standard body balances the two to prescribe 48 bytes + 5 bytes, which is equal to 53 bytes.  Therefore the maximal efficiency could be about 90% only. (Refer Slide Time: 18:58 - 19:25) 

Slide Time: 18:58 - 19:25
Now we will discuss ATM layers. It was done by a committee in which there was a dispute and the ultimate result was that the ATM protocol and its layers, etc. were framed. These are quite complex. ATM is the three-dimensional figure and now we will talk about some of the important sub-layers in it. On the control and management side of it, there are number of layers and on data side there are number of them. (Refer Slide Time: 19:26 - 21:18) 

Slide Time: 21:19 - 22:23
The layers are: CS, the convergence sub-layer, SAR, the segmentation and reassembly sub-layer. These two layers put together is the ATM adaptation layer, which is called AAL. There are different kinds of AALs. AAL 1,2,3,4,5, etc. but they all have these two sub-layers. Then we have the ATM layer and this is somewhat in-between the data link layer and the network layer. The transmission convergence sub-layer, in which the transmission will come and will be put back, and the physical medium dependent sub-layer (PMD) together form the physical layer. (Refer Slide Time: 21:19 - 22:23) 


Slide Time: 22:23 - 23:03
In the above slide, there is an ATM adaptation layer, an ATM layer and a physical layer in the end system. This is a very simplistic view of the ATM layers. There are other layers for control and management function, which will be dealt with later. There are two end systems communicating from the ATM adaptation layer. The ATM adaptation layer will readily communicate with other higher layers of the software. It will come through this AAL, physical layer, then go only up to the ATM layer and then go to the other side. This is a simple view of a stack. (Refer Slide Time: 22:23 - 23:03) 


Slide Time: 23:03 - 23:27
ATM layer?s adaptation is mapping of application (e.g.: voice, data, etc.) to ATM cells. The physical layer could be SONET or it could simply be a DWDM system. ATM layer handles transmission/switching/reception, congestion control, cell header processing, sequential delivery, etc. (Refer Slide Time: 23:03 - 23:27)  

Slide Time: 24:40 - 25:03

Now let us discuss about the layers in detail. The top one is the top sub-layer of the AAL (ATM adaptation layer), the convergence sub-layer. It offers different kinds of services to different applications. Here the different types of services are supposed to converge and all of them are supposed to do ATM. So for convergence this ATM adaptation layer is used. For example, for a voice channel, constant bit rate traffic is required; and similarly for data, some other kind of traffic is required. So different classes of services were defined depending on the kind of AAL (AAL 1,2,3,4,5, etc.) and the convergence sub-layer negotiated that and came to a bandwidth contract. So the different services that are offered are the CBR, which is constant bit rate or bandwidth guarantee, and which is suitable for real time traffic. ABR is for available bit rate, suitable for bursty traffic and feedback, bout congestion. UBR, which is unspecified bit rate, is the cheapest of all and suitable for bursty traffic, may be data traffic. It provides a specific AAL service at an AAL network service access point (NSAP). (Refer Slide Time: 24:40 - 25:03) 

Slide Time: 25:34 - 26:16
NSAP refers to network service access point. The other sub-layer of AAL is the segmentation and reassembly sub-layer. It segments higher level user data into 48-byte cells at the sending node and reassembles cells at the receiving node. This sub-layer is usually implemented with ASIC. One of the reasons ATM was envisioned as a system, which will really scale to very high speed, is that cell segmentation and reassembly is to be done very fast. Usually it is done with the help of ASIC. ASIC is an application specific IC for doing the segmentation and reassembling. It tears down messages passed from the upper layer and converts them to cells. Some padding may be necessary to make it a multiple of 48 bytes. At the destination the stream of cells are reassembled. (Refer Slide Time: 25:34 - 26:16) 



Slide Time: 26:30 - 26:50

The different types of AAL are AAL 1,2,3,4, and 5, which give different classes of service, class A, class B, class C and class D. Class A is connection-oriented CBR (e.g. voice) and it is supported by AAL1. Class B is connection-oriented VBR (e.g. packet based video) supported by AAL2. Class C/D may be connection-oriented VBR (e.g. file transfer), connectionless VBR (e.g. LAN data) supported by AAL 3/4, i.e. AAL 3/4 may be connection oriented VBR or connectionless VBR. AAL 5 is a simple and efficient adaptation layer (SEAL) supporting class C/D for bursty error control at higher-layer protocol. These AALs are complex since ATM got into the service providers? backbone. But many of these AALs, etc. were never widely deployed. (Refer Slide Time: 26:30 - 26:50)



Slide Time: 27:03 - 27:53
The convergence sub-layer (CS) interprets the type and format of incoming information based on 1 to 4 classes of service assigned by the application. Class A is constant bit rate (CBR). It is connection-oriented and there is strict timing relationship between source and destination, i.e. voice. If such a very sensitive quality of service like voice is required, Class A service can be used. But Class A service is more costly. (Refer Slide Time: 27:03 - 27:53)

Slide Time: 27:54 - 28:15
Class B is variable bit rate (VBR) service and connection oriented. It has strict timing, e.g. packet mode video for video conferencing. Class C is connection oriented VBR but without a strict timing service. So there is a slight difference between class B and class C; e.g. LAN data transfer applications. Class D is connectionless VBR with no strict timing; e.g. LAN data transfer applications such as IP. For example if it is frame relay, then the person who had taken this frame relay service should have some original negotiation about the speed. So class C is a little better than class D, but not better than class A or B. (Refer Slide Time: 27:54 - 28:15)


Slide Time: 28:17 - 2:32

AAL 5 was introduced for data services. It supports both message mode and stream mode. In the message mode, a packet of length up to 65 kB may be passed to the AAL layer to have it delivered to the destination either on a guaranteed or ?best-effort? basis. (Refer Slide Time: 28:17 - 2:32) 


Slide Time: 29:53 - 30:20


The service categories available are ABR, UBR, CBR and VBR. ABR is Available Bit Rate. In this source bit rate, source follows network feedback and there is maximum throughput with minimum loss. Hence the network gives some feedback. This is just to give you an idea about how quality of service is handled in ATM because a lot of things which were learned in ATM are also employed today in some other guise in MPLS or some kind of very new IP technology. How quality of service can be guaranteed still remains a very important issue in networking today. As people are talking about convergence of voice, data, video, etc., into the same network, we require some guarantee about quality of service. Whatever happens in the pure data network like delay or jitter etc. may not be acceptable for this kind of service. So for convergence, quality of service is important and ATM offered a good quality of service. UBR, Unspecified Bit Rate, is of course the cheapest of all. The user sends anything with no feedback and no guarantee. Cells are dropped during congestion between UBR cells. (Refer Slide Time: 29:53 - 30:20)

Slide Time: 30:20 - 31:00

CBR, Constant Bit Rate, is one in which the user declares the required rate. For this, throughput delay and delay variation are guaranteed. In VBR the average and maximum rates are declared. It has two different types. One is real time VBR for voice, conferencing, etc. with maximum delay guaranteed and the other is non-real time VBR for stored video. (Refer Slide Time: 30:20 - 31:00)



Slide Time: 31:01 - 31:35

Let?s compare ABR and UBR. In ABR, the queue is in the source because the source takes the feedback from the network and adjusts the rate. In UBR, the queue is in the network. In ABR, if the queue gets filled it gets dropped and pushes congestion to the edges. In UBR, there is no backpressure because if there is pressure, the UBR will be dropped. ABR is good if the network is end-to-end ATM. UBR is the same whether it is end-to-end ATM or backbone to ATM. ABR is very fair and good for the provider and UBR is simple for the user. But UBR is generally unfair even though it is simple for the user. (Refer Slide Time: 31:01 - 31:35)  


Slide Time: 31:38 - 31:45

There is also a concept of guaranteed frame rate (GFR). It is a UBR with a minimum cell rate (MCR). It will try to guarantee this minimum cell rate but beyond that, it is UBR. So GFR is a frame-based service or guaranteed frame rate service. In this, complete frames are accepted or discarded in the switch, and traffic shaping is frame-based. All cells of the frame have the same cell loss priority (CLP), whether they are inside the MCR or not. (Refer Slide Time: 31:38 - 31:45)  

Slide Time: 31:47 - 33:10
All frames below MCR are given CLP = 0 service, and all frames above MCR are given best effort CLP, i.e. CLP = 1 service. (Refer Slide Time: 31:47 - 33:10)



Slide Time: 33:10 - 34:01
Having talked about the different types of quality of service that are available in ATM, now let us look at the ATM layer and then we will look at the data link layer of ATM. ATM layer is akin to the network layer of OSI, although it has data link layer characteristics. As seen already, this is somewhere in-between network layer and data link layer. ATM uses globally unique addresses using the NSAP format of ISO. This is used for setting up connections. Path and circuit identifiers are used once a connection is established. (Refer Slide Time: 33:10 - 34:01) 


Slide Time: 34:05 - 34:15
ATM interfaces are different types of interfaces designated in the standards. One is a computer connecting to a private switch. There is a hierarchy of switches like private switches and public switches. Here computer in the LAN may be connecting to a private ATM switch. For this there is a private UNI or private user network interface. Similarly when a private switch is connected to public switch there is a public UNI, that is, a public user network interface. Public switches talk to each other using the interface called NNI (Network Node Interface). (Refer Slide Time: 34:05 - 34:15) 


Slide Time: 34:40 - 35:08
UNI is the user network interface (public and private). NNI is the network node interface (private and public). PNNI is Public NNI. B ICI is broadband inter carrier interface between two carriers. So if there are two different carriers, there is an ISI defined for that.  DXI is data exchange interface with a router, etc. Different interfaces were defined but some were not fully defined because they were not very widely deployed. (Refer Slide Time: 34:40 - 35:08) 

Slide Time: 35:33 - 35:55
There is a hierarchy of switches. In the carrier, there are the carrier backbone switches and carrier edge switches in the service provider frame or central office. Enterprise switches, which stopped at the carrier edge switches, LAN or campus backbone switches and the workgroup switches are in the customer frame. Actually the hierarchy of switches means they are all basically ATM switches with different interfaces; different software and different protocols are given and these links differ depending on where that switch is. (Refer Slide Time: 35:33 - 35:55) 



Slide Time: 35:56 - 36:29

Let?s see the physical layer functions of ATM. It transports ATM cells on communication channels and defines mechanical specs like connectors, etc. There are two sub-layers. One is the PMD or physical medium dependent sub-layer. It has medium dependent functions like bit transfer, bit alignment, optically electrical optical (OEO) functions, etc. (Refer Slide Time: 35:56 - 36:29) 

Slide Time: 36:48 - 37:08
The other sub-layer is the transmission convergence sub-layer. It maps cells into the physical layer frame format like DS 1 or STS 3 on transmit and delineates ATM cells in the received bit stream. It is a wavelength, which generates HEC, i.e. header of cells for transmission. It generates idle cells for cell rate decoupling or speed matching. Based on the kind of transport it is using, all these speed matching, etc., have to be done and hence, this sub-layer is called transmission convergence sub-layer. (Refer Slide Time: 36:48 - 37:08) 



Slide Time: 37:12 - 37:20
Let?s see the physical layers of ATM. In ATM no particular medium was specified and so many media are possible starting from multimode fibre (100 Mbps using 4b/5b), 155 Mbps SONET STS-3c, 155 Mbps using 8 b/10 b, single mode fibre using 155 Mbps STS-3 c, 622 Mbps, plastic optical fibre using 155 Mbps, shielded twisted pair using 155 Mbps 8 b/10 b to coaxial using 45 Mbps, DS3, 155 Mbps. (Refer Slide Time: 37:12 - 37:20) 

Slide Time: 37:23 - 37:40
Other media starting from unshielded twisted pair, UTP 3 (phone wire) at 25.6, 51.84, 155 Mbps, UTP 5 (Data grade UTP) at 155 Mbps to DS1, DS3, etc., are also possible. (Refer Slide Time: 37:23 - 37:40) 



Slide Time: 37:41 - 38:43
Actually a serious attempt was made to inter-operate with several L1, L2 and L3 technologies.  However, since ATM survived only in the service provider?s backbone and at the edge fibre, the single-mode fibre remains the most important medium today. (Refer Slide Time: 37:41 - 38:43) 


Slide Time: 38:45 - 38:56

How is ATM SONET mapping done? ATM SONET mapping is done because ATM may finally get carried by a SONET transport at some point in the WAN in a good and easy manner. In a SONET there is a pointer pointing to the beginning of the payload and this payload can actually be anywhere in the frame. These cells are packed in the frame. So cells are mapped row-wise into the frame. Cells could contain data or be empty. Some empty cells might have been put over there for late matching. (Refer Slide Time: 38:45 - 38:56) 


Slide Time: 41:08 - 41:45

Since ATM is coming at a particular rate, depending on what rate it is, the ATM is connected to the next using an SDH transport. At the user end ATM has to give some guarantee about bandwidth, etc. It has to do some provisioning and for that it carries the provisioning across this SDH link. SDH is not very difficult because, depending on what speed or what rate is required, the next higher-sized container is chosen. SDH, as you know, can accommodate various types of containers like VC 1, VC 2, etc.  So it can have various types of containers, various rates, etc., and tributaries. You take the kind of container that gives guarantee about the rate. This is how the service provider provides the virtual link. The container may be in the infrastructure of the same service provider or it may be in the infrastructure of some other service provider. Then we can negotiate and configure the SDH switch so that the ATM stream will get the kind of bandwidth across the SDH part of the backbone. So this link is a virtual one. The ATM contains a number of virtual paths and each virtual path contains a number of virtual circuits. One particular pair of users is using one particular virtual circuit so that virtual circuit would be identified with a VC number known as a virtual circuit identifier and a VP number or the virtual path identifier. For good management function, ATM is divided into virtual paths and virtual circuits. (Refer Slide Time: 41:08 - 41:45) 

Slide Time: 42:07 - 42:54

ATM cell structure has 48 bytes of payload and 5 bytes of header. The header contains so many fields. There are 16 bits or 2 bytes for the virtual circuit identifier (VCI) and 8 bits or 1 byte for the virtual path identifier (VPI). The virtual path and the virtual circuit together will define a particular stream, a particular ATM stream coming from a particular virtual circuit which is originating from some particular user somewhere. Then there is an 8-bit HEC. (Refer Slide Time: 42:07 - 42:54) 



 Slide Time: 42:55 - 43:35
This structure of the cell header shown is not constant across all interfaces. GFC is present in UNI, but in NNI this GFC has been dropped and VPI is increased. VPI is only 8 bits in UNI but is 12 bits in NNI. When you are doing network node interface, a lot of paths, etc. are coming; you require more bits for identifying the path and so there is 12 bit of path identifier. In UNI there is 12 bit of VCI identifier, but only 8 bit of path identifier is present in NNI. (Refer Slide Time: 42:55 - 43:35) 


Slide Time: 43:35 - 43:56

Now let us discuss about ATM cell format. The first one is generic flow control (GFC).  Once again this was put there with some idea but it was not used much. This has 4 bits and is used for local flow control between the network access point (typically a switch belonging to the network provider), and one or more end stations. This is used for multiple access for more than one station and for reducing the transmission rate for single nodes, etc. This is used to do some local flow control at the LAN end between the switch and the users. But this is usually ignored. (Refer Slide Time: 43:35 - 43:56) 


Slide Time: 45:45 - 46:06
VPI, the virtual path identifier, has 8 bits; but since GFC is irrelevant within the network, 12 bits are used. These may be thought of as the high order bits for the virtual channel identifiers. A virtual path contains a number of virtual channels. The switches store per path parameters so that individual channels do not need any signalling. So, one pair of users is using one channel, one virtual circuit. This virtual circuit has to be identified and then given some kind of bandwidth contract. Some kind of class of service will be negotiated for this. In a big ATM network, thousands or even millions of circuits may be set up and toned down at a very high rate as many people are using millions of switched circuits under a service provider. These virtual circuits may be set up and toned down in a very short time. Theoretically with each of these virtual circuits, some kinds of quality of service parameters have been negotiated. But these are grouped together for the similar kind of services and these virtual channels are put together so that only paths are to be stored. To avoid implications in finding and routing, some of these virtual circuits are put together to form virtual paths. We have the VPI identifier for this. This VPI has the higher order bits specifying the channel, and the lower order bit specifying the VCI. (Refer Slide Time: 45:45 - 46:06) 


Slide Time: 46:12 - 46:42
However, if a virtual path is already set up and is scantily used, then resources are wasted. So, dynamic renegotiation of VP capacity can be used. VCI, the virtual channel identifier, is 16 bits. VC 0 to 15 are reserved for special purpose, and others are used for actual communication. (Refer Slide Time: 46:12 - 46:42) 


Slide Time: 47:05 - 47:10
PT is the payload type, which is of 3 bits. If high order bit is 0, the second bit indicates congestion and third bit indicates end of AAL 5 frame. 100 and 101 are reserved for link management. CLP (cell loss priority) bit is used by the source for making priority.  Intermediate switches may mark it for violating agreements. Depending on whether CLP = 0 or CLP =1, the cells may be dropped or not dropped in an intermediate switch. HEC is an 8-bit header checksum. This is very important in checking whether the header has an error. This has some other function, which will be dealt with in data link functions in ATM. (Refer Slide Time: 47:05 - 47:10) 


Slide Time: 47:18 - 47:38

The next sub-topic is data link layer in ATM. This consists of the transmission control layer. (Refer Slide Time: 47:18 - 47:38) 


Slide Time: 47:40 - 48:06
Each ATM cell has a 5-byte header, in which the last field, HEC, is a checksum for just the header. The TC takes the cells from the ATM layer, adds an HEC to them and sends them as bit streams to the PMD on the transmission side. The bit streams may or may not have a separate transport. (Refer Slide Time: 47:40 - 48:06) 


Slide Time: 49:17 - 49:44
On the receiving side, the incoming bit streams are formed into cells and passed on to the ATM layer. Here the cell boundaries are to be detected. It also discards cells with invalid headers and processes OAM cells for administration management and control. One problem which came up was the ATM forum wanted the ATM to be deployed everywhere from the backbone, provide a switch right down to the user. So nothing much was specified about the physical layer. Since they wanted neutral to the kind of physical medium through which the ATM will pass, another problem came up with receiving the bit streams. Cell starting and cell ending couldn?t be identified on the receiving side. (Refer Slide Time: 49:17 - 49:44) 

Slide Time: 51:00 - 51:12
In some cases, the underlying physical layer helps. For example, when ATM cells are carried over SONET or SDH, the SPE pointer in the SONET header points to the first full cell. So we immediately know where it starts. In other cases every 40-bit sequence is tested for being a valid header. The 8 bits at the extreme right will be valid HEC for the remaining 32 bits. HEC has two functions. One is that it gives you some checking mechanism on whether the header is correct or not and the other is for synchronization function. When a bit stream is coming and if it is 40-bit long, it is taken. Now if these 40 bits happen to be the header, the last 8 bits will be the checksum, assuming that the entire header has come correctly. So the last 8 bits would be a checksum for the other 32 bits in the header. If it is a header with an error, it is neglected. Then, the next 40 bits are taken and tested again, assuming that a header has come correctly. (Refer Slide Time: 51:00 - 51:12) 

Slide Time: 52:40 - 53:27
The TC goes through a HUNT; this is the hunting phase. And once it gets a header it gets into the PRESYNCH stage. It has one header. But, out of 40 bits, 8 bits are the checksum of the other 32 bits. So the probability is that this was not a header but a user payload and was wrongly interpreted as a header because these 8 bits matched the checksum for the other 32 bits. So, now it goes into the PRESYNCH stage. If this is indeed the header, then the first 5 bytes would be the header, the next 48 bytes would be data and the next 5 bytes would again be a header and you continue checking this for some time. This is a PRESYNCH stage. When this synchronization is done for some time and you are reasonably sure that this cannot be a coincidence in the data given by the user, then we are indeed locked on to the header and now TC is synchronized. The TC goes through the HUNT, PRESYNCH and SYNCH stages to detect the cell boundaries. If the number of HECs is found to be incorrect then TC is said to have lost synchronization. Then it has to be resynchronized. This heuristic defies the layered architecture. (Refer Slide Time: 52:40 - 53:27) 


In this simple state diagram, the signal is in the hunting stage. At this stage the signal looks at every 40 bits and tries to figure out whether it could be a possible header or not. When correct HEC is detected it goes to the PRESYNCH stage. If correct HEC is not detected, the signal goes back to the hunting stage. If a few consecutive correct HECs are found, then it is synchronized. If a few consecutive incorrect HECs are found, then the TC will determine that it has lost synchronization and it will go back to the hunting phase. In the next lecture we will talk first about ATM addresses, ATM routing, etc.


Preview of the next lecture
Lecture No. 25
ATM Signalling Routing and LAN Emulation

In the last lecture we discussed ATM technology. We saw, how it handles cells, how it make cells, etc.  Now, in the first half of this lecture we will discuss ATM signalling and routing.  Ethernet networks and other kinds of networks are ubiquitous. In the second part, we will discuss about how an IP network and an ATM network will interoperate when we use ATM as the backbone.  (Refer Slide Time: 55:03 - 55:09)

Slide Time: 55:11 ? 55:36
Next we will discuss ATM signalling, routing and LAN emulation. (Refer Slide Time: 55:11 ? 55:36)

Slide Time: 55:38 - 55:42

ATM connections are of various types, the most predominant being the switched virtual circuit, in which a path is set up and taken to the destination.  The other one is permanent virtual circuit which is pre-coded.  There are also other connection types like simple point-to-point connection, symmetric or asymmetric bandwidth connection (Uni- or Bi-directional), point-to-multi point connection (Uni-directional) and data replicated by the network. (Refer Slide Time: 55:38 - 55:42) 

Slide Time: 55:43 - 56:53

This is an example of a point-to-multi point network.  (Refer Slide Time: 55:43 - 56:53)

Slide Time: 56:54 - 57:28

In an ATM connection set up, a signal is set up where the source and the destination are shown.  There are intermediate switches in between.  From the source there is a set up signal which goes to the intermediate switch.  The switch sends back some kind of an acknowledgement saying that the call is proceeding and sends the set up signal to the next hop and so on.  Each of them will immediately send back some kind of acknowledgement and then when the call is accepted a connect signal will start flowing in the opposite direction. When it reaches the source then a connect acknowledgement will flow and for this connect signal the circuit is set up.  Alternatively the destination may reject the signal and then it will send a Region Release Signal back. (Refer Slide Time: 56:54 - 57:28)

Slide Time: 57:29 - 57:31
On the other hand, when the circuit is as shown above, for taking it down the source will send a release.  When it finishes sending it will send a release till it reaches the destination.  The destination will then send a release all the way back.  The release could be initiated by the sender or the release could be initiated by the destination. (Refer Slide Time: 57:29 - 57:31)

Slide Time: 57:33 ? 58:13
Then connection gets terminated and the release is completed. (Refer Slide Time: 57:33 ? 58:13)

Slide Time: 58:15 -51:20 min
There is another approach to this not using LAN but using classical IP over ATM.  The definitions for implementations of classical IP over ATM are described in RFC 177.  This RFC considers only the application of ATM as a direct replacement of the ?wires?, LAN segments connecting IP end-stations (members) and routers operating in the classical LAN-based paradigm.  Issues raised by MAC level bridging and LAN emulation are not covered here. (Refer Slide Time: 58:15 -51:20 min)


When we look at classical IP over ATM, address resolution and encapsulation are the two issues which are to be considered.  Encapsulation consists of putting appropriate ATM header/trailer to a packet, converting it to a number of cells and then sending it.  This means that when you have an ATM packet and a classical IP is running over ATM and you have got a big packet, you have to break the cells up and put a proper header on each of them and then send them.  ATM features are not utilized and inter network traffic handling is clunky.
COMPUTER NETWORKS
Prof. Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture-25
ATM Signaling, Routing and LAN Emulation
 (Refer slide time: 00:35)

We have looked at ATM technology in the box, in the sense that how it handles cells, how it makes cells. Today, in the first half of this lecture or first part of this lecture, we will discuss ATM signaling and routing. Since Ethernet networks and other kinds of networks are ubiquitous, everywhere, if ATM is used in a backbone, how they would interoperate between an IP network and an ATM network. We will talk about that in the second part of this lecture. (Refer slide time: 01:23 - 01:30min)

So today we talk about ATM signaling, routing, and LAN emulations.  (Refer slide time: 01:31 - 04:43 min)


The first concept is that ATM uses virtual circuits; that means there are two ways to use packets: carry entire destination address in header, or carry only an identifier, also known as label. Labels have local significance, addresses have global significance. Signaling protocol fundamentally maps global addresses or paths or sequence of addresses to local labels. We will discuss this in much more detail when we discuss routing of IP packets, which we will take up after this lecture.
Usually when you have a packet switching network, then each packet is considered in one extreme, that is, in the IP end. Each packet is considered on its own. That means each packet must contain the destination address at the very least. It also contains the source address that is different; so it must contain the destination address, so that looking at each packet, an intermediate router would know where it should go. That is one end of the spectrum, whereas in the connection-oriented system we know that the physical connection is set up. In ATM we set virtual circuits or virtual paths. In virtual circuits or virtual paths, what happens is that these ATM cells are very small, only 53 bytes long. Each of the cells will contain some local label; so a path is set up. Now setting up a path in ATM means that each of the intermediate switches would know that a flow of cells is going to go through them ? from one source to some destination. They make provision to accommodate this flow; they make provision for the virtual circuit, and then, once they do that, they set up this virtual circuit in the starting phase. After that, each of the cells need not contain any specific identifier or specific address for the destination. It just needs a small address, which tells the intermediate switches which virtual circuit to use. It contains a virtual circuit identifier. Actually it is divided into two parts as we have seen in the cell header: we have the VPI part and the VCI  part. Simply looking at that label, the intermediate switch would know which virtual circuit to use.  (Refer slide time: 04:44 - 05:10 min)


Before this virtual circuit, we see that we have the samples. Suppose we have the data in the ATM cell, the data would be preceded by simply the virtual circuit identifier and this might have two parts: VPI and VCI, whereas in a regular datagram the entire address may have to be there in each packet. (Refer slide time: 05:11 - 06:57 min)


In VPI/VCI assignment used in this case, all packets must follow the same path unlike a datagram, because this virtual circuit is set up before any actual flow of packets begins. There is a time for the circuit set up and this circuit is not a physical circuit we have in a telephone network; but this is a virtual circuit, that means each of the intermediate nodes simply knows that a flow is about to begin. So this has to be set up. Once this is set up, all packets would flow through the same path. Switches store per VCI state, e.g. QOS (quality of service) information about this particular VCI. Signaling implies separation of data and control. When we do ATM signaling, we are talking about setting up the entire path. We will come to that; small ids can be looked up exactly much quickly in hardware ? that is one good thing. If you have a small VCI, that means, virtual circuit identifier, it can be looked up very quickly in a hardware ? this is a bottle neck in a router. This can be handled very fast, it is harder to do this with IP address. That means this longest prefix match ? we will come to that later on. The setup must precede data transfer. This is the other disadvantage of this ? the VPI and VCI must be set up, which means it delays short messages. There are two types of virtual circuits: switched and permanent virtual circuits. (Refer slide time: 06:58 - 07:43 min) 


This is an example, the switch knows in the input ports say 1 and 2, 1/37. If these are the VPI or VCI identifiers, out will be through port 3, 1/35. Similarly port 1 for 34 will go to 4, 2/56 and so on. There is a table which simply quickly matches the VPI/VCI values and puts it on another VCI/VPI pair on the other side. This is how VPI/VCIs are assigned and used. (Refer slide time: 07:44- 10:16 min)






We now come to ATM addresses. This address is different from the VPI/VCI; that is, the virtual circuit identifier/virtual path identifier, which is simply local. But for setting up the path, initially you require an address, and you cannot do with a local address. You have to do with a globally consistent address. This global address of ATM is 20 bytes long. There is a 20-byte long ATM address, which you use for setting up a circuit. I mentioned previously that there are two types of circuits: switched virtual circuits and permanent virtual circuits. In permanent virtual circuits, a path is set up initially manually. The path remains a permanent virtual circuit. Whenever there are two end points between which a lot of traffic will flow, you may skip this overhead of path set up and set up a permanent virtual circuit; this is like a leased line. Whenever there is something to go from this source to that destination, it will simply use the pre-existing permanent virtual circuit or otherwise, we may have a switched virtual circuit. That means this virtual circuit is set up on the fly, as the network is being used. In an ATM switch, hundreds of thousands or may be millions of virtual circuits may be set up or taken down every second. This is a very fast process. For setting up this virtual circuit, we require an ATM format and this is 20-byte long address. Unlike an IP address, which is only 4 bytes, this is 20 bytes. This is a very long address, left to right hierarchical. in this, there are level 1, level 2 level 3 and level 4. The first two levels have a 13-byte prefix, this are the levels of hierarchy. This part is usually used for the actual network addressing inside. Since it is such a long address, (Refer slide time: 10:17- 11:56 min)

it can accommodate various schemes. ATM was conceived as a technology, which will subsume and absorb all pre-existing technologies. The people who designed ATM tried to accommodate different kinds of addressing schemes in one super addressing scheme, which they say is ATM addressing scheme. As you see here, there are different types of addresses possible. Since there are 20 bytes, it is possible to have more number of schemes. These are the schemes 1 byte + 2 byte 3 + 10, 13 ? this is supplied by the network. These 6 bytes are end-system supplied and not used in routing. This may be used inside the host; 1 byte for maybe de-multiplexing inside. These network supplied parts are 1 byte, which indicates the scheme. The three NSAP (network service access point address) formats are DCC, ICD, and E.164 or ICD number or DCC number. This is a data country code, which uses 2 bytes and 10 are used for the other part of the network. (Refer slide time: 11:57 - 13:07 min)
 
Actually here, authority and format identifier is the first thing. So 39 is ISO DCC; 47 is British Standards Institute ICD; and 45 is ITU ISDN, which means that this E.164 is actually an ISDN number. ISDN uses 15 characters, i.e. 15 binary coded decimals, that is, 7? bytes. This entire ISDN number, which again can subsume telephone numbers, can be put over here. ISDN uses E.164 numbers. ATM forum extended E.164 addresses to NSAP format and E.164 number is filled with leading 0s to make 15 digits, that is, AF16 is padded to make 8 bytes instead of 7? bytes. End system identifier is the other part. This is the end system identifier part, and these 6 bytes could be various things, specifically (Refer slide time: 13:08 - 13:42 min)

this 6 byte could be 48-bit IEEE MAC address. Remember the MAC address that we used in the data link layer is 6 bytes, supplied by IEEE. The entire 6 bytes can straightaway be incorporated in the low order bits of the address. Selector is for use inside the host; all ATM addresses are thus 20 bytes long. There are various ways you can route this ATM; given an ATM address there are various schemes possible. (Refer slide time: 13:43 - 14:03 min)

Since various schemes are possible, ATM addresses could be of variable lengths and have an initial domain part and a domain specific part. The initial domain part consists of two fields as we have already mentioned: AFI, that is, the authority and format indicator, (Refer slide time: 14:04 - 14:15 min)

and IDI, that is, initial domain identifier. This identifies the domain within the purview of a given addressing authority.  (Refer slide time:14:16 - 16:18 min)

In a particular format, say ICD (international code designator), all addresses have a unique fixed length prefix. The high order DSP or the high order bits of domain specific parts roughly correspond to the low order part of network number in IP; ESI is the second. This particular scheme can subsume a lot of other schemes, which were possible. The reason for showing this in this fashion is that a similar thing was tried later on in IP version 6. We have a very large address field where a lot of previous schemes could be subsumed in this. One of the difficulties with this IP addressing scheme was that it was done in a very haphazard manner, unlike the telephone number. For example, the telephone numbers are geographically distributed. That means, you put particular first few digits and that will immediately indicate which country and region you are calling. It is easy for the router to just look at the first few digits and send it to the trunk. But that is not possible in the IP version 4 which is used, because it has no geographic correlation. You have to take keep a very long table and look into the table. Once again when people developed this ATM addresses or the IP version 6, they tried to bring back some order into this addressing scheme. (Refer slide time:16:19 - 16:36 min)


 For setting up connections: IP ATM supports both permanent virtual circuits and switched virtual circuits. PVCs are pre-coded in each switch along the way and are always present. They are like leased lines, which do not need any connection setup. (Refer slide time:16:37 - 17:06 min)

For connection setup there is a user network interface or UNI, and the network interface, which is the NNI. There is a part called Q.2931, which is an ITU protocol for setting up paths and this SSCOP means service specific connection oriented protocol. You have the AAL and ATM. These layers have already been talked about earlier. (Refer slide time: 17:07 - 17:49 min)

Unfortunately this whole scheme turned out to be quite a complex one. We will not go into all the details. We have already seen the ATM layer and the AAL layer, which is in the user plane. Today we are going to talk a little bit on this control plane, which has one service AAL and three parts: SSCF, SSCOP and AALCP or AAL common part. For setting up the circuits, etc., we have Q.2931, BISUP and PNNI or public network-to-network interface. (Refer slide time: 17:50 - 18:30 min)

UNI is the user interface of the ATM networks and consists of signaling protocol for setting up circuits of a certain quality and the format of the cells. The NNI deals with the issue of signaling and data transfer as well as routing, data transfer, operations and management. Remember there is also a slight difference between the cells which go through in the UNI part and this NNI part. In the NNI part, the GFC is dropped and we use the whole thing to accommodate more number of virtual circuits; but that is a small technical point. (Refer slide time: 18:31 - 19:01 min)

If you look at the control stack, that is, control plane stack, we have this Q.2931 sitting on this SSCF, this SAAL, that is, service ATM adaptation layer, which sits on the ATM layer, which again is on some physical layer, there is a virtual link between two ATMs, AALCPs and so on between a stack from the source to the destination or from one hop to the other. (Refer slide time: 19:02 - 21:14 min)


Let us see what Q.2931 is. We will not go into this, once again the protocol is quite complex. We will just touch upon some aspects of it. This is an ITU protocol for setting up a connection. First it sends a request in the meta signaling channel 0 to negotiate a quality of service for a signaling channel. What is done is that, for setting up the circuit, you have to do some communication. This communication again will be through ATM. Actually for that, it will require some kind of virtual path and virtual circuit. ATM is quite strong on quality of service. There is a question of quality of service of the service channel also. Although the service channel would be used for a very short time, once the circuit is set up, that service channel may be released. There is a meta signaling channel called 0, where you can negotiate the quality of service for the weak signal channel with the quality of service. Otherwise, there is a default which is VP0, VC5, that means, virtual path number 0. In virtual path number 0, which is a bundle of circuits, take the VC 5, which is a standard channel, where you put your request for signal, for setting up the path. Now if this is successful, then a new VC is assigned for connection setup requests and replies. Then you first make the request in this channel then a new VC would be assigned for this particular connection setup. Q.2931, if you remember, is at the top of the control plane protocol stack, which initiates at the setting up of the circuit and handles setting up of circuits. (Refer slide time: 21:15 - 21:59 min)


Below the Q.2931 we have the signaling AAL; that means, signaling ATM adaptation layer, which again contains three parts: service specific coordination function, which provides interface Q.2931; interface between Q.2931 and the ATM stack; service specific connection-oriented protocol, which is the SSCOP ? this handles error, loss and recovery. All these are communications for setting up circuit for communication for the control purpose and the AAL common part, which handles error detection. This is roughly the stack. (Refer slide time: 22:00 - 23:59 min)


There are various kinds of parameters in the forward direction and in the backward direction. There are various parameters that you can specify for the quality of service. With ATM, when it was introduced, a serious attempt to handle the issue of quality of service was made. It has now become so important that in the IP domain also, which is turning out to be the dominant technology, quality of service has become important. People have thought of various schemes for handling quality of service. Many of the schemes that people had already thought about with ATM have been adopted in various ways. We will talk in detail about quality of service later on. 
Today, we will just touch upon it; there are various parameters like peak cell rate ? that means the peak rate at which you will be pumping inside; sustainable cell rate; maximum burst size; etc. All these different parameters can be negotiated for one particular virtual circuit. When a path is set up along the way, each of the ATM switches on the way makes some provision for supporting that particular new virtual circuit with that kind of service. If it cannot handle that, may be some negotiation about it can be handled. Leaky bucket is some kind of congestion control algorithm. We will discuss it later. (Refer slide time: 24:00 - 24:29 min)


ATM connection types are of various types; the most pre-dominant ones are the switched virtual circuits, where you set up a path and take it down; or permanent virtual circuit, which is pre-coded. There are other connection types, e.g. simple point-to-point connection, symmetric or asymmetric bandwidth connection, point-to-multipoint connection, data flow in one direction only, or data is replicated by the network. (Refer slide time: 24:30 - 24:35 min) 

So this is an example of a point-to-multipoint network. (Refer slide time: 24:36 - 25:45 min)

When you do an ATM connection setup, you set up signal and maybe this is the source and this is the destination. These are the intermediate switches. From the source, there is a set up signal, which goes through the intermediate switch, which sends back an acknowledgement saying that the call is proceeding and sends a set up signal to the next hop. The set up signal is then sent to the next hop and each of them immediately will send back an acknowledgement. When the call is accepted, a connect signal will start flowing in the other direction and when it reaches the source, a connect acknowledgement will flow and each of them will give this connect acknowledgement for this connect signal. The circuit has now been set up. Or alternatively, the destination may reject it; then it simply sends a region release kind of a signal. (Refer slide time: 25:46 - 26:23 min)

For circuit setting up and for taking down a circuit, the source will send a release; that means he has finished sending release and release complete will go on. Then finally the destination will send a release all the way back. A release could be initiated by the sender or the release could be initiated by the destination also.  (Refer slide time: 26:24 - 26:28 min)

The connection gets terminated and release is completed. (Refer slide time: 26:29 - 28:14 min)


PNNI is the private network-to-network interface; that means between the end system and the switch, we have the UNI and we have the NNI. PNNI is a private network-to-network interface and this could be between two switches or two entire networks. PNNI uses link state routing protocol for ATM networks. We will look into the details of link state routing when we deal with OSPF in the IP worlds.
Since IP is the more prevalent technology, we will discuss it in detail when we do it here. Each node will periodically broadcast the state of the link to which it is connected to all parts of the network. This way all the switches get some global picture about the status of the various links, so that they can run some algorithm locally in a centralized fashion and find out all the possible paths. They use this for setting up the path later on. (Refer slide time: 28:15- 30:17 min)

Actually the situation is a little more complex than what I have just said, because there is a hierarchy mechanism that ensures that this protocol scales well for large, world wide ATM networks. A key feature of the PNNI hierarchy mechanism is its ability to automatically configure itself in networks in which the address structure reflects the topology. There are two things here: one is this hierarchy we are talking about, and the same kind of thing is used in OSPF in the IP domain. When we discuss OSPF, we will go into the details of this; but there is a hierarchy over there. Imagine what would happen if all were ATM switches, which they are not, but suppose they all were. That was the vision; and if the ATM switches were communicating with everybody else with their link states, the database and everything would become huge. In order to scale to a very large network, it goes through in a hierarchical fashion, meaning you can have a hierarchy of  networks and at each hierarchy, the peers run some kind of PNNI for routing within that level of hierarchy. And at a lower level, they will again run PNNI for that level and various hierarchy levels are possible. You have seen that the ATM address is given in such a way that we have only shown the broad boundaries but within that boundaries, it can be further divided into a number of hierarchies and again different designated authorities can break it up in a different way. All these flexibilities are possible; PNNI allows that in some plane, in some hierarchy, one kind of addressing scheme is used and in another, a different kind of addressing scheme is used. (Refer slide time: 30:18- 31:36min)

The substance is that it scales to very large networks, supports hierarchical routing, supports quality of service, supports multiple routing metrics and attributes, because when you broadcast the link state, i.e., the state of the link, you may also broadcast all different kinds of parameters about the links that can be handled in that link or how much can be handled by the switches, etc. can be propagated throughout the network. If you have a fair idea about what is possible and what is not possible, then you can plan your route in the source in a particular fashion. Use of source routed connection setup: since the source has the global picture, it will use that global picture to compute the route and set up the connection. Once it decides on the route, it used the Q.2931 to give all the connection setup. That set up request, acknowledgement and release etc. will be used. It operates in the presence of partitioned areas. (Refer slide time: 31:37 - 32:16min)

 PNNI features provide dynamic routing; that means, the link states may change from time to time. Each of the switches are going to broadcast their link states; it is responsive to changes in resource availability; separates the routing protocol used within a peer group from that used among peer groups. Various hierarchies are possible: interoperates with external routing domains, which are not necessarily using PNNI and supports both physical links and tunneling over virtual circuits. (Refer slide time: 32:17 - 33:44min)

This is an example of hierarchy. This big network, which is again a network of networks, is represented by one node in the top level of the hierarchy. Similarly, you use PNNI to plan a route like this. Within the network you might have to again do a planning and each node may again be corresponding to another network at a lower level. At each higher level of the hierarchy, you can use PNNI to plan the route if there is A.1.1. Its view of A.1.1 is something below A.1.1, A.1.2, A.1.3 are explicit. These have abstracted notions of A.1.2 and then you come to B; these are B and C. Although when the call setup is passing through B, it may come all the way down and do the actual path setup. That is how the hierarchy works.  (Refer slide time: 33:45 - 34:10min)

At any level of the hierarchy, A.1.1 will make a source route, which goes through A.1.2, then B and C. So the source specifies the route as a list of all intermediate systems in the route. This was the original idea also in the token ring. (Refer slide time: 34:11 - 34:30min)



For this, it uses a designated transit list (DTL), which is in the form of a stack, as I will show you. Source route is across each level of hierarchy; there is an entry switch for each peer group; it specifies complete route through that group; and set of DTL manipulations is implemented as a stack. (Refer slide time: 34:32- 34:45min)

A and B may be at the bottom of the stack; A.1 is at the top level of the hierarchy and this is how it is put in a stack and the path is completed. (Refer slide time: 34:46- 35:32min)

We now discuss the quality of service parameters. I will just mention some of the metrics, etc. We will not go again very deep into this because we do not have that much time. But it will give you some idea about what we mean when we say quality of service. One such parameter metric is maximum cell transfer delay; that means, the delay from the beginning of the first bit of the first cell to the last bit of the cell. Others include maximum cell delay variation; the variation of this time; maximum cell loss ratio; whether the cells could be lost, and if so, what is the maximum cell loss ratio; administrative weight, etc. (Refer slide time: 35:33 - 35:55min)


The attributes of the parameters are available cell rate or its capacity, whether it is available; cell rate margin is allocated minus actual; variation factor; branching flag; restricted transit flag. These are all different parameters and their attributes ? QOS parameters and their attributes. (Refer slide time: 35:56 - 37:27min)


One way this is handled is the generic call admission control, which happens when you are on the source side, i.e., when you are deciding on the route and you are doing a source routing, at that point whether you can admit a request, which has finally come from the user, from the UNI, etc. But at that particular point of time before making the request, there is an admission control, which is a generic cell admission control run by a switch for choosing a source route. It determines which path can probably support the call; that way it will try to  route the call setup. Actual call admission control is run by each switch. In the beginning we will run a GCAC as well as ACAC and the intermediate switch simply runs an ACAC or the actual call admission control to check whether that request has come to this switch, whether it can handle this request or not; this is the protocol which is running. (Refer slide time: 37:28- 38:06min)

There are traffic management functions; the call admission control is a kind of traffic management.  Traffic shaping means limit burst length, space out cells, etc. for getting a maximum throughput; usage parameter control is to monitor and control traffic at the network entrance of various traffic management systems. Both quality of service management and traffic management are possible in ATM, and there are extensive protocols for negotiating these various parameters and for setting it up. (Refer slide time: 38:07- 38:39 min)


The functions of traffic management are as follows: there is selective cell discard with CLP or cell loss priority. If CLP is 1, the cells may be dropped if the situation warrants. It is something like an unspecified bit rate and it may have a very low priority. Cells from non-compliant connections may be dropped. There is also frame discarding. One example of feed back control is an ABR scheme. (Refer slide time: 38:40- 38:46 min)


We will just quickly look at the peak cell rate. I am not going into the details of these: cell transfer delay, cell delay variation, cell delay variation tolerance, cell loss ratio, etc. are the parameters. (Refer slide time: 38:47- 39:21min)

Explicit forward congestion indicator: we will just have a quick look at this ABR system, which is a binary rate system, which sends an EFCI. This is explicit forward congestion indicator, which is set to 0 at source and congested switch is set to 1. Every nth cell destination sends a resource management cell to the source. (Refer slide time: 39:22- 39:43 min)

What happens is that somewhere in-between if an intermediate source, or EFCI, is congested, this may set it to 1 and then that information may flow back finally to the source and the source may try to restrict its requests. (Refer slide time: 39:44 - 40:16 min)


Sources send 1 RM cell every nth cells, the RM cells contain the explicit rate that has been asked for; the destination returns the RM cell to the source. The switches adjust the rate down; that means if it is congested, it may adjust the rate down and the source adjusts to the specified rate. Whatever rate comes through this, going up and coming down through this negotiation, is what the source finally has to accept. (Refer slide time: 40:17 - 41:38 min)

We will talk a little bit about LAN emulation, which is emulating a local area network when the backbone is of an ATM or when you use an ATM but still want to use the Ethernet and IP, specifically IP, in the nodes. And one very specific reason this is required is as follows. One of the reasons ATM was not successful in the LAN segment, although it was put as a LAN solution also in enterprise LAN solutions, was that there was hardly any software which was developed on ATM. Whereas a huge amount of software has been developed using IP and so much of software has been developed in IP, you cannot throw it up nor can you translate it to an ATM software overnight; that is very difficult. People took a more pragmatic approach and thought the backbone network technology be that of ATM or have an ATM. Let us emulate the LAN so that your software IP based software can still run. (Refer slide time: 41:39 - 42:02 min)


Problem: it needs new networking software for ATM solution. Let ATM network appear as a virtual LAN. How can an ATM network appear as a virtual LAN? LAN emulation is implemented as a device driver below the network layer. These are LAN emulation bridges actually; if there is an ATM, this will look like a LAN. (Refer slide time: 42:03- 42:41 min)

For this, 1 ATM LAN can be n virtual LANs. Many virtual LANs can be there in the same ATM LAN. Only one of them may be sufficient, this is a logical subnet interconnected via routers. This is the abstraction, this is the picture that we want to give to the world. It needs drivers in hosts to support each LAN; so in actual practice, only IEEE 802.3 and 802.5 were supported, although FDDI could also be done. (Refer slide time: 42:42 - 43:21 min)

This is the picture: we have an ATM switch, we have some LANE servers, we have multiple LANs on this, we have a LANE server A and a LANE server B. The logical view would be as if a router is also connected to the ATM switch. The logical view  looks like an IP network. We have A1, A2 connected via this network A and B1, B2 and there is a network B, which is connected. These two IP networks are connected via routers. This is the logical view, although the actual view is emulated here. (Refer slide time: 43:22 - 45:55 min)

It requires several components; one component is that we require a LAN emulation client in each host. Each host must have LEC or LAN emulation client, which is a small software, which can be loaded in each host. LAN emulation configuration server or LECS will be in one central server, it will be taken as the LAN emulation configuration server. Whenever somebody wants to join the LAN or leave the LAN, the LAN emulation client will first ask for the parameters from the LECS. It has to know the address of the LECS and here is a LAN emulation server itself, the LES, and a broadcast and unknown server. If there is something, like the broadcasting in a network, which is done, we know that it is in a particular network for an ARP. We want to do an address resolution protocol. What we do is that we broadcast a request, i.e., the IP address; what is the data link address? Remember that this ATM works on point to point, mostly, as a point-to-point connection. Although point-to-multipoint is possible, but it is not in both directions. Instead of trying to do the broadcast from the host itself, what it does is that if each host in this LAN emulation client has to broadcast anything, it will send it to another server called bus; there is a virtual connection between every host and the bus, and the bus will send the broadcast to each of the hosts. That way in an indirect fashion, the broadcast takes place. Similarly there is an unknown server; that means I do not know the address of the server like ARP. Once again, you send the request to this bus and the bus will find out and finally give you the address. These are the main components of LANE, namely LEC, LECS, LES and bus. (Refer slide time: 46:00 - 46:51 min)

What does the LES do? The basic function of the LE server is to provide directory, multicast, and address resolution services to the LE layers in the work stations. That is what the LAN emulation server does. It also provides a connectionless data transfer service to the LE layers in the workstation if needed. The parameters for setting up a server, etc., will be known to the LECS, which will communicate to the LEC as the LEC joins the network. (Refer slide time: 46:52 - 47:44 min)

Initialization: The client gets the address of LECS from its switch, uses well-known LECS address, or well-known LECS PVC. There has to be a particular PVC, which it starts using automatically. The client gets server?s address from LECS. It also discovers its own ATM address if required for direct VCs. That means if it wants to do some direct communication between two nodes, it has to get the ATM address of the other side if it wants to set up a direct VC instead of going via the servers. In that case it will require its own ATM address also.  (Refer slide time: 47:45 - 48:00 min)


It does a registration; client sends a list of its MAC addresses to the server; declares whether it wants ARP requests. These have to be known to the server so that the server can give the service to the other nodes connected to that network. Client sends ARP request to server. (Refer slide time: 48:01 - 48:19 min)

Address  resolution: client sends ARP request to server; unresolved requests are sent to clients, bridges, servers and the ARP; client sets up a direct connection. This is how a connection is set up. (Refer slide time: 48:20 - 48:40 min)

Broadcast to unknown server or bus: as I said,  it forwards multicast traffic to all members. Clients can also send unicast frames for unknown addresses there. Suppose some address is not known, you send it to bus and then bus will try to find out. (Refer slide time: 48:41 - 49:54 min)

There is a flush protocol. That means clients can send unicast packets via bus while trying to resolve the address. What might happen is that client may try to get the address, then get some address, maybe send it directly. What might happen is that something which was not sent directly here, the packets may come out of order. Remember I mentioned that in ATM, one guarantee is that cells will not come out of order. Cells may get lost, but cells will not come out of order, unlike pure datagram services. But in this particular case this may happen because we are not talking about one particular VC. In one particular virtual circuit, the packets will indeed not go out of order but in this particular case, they might. When direct VCC is set up, client sends a flush message to the destination, destination returns it to source, can then send packets on VC. This is a flush message that this problem is solved. (Refer slide time: 49:55 ? 50:38 min)

There is another approach to this, which does not use LAN but uses classical IP over ATM. What is classical IP of our ATM? The definitions for implementations of classical IP over ATM are described in RFC 1577. All the details are here; once again, we will just simply mention it very quickly. This RFC considers only the application of ATM as a direct replacement of the wires; local LAN segments connecting IP end stations which are the members; and routers operating in the classical LAN based paradigm; issues raised by MAC level bridging and LAN emulation are not covered. (Refer slide time: 50:39 -51:20 min)

If you want to look at classical IP over ATM, you have to do address resolution and encapsulation. These are the two issues to be considered here. Encapsulation consists of putting appropriate ATM header trailer to a packet, converting it to a number of cells and then sending them. This is encapsulation, which means that you have an ATM packet. This is classical IP running over ATM when you have a big packet. You know that cells have to break it up, and put a proper header on each one and then send them. ATM features are not utilized and inter network traffic handling is clunky. (Refer slide time: 51:21 - 52:21 min)

 Each of the IP sub-networks is a logical IP sub-network. All members of a logical IP sub-network are able to communicate via ATM with all other members in the same LIS, which means that if two nodes are in the same logical IP sub-network, i.e., same LIS, you would set up a VC on. There is a VC between every pair within or amongst all nodes in a particular logical IP sub-network. There is a VC mesh; everybody can communicate to everybody else. Communication to hosts outside the LIS, local LIS, is provided via an IP router. This router is an ATM endpoint attached to the ATM network that is configured as a member of one or more LIS. (Refer slide time: 52:22 - 52:49 min)

Naturally, a router may be a member of more than one network. Similarly, the network is configured as a member of one or more LIS. You have to do an address resolution. The valid question is that, if the IP address is this, what is the ATM address? You have to do an ATM ARP: IP address to ATM address translation, address resolution protocol is used. Inverse ATM ARP means VC to IP address; solution: use ATM ARP servers. There are ATM ARP servers there. (Refer slide time: 52:50 - 53:49 min)

This is a diagram, suppose this is a logical IP sub-network number 1 and this is logical IP sub network number 2, each of them has its own ATM ARP server for doing ARP, that means, IP to ATM address translation and vice versa. Nodes are connected; if A1 wants to communicate to B2, naturally at the top level, you give the IP address. Previously we were breaking it up into data link address. Finally what is the route to take if it is in some other list? If it is in the same list, you have a direct VC to it and you have a direct virtual circuit; you take that. Each LIS has an ATM ARP server for resolution; clients are configured with the server?s ATM address and clients register at start up periodically. (Refer slide time: 53:50 - 55:08 min)

In ATM, ARP protocol is used to resolve a host IP address for a known hardware address. It is the inverse ATM ARP. As you can understand, this ATM is a rather complex technology and this is one of the reasons it was not so successful. As you will find later on, many of the ideas which were used in ATM, namely, about this quality of service, about setting up flows and setting up virtual circuits with VCI VPI labels, these ideas were later on adopted in IP and we will discuss it a lecture called MPLS, that is, multi protocol label switching, where the similar ideas have been used to give this kind of services later on. Also, ATM is used quite extensively in big backbone networks because of the various facilities. Although it has moved out of the LAN segment, these days the gigabit Ethernet has replaced it simply because of the host. Thank you.


      Preview of the next lecture (Refer slide time:55:11-55:12)


So today we will start our discussion on routing. Actually we have already talked about routing a little bit in different context specifically in the context of ATM. How the ATM virtual path are set up. Today, we will talk about the major area. We will start our discussion on the major area  of routing which is how and specially with reference to the TCP IP stack. That is how packets are routed in a IP network (Refer slide time: 55:54 -56:04)

we will talk about. Today, we start the introduction and to routing, we will take up the discussion about different routing protocols in next set of lectures (Refer slide time: 56:09 - 56:48)

Let us just recollect what the job of the network layer or what is routing. This is to carry data end to end, i.e. from source to destination perhaps through a number of intermediate subnets depending on whether connection oriented or connectionless services are used. Other functionalities may be incorporated at this layer, we will talk about this later on.  We are talking about IP routing of IP packets and how you can have a virtually connection oriented system on that. We will discuss it later on, the point is unlike data link layer remembering the next hop just the link which is immediately adjacent. That has some advantages in the sense that whatever information you require about it are locally available here. Routing is the major problem, we are talking about routing over multiple networks and towards a very remote system. The packet might have to take many hops maybe 10, 20 even 30 hops to reach the end point and when you take naturally. Let say 20 hops, the area you are sort of serving becomes so large with so many machines connected to it. How to  keep track and naturally switch so many machines with so many links is the problem. Some of the link may go down some of the machines,  may come up and when I say machines it may refer to actual either PCs servers etc. They may also refer to the network boxes like other routers (Refer slide time:58:03-58:29)

Calculate the check sum which is for your connection. As we know can transmit to the next hop and send an ICMP packet if necessary. ICMP is for internet control message protocol if the routers may use ICMP packets for sort of talking to each other and sending various messages if necessary. We will see 1 example, 
