Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture # 1

Introduction to Database Management System

Welcome to this course on introduction to database systems. This is an introductory course usually taught at the first semester of third year under graduate course, is also expected that you should have done course on data structures, file systems probably operating system to better appreciate the concepts covered in this course. Though the course material may not assume completely that you have understood all the prerequisites, it could also be done probably independently because wherever the concepts are required they would have been covered and some difference to the background material would have been indicated in the appropriate sections. It’s a 42 lecture module of one hour duration and expected to be covered in a semester long course. 

What we will be doing is the two instructors in this course, when myself will be covering the transacting processing system at the later stage of the course and the other instructor Dr. Srinath will be covering the initial sections on database design ER modeling and other basic concepts in the initial sections of the course. What I am going to do in today’s lecture is give a brief introduction to databases and show the importance of  databases and also in the process introduce some of the basic concepts that we will be covering in much more depth as we go in to the course. 

As the term databases indicates ever since data has been digitized and we are able to store data in digital form, we see that the mode of data that is being stored by corporates and other organizations as increased from a few kilobytes over a period of time to now terabytes. Now databases in that sense have become an integral part of our day today life in that we actually do lot of transactions, our day today transactions whether it is railway reservation or it is an airline ticket reservation or withdrawal of money from a bank. In some sense we are actually working with the underlining databases that this organizations have. They actually store the costumer data and other information, user information in the form of databases, in the form of databases where it is accessible by a number of other entities so that our regular transactions like withdrawal of money from the bank or reserving a ticket for a train, these are actually are business processes that are working with the underlining data that is stored in this organizations.

In that sense databases are an essential thing in the business processing world and they become a key entity in terms of developing these applications, business process applications. To show what kind of you know, importance database have assumed in recent times, I will just show you an example of a business transaction in how we actually work with these business transactions in our day today life. I realized only yesterday that I actually have to pay a premium for one of my LIC policies and in normal sense what we do is for paying this policy, the LIC actually sends us an reminder which is a paper copy posted through the postal mail and normally look at this remainder and then you will actually send somebody or you go to the LIC branch and try paying the premium across through your cheque or something bank cheque or other mechanism, if you carry the cash you will use the cash to actually deposit and then take the receipt back. 

All this requires that you physically now move from one place to the other place and sometimes stand in the queue, if there are more people waiting to do this transaction and then the process you will really have to spend time in doing this transaction, business transactions in your day today life. But now with the digitization and the storing of this data into the digital form into the databases, what you start realizing is that it is possible for you to actually access this data online and then see that you have to pay a premium and then see you know if you know actually do this and get on to your bank you know and then pay the premium online through the bank and also get the receipt of this [inaudible] transaction back on your email, in your email box which means the whole transaction can be completed sitting in your office without moving even an inch from your office and fully finish this entire business transaction. And this is sometimes what we call as work flow to show how this work flow actually takes place. Let us see how the scenario which had explained earlier can be managed and I will show you online, how this whole thing actually can be done. 

(Refer Slide Time: 08:47)

 

So we will actually go to the LIC’s database. So this is the LIC’s portal which actually gives the user the ability to login to this LIC portal. Now I basically try login in to this database. So once I login here, I should be able to get all my data relating to what are the policies that I have with LIC and what are the premiums that I paid earlier with LIC and what are the due dates. 


(Refer Slide Time: 09:09)

 

And if I actually want to calculate the lone information, I should be able to do all this say for example you know you [inaudible] pay premium which have paid online, so I can say that in this particular year I want to see what are the premium that I have actually paid which shows that these are the premium that I have paid already online in this particular year. So this database actually shows you know, for example if you want to see the receipts you can see online. So all this is being managed for you by the LIC database. 

(Refer Slide Time: 00:09:39)

 

So once you logged in and all those informations relating to you is available, this becomes a one single point access for you. And now we can say for example, one of the transactions that I want to execute now is I actually want to actually do an online premium payment. 

(Refer Slide Time: 00:10:19) 

 

And once I want to do this online premium payment, I have to choose the policies for which I want to pay the premium online and it suddenly gives me a lot of options here. I can say this is the premium for which I want actually do it online. So now what I will do is I will say submit, so this should take me saying that yes this is the premium that I have to confirm which says that would you like to pay for this particular policy, this much amount of premium online. 

(Refer Slide Time: 00:10:40)

 
Now if I say yes actually it will say you know I can only be paying the premium in my name and all that you know things which are indicated here and it also tells that I am going to get on to more secured way of paying this premium.

(Refer Slide Time: 00:11:05) 

 

This is the transaction id automatically generated and it says how do we actually want to make this payment. I can say since I have now various ways in which these can be pay probably, you can see that there are now city bank debit card you know, SBI. Let us say that I have SBI account, so I will now try to do this using the SBI net banking. 

(Refer Slide Time: 00:11:37)

 

So this will take me to SBI database now. It automatically pushes me to the state bank of India’s gateway, payment gateway. Now this will put me into the, you know database of state bank of India.
 
(Refer Slide Time: 00:12:03)

 

Now I will login into state bank of India's database, give my details and now say let me submit this which should say yes, now it says I have logged into this. 

(Refer Slide Time: 00:12:11)

 

And it says now would you like to confirm paying this money to LIC, you say confirm. 

(Refer Slide Time: 00:12:25)

 

So it says verify and confirm life insurance con transactions detail, so it gives the client code, it gives the Indian rupees and date and you say service charge is 0 and the branch is Indian institute of technology Chennai then I say confirm.

(Refer Slide Time: 00:12:44)  

 

And this should now, it says that I have actually paid, your payment request is being processed. 



(Refer Slide Time: 00:12:50)

 

So this will basically, underlying take into the SBI’s database. Now I am gone to LIC, looked at the premium that I need to pay and I told this much premium I need to pay to LIC and then I have gone to my SBI account. Now I said this much amount be given to LIC for paying my premium and it’s automatically done. 

(Refer Slide Time: 00:13:15)

 

Now you can see I come back to LIC’s portal and it says thank you for paying premium though the payment gate way where the receipt has been generated for the payment, click here to view or print. A confirmation mail with a copy of the receipt has been sent to the mail id provided at the time of registration. 
So you can see that now it will give me the full registration, the receipt which is needed for me if I storing it for future requirements to show that I have paid this premium. This is important because at a later point of time, we need to say that yes we have paid this premium, so we need this receipt which tells that this is the receipt.

(Refer Slide Time: 00:14:04)

 

And interestingly if you basically look at it, this receipt is electronically generated and is digitally signed. So it doesn’t require any more signature of anybody else, the LIC can in future confirm that since it is digitally signed by LIC it can be confirm and saying that this receipt is generated by LIC and you can see the whole details are available for me here and I can store it for my future reference. This is what we mean by actually a business transaction and a work flow. As you can see that I am moving from one database to the other database and in the process actually I am accessing data that is stored in this organizations databases and doing business transactions with them. And please remember this is very important because these are business transactions.  

At a later point of time, the bank cannot say that it has not paid this amount to LIC and LIC at the later point cannot deny that I have not received this amount. So there are lots of issues involved here like the properties of the transaction that we need to ensure saying that the customer is protected again as double payment. For example if he clicks twice, he has to be protected saying that this transaction is actually carried out once and not twice and his money as is available in his bank account is properly reflex this payment. So as you can see this has become now an integral part, this is one of the business transactions I could execute this online sitting right now here and you can imagine, it can be done very late in the night when you realize that you have to pay this premium and with no intervention of the bank officials with the no intervention of the LIC officials. Since this data is digitized and exposed, you are able to actually as a customer access all these data and do this online without any hassles.
And hence this becomes an integral part of our day today life. And hence databases are an extremely important subject for us in terms of developing these applications and making them usable by in our day today life by people. Lot of other examples also exists for example, if you go to clear trip dot com you can similarly make payments and buy your tickets and do so many other things with respect to booking your reservations and things like that. So in some sense this course is going to focus you and tell you the underlining examples or principles in developing these kinds of databases. And what I am going to do in the next half an hour is to actually show you an example of how I could have developed this example and what are the different components that are available for you when you are actually developing these kind of applications.  

For this what I am going to do is I am going to take a simple application that we have in IIT madras and we are going to show how incrementally I can develop a database model and a database application for this business process within IIT madras. The business processes actually one of trying to allow people to purchase computer systems and peripherals by the faculty of IIT madras where there are different vendors who can supply these peripherals at different prices. So the idea is actually to allow the faculty to be able to purchase these systems at a very competitive rate and what IIT madras does is actually it enters into what is called the rate contract with various vendors for a supply of computer peripherals and computer systems. 

Now this process is done every few months to actually take care of the varying rates of this computer items and the vendors can bid at different points of times and then the best competitive vendor is chosen for a given computer system. And then that is made available to all the faculty so that they can use that rate contract system to actually obtain their necessary computer items without going through any other purchase procedure. And this is done often to actually allow the systems which keep changing in terms of their configuration to be brought in to the rate contract system. 

Now one of the requirements here is the vendor should be able to quote at regular intervals into for IIT for various computer system configuration. They should also be able to see what IIT‘s requirement is and once these things are chosen, the systems are chosen then they should be made available to the faculty so that they are able to actually choose whatever systems they need and are able to generate the required papers for purchasing those systems. Now when we initially had a paper based system, all the vendors used to quote, supply their quotes on paper and then they used to be personal in our purchase department who use to enter all the things into a word processing system like Microsoft word providing a comparative statement of different vendors for this computer configurations. And so it’s basically file systems that we are using but then lot of processing, manual processing of entering these data in that which is usually other problem because this involves a TDS, a typing of the details provided by the vendors and then circulation of this papers to various members so that they can look at the details and then choose the systems that are most competitive. 

Now the problem in this particular case as we can understand is that which used to take nearly 3 to 4 weeks minimum for the entire process starting from the vendors quoting to the selection. It used to take 3 weeks to 4 weeks and used to involve lot of manual processing by various people and also circulation of large quantity of papers containing this data two various members involved in choosing these things. So obviously this is a very good case where one can think of how this whole processing can be improved as a business case and see how one can bring in an automated processing by choosing databases as an example of introducing data base systems for automating the entire business process or work flow from one end to the other end to just show what are the likely business processes or work flow elements that will be involved in this. I will just take you to a site which actually gives a first cut requirement of what was developed to show how this whole thing can be sort of automated in using the database concepts in a nice way. 

(Refer Slide Time: 00:22:53)

 

So this actually gives an initial flow of how we could probably introduce the database systems for automating this business process. And one of the key things as I was explaining was to actually search through these rate contract details which were approved which could be used by various faculty members in choosing their requirements.  

Now as given earlier now I can actually quote through this which actually says that you can search through this rate contract details. Let us understand this one business process of how exactly one can search through this rate contract. 








(Refer Slide Time: 00:23:33)

 

This actually take you to the various other menus that we have, for example one can search through computer system configuration or add on parts or space for computer systems or printers and scanners and note book computers. So if I go into computer system configuration then I can search through this computer system configuration. 

(Refer Slide Time: 00:23:55)

 

Now I can search for example IIT specification number wise where each specification will give the different specifications for the computer systems or you can have the search through vendor name or through brand name or processor name or a combination of this.
For an example let us say I actually click on specification number wise and submit this then I can actually see all the specifications IIT has in this particular case. There are ten specifications in this case let us say I actually choose the fourth specification and submit that specification.

(Refer Slide Time: 00:24:42)

 

Then now I can see all the vendors who actually quoted for this specification right and their prices.

(Refer Slide Time: 24:47)

 

For example I can see here there is a company InfoTech limited which quoted for the specification 4 at 25000 rupees for G 33 chipset. For P 33 P 35 chipset actually it is available at 25500. So what this gives is actually all the vendors and whoever has quoted for the system and they prove to be in the rate contract and what are available in this rate contract can be searched by using this. One can in fact do a little, better search by saying for example if you want to actually take a specific processor or a CPU clock speed and also a vendor name then you can actually submit this quote here.
 
(Refer Slide Time: 00:25:50)

 

Now you can say a canonical electronics private limited is a vendor name and I want clock speed 2.8 and I want to see all specifications matching these two requirements then I will basically submit this. I get one specification matching this requirement which says that this is the available system for me in the rate contract which I probably put order if that satisfies my requirement. Now to understand what is happening here as you can see here we need to get this information organized in the back end database and have this information stored properly into our database system. 













(Refer Slide Time: 00:26:02)

 

Now this is where actually the data model becomes extremely important, what is the data model and how do we store data into that system and how do we access that data as we need it. 

(Refer Slide Time: 26:43)

 

For example in this particular case you can see that I am actually accessing the data by actually saying that I actually need to retrieve the data which matches a particular specification number and a particular vendor number. So I am actually going into the data and trying to match the required values and retrieving the data as they match that particular value that I have supplied. 
It’s quite possible that I actually navigate through data which means that I actually retrieve at one level. For example you can see when actually go with specification number I am actually retrieving the data at one level. Now we can see these are all the data that I have as for a specification number is concerned. Now I can say for 5 is what I am actually choosing which means that I am actually drilling down, I am navigating using into the database by saying for 5 now I need the information. So I get to the second level by saying please get the information on 5 for me. 

This is what we understand as a navigational query where you are navigating through the data, has a post to actually retrieving all the information that satisfies a particular criterion. For example one can say I want to find out all the students who have registered my database systems course or my paradigm programming course or my grid computing course or I want to find out for example a further intersection of the sets by saying I want to get all those students who registered for my paradigm programming course and also my course on grid computing.  

(Refer slide time: 00:27:54)

 

So in that sense we have various ways we retrieve this data and hence the data model becomes extremely important, what is the kind of a data model we have becomes extremely important. And also another interesting fact that we should see when we see data models is that here is a case where the data is well organized as a table. For example I have an IIT serial number and have an IIT specification, I have a chipset, have a company name unit price, unit price without monitor, category thing like this. This is more like a table organization of the data.  

Now lot of information if you see for our business purposes gets very easily organized as flat tables of this nature and that is one kind of a data model that is extensively used while storing the data. Their other data models that are possible for example you could have data models where you have an object where the information could be unstructured which could mean that you have a field name and you have a corresponding value but then one field in that particular object could actually point to another object and that could point to another object. When example for such a kind of data model or real world data model is for an example, if you want to store the information about maruti 800, what are all the parts maruti 800 has, it fits more as a object data model because you are going to specify now what kind of engine maruti 800 has and within that what kind of other parts that engine has.  

So you are going to actually store the information more in terms of objects and the objects linking each other and that becomes an object data model where you have a flat model like this, flat table base model is often also called the relational model. And as you go into this course you will see in detail how the relational model can be used for storing data and how the relational database system can be used for developing applications that fit very well into the relational model. 

What I have shown here is a simple example of using the relational model for building this computer rate contract system. I will further go down and show how the relational model fits in here in terms of developing the application. One of the important things that we should note down here is for example this also enables the vendors to come in and give information of what they have directly once I have enabled this database system.

(Refer Slide Time: 00:31:41)

 

That can be shown here for example by saying that a vendor who wants to quote to me can login here. For example you can see here it allows the vendor to login here and in this particular case I can give a guest name, guest is the login and then with the password you can login now which shows that now he is allowed to actually enter my system and he can quote to my various requirements.

 
(Refer Slide Time: 00:32:06) 

 

For example in this particular case I have a computer system configuration. Now for the computer system configuration there are different kinds of specifications which I need.

(Refer Slide Time: 32:24)

 







(Refer Slide Time: 00:32:38)

 

So he can say that he wants to quote for the specification one here, he can also click here to see the specifications that I have at the moment for the computer systems. So this gives the list of specifications that he has for the computer systems. 

(Refer Slide Time: 32:59)

 

Now from this he can choose for which specification he wants to quote for the computer system configuration. Once he decides his specification for which he is going to quote, you can basically allow him to quote here by saying that now he can give this information, in fact he can say that this particular specification one he wants to quote it let us say at 25000 then you can say he can supply that information here unit price and he can say the unit price without monitor could be 18000 in this case assuming that he has some calculation for the monitor. 

(Refer Slide Time: 00:33:17)

 

So he basically can give this information and also put some comments saying that extra something here. If he wants to say that the additional component, cost component that he is going to use in this. Once he is decided to quote for this, he can say that this is the information that I can give it to you as for a specification one is concerned then he can say he is actually quoting for this which means that this information gets into my database now.
 
(Refer Slide Time: 00:34:19)

 
I will show you in a minute where exactly this gets in and gets stored for me and that’s where basically the entire relational database system concept makes sense. Once he is doing this update, this goes in and get stored in an appropriate table in my database system and I will be able to retrieve or use this information later to make my decisions or other business processes or transaction to access this particular data. 

(Refer Slide Time: 00:34:58)

 

Now you can say that this confirms saying that you have logged in as a guest here and you have quoted from my computer system and the quotation is registered into the database. This is extremely useful for various reasons because now the vendor doesn’t need to actually come to IIT madras. He can be anywhere, he can be situated in Delhi, he can be in Chennai, different parts of Chennai, all that he needs to do is he need to registered with me so that he has a id and the password. Once he has that he just logs into my system and able to quote into my system. 

And once he quotes that quote is actually stored into my database system and other processes and transaction in my database system will be able to actually access and do the necessary processing here. Now let us understand what are the key challenges are conceptual thing that we should be understanding in building this. Now for a minute let us understand this application, what we have as different entities in these applications and how do actually store this data or create my tables and my data and then how do actually make the transactions work with this particular data that I have underneath. 

Now the most important requirements in this particular case is to actually see that underneath I have actually created a database system. In this particular case I am using a MYSQL relational database system which actually allows me to actually create tables and store the database underneath server for me.
 

(Refer Slide Time: 00:36:36)

 

Now what are the different ways actually I can now put my data into this system? For example here is the key database system that I am using for actually creating this rate contract system that is called rate contract rc-april¬¬-6 06 and this has actually got all these tables as you can see I have actually got several tables underneath which are all storing the information for me. For example one can see that there is an approved quotations for add on systems, this is one table. Then I have the approved quotation for computer that is another table and approved quotations for note books and approved quotation for printers. 

Now we can see these are all the tables in some sense they form the schema of the database and this is very critical in database design which you are going to study as part of your course here. How do I design my database system? The starting point for design of the database systems is actually understanding the different entities that are there in your domain. For example in this particular case there are vendors, there are users, there are people who select based on the what the vendors have quoted. So you have actually got different entities and they have some relationship with each other. 

For example vendors quote into your data and then there are people who actually pick information from this and choose. So actually you have to understand what are the key entities in your domain and how exactly each entity in terms of what attributes these entities have. For example one of the things that you can see here which is one of the tables for example here shown here is the quotations for computers which is something that I have used here, this is one of the tables. 

Now this table has different fields, these are entities for example if you see quotation computers has some kind of a table which comes from my entity which we will see in a minute what is the relationship and how do I actually break an entity in to a table. Now in this particular case you can see serial number, chipset, vendor name these are all the attributes or the fields that I have for this particular table. If you take for example vendor name and they all are registered with IIT madras then vendors will have a user name and a password and then their address. These are all the attributes that a vendor entity will have and when actually I have this vendor as an entity, one way I could possibly create this information in to my database is by actually converting this entity into a table. 

For example in this particular case as you can see a simple thing like users for example has, to the users of the system and they have the information on company user name and password being available in this particular table. So if you see user as an entity, these are the fields that will be available as part of that entity. So one easy thing when you are designing your database system is to actually convert the entities into tables, relational tables and store this information. Now there are lots of issues and in terms of converting an entity in to a table because if this information is duplicated for example this company name and id is duplicated in other places. For example it is available in other places, the company name is available in other tables then what happens is this duplication of data will create problems when you actually, a company says that now it has changed its name. Then you have to go and then start changing this name at multiple locations. 

So one of the key things that we understand, when we are designing database is that one fact at one location. So you don’t want to store the same piece of fact at multiple locations because it causes lot of problems for you. So database design concepts will explain how exactly this process of design could be done and how one can come up given an entity relationship diagram, how you can come up with tables like this. For example the database design in this particular case has all these 23 tables and this is the essential part when I design my system and this is the key of the whole concept and how do I actually arrive at these 23 tables and each table for example has its own for example in the case of quotations, you can see this is the schema that I have. 

You can see the table here has serial number, chipset, vendor name, unit price, bulk price bulk two prices. For example if they quoting for more then some number of systems whether there is a discount additional with monitor, without monitor all these stuff and then whether the monitor is what kind of monitor brand that they are supplying with me and what are the comments. For example, whether he wants vat to be added or its included and all this. So this is what we understand as the database design. 

Now these schemas also changed. For example tomorrow I don’t want this bulk two price to be quoted because it is no longer my requirement. Then I drop this particular field from my quotation for computer which means that I no longer need this particular field to be available. This is what we understand as schema change in my system, in my database. 
And often these schemas change because as the systems evolve, business processes evolved and systems evolved schema changes are natural. And your system should be able to cope up with this schema changes that’s other important issue when we actually deal with databases.  

Now in this particular case we will go and see the quotations for computers and see what we have done earlier in terms of these quotations whether that is available in this particular case. As you can see here I am using the php admin and directly getting into the database. For example in the particular case, I can go and then browse for example the values of the database. 

(Refer Slide Time: 00:44:30) 

 

Now you can see that these are the values that are available for this particular field. So I can also probably browse the entire database, so you can see that this information that’s available in this particular table can be, you can use the browse and you can see what is the information available. For example quotations for computers you can see now here, you have Dev systems which quoted intel G 33 chipset for this unit price, this is a bulk price they didn’t quote bulk two price. Then it’s a category is its a local vendor and the monitor is the viewsonic and then a some comments for people to look at for both viewsonic or acer TFT monitors, you probably would like to supply both of them. So you can see this. This is what we have as the information right now. One can even see how this data gets changed when something got updated. 

For example you can go to the, as we saw earlier you can go to the computer system and once you have quoted for that, you probably can see based on here what I had given earlier as a guest has come up now here.











(Refer Slide Time: 00:45:58) 

 

You can see that I have inserted earlier as a guest and quoted 25000 and 18000 as the unit price and without monitor I have quoted 18000. So that information is reflected here as you can see here and the default samtel has been picked up and I said vat is extra and that is also available for me here. Now as far as the work flow is concerned, I can use this information whatever people have quoted and pick whatever quotations that I want, put them into the approved quotations for examples for computers. This will basically reflect the approved quotations for the computers. 

Now you can see all the approved data is available in this particular case and then this is the approved systems for computer systems and that’s how the information can be organized in terms of your database tables. I can pick the informations from one table and then you know it can be added on to the other table. All this is what basically are called transactions. So you are going to look at in this course in detail how transactions operate on the database system on the underlining data and change the state of the data as it goes. For example each one of this can be seen to be a transaction for example, one can think of which are the things that are actually transactions in this particular case. 

I am searching through the computer configuration as I have actually shown you earlier. Now in this particular case I am actually the first level search is not a transaction because it is just giving me all the fields that I can use for searching. Now you can see once I come here, this is the point where I choose a specification and once I choose a specification or a vendor name for example I am actually trying to make a query into the database. It is at this stage actually it goes and picks up this data from the database table that I have underneath. 

For example these are all the vendors that I have in my system and now once I choose one more vendor here and I say, let us say I am actually choosing the one of the vendors that is available here and then submit that thing. It again goes and reads the data from the database. These are actually called the read only transactions because all that they are doing is they are just going into the database and reading the information that I have and then giving me in a convenient form where I can look at it through my browser. So lot of thing will be happening underneath but then ultimately all that it is required here is to go, carry the database, get the required information and show this information for somebody who is asking for that information.
 
(Refer Slide Time: 00:48:26)

 

So these are actually read only transaction, they are just reading the values that are stored in the database. 

(Refer Slide Time: 00:48:53)

 
But as opposed to this kind of transactions, one can even think of the other kind of transactions which we saw here. We actually said that one can login as a guest and once he logs in as a guest now, you can sort of quote for the computer system. Now once he says that for a particular system, 3 is quoting now and then he give some data here saying that please take my quote as let us say 12000 and then give the other thing as 8000. Then I think what we are doing in this process is, we are actually updating our underneath database.

Once he says quoted what is happening is you will be seeing that the whole quote has been registered into the database which means that the data has been now been written in to the database. So these are actually write transaction, they are writing in to my database. There are some transactions which can go and both do both read as well as write from my database. To summarize what’s happening here, what we are trying to do is given an application like this, we have been able to design the underneath tables which are the database models and store the data in that particular model and have our transactions which are actually accessing this data and supporting various business processes that I have in this particular case searching through my rate contract, allowing my vendors to quote into the database and allowing my faculty to choose and then order the related computer systems. 

So the entire business processing of the data is automated using an underlining database system and hence database systems become extremely critical in terms of supporting automating this processes and eliminating manual processing and manual production of the papers so that the whole work flow gets completely automated and we have a simple and elegant and nice way of actually handling our business processing systems. 

Now what you are going to see in this course as part of this course is you are going to see how this database design can be done, what is the meaning of transactions and how they operate and how do they actually maintain the consistency of the underlining data and provides certain properties when we develop these systems and make it available for day to day use for the people. So these issues are going to be covered in depth in the next 41 lectures that we have in this course. Thank you.  

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 1a

Conceptual Design

Hello in today session we would be looking at an important concept in the design of databases namely what is called as the conceptual design of databases using model called the entity relationship or the ER model. 

(Refer Slide Time: 00:01:24)

 

Now you might have heard somewhere that databases today are based around the relational model where the fundamental aspect of the building block of a database are tables. So where does this ER model come in to the picture and what is its relevants and why do we need something like an ER modeling for databases? So in order to answer that question let us begin with looking at a small description of what a typical database design process looks like. This is just a simplified representation of what a database design process looks like.   









(Refer Slide Time: 00:01:57)

 

However this captures the gist of most database design process. Whenever we talk about databases, we should always remember that databases are always embedded within some kind of an application context and this is called the information system within which the database is embedded. So databases could be embedded within for example for a banking solution, the databases could be embedded within railway reservation solution, there could be embedded within a university setup, a company whatever. 

Now whatever this setup is, this context or the information system context is what is called as a UoD or the universe of discourse. The universe of discourse basically tells us what are the building blocks that make up this context within which this database is going to be run. Now generally when we talk about designing a database system, we first talk about looking in to the UoD and collecting requirements from the UoD. We have to make an analysis of the UoD saying for example, suppose you want to build databases for a bank, we have to first identify or we have to first understand how does the bank conduct its operations, what kinds of operations do they have and what are their rules, what are their norms and what identifies a correct operations from an incorrect operation and so on and that is going to give us two kinds of requirements. The first is what is called as database requirements or you might also term it as data requirements and the second is what is called a functional requirement.  

Now as you might have imagined that database requirements are handled by DBMS or the database management system. However a DBMS by itself is not a complete solution. To give you an analogy DBMS is something like an engine. Now we can’t just have an engine, we have to build a car around it or we have to build a bus or whatever it is around it. So this layering around the database system is what is called as the application programming or the information system.

So the functional requirements go to design in the application program or the larger information system with in which the database is going to be. 
 
(Refer Slide Time: 00:04:24)

 

Now   continuing with this process, we take the database requirements and come out with a first level or what we called as a high-level conceptual design. We have to first understand how this database requirements look like, what are the different data elements and how are they related? We need to first understand in a way or rather represent it in a way that is understandable by human beings rather than computers. We have to first understand the problems first before we give it to the computer. 

So database requirements go into what is called as the conceptual design and the output of the conceptual design is what is called as the conceptual schema. Now this conceptual schema is what we are going to be taking up today in more detail but going on with the process itself, let us see what happens with the functional requirements. So functional requirements also go through a similar process where you do functional analysis, you analyze what are the different functions.

If it is a bank, what are the different functions? Opening an account is a function or making a bank transfer is a function. Now what are the constraints that holdup these functions what is a correct function, what is an incorrect function and so on. So once these functional analyses are made, we get what are called as high-level transaction specifications. In fact we will be revisiting this topic called transactions as a separate module where we will see that many times, we need to club different operations of the database into one logical unit or what we called as a transaction.

Now transactions are what are going are the building blocks of the functional requirements or the functional analysis of the information system. Now you can also see down here that all of these, until now all of these are DBMS independent process. 
So what is meant by DBMS independent process? It means that doesn’t matter whether you are using an oracle database or an IBM db 2 database or a MYSQL or postgres or whatever it is. You will have to look at the UoD, you will have to understand your UoD, you have to come out with a conceptual design which has in no way relevant to the actual implementation which the DBMS does and you have to do a functional analysis which is completely independent of the DBMS that we are using.  

Next comes the DBMS dependent processes. So we ended up with the conceptual schema previously in the database stream. You can say that there are two different streams of a processes happening. One is the database stream and the other is the functional stream or the application stream.
   
(Refer Slide Time: 00:06:50)

 

So in the database stream, we had ended up with the conceptual schema. Now the conceptual schema in turn should be transferred in to what is called as a physical design. Now what is physical design? The physical design actually tells you how the data itself resides on the database, on the computer. So obviously this is a DBMS dependent process. So the way, even though there are many standards that exists now the way that let us say oracle stores its data would be different from the way db 2 stores its data. Even though there due to some standards there are going to be slight variations. So this physical design results in what is called as an internal schema or the schema which actually should goes into the DBMS and similarly the high-level transactions specifications that we are talked about are also going to impact the physical design of the database. 

I mean suppose certain kinds of operation have to be performed together. It makes sense to design them near to one another so to say. Let me just use the word in slightly lose fashion here to one another so that it becomes easier to handle transactions on. So you get inputs both from the conceptual schema and from the high-level transaction specs in order to make your physical design of the database. 

Similarly the application stream goes off into application program design and transaction implementation, finally the application software. So under, from all this now we are going concern ourselves in this session with just one aspect of this entire process that we looked at namely the conceptual schema. Now what is a conceptual schema and why is it important for database design?

(Refer Slide Time: 00:09:06)    

 

So let us have some observations about what is a conceptual schema. We just now saw that the conceptual schema is an outcome of the high-level conceptual design of a database. So this is one of the first things we are going to do. We are going to understand our UoD or the universe of discourse. It tells us what kinds of data that the UoD is going to handle and based on this, we are going to come out with a high-level design.  

So it’s a very concise description of the data requirements, what kinds of requirements do users need and how are the data related between one another and so on. Again let me take a brief example. Suppose you are building banking solution. One of the first things that you are going to note is that in a bank there should be accounts, there should be customers, there should be customers or account holders or whatever you call them and   there should be of course monitory transactions and there should be some kinds of logs and ledgers and account books and so on. So all of these represent some kinds of data elements and a very high-level conceptual design would identify each of these data elements and say this is how they are related. So it includes the description of what you called as entity types. 

These are the entities that makeup the UoD. In a bank the customer is an entity, an account is an entity and a ledger is an entity. Similarly in any other UoDs, there would be several different entities like that whether it is a railway reservation system or a university or whatever, they’re going to be different kinds of entity types and there are relationships across the entity types.   

So customer holds an account, an account transfers to another account and so on and so far. So there are relationships between different entity types and there may also be certain constraints that are imposed either by physics or by the UoD itself. In a bank for example     one might say that in a SBI account, balance may not go below 50 rupees or whatever that’s a constraints that is imposed by the UoD that says you cannot have less than 50 rupees in your account in order to make a term, in order to make a term working.   

On the other hand there could be other constraints that are imposed by physics itself, the physical reality. For example if you are modeling a railway reservation, you can’t obviously have two trains starting at the same time from the same place, from the same platform. So it is simply because those two cannot exit at the same time. So there are different kinds of constraints that exist among data elements and this is what we are going to capture during the conceptual design. And another thing to note here is a conceptual design has no implementation details. 

Here note that it earlier we had noted that a conceptual design is a DBMS independent process. So that means we are not really worried here about how this conceptual schema is going to be implemented within a database systems and the main idea of a conceptual schema is ease of understanding. We should be able to say to your end user this is what we have understood from your description, this is what we have understood of your needs and your end user should be able to understand your understanding so to say. So he should be able to understand your understanding, even though the end user may be a non-technical user.  

So the main idea for behind the conceptual schema is ease of understanding and it is used mainly for communication with end users who are of non-technical nature.     
















(Refer Slide Time: 00:13:08)

 

So here is a small example. An example, company database. So suppose we look at a company, what are some of the first things that we can identify. First some of the first things we identify are a company of course has employees, we can see them. Then company has departments, each employee works in different departments and so on. Then once we start speaking to employees, we find that there are also different projects and each department has some one or more projects and a project may span different departments and so on.  

And once we start to the HR, talk to the HR department, we note that each employee also has some dependence and there are also covered in some way or supported in some way from the company and so on. Now if you look at this slide carefully, here we see that there are two different kinds of things that we are noting here. All these things before the colon, we see the things like departments, projects, employee, dependence and so on and after the colon you have a terms like name, department id, manager or name, project id, location or name, pan number, address and so on.   

So what this says is that all this names before the colon like departments, projects are entity types. They are different entities say department is an entity, project is an entity, employee is an entity and so on but once after the colon like name or manager or location and so on are what are called as attributes of this entity that means they belong to this entity. For example the name occurring after department here belongs to this department, the department id belongs to this department. 

Similarly the project id belongs to project and address belongs to employee, date of birth belongs to employee and so on. So it not only belongs to employee, it also tells us or it describes something about the employee. So if there is an employee, an employee has a name, an employee has a pan number, if he is paying income taxes regularly that is he has an address, he has a salary, he has well, he or she has a gender and date of birth and so on. So all of these attributes define what is an entity type and what are the characteristics of that entity type.
   
(Refer Slide Time: 00:15:35)

 

So let us formulate things a little bit now. In the ER model or the entity relationship model, we have standards notation of representing entity types and their attributes. As you can see in the slide here, entity types are represented by rectangles. So department is an entity and attributes of the department like department name, department identifier, manager of the department and so on are represented in the form of ovals or something like balloons hanging over this entity types.    

So there are some definition here. So if you review this, an entity represents an object of the real world that has an independent existence. Note that term independent existence that is you should be able to identify an entity independent of every other entity in the system. Address is not an entity because address belongs to some entity called employee or department or whatever. However the employee itself is an entity because it is an independently occurring object in the UoD and an entity has attributes or which are properties that describe the characteristic of this entity.   











(Refer Slide Time: 00:16:53)

 

Now what are the kinds of attributes or what are the different kinds of attributes that could exist? We saw name and age and date of birth and so on but can we classify this attributes into different kinds of attributes and it can tell us something about characteristic of the entities. So there are several different classifications of attributes, we are going to look at small number of such classification. One of the first classifications of attributes is simple versus composite.   

So what is a simple attribute? A simple attribute is let us say something like age. You are 30 years old, 30 age equal to 30. So that’s a simple attribute that is just one value associated with an attribute. So when you say age, you get back one value called 30. On the other hand suppose you say name, in some countries we say name is name, you just talk about one name for an employee but in several different places when you talk about name, you have to specify a first name and middle name and last name and title and nick names or whatever so. 

So basically a name is composed of several other attributes within it, so that is what is called as a composite attribute versus a simple attribute which has just one value and there could be a something like a single-valued attribute or a multi-valued attribute. Now what is a single-valued attribute? Again age is a single-valued attribute. At any point in time at least you have just one age that is at this point in time let us say you are 30 years old or 35 years old or whatever. On the other hand at any given point in time, an attribute may actually have multiple values.  

Now take the color of a peacock for example. No matter when you are going to measure this, no matter at what time you are going to take a snap shot of a peacock, you are always going to find many colors. So this is a multi-valued attribute, that is the attribute itself is defined by multiple values. How is this different from a composite attribute? Let me pass here for a little while so that you can ask this question to yourself again. How is a multi-valued attribute different from a composite attribute? Now composite attribute if you saw, if you notice closely is made up of different sub attributes. 

For example name is made up of first name, middle name, last name and so on and initials for example or title and so on. All of these need not be of the same type. I can say title can be either mister, doctor, misses, whatever and so on only these three types. On the other hand name could be anything, any string and so on. On the other hand when we look at multi-valued attributes, all the different values that it takes, color of a peacock for example it might have different attributes, many attributes but all of them denote color blue, green, red, white, whatever all of them are color, all of them are of the same types. So multi-valued attribute are different values of the same attribute type but a composite attribute is a collection of different smaller attributes in itself.

(Refer Slide Time: 00:20:19)     

 

And then there, what are called as stored attributes versus derived attributes. Now stored attributes is something which you just store in the database, your date of birth for example. So when you ask what is your date of birth, you just give some date and then store it in the database. However suppose you were to ask what is the age of an employee. Now suppose you know today's date and you know the date of birth of the employee, you can always derive the age of the employee. So age in a sense is a derived attribute, a good database design would put age as a derived attribute because a database hopefully is going to be used for a long time. It’s not just today, today an employee might be 30 years old but 5 years later he still won’t be 30 years old, he would be 35 years old. So it’s always best to have age as a derived attribute because you always know what is the current date and you always know the stored attribute called the date of the birth.  

Similarly there are what are called as null attributes or null values for attributes. Sometimes an attribute may not have any associated value for a particular case and let me give an example. Suppose a student can take at most two elective courses in a semester   and at least one elective course and at most two elective courses. Now let us say there is a student who has taken just one elective course. Now because he can take at most two elective courses there are four different, there are two different slots, elective one and elective two. Now let us say after, at the end of the course is give a grade for each of the courses, that each of the courses he has taken up. Now he gets a grade for the first course but he doesn’t get a grade for the second course because he didn’t take the second course at all, elective two has no grade. But this grade is not we can’t say that he has scored zero in elective two. So a null value is different from zero or unknown or missing value. This is not a missing value this is not a, this not zero or this is not an unknown value, this is a null value. That means that something like when you say n a, when you write in   application forms this is a null attribute.     

So let us come back to entities and define a few more terms here with respect to entity types. So we saw that an entity type, I have been using the word entity type but have never really formally defined this term. So an entity type is a definition or defines a collection of different entities of the same type. 

(Refer Slide Time: 00:22:47)

 

For example in this slide, we give this example of department. So a company could have different departments but when you say an entity type department it means that it represents all departments in the company. So it is a definition of a real world object type. So it’s not this department or that department, it is just department. It is a type that the entity is going to define.   

Now any specific collection of entities of a particular type is what is called as an entity set. For example if you take supplies departments plus accounting department. Now suppose we take both of them in some collection. We call this as an entity set, these are two different departments of the same entity type. So we also say that an entity type as opposed to an entity set. So an entity type is set to describe the schema or the intension of an entity set. 

What is that mean? It basically says that how should the structure of this entity or entities of this type look like. So suppose we say that department should contain a department id, it should have an address, it should have a manager, it should have several other attributes that means that no matter whether it is the supplies department or the accounting department or the systems programming department or whichever   department it is, they should be defined by these attributes. That is they should have a department identifier, they should have a clearly defined address, they should have one % designated as manager and so on.    

So an entity type describes what is called as an intension or the schema for an entity set   or different entities of the same type. So here are again a review of the representations entities types are defined by boxes and attributes are defined by ovals and different kinds of attributes have different kinds of ovals may be its not completely visible here. The multi-valued attributes are defined by an oval having a double line.   

(Refer Slide Time: 00:25:03)

 

And similarly derived attributes are those which can be derived from other attributes are defined by a dotted line instead of a solid line.    








(Refer Slide Time: 00:25:38)

 

And we now come to another key issue when we are talking about entity types and namely this is about key attributes. Now what is a key attribute? Now if you notice carefully when you said that each department has to have a department identifier and manager and address and so on. Now why do we need a department identifier for each department? This is to be able to identify each department uniquely.  

Now if I say department number 10, it means just one department no matter what its name is. Department name could be ambiguous, it could mean supplies and accounts and   suppose we might have a department called administration, it might mean both supplies and accounts or whatever so. A name could be ambiguous but when we say an id or an identifier, it’s a something like what you have a pin code in post letters. It uniquely identifies each entity of this entity set. So the department id attribute is unique for each department that belonging to this entity set. So such attributes which can uniquely identify entities are what are called as key attributes or keys.   

So the key attribute as you can see here is specified by an underlined definition. So the department id is underlined here to denote that it is a key attribute. 











(Refer Slide Time: 00:27:14)

 

Now in some cases it need not always be just one and that is the key attribute. There could be more than one attributes which are key attributes in which case all attributes which form part of the key are all showed underlined. Now what is meant by a key attribute? So an attribute or a set of attributes that uniquely identify entities in an entity set. So is what is called as a key attribute? Now like I said it need not be just one attribute that is a key attribute, there could be more than one attributes which is a key so in which case the key attribute is called a composite key. So the key is made by combining two or more attributes together. 

A composite key should be minimal what is it mean to say a composite key is minimal. Let me first give you the definition of what is mean by minimal and then we can see an example. So no subset of a composite key should be a key itself. For example let me take the attributes department id and department name. Is it key attribute? Of course it’s a key attribute because if I take department id, department number 10 supplies, it uniquely identifies one particular department.  

However the second attribute here, the name called supplies is redundant. We don’t need to have this attribute here in order to uniquely identify a department. It’s sufficient enough if you say department number 10. So that’s what is meant by a composite key that is department id and name is not a composite key because it is not minimal in nature. No subset of a key attribute should be a key attribute in itself in which case this is not so. So, key attributes are shown underlined in the ER diagram.  






(Refer Slide Time: 00:29:14)

 

There are also certain other properties of keys which we are going to revisit again when we are going to look into the enhanced ER model. What is called as the retention of key attributes in all extensions? So let me not take this property right now and we will come back to this again that the key attribute should be retained for all extensions of a particular entity type. Now it may so happen that a key attribute should uniquely identify an element but an element need not have just one key attribute. 

Of course I can have two keys to my house, key from the front door and the key from the back door. Now it is fine but the only thing is this key should open only my house door not somebody else’s house door. So that’s the idea that is a key should uniquely identify a house but a house could be identified by more than one key, you could either enter through the front door or the back door.  

Similarly I can have a let us say on a computer network, you can uniquely identify a machine by its ip address or so to say an ethernet address on a lan and so on but a machine could have more than one ethernet cards in which case both of them uniquely identify the machine. So there is no restriction on how many keys that you are going to have but usually we are going to use one key, so what is called as a default key. Usually we are going to enter through the front door not always through the back door. 

So usually we are going to use just one key but you may have more than one keys that define a particular entity type. And there could be some entity types which have no key attributes. We are going to take a look at one such example little later on and it may not be possible to define any kind of a key attribute for such entity types and such entity types are called weak entity types or entities of such type are called weak entities. So they have to what we called as totally participate in some relationship in order to define   themselves.  

Now we come to the next definition of, definition in the ER model what is called as a domain, what is meant by domain of attributes. Now a domain to put it in a very informal fashion is going to show you the space in which an attribute is going to be defined.  

(Refer Slide Time: 00:31:25)

 

For example if I say age of an employee and there are some set of rules that you cannot have employees lesser than 18 years and greater than 65 years. The domain is basically a number between 18 and 65. Obviously it can be, there are also some physical limitations in the sense that the domain of an age can never be negative so that’s the nature imposed restriction but there could be other restrictions and which basically defines the space within which any value of this attribute can reside.   

Now if a composite attribute has different sub attributes or like we saw name has different sub attributes first name, middle name, last name and so on. And each of them have the room domains let us say D¬¬¬¬1 D2 Dn¬. So we see that the domain of the composite attribute is Cartesian product of the domains of individual attributes. So the first attribute can take a value between 18 and 65 and the second attribute can take a value between 0 and 10 and so on. So the domain of the entire attribute is just a Cartesian product of all of these terms. 










(Refer Slide Time: 00:33:01)

 

So let us briefly come back to this company example and see likewise whatever we have seen until now. So we can think of different kinds of entity types like department, project, employee and so on and there are different kinds of attributes for each kind, each of these entity types and there are certain key attributes here which are underlined and then there are certain composite attributes which are shown like this that is name is composed of first name, middle name, last name and so on and like that and several other entity types that we can identify during our analysis.  

The next concept that important concept that we are going to be looking at in ER modeling is the notion of relationships like the modeling itself says it’s about entities and relationships. So we just until now we looked into what are called as entity types. So we defined as an entity type, an entity set, attributes, different kinds of attributes and keys and domains and so on.  

Now let us see how can we relate entities of different types. So a relationship are to be precise relationship type defines a relationship between two or more entity types. So the slide here shows a relationship type between an entity type department and an entity type employee and it says managed by.  










(Refer Slide Time: 34:35)

 

So it basically says a relationship type specifies that any department or any entity of type department should be managed by some employee or some entity of type employee.   

(Refer Slide Time: 00:34:54)

 

So let us go further and make some definitions. First of all, let us first defined what is meant by relationship type. So relationship type R is an association among different entity types, it need not be just two entities it can be any number of entities. So there could be n number of entities and defines, it basically defines a set of associations, what is called as a relationship type.   

So if you look at it a little bit carefully, a relationship type is just a subset of the Cartesian product of all of the entity types. So that means an entity instance or entity or entity of this type could be related to some entity of this type and some other entity of the next type and some other of the next type and so on and all of this define one instance of this relationship.    

(Refer Slide Time: 00:35:49)

 

So let us dwell deeper into what are meant by relationships and what are some of the characteristics that define relationships. Firstly, the notion of the degree of a relationship type. So what is a degree of a relationship type? It’s simply the number of entities or that are involved in this relation, it is how many, between how many relations is this relationship going to establish an association.

We saw earlier a relationship type of degree two that is a department is managed by an employee. So there are two entities that participate in this managed by relationship. Such kinds of relationships are called binary relationship, binary basically because there are two entity types involved. Similarly there could be unary relationships, there could be ternary relationships and so on and there could be n array relationships, n different types of relationships.    










(Refer Slide Time: 00:36:49)

 

Now we come to kind of a tricky problem here. Now have a look at this slide carefully. The first picture above shows the relationship type, it shows employee works for department and the second relationship and the second picture below shows employee as an entity and department in which the employee works in as an attribute of employee. So which is correct? Is department an attribute of employee that is does the department in which an employee works in describe the characteristics of an employee or is it that departments and employees have separate existences or separate entities and there is a relationship type between them.  

So let me confuse this problem even further, take a look at this slide here. So in the earlier slide, you could have probably said that no department cannot be an attribute because department has an independent existence. We already saw that each entity type should have an independent existence. So because departments have an independent existence they can't be an attribute.  














(Refer Slide Time: 00:37:50)

 

But have a look at this slide here. The first picture shows employee works for department as a relation and then department has a department id as one of its attributes. Now I am going to take this department id attribute and put it in to employee and say employee works in this department, so this employee is associated with this department id. So now which is correct or which is wrong? So is the first one correct or the second correct? The answer to this obviously is it depends on the particular situation in which you are looking into.   

Now the first one where we show that employee works for department as a separate relation depicts the relationship between employee and department and the second one just depicts what are all the different characteristic that describe or what are the different attributes that describe the characteristics of employee that means if I am talking about an employee entity and if I say that an employee is or in some sense an employee is very closely tighed to his department. So he has no identity without his department id in which case you have to say department id is a part of the attribute or is one of the attributes of the employee entity type.    












(Refer Slide Time: 00:39:46)

 

So relationships versus attributes, it’s a slightly a tricky problem always, so in order to determine whether a particular thing is actually a relationship or it is an attributes. And in some kinds of data models like what are called as functional data models or in object databases, relationships are almost always refer to as in the form of attributes. For example in object databases, you talk about relationship by storing an object reference. Let us say you have a relationship between employee and department, so there is a reference to a department object within the entity object and vise versa and a reference to an employee object in the department object and so on.

Similarly in relational databases which we are going to study much deeper, we see that relationships are established by what are called as foreign keys that is there is one table that describes one kind of an entity and a relationship between this table and the other table is described by a foreign key attribute which says that this entity of this type is related to some other entity of the other type. Now we come to another important issue in the when you are talking about relationship, this is the notion of constraints.













(Refer Slide Time: 00:40:59)

 

So when we talk about relationships almost always relationships are defined by certain kinds of constraints. Take the examples of employee managing a department. One of the most simplest constraints that we can think of is department should be managed by at most just one employee. We can't have two mangers or two heads of a department, we should have a just one head of a department and you might also establish a constraint that one person can manage at most one department at a time. 

In some cases there could be allowed to manage more than one department at the same time. So that means in that case there is no constraints from employee to department but there is a constraint from department to employee. So when you are talking about constraints and relationship types, we are mainly concerned with two kinds of constraints what you call as cardinality ratios and participation constraints. So what are these cardinality ratios and participation constraints let’s have a look at them. 
  














(Refer Slide Time: 00:42:15)

 

So take a look at this relationship again. Here now I have replaced this managed by with another relationship called works for. Now works for is slightly more general than managed by in which ways it more general then managed by. A department can have many employees working for it.   

However sometimes we might have to mandate a requirement that an employees can work for only one department not for multiple departments. So this is shown in one of two different ways either like this that is N : 1 which says that N employees to one department or something like this, so which says it could be N employees working in one department and so on. So on the other hand what happens if this were to be M:N   something like 4 : 3 or 2:1 or 2:5 or something like that. So that means that let us say 4:3 so that means that a department can have at most 4 employees and an employee can work in at most 3 departments and so on. So basically you can represent a cardinality constraint or cardinality ratios in this case that says participations in this constraint is defined or constrained in this, by this cardinality or number of entity types.    

The second kind of constraints that’s important is what are called as participation constraints. What is a participation constraint?  










(Refer Slide Time: 00:43:49)

 

Take a look at this slide here. Now this slide shows another relationship type which says department handles project. Now a department may handle several different projects. Suppose there is a restriction that a project has no existence unless it is associated with a department. Now suppose I have a project for developing some kind of software. Now this project does not exist if it is not associated with some department, if there is no department which is in charge of this project.   

Basically what this says is that if every project has to be associated with a department then the very existence of project depends on this relation. So only if an instance of this relationship exists only then can a project entity type exist or such kinds of constraints are what are called as participation constraints. They are defined by a double line here may be it’s not fully visible but it is a double line here. So that means that the very existence of this project is dependent upon the existence of a relationship of this kind.   

Going on further in to participation constraints, we say that the entity type in this case project is said to totally participate in to this relationship because if it doesn’t participate it doesn’t exist anymore.   











(Refer Slide Time: 00: 45:16)

 

So a participation constraint in which an entity type existence is dependent upon the existence of this entity type of this relationship type is called as total participation. That is the entity type or participates completely or totally in this relationship. We can also think about attributes for relationships and not for entities themselves, just like we saw   attributes could be associated with entities. We could also allocate attributes for relationships. 

Have a look at this figure. Here it shows department as an entity type and says that department handles certain projects and project totally participated in this relationship and then an attribute called budget is placed for this relationship. So what is this mean? It basically means that this budget or the budget that is specified here is allocated for this project associated with this department or is specifically allocated for this relationship type.   















(Refer Slide Time: 00:45:50)

 

So in cases where let us, the project is handled by more than one department that means that this budget is not allocated to the project. This is allocated only for this project for working on this department and so on. It is not allocated to the department as well that means the department cannot use this budget for anything else; it has to use it for this project only. So this budget actually is a constraint on the relationship type, so it belongs neither to department nor to the project in its entirety. 

However there is certain kind of relationships where we can actually move the attributes from the relationship type to one of the entities. What are those kinds of relationships? Take a look at this slide here. The first figure shows a 1: N relationship that is a project can be associated with at most one department and there is an attribute called budget that is allocated to this relationship.
 















(Refer Slide Time: 00:47:23)

 

Now if you see carefully, we do not do correctness if we move this budget attribute from the relationship to the project site. So if I say that this budget is allocated to this project, it does not lose any semantics because a project can be associated with just one department. In the previous slide, a project could be associated with M different departments that is   it could be associated with many departments.   

So here since project can be associated with only one department, it does not lose semantics if the budget is allocated to the project itself. So one of the last things definitions that we are going to be looking at today are is what is called as a identifying relationships. Have a look at this slide here, this slide here shows a figure where an employee is identified by pan number. So assuming that every employee is a tax payer and has received pan number from the government, an employee is uniquely identified by his pan number. 

Now let us say we prepare, the company prepare some kind of an insurance record for each employee.  












(Refer Slide Time: 50:00)

 

Now let us say this entity called insurance record which contains certain parameters or whatever is an entity by itself because it has an independent existence. You can see an insurance record and note that it’s an entity by itself. However you see that an insurance record has no existence, has no meaning unless it is associated with somebody unless it is associated with some employee or some department you want or whatever. So this is an example of an entity type which is a weak entity type that means it has no keys. The key for the insurance record is of course the pan number itself or pan number of the employee. So the key of the employee which defines an insurance record forms the key for the insurance record as well and the relationship that defines this, such a kind of association is what is called as an identifying relationship.  

So this relationship here in this slide which is called as insurance details identifies insurance record with an employee. It basically ties in insurance records with employee so that an insurance record also gets independent existence by themselves. So as you can see insurance record totally participates in this relationship. Obviously it has to totally participate in this relationship but on the other hand not every total participation may mean an identifying relationship, we actually saw an example earlier where we saw         two different entity types department and project in participation but which is not a weak entity type.  

So, identifying relationships are again defined by double lines on the relationship types itself. So let us briefly pursue a summary of the different notations we saw. The first thing that we saw was an entity type and defined by a box and a weak entity type which is defined by a dotted line which is not clearly visible here.  




(Refer Slide Time: 00:51:10)

 

Then relationships types are defined by rhombus and the identifying relationship types are defined by double lines.  

(Refer Slide Time: 00: 51:38)

 

Similarly attributes are defined in different ways that is normal attribute is an oval, a key attribute has an underline there and a multi-valued attribute has double line and a derived attribute has a dotted line. So with this we come to the end of this session where we had a brief look at what is meant by conceptual design of a database. 
So to briefly summarize that a conceptual design of a database is meant for non-technical users, it’s a high-level design and mainly composed of diagrammatic notations like entities and attributes and so on. And within this diagrammatic notations, we saw that there are several different characteristic, there could be entities, entity type, entity sets attributes, multi-valued attributes and key attributes and relationships, an identifying relationships and weak entity types and so on.   

So one of the first step that we do in database design is to be able to identify this entity types and relationships and to be able to build this ER schema. So that brings to the end of this first secession on conceptual modeling of database systems.

Thank you. 

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 2

Conceptual Design

Greetings to you all. We have been talking about conceptual modeling of databases in previous session. So in this session let us continue with this subject into some more detail and before we continue or before we start with today's session, let us have a brief review of what we looked into conceptual modeling in the previous session. If you remember we first talked about what is a typical database process or design and development process looks like. 

We first start with analyzing the UoD or the universe of discourse and the analysis of the UoD reveals us two different kinds of requirements. One is the data requirements and the second is the set of process or application requirements. So the database runs with in an application context like I gave the metaphor in the previous session, a database is like an engine and the application context is the overall body or the car or bus or whatever that you build on around the engine, so both are important when we are trying to analyze the UoD. 

So coming to a database analysis or design of the database system, we first start with a high-level description of what the database should handle and this high-level description should not include any DBMS specific terms or DBMS specific issues. It is mainly meant for the end users, its mainly meant for us to show the end user saying this is what we have analyzed or this is what we have understood by analyzing your universe of discourse and it is mainly meant as a conceptualization of what are all the data requirement that exist in this domain.  

So one of the most popular models for performing this conceptualization or building a conceptual schema is what is called as the ER diagram or the ER model, the entity relationship model. We saw that entity relationship modeling are made up of essentially two kinds of building blocks, entities and relationships. So we looked at different nuances of entities and relationships and several issues that affect them like constraints and attributes and so on. 

So let us briefly describe some notation, this is not exhaustive review but review of some of the main points what we looked at in the entity relationship diagram. 






(Refer Slide Time: 00:03:46)

 

An entity type is described by a rectangle like this, a simple rectangle and an entity type is something which represents a class of entities or objects that have an independent existence like a customer is an entity or a staff is an entity, account is an entity in a bank or in a company a department could be an entity, manager is an entity and so on. Any logical U-net that has an independent existence is called an entity and an entity type is an intension or some kind of a schema for a class of entities or for a set of entities. There could be a set of different managers but all of them share the same attributes or the same properties of the entity type called manager. 

Similarly there could be several departments but all department share the same properties of the entity type called department. And then entities are associated with attributes which describe the characteristic of the entity and we saw that attributes are in turn defined by the domains. For example the age of an employee is defined by a domain that it should be greater than or equal to 18 years and less than or equal to 65 years typically. So a domain represents a space within which an attribute lies. And even within attributes we saw that there are several kinds of attributes, you could have a normal attribute, a simple attribute, an attributes which takes just one value or you could have a multi-valued attribute, we gave the example of the color of a peacock. 

It doesn’t have one color, it has many colors and so it could have different values for the same attribute. An attribute could be a composite attribute that is it could contain many sub attributes like first name, last name, middle name and so on. It could be a derived attribute like the age of a person which can be derived, if you know the date of birth of the person and the present date. 

So there are several different of attributes, so all entities of a given type have the same set of attributes. Now we also saw what are called as key attributes or what are called as keys. I have not depicted this here in these diagrams but key is some set of attributes that can uniquely identify an entity within an entity type. So if I have a set of employees and suppose employees are given an employee identification number, the employee identification number becomes the key. If they don’t have an identification number, we usually have something like the income tax permanent account number called the pan number or something which forms the key, it uniquely identifies a person.

On the other hand we can't use something like name or age as the key. Two persons can have the same name and of course two or more people are very likely to have the same age. So it cannot uniquely identify a particular entity whereas something like the identification number identifies the entity uniquely. So key attributes identifies this   entity types uniquely and there need not be just one key attribute, there could be more than one key attributes and like I gave the example of I need not have just one key to my house, I can have two keys, one for the front door and one for the back door. But we usually use just one of the keys which the default basis and sometimes use the other key frequent and then we talked about what is meant by a weak entity type. 

A weak entity type is a kind of entity type which does not have a key attribute. We also saw an example yesterday, the example of an employee and his or her insurance record. So an employee has a key attribute, the employee number or pan number or whatever but the insurance record does not have any existence without a corresponding employee or without a corresponding person to be more general without being associated with a corresponding person.

Now once an insurance record is associated with a particular person whatever way, whatever we use to identify the person becomes the key to identify the insurance record as well. So for example identify an insurance record with a person having a particular pan number, I can as well identify the insurance record with the same pan number as I am identifying the person and we saw the second building block which are called relationships between attributes. 

So a relationship basically ties in or brings in an association between two or more entity types. So even within relationship there are several different kinds of relationships and we saw certain constraints that identify some kinds of relationships. One of the first thing we saw was the cardinality constraints, a relationship which has a cardinality constraint says that, says how many entities of a particular type can participate in a relationship. For example we saw that department managed by manager. So it could be, there could be a constraint that a department may have just one manager or one head and one person may manage at most one department at a time. 

So there is a cardinality constraint that exactly one, one department may be managed by exactly one manager. On the other hand if I have something like works in, an employee works in department. So it’s rather than a 1:1 relationship, it is a 1: n relationship. One department may have n employees and there could be a constraint that an employee may be associated with just one department so that for n employees there are just one department so on.  

And we also saw what is called as an identifying relationship. An identifying relationship is the one, is a relationship that identifies a weak entity type with a strong entity type. That is if I have an employee and says insurance has this insurance record using some kind of a relationship, it means that this relationship is giving an identification or an identity for this weak entity type called insurance record. So such relationships are called identifying relationships and we are also saw what is called as a total participation within a relationship. 

So the same thing here, an insurance record will have no existence without its corresponding employee or without a corresponding person. So this insurance record is set to totally participate in this identifying relationship. However we also saw that just because there is a total participation does not mean that the entity type is a weak entity type. How do we, what is an example for that? We saw an example of department managers project. 

Let us say that a project has to be associated with a department otherwise well, it will not get funding, it won't get off the ground or something like that. So in this case this is a total participation that is the existence of project, the existence of projects will depend upon this relationship that is the existence of some department that is willing to manage this part.  

However a project by itself need not be a weak entity type that means it may have a separate key by itself, a project will have its own separate project identification number which may have no relationship with the department number. Not every total participation implies identifying relationships but an identifying relationship implies total participation. 

So we will be continuing further on today with some more notations and which can give us greater expressiveness to express what we perceive as relationships and associations between data elements in our UoD.
















(Refer Slide Time: 00:11:47)

 

Now these kinds of, these sets of notations that we are going to see today are what are called as enhanced ER notations  or sometime also called as a extended ER notations and abbreviated as EER notations.
  
(Refer Slide Time: 00:11:56)

 

One of the first relationships or associations that we are going to see today is the notation of subclassing or inheritance. Subclassing essentially face what is called as an is-a relationship as you can see in the slide here. An entity class B is said to be a subclass of another entity class A if it shares an is-a relationship with A. 

What is meant by an is-a relationship? Have a look at these examples. A car is-a vehicle, a monkey is-a primate and a primate is-a animal or is an animal whatever. A manager is-a employee. So if you have noticed here an is-a relationship identifies a specialization of some particular entity type. A car is-a vehicle but not all vehicles are cars, there could be trucks, there could be bicycles, there could be scooters and so on. So vehicle is a more general class of cars. 

Similarly if you say a maruti 800 is a car but not all cars are maruti 800’s. So car is a generalization of maruti 800 and maruti 800 is a specialization of the entity type called car. Similarly a monkey is-a primate and a primate is-a animal but not the other way. So the entity class B that is at the left hand side of the is a relationship is set to be a specialization of entity class A or on the other hand entities of class A are set to be generalizations of entities of class B.  

So what are the properties of generalization and specialization? Have a look at this slide and there are some interesting properties when we are talking about a generalization and a specialization relationship. Now suppose I have an entity of type car. So let us say this entity with a particular registration number is a car that exists in the database. Now in the database I also have some entities of type vehicles. Now should this car entity type belong in the vehicle entity type? If you think about it carefully, you will say that the answer is yes because the simple reason is that a car is a vehicle.
 
(Refer Slide Time: 00:13:46)

 

If I have an entity that exists in the set of cars, the same entity should also exist in the set of vehicles. So an entity cannot exist in the database that merely belongs to a subclass, it also has to belong to the super classes and subclasses undergo type inheritance of the super class. What is meant by type inheritance of the super class? Again notice carefully here. Now let us say that we are going to describe some properties of vehicles. Now what kinds of properties can we think of vehicles? Vehicles will have wheels. Vehicles will have I mean depending on what you call vehicle I mean you could also called a rocket as a vehicle. 

So assuming that we are only looking at road vehicles. Suppose I say that a vehicle is represented by wheels and it should have some kind of controlling mechanism, it should have a driver seat or something of that sort and it should move. Now you see that all of these characteristics apply to all subclasses whether it is a car, a bicycle or a truck or a van or whatever all of these have to move, they have to have wheels, they have to have some kind of a control mechanism whether it’s a handle or a steering wheel or anything like that and so on. 

So, all attributes that describe a general class has to be inherited by the special classes, that the specialized class. Each member of a subclass has the same attributes as that of the super class entities and participates in the same relationship types. The second aspect is also important, if a general class entity participates in a particular relationship type, a specialized class entity should also be able to participate in the same relationship type. That is if I can use a vehicle to go from point A to point B, I should be able to use a car to go from point A to point B or I should be able to use a truck or a bicycle or whatever. 

So if there is a relationship that exists between vehicles and let us say an employee using that vehicle, you should be able to replace this vehicle with any of the subclasses and the semantics should not change. The semantics of the entire database system should not become incorrect that’s why said we are looking at road vehicles I mean it’s a matter of naming the particular entity, a car is a road vehicle. 

Obviously if I also consider rockets and airplanes as vehicles, this doesn’t hold anymore. Using a rocket let us say I can go from here to the moon but I can't do that using a car. So you won’t be able to replace a subclass, subclass entity wherever a super class entity arises. So when you are coming out with inheritances and special generalization and specializations in your database, you should be aware of the fact that this type of, this kind of replacements of general class entities with special class entities should be possible that is what establishes a correct inheritance relationship with a incorrect inheritance relationship.   

So here is another example suppose I say that manager is an employee or a manager could also be an employer. So as you can see here the inheritance relationship is depicted by a U in the relationship type. That is there is a straight line with a U which represents an inheritance relationship or a specialization relationship.







 
(Refer Slide Time: 00:17:56)

 

If you note that if this is an employee here and an employee is uniquely identified by the pan number, the same pan number can uniquely identify the manager as well. So what is that mean? It means that any key attributes that uniquely identifies, that uniquely identify entities of a super class or a more general class can also identify attributes of the special classes. 

However special classes may have some more attributes in addition to the key attributes that the general classes have. For example a manager may also have one more identification which says for which department is a manager or what is a scale or whatever. Now in order to be able to identify one manager from another uniquely, you may have to combine that attribute with a pan attributes here. So there may be other attributes that form the key for the special classes but all the key attributes of the general class has to be retained in the special classes. 

So another example of type inheritance, in this case it’s more of a key inheritance the key has to be inherited directly. The process of creating subclasses out of a given entity type is called specialization. 











(Refer Slide Time: 00:19:42)

 

That is suppose I have a particular entity type, suppose I have identified that the UoD here requires vehicles. Now out of these vehicles I identify that they require vans, they require cars, they require trucks and so on. And then I also identify that van is a vehicle and a truck is a vehicle and a car is a vehicle and so on. So I should be able to form what is called as an inheritance tree. So this process is called specialization. 

On the other hand its also possible that we go in the reverse fashion. We first look at the UoD, we first go through the company, talk to people and see what is happening and then we identify different entities. We see that the company uses cars, the company uses buses, the company uses motorbikes, the company uses trucks and so on. The company uses vans and then once we have listed all of these we start seeing relationships among them, we say that all of these are vehicles and all of these share the same attributes as far as the company is concern. And then we put all of them in an inheritance tree and this is what is called as a generalization.  

Now before we go to the next slide, let me inter check here to note that this is not such a straight forward process, which entity is a special class and which entity is a general class is not such a straight forward process. Sometimes depending on the usage context which becomes a general class and which becomes a special class, a specialized class may change from one context to the other.

Let me give a particular example. Take two entity types an airplane and a glider. Now which is correct? The first one which says a glider is an airplane without engines or whatever, a glider is a airplane or an airplane is a glider. Which is correct? So if you look at it carefully, let us go back to what are the properties of specialization and generalization classes.

The first property of specialization classes is that wherever I am planning to use the generalized class objects, I should be able to use a specialized class objects. So is that always true, wherever I use airplanes can I use gliders? Maybe or maybe not, I mean depends, it depends on the context and secondly whatever attributes that the generalized class has, has to be inherited by the specialized class. 

Again this seems to say that let us say an airplane has several different attributes, it has engines, it has wings, it has wheels, it has controls and so on. A glider also has all of them except that it doesn’t have an engine. So it seems to suggest that an airplane is a glider is a correct one. So glider is more general and an airplane is more specific, so because a glider has a smaller number of attributes and an airplane has a larger number of attributes. 

However look at it in the context of learning how to fly a glider or learning how to fly an airplane. Now if you see that let us say I have different paragraphs about or different kinds of skills that I have to learn for flying an airplane and for flying a glider. It could well be the case that depending on the sophistication of the airplane, there are some airplanes here where you don't have to do anything, you just have to go and plan your journey and push a button and it will take you there with all auto pilots and so on and so forth and so on.

So depending on the context, you may actually have to learn more to fly a glider than to learn than to fly an airplane. So if the number of attributes or the different kinds of skills I need to fly this, you see that the opposite inheritance tree is valid that is an airplane that is a glider is an airplane that is an airplane requires smaller number of skill sets to fly while a glider requires a larger number of skill sets to fly therefore a glider is a subclass of airplane.

So as you can see that it is not such a straight forward thing to identify is-a relationship. So it depends on the application context and we should not ignore the application context like we saw in the previous session, a database cannot ignore the information system context within which it is going to be run. Is the application context about building an airplane or a glider or is a application context about flying an airplane or a glider. Now that much change the inheritance semantics in our conceptual schema.












(Refer Slide Time: 00:24:57)

 

So coming back to specialization and generalization processes, let us take a small generalization example and see how we go about it. Let us say we have identified two entity types in our, back to our company database. So let us say we have identified an entity type called secretary and the secretary is identified by a pan and salary and the kinds of skills the secretary has typing short hand or whatever so on and so far. 

Similarly we have identified manager and we see that manager also has a pan number. A manager also has a salary and there is an experience field saying what kind of experience the manager has. Now when we see two or more entity types sharing the same kind of attributes for a large extent that is out of three attributes two are similar here. It gives us reason to believe that probably these two are special cases of the same general class. So we can generalize them something like that.
















(Refer Slide Time: 00:25:59)

 

So we can create a more general class let us say called employee and then say secretary is a employee and manager is a employee. So note that the attributes that where first a part of secretary and manager have gone here that is only the common attributes between secretary and manager have moved up the hierarchy to go to the employee class and all those attributes which are specific to this specialized classes remain in the specialized classes that is skills remain here and experience remains here. Now in some cases it maybe able to, we maybe able to identify precisely how to distinguish one special class to specialized class to another specialized class, have a look at this slide here. 

(Refer Slide Time: 00:26:41)

 

This slide show an entity type called employee which is defined by attributes called pan and salary and one more attribute called job type which is not actually shown here. Now there are two specialized classes secretary and professor. Now suppose we identify a property that every professor has a job type as academy and every secretary has a job type called admin. 

So each professor belongs to a category of academic jobs and each secretary belongs to a category of administrative jobs. So we know exactly how entities of one specialized classes can be distinguished form entities of another specialized classes. So this is how we identify this here, we say job type and then we say admin is secretary and academic is professor. So such kinds of definitions are what are called as predicate-defined subclass. These subclasses are defined by the values of one or more predicates that exist in the ER schema. 

Next we go to an example where we see that in some cases not all subclasses maybe unique. Now let us take back the example of secretary and professor. You see here that while denoting this subclasses, we have drawn a circle with a small D here. Now what is this D denote? This D denotes the fact that these two subclasses are disjoined. What is meant by disjoint here? That is they are mutually exclusive, no secretary is a professor and no professor is a secretary because all secretaries have to have a job type as admin and all professors have to have a job type of academic. So the set of all secretary is a disjoined set from the set of all professors.

(Refer Slide Time: 00:28:59)

 

But this need not always be the case, sometimes two or more specialized classes or specialized entity types may actually overlap they need not be mutually exclusive from one another. For example suppose in some university there are notions of chair professors, chair professors are usually supported from external sources of, external funding sources but for all practical purposes they work as any other professors here. 
Now it could well be the case that some professors are chair professors and some chair professors are normal professors that is they need not be supported by a project but they also work in other activities and so on. So such kinds of inheritance trees or subclasses are set to be overlapping subclasses. So these two subclasses need not be disjoined from one another and this overlap, this kind of overlapping maybe either partial or even total overlapping. 

Now you might have, you might notice that every chair professor is a professor but not the other way around and so on. So it’s a total overlap as far as chair professor is concerned. On the other hand if there are some chair professors who are not teaching, let us say who are not doing the normal activities of a professor here then the overlap is partial overlap. 

(Refer Slide Time: 00:30:33)

 

The next kind of generalization technique that we are going to do see is what is called as a union type or this is also called as a category. Now have a look at this slide a little more carefully. Now this slide shows entity type called account holder in a banking scenario. Now when you ask a banker who or what is an account holder, he will probably tell you that the account holder is just an abstraction, it is an entity, it does not necessarily represent a person because it may actually represent an institution. An institution maybe an account holder or an individual maybe an account holder or sometimes in some cases accounts maybe held by families or sometimes dynasties and so on. 

So as far as the bank is concern, all of them are just account holders and they are just abstractions. But all of them share, all of them have their own set of attributes and have different sets of characteristics obviously an individual is different from an institution. Institution has characteristics like number of employees and so on which an individual which may not make sense for an individual entity type. So each of these entities here in the top most in the top run here may have their own sets of attributes which may not be in common with one another but all of them are account holders here. 

So each individual may have its own different key for example a pan number for an institution and address for a family or some kind of registration number for an institution but all of them are account holders as far as a bank is concerned. So such a kind of relationship is what is called as a union type. If you are familiar with programming, in C programming you have this notion of unions which has very similar co-notations. That is an account holder is either an individual or a family or an institution, a union type is also called a category. 

(Refer Slide Time: 00:32:51)

 

And just like we saw in the case of inheritance where in the case of subclassing where a subclassing, subclass could be either disjoined or overlapping we can have what is called as a partial union or a complete union. Now for example here, not every individual that exist in the database could be an account holder. An individual is an entity and an entity is something which has its own independent existence. So we may be keep in track of individuals for our own purposes but some individuals in the database could be account holders.

Similarly we maybe keep in track of institutions for some other purposes but some institutions in our database could be account holders. So when only a part of the entity set of individuals form or participate in this relationship, we call this as a partial union. On the other hand if every individual that we hold in our database is an account holder or participates in this union relationship then it’s a full union relationship. 




(Refer Slide Time: 00:34:04)

 

So that was briefly about generalizations and specializations and in fact this is a very crucial concept in being able to obtain the notion of abstraction that is to be able to abstract away unnecessary details from a special class and go to the general class. So if you are able say that we are going to use an entity type of a general class, it means that it has just enough details that is necessary for this relationship to exist. 

That means if I say that I need a car for this particular activity, I don't need to worry about what kind of a car is that, what color of the car is that or what is the horse power of that car or whatever. It’s all this attributes are specific to particular kinds of cars but for this particular activity any car would do. So we are essentially abstracting a way or covering up all the unnecessary details and looking at only the necessary detail what is required for a relationship to exist. 

The next concept that we are going to be looking at here is the concept of higher order relationships. Until now we have been considering relationships with a degree of two. Recall that the degree of a relationship is the number of entity types that participate in this relationship. Now this slide shown here shows a relationship with a degree three. Have a look at this relationship carefully. It says that the relationship is called supplies and it relates three different entity types the supplier, part and project. 

What is the relationship say or what is a semantics of this relationship again? There are three different entity types supplier, project and part. So basically it means that the supplier supplies this particular part for this particular project. Now if you think carefully it is not possible to reduce this ternary relationship or a relationship of degree 3 to any number of relationships of degree 2. A supplier may supply some parts but not all parts maybe designated for this particular project. 

A supplier may supply for a project but you may not supply all parts that are required for the project. Now a project may use a certain parts but not all parts that are used by a project or may be supplied by just one supplier, there could be any number suppliers. So we cannot reduce it to three binary relationships without losing meaning. Let us try to do that and see what happens.

Now the closest possible binary relationship that tries to simulate this ternary relationship is something like this. A supplier supplies to a project, a supplier stocks some parts and a part is required by a project or a project requires certain kinds of parts. 

(Refer Slide Time: 00:37:57)

 

And to be fair, to be sure we also note that a part is a weak entity type, it has no existence by itself it has to be either associated with a supplier or with a project. So even when you do that this is probably the closest we can come to simulating the entity relationship but not quiet close. As you can see it can still, it just because suppler stocks certain parts doesn’t mean that this part will be required by this project or vice versa.  

Let us take another example of higher-order relationships and see whether we can reduce it to lower order relationships without losing meaning. So this slide here shows a relationship which is a ternary relationship called offers. So it says that instructor offers a course during a semester. Now there are also other relationships that we have identified in the database and which says that instructor taught_during certain semester or instructor can_teach a particular course or a courses offered_in a particular semester. 

Now note that if I have an instance of this relationship that is offers I s c. What is i s c means? If there is an instance of this relationship called offers for a particular instructor in a particular semester for a particular course, this implies that the instructor has taught_during this semester and the instructor can_teach this course and the courses offered_in this semester.
(Refer Slide Time: 00:40:26)

 

That is taught_during i s and can_teach i c and offered_in c s that is an existence of this ternary relationship implies the existence of all this binary relationships. However the converse need not be true, the converse that is suppose I have instructor can_teach, instructor I can_teach course c and instructor I taught_during semester s and course c was offered during semester s but that doesn’t mean that the same instructor has offered this course during the semester is. 

Instructor I can_teach this course but and the courses offered during a semester and the instructor taught_during that semester but that still doesn’t say that the instructor taught the same course during the semester, he could have taught some other course and this course could have been taught by somebody else. So while this is true the converse is not true, reducing if instances of binary ration relationships exist, we cannot be sure that the instance of a ternary relationship also exists. 

Cardinality constraints on higher higher-order relationships. So what is it mean when we say when we put cardinality constraints on higher-order relationship? Here is an example this examples shows again the instructor semester course example, so it has put a 1 here and a N here and a N here. 










(Refer Slide Time: 40:50)

 

So that means that at given course-semester combination should have only one instructor that is in a particular course for a particular semester there has to be only one instructor. On the other hand a given instructor may have any number of course-semester relationships that is given instructor can teach in any number of semesters and any number of courses. So, again if you think about this carefully, if I have a set of all this relationship types instructor course-semester and so on. 

How do I identify an instance of this relationship type uniquely? The key here is the course-semester pair. So if I take an instructor, an instructor may offer any number of courses in any number of semesters. However if I take particular semester and a course you see that it can uniquely identify an instructor that is because every course and semester pair should have just one instructor associated with it. 

The last concept that we are going to look at in this session is the notion of aggregation. This concept is usually used in what is called as knowledge management or KM in the concept of ontology’s and so on.
 











(Refer Slide Time: 00:41:51)

 

So an aggregation basically aggregates a particular ER schema and makes into an entity at a higher level of abstraction. Note the certain difference or and very important difference between the kind of abstraction introduced by aggregation and the kind of abstraction introduced by inheritance or specialization. Aggregation brings about the concept of composition or contains relationships.  

So here this slide show an aggregated entity called offering which contains one or more instances of the relationship called instructor, course and semester. So an instructor offering a particular course in a particular semester is called a course offering or an entity type called offering. So the relationship between the offering entity type and this relationship called offers is that of contains offering contains offers. 

On the other hand the relationship between generalized and specialized classes is that of is-a relationship or rather between specialized and generalized classes. A car is-a vehicle and a bus is-a vehicle or monkey is-a primate and so on but aggregation offers the concept of containment this contains this contains this and so on. 

So even aggregation brings about a kind of abstraction that is you are covering up unnecessary details, if I am not really required to know what is the structure this offering. I don't need to really worry about that. So this slide here shows the relationship between offering and offering that is one course offering requires another course offering. So let us say course number A requires or has a prerequisite that some other course lets it said has to be taken up by the student. 

So a course offering of A requires a course offering of Z, so without Z being in the database I cannot have a course offering of A. So here the abstraction basically throws away all details which that talks about what exactly an offering is about. 

(Refer Slide Time: 00:44:44)

 

So with that we have covered the major parts in the enhanced ER notation or EER notation. So before we conclude the session let us take up a small example of a  university database and see how or in which kinds of situations do we get these inheritance and generalization and specialization. How do we go about identifying that we might probably generalize here or even probably specialize here and so on. Now take up a small university database. Now this example here is by no means exhaustive I mean we cannot build a complete database in the course of a session like this but we just trying to see what kinds of typical problems or typical kinds of issues that can arise here.
 
(Refer Slide Time: 00:45:38)

 

So some basic entity types, so each universities has a student and of course several other entity types. Again I am abstracting away unnecessarily details that is students, faculty members and staff and so on and so on. So let us say we have identified a basic as entity type called student.  

Now we then go about adding some attributes, for students. We note that each student is given a roll number which uniquely identifies the students as long as the student is in the university. Each student has a name, each student has a gender, a date of birth, address and so on and then we go about looking at other entity types. 

Let us say we say that faculty is an entity type and then we talk about what are the attributes that characterizes a faculty member. Then we come out with some more attributes like this let’s say each faculty has an employee number, each faculty has a name and a gender and date of birth and address and so on.

(Refer Slide Time: 00:45:57)

 














(Refer Slide Time: 00:46:16)

 

Now if you see student and faculty they look quiet similar all ready. Then we identify let us say some kind of non teaching staff and then we see that even they have the same kinds of attributes that is employee number, name, gender, date of birth, address and so on.

(Refer Slide Time: 00:46:51)

 

So which tells is that we are actually looking at different entities of the same generalized class. So what kinds of generalization can we make out of these three different classes? If you see staff and faculty, there is hardly any difference between the two entity types but between faculty, staff and student there are certain differences. So how do we identify these differences here? This brings us to a generalization and specialization tree. We see that faculty and staff can both be categorized as employees and they have the same key called employee number.

On the other hand the same, the key called employee number cannot identify a student, a student is given a roll number. So employee and student do not belong to the same level as a faculty but they belong to the same entity type called person. Now what is the property of this person entity type everything else that was common between the three entity types. 

(Refer Slide Time: 00:47:41)

 

What are the common attributes between the three entity types name, gender, date of birth, address? So the attributes that going to person would be all of these attributes name, gender, date of birth, address and so on which all of them share whether it’s a student or a faculty or staff all of them share. Similarly we start looking at certain association let us say a faculty works in department and a faculty heads department and we identify certain kinds of association constraints that says that n number of faculty member may work in a department while only one faculty member may head a department. And we also identify some more associations which says that n number of students maybe registered in a particular department. And we can also find some aggregations which says that a project involves particular department or project is headed by a faculty member and a faculty member belongs to a particular department and we see that this whole thing can be aggregated into an entity type called sponsored project.

So a sponsored project means that there has to be a project entity which is involving a particular department and is headed by a particular faculty member and so on. So how well schema is aggregated into the sponsored project entity type?
 

(Refer Slide Time: 00:48:40)

 

(Refer Slide Time: 00:49:02)

 

We can also see a certain higher-order relationships. For example let us say some foundation, some organization or non-governmental organization or whatever supports a particular project and a particular department. So again we see here that foundation supports department on this project and we can see that we cannot reduce it to binary relationships, foundations may support sponsored project and may support department but the ternary relationship says that for this project and for this department this foundation is supporting. 

  
(Refer Slide Time: 00:49:02)

 

(Refer Slide Time: 00:49:53)

 

So have a look at this higher-order relation relationship here. Let us say I have a relationship that says a faculty member collaborates with some other faculty member on a particular project. So note the double use of this faculty entity type that is this faculty member collaborates with this faculty member on a particular project.  





(Refer Slide Time: 00:50:30)

 

And so one faculty member probably, the head of the project may collaborate with n other faculty members on n other on n different projects. 

 

Now you can reduce it like this that is a faculty member collaborates with other faculty members and the same collaboration extends to project.
 





(Refer Slide Time: 00:51:09)

 

However you can't reduce it like this that is a faculty member collaborates with another faculty member and works on a project because we are going to lose semantics.
 
(Refer Slide Time: 00:51:27)

 

So that brings us to the end of the second session of enhanced entity relationship concepts. So before we conclude let us briefly go through the different concepts that we learn today. The first concept that we learned was about generalization and specialization where you achieve abstraction using an is-a relationship. So in a generalization and specialization relationship, for any entity of the general class can be replaced by any entity of the special class or the specialization without losing semantics only then will you be able to say that my generalization is correct. It need not always be correct just because something looks like is-a would hold doesn’t mean that the generalization is correct. 

So we saw the notion of inheritance that is each specialized class or a subclass inherits all attributes including key attributes and constraints and relationships from the generalized class and we also saw the notion of overlapping subclasses and disjoined subclasses and how to build a entity type using union types or categories and we saw the notion of higher-order of relationships and how they cannot be reduced to lower order relationship without losing semantics. And the final concept that we saw today was the notion of aggregation which is again a kind of abstraction relationship but however which establishes the notion of containment rather than is-a that is abstracted by the specialization relationship. So that brings us to the end of this second session on enhanced entity relationship concepts. 

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 3

Relational Model

Hello everyone, we have been looking into the process of database design and let us continue with this in this session as well. As we saw in the previous sessions, a database design goes through several different phases and we have been mainly looking into the conceptual design of a database. A conceptual design essentially means a high-level design of the database or the database system which is mainly meant for targeting the end users that is trying to explain your database model to the end users. Today we are going to look at another model of data which is called the relational model.

(Refer Slide Time: 00:02:04)

 

And how do we place relational model with respect to the entity relationship model that we have been considering until now. In order to answer this question, let us revisit our typical database design process that we saw in one of the previous sessions.
 








(Refer Slide Time: 00:02:10)

 

As we had seen that a database design process is contained with in a universe of discourse that is a universe essentially is the information system context within which a database is designed whether it is a bank or whether it is railway reservation, whether it is even your mobile phones. In many of these different application context databases are usually embedded. So it is this application context that makes up the universe of discourse. 

Now once we analyze the universe of discourse, we essentially get two kinds of requirements one was what was called as the database requirements as shown in the slide here and the other is what is called as the functional requirements. So the database requirements essentially meant, what are the data elements that makeup the system and how are they interrelated and how should we make sense out of the data elements and functionality requirement or functional requirements or the application programming requirements which say what kinds of processes have to run on this databases and what are the semantics of these processing’s. 

So these requirements in turn gave rise to two kinds of parallel processes. The database requirements gave rise to the conceptual design of the database and from the conceptual design came the conceptual schema and we saw in the previous class that the conceptual schema is usually build using the ER model that is an entity relationship diagram.






 

(Refer Slide Time: 00:03:24)

 

Now let us follow this upper stream that you see in the slide here a little bit further and see what happens to the conceptual schema. 

(Refer Slide Time: 00:3:57)

 

Now the conceptual schema which is typically meant for are essentially meant for communication with the end user is in turn going to give rise to the physical schema. What is a physical schema? The physical schema is essentially the schema that is actually build on the database system on the computer and as you might have imagined, while the conceptual schema is oriented towards human understanding that is communicating your schema or communicating your design with the end user, the physical schema is oriented towards machine understanding or essentially efficiency in terms of storage and retrieval of data elements. So the physical schema is optimized towards quick updates, quick inserts, easy searches and so on. 

So it is one of this physical schema or the building blocks of such a physical schema is what we are going to see today and the model that we are going to see today the relational data model is the most widely used data model in most databases today whether it is any kinds of application context, whether it is banks or railways or telephone exchanges or whatever. Any of these application context typically used relational model to store data as part of the internal schema structure.

(Refer Slide Time: 00:05:22)

 

So what is the background of the relational data model? The relational data model was introduced in the 1970’s, the early 70’s by Ted Codd from IBM research and in fact there is you can do a web search for Ted Codd today and many of his seminar papers that where the database or the relational model was proposed are actually available over the internet and you can actually have a look at them to see why it was so influential.

Before the relational model was proposed there where another models like hierarchical and network model and so on which were not very amenable to internal storage which are not very efficient in terms of storage and retrieval complexity and so on. And the relational model was extremely elegant in terms of storage and updates and retrieval searches and all these. The main concept behind the relational model is the notion of a mathematical relation. You might have studied in course on discrete mathematics and mathematical relation is just a mapping between two or more sets, so where each set constitutes a domain and a mapping between each of these domains forms a mathematical relation. In intuitive terms a mathematical relation is no different from what we understand as a relationship in normal English.

A relationship is similar, is essentially some kind of an association or some kind of linkage between two or more different data elements. An employee is associated with a department, an employee has a name associated with him, an employee has a id associated with him, an employee has a salary that’s associated with him and so on and so forth. So it’s this mathematical relation let us say the set of all employees versus a set of all salaries, you can establish a mathematical relation between these two sets. It is this set that forms the underlying basis for the relational model. So this is the standard database model for most transactional databases today.  

So let us go step by step into the nuances of the relational model. So what exactly are the building blocks or the essential concepts that makeup the relation? So what is a relation in turn? A relation intuitively represents a table as you can see in the slide here or it’s also called a Flatfile of records. This slide shows here a small table which has three different columns. The first column is named as roll number, the second column is named as name and the last column is named as date of registration and there are several different rows and each row corresponds as you might have guessed by now each row corresponds to one particular record or one particular student in this case.
  
(Refer Slide Time: 00:07:29)

 

So the first row has a student by name Adithya and his roll number is 2003-01 and there is a particular date of registration. Now this is one data element, one set of data elements that are interrelated, so one schematic data element. Now, this row is independent of the second row which talks about another student called Ananth Kumar and with his own roll number and with his own date of registration and this in turn is an independent data element that is independent of both the first row and the third row.

So each row in a table represents, a collection of such related data values are what is called as an instance of the relation. The relation in this case is or the schema in this case is a table comprising of three different columns roll number, name and registration. An instance of this relation is one of these rows, one which says 2003-01 and name as Adithya and a date of registration as 12- 8- 2003. So each row in a relation is called a tuple and each column is called an attribute of the relation.

(Refer Slide Time: 00:09:32) 

 

So let us dwell little bit deeper into the relational model and before we do that we need to define some certain crucial elements in the relational model. Now the first definition we are going to comeback to it again, the first definition in the slide talks about the notion of an atomic data type. Now have a look at the slide once again, a data type is said to be atomic if it cannot be subdivided into further values. 

Remember in one of the previous sessions we talked about attributes in an ER schema being either a simple attribute or a composite attribute. A composite attribute is a non atomic attribute that is for example name if the name of a person in turn comprises of other attributes like first name, middle name, last name, title, initials and so on. This is not an atomic attribute. 

On the other hand the age of a person is an atomic attribute because you cannot sub divide this attribute into further sub attributes. Now this is important because a relation is defined only over atomic attributes as we will see in the next slide and the next definition that we are going to consider is the notion of a domain. We have already seen the notion of a domain in the ER model and even here the notion of a domain is no different. A domain is basically a set of atomic values which defines the space within which an attribute might obtain a value for itself. 

For example we had seen the examples of the age of an employee which may not be lesser than 18 years and greater than 65 years. So the age of an employee is a set of all numbers maybe even fractions between 18 and 65. So this constitutes the domain for this attribute called age. Similarly there are domains for names and dates and so on and so far. This slide actually shows some examples like the set of all integers, the set of all valid student roll numbers, the set of all Indian cities with population above 6 million or whatever. Anything that defines a space of possible values is called a domain. 

A relation schema or a relational schema, this is a crucial definition. The relational schema is as you can see in the slide has a very specific notation. A relational schema is defined by the name of the relation. Here it is shown as R and a list of attributes, here its shown as A1 A2 extra to until An. So this is the name of the relation R, in the previous example the name of the relation was called student record and the attributes are roll number, name and date of registration. So each Ai shown in this definition, here is the name of certain some attribute or as shown in the slide here is the name of the role played by some domain which is an other way of putting the same thing, name of an attribute or the name of a role played by a particular domain. 

Now for example in the previous case their roll number, the domain of roll numbers was the set of all possible valid student roll numbers. Now what is the role that this domain plays in this relation is the attribute which is the part of the relational schema. So few more definitions. Just like we had that degree of relationship in the ER model, we define the degree of a relation in a pretty analogies fashion, it is simply the number of attributes in the relation schema.
 
(Refer Slide Time: 00:13:08)

 

For example in the previous schema that we took, the degree of the relation was three, it had three attributes roll number, name and date of registration. So a relation R or what is even called a set of instances of relational schema, a relation this is different from a relational schema. The relation of a relational schema is a set of tuples which belong to the schema or which conforms to the schema along with the schema itself. So a set of tuples of the form t1 t2 extra tm were each tuple is of the form of n different attributes or n different values one for each of the attribute. 
So again in the example that we saw earlier, it had three different tuples. There were three different student records with three different roll numbers. So the entire set that is the schema plus the set of all tuples is called a relation. So a relation as you can say, it can also be defined as a subset of a cross product of all this domains. So the cross product of the set of all roll numbers times the set of all student names times the set of all registration dates that are possible. Now a subset of this cross product is what is going to form a relation. Now what is some of the characteristics of a relation in the relational model? Ordering of tuples, that is one of the first issue that we are going to look at. This slide shows here three different characteristics of tuples.   

(Refer Slide Time: 00:15:02)

 

Ordering of tuples: Mathematically the relational model does not have any ordering that is specified over the tuples. Now it does not matter as far as the mathematical model of the relational schema is concerned whether the set of all tuples are ordered roll number wise or name wise or whatever. It is just a set of tuples that are in conformance with the relational schema. So but in reality of course they do have some kind of an order which is specifically the order in which they are stored on the disk whether they are stored in sorted order or not, it doesn’t matter but they have some kind of an order in reality. 

Ordering of attributes: Note that a tuple, you might have studied in course on discrete mathematics a list is something where the order is important. A list is different from a set. So tuple is basically a list of n different elements which means that the order of these n different elements is actually important. So if the relational schema says that my relational scheme is called student record and I have these attributes roll number, name and date of registration and if I have a tuple having three different values the first value corresponds to roll number, the second value corresponds to name and the third value corresponds to the date of registration. 

So the ordering is important but that is mathematically speaking. In fact, in reality though we can do away with ordering within a tuple as well. I can as well dereference a particular attributes by its name, I can as well say what is the roll number value of this relation, what is the name value of this relation, what is the date of registration value of this relation and so on. And values of tuples which is the third characteristic that we are going to look at today. So like I mentioned before each tuple or each value that makes up a tuple is assumed to be atomic in nature and this is what is called as a first normal form assumption.  

In fact we are going to see in one of later sessions that the first normal form is just the first step in a series of different normal forms in which the database can be optimized for enhanced maintenance of the data. What do we mean by enhance maintenance? Easy addition of data elements, easy searching for data elements, easy updation of data elements and so on. And this is primarily the reason why it is stipulated that or why it is required that each data value that makes up a relation are to be atomic in nature and atomic as we had seen earlier is something that’s not composite that is some value that cannot be subdivided into further semantic values. So just like entity relationship model, a relational model also are specified by certain kinds of constraints on the data model. 

One of the first constraints that we are going to look at is pretty obvious which is what is called as the domain constraints. What is the domain constraints say? Each value of an attribute within a relation has to have a value which lies within the domain which is pretty obvious. So the roll number of a particular student has to be a valid roll number that is it has to belong to the set of all possible roll numbers If the set of all possible roll numbers range from 1 to 150, I obviously cannot have a roll number which is 200 or I obviously cannot have a roll number which says A B C and so on. 

So the domain constraint specification basically states that each data value that makes up a relation has to be or has to belong to the domain in which it is the domain of the attribute in which it utilizes.  
















(Refer Slide Time: 00:18:16)

 

The second constraint in the relational model is what is called as the key constraint and this is quiet similar to the key constraint that you saw in the ER model as well. In any relation there could be a subset of attributes that have a property that for every tuple in which those attributes appear, they have a unique value for those tuples. Such kinds of attributes are called super keys. Now what is the use of a superkey? Obviously to be able to uniquely identify every tuple in a relation.    

In the previous example where we took that a relation having three different attributes roll number, name and date of joining you can see that roll number forms a superkey because roll number is an attribute whose value is unique for every tuple in the relation. However you can also combine roll numbers along with the names and you can see that it’s always going to be unique. If the roll number is unique roll number plus name is obviously going to be unique. So roll number plus name or in fact the entire record can be called the super key for each record which brings us to another property of the relational model. 

In the relational model each tuple is distinct that means in the worst case the entire tuple is the superkey. The relation does not allow multi multiple tuples having the same value in the sense that it is a set of tuples and not a multi set of tuples or what is typically called a bag of tuples. So each tuple are to be different in value from the other. So, a key is a set of tuples in which is defined in a similar fashion as in the ER model a key of a relation is a subset of the superkey such that no subset of a key is a key in itself. 

If you remember this was more or less very similar to the way we defined a key in the ER model itself. So roll number in the case of the student record is a key and if you take away the roll number, you cannot identify or you cannot distinguish one tuple from the other. 

So however roll number plus name is not a key in itself. Why? Because you can still take out the name attribute from the key and you can still uniquely identify each tuple using just the roll number because the roll number is sufficient to identify each tuple uniquely in the relation. So there are also called minimal superkeys.

(Refer Slide Time: 00:22:10)

 

So this slide shows the talks about what we saw just now that in the student table roll number and name is a super key. However it is not a key or it’s not a minimal super key because you can still take away name and still be left out with the key that is you can still identify each tuple uniquely in the relation or in the table. So, just roll number is a minimal or a minimal super key. Now, just like we had seen in the example earlier that a house can have more that one keys, the key for the front door and key for the back door. Any relational scheme or any relation can have more than one key or so on. 

Take an employee record and usually employees are given employee identification number and they also have a pan number which is given by the government. Now using either of these two, you can identify an employee uniquely because each of them are unique for an employee. So each such key or each such set of subset of attributes which can uniquely identify tuples in a relation is called a candidate key. So either the employee number or the pan number is a candidate key in itself but usually one of those candidate keys are used for identifying tuples in a relation.

In a company contacts its usually the employee number, we do not usually identify people with that pan number when you are talking about their performance records or salary statements or anything of that sort, we usually talk about their employee number. Now whichever candidate key is used for the purpose of retrieval in quires and insertions and so on is called the primary key of a relation. So in the student relation that we saw earlier usually it’s the roll number is what we use for students, so the roll number is a good primary key.
Now just like their entity constraints as part of the ER model, there are certain constraints that make up the relational model as well. The first constraint is what is called as the entity integrity constraint. So what is the entity integrity constraint? It essentially says that whichever entity the, now look at what we mean by an entity here. It’s a slight change of nomenclature what we mean by an entity, here is a tuple in the relation.
 
(Refer Slide Time: 00:24:14)

 

So the primary key of a tuple can never be null obviously because we won't be able to identify each tuple uniquely. The second integrity constraint that’s important in the relational model is what is called as a referential integrity constraint. Now what is a referential integrity constraint mean? Sometimes some set of attributes in a relation may point to certain other tuples in another relation. We had seen such an example in one of the previous sessions as well, when we said that a department is headed by an employee who is the manager or the head of the department. So that the headed by is usually contains an employee id of the person who is going to head this department. 

So here what we are and ensure that we are going to get here is what is called as the referential integrity. Here there is a reference from the department entity to the employee entity. Now this referential integrity constraint basically says that whenever I make a reference from any tuple to any other tuple, it should make a reference to an existing tuple. That means I can appoint somebody, I can appoint some employee with some employee id as manager of the department as long as or only as long as such an employee already exist in the database. So such a reference is what is called as a foreign key. 

So in any given relation set of attributes is set to be a foreign key if the following rules hold. The first rule basically says that the attribute in foreign key has to be the same as the primary key of the other relation, this is fairly obvious. If I am going to say that the department is headed by an employee and I write an employee id here but use employee name as the name of the person who heads, it obviously is not a foreign key. It has to be either both the primary key in the employee record and the reference in the department record has to be names or they both have to be employee id’s in some sense. And for every tuple in the referencing attribute or in the referencing relation like department, the attributes in its foreign key refer to existing tuples that is each department should have a manager who exists, who already exists in the database or there should be null that means there should not have a manager at all. So either of these two should hold for the referential integrity constraint to hold.

(Refer Slide Time: 00:27:36)

 

So referential integrity is usually depicted in a diagrammatic fashion as shown in this slide here. This slide shows two different schemes or relational schemes, one called the employee schema and the second called the department schema. So the employee schema has employee id, name, works_in and reports_to. So employee id would be the primary key here and the works_in actually is a foreign key which refers to the department id and reports_to is another foreign key which refers to another employee id within the same relation. 

So note that foreign keys can be from a relationship to itself but the same referential integrity constraints hold. That is if an employee A is reporting to employee B, employee B should already exist in the database by the time employee A is being added to the database.
 







(Refer Slide Time: 00:28:35)

 

And the last kind of constraint over a relational model is what is called as the semantic integrity constraint. An semantic integrity constraints is usually more of an application specific constraints something like the age of an employee cannot be less than 18 years and greater than 65 years. So this is not per say part of the relational model but usually this is important to be implemented within a database context and automatically checked and verified that these integrity constraints are maintained. Now let us quickly look at what are some of the basic relational algebra operations that make up the relational model. Now essentially the operations of relational algebra can be categorized into one of two different kinds of operations namely retrieval of data or updates to the database. 

(Refer Slide Time: 00:29:07)

 
Now updates to the database are usually handled by what are called as insert and delete operations. We should not be looking into insert and delete operations in any detail in this session mainly because they don't have many, very many properties that we can explore at this moment. So right now we will be looking mainly at the retrieval operations and retrieval operations are handled by two basic operations called select. Select is denoted by a sigma as shown in the slide here and the project operation which is denoted by a pi which is also shown in the slide here. 

So let us go to the select operation. The select operation is a very simple operation that is used to select a set of tuples from an existing relation. So remember a relation is basically a set of different data tuples along with the schema. Now given this set of tuples and schema, we can use the select operation to select a subset of those tuples. Now this slide here shows an example which says select salary greater than 3000 from employee. So as it shows here sigma salary greater than 3000 as a subscript and employee has the parameter this operation.

(Refer Slide Time: 00:30:11)

 

So as you can see there is a operation here, there is a condition and there is a domain or there is a relation over which the operation is going to be performed. So this is going to select the set of all records from the employee relation where the value of the attributes salary is greater than 3000. 








(Refer Slide Time: 00:31:15)

 

So the general form of select is shown in this slide, its simply as like this select condition relation. So where condition is a conditional expression, I have written a slightly formal grammar of how a condition looks like. Essentially condition is a logical expression over attributes names, something like select salary greater than 3000 and gender equal to male from employee. So which basically says give me all male employees in this relation whose salary is greater than 3000 and so on.
 
(Refer Slide Time: 00:31:48)

 

Now what is some of the properties of the select operation? The first property which you might have noticed here is that the select operation is unary in nature. What is a unary operator? A unary operator is something which operates on just one operand. The select operator operates on just one relation even if my database has many different relations, the select operator operates on just one relation and we have to some how make sure that when we are giving the select operator we have just one relation as the argument of this select relation. And, each selection criteria the condition basically that’s specified is applied to each tuple separately. 

The condition that we specified here was select salary greater than 3000 from employee. Now it’s going to apply this condition separately to each tuple. Basically it also means again that each tuple in a relation is independent of the other and the degree of the relation that emerges out of a select operation. Note that the output of a select operation is a relation in itself because it has just taken a subset of the tuples from the given relation and return them along with the schema. 

So the input to the select operator is a relation and the output is also a relation and the degree of the output relation is the same as the degree of the input relation. That is if the employee table in this example here had 4 different attributes, the output of the select operator also has 4 different attributes and in the same order as well. However the number of tuples returned by a select operator is bounded by the number tuples that already exist in the relation.

That is this slide shows this as in a very compact fashion, the cardinality of the select output relation is less than or equal to the cardinality of the select input relation and the last properties shows that select is commutative which is again quiet interesting and important. This slide shows here that if I select based on condition c1, an output of a select based on condition c2 for R, I can as well replace c1 by c2 and c2 by c1 and it doesn’t matter. So you can verify that for yourself that the select operator is commutative whether it doesn’t matter in which order I am going to apply the conditions. The second operator that we are going to be looking at is what is called as a project operator.

Now select operator if you have absorbed carefully is going to return entire tuples, it is not going to modify the schema of the relation which is given as an input. For example if the employee relation is given as an input and the employee relation has four attributes in some particular order, the same set of attributes in the same order is what is going to be returned by the select operator. 

On the other hand what if we can select over columns rather than select over rows and return a different relation with possibly a different scheme. In order to do that we are going to use the projector operator.







(Refer Slide Time: 00:34:33)

 

The project operator as shown in the slide here is quiet similar to the select operator in the sense that it has first the command called project which is denoted by pi and a list of attributes, here it shows name, salary from employee. So the output of this relation is again another relation. However with a different structure from that of employee that means it is going to return just the name and salary attributes or the name and salary columns of the employee table as part of its output. So we can also say that it has projected the employee relation on to the selected set of list of operation. 

So the general forms of the project operation is simply of the form project, attribute list and relation. So I can just give a list of attributes and the project operator returns relation in the same order that is being presented in the attribute list. Hence if I gave salary before name, it would also return salary before name in the tuple that is returned as part of the project operation. So what are the properties of the project operation? The first property is that which basically is one of the main properties of the relational model that is a relation may not have many duplicates.













(Refer Slide Time: 00:36:13)

 

(Refer Slide Time: 00:36:53)

 

That means when I am projecting let us say I am just projecting name and salary attributes from the employee relation, it may so happen that there maybe two employees with the same name and the same salary but with of course different employee identification or employee numbers. So the project operator actually would start forming duplicates in the relation that emerges out of this however duplicates are not alone. So the project operations remove duplicates from its results when it returns results. And the number of tuples returned by project is less than or equal to the number of tuples in the specified relation. How can you verify that the number of tuples is less than or equal to, why not equal to because I am just asking for certain columns in the relation. 
The answer to this lies in the first point that is there are no duplicates. So when I selected just the name and salary attributes from the employee table it may so happen that their maybe certain duplicates that exist in the output relation. Now because the duplicates are removed, the number of tuples in the output relation is actually less than the number of tuples that forms the database, that was in the relation in the first place. So when the attribute list of project includes the superkey then the number of tuples is same as the number of tuples as in the database which again follow from the first two points and the last point is again important, the project is not commutative. 

So have a look at the slide once more, it gives an example project l1 and project l2 out of R. If this were the case then this would become, this would be equivalent to just saying project l1 from R if and only if l1 is a substring of l2. So for example if l1 is just name and l2 is name, salary then I could just say project name from R instead of saying project name, salary from R. On the other hand if I try to do it the other way around then it becomes an incorrect expression. So I can't project name, salary after projecting just name from the relation 

Composition: Now this is another property of the relational model. If you notice again carefully, we have mentioned this point in passing when we looked at both select and project but this is going to be very important now. The input for the relational operator select as well as project is a relation and the output is also a relation. The select operators return a relation containing only a subset of the tuples of the input relation. Similarly the project operator returns a relation which contains only a subset of the attributes of the input relation however both select and project returns a relation. 

Now this brings us to a very important property that a relation can be dynamically defined, it need not actually statically exist in the database. That means to say that I can put a project operator to the output of a select operator and it would still make sense because the output is a relation and the project operator also expects a relation. So this is what is called as composability of the relational operators. So relational operators can be composed as shown in the slide here that which shows project name, salary as the outermost operator and there is an innermost operator called select. 














(Refer Slide Time: 00:41:01)

 

So if you can look at the slide again, there is a project operator here. The project operator is operating obviously on a relation. Now what is this relation? The relation doesn’t exist when the project operator is performed, in fact it exist only when the select operator is performed. That is once the select operator finishes, it basically brings out a relation which is the set of all tuples where the salary field is greater than 3000 from the employee record or the employee relation and this set of tuples which is dynamically created forms the input for the project operator which is going to be another relation. 

Now because this is going to be another relation, it can be very well assigned to a relation called salary statements for example here. So which says salary statement equal to project name, salary from where select salary greater than 3000 from the set of all employees. Now before we conclude today, we will just look at one major question which I am sure you would be asking yourself. Now the question is both project and select operators expect only one relation. Now does it mean to say that I cannot ask any queries that span more than one relation, should I ask every query over just the employee record, over just the department record I mean the department relation or employee relation or so on. Can I not ask any question that spans employee and department relations together and so on.
 










(Refer Slide Time: 00:42:56)

 

So the answer to this is the Cartesian join obviously. So essentially what we have to do here is because both select and project operators require just one relation, we have to some how ensure that even if you have more than one relation they all fall back or they all combine to form just one relation. So we are going to look at one such very rudimentary operator to make just one relation which is namely the Cartesian join. In fact there are other much more efficient ways of combining two or more relations which we are going to explore in the next session. 

So the Cartesian join as you might have imagined is very similar to the Cartesian product between two sets. What is a Cartesian product between two sets? You just take each element of one set and combine it with each other elements of the other set. So in the case of relations, you just take each tuple of one relation and combine it with every possible tuple of the other relation. So the slide here shows such an example. There are two tables here, one table is called student and the other table is called lab. Now student table has three different attributes roll number, name and lab. And lab table itself has three different attributes that is name faculty and department. That is the name of the lab, the faculty heading the lab and the department in which the lab belongs to. 

Now consider the relational query here shown below the tables, select student . lab equal to lab . name. So that means to say that lab attribute from the student table equal to name attribute from the lab table from a Cartesian product of student and lab that is combine student and lab to get it. So what is the output of this relation or how is this relation or how is this query evaluated? Let us first straight away compute the Cartesian join or the Cartesian product of the two relations student and lab. Now what I have done here is I have take the student relation that is the student relation had 4 different students. So for each student I have combined it with each possible lab tuple to form one big relation.


(Refer Slide Time: 00:45:16)

 

So here it says note how the attribute names are changed that is it becomes student dot roll number, student dot name, student dot lab, lab dot name, lab dot faculty and lab dot department. Now the query that we require to match was student dot lab equal to lab dot name. Now all these quires that match are shown here in pink. Now it is these tuples that are going to be returned. So the result of the query would be something like this. That is where the student working in a particular lab is same as the name of the lab for whose record that we are maintaining. 

So essentially what we have done here is that from two different attributes or two different relations student and lab, we have made just one relation by their Cartesian product and then given it as just any other input to a select operator. So it has become just one relation as seen in this slide here. The last slide that should be looking here today would be what exactly are the properties of this Cartesian join operator, just like we saw the properties of project and select.














(Refer Slide Time: 00:46:14)

 

(Refer Slide Time: 00:46:59)

 

Now the Cartesian join represents what might be termed as a canonical join between two relations that is it just joins every relation from the first tuple to every other relation in the second tuple. In the example here it just joined every student record with every lab record whether it made sense or not. 

It was if a human being read the two tables, he would have noted that many of these joints do not make sense that is even though it says that the student works in a particular lab that record is joined with some other lab which has no relationship with what the student is doing. 
So essentially if the number of tuples that come out of a Cartesian join is actually the product of the number of tuples that exist in each of the relations that make up the join. So hence Cartesian join as you might have imagined is actually two inefficient for joining tables, especially if you note that one table has 10000 records and the other table has 1 million records and the Cartesian product would be 1 million times 10000 records. And probably the output would be something which maybe 10 records or so which is not clearly worth it. So Cartesian join is mainly of theoretical interest in the sense that this is a canonical form of join operator by which we can join two or more relation to form just one relation because each operator requires just one relation as its input.  

So in the next session we are going to be looking at several other join operators especially what is called as the theta join operator and see how it is actually computed and how its going to be more efficient than the Cartesian join operator. We are also going to look at certain more, certain other relational algebra constructs and see how we can express the several different quires using the relational algebra.

So to summarize what we saw today the relational model is a data model for the internal schema of a database and the internal schema is something which is oriented towards optimized performance on a computer rather than human consumption that is something that is meant for human beings to see and understand and so on. 

And the relational algebra, an algebra is some kind of formalism over which we can build sound software that is something that’s based on a mathematical formulism can be used to build sound software. So comprises of operators that very elegantly take relations as input and produce relations as output and then you can start combining relations from one another and so on. And we also just started to see how we can combine relations so that we can give just one relation as an input that is required by each of the relational operators. 

In the next session we are going to be looking at certain more sophisticated forms of combining relations which are much more efficient both in terms of space and time required to compute these joins. So this brings us to the end of today's session. 


Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 4

Relational Model

Hello and greetings. In today's session we shall be exploring relational algebra into some more depth. We started with relational algebra in the previous session with the definition that relational algebra is a data model that is based around the mathematical concept of a relation. Let us briefly review what we have learned about relational algebra before going further on into some more concepts in the relational algebra. So just to revise what we have already seen, the relational algebra is based up upon the notion of a mathematical relation.

(Refer Slide Time: 00:01:48)

 

A mathematical relation as we saw before represents a mapping between two or more domains. Essentially one relation represents a mapping between two domains that relates each element of particular domain to some other element of another domain. So for example I might have the set of all student names being related to the set of all roll numbers and a mathematical relation is a subset of this mapping that is the set of all valid assignments between students and roll numbers. So this forms underpinnings or mathematical underpinnings behind which relational model is based upon and as we also saw yesterday, the relational model is mainly meant for the design of the internal schema of a database. An internal schema is meant or is meant or optimized towards machine consumption that is its optimized towards efficient storage, retrieval and queries of the data rather that human consumption that is trying to look at the data model and trying to understand what the data modal does or what the schema is all about. 
So the building block of a relational model as we saw yesterday and it’s also shown in the slide here is a set of relations that contains a set of attributes and each attributes belongs in a certain domain. For example the set of all students belongs to the domain of the set of all valid student names. Similarly the set of all age attributes belongs to the set of all valid age for a given employee or student or whatever. So we also saw the notion of a relational schema which describes how this relation looks like that is what are all the attributes that makes up these relation and what are the domains of the attributes that make up these relation.

An each relation is dereferenced by a relation name and every instance of a relation that is every data element that conforms to this relation is called a tuple and each tuple in a relation is independent of every other tuple in the relation. Hence for example if I have one record of a student compressing of the roll number, name, date of registration, date of birth and so on that constitutes a tuple in the relation called the student relation and each tuple that is each record about student is independent of the other records of other students. And a relation by itself is a combination of tuples plus schema that is given a particular schema and a set of tuples that conform to this particular schema is what is called as a relation.

As you saw in the previous class, a relation is the input and the output for most of the relational algebra expressions like select and project which we had seen. And what are the properties of relations? The first property is about the ordering or the lack of it that is tuples in a relational schema or in a relation need not have any order, they need not be placed in any particular order. There relation is just a set of tuples that conform to a particular schema and the second property was about duplicates. Traditionally or the pure relational model does not allow for duplicates of a tuple which means to say that the each tuple is unique when you take a tuple in its entirety.

So we also made a statement that by default the entire tuples forms the super key for a tuple that is using the contents of a tuple, you can identify each tuple uniquely in the relation. We are going to actually generalized on this concept today to see how things would change, if you can allow for duplicates of the tuples and whether it is required or whether it is desirable to allow for duplicates to exist in a relation. We also saw that the relational model is defined by certain kinds of constraints and one of the first of which is the key constraint. So we defined a super key as something which can identify a tuple uniquely and we also defined a minimal super key or a key which says that no subset of which is also a super key in itself. And we also saw the notion of a candidate key where two or more sets of minimal super keys can be used as keys and the notion of the primary key and a foreign key when it comes to referential integrity constraints.

We also looked at entity constraints which says that the primary key of a relation may never be null. So you cannot have a tuple in which the primary key is null because null is not a valid value for an attribute. It basically says that the attribute is not applicable that there is no value associated with null. And we also saw basic retrieval operators select and project operators represented by the Greek letter sigma and pi and select is an operator which selects a subset of tuples from a given relation without changing the schema of the relation that is the input relation and the output relation from a select operator have the same schema while a project operator selects in a sense, specific columns of the input relation that is it changes the schema of the relation without changing the data in the tuple. It does it or it may change the number of tuples, if we mandate the fact that the output of the project relation has to be a set that is it cannot be a bag or a multi-set that is each tuple from the output relation has to be unique. So, in which case the number of tuples that are returned would be less than the number of tuples that already exists in the relation.

We also saw that both select and project operators require specific relation that is exactly one relation has input and provide exactly one relation as output. Now whatever it do when we have more than one relations on which we have to answer a query. So we saw one possible solution to this that is to use the Cartesian join or the Cartesian product. Now how do we define the Cartesian product over relations? Remember what is meant by the Cartesian product over sets. If you have two sets A and B, a Cartesian product A times B or A cross B is the set of all mappings from all elements of A to all elements. This is the same definition for a Cartesian product for relations as well when we consider relations as simply set of tuples.

So a Cartesian join between two or more relations is the combination of set of all tuples from every relation to every other relation. So, a Cartesian join as we saw yesterday is unnecessarily expensive in the sense that if I have m tuples in the first relation and n tuples in the second relation it needs to first compute m times n or m n number of tuples to generate a table which in turn goes as input to the select and project operators and from where selection has to be made. Clearly this is very inefficient for most operations on involving two or more tables. So let us move on today to look at other forms of join operators which generate for lesser number of tuples than the canonical join that is represented by the Cartesian join. 

The first join operator that we are going to see today is what is called as theta join operator. This as shown in the slide here, a theta join operator shows a join symbol which has essentially something like Cartesian product symbol with two parallel lines and which has a subscript called theta which is again shown in the title here.













(Refer Slide Time: 00:09:26)

 

So a theta join combines two or more relations or combines the tuples of two or more relations in a way that is specified by a join condition and the join condition is specified by the operator theta. So the slide here shows a specific example where the same relation that we took yesterday the student and lab relation is computed using a theta join operator. That is the slide shows student in a join operations with the lab condition that is the student relation theta join lab such that student . lab equal to lab . name.

So student . lab equal to lab . name is the joint condition and the theta join operator is the operator that is going to combine student and lab relations. So this is the relation that is shown here and as you can see the output of this relation is the same as the select operator that we saw in the previous session. That is select student . lab equal to lab . name from student time’s lab that is the Cartesian join between student and lab. The only difference here is that the condition for this Cartesian join is specified as part of the join operator itself. Now how many tuples does this generate?














(Refer Slide Time: 00:11:20)

 

Let us have a look at the student and lab tables from the previous example. The student table has four different tuples and the lab table has three different tuples. Now the Cartesian join operator initially generated 4 times 3 that is 12 different tuples as input to the select operator.

(Refer Slide Time: 00:11:39)

 

On the other hand, the theta join operator starts with the condition that student . lab equal to lab . name is a prerequisite for computing the join between the two tables. So in this slide here such tuples where student . lab equal to lab . name is shown in a different color, are shown in pink. So, as you can see there are only 4 such tuples that match this condition. Hence the number of tuples that are generated as input to this select condition is just 4 instead of 12.

(Refer Slide Time: 00:12:13)

 

So how is the theta join computed? Well, the general answer is it depends. It would be note that in this case, it would be most efficient to compute this theta join operator. If lab dot name were to be a primary key of lab that is the lab is being referenced by the name and student . lab is a foreign key in to the lab relation and of course referential integrity is maintained. So because we have to compute the equality student . lab equal to lab . name, all I need to do is take up each student . lab attribute and search for the corresponding tuple in the lab relation because it is a foreign key and lab . name is a primary key this search can be uniquely done efficiently using several techniques which we are going to see later like indexing or hashing and then you compute the join between the two relations.   















(Refer Slide Time: 00:13:25)

 

So the general form of the theta join relation is shown in this table or is shown in this slide. It’s simply two relations with the join operator with a subscript join conditions, so the join condition is simply a logical expression over the attributes of R and S, which in the previous case we saw was the equality condition that is student . lab equal to lab . name. It need not necessarily be equality condition, it could actually be any other logical condition like less than or greater than or so on. Now it may so happen that in some cases the join attributes may be null which is also true even if there is referential integrity that is there may be a lab . name which is missing from a student record. 

Now in such cases those tuples do not appear in the result that is whenever I cannot combine two or more relations or whenever a particular tuple cannot be combined with a corresponding tuple from the other relation, such tuples do not appear in the final result. So some more properties of the theta join operator. We saw a join condition now which sets student . lab equal to lab . name. Now the condition here or the logical operator here is the equality condition or the equal to condition. 













(Refer Slide Time: 00:14:34)

 

Now theta join operators in which the only comparison operator that is used is equality condition is called an equijoin operator or an equijoin condition. So what we saw actually was an equijoin and theta join is more general in the sense that it could mean any other kind of attributes. Now a special kind of equijoin is of particular interest and this is what is called as natural join between two or more relations. So, natural join is denoted by star as shown in the slide here. It is an equijoin where some of the attribute names between the relations that are participating in this join are the same.

Now consider this example, consider again the student and lab example. Now let the student relation be modified like this that is the student relation has attributes name roll number, name and lab name instead of saying just lab. Similarly we have modified the lab relation as lab name faculty and department instead of just name. Now as you can see here, the lab name attribute or the name of this attribute is the same between the student and lab relations. 

Now if I say student star lab which denotes a natural join between student and lab relations, it returns me a relation which is of this structure. That is the first three are the attributes of student and the last three are the attributes of lab and the middle attribute here lab name is the common attribute between student and lab. Now it is going to join only those tuples which where this common attributes match. So, this is a special kind of equijoin operator where not only equality condition is assumed but its also assumed which are the attributes on which the equality condition is operated upon. So the natural join simply takes attribute names which are the same in the two relations and then computes an equijoin over them.  




(Refer Slide Time: 00:17:13)

 

So the next operator that we are going to be looking at is what is called as a renaming operator. We just saw here now that suppose we modify student as so and so or suppose we modify lab as so and so, we can just use natural join. Now can we formalize this notion of modification into the relational algebra itself? Can we introduce a notion into relational algebra by which we can say this relation is modified as this relation and then used in this expression and so on. 

Now one way to achieve modification is by assignment. Yesterday we saw that the input and output of select and project operators are both relations therefore the output of these operators can be assigned to a new relation name and then this becomes a relation by itself. Now this assignment can be further generalized such that we not only assign to a new relation name, we also assign the attribute names of the new relation. So this slide shows the idea here, the first expression here projects the following statements that is it takes the student relation and then projects roll number and lab attributes of the relation.

Now once you get a relation as an output, it is in turn assign to another relation called TA or may be teaching assistant with the roll number attribute name replaced by id and lab replaced by lab name. So the output of this relation is another relation with its own name in this case TA and with its own attribute name that are different from the attributes names of the incoming relation. 

Now this renaming operator or this kind of renaming can be implicitly achieved without an assignment statement by using the rename operator which is identified by the Greek symbol row which is shown below in the slide here. So the statement here, the relational expression here shows that the same thing that as the assignment statement above that is it projects roll number and lab from the student relation and then computes a rename or gives them as part of a rename expression that says rename it as TA id and lab name.

So how does the rename operator work in general? In general the rename operator contains or may be defined by the following properties. This is shown in this slide here. The first form of the rename operator shows that the entire relation, the input for the rename operator is of course a relation and of course the output is also a relation. So the entire relation is being renamed that is it is given a name S and the attributes are given names B1 B2 extra until Bn. 

The second kind of rename expression that is row subscript S operated upon relation R, we will rename only the name of the relation that is the output relation is called S in this case and in the third case where row has the subscript of B1 to Bn within braces or rather within parenthesis and the input is the same relation R, the renaming happens only on the attribute names and not on the table name itself.

(Refer Slide Time: 00:19:51)

 

That is the name of the table or the name of the relation remains the same and the attributes are renamed to be called as B1 to Bn in this case.   













(Refer Slide Time: 00:21:08)

 

The next set of operators that we will be looking here are the set theoretic operators. Again the set theoretic operators here operate on relations rather than specific sets as such. Now a relation is also a set but it could be multi-set or it could have certain differences when we are talking about joins and so on and so far. So the set theoretic operations like unions, intersection, set difference they can be applied in the relational model as well.

Now this set theoretic operation can be applied only to what are called compatible relations. Now what is meant by compatible relation between when we are considering two or more relations? That is can I compute a union operator between let us say a student relation and an employee relation or a student relation and a project relation. Now some of these union operators may make sense and some of these may not make sense. 

Now that is why we formally define the notion of compatibility or union compatibility between two or more relations. Now what do we mean by compatibility? The formal definition is shown in this slide here. Suppose we consider two relations A and B or in this case R and S and suppose they have a set of attributes let us say A1 to An and B1 to Bn, they are compatible if and only if first of all you might have already noticed that the number of attributes are the same. That is the R has set of attributes A1 to An and S has a set of attributes B1 to Bn. So firstly the number of attributes are the same, if they have to be compatible and the domain of every corresponding attribute is also the same. That is the domain of A1 is the same as the domain of B1. If A1 can span the set of all valid roll numbers, B1 should also span the set of all valid roll numbers and so on. Similarly for A2 to until An, B2 until Bn.

So the set of all corresponding domains are the same. Note that there is nothing here about the names of each operators. Roll number could be called roll number in relation A and could be called id in relation B, it doesn’t matter as long as the domains of each of these attributes are the same, we should be able to compute the set theoretic operations like union intersection and set difference.

Now assuming that we have two or more relations which are compatible, how do we compute set theoretic expressions? The union operator R union S which is shown in the slide here it simply returns the set of all tuples that are present in either R or S or both and of course without any duplicates that is any tuple. Note here that entire tuples are compared between R and S, the entire tuples where the corresponding elements have to be same or compared. That is suppose I have one student relation or one record about a student in R and another record about a student in S, it just combines both of them that is the output of R union S is the set of tuples that lie either in R or in S or both and of course without duplicates. 

Similarly the intersection operator returns the set of all tuples that are present in both R and S which is same as this intersection operator on sets. And similarly the set difference operator R minus S returns a set of all tuples that are in R but not in S. So the set of all tuples that are unique to R but and not present in S would be the output of R minus S. So we can note that this standard properties of set theoretic operations also apply here that is union and intersection are commutative R union S equal to S union R and R intersection S equal to S intersection R. However the set difference is not commutative, S minus R is not the same as R minus S. 

So we shall be coming back to these set theoretic operators again when we relax the fact that a relation may not contain duplicates, now what happens if you allow for duplicates in the tuples. The next operator that we are going to be looking at is the division operator. The division is a slightly unintuitive operator in the sense that it needs a little bit of explanation to understand what or where a division is going to be used. A division operator is essentially used in cases where we may have to identify data elements that are associated with some other data element whenever the other data elements occur that is for all properties of the other data elements.  
















(Refer Slide Time: 00:25:43)

 

Have a look at this slide here which shows a particular example. Now firstly there is the definition here the division operator is used to denote conditions where a given relation R is to be split based on its association with every tuple in another relation S. Let me go straight to the example here and then go back to the explanation of division. The example shows two relations R and S. The R relation has two attributes A and B and the S relation has just one attribute A. Firstly the division of R and S is going to return the attribute B  that is the R divided by S is the set of all attributes B such that there is some relation between the attributes in A. So what is that relation? The set of all attributes B contained in R that are associated with all values of attribute A of S. That is suppose let us take the example of b1. Is b1 associated with a1 in R? Yes it is, a1 b1 is here. Is b1 associated with a2? Yes it is, a2 b1 is also here and these are the only two values in S.

Hence b1 is a valid result in T equal to R divide by S that is every data element here that is associated with every other data element in the other relation is what is going to be returned as part of this relation. Consider an example something like which employee has worked with some other employee on all projects that he has worked. Let us say which employee has worked with some employee named Arun or something on all projects that Arun has worked. So suppose these were all the project that Arun has worked and there is this employee b1 who has also worked in all these projects, you are going to get b1 as an output. 

Consider the case of b2 here, now b2 is associated with a1 but it is not associated with a2 there is some a3 with which it is associated. Hence b2 is not part of the result but b3 which is again associated with a1 and a2 it is part of the result. So the division operator in some sense divides the first relation based on which data elements in the second relation in some way completely divides that, that is associated with all data elements of the first relation. We are going to look at an example later on where division operator is going to be used and what is the power of the division operator.
(Refer Slide Time: 00:29:12)

 

There are also other relational operators which are also called additional relational operators which we shall briefly mention and have a look at their properties. We are going to mainly look at two such operators namely the outer join and the outer union operator. Now, going back to the notion of join, recall that join or a theta join operator takes a join condition as one of its input. Now suppose any of the attributes which match condition is null then such tuples are not further processed at all that is they are just thrown away from the relation. However in some cases it may be required to compute all possible joints even when the join attributes are null and this is what is called as outer join. So consider two relations R and S, now suppose let us say for the sake of simplicity we have defined a natural join between R and S.

Now every attribute in R which is participating in the joint should point to or should refer to an existing attribute in S that is it should not be null and the attribute that it refers to should exist in S, only then the natural join can be processed. However there can be two possible scenarios, the left outer join that is suppose the attributes that are participating in join the attributes of R that are participating in join are null. The left outer join includes such tuples even when the attributes that are or the referencing attributes are null. Similarly the right outer join includes tuples even when the referenced attributes does not exist.

So it basically replaces them with nulls and then just includes that part of the tuples that is that part of the tuple from R which has data in it and the rest of the data elements would be null. So it is some kind of a union or a canonical, well I shouldn’t say canonical but some kind of a union operator where you include every tuples any way, its some kind of an inclusive operator or inclusive join operator where it includes tuples any way even when the corresponding attributes are null. On the lines of outer join we can also define outer union operator. 

Now we saw that the union operator that is union between or union or intersection or any set theoretic operators between two or more relations can be performed only when they are compatible. Now what did we define as compatibility as? The compatibility was that both of these relations should have the same number of attributes and the domains of the corresponding attributes should be the same in both these relations. Now the outer union is basically a relaxation of this constraint and the example shows two relations R X and S Z that is X is a set of all or the list of all attributes of R and Z is the list of all attributes of  S.  

Now suppose X and Z are not compatible, however a subset of X and Z are compatible that is W a subset of X and Y are subset of Z are compatible. Now a union or an outer union operator computes the union based on this compatible subset of these relations and then simply includes all other relations or all other attributes as they are in the relation. So similarly the outer union operator is simply some kind of an inclusive union operator that includes tuples or works on relations even when they are not union compatible or even when they are not perfectly compatible between them. So theoretically speaking there are, do we need all these operators or can we express one operators from other or has a basis of using other operators. This slide shows what is called as the complete set of relational operators. 

(Refer Slide Time: 00:33:49)

 

Now as you can see in the slide here, there are the following operators select, project, union, set difference and the Cartesian product. Now this set of operators is called the complete set of relational operators because every other kind of relational operator can be expressed as a sequence of the above operators. I am just giving two examples here but you can verify that for yourself taking each operator and trying to express it as a sequence of the other, one of the complete set of relational operators. 

For example R intersection S can be or is equivalent to the R union S and the difference of R minus S union S minus R. So let us not go into the set theoretic operation to prove this equality but I am sure this is quiet obvious that you can express the intersection using union and set difference. Similarly joins can be expressed theoretically using a select condition over Cartesian products. We already saw that in the example there were student . lab equal to lab . name or in this case here R join condition S is the same as select condition over R times S. The first one here should be the join symbol and R join condition S is equivalent to select condition over R times S.

(Refer Slide Time: 00:35:25)

 

Now we are going to look at making one generalization over or trying to relax particular constraint on relational expression or on relations. Now until now we have been saying that relational operators should take relations that are sets and not multi-sets and return relations that are also sets and not multi-sets. However in some cases it might be necessary or even desirable to allow for the existence of duplicate or duplicate tuples in in relations both in the input and output relations. 

Now first of all such relations or such sets where elements can occur more than once is called a bag or it’s also called a multi-set. So bag or a multi-set is a set that may have multiple occurrences of a given element. As we can say it’s a generalization over the present notion of a set that is every set is a multi-set in which each element occurs exactly once. However a multi-set is something where an element can occur more than once. 

Now in some cases bags are actually necessary in the relational models not only desirable but also necessary. When are these conditions? Consider the case that we are querying the database to compute the average marks obtained by all students in a particular semester. So we have a student relation in which there is one of the attributes which is called as marks. Now we project this attributes saying project marks based on student.

Now based on the set of all marks that it has projected, we compute the average mark by computing the sum of all these marks and divided by the total number of entries that are there. Now in this case if duplicates were actually to be removed, we cannot compute the average in a correct fashion. We will actually be losing information when we change the multi-set or when we remove duplicates and make it into a normal set. So when we are computing aggregate relations like sum and average, we actually need multi-sets or duplicates in the relation, the duplicate should not be removed from the output relation. 

Similarly if we can tolerate duplicates in the relations somewhere while evaluating relational expressions, it may make things much faster. For example every time I return the output of a project operations, I may have to spend considerable amounts of time trying to looking for each tuple and seeing whether there are duplicates of this tuple in the output relation. So especially when computing projects and unions duplicate removal may take a significant amount of time. 

Now if I have a relational query that has several project operations and union operations and are embedded somewhere deep in the query, it may be terribly inefficient or to be computing or to be eliminating duplicates every time we compute project and union operator. So, sometimes it may be necessary or it may be desirable to tolerate bags or to tolerate multi-sets as part of a relation as part of an intermediate output in a relational expression. Now how does this generalization from sets to multi-sets affect relational operator? 

(Refer Slide Time: 00:39:09)

 

Now consider two bags R and S. Now in the first bag let a tuple t occur n number of times and the same tuple t occur m number of times in R in S. So how do we define set-theoretic operations based on these bags? The first operation that we define is called the union of bags or it’s also called the disjoined union of sets and it is denoted by R plus S. You can also denote it by R union S when we are sure that R and S are bags and not sets. 
So its simply denoted by R plus S and its simply contains R plus S that is the cardinality of R plus the cardinality of S number of tuples that is both bags are simply combined, every tuple in S is combined with every tuple in R and that’s it. That is it has m plus n occurrences of this tuple t which has been repeating. Now when you are compute the intersection of bags, R intersection S what happens to tuples that occur multiple times? So tuple t which occurs n number of times in R and m number of times in S occurs only a minimum of m and n number of times in R intersection S. So as you can see the generalization here that is in sets, a tuple will appear in R intersection S only if it appears in both R and S. Here it will appear the minimum number of times it appears in both R and S.  

Similarly the set difference between bags, the set difference between R and S, R minus S is where the tuple t occurs n minus m times that is n number of times it had occurred in R and m number of times it had occurred in S. So the number of tuples that is going to occur in R minus S is n minus m, if and only if n is greater than or equal to m. If n is less than m then the number of times is going to appear is zero, of course it can't appear negative number of times. So tuple appears does not appear, if the number of tuples in R does not out number the number of tuples in S. You can think of it has something like canceling out tuples from R and S. So for every tuple in R, we cancel out ever tuple in S and then see how many tuple are remaining in R after we have cancelled out all tuples in S and that is the number of tuples that we are going to take in R minus S. 

(Refer Slide Time: 00:41:45)

 

Now what is the other operators on bags? The select operator does not change, the select operator operates the same whether it is on sets or bags. You just take a select condition and apply it to each tuple in the input relation regardless of whether the relation is a set or a bag. Similarly the project operator becomes simpler. It does change but it becomes simpler that is a project operator simply takes the requested columns or requested attributes and gives them out, there is no need to eliminate duplicates.
Similarly the Cartesian product of bags, it’s also the same thing. It is if a tuple t occurs m times in R and n times in S as before then the tuple t S that is t combined when with S occurs mn number of times. That is every tuple in R is combined with every tuple in S regardless of how many times they appear in R m, in each of these relations.

(Refer Slide Time: 00:42:45)

 

However there are certain algebraic expressions on bags that do not hold when the relations are tuples. Have a look at these algebraic expressions that are shown in the slide here. The first algebraic expression is the distributivity over the set difference operator that is R union S minus of T is the same as and is equivalent to R minus T union S minus T. You can easily verify that this expression is true when R S and T are sets. However this expression is not true or does not hold when R S and T are bags and not sets, when they are multi-sets.  

Now why is it not true? Let us take an example, this is called the 1-1-1 principle. Now consider that a tuple t occurs exactly once in R S and T. There are some particular tuple or some particular data element that occurs exactly once in R S and T. Now when we compute R union S, in the resulting set this tuple occurs twice if it is a multi-set or if it is a bag. Now once we consider and then once we compute R union S minus T, this tuple which we are considering now would have occurred once that is 2 minus 1 number of times, once it could have occurred. 

On the other hand look at the right hand side here. When we compute R minus T, this common tuple which had occurred exactly once is not going to occur in R minus T, it’s going to occur zero number of times. Similarly in S minus T this tuple vanishes, it occurs zero number of times. So in the union between R minus T and S minus T, this tuple does not exist at all whereas there is one occurrence of this tuple in the left hand side of this expression. 

So this expression does not work when R S and T are bags and not tuples and not sets. Take this second expression. The distributivity over intersection and union that is R intersection S union T is equal to R intersection S union R intersection T. This is of course easily verified when R S and T are sets. However this does not hold when R S and T are bags and this can again be easily verified by considering a specific counter example which is called the 2-1-2 principle. 

I will not be going into detail into the 2-1-2 principle, you can use the same argument as we have used in the first case where we took the 1-1-1 principle that is consider a tuple that occurs exactly once in R S and T. Here consider a tuple that occurs exactly twice in R, once in S and twice in T and see what happens and see if the left hand side of this expression is equal to the right hand side of the expression as far as this tuple is concerned. And you can see why this relation does not exist, does not hold or this equality does not hold.

The third expression is also significant when we are considering bags and not sets. Expression gives a select operator that is select C or D, C and D are some conditions over attributes of over R, so I am selecting C or D over R. That is select any tuple where either C or D or both holds and give me all those tuples. Now if it were a set that is if R where to be a set, I can rewrite this as select C union select D that is select C over R and union it with select D over R but the same thing does not work when R is a bag and not tuples.  

Now again you can take a very simple counter example to show that this is the case. Now consider a particular tuple where both C and D are true. Now that tuple is going to be return only once in the left hand side of this relation but this tuple is going to occur twice in the right hand side of this relation because its going to be returned once from C and once from D and when we are taking a union or disjoint union, we are going to just add up both of them and its going occur twice in this relation. Hence this does not work when R is a bag  

So tolerating bags is not only desirable but sometimes also necessary however bags pose their own unique problems, unique issues when we are considering set theoretic operations and algebraic expressions over bags and which we have to keep in mind when we say that when we either decide to tolerate bags or not tolerate bags. In a nut shell we have covered quiet a few of quiet a significant part of what constitutes relational algebra expressions and what constitutes or how to write queries in relational algebra. 

In the next three slides let me give a small example of relational algebra queries and how queries can be composed from one another. So this slides shows a very small database schema comprising of 5 different relations employee, department, department locations, projects and works_on. So employee is a relation that talks about details of an employee, it has the first name, middle, initials, last name, the employees pan number, date of birth, address, gender, salary, the supervisor of that employee and the department number where the employee works.

And the department contains department name, department number which is the key here, all primary keys are shown underlined and the pan number of the manager and the start date of the manager.
 
(Refer Slide Time: 00:48:18)

 

Department location only shows each department number and the location where it is located and project shows project name, project number, location and the department number where the project is working. And similarly works_on talks about this employee works on this project for this number of hours and so on. So let us take some typical queries very quickly and go through how we can answer these queries. 

Query one, the first query which we are going to consider says that retrieve the name and address of all employees who work for the research department. So how do we answer this query. First of all we take the set of all tuples that form the research department that is select DNAME equal to research from department, the set of all tuples which are the research department. 

Now compute which are the set of all employees who works in the research department? How do we compute that? Compute a join between research department and employee where the department number is the Dnumber. Recall that in the employee record there was a Dnumber here which showed which is the department number where the employee work.   






 
(Refer Slide Time: 00:49:24)

 

So compute a join, an equijoin where this is this. Now from this we have got all details of employees who work in the research department, from that we need only the first name last name and address because that’s what the query asked that is the name and address for employees, so project as a last query. Query two: Find the names of all employees who work on all projects controlled by department number 5. So have a look at this query again. We want the names of employees who work on all projects that are handled by this department. So how do we go about answering this? First of all let us find out what are all the projects that are being handled by department number 5. 

So department 5 project is the name of the relation which says project, the project number and select from project where department number equal to 5 and project only the project number. Then which are all projects that employees work on? Take the pan number and the project number and then work on this. Now what we have to do is that we just have to compute a division between employee project and department 5 project which basically gives us the set of all pan numbers of employees who work on all projects or who are associated with all projects of department 5 which is the result. 



(Refer Slide Time: 00:50:39)

 

That is we in turn use that to and combine it with the employee record to return the first name and last name of employees. So in this way we can, as you can see here for any given query we usually need to perform a series of operation, series of relational algebra operations before we get to the final result. 

(Refer Slide Time: 00:52:13)

 
                                                           
So let us summarize what we have learnt today in a brief fashion. So we saw the definition of relational schema, the notion of a relation, domains and attributes and the characteristics of relations especially with considering duplicates and ordering of tuples and so on.  
We also saw the basic relational algebra retrieval operations that is select and project and so on and set theoretic operations and relations and also how this set theoretic operations change, when we relax the notion of the relation from being a set of tuples to a bag of tuples when we can allow for duplicates. We also saw why in some cases, it’s not only desirable but also necessary to use bags. We also saw how we can, given a particular user requirement how we can go about formulating a relational algebra query in a step by step fashion. So that brings us to the end of this session on relational algebra. Thank you. 


Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 5

Structured Query Language

Hello and greetings. In the ongoing saga of management of data, we have covered the aspects of high level schema design using entity relationship modeling and one kind of a data model called the relational model which is mainly used for designing of low level schemas. Today we are going to look at something more concrete that is something which you are likely to be using on a day today basis, if ever you are going to be working in databases. Especially if you are a database user or database administrator, this is the SQL language or the structured query language.

(Refer Slide Time: 00:01:54)

 

The structured query language is the standard query language that is used by most database management systems that are available today and this is a language that is definitely required to be known by anybody who is going to be using a database management system. So let us move into structured query language today.
	







(Refer Slide Time: 00:2:19)

 

Now what is meant by a standard for query languages? Why do we need a standard for query languages? I just mentioned that the structured query language is the de facto standard or it’s rather these standard for relational databases that all databases or most of the database users today use SQL in some form or the other. Why do we a standard query language for database management? Now today at this point in time, in the database arena there are large number of database management systems that are currently available.

Some of them are commercially available, some of them are freely available and some of them have been implemented in academic institution, some of them have been implemented in companies, industries and so on. We have oracle, we have IBM db two, we have Sybase, Microsoft sequel server, MySQL, Postgres and what not. There are so many different varieties of databases.

Now suppose we were to have a different query language being adopted by each different database management system, it becomes extremely difficult if not impossible to be able to port application program from one database to the other. We will have to write software that specifically uses let us say MySQL or software that specifically uses oracle and so on. However with a common query language like SQL, all that the application programs need to know is SQL. They should be able to speak SQL and using SQL we should be able to connect or use any database management system. So this is what is depicted in this in this slide here.

Suppose you have several different database systems, in fact in some application context you may have several DBMS within the same application context. A huge company for example may use several different kinds of DBMS systems in their different branches, while all of them have to deal with the same application, same work flow procedures or same business logic or business processes and so on. So suppose we have several different databases or DBMS as shown in the slide here and suppose all of them are able to speak a query language like SQL, all that the applications need to do when there are different application says that they should be able to speak SQL.

And they should, it doesn’t matter which DBMS they are going to be working. But reality obviously is much more complex than these and while this was the reason why a standard for database query was introduced, it is nowhere near to achieving its objectives. that is in reality however we still see applications that say this application is meant for MySQL, this is meant for oracle database, this is meant for db two and so on because there are number of additions, there are number of other features in addition to SQL that are unique to each different applications and so federating or working across different, connecting different database system is a completely different question all together now.

And data integration is a completely different question which we would be exploring in much more detail in a later session. So coming back to SQL, SQL stands for structured query language and this was introduced the genesis of SQL was at IBM research for database system that they had built called system R. Just like seminal paper for cord, you can give an internet search for a system R and there are of course some very good papers that are been, that are available on the net that talks about earlier days of SQL.

As you will see today, SQL contains a lot of constructs like select and set operators and Cartesian products which are quite similar to relational algebra that we saw in an earlier session and SQL also works on a relational data model very similar to relational algebra.

(Refer Slide Time: 00:05:57)

 

However SQL has no relation to relational algebra or very little relation to relational algebra in itself. The mathematical foundations for SQL is another data model called the tuple relational calculus which is also another data model based on the relational data modal that is the notion of mathematical relations. But there are some similarities in terms of terminology between what we have seen in relational algebra and SQL. And SQL is a database independent language and in this session, we would be looking at construct from the SQL two standard and SQL three standard has some more facilities that are provided. And SQL is both a data definition language and a data manipulation language. What is it mean? A data definition language or a DDL essentially defines data elements. That means what are the data elements in a relational model? Of course a relation that is relation, attributes, domains, values, constraints, keys and so on. 

So all of these can be defined using SQL. Using SQL you can define a relation, you can define its attributes, you can define its domains, you can define constraints across attribute, relationships, key constraints and entity constraints and so on. And it’s also data manipulation language in the sense that you should be able to add more data into a database, you should be search for some data, you should be able to retrieve some data, you should be able to modify data elements and so on. So it’s both DDL and a DML.  

(Refer Slide Time: 00:8:30)

 

So, in terms of terminology while we use the terms relation, tuple and attribute in relational algebra usually the terms table, rows and columns are used for the corresponding terms. We shall be using either of these terms interchangeably as they mean the same thing whether it is a relation or a table, we saw in the session on relational algebra that relation can be represented in the form of a table where each attribute of the relation is a column which is what is returned by the project operator. And each tuple in the relation is actually a row which is what is returned by the select operator.

The SQL two standard defines methodologies for introducing schematic structures into our databases. A schema is created by what is shown here as the create schema command. So the slide here shows a small example which says create schema univ authorization dean, so it creates a schema called univ which is owned by the user with a user id called dean.
(Refer Slide Time: 09:21)

 

We shall be looking into this authorization in much more detail when we are talking about security and authorization in databases where different users of database management system are usually given certain privileges and certain authorizations which authorizes them to do certain kinds of data manipulation operations on the database. So hence if the owner of this schema is this user id called dean then a dean is given certain kinds of authorization that is defined by some default values which can also change here, which would typically include adding a table, deleting a table, adding rows deleting and so on. I mean any kind of activities that a typical owner of a database would do.

A catalog in SQL terminology is a named collection of schemas. So suppose I have a collection of different relations and combine them within particular name, this is called a catalog. And a catalog is required or it is required for different tables or different relations to be within the same catalog if I have to be able to enforce referential integrity on my schema. So using a scale you are able to enforce referential integrity only within tables that lie within the same catalog.  













(Refer Slide Time: 00:11:19)

 

Creation of a table: As you know table is another term for relation and a table can be created with the command create table command, just like create schema for schema. So the slide here shows an example create table command of the form create table name and some kind of column descriptions. The parts of the syntax enclosed within box braces are optional structures that means you can refer to the table name when the context is clear or otherwise you need to identify the schema within which the table belongs. You can say something like create table univ.department that means create table called department under the schema called univ. 

There are also certain column descriptions which we are going to see shortly which can say what are the different attributes that form the table and what are the domains of each of these attributes and what kinds of constraints that each of these attributes have. So before we look into column descriptions, let us say what kinds of data types are also domains that SQL two supports. There are several different kinds of domains that SQL two supports and some of the most commonly used domains are as shown here. There is the numeric domain which is identified by different data types called int, small int, float, real and double precision.











(Refer Slide Time: 00:12:30)

 

Each of these refer to different sizes that are of the data word that is being stored as part of this domain. There are formatted numbers like decimal I, j which basically means that this number has i number of digits of which j number of digits occur after the decimal point. Hence if I say a decimal 8, 2 it means that there are a total of 8 digit in this decimal of which two digits, the last two digits occur after the decimal point. You can also define character strings of either fixed length which is shown by char of n or of varying length which is defined by varchar of n.

So I can say that this attribute name is a varchar of say 60, so that means that this attribute can store a string whose length can vary anywhere up to a maximum of 60 characters. Then there are also big strings that you can store either a fixed length or varying length and similarly a special data types like date, time and time stamp and text and several other data types like binary objects and so on. But typically we would generally be working with numeric or character string in a typical transactional database system like employee records or railway reservation or whatever the standard transactional databases that we would be considering here.












(Refer Slide Time: 00:14:34)

 

You can also create your own domain, name domain by using the create domain construct. The slide here shows an example which says create domain roll number type as int of 6. If I know that the roll numbers that I give to students in our institute would be an integer having 6 digits, I can as well use roll number type instead of int of 6 wherever I need to store student roll numbers.

The advantage with this is that tomorrow if this domain definition is changed, I need to change it at only one place where I am defining the domain called roll number type. There are also certain constraints and default values that you can specify using the create table command which you can specify on an attributes. for example the not null constraint, the first constraint is that we are going to take up is a not null constraint which says that this attribute name for which I am placing this constraint can never have a value called null.

So it disallows null as a valid value for this constraint. For example I can specify that the age of an employee may never be null that means I need to always have a valid age for an employee record that is entered in this data value, if I have to insert a particular row into the database.










(Refer Slide Time: 00:15:17)

 

Similarly I can use the default construct, the default construct shows what would be the default value for a particular data item. so if the not null is specified and I also specify a default value, whenever let us say I specify that age of an employee not null and default 18. So whenever the age of an employee is not known, the default value namely 18 in this example is put in its place. Then there is the primary key constructs, the third constraint in this slide which specifies one or more attributes of this tables as the primary key of this record. Similarly there is the constraint called unique, I can say for one or more attributes I can say unique which essentially says that this attribute has to have unique values or distinct values for each row of this table.

In other words it means that it is an alternate key or a secondary key. Note that a key is something which can uniquely identify every tuple. Hence the key has to have a unique value for each tuple in the relation similarly I can also specify a foreign key construct that ensures referential integrity especially when some part of a table is actually referring to some other parts of another table, I can specify them as foreign key in order to ensure that they refer to existing aspects of the other table.

So just to brush up what is referential integrity, if I have some aspects of my table referring to another table it should either refer to an existing tuple or existing row in the other table or it should be null that is I should not refer to any row in the other table or I should refer to an existing row in formal terms this is what is meant by referential integrity.  






(Refer Slide Time: 00:18:16)

 

So here is an example of table creation. This slide shows an example table which says a create table employee, the name of the table or the name of the relation is called employee and then there is a set of attribute descriptions. The first attribute called pan number, the permanent account number of an employee is given a varchar that is variable character string of 16 letters and it is shown as not null and it is shown as unique. That is this pan number may never be null and it has to be unique across each employee that is each employee has to have a distinct pan number. If you go down the slide you can see that there is another construct called primary key and emp number that is the second attribute employee number which is also shown as not null is termed as a primary key.

Hence the pan number in this case because it’s unique can form a secondary key or an alternate key of this relation. Then there are other attributes like name, gender, date of birth, address, salary and reports to which basically shows the employee number of the manager or the person to which those employee reports to. So you also say another constraint called foreign key which says that reports to, the field called reports to is actually a reference to another record belonging to the same relation called employee and referring to the employee number in this record.

Therefore whenever I enter a table and I enter a particular employee number for reports to, suppose I enter details of an employee and enter the details of the manager by specifying the employee number of the manager to whom these employee reports to then because of the foreign key constraint that is specified here the database management system will verify whether a record for the manager already exists. If the manager record does not exist that is the employee with that reports to employee number does not exist then addition of this employee record will fail as part of the database management.

Deletion of tables: Tables can be deleted, the terminology used for table deletion is called drop. So tables can be dropped using the drop table command, this is shown in this slide here.

(Refer Slide Time: 00:21:05)

 

You can use, the syntax is something like this drop table and name of the table and there are certain optional attributes which says dependent cascade or restrict. So the first three terms are obvious, drop table name that is the table name by the given name should be drop. What is the dependent clause or what is it do? So if dependent is termed as cascade if I say drop table employee dependent cascade, then any foreign key constraints that the table holds or views that reference the table, well we have not come to views as yet but for the moment let us just not consider that. But when I drop a table let us say employee, any foreign key constrains will also be dropped.

So the dropping is in some way a cascading process because of table employee, any foreign keys that the table references to or all going to be dropped in a cascading fashion. on the other hand if the restrict option is specified then a table is dropped only if it does not have any references, incoming references that is only if nobody references the table only then will a table be allowed to drop. Just like table deletion, you can also delete an entire schema using the drop schema command. This is shown in the slide here. The drop schema command also has a very similar syntax, it is like drop schema name and either cascade or restrict.

So drop schema and name is obvious that is you have to drop a schema by the given name and if the cascade option is provided then all tables that are there in the schema will be automatically dropped. On the other hand if restrict option is specified, then a schema is dropped only if it does not have any table or if it does not have any elements. How do we modify tables? How do we alter an existing table definition? So table definitions can be modified using what is called as a alter table command. Note that alter table or modification of a table is modification of the schema of the table not the data in the table. That is we are not modifying existing data elements or adding or deleting data elements from and to the table, we are actually modifying the table definition or the table schema.

(Refer Slide Time: 00:23:04)

 

(Refer Slide Time: 00:23:12)

 

The table schema is simply a definition of the set of all attributes that form the table and their domains. so the slide here shows an example which says alter table company dot employee add job varchar 20, so it essentially adds a new column to the employee table with the name called job and domain varchar that is a variable character string with a maximum size of 20 characters.
Now suppose the table that I am altering has already containing is already containing some data that is I have created a table, I have added certain data elements and I have used the database for sometime and then suddenly I give an alter table command and say add a new column like job. Now what happens to, what value should job get in all of these tables? Because I have not specified any value as part of the alter table command.

You might have guessed it that it is going to be given a default value of null. So a new column called job is going to be created with a value of null but what happens if I specify a constraint called not null? Suppose the slide here reads alter table company dot employee add job varchar of 20 not null. Now what should be the, what should be the value that has to be filled in for this new column?

(Refer Slide Time: 00:25:28)

 

The answer to this is in the next slide here. Now unless a default condition is specified, unless a default value is specified you cannot use the not null constraint, as simple as that.
That is if I don't use a not null constraint and the query is just like this that is which says alter table company dot employee add job varchar 20, it just adds this new column with all null attributes. On the other hand if I wanted to specify not null then I should also use a default value which is what is going to be filled for all the data elements in this new column.

So I can say default employee or default shop floor or something like this. So the default value called the shop floor is going to be filled for all of the elements in this new column which should later be change for specific rows using some other command which we are going to see later on.




(Refer Slide Time: 00:26:31)

 

Now we just saw how to modify a table by adding a column. What if we need to delete a specific column? The syntax is again quiet similar to that of deleting tables. We use the key word called drop, so this table here shows an example alter table company dot employee and as it to drop the column called pan number and there is an option called cascade.

Now drop pan number is obvious that is the column called pan number is going to be dropped. If the cascade option is used then all constraints that refer to this column are also dropped automatically. That is if some other column refers to this column as in the form of a foreign key or a view or so on then they are all going to be dropped automatically, it’s a cascading process. Similarly if restrict is used then a column is dropped only if there is no incoming references to this particular column.
















(Refer Slide Time: 00:27:41)

 

It is also possible to alter a column definition rather than just adding and deleting new columns. Column also has a particular domain and a particular constraint set of constraints that are associated with it. Now it is also possible to add and drop these domains or constraints that refer to a column and this slide here gives certain examples. The first example shows alter table company dot employee and in turn says alter reports to that is alter the column called reports to set default as 007. So what is this do? This basically says that wherever the reports to, wherever the reports to column is null set it with the default value called 07.

So whoever whichever employee does not have any manager, assign him to the manager called 007 which is what is being set by this command. The second command shows alter table company dot employee, alter reports to drop default which is basically the other way around. That is suppose it already has a default value then all those rows where this column has the default value are set to null and the default value is going to be dropped for this column.













(Refer Slide Time: 00:29:11)

 

We now come to the main operation in SQL, the most frequently used operation for retrieval of data elements from um from tables which is called the select operation. We have not really seen how to add data into a table as yet but let us first see how to retrieve data from a table and then we are going to consider how to add or modify data elements to and from a table.

So the select operation is a most detailed operation in SQL and is the most frequently used operation and it has the variety of forms which we are going to see in a step by step fashion. So the SQL operation is the basic retrieval operation, the select operation is the basic retrieval operation in SQL, it has no relationship with the select operation in relational algebra. Just to reemphasis the point that SQL is actually based on tuple relational calculus and we are going to see here that the select operation of SQL can perform both select and project that are defined in relational algebra.

And SQL select one major difference between SQL select and that of relational algebra is that it considers relations as a bag. We saw in the session on relational algebra that by default relational algebra expects relation to be sets and when we convert them to bags we have to take care of certain algebraic conditions with sets which does not necessarily hold for bags. But SQL by default considers tables to be bags and not sets. There may be multiple occurrences of the same tuple.








(Refer Slide Time: 00:31:05)

 

The basic syntax of a select operation is shown in this slide here. It is very simple, it says select attribute list from table list where a given condition. so there is an example which shows here select employee number, name that is the list of attributes emp number, name from employee which is the name of the table where reports to equal to 007 which essentially means that show me all employees, that is give me the employee numbers and names of all the employees who reports to a manager whose employee number is 007. The select from where is the basic operation that we saw here and it can also act on multiple tables, it need not act on a single table. Until now we have being considering one single table called the employee table. Now let us work with two tables just to show that select can act on multiple tables
(Refer Slide Time: 00:32:03)

 
Now let us first define a new table called department. so this slide shows the definition of department that is create table department and which says where the first attribute is called dnumber which is also the primary key, the department number which is int of 6 and not null and name address and head that is name of the department, address of the department and the head, the employee number of the person who heads the department. Now it also retrace the fact that head is a foreign key that refers to employee number from the employee database or from the employee table.

Consider the following query what is the name of the person who heads the supply department? If you look back at the definition of department, we have seen that the department contains department name, address and head which is the employee number. It does not contain the employee name. But the query here requires the name of the person who heads the supplies department. So the name of the department is supplies and we require the name of the person, so this can be specified by small SQL statement like this, select employee dot name from employee and department where employee number equal to head and department name equal to supplies.

(Refer Slide Time: 00:32:51)

 

As you might have imagined this is quiet similar to performing a relational algebra select on a Cartesian product of two tables. In this case there is a Cartesian product of two tables employee and department and we are stipulating the fact that employee number equal to head in this Cartesian product. That is considering only those tuples where the head of the department corresponds to the employee number of record in an employee and the department name equal to supplies. And also note the use of that table name in order to disambiguate attributes having the same name. Now even the employee table, in the definition of the employee table the name of the employee is specified by an attribute called name. 

But in the same way in the department record as well, in the department table the name of the department is also de referenced by an attribute called name. Now when we say select name, which do we mean? Do we mean the employee name or the department name? In order to disambiguate this we can prepend the name of the attribute with the name of the table. So the query here says select employee dot name rather than saying just name and then also in the where condition where department dot name equal to supplies. So for some strange reason if some employee is called supplies that should not be matched, its only the department name which should be matched against supplies. However this disambiguating attributes by prepending them with the table name is not always sufficient.

(Refer Slide Time: 00:35:40)

 

Consider the next query here. Now the query here says what is the name of the person to whom Arvind Kulkarni reports to? Now here is an employee with name called Arvind Kulkarni and he reports to some person. Now we need to know the name of the person. Note that in the employee table, we only have the employee number of the person to whom each employee reports to. So obviously we need to have a join of the employee table on the employee table itself that is you have to have a self join for the employee table. So suppose we write a query like this that is suppose we try to write or we try to disambiguate attribute names by putting the table names before them. So such a query shown here that is select employee dot name, so we need employee dot name from employee, employee because both employee and manager are both employees. 

So it’s a select employee dot name from employee, employee where employee dot name equal to Arvind Kulkarni and employee dot reports to equal to employee dot emp number. Obviously you see that there is something, there is quiet a bit that’s wrong here.
You don't know which employee table are you referring to, is it the first employee table or the second employee table?

(Refer Slide Time: 00:37:14)

 

So this is still ambiguous. So in order to disambiguate this attributes names in such a situation, SQL provides as with the opportunity of using what are called as aliasing. So aliasing can be used as follows. Now consider the same query shown in this slide here. So this slide for the time being concentrate only on the second line of this slide where it says from employee, employee as boss. 

So the entire query is like select boss dot name from employee and employee as boss. So essentially what it saying here is that take the first table employee and the second table employee, however use an alias called boss for the second table. So we know whether we are talking about an employee or his boss. And then we say employee dot name where employee dot name equal to Arvind Kulkarni and boss employee number is the same as the employee reports to number. 
















(Refer Slide Time: 00:37:36)

 

So employee dot reports to equal to boss dot employee number. So in this case we will be able to identify which name are we referring to from which relation.

(Refer Slide Time: 00:38:44)

 

Suppose we omit the where clause in the select from and where syntax and we just give a query of the form that is shown here. That is select name, pan number from employee. What is going to be the output of this? As you might have imagined this, such a select statement is similar to the project operation in relational logic. So what this statement does is it returns all rows in the table called employee, however only the columns name and pan number.
So it is similar to saying project name and pan number from employee. And what happens when the where clauses omitted and instead of saying just one table name, we actually specify more than one table name. This is shown in the query here. It says select employee dot name, department dot name from employee, department that’s it.

(Refer Slide Time: 00:39:43)

 

Now what happens here in this case of course we get only two columns as output that is employee dot name and department dot name, however we get all possible combinations of employee dot name and department dot name. In other words we have computed a Cartesian join or a Cartesian product between employee and department with this operation.
(Refer Slide Time: 00:40:07)

 
Suppose we want to select all columns of particular table that is similar to the select operation in relational algebra. We want to select all or entire tuples and based on certain conditions, in such a case you can use the term called star as shown in this query. Here it says select star from employee where name equal to Bhadriah. So, essentially this query is similar to the relational algebra expression which says select or sigma name equal to Bhadriah from employee. That means the entire row or the set of all attributes of relations where the name attribute is called Bhadriah is going to be returned.

Similarly, if I say select star from employee, department in the second query that is shown in the slide here, it computes the complete Cartesian product between employee and department. We now come to the fact that how tables are treated in SQL. In relational algebra we have seen that by default relations or tables are considered to be sets. On the other hand in SQL, tables are considered to be multi-sets or bags that is multiple tuples having the same values are tolerated. We have also seen why this is sometimes not only desirable but also necessary, it is desirable because it is expensive to remove duplicates.

(Refer Slide Time: 00:41:13)

 

Suppose I return a query with 10000 records of which there may be hundreds of duplicates. I need to perform, I need to first sort each of these, this whole set of records and then remove duplicates and then reorder the records in whatever order that the user has asked for. Therefore it is very difficult or it’s an unnecessary over head to remove duplicates, therefore it is desirable in many cases to tolerate duplicates. And in some cases it’s actually necessary to tolerate duplicates. We have also seen examples of these. Suppose I want to compute the average marks of all students in a particular course, it is not only desirable but it is actually necessary that I retain the duplicates because the duplicates all contribute to the total number of marks which have to divide by the total number of occurrences to find out the average marks.

So for computing any aggregate properties, I need to have duplicates. However in some cases if I want to remove duplicates explicitly from the output of a query, in SQL you can give the clause called distinct. As part of your select statement, the table, the slide here shows such an example, it says select distinct name from employee so which simply says that show me the set of all distinct names that the employees have. So if two or more employees have the same name then they are shown only once as part of this query.

(Refer Slide Time: 00:43:32)

 

Similarly one can perform several set theoretic operations like union, intersection and set difference using SQL. So union is operated by using the clause called union and intersection by the clause called intersection and set difference by the clause called except as shown in the slide here. Now, by default union, intersection and except assume that the sets that they are operating upon are actually sets and not multi-sets. So note the use of distinct in this example. The example here says that select distinct name from employee where salary is greater than 3 lakhs union select distinct name from employee where salary is less than 24000. 

So essentially what its doing is that it is selecting the set of all names of employees who are earning more than 3 lakh and combining them with the set of all names of employees who are earning less than 24000 and duplicates are removed in these sets of name. So the union operator assumes that duplicates are removed, when it is performed in the union of these two sets.







(Refer Slide Time: 00:44:55)

 

On the other hand if I want to tolerate duplicates or if I want to specify that the sets are actually bags and not pure sets, tTherefore I need to perform a disjoint union or a disjoint intersection. Remember what is the disjoint intersection of two sets, if a tuple or if the data item occurs multiple times in an intersection, for an example it has to occur the minimum of the two number of times. So if I have to specify that I am actually working on bags and not sets, I need to specify that with the key word called all which is shown in the slide here. So if I have not specified the distinct construct in my select statements as in my previous examples, I should use the term union all for disjoint union and intersection all for bag intersection and except all for difference or set difference between bags.

We can also perform comparisons over character attributes especially string attributes by comparing partial strings or comparing wild cards. So this slide here shows two such examples. The first examples says select star from employee where name like percent, Arun percent so note the use of firstly the key word like and secondly the use of the percentage symbol. So a percent symbol matches any number of characters wherever it occurs, therefore this query here matches employee name where the employee name contains Arun as a substring, A r u n as a substring and where it may be preceded by any number of characters and succeeded by any number of characters.









(Refer Slide Time: 00:45:53)

 

So while the percent symbol matches any number of characters, a single character can be matched with the underscore symbol. So suppose if I had said where name like underscore arun underscore underscore, so it essentially looks for one character before Arun and two characters after Arun. So any kind of character, any kind of name where Arun occurs as a substring with exactly one character before it and two characters after it. One can also specify arithmetic operators like addition, subtraction, multiplication and division. So I can say where salary plus perks not greater than 50000 or so on.

And I can also use these arithmetic operator not only in the where clause but also in the select clause. So have a look at the example shown in the slide, this slide shows a query which says select 1.1 times salary that is 1.1 into salary from employee where salary greater than 3 lakh, so which basically says that show me what would be the figures, salary figures of employees, if salaries where to be raised by 10 % effectively.















(Refer Slide Time: 00:47:22)

 

That is I am multiplying existing values of salary by 1.1 and showing that as the result.

(Refer Slide Time: 00:48:21)

 

So this brings us to the end of the first session on the structured query language where we have looked in to the basics of what makes up the structured query language. And we have seen how to create a schema using SQL and what is a catalog that is a collection of tables and how to specify the structure of a table by specifying the name, the attribute names, the attribute domains, the constrains on the attributes like not null, unique, default values and so on and the key constraints like the primary key which identifies what is the primary key in this and also referential integrity constraints like using the foreign key constraint.

We have also seen how to alter schemas and table constructs or table structures and what are the implications of these constraints on these modifications? That is what happens if I drop a particular column name but that column name is actually referred to as a foreign key from some other table. Now in such cases I can also specify whether to drop all foreign key references or to drop this column only if there is no foreign key reference coming into the table.

So using either the notation of cascade or restrict. We also saw the most widely used operation in relational algebra namely the select operation. However we have not finished looking into the different forms of select operators. In the next session we would be looking at some more features of select as and how to nest select operators and how do we dereference or how do we disambiguate attribute names in the nested select operators. However as we have seen because select is the most widely used SQL operator or SQL statement, it has varied number of forms and several different notations and one of the main properties that we have to remember about the select operator is that the select operator treats tables as bags or as multi-sets rather than as sets that is it tolerates duplicates in the sets unless we specify explicitly that we do not want duplicates and this is specified by the distinct clause.

So to summarize, the slide here shows the summary of what we have seen today. We have seen an introduction to the SQL standard tables, attributes and values and we saw how schemata are created and tables can be created and constraints and essentially the select operation in its different forms that brings us to the end of this session. Thank you. 

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 6

         Structured Query Language II

Hello and greetings. In our ongoing explorations of databases, we had started out exploring the default or the standard for database querying namely the structured query language or SQL. 

(Refer Slide Time: 00:01:36)

 

So let us continue with SQL in this session today and look at some of the more advanced aspects and what kinds of queries that we can express within simple SQL statements. So before we begin let us summarize what we have studied until now in terms of SQL. Now we looked into an introduction to the SQL standard and what are the building blocks of SQL namely tables, attributes and values which or tables, rows and columns and we also saw how to create schema in SQL and what is meant by a schema and how do we create tables and what kinds of column descriptions can we give when we create tables and what kinds of constraints can we specify when creating tables and so on. 








(Refer Slide Time: 00:01:53) 


 

We also saw how to specify entity constraints specifying what is a primary key and what are the secondary keys in a relation or a table. We also saw how to specify constraints like something should not be null or something should have a default value and we also saw how to specify foreign key constraints so that referential integrity is maintained and we also started by looking into the different forms of the select operation which is perhaps the most frequently used SQL operation for retrieving tuples from the database.

So the most generic form of select operation is the select from where clause where you say select attribute list and say from table list where a given condition holds true. We also saw how we can disambiguate attribute names especially when we would be using multiple tables and two or more tables have attributes with the same name. One simple way of disambiguating attributes names is to prepened the name of the table before the attribute name. So we saw something like instead of saying name, we say employee dot name or department dot name and so on.

So, on the other hand we can also use aliasing for disambiguating names mainly because when table names are also the same, when we saw the example of a join between employee and employee tables. 









(Refer Slide Time: 00:03:01)

 

So we basically use the notion of aliasing where you say use the term or use the table employee and call it as some other name like say e or boss or whatever. Then we also saw how we can select from multiple tables, the Cartesian products of multiple tables and what happens when we omit the where clause and so on. And we also saw the different kinds of set operations on tables. By default in SQL and very unlike relational algebra, in SQL tables are treated as multi-sets or bags that means it can tolerate or it is valid to have different tuples of the same having the same data in a SQL table which is not valid in or which is not valid by default in relational algebra.

So we can make a table into a set by using the keyword called distinct. So when we say select distinct some query, a query is returned with all duplicates removed from the query. We also saw how to perform set operations on tables and sets by default, these operations by default assume that table are sets like union, intersection and except for set difference. If we have to specify that the tables are not sets but instead they are bags, we have to specify, we have to use the qualifier called all. So we say union all whatever, so we say employee union all department and so on or employee union all managers and so on. 

And we also saw how to compare substrings using the percentage symbol or the underscore symbol which matches a single character and we also saw some arithmetic operators like addition, subtraction, multiplication and so on which need not just be used within the where clause but they can also be used right after the select clause itself. 






(Refer Slide Time: 00:06:21)

 

So let us move on further today and in today's talk, I will be using two example tables in order to illustrate several features of SQL. So let us look into these tables once again. The first table that we are going to be considering is the employee table. So the employee table contains employee number which is the second field shown in the slide here as the primary key. It also has a secondary key called pan number of the employee which obviously has to be unique and non null and employee is given a name, gender, date of birth, address, salary and reports to which is a foreign key. 

That is reports to contains another employee number which refers to the manager to which this particular employee reports to. If reports to is null then it means that this employee is the head of the company or that is he doesn’t report to any other employee. Then there is a department number or the dnumber which shows where the employee works in. The second table that we are going to be considering today for our examples is the department table. 

 












(Refer Slide Time: 00:07:39)

 

The department table as shown in the slide here is indexed by or uses as primary key the field called department number or dnumber which is the same as the dnumber used in the employee relation that means the same domain. Department also has the name, address and head which is again a foreign key which points to the employee number of the employee who heads a particular department. So here are the specific declarations which says primary key is dnumber and head is a foreign key which references employee number in the employee table. 

So we had started out with looking at arithmetic operator, so let us continue from that point on. And we had seen how you can use plus, minus, star and slashes to perform arithmetic operations namely addition, subtraction, multiplication and division. Similarly you can use the operator called or you can use the keyword called between to check for a range that is to check whether a given parameter lies within a given range. 















(Refer Slide Time: 00:08:17)

 

The slide here shows an example which is of the form select star from employee where salary between 3 lakh and 4 lakh. Therefore this query returns all employee records select star in this case note that the star here is not the multiplication operator but star here refers to the entire tuple in this when star occurs by itself. Therefore this query selects the entire set of tuples, the entire set of columns for all tuples where the salary field or the salary attribute is between 3 lakhs and 4 lakhs. It is also possible to sort the output of a given query using one or more parameters. 

The sorting is achieved by a new construct called order by, this construct is shown in this slide here. This slide shows another small query which says select star from employee where salary is greater than 3 lakhs and order by name and employee number. Therefore this query returns the same result as the previous query that is the set of all tuples, the complete tuples for all employees well not exactly the same of course for all employees whose salary is greater than 3 lakh but not necessarily less than 4 lakhs and the output here is ordered first by name and next by employee number. So that means that it is first ordered, the records are first ordered by name and wherever there is a tie that is two or more employees having the same name, it then orders those tuples using employee number as the ordering attribute. 










(Refer Slide Time: 00:09:41)

 

(Refer Slide Time: 00:10:39)

 

We now come to the next aspect of SQL querying perhaps what gives it compositionality so to say that is by which you can compose queries, bigger queries from small queries this is what is called as the nested query. Note that just like in relational algebra, the output of an SQL statement is a table especially the select statement. The output of a select statement is a table and the input is also a table. It can be multiple tables which are treated as a Cartesian product in which case. Therefore the output of a particular query can be used as part of another query to perform further searches.

So the slide here shows such an example where a query has two different parts in it. For the sake of convenience they are shown in two different colors white and yellow. There is an outer queries so called outer query which says select name from department that is select the names of departments where dnumber in, so in is a new keyword that we are also introducing here which essentially stands for set membership, that is where the dnumber or the department number belongs to the set of all tuples that are returned by the query which is shown in yellow. That is where dnumber in the query called select dnumber from employee where salary is greater than 3 lakhs. 

So let us analyze this query. What does this query do? Firstly take a look at the inner query. The inner query says select department numbers of all employees whose salary is greater than 3 lakhs and the outer query is saying select the names of those departments which are returned by the inner query. So essentially this query is asking the database show me all departments which pays greater than 3 lakhs as a salary. So the highlighted query, the yellow part of the query here is called the nested query and the non-highlighted query is called the outer query in this case. 

So let us continue with nested queries and note that a nested query contains two different queries and the inner query or the nested query in this case returns a complete relation or returns a complete table. For all practical purposes we can consider the table as sets or bags comprising of different tuples and the in clause that we used in the previous slide performed exactly that, that is did exactly that. That is it considered the table that is returned by the inner query as set and essentially performed a test for set membership.
 
(Refer Slide Time: 00:13:11)

 

That is whether the dnumbers specified in the outer query belongs to the set that is run in the inner query. Similarly one can do a check for a particular attribute against all elements in a set using the keyword called all. This is shown in the slide here. The outer query says select name from employee where salary is greater than and then the inner query begins this is shown in the slide here. So the inner query here says select salary from employee where supervisor equal to 007. So let us try to analyze the query step by step, first take the nested query or the inner query here. It says select salary from employee that is give me the salaries of all employees where it should actually be reports to there is a small bug in this slide where reports to equal to 007 that is give me the salaries of all employees who report to 007. So we just want to know what is this employee number 007 or agent 007 pays all of his subordinates. And then the outer query says select name from employee that is give me the names of all employees where salary greater than all of this inner query. That is whose salary is greater than all of the salary that is paid by 007 that is it is greater than the maximum salary that is paid by 007. So it returns the names of all employees whose salaries are more than the salaries of all those who report to 007.

(Refer Slide Time: 00:15:45)  

 

Now what happens to disambiguation of attribute names when we are considering nested queries? So this slide here shows such an example. Firstly, the rule which says that any unqualified attribute name in a nested query applies to the inner most block of the query. So what does it mean? Have a look at the query here. There are again two levels to this query, the outer query says select E. name and then I say from employee as E that is I am aliasing employee table as E where E.employee number in and the inner query begins which says selects reports to from employee where E.name. Note that I am using the alias called E inside the inner query as well, while the alias is actually defined in the outer query and then I say E.name equal to name. 

So what is this query do actually? So if you notice closely, the outer query as well as the inner query works on the same table called employee. The outer query calls itself as E and the inner query does not change the name of the table, it is still called employee. Therefore in the inner query when I say select reports to that is give me the set of all employee ids of the bosses of all employees where the name of the employee is the same as the name of the boss that is E.name here would be the boss here because I am looking for E.employee number to occur within this set of all employee numbers who are bosses. So the query essentially returns all employee names who have the same name as their boss. 

(Refer Slide Time: 00:17:49)

 

So let us look at few more definitions pertaining to nested queries. The kind of query which we just saw now that is where a particular alias is defined in an outer query and then used inside an inner query, such kinds of queries are called correlated nested queries. That is there is a greater correlation other than the fact that an inner query or a nested query occurs within a condition, there is a greater correlation between these two queries. Now how do we understand or how do we analyze the behavior of a nested query. Now to understand how a nested query begins, it should be noted that or it is sufficient if you just note that every select query is performed or every select condition is performed exactly once on each tuple that is specified in the table that, for the table specified in the query. 

Therefore let us have a look at the previous slide once again. In the previous slide for the time being consider just the outer query. Assume that there is just one select statement which is the outer query. So this statement says select some attribute that is E.name from the set of all tuples in employee where some condition that is E.employee number in whatever. So let us not worry about what the condition is let us just assume that there is some condition called C.

Now this condition that is in this case the nested query and the set membership function that is E.employee number in that nested query, this whole condition is checked once for every tuple that forms the employee record that is that forms the employee table. So for every employee, this particular condition is checked that is for every employee we are checking whether his employee number occurs in the set of all employee ids of people who have the same name as one of the subordinates. So in that way as long as we remember this fact that select query or the select condition is checked once for each tuple, its easy to understand nested queries in some kind of recursive fashion that is you have to understand this for each level in a given nested query. 

(Refer Slide Time: 00:20:25)

 

So continuing further with next nested queries we can use a term or the keyword called exists or of course not exists to check whether the output of a given nested query is empty or not. So if the output is not empty then exists returns true that is there exists some results from this query and if the output is empty then exist returns false or not exists returns true. So there is an example given in this slide regarding this which just says select D.head where  from department as D of course that is alias department as D where not exists where the following condition does not exist.
 
What is the following condition which is the inner query, select employee number from employee where reportsto equal to D.employee number. So that means essentially I am looking at the head of all departments that is I am looking at D.head, so the employee numbers of the heads of all departments where this does not exist. What is this? Select employee number from employee where reportsto equal to, this should actually be a, there is again another bug here, this should actually be employee.employee number that is or D.head rather. So where where reportsto equal to this head that means give me the set of all heads of departments who do not have anything, who do not have anyone working under them because this condition should not exist is what we are checking for. 






(Refer Slide Time: 00:22:15)

 

We can also specify explicit sets. Until now we have been looking at implicitly defined sets, we have looked at set membership in the form of in condition, we have looked at set comparison in the form of all condition and we have also checked for empty sets using the exist condition.
 
(Refer Slide Time: 00:22:33)

 

But all of these sets where actually specified in the form of a query that is the nested query. 

We can actually specify sets in a more explicit fashion using just parentheses that is we just parenthesize and enumerate all elements of the set explicitly. The slide here shows one such example which says select name from employee where reportsto in 007, 008, 009. Therefore what it says is that give me the names of all employees who report to either 007, 008 or 009 or that is whether the reportsto field or the reportsto attribute holds one of these values in this. 

one can check for null values in an SQL table using an SQL command called null or a SQL keyword called null capital n u l l and you can also check for whether something is null or is not null. So remember what constitutes a null value for a particular attribute. a null value is a value which is not applicable or an attribute which has no value or no semantic value associated with for a particular tuple. This is different from saying that the value the attribute is zero or unknown. 

So the example here shows a query which says select E.name E.salary from employee has E, so that is select the names and salaries of employees where something exists. What is that exists or that is where the following set is not null. What is the set which should not be null? The set which says select employee number from employee where E.reports to equal to employee number and reportsto is null. So let us analyze the interior query or the inner query again. 

(Refer Slide Time: 00:23:18)

 

It says select or give me the set of all employee numbers from the employee table where E.reportsto that is the employee from the outer query is a subordinate of me that is the employee in the inner query and reportsto is null that is I don't report to anybody else. So   essentially the query returns the names and salaries of all people who report directly to the head of the company. 

That is this the name and salary of all people where the following condition exists. The following condition here is the set of all their bosses who also happens to be the head of the company that is who doesn’t have any other supervisor himself or other self. So note that here the use of the condition is null that is whether reportsto is null. This is different from saying reportsto is 000 suppose 000 is the valid employee number, this is different from null that it means that this employee has a boss. But in this case this is looking for the fact where this employee does not have a boss or does not have anyone to report to.
 
(Refer Slide Time: 00:26:04)

 

Just like we have been using aliasing for renaming table names, the as key word or the as clause can be used to rename attribute names as well. So the slide here shows such an example. It says select name as employee underscore name from employee. So what does it do? It simply selects the set of all employee attributes from this table, however while returning it when the table is returned the name of this attribute is changed from name to employee underscore name.

Now this in turn may probably be as part of a larger query were this would matter or even if it is just printed out on the output, the name of the attribute would have changed from name to employee name or employee underscore name. Note that when a select query is run over multiple tables that is the from clause contains multiple tables select name, salary from employee, department. By default it assumes that we are having a Cartesian product between the tables that are specified. Therefore if we have to compute a join between employee and department for example we have to identify, we have to explicitly identify which is the attribute and which has to be compared. 

Therefore in a simple select operation, let us say we have to join, we have to compute a join between employee and department we would say something like select star from employee, department where employee.dnumber dno.
 
(Refer Slide Time: 00:26:58)

 

Remember what the employee record look like. The employee record had one of the fields as the dnumber or the department number of the employee where he is working in, so where employee.dnumber equal to department.dnumber. So we have to explicitly equate these two attributes to perform a join. Otherwise it is constitute as a Cartesian join between employee and department.

On the other hand we can specify a join that is rather than a Cartesian product using the keyword called join, the slide here shows such an example. The slide shows a query which says select name, address from employee join department. Now when we just say join without any further qualifiers, there is still not enough information to identify which attributes to use for join condition. What is the theta or what is the join condition here? Therefore we specify that condition explicitly. So we say as shown in the slide here select name, address from employee join department on dno equal to dnumber. 

Note that dno is an attribute of employee and dnumber is an attribute of department. So this is specifying an equijoin condition that is it is equating dnumber dno to dnumber and of course there is a where condition which says department.name equal to research. Therefore it is computing the join between employee and department and selecting only those tuples where department name is called research and then printing the set of all names and addresses of employees who work in this department.







 
(Refer Slide Time: 00:29:53)

 

It is also possible to specify natural join directly without having to specify the equality condition. Note that in a natural join if we have two tables, at least one of the attribute names should be the same or should be common between the two tables. So natural join which in relational algebra was depicted by the star operator just performs in equijoin between two relations where the set of all, where it has some subsets of attributes having the same name. So the same condition also holds true in SQL that is if you are performing a natural join, you have to have at least one attribute name which is the same. 

You can always change the name of a table and its attributes names using the as clause which we have already seen. So a natural join may be specified using the natural join construct. It automatically finds attributes having the same names for performing the join and of course relations can be renamed, relations as well as attribute names can be renamed in order to accommodate natural join. So there is a specific example here which says select employee.name, department.name from employee natural join department. 

In our specific example though, the only attribute name which was common between these two was the name clause itself. Therefore for our particular example schemas that we have taken, this query may not make sense but the syntax of the query is illustrated. The main idea behind the example is to illustrate the syntax of the query which just says select employee.name, department.name from a natural join between employee and department. That is to identify all attributes which have common names between employee and department, we compute a natural join and then project the set of required set of attributes from them.




  
(Refer Slide Time: 00:32:04)

 

Similarly other kinds of joins in which we saw in the session on relational algebra can also be specified using the appropriate keywords in SQL. Just like you have natural joins, you can also specify left outer join, right outer join and full outer join as part of any SQL statement. We next come to the notion of aggregate functions in SQL. Until now we have been working on generating a set of tuples or the set of all tuples that match a particular   criteria.
 
(Refer Slide Time: 00:32:26)

 

Sometimes we may need aggregate properties of a query rather than the set of all query results. So there are number of aggregate function in SQL, some of them are shown here. There for example count which counts the number of tuples in the query result, sum which computes the sum of the set of all values in the query result in which case the query result should return single numeric attribute. Average which computes the average of all values in the query result. Again the query should return numeric attributes, max and min which computes the maximum and minimum values of all the query results and of course max and min will work if the query results are numeric or they are ordinal which basically means that they have some kinds of total ordering that is specified among them. 

For example date, date has a total ordering you can always compare two or more dates. The query can result the maximum and minimum among dates and of course char, you can always compare characters using their ASCII equivalent. So the maximum and minimum can be returned. Here is a small example query shown in the slide which just says select count of salary min of salary max of salary sum of salary and average of salary from employee which is obvious. It just counts the number of salary elements, as you can see I could have as well set count star, it does not make any difference because it just counts the number of tuples in the relation and min of salary, min of the salary field max of the salary, salary attributes, sum and average of all the salaries.

(Refer Slide Time: 00:34:40)

 

Since aggregate functions return a single value and not a table, they can actually be used as a part of a logical operator within a where clause. Until now when we have been talking about nested queries, we have been treating the nested query as the set and we have only been applying set theoretic operators like in and all and exist and not exist and so on. But once we use aggregation function and reduce our query result to a single value, I might as well use it as part of a logical operator. 

The example in this slide shows such a case which says the outer query says select E. name from employee as E where the inner query says select count star that is select the count of all, the set of all tuples that are returned by this query and what is the query? From department as D where department. head equal to E.employee number. That is it is selecting the set of all employee numbers who heads departments and counting the set of all such queries that is it is count, it is for each employee that is search in the outer query it is first searching the set of all departments that the employee heads and return in their count. And this count is compared with two here that is greater than or equal to two. Therefore the semantics of this query is return the names, note that we are finally retuning E.name. Return the names of all employees who head two or more departments. 

(Refer Slide Time: 00:36:32)

 

In some cases when we are talking about aggregate functions, it is not really desirable to apply the aggregate function to the set of all query results, set of all tuples that have been returned by the query. It may be more desirable to apply this, the query or the aggregation functions two different sub groups of the query results. Now you can specify such sub groups using what is called as the group by clause. The group by clause is shown in this query here in this slide. 

The slide shows a small example, select dnumber dno, count star, average salary from employee group by dnumber. What it does is for each department that is it first checks the or it first scans the set of all tuples in the employee table and then groups the set of tuples according to dnumber or dno. So for each department that are contained in the employee table count the number of people working in it, that is because we are counting the set of all employee records which have this dno. So count the number of people working in this department and also count the average salary of the people who are working in this department.

 


(Refer Slide Time: 00:38:04)

 

But note that the group by function is computed after computing the results of a query. That is the query first scans the set of all employee records as in the previous example and then groups those records or groups those tuples based on dno and then performs the select operation. However if you want to select certain tuples based on some aggregate property, not an individual property note the important difference here. 

Suppose they want to select a set of tuples from a table based on some aggregate property, we can use different keyword called having clause. The slide here shows such an example, it says select dno that is select the department number count and average salary from employee group by dnumber that is it’s the same query as earlier that is for each department return me the count and number of people working in it and average of salary having count greater than 20. That is there is a further constraint here. For all departments having more than 20 people working, show me the department number, the number of people that are working there and their average salaries. 

Note the difference between having and where clause. You might you might be wondering we could have just used the where clause, select from where, where actually specifies a particular condition and having is also specifying a particular condition. Now what is the difference between having and where?







   

(Refer Slide Time: 00:39:49)

 

Having versus where: Where conditions apply to individual tuples independently that is the where condition where I say something like salary greater than 3 lakhs is applied to each tuple independently. However the having condition applies to groups of tuples, its an aggregate property that is having count greater than 20 that is the number of tuples is greater than 20 or the average of salary is greater than something else and so on. So having is specifies a condition that applies to a group of tuples, whereas where specifies conditions that apply only to individual tuples. 

So let us look back at select now. How does select look like after going through all this different variants of select? Now once I look back looking at all this different variants, we see that the select condition has the following constructs. Select attribute and or function list I can also give a function remember, I can always say select 10 times 10 star salary. That is what happens if my salary increases by 10 times and so on. 

So select attribute list or function list from table list and the rest are all optional, where condition is an option. if you don't specify where then every tuple is checked. Group by grouping attributes. How should the tuples or how should the output be grouped? Having group condition that is some kind of aggregate properties that we can check. Then finally order by which is the sorting condition for the output. 









(Refer Slide Time: 00:40:30)

 

(Refer Slide Time: 00:41:44)

 

We now move into other operations within SQL, select obviously is the most widely used operation and hence the most detailed in terms of some its syntax but we still need operations for inserting or adding data into tables and modifying tables, deleting data from tables and so on. By modifying tables here I mean modifying data in tables not modifying the structure of tables which can be done using the alter table command. So insertion of data into tables can be performed using the insert command. The insert command is shown in the slide. Just like we have select from, we have insert into. 

So we say insert into employee values and I give the entire record for the employee within parenthesis. So I am inserting a complete record where the first 1002 stands for the pan number, the second 002 stands for employee id, the third field Bharath Kumar stands for the name then M stands for the gender, 9-5-1973 stands for date of birth and so on. So we can specify the entire tuple in line into the query itself and corresponding fields and the corresponding attributes are matched. So inserts an entire employee record with corresponding values.
 
(Refer Slide Time: 00:43:16)

 

On the other hand I can insert or I can specify only partial set of attributes within the within the tuple. So this slide shows such an example which says select into employee or rather insert into employee name, address and dnumber values Arun K Ysehwanthpur and 5. Therefore it returns or it inserts only those fields or only those attributes called name, address and department number or dno. What happens to the other attributes? The other attributes will either get a null value or the default value if a default is specified. 

Therefore you can see that there is an implicit constraint in this select operation. That is I cannot leave out, when I am inserting a tuple I cannot leave out any attribute name which contains a not null constraint and does not have a default value. If it does not have a default value and it is not allowed to be null then I have to specify a value during insertion or else insertion is going to fail.








(Refer Slide Time: 00:44:26)

 

An insert or insertion of more than one tuples to a table can also be performed using a select operation. Note that select actually returns a table. Now if I return and table is nothing but a set of tuples. Now if a select operation returns a set of tuples and these tuples are in a format that a ready to be inserted into another table I can directly specify the select command within the insert command. This is shown in this example here. First I just create a table called EMD which contains just three attributes employee, manager and dno that is the department number. Then I give an insert command from the employee and department table using the following syntax. 

I just say insert into EMD that is insert into the new table name and what should I insert? The output of the following select operation then I just give the select command that is select E.employee number as a employee, E.reports to as manager and D. dnumber as dno and then from employee as E join department and so on. So basically I am getting a set of employee number, manager number and department number which is what is going into the new table.












 
(Refer Slide Time: 00:45:08)

 

(Refer Slide Time: 00:45:49)

 

So what are the properties of the insert command? Let us summarize the insert command once again. So insert tuples that are specified as part of the command and all attributes which are not null and do not have a default value have to be specified as part of the insert statement. And what happens if I give the same insert statement twice? That is the same insert statement with the same set of data elements and it is actually performed twice. It doesn’t, insert does not do any checks that is remember that tables are treated as multi-sets rather than sets. Therefore insert just goes and inserts the tuple again. That is second occurrence of the same tuple. This is done as long as the second insertion does not violate any unique constructs. 
That is if I give a set of data values in the first tuple and give the same set of data values in the second and one of the attributes has to be unique then the condition, then the constraint fails and in turn insert also fails. So as long as the unique construct is not violated, insert will just insert multiple tuples into the table. Insert also fails if referential integrity is violated, this if your DBMS supports it of course that is if I try to insert an employee number as manager where which refers to a manager entity which does not exist, referential integrity fails. 

Multiple tuples can be inserted within a single insert command, first of course by using the select statement or by just giving multiple tuples one after the other separated by commas and each tuple is enclosed within parenthesis.
 
(Refer Slide Time: 00:47:42)

 

Deletion of tuples: How do we delete tuples from a table? Deletion of tuples is very similar to the select statement and has the same structure which is of the form delete from where. It says delete from employee where some condition that is employee number equal to 007, the first one which just deletes one tuple. Note that employee number is the primary key therefore it is unique and therefore it just deletes one tuple. 

On the other hand the second tuple says delete from employee where department number in the set of all departments that are headed by 007, so it can delete possibly more than one tuple in this statement. On the other hand if I remove the condition and I just say delete from employee, it deletes all tuples. Note that deleting all tuples is different from dropping the table. Here deleting all tuples corresponds to truncating the table that is the table exists but it has no tuples in it. Whereas, if I drop the table the table itself does not exist in the database.


 
(Refer Slide Time: 00:48:54)

 

Updation of tuples: How do we modify tuples? You can update tuples by using the update operation and the update operation also has a very simple syntax which is of the form update set where that is update employee as shown in this example here. So update employee set whatever updation I need to make that is set salary equal to salary times 1.1 where reportsto equal to 007. 

Therefore what I am doing here is that for all people working under 007, I am updating their salary by or I am increasing their salary by 10 %, I am giving them a 10 % rise by this update statement here.
 
(Refer Slide Time: 00:49:37)

 
We now come to the last leg of this session where we take up the notion of views or virtual tables. This is again an important concept when designing large databases. View as you can typically or intuitively understand is one particular view of the database that is one particular projection of the database which is suitable for certain kinds of work. That is let us say a HR manager needs to know only the employee that is the HR related details of an employee. He doesn’t need to know the technical details of an employee or the project related details of an employee. 

Therefore we have basically created a view of the employee record for the HR manager which is different from the view created for let us say the project manager. So table or a view is also called a virtual table, this is a table that is derived from other tables. In contrast a table that exists in the database is called a base table. So a view can be derived from either other base tables or other views. Views are need not be stored, it’s not that they are not stored but views need not be stored in the database. That is the data contained in the views need not be stored in the database. But they are typically stored as queries and not as tables and update operations for views are limited, this is as the result of storing them as queries but querying is not, you can query but updation is limited in terms of views.

(Refer Slide Time: 00:51:24)

 

So views can be created in SQL using the create view command. This is again pretty simple construct which is shown in the slide here which says create view and EMD which is the name of the view and there are three fields EMPL, MGR and department. So this is the structure of the view that is the name and the attribute list and this view is created as this query that is whenever I need to compute this view, I just need to run this or execute this query. This query is going to return a table which will populate the data that is required by this view.

What are some of the properties of views? Because a view is stored as a query, a view is always up to date. I don't need to modify a view, when I modify some data elements in my table because there is no data element that is stored in a view. Hence characterization of a view is not done during view definition time that is when I define a view I just leave it like this. When I define a view like this, it is just left like that. It is not characterized, it is not computed. 

(Refer Slide Time: 00:52:06)

 

But this computation is done during the time of query that is when I give a query on a view this characterization is done. Views can be deleted using the drop view command just like we can delete a table. The efficient implementation of a view is a pretty tricky problem and we shall be addressing this view maintenance as a separate session in itself because how do we efficiently maintain a view and execute queries over. 

So that brings us to the end of this this session. So let us briefly summarize or look at the titles of all the different topics that we have studied. We looked into a SQL two standards tables, attributes, values and constraints, entity constraints, foreign key constraints and so on. We looked at several different kinds of select operations select from where and  disambiguation, aliasing, selecting from multiple tables, set operations, multi-set operations, substring operations, arithmetic operations, existence checks and null checks nested queries and aliasing and scope in nested queries and  group by constructs and having constructs and so on.





 

(Refer Slide Time: 00:53:14)

 

(Refer Slide Time: 00:03:01)

 

And then we also looked at SQL statements for insertion, deletion and updation of tuples from database. Finally we looked at the notion of views or virtual tables or rather I should say that we have just scratch the surface of views or virtual tables and saw how we can specify a view in SQL using the create view command. So that brings us to the end of this session. Thank you. 

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 7

         ER Model to Relational Mapping

Hello and welcome to the next lecture in the DBMS course. Until now we have seen two main kinds of data models for representing data or managing data in several different application context like I say whether it is insurance or banking or railway reservations or company databases or so on.
 
(Refer Slide Time: 00:01:18)

 

These two models where, the first was the entity relationship model or what is called as the ER model and the second was the relational data model. We also saw a typical database design process and placed these models into appropriate positions in the process. The entity relationship model or the ER model is essentially meant for human comprehension. 

It basically is meant for creating a conceptual database or conceptual schema or what is termed as a logical schema of the database system and the relational data model is used for the physical schema or something that is implemented in the DBMS.






(Refer Slide Time: 00:01:46)

 

So both are DBMS independent models that is no matter which companies database that you are going to use, you can still use the same ER model for representing your data and even the same relational model for representing the schema that goes onto your DBMS. Now in this session we are going to address one important issue. Now we are going to ask a question, are these two different data models ER and relational model completely independent of each other or are they the same or is there some way I can map between the ER schema and the relation schema without having to break my head too much essentially. That means to say or to put it, to take it to its logical extreme, can I design some tools or can I design some kinds of software that takes an ER diagram of a given system and generates appropriate relational schemata for the system. 

And we have also seen in the session on functional dependences, we have seen how we can optimize a given relational schema up to a certain extent using some kinds of automated techniques. We have seen how to take a relation to a BCNF or third normal form or the fourth normal form and so on. So suppose we want to build a tool to automate this process, we need to be able to first map between an ER database schema and a relation schema and then use techniques from functional dependencies to optimize this relational schema so that we can build a database application around it. 

So what we are going to study today form the underpinnings of what are called as lifecycle tools or database lifecycle tools. There are several different lifecycle tools which provide support for the entire lifecycle of a database systems, starting from the conceptualization of the problem to the actual implementation of the application and maintenance of the database and so on.



 
(Refer Slide Time: 00:04:25)

 

So before we begin let us briefly summarize what we have learned about ER models and relational models. The ER models as we already know is used for conceptual modeling of the database schema, conceptual modeling or to create the logical database schemata. This is meant for human comprehension, this is essentially used to show end users what you have understood about their problem domain. This is a high-level database design and there are no implementation details about that are included as part of the ER model. And of course the ER model is a DBMS independent and it is made of building blocks like entities, relationships and attributes which can be attributed to both entities and relationships.  

In contrast a relational data model is the data model that is most popularly used for physical schema design. A physical schema is the schema that is actually implemented on the computer, therefore the relational data model is meant for or optimized towards machine conception. That is how do we efficiently store data in my database, how do I efficiently search for a given data element, how do I efficiently update a given data element so that it does not create anomalies, how do I efficiently delete data elements again without creating any anomalies and so on.










  
(Refer Slide Time: 00:05:10)

 

And of course the relational data model is also DBMS independent that is no matter what kind of database that you use, you can still use the same data model as far as the database that you are using is a relational database. You can use the same data model to represent your data on the DBMS. Of course reality is quite different from the concept of DBMS independent and some DBMS systems may include more features than traditionally what is supported by the relational model. 

The relational model also supports some kinds of automated optimization techniques which we have seen in the session on functional dependencies where you can optimize a given relational schema, you can reason whether a given relational schema is optimal or not whether it’s going to create redundant data in its DBMS or whether it’s going to create some kind of anomalies during updation and deletion or so on. And, how you can systematically change the database schema without changing the correctness but increase in the overall efficiency in terms of retrieval and updates. And what are the building blocks of the relational model? We have relations which comprises of several different attributes and the notion of keys forms, a very crucial role or place a very crucial role in the relational database model.











(Refer Slide Time: 00:07:23)

 

Let us come back to the ER model and look at some of the notations which we will require if we have to study translation into ER models. The entities are represented using rectangles and a strong entity type that is an entity type which has its own key attribute and which represents a physical or which represents some kind of a logical entity of the of the real life is represented by a rectangle with solid lines surrounded. For example the slide shows this entity type called employee which depicts all objects of type employee which are present in the current system. 

On the other hand we also have what are called as weak entities. Weak entities are those which do not have an existence of their own or without being associated with a strong entity type. The slide shows the example of an insurance record. An insurance record doesn’t mean anything unless it is associated with some person. In a company for example an employee. Therefore when we talk about insurance record we have to say, whose insurance record and so on. So that is the general  idea and more specifically the insurance record entity type does not have any key attribute, it has to be associated with a strong entity type called employee which in turn has a key attribute. 

Therefore such weak entity types are depicted using dashed lines or dotted lines for the rectangle. we then have the relationship type for example a relationship called handles, so employee handles project or something like that which is represented by a diamond and a normal relationship type is represented by a diamond using solid lines whereas, what are called as identifying relationship types. That is the relationship types that identify a weak entity or provide an identity for weak entities by associating them with strong entities, they are shown with double lines in the diamond.



 
(Refer Slide Time: 00:09:36)

 

Entities and attributes are associated or entities and relationships are associated with attributes which are some values in a given domain. Attributes are depicted using ovals and normal attribute or a simple attribute is depicted by an oval with a solid line and key attributes in this example an attribute called pan or pan number which uniquely identifies each income tax payer is shown as a key attribute and it is shown underlined saying that this attribute is a key attribute for this entity type.  

And then there are multi-valued attributes which can have several values for the same attribute. We took an example of the color of a peacock. Now the color of a peacock is actually given by several different colors and all of which in combination form the color of the bird. Such kinds of attributes are depicted using double lines as shown in the slide here. And then there are derived attributes that is attributes whose values can be derived from other attributes and these are shown using dotted lines. We took an example of the age of an employee that is if you know the date of birth of an employee and the current date, we can derive the age of an employee.












 
(Refer Slide Time: 00:10:58)

 

Let us look at some definitions from the relational model. The relational model is based around the notion of a mathematical relation. Now a mathematical relation is set to comprise of atomic values or atomic data values. And what is atomic data value? A data value is called atomic if it cannot be sub divided into smaller values for example the age of a person. 

Similarly each data value is set to reside in a domain, in the ER model a domain is also called a value set which is term that generally used by several people and in the relational model usually the term domain is used which is going to specify the range of values that a particular attribute can take. 

Similarly a relation schema or a relational schema is denoted by a schema name that is in this example is shown by the name R and a set of attributes in this example shown by A1 A2 until An and each attribute has a specific value that lies within the domain specified as domain of Ai.  













(Refer Slide Time: 00:12:13)

 

We have also defined what is known as the degree of a particular relationship. The degree of the relationship is simply the number of attributes in its relation schema. If you remember the same definition of the degree of a relationship also applied to the ER model. That is a relationship diamond can be a binary relationship or a ternary relationship, unary relationship or a n-ary relationship that is it can be associated with 1, 2, 3 or any number of entity types and the slide shows that the relation is actually a subset of the Cartesian product of all of the domains that form the attributes. 

In the relational model the notion of keys play a very crucial role especially we saw in the notion in the process of decomposing relational schema in order to make them normalized or conform into let us say BCNF or third normal form or fourth normal form and so on.
 















(Refer Slide Time: 00:12:59)

 

So let us revisit the notion of keys in a little more detail and keys are again very important when we translate from an ER model to a relational model. We have to be aware which attributes are the key and which attributes are the foreign key and so on. So a key constraint in the relational modal essentially defines the notion of a superkey which is a set of attributes of a relation which can uniquely identify each tuple in the relation that is each instance of the relation and a key or a minimal superkey is something which is minimal in a sense that if you remove any element of the minimal superkey, it seizes to be a superkey anymore. And there is also the well-known entity integrity constraint in the relational model which says that the primary key of a given tuple may never be null.

The primary key is the minimal superkey that is going to be used to uniquely to identify a given tuple in the relation. And we also saw the notion of referential integrity which is again an important issue in the relational data model and the referential integrity constraint says that if a tuple of one relation refers to another tuple of another relation, it should refer to an existing tuple. That means foreign keys that is primary keys of another relation embedded into the tuple of yet another relation should refer to tuples that already exist in the first relation. 











(Refer Slide Time: 15:09)

 

So the foreign key constraints are shown in the slide here that is first of all the attribute of the foreign key or the domain of the foreign should be the same as the domain of the primary key of the other relation and they have to refer to existing tuples in the other relation.
 
(Refer Slide Time: 00:15:27)

 

And we also saw that relations can be or popularly viewed as tables and which is what is a notion used in SQL that is a relation of the form student with three attributes roll number, name and lab can be re specified in the form of a table with the name student and three kinds of columns called roll number, name and lab.
(Refer Slide Time: 00:15:53)

  

So let us now come to the issue of mapping between a given ER model under a relational database model. Now I had said in the beginning of this session, why such a mapping is important. There are several different commercial tools that are available which are called as lifecycle tools of DBMS design. A lifecycle tool provides support in several or in most of the phases of a typical database lifecycle. 

That means the tool should be able to or using the tool you should be able to create a logical schema, talk to your end users saying this is what I have understood by your requirements of your system, these are the different data elements, these are the different functionality requirements that form your system and so on and then using the same tool you should be able to create physical schemata from the logical schema by automatically translating them to whatever extent possible. 

In practice it’s not possible to completely automate this process that is automatically generate a relational model and optimize it. Sometimes some kind of human intervention is necessary, in order when the human knows some domain knowledge cannot be captured into the ER model but there are several such tools, an example is the tool called ERWin from computer associates which provide such a support for automatically translating between ER and the relational model.
 








(Refer Slide Time: 00:17:34)

  

So let us see how we can go about such a translation. The first case that we are going to take is the case of a simple relation or a simple entity. So the slide here shows a simple entity type called department and it has three different attributes department name, department id and manager and the department id obviously is the key or the key attribute of this department. Now given such a relation, it is fairly obvious to, given such an entity type it is fairly obvious to see that it can be translated into a relation of the type department that is also shown in the slide here just below the figure.

So this ER model can be translated into such a relation where the name is called department and which has three different attributes department id, department name and manager. And also note that the key attribute is retained that is the department id which is the key attribute of this entity here becomes the primary key of the relation, that’s found. 

So this is straight forward. That is as long as we have a simple entity type with simple attributes, note that the attributes are also simple, there are no multi-valued attributes or composite attributes and so on. And it can be translated in a straight forward fashion to the relational model.  











(Refer Slide Time: 00:18:57)

 

What happens if we have a composite attribute? Remember that a composite attribute is something that is made up of sub attributes. A composite attributes is different from a multi-valued attribute that is a multi-valued attribute is something which can have many values for the same attribute, the color of a bird can have several different colors. On the other hand a composite attribute is made up of two or more other attributes each having its own domain. For example the slide shows a composite attribute called department id for the same example of a department entity type, so the department id is a attribute here which has which is a composite attribute which in turn is made up of two other attributes called location id and region number so on. 

Now a location id could have a different domain, let us say location ids are given alphabets like a b c d and so on and region number are given numbers 1, 2, 3, 4 and so on. So both of them may have different domains and they combine to form the attribute called department id which in turn is also the primary key of this department. Therefore we are considering two different aspects here, one is how to deal with composite attributes and the second is what happens if the composite attribute is the key attribute of the entity type. 

So the slide here shows the example of and shows how we can translate this into relational model. Firstly, the name department of the entity type becomes the name of the relation called department. And all the other simple attributes are retained, department name is retained as department name, manager is retained as manager and only here for the composite attribute, the department id never appears here. It’s just that all the simple components of the composite attribute are straight away loaded into this relation. That is location id comes here and region number comes here and in fact if either of these two let us say location id or region number is again a composite attribute and it has some more attributes, just take all the simple components of the composite attributes. 

So don't take region number and just take whatever is the simple component of this region number and add it to the relation here. And all of these simple components which form the composite attribute which is the key becomes the primary key. That is the primary key here is a composite key made up of two or more different attributes which combinedly identify or help in identifying a tuple of the relation.  

The next example that we are going to see is the example of how to map relationships. First of all let us look at the following relationship that is shown in this slide here. What characteristics can we ascribe to the relationship that is shown here. Firstly, we notice that the relationship here is a one is to one relationship that is one employee is associated with one insurance record or rather the other way around in this case, that is one insurance record is associated with one employee and have a look at the association as well. 

(Refer Slide Time: 00:21:51)  

 

The association or the relationship type is an identifying relationship type that means the relationship type called insurance details shown in the slide here is used to identify or provide an identity for the insurance record by associating it with an employee. And also the insurance record has a total participation in this relationship that is insurance record has no existence without this relationship. 

Now how do we translate such entity types? So essentially the idea here is how do we translate insurance record into the relational model? For weak entity types the translation is shown here in the slide below. Just create a relationship or a just create a relation of the same name as the entity type but since it does not have a key to because weak entity types do not have key attributes. Since it does not have a key attribute, use the key attribute from employee with which it is associated with and take that key attribute and make it into the key attribute of insurance record. 

However note that since pan number here is also the primary key for employee, this has to be made as a foreign key of insurance record that means whenever we are updating or altering the table, we have to use the cascade option. Whenever let us say the employee type is updated or deleted that means to say that if the employee relation is deleted from the database, this has to in turn delete all the insurance record relations from the database itself because insurance records do not have any existence without the employee records. 

So the three step here in order to translate a weak entity type is to first identify or is to first locate the identifying relationship and see which entity type is this weak entity type associated with and use the primary key of that entity type as the key for the weak entity type or the record of or the relation for the weak entity type and make it into a foreign key of this entity type and use cascade options whenever updations or deletions or performed on the strong entity type.  

(Refer Slide Time: 00:25:20)

 

Let us move on with translating relationships. So how do we translate, let us take the simplest form of relationship again the 1:1 relationship. We saw what happens or how do we translate 1:1 relationships, when weak entity types are considered. Now let us consider an entity type which is not weak but still is involved in a total participation, this is shown in the figure here. The figure shows a relationship type called managed by which relates two different entity types that is a department and manager and there are attributes, relevant attributes are shown for each of them. That is the department has a key attribute called department id and a manager has a key attribute called employee number and the relationship itself has a key attribute called secretary that is a secretary is assigned for a department that is managed by a manager. 

That is the secretary attribute does not have an existence without an existence of this relationship that is if a department is not managed by a manager then there is no secretary that is associated with this. 
Now how do we translate this? The translation is again shown in the slide below. So first create an entity type or create a relationship, create a relation called manager. Let me repeat this again, for this relationship create a relation in the RDBMS model called manager with the following attributes. Now you might be wondering why should we create a relation called manager, why not department. Now let us think about it a little further. See in this slide here that manager is a strong entity type, it is not a weak entity type. However it is involved in a total participation in this relationship type. 

What is a total participation? The total participation is that the entity type does not have any existence without being participating in a relationship type of this kind. So what it essentially means here is that a manager has no existence that is a manager would probably be just an employee, so a manager would have no existence unless here she is associated or is given a department to manage. It is the entity type that is involved in the total participation is taken as a primary entity type or the base entity type, primary relation called manager and then employee number becomes the key here that is the key for manager and the department id which is the primary key for department becomes a foreign key in manager and whatever attributes are associated with the relationship itself become attributes of this relation here that is of the manager relation here. 

So therefore the manager relation has a primary called primary key called employee number and a foreign key called department id. Note that this makes sense, when we note that manager does not have any existence without this relationship. That is referred to the problem of referential integrity in relational data model. What is the referential integrity stipulate? Whenever a foreign key refers to a tuple in another relation, the tuple should exist that is it should refer to an existing tuple in the other relation. Now if we had department as the base entity or the base relation here, we cannot use employee number as a foreign key because the manager relation won't even exist before this relation that is managed by is formed. 

On the other hand department has an independent existence without whether or not a manager is associated with it. Therefore that forms a rational behind, why we choose the entity type which is involved in total participation as a base entity type for the translation.  














(Refer Slide Time: 00:30:03)

 

So let us summarize this previous slide once again. So in any 1:1 binary relationship between types S and T, choose one of them as a base relation. In case one of them is involved in a total participation choose that as the base. If neither department nor manager where to be involved in total participation, it doesn’t matter which you are going to choose as the base relation. Include the primary key of the other entity type as a foreign key in the base relation and include any relationship attributes as attributes of the base relation.  

(Refer Slide Time: 00:30:42)

 

Consider the example shown in the slide here. What happens if in a 1:1 relationship both entity types that is both entity types that are participating in this 1:1 binary relationship are involved in a total participation. Take a look at the slide here. The slide shows two entity types project and consultant and each project is uniquely identified by a project id and each consultant is uniquely identified by his or her pan number. And there is a relationship type called consultation which has its own attribute called secretary and it’s a 1:1 relationship and both of them are involved in a total participation that is a project has no existence unless it is being consulted by a consultant and a consultant has no existence unless here or she is associated with a project. So neither of them will have independent existence without the other. 

In such cases we cannot identify any relation as the base relation. If we identify project as a base relation and try to use pan number of the consultant as the foreign key then referential integrity could be violated. It’s the same in the other way around as well. If we use consultant as the base relation and try to use project id as the foreign key, again there is a chance of violating the referential integrity. In such cases the simplest way is to take the relationship type, in this case the consultation as the base relation. That is form a relation called consultation and use project id and pan as the primary key of consultation and then all of the attributes from both of them will become attributes of this relation. 

(Refer Slide Time: 00:32:40)

 

So in case both entities in a 1:1 binary relationship are both in total participation then we merge both of the entity types into one usually in the name of the relationship that is in the name of the relationship called consultation in the previous example. Now let us see how do we map one to N relationships. What is a 1: N relationship? That is N entities of one of the entity types could be associated with one entity of the other entity type, that is it form some kind of a tree relationship that is one entity being associated with N different entities of the other type. 

So the slide here shows such an example that is employees works in department that is employee is an entity type, so N different employees can work in one department that is one department may have several employees but each employee is associated with only one department. And of course there are keys called employee number for employee and department id for department. The slide also shows how we can reduce this to a relation. The simplest way is to take the entity type on the N side of a relationship. 

(Refer Slide Time: 00:33:02)

 

So in this case the employee, so take this as the base that is translate it into a relation called employee and the primary key of employee becomes the key of the employee entity type here and the department id becomes a foreign key here. 
(Refer Slide Time: 00:34:19)

 
So this is as simple as that. That is for each binary one is to N relationships identify the relation S that represents the entity type on the N side. Why is this so? Because each entity type on the N side uniquely identifies a department that is uniquely identifies the entity type on the other side. Therefore we can use the primary key of the entity type on the other side as a foreign key in the base relation. Therefore use this as the base relation and create a relation including the key of the other entity type as the foreign key.  

(Refer Slide Time: 00:35:03)

 

How do we map M N relationships? Now what is an M N relationship? An M N relationship essentially says that M different entities of the first type can be associated with N different entities of the second type. Therefore there is no unique identification that is given an employee in the previous case, one could uniquely identify the department with which the employee is working in because each employee can work in at most one department. 

on the other hand here let us say a relationship called deputed to, so an employee could be deputed to several departments let us say. So M different employees can be deputed to N different departments and of course employee has employee number as the key and department has department id as the key and so on. And deputed to also has an attribute called record number which maintains a record of which employee is deputed where and so on. 

So in order to translate such relationships, note the steps that are shown in the slide here. There are three different relations that are formed, one is the employee relation that is one of the entity types of this relationship type. So the employee relation is formed with employee number as the primary key and all the other attributes that form the employee entity type. Similarly department relation is found with department id as the primary key and all other relations and then a separate relation is created for the relationship type itself. 
So deputed to becomes a separate relation by itself and then uses employee number and department ids as foreign keys and also as the primary keys of this relation and whatever attributes that belong to the relation becomes part of or belong to the relationship type becomes part of the relation that is created here. That is the record number attributes become one of the attributes of deputed to relation. Note that we cannot move this record number that is the attribute of this relation either the employee or a department because it does not uniquely identify either employee or department. 

Each employee could be associated with N different record numbers because they could be associated with N different departments and similarly each department could be associated with M different record numbers because M different employees could be working in that department. So this slide shows summarizes, how M is to N relationships are translated. In a M: N binary relationship, it is not possible to collapse the relationship into one of the entity types because neither of the entity types uniquely identifies the other entity type. 

Therefore a separate relation is required and usually this is in the name of the relationship type itself. So a separate relation is required in order to complete the mapping and of course the cascade option should be used whenever updates are performed on any of the relations pertaining to the entity types.

(Refer Slide Time: 00:37:57)

 

That is whenever the entity type called employee or department is updated or deleted then these changes should be cascaded so that they are reflected in the deputedto relationship as well. That is if employee entity type is deleted then of course the deputed to relationship should also be deleted. Now one might ask the question is it possible to use the strategy that is use the relationship type as the base relation rather than any of the participating entity types. 

Can we use this strategy for mapping 1:1 and 1: N relations as well because M is to N is simply a generalization of 1:1 and 1: N relationships. So of course it is possible that is take the example of employee works in department that is N different employees working in one department. We can still create a separate relation called works in where it can use the employee number and the department id as the foreign keys of this relation. However it just creates an extra relationship or extra relation in the database that is totally unnecessary but this is sometimes actually attractive to use than collapsing the relationship into one of the relations especially where, especially if we have to avoid null values.

(Refer Slide Time: 00:39:06)

 

That is especially if we have to, especially if we have cases where there are some employees who do not work in any department. if N different, if an employee can be associated with at most one department it means that an employee can also be associated with zero departments. So in that case the department id field of employee would be null. It does not violate referential integrity because remember that referential integrity says that a foreign key should refer to an existing tuple or else should be null. Therefore it does not violate referential integrity but creates a lot of null values in the database schema, in the database itself. 

So if we have to avoid null values, it is actually preferable to use the relationship type as the base relation when performing the translation. How do we map multi-valued attributes? We have seen how to map composite attributes and a simple attributes and keys and so on but what happens if there are multi-valued attributes. Composite attributes are different from multi-valued attributes in the sense that each of them can have several different domains that is it is just a combination of several simple attributes. so we just open up the combination when we are translating a composite attribute and then include all the simple attributes that form part of the composite attribute. 

(Refer Slide Time: 00:41:11)

 

On the other hand a multi-valued attribute is not composition of several sub attributes instead it is an attribute that can take on several values instead of one value and the example, the slide shows is that of a bird. So a bird has a multi-valued attribute called color. So what is the color of this bird? A bird could have several colors it need not have just one color and of course there is a primary key called species which identifies each bird uniquely. 

So in order to translate multi-valued attributes, take a look at the lower half of the slide which shows two different relations which make up this translation. The first relation shows a relation called bird with species as a primary key and all other attributes except the color attribute, all other attributes of the entity type called bird. 

And then a separate relation is created called bird colors where species and color are both included and are both of part of the key that is combinedly define the key of this bird. therefore we say that birds species eggs has color Y, eggs has color Z, eggs has color A and so on. So the color attribute may be repeated in several or several tuples or rather the species attribute may be repeated in several tuples, once for each different color that the bird can take and both of them that is species and color become the primary key for bird colors.  









(Refer Slide Time: 00:43:34)

 

So for each multi-valued attributes of a given entity type, we have to create a separate relation that has a primary key of S paired with all possible values that the multi-valued attribute can take. and of course the cascade options should be used for referential integrity on the bird relations that is whenever the bird relation is deleted or updated, the corresponding changes has to be made in bird colors as well.   

How do we map n-ary relationships? Until now we have been looking at binary relationships, what happens when there are n-ary relationships and different entities forming part of the relationship. The slide here shows such an example that is the standard example of suppliers supplies part to project. So there is a supplier who is uniquely identified by the supplier or the sales tax registration number or something STReg number and there is a project that is uniquely identified with project id and there is a part that is uniquely identified by part id and then the supplies relation which relates all of these three different entity types.

So the simple way of translating this is to use a separate relation called supplies as the base and of course separate relations each for supplier, project and part with their corresponding primary keys and the supplies relation which has the primary keys of each of these relations has the foreign key of this relation.  









(Refer Slide Time: 00:44:05)

 

(Refer Slide Time: 00:45:15)

 

So for each for each n-ary relationship of any type R where n is greater than 2, we have to create a new relation S to represent this relationship, to represent this relationship type R. and of course the primary keys of the participating relations become foreign keys in this new relation and the cascade option should be used for all of the relations that correspond to the entity types that participate in this relationship type. What happens if one of the relations in an n-ary relationship type is a weak entity type? That is let us say part is a weak entity type, there is no existence for part unless it is associated with a supplier supplying it to some project.

In that case we have to identify, we have to first identify the entity type which gives an existence to part and the part relation here has only foreign keys it does not have any primary keys but the supplies relationship does not change. That is it doesn’t have any part id, it just has a, it simply has the supplier primary key and the project primary key without the part primary key. Therefore we get two different relations that have foreign keys and they don't have their own primary keys.  

(Refer Slide Time: 00:46:56)

 

So that bring us to the end of this session that talked about, that gave a, that talked about how we can map ER models into relational database models using several different rules. and of course this is not a comprehensive set of rules because there are several other sets of rules used for example in derived attributes or enhanced ER models like generalization and specialization which are not covered here but all of them in totality are used to create the basis for any kind of a tool, software tool that can translate between given ER schema and its corresponding relational schema. 

So this slide shows a summary of each of this mappings that is it gives the set of thumb rules saying if this is what is given in the ER model then what happens in the relational model. So in an ER model if an entity type is given then a corresponding entity relation that is a relation in the name of the entity type is created. If a 1:1 or a 1: N binary relationship is given then we create corresponding foreign keys from the N side to the one side or from the weak entity type to the strong entity type in this relation. So we create a relation and create appropriate foreign keys from them. If a M: N relationship type is given as shown in the slide here then we create a relation with the name of the relationship type, so there is the within quotes which shown as relationship relation with two foreign keys that is one for each entity type that participates in this relation.

  

(Refer Slide Time: 00:46:57)

 

If an n-ary relationship type is given, it is still the same strategy that is we create a relationship relation with N different foreign keys that is one for each entity type or rather N different foreign keys as long as these entity types are strong entity types. If there is a simple attribute in a ER model that simply becomes an attributes in one of the relations in the relational model. if it is a composite attribute in the ER model then it becomes the set of simple attributes that is you take the simple part of all composite attributes that is just go on finding the simple attributes that form the composite attribute and then make all of them as part of this relation.  

(Refer Slide Time: 00:49:47)

 

If it is a multi-valued attribute then we need to create a separate relation and a foreign key that is you have to associate the primary key of the base relation with each possible value of the multi-valued attribute. If it is a value set it becomes a domain, value set in the ER model it becomes the domain and a key attribute in the primary in the ER model will become either a primary or a secondary key in the relational model. So that brings us to the end of this session.

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 8

Functional Dependencies and Normal Forms

Two different kinds of database models now, when we are talking about how to design a database system. The first data model that we saw was the entity relationship model which is especially used for building conceptual schema.
 
(Refer Slide Time: 00:01:31)

 

So as we have defined a conceptual schema is something that is meant for human consumption that is it is meant for communication with end users so that they can understand what the database designers have really understood about their domain, what kinds of entities exist, what kinds of relationship exist and so on. But an entity relationship schema is not really the best data model for storing data in a computer. And for storing data that is the internal schema or the physical schema it is usually the relational model that is used and the relational model in some sense has some kind of common terms with the entity schema like we have seen the concept of keys, foreign keys and referential integrity and so on which hold both for entity schema and the relational schema. 

However the relational schema is mainly meant for the physical schema or the internal schema while the conceptual schema is meant for human consumption, while the physical schema is meant for the computer. 

Now usually what happens in a database design process is that we use some kind of tools or some kind of case tools to build an ER model or of the information system that we are trying to design and there are number of automated techniques by which or number of automated tools which can take a ER representation of a schema and then convert it into a relational schema. However while this is automatic, this need not always result in the most optimal form of the relational schema. And what do we mean by optimal? We are going to try to formulize this notion today and try to see which kinds of relational schema or schemas are better than which other kinds and are there some kind of techniques, formal techniques that I can use by which without trying to understand the semantics, just given a relational schema doesn’t matter in what context can I try to optimize it into a fashion that can help in efficient storage and retrieval of data.

So today in this session we are going to look at an important concept of relational database design namely the idea of functional dependencies. The notion of functional dependencies is fundamental to the design of different normal forms which we are going to see in a relational database schema. So what are functional dependencies? Functional dependencies are a frame work for systematic design and optimization of relational schemas. Of course there are many more nonsystematic options for optimization. For example if we know something extra about the operational domain or the specific database in use, we can up always optimize something more than what we can do with the relational, with functional dependencies. 

(Refer Slide Time: 00:04:20)

 

However functional dependencies are some kinds of techniques that are database independent and can be used in a formal fashion in order to achieve some level of optimization or some level of efficiency in terms of database design. We are going to see how we can define keys. We have looked at the notion of super keys, primary keys and so on. How we can define the notion of keys using functional dependencies? In fact functional dependencies are more generalization or is a generalization over the concept of keys and they are crucial in obtaining the correct normalized schemas when we are trying to design database systems.
 
(Refer Slide Time: 00:05:33)

 

So let us first define the notion of a functional dependency. Let me first take the definition here, the formal definition here and try to explain it in formal terms. The definition reads in any relation R, if there exists a set of attributes A1 to An and an attribute B such that if any tuples in the relation have the same value for A1 to An then they also have the same value for B. So essentially what it means? This is a formal way of saying that these set of attributes A1 to An uniquely determine the value of B. That means if two or more tuples have the same value of A1 to An, it’s not possible essentially that means to say that is they should have the same value of B. They can't have different values of B because these sets of attributes A1 to An uniquely determine the value of B. 

So this is called a functional dependency and a functional dependences is written as shown in the slide, here it is written as A1 A2 extra An and there is a right arrow to B. So an important thing to note here is that functional dependencies define properties of the schema and not of the tuples in the schema. What is that mean? Essentially this kind of dependency that exists between A1 to An and this attribute B should be a property of the entire set of tuples that are there in the schema. For example I might say that the employee number uniquely determines the name of the employee in any relational database schema. 

However we cannot say that let us say the employee name uniquely determines the age of an employee because there could be two or more employees having the same name but different age. But it could well be possible that there is certain employee with very rare name and there is nobody else who has that name and therefore for this particular employee, the name uniquely determines the age, uniquely determines his or her age. But this is not a property of the schema in itself, this is a property of that particular tuple of this particular employee who has this rare name so that there is nobody else with that name therefore we can uniquely identify the person and the age of that person. So when we are talking about functional dependencies, we are talking about properties of the schema which holds for all relations or all tuples in the schema.
 
(Refer Slide Time: 00:08:21)

 

In any given schema there could be many functional dependencies, let us say the set of attributes A1 to An can uniquely define, it’s also called uniquely determine B or defines B or so on. So A1 to An can uniquely determine B1 or B2 or B3 until Bm so this is written as shown in the slide here that is A1 to An and right arrow B1 to Bm. Now why is this called a functional dependencies, what is so functional about functional dependency? If you are familiar with the mathematical notion of a function which is a special kind of mapping or a special kind of relation between sets, you can see here that the notation that is used for example A1 to An or uniquely determining some attribute B is precisely the notation used for a function. 

It basically determines because A1 to An uniquely determines a particular attribute B, it acts as a mapping or it act as a function which maps this set of attributes A1 to An uniquely to each B that it defines. This precisely acts as the mathematical notion of a function. However there need not be any computation that determines this unique definition. For example the employee number may uniquely determine name, however just given the employee number I mean I won't be able to compute, I won't be able to determine what is the name. It’s just that each employee number has a different or identifies an employee with a different name or identifies uniquely each employee. However, it is not possible to determine what is the name of the employee given just the employee number, we still have to access a database to do that.
 


(Refer Slide Time: 00:10:24)

 

So let us revisit the notion of keys now when we say functional dependencies. As you might have noticed the notion of functional dependency is actually a generalization over the notion of keys. That is in the previous slide here if A1 to An uniquely determine the set of all attributes which form the relation let us say A1 to An combined with B1 to Bm forms the entire relation. Then we see that A1 to An is nothing but a key for this relation that is for the entire relation comprising of A1 to An and B1 to Bm and similarly minimal super keys or candidate keys can be defined analogously. 

Let us take some examples now. Consider a relation of the form that is shown in the slide here. The slide shows a relation called movies which has the following set of attributes, it has title, year, length, film type, studio and star. So, going by the standard definitions of what is a movie and what is title and what is the year of its release, the length of the movie and so on. We can from common sense reasoning, identify some kinds of functional dependencies.














(Refer Slide Time: 00:11:11)

 

For example given the title of a movie and the year in which it was released, I can probably uniquely determine what is the length of that movie and similarly given there is a bug in the second slide here, second functional dependency here. Given a title and the year of release of a particular movie, I can probably determine what is the film type. Given a movie of a particular type, of a particular name and the year in which it was released, I can probably say well this is an art movie, this is a commercial movie, this is social documentary, this is a comedy tragedy whatever.

Similarly I can probably also say title, year uniquely determines say film studio. However title, year may not uniquely determine the star that is the actors who star in that movie because there could be more than one actor who have stared in that movie and you cannot uniquely determine given the title and the year you cannot uniquely say well this is the movie star who acted in that movie because it need not be complete, there could be other stars as well.  














(Refer Slide Time: 00:13:03)

 

So let us see what kinds of properties we can identify about functional dependencies and what we can do with these properties. Note that if in any given relation R, if I have a functional dependency of the form A to B where A is a set of attributes. Now I am using the term A that is without a subscript to determine actually a set of attributes. So if the set of attributes A defines a set of attributes B and the set of attributes B uniquely define another set of attribute C then we can easily identify the functional dependency saying A also defines C. So there is an example here, if the employee number is given in such a way that it also defines the job of the employee. 

Let us say all employees who do supplies work are given numbers between 100 to 200 and all employees who do administrative works or a clerical job is given numbers between 201 to 300 and so on. So employee number suppose uniquely defines job and let us say job also uniquely define salary that is every employee of a particular job has the same salary. Then we can easily say that the employee number actually defines salary that is given just the employee number, I will be able to determine what is the salary for that employee. 

Some more definitions. Suppose we have two sets of functional dependencies A and B. Now again A and B are sets here and we have two sets of functional dependencies A and B and that is A defining B and C defining D. Now these functional dependencies are said to be equivalent, if the set of all relations that satisfy the first functional dependency is the same as the set of all relations satisfying the second functional dependencies. Similarly a small generalization over this definition, we say that a functional dependency S follows another FD called T, if the set of all relation instances satisfying T also satisfies S.



(Refer Slide Time: 00:14:44)

 

So we will take up examples here, the follows property of a functional dependency is extremely important by which we will be able to compute what is called as the closure of a functional dependencies. That is suppose I give some kind of functional dependencies saying A defines B. What else can I say about that? What else can I infer from this functional dependencies is what we are going to take up. Before we move on to inferring functional dependencies, we need to complete a few more definitions. The next definition that we are going to take up is the notion of a trivial functional dependency. Now a trivial functional dependency may look of course trivial but it is quiet important when we are trying to define let us say normal forms.

(Refer Slide Time: 00:16:07)

 
Now have a look at the example in the slide here. This slide shows a functional dependency of the form title, year defines title which is obviously true. Every attributes defines itself that is every attribute uniquely identifies itself, therefore any functional dependency in which the right hand side is contained within the left hand side. That is the antecedent or the right hand side of a rule is a subset of the left hand side of a rule then it is said to be a trivial functional dependency. 

If there is at least one element in the right hand side which is not part of the left hand side then it is called a non-trivial functional dependency and usually we are interested in what are called as completely non-trivial functional dependencies which says that none of the elements in the right hand side belong to the left hand side. We will see that this notion of triviality is not so simple because essentially when there are circular dependencies, it becomes quiet tricky to handle functional dependencies which are removed triviality from functional dependencies. 

We now come to an important aspect of functional dependencies, the notion of closure of FDs. What do we intuitively understand by the term closure? If we are familiar with the notion of closure and say discrete mathematics, you would probably recall the definition that the closure we say that a particular algebra is closed when I say, when I take certain elements from or certain operands from a set that defines the universe and then perform an operator that performs some kind of a function on these operands and the result of this function also goes back into this set.
 
(Refer Slide Time: 00:17:42)

 

For example if I take two integers and add them, the result is again an integer. If I take two integers and subtract them, the result is again an integer. Now this is what is called as closure that is I take certain operands or take certain elements from a universe, perform a function on them and the result of that function also belongs to the same universe. So let us define the function, the notion of closure on functional dependencies in an analogies fashion. The closure of functional dependencies defined by A that is A is a set of attributes in any given relation R is the set of all attributes that are eventually defined by A. That is how we can eventually define or uniquely determine what all attributes can be uniquely determined eventually by A. 

For example let in a given relation, let the set of attributes A uniquely define a set of attributes B. Similarly let the set of attributes B uniquely define two other sets of attributes C and D. Now let the combined sets of attributes B and D uniquely determine another set of attributes called C. Then we say that the closure of A contains all of these attributes that is A union B union C union D union E that is all of that and actually because all of these attributes can be eventually defined from A. From A we can eventually define B, from B we can uniquely define D and combining B and D we can uniquely define E. So all of these attributes contained in all of these sets can be eventually uniquely defined by A. 

What about elements that lie within A, can they be uniquely define by A? The answer is yes. This is because of the trivial property of functional dependencies that is every attribute in A uniquely defines itself which is a triviality rule. however closure also just A union B union C union D union E that is shown in the slide does not complete the closure of A.

(Refer Slide Time: 00:20:52)

 

If for example there is a subset of A or subset of any element of closure of A and if that subset of attributes defines some other sets of attributes F such that F is not part of the closure of A then we also add F to the closure of A.   




(Refer Slide Time: 00:21:13)

 

So, let us first see look at an algorithm by which we can compute the closure of a functional dependencies then we look at a few examples of closures. So given a relation R and a set of attributes A, how do we compute closure of A? So initially we start with the set saying the closure of A equal to A that is equal to all elements of A. This is true because of the trivial functional dependencies, every element of A trivially defines itself. Now for every A prime that is a subset of A, if there exists a functional dependency of the form A prime defines B and B does not belong to A that is B is a set of attributes which does not belong to A as of now then just add B to the set of elements in the closure of A and repeat step two until no more attributes can be added to closure of A.   

So the closure of a set of attributes A is also denoted by A with a superscript of plus that is A plus and also note that suppose we run this algorithm and A plus contains the set of all attributes of R, what can we say about the set of elements in A? A is of course a super key of R because using A, we can eventually uniquely define all elements of R. So we now come to the next property of functional dependencies, the notion of inferring functional dependencies or following that is how do we find out functional dependencies which follow from one another.
 










(Refer Slide Time: 00:22:53)

 

Given a set of functional dependencies and given no more knowledge about the domain in which a relation exists, what can we infer about any more functional dependencies that exist in R. So, we have already seen one rule for inferring which is a transitivity rule which we will revisit again now and before that note another rule that is shown here. 

Suppose A B C and D are sets of attributes of R such that the following functional dependencies exist that is A uniquely defines B, B defines C and C defines D. Now   based on the closure property and transitivity, we can easily say that A defines D. However there could be some elements of D which are actually contained in A that is D need not be completely disjoined from A. So let D subscript A that is D A be the set of all attributes in D such that they belong to A that is they are actually a subset of A.

Now take away these attributes that is let D prime be defined as D minus D A that is shown in the slide here. Once we do this, we see that we have actually inferred a non-trivial functional dependency. That is A defines D prime that is we started by saying that because of the transitivity rule A defines D. However there is some triviality that is there in this definition and once we remove that triviality, we have actually encountered or we have actually inferred a new functional dependencies which is non-trivial that is which is of the form A defines D prime. So functional dependencies which are specified or which are given to begin with are called stated functional dependencies and FDs which are derived are called inferred functional dependencies.
 






(Refer Slide Time: 00:25:18)

 

Now, given a set of relation, suppose we have a set of functional dependencies that are stated. This set of functional dependencies is called the bases of the relation. For example in the movie relation that we saw where a movie is defined by title, year, film type, star and so on. The functional dependency is the title and year define the length of a movie was given. Similarly the title and year define the film type was also given, so all these functional dependencies that are given are called the basis of the relation. Now if the basis of functional dependencies are such that no subset of this basis is also a basis. That is none of these functional dependencies that form the basis can be derived from one another then it is said to be a minimal basis for the relation.

(Refer Slide Time: 00:26:18)

 
So how do we go about inferring functional dependencies that is computing the closure or   getting inferred functional dependencies and so on. How do we go about inferring functional dependencies, given a basis that is given a set of functional dependencies. For this, there are set of axioms or set of rules that define how functional dependencies behave. These are called Armstrong's axiom which are quiet useful when we are talking about properties of functional dependencies. 

The first property is the notion of reflexivity which is like saying, which is easily obvious based on the triviality rule that is if set of attributes B is a subset of the attributes A then A functionally defines B. For example, theater and year or rather the title and year of a movie functionally defines title. That is if I have a set of attributes, these sets of attributes functionally define every possible subset of this attributes.   

Similarly, the second role is that of argumentation that is if I know that there exists a functional dependency of the form A defines B that is theatre, year defines length. Then I can add a set of attributes to both sides of the functional dependencies without violating the dependency. For example I can say that if title and year of a movie determines the length of the movie then I can add an attribute called star or studio to both sides of the functional dependencies without altering the semantics. That is I can say title, year and studio of a movie uniquely determines the studio and the length of the movie.  

Also note that I need not add studio to the right hand side because it forms a triviality rule that is there is some triviality that entails from adding the same attribute in both sides. I can as well add, I can just add a new attribute just to the left hand side and still the functional dependency holds. Take the same example again. Suppose I say title and year uniquely determines the length of a movie. Then I can as well say that the title of the movie, the year of the movie and the studio in which the movie was shot uniquely determines the length of the movie which is fine because adding new information to some set of attributes that uniquely determine something does not alter the dependency. 

Similarly the last rule is that of transitivity which we have already seen that is if A uniquely defines B and B uniquely defines C then we can infer that A defines C. The next concept that we learn here is the notion of projecting functional dependencies. What happens if I compute a project operation on a relation. You know what a project operation is in standard relational algebra. 

A project operation takes certain columns or certain attributes of a relation and produces a new relation out of these attributes that means it actually throws away certain attributes from the original relation. Now suppose let R be a relation and F of R be the set of all functional dependencies in R. Now suppose I project another relation S from this new relation R by throwing away certain attributes. Now what can we say about F of S? What kinds of functional dependencies exist in F of S? 




(Refer Slide Time: 00:29:23)

 

Obviously for example if I say that from the movie relation, if I throw away the year attribute of this relation then obviously I cannot have the functional dependencies which says title, year uniquely determines length because the year doesn’t exist in the new relation at all. Therefore at this point, at the first step we can say that functional dependencies in the new relation S should satisfy the following properties. What are these properties? 

Firstly they should follow from the functional dependencies in R that is we should be able to infer them from the functional dependencies that existed in the earlier relation. Secondly and, which is also quite obvious that they should involve only attributes of S. Obviously they cannot involve attributes of R which don't exist in S. A functional dependency only has to involve attributes that exist in the current relation. 

So how do we compute the functional dependencies on a projection? Now consider this example rather than going through a formal set of rules. Let me explain it by an example. Now given a relation R containing the following attributes A B C and D and let us say the following functional dependencies that is A defines B, B defines C and C defines D.
 










(Refer Slide Time: 00:31:24)

 

Now suppose S is projected from R and B is thrown away from R in getting S. So S has just three attributes A C and D. Now what should be F of S? That is what should be the set of functional dependencies that lie in S. Obviously we cannot have A defines B because B doesn’t exist at all in this relation and we also cannot have B define C because again B doesn’t exists in this relation. So but can we say anything else about these dependencies.   

Now going back to R, let us see what happens when we compute the closure of A. Let us compute the closures of each attributes. So, I have not included the trivial functional dependencies in computing the closures that is firstly we start from A defines A obviously then we have a given rule which says A define B, a basis rule from which we can infer that A define C because there is another basis rule which says B define C and from which we can still infer A defines D because there is another basis rule called C defines D.  

So in R, the closure of A is A defines B, A defines C and A defines D. In S, all we need to do is take away all these or throw away all this functional dependencies that contain attributes which do not exist in S. So here A defines B is one such attribute, is one such functional dependencies. It contains B which does not which is not part of S. Therefore the set of, the closure of A in S is essentially A defines C and A defines D or A C D.   


Similarly the closure of C in S is C defines D that is C, D in this case and the closure of D is just D. Now since the closure of A contains all attributes of S that is A C and D, we don't need to compute anymore closures that is we don't need to compute the closures of A C or A D or A C D because we already have all attributes of A, we already know how to uniquely determine each attributes of A.   

Therefore the functional dependencies that exist in S are A defines C, A defines D and C defines D and of course there are trivial functional dependencies like D defines D and C C defines C and A defines A which are not shown as part of this set here. So a simple way to compute the closure or compute functional dependencies in a projection is to first compute the closure of the set of all functional dependencies in the original relation and then throw away everything which contains attributes that do not belong to the new relation.
  
(Refer Slide Time: 00:35:10)

 

So what is a use of all this functional dependencies and closure and transitivity and axioms and so on. These sets of underpinnings are used for normalizing or optimizing relational schemas for better performance. What do we mean by optimizing or what are we optimizing against, what is the property that we are trying to remove, what is the undesirable property that we are trying to remove. This undesirable property is the property of redundancies in relational schemas. If a relational schema is badly designed then it is going to contain several redundant information which results in a number of anomalies. 

What are these kinds of anomalies and what kinds of redundancies are these? Let us have a look at them. Redundancy of course means that some kind of information is repeated across different tuples. The same information saying the title is this one or the year is this one or the star is that one and so on. 

So if the same information is repeated across several different tuples then we encounter two kinds of anomalies. What are these anomalies? The first anomaly is that of updation. Suppose I need to update a data element using let us say the update clause of SQL. I cannot update in just one tuple, especially if this particular information is repeated across several different tuples. If the title of a movie is repeated across several different tuples in the database and later on I see that the title of the movie is entered incorrectly, there is a spelling mistake. So I need to change the title of the movie in every tuple there in which it occurs. 

Similarly, the notion of deletion anomalies. Suppose I delete a tuple which suppose I need to delete a tuple about a particular movie, given a particular title. I need to delete all tuples which contain this title as its attributes. So I need to essentially search the entire database and delete it in several different places, otherwise it again creates anomalies. So how do we design relational schema, so that we can remove redundancies and remove these kinds of anomalies.
 
(Refer Slide Time: 00:37:37)

 

Now have a look at the movie relation once again. The movie relation has the following attributes that is title, year, length, studio and star and we also have the functional dependencies which says title and year uniquely determine length and title and year of a movie uniquely determine the studio in which the movie was shot. However title and year does not uniquely determines star because there could be more than one actor who have stared in the movie.  

Therefore for every actor who has stared in this particular movie, you have to repeat the information of length and studio across these tuples. Let us say some movie has two or three different actor say Shahrukh Khan, Hrithik Roshan or whatever. Now in a given movie let us say some movie X Y Z that is shot in the same year in the same studio and has the same length, it’s only the last field the star which changes from say Shahrukh Khan to Hrithik Roshan and whatever. 

So for each actor who was stared in the movie, I need to repeat all the other information that forms part of this tuple which is what is the notion of redundancy. And suppose after doing all this, I find that I have entered the length of this movie incorrectly and I need to change the length of the movie, I need to change it in all of these different tuples where this is stored and similarly the problem of deletion.
 
(Refer Slide Time: 00:39:22)

 

So anomalies are removed from a relation by the process of decomposition. Now what is meant by decomposition? Given a relation R containing a set of attributes A, you decompose the set of relations into two different relations S and T such that the sets of attributes in of S and T are a subsets of the attributes of A and there are no anomalies in S and T. Now a decomposition that does not contain any anomalies is said to be in what is called as Boyce-Codd Normal Form or it’s also abbreviated as BCNF, so a BCNF has the following property. 

Suppose given a relation R with the set of attributes A, it is said to be in BCNF if there is a non-trivial. Note the emphasis on the word non-trivial here, if there is any non-trivial functional dependency of the form A prime defines A double prime that means A double prime is not a subset of A prime. So if such a non-trivial functional dependency exists then it means that A prime is a super key of R. That is there is no functional dependency of the form A prime defines A double prime in which A prime is not the key. If that is the case then the relation is not said to be in BCNF. So we will take up examples of this once we complete the notion of decomposition into BCNF and when it becomes more clearer.









(Refer Slide Time: 00:40:51)

 

Now how do we decompose a relation such that it forms or it becomes comply into BCNF. Now suppose in a given relation R, let there be a functional dependency of the form A prime defines A double prime which violates BCNF. What do we meant by violating BCNF? It means that A prime defines A double prime is firstly non-trivial that is A double prime is not a subset of A prime and A prime is not the super key of R that is   it is some other sets of attribute. 

In order to bring R into BCNF, we decompose R as follows. First take the set of all attributes that are defined by A prime. Now A prime defines A double prime, it may define something else and so on. now let B be the set of all attributes that lie in the right hand side of any functional dependencies that are defined by A prime. Now remove the set of all attributes A prime along with B and form a separate relation and retain the remaining set of attributes along with A prime to form the other part of the decomposed relation R. Let us have an example which makes this very clear.












 

(Refer Slide Time: 00:42:22)

 

Consider the movies example once again. Let us have a look at this relation movies having the following attributes title, year, length, studio and star. Now here the following functional dependency holds that is title and year uniquely determines length. Title and year uniquely determines studio and title and year uniquely determines, it does not determine star actually. So this is actually a BCNF violating functional dependency because title and year obviously cannot be the key for this relation because it does not uniquely determine star. So title and year is not a super key as the star attribute is not in the closure of title and year. 

So to decompose this relation, just remove title and year along with length and studio. So there is no star here, just remove title and year along with length and studio that it define and put them in a separate relation and retain title and year along with star to form the other relation. Therefore we get two kinds of relations that is movies is divided into title, year, length and studio because title and year define length and title and year define studio. So we have separated them from movies and whatever is left out that is star is combined with A prime which is title and year and retained as it is.











 
(Refer Slide Time: 00:43:33)

 

So movies one and movies two are decomposed forms of movies and it is also easy to verify that movies one and movies two are BCNF complaint.
 
(Refer Slide Time: 00:44:08)

 

There is one property of BCNF relations, the notion of two attribute relations. Suppose I have any relation that has just two attributes, we don't need to do anything, it is always BCNF complaint. How do we ensure or how do we prove that a two attribute relation is always BCNF complaint. 

Consider these four cases, let us say there is a relation R which contains just two attributes A and B. Now there are four possible scenarios. The first one is there is no functional dependencies between A and B that is there exists no non-trivial functional dependencies. Only trivial functional dependencies are there, A defines A and B defines B in which case R is definitely in BCNF. Secondly there is a non-trivial functional dependency that is A defines B but there is no functional dependency of the form B defines A which is also no problem because in this case A becomes the key because there are just two attributes and A is defining B and B is not defining A. So A is the key of the relation and the relation is in BCNF. 

Similarly if B defines A and A does not define B then B becomes the key and suppose A defines B and B defines A which is also no problem because both A and B are keys and both A and B are candidate keys and we can use one of them as super key or the primary keys which is fine and the relation is also in BCNF.
 
(Refer Slide Time: 00:45:46)

 

We now come to another form of normalization which is called the third normal form of a relation. The third normal form is useful because in some cases it is not possible to decompose the relation such that the decomposed relations are BCNF complaint, it is not possible to decompose without losing some information. Now let us understand this by an example. Now consider another relation of the form which is shown in the slide here. Let us say we have a relation called drama and it has the attributes title, theater and city. That is there is a particular drama troop that is performing a drama having a particular title in a particular city and in a particular theater in the city. So therefore we can identify the following functional dependencies, the first one is title and city. 

The title of the drama and the city in which it is being played will probably determine the theater as well. So we can say that in this drama played in this city is being played in this theater only. Suppose there exist a functional dependencies of this form and let us also assume for the sake of argument in this case that given the name of a theater, we can uniquely identify where the theater is. Given the name of a drama theater, let’s say something like kalamandira or guru nanak bhavan or whatever, some kind of theater name we immediately know which city contains the theater.   

Now FD2 violates BCNF, as you can see here because theater is uniquely defining city but theater is not the key, in fact title and city form the key, title and city define theater. So given a title, drama title and a city we can identify theater therefore this is a BCNF violation in violating FD. However, based on the decomposition rule if we decompose drama into two relations title, theater and theater, city base based on this rule here because theater defines city. 

Now based on this rule if we decompose drama like this into drama one and drama two, it will actually be incorrect. Why? Because once we perform the join, once when we decompose relations and we join back the relations, we should retain all the properties of the original relation. Now when we perform the join between drama one and drama two then the key constraint will no longer hold. Let us look at this by an example.
 
(Refer Slide Time: 00:48:03)

 











(Refer Slide Time: 00:48:46)

 

Let us say drama one contains title and theater entries like this, there is a particular drama with a title say Yugant and it is in a particular theater. Now there is another drama with a different title but in the same theater maybe at some other time. Now this theater uniquely identify city, let us say given a theater name I can uniquely identify city. Now if I join, if I perform a natural join between theater that is drama one and drama two based on the attribute theater what are we going to get. 

(Refer Slide Time: 00:49:23)

 

We will get a table like this that is a title, theater, city and title, theater, city. Now if you see given a title and a theater, there is no unique title and title and city is no longer the key that is we don't know we can have possibly different, let us say we can possibly have different sets of attributes that are defined by the same title and city combination. So it does not uniquely determine the title that is theater and city does not uniquely determine the title of the drama. Such dependencies or such discrepancies occur because of a particular property and which is what we are going to see in this slide here. So we are going to define the notion of third normal form which is essentially relaxation of the second normal of the rather the BCNF or the Boyce-Codd normal form assumption. 

What is a relaxation that we are doing here? If you notice in the BCNF violating constraint that is theater defines city, the right hand side of the relation that is right hand side of the functional dependency called city is actually part of the primary key that is title and city was the primary key.
  
(Refer Slide Time: 00:50:05)

 

So in order to accommodate such cases we use a third normal form assumption which says that any relation R is said to be in third normal form, if there exist any non-trivial FD of the form A defines B either A is a super key which was all the condition we need for BCNF. And here we have an extra condition which says or B is a member of some key. So an attribute that is a member of a key is called a prime attribute. So therefore either B should be a prime attribute or A should be a super key which is what makes the relation into a third normal form relation. 

We shall not be going into details of how to prove properties of third normal form relations. However we will suffice it to note that it is a slightly general form of the BCNF or the Boyce-Codd normal form. Let us quickly visit the last kinds of dependencies and the next normal form that results from it, which is known as the multi-valued dependencies. Now even in BCNF, BCNF is a strict form of decomposition so even in BCNF we have not fully removed all possible redundancies. 

Consider this following example here. The slide shows an example relation called drama which has the following attributes title, theater, director and genre of the drama. And we note that drama is in BCNF because let us say title is the key, let us say each drama is played in precisely one theater for the sake of argument. Therefore title uniquely determines theater and director and the genre of the drama.  

(Refer Slide Time: 00:51:33)

 

However it may well be possible that a given title may be classified into two or more genres that is a given drama could be classified as comedy and it could also be classified as a social commentary. Now because it is classified as under two or more categories, there exists what is called as this one to many relationships between title and genre. So every time we identify the set of all comedy dramas, the theater and director has to be repeated. 

Similarly every time we identify social commentaries, the theater and director has to be repeated because it is the same drama. So this is what is called as a multi-valued dependency. Now what is a multi-valued dependency in a more formal fashion? Now suppose in a given relation A, there is a non-trivial functional dependency of the form A prime defines B and suppose A prime is also a key for because it’s in BCNF. 

Now suppose if B is completely independent of all other attributes of the relation then we say that there is a multi-valued dependency. In this case the attribute theater and director are completely independent of the category or the genre of the drama. It has no relationship between, a theater may play any kinds of drama and a director may direct any kinds of drama himself.



 
(Refer Slide Time: 00:53:37)

 

So it’s completely independent of that, so we say that there is a multi-valued dependency.
   
(Refer Slide Time: 00:54:37)

 

So we defined the notion of a non-trivial multi-valued dependency in order to remove them. So firstly what is the notion of a non-trivial multi-valued dependency? A multi-valued dependency of the form A, define A prime defines B is non-trivial if B is not a subset of A prime which is the non-trivial property for BCNF as well and there exist certain other attributes in addition to A prime and B. That is A prime union B is a proper subset of A, the set of all attributes that is there exist some more attributes in addition to B. 
So a relation R of A is said to be in fourth normal form, if for every non-trivial functional dependency of this form that is A multi-valued dependency B, A prime is the super key.
 
(Refer Slide Time: 00:55:31)

 

So we shall not be going into more details of the fourth normal form and for this session we will suffice it to say that fourth normal form is an even most stringent criterion for removing duplicates or removing redundancy in relations. So essentially the normal forms can be categorized like this. Third normal form is the most lenient among the three and next is BCNF and finally the most stringent is the fourth normal form. So if any relation that is complaint to fourth normal form is automatically complaint to BCNF which is in turn automatically complaint to third normal form.
(Refer Slide Time: 00:56:15)

 
So we now come to the end of this session and let us briefly summarize what all we have studied here. So we studied the notion of functional dependencies which is a generalization over keys and properties of functional dependencies like transitivity, reflexivity and augmentation, extra. We also saw the notions of trivial and non-trivial functional dependencies and how they affect BCNF and how we can decompose relations into BCNF so that we can remove redundancies.

However we see that not all relations can be decomposed into BCNF without losing information because of which we have also, we also need the concept of a third normal form. And we finally saw the notion of multi-valued dependencies which can remove redundancies that exist in BCNF and the fourth normal form which remove multi-valued dependencies. So that brings us to the end of this session. 
Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 9

ER Model to Relational
Model Mapping

Entity relationship model are what is called as the ER model and the second was the relational data model. We also saw a typical database design process and placed these models into appropriate positions in the process. The entity relationship model or the ER model is essentially meant for human comprehension. It basically is meant for creating a conceptual database or conceptual schema or what is termed as a logical schema of the database system and the relational data model is used for the physical schema or something that is implemented in the DBMS. And both the DBMS independent models that is no matter which companies database that you are going to use, you can still use the same ER model for representing your data and even the same relational model for representing the schema that goes on to your DBMS. 

Now in this session we are going to address one important issue. Now we are going to ask the question, are these two different data models ER and relational model completely independent of each other or are they the same or is there some way I can map between the ER schema and the relational schema without having to break my head too much essentially. 

That means to say or to take it to its logical extreme, can I design some tools or can I design some kinds of software that takes an ER diagram of a given system and generates appropriate relational schemata for the system. And we have also seen in the session on functional dependencies, we have seen how we can optimize a given relational schema up to a certain extent using some kinds of automated techniques. We have seen how to take a relation to BCNF or third normal form or the forth normal form and so on. 

So suppose we want to build a tool to automate this process. We need to be able to first map between an ER data database schema and a relational schema and then use techniques from functional dependencies to optimize this relational schema so that we can build a database application around it. So what we are going to study today form the underpinnings of what are called as lifecycle tools or database life cycle tools. There are several different lifecycle tools which provide support for the entire life cycle of a database systems starting from the conceptualization of the problem to the actual implementation of the application and maintenance of the database and so on. 

So before we begin, let us briefly summarize what we have learned about ER models and relational models. The ER models as you already know is used for conceptual modeling of the database schema, conceptual modeling or to create the logical database schema. This is meant for human comprehension this is essentially used to show end users what you have understood about their problem domain. This is a high level database design and there are no implementation details about, that are included as part of the ER model.
 
(Refer Slide Time: 00:04:11)

 

And of course the ER model is a DBMS independent and it is made of building blocks like entities, relationships and attributes which can be attributed to both entities and relationships. 

(Refer Slide Time: 00:04:56)

 

In contrast a relational data model is the data model that is most popularly used for physical schema design. A physical schema is the schema that is actually implemented on the computer, therefore the relational data model is meant for or optimized towards machine consumption that is how do we efficiently store data in my database, how do I efficiently search for a given data element, how do I efficiently update a given data element so that it does not create anomalies, how do I efficiently delete data elements again without creating any anomalies and so on. 

And of course the relational data model is also DBMS independent that is no matter what kind of database that you use, you can still use the same data model as far as the database that you are using is a relational database. You can use the same data model to represent your data on the DBMS. Of course reality is quiet different from the concept of DBMS independent and some DBMS systems may include more features than traditionally what is supported by the relational model. 

The relational model also supports some kinds of automated optimization techniques which we have seen in the session on functional dependencies where you can optimize a given relational schema, you can reason whether a given relational schema is optimal or not whether its going to create a redundant data in its DBMS or whether its going to create some kind of anomalies during updation and deletion or so on. And how we can systematically change the database schema without changing the correctness but increasing the overall efficiency in terms of retrieval and updates. 

What are the building blocks of the relational model? We have relations which comprises of several different attributes and the notion of keys forms a very crucial role or place a very crucial role in the relational database model. 

(Refer Slide Time: 00:07:09)

 

Let us come back to the ER model and look at some of the notations which will require if I have to study translation into ER models. The entities are represented using rectangles and a strong entity type that is an entity type which has its own key attribute and which represents a physical or which represents some kind of logical entity of the real life is represented by a rectangle with solid line surrounded. For example the slide shows this entity type called employee which depicts all objects of type employee which are present in the current system. 

On the other hand we also have what are called as weak entities. Weak entities are those which do not have an existence of their own or without being associated with a strong entity type. The slide shows the example of an insurance record. An insurance record doesn’t mean anything unless it is associated with some person. In a company for example an employee, therefore when we talk about insurance record, we have to say whose insurance record and so on. So that is the general idea, more specifically the insurance record entity type does not have any key attribute, it has to be associated with a strong entity type called employee which in turn has the key attribute. Therefore such weak entity types are depicted using dashed lines or dotted lines for the rectangle. 

We then have the relationship type for example a relationship called handles, so employee handles project or something like that which is represented by a diamond and or normal relation type is represented by a diamond using solid lines whereas what are called as identifying relationship types. That is the relationship types that identify a weak entity or provide an identity for weak entities by associating them with strong entities they are shown with double lines in the diamond.

Entities and attributes are associated or entities and relationships are associated with attributes which are some values in a given domain. Attributes are depicted using ovals and a normal attribute or a simple attribute is depicted by a oval with a solid line and key attributes, in this example an attribute called pan or pan number which uniquely identifies each income tax payer is shown as a key attribute and it is shown underlined saying that this attribute is a key attribute for this entity type.
 
(Refer Slide Time: 00:09:22)

 
And then there are multi-valued attributes which can have several values for the same attributes. We took an example of the color of a peacock. Now the color of a peacock is actually given by several different colors and all of which in combination, form the color of the bird. Such kinds of attributes are depicted using double lines as shown in the slide here and then there are derived attributes that is attributes who’s values can be derived from other attributes and these are shown using dotted lines. 

We took an example of the age of an employee that is if you know the date of birth of an employee and the current date, we can derive the age of an employee. Let us also look at some definitions from the relational model. The relational model is based around the notion of a mathematical relation. Now a mathematical relation is said to comprise of atomic values or atomic data values. And what is atomic data value? A data value is called atomic if it cannot be subdivided into smaller values, for example the age of a person. 

Similarly each data value is said to reside in a domain. In the ER model, a domain is also called a value set which is a term that is generally used by several people and in the relational model usually the term domain is used which is going to specify the range of values that a particular attribute can take.

(Refer Slide Time: 00:10:44)

 

Similarly a relation schema or a relational schema is denoted by a schema name that is in this example shown by the name R and a set of attributes in this example shown by A1 A2 until An and each attribute has a specific value that lies within the domain specified as domain of Ai. We have also defined what is known as the degree of a particular relationship. 



(Refer Slide Time: 00:12:07)

 

The degree of the relationship is simply the number of attributes in its relation schema. If you remember the same definition of the degree of a relationship also apply to the ER model. That is a relationship diamond can be a binary relationship or a ternary relationship, unary relationship or a N-ary relationship that is it can be associated with 1 2 3 or any number of entity types. And the slide shows that the relation is actually a subset of the Cartesian product of all of the domains that form the attributes. 

In the relational model, the notion of keys play a very crucial role, especially we saw in the notion, in the process of decomposing relational schema in order to make them normalized or conform into let us say BCNF or third normal form or forth normal form and so on. So let us revisit the notion of keys in a little more detail and keys are again very important when we translate from an ER model to a relational model. We have to be aware which attributes are the key and which attributes are the foreign key and so on. 

So a key constraint in the relational model essentially defines the notion of a superkey which is a set of attributes of a relation which can uniquely identify each tuple in the relation that is each instance of the relation. And a key or a minimal superkey is something which is minimal in a sense that if you remove any element of the minimal superkey, it seizes to be a superkey anymore.









(Refer Slide Time: 00:12:44)

 

And there is also the well known entity integrity constraint in the relational model which says that the primary key of a given tuple may never be null. The primary key is the minimal superkey that is going to be used to uniquely identify a given tuple in the relation. And we also saw the notion of referential integrity which is again an important issue in the relational data model. And the referential integrity constraint says that if a tuple of one relation refers to another tuple of another relation, it should refer to an existing tuple that means foreign keys that is keys of, primary keys of another relation embedded into the tuple of yet another relation should refer to tuples that already exist in the first relation. 

So the foreign key constraints are shown in the slide here. That is first of all the attribute of the foreign key or the domain of the foreign key should be the same as the domain of the primary key of the other relation and they have to refer to existing tuples in the other relation. And we also saw that relations can be or popularly viewed as tables and which is what is a notion used in SQL that is a relation of the form student with three attributes roll number, name and lab can be specified in the form of a table with the name student and three kinds of columns called roll number, name and lab.
 










(Refer Slide Time: 00:14:54)

 

(Refer Slide Time: 00:15:13)

 

So let us not come to the issue of mapping between given ER model and a relational database model. Now I had said in the beginning of this session why such a mapping is important.






(Refer Slide Time: 00:15:39)

 

There are several different commercial tools that are available which are called as lifecycle tools of a DBMS design. A lifecycle tool provides support in several or in most of the phases of a typical database lifecycle. That means the tool should be able to or using the tool you should be able to create a logical schema, talk to your end user saying this is what I have understood by your requirements of your systems, these are the different data elements, these are the different functionality requirements that form ER system and so on. 

And then using the same tool, you should be able to create physical schemata from the logical schema by automatically translating them to whatever extent possible. In practice it’s not possible to completely automate this process that is automatically generate a relational model and optimize it. Sometimes some kind of human intervention is necessary, when the human knows some domain knowledge that cannot be captured into the ER model. But there are several such tools an example is the tool called ERWin from computer associates which provide such a support for automatically translating between ER and the relational model. 

So let us see how we can go about such a translation. The first case that we are going to take is the case of a simple relation or a simple entity. So the slide here shows a simple entity type called department and it has three different attributes department name, department id and manager and the department id obviously is the key or the key attribute of this department.





 
(Refer Slide Time: 00:17:20)

 

Now given such a relation it is fairly obvious to see, given such an entity type it is fairly obvious to see that it can be translated into a relation of the type department that is also shown in the slide here just below the figure. So this ER model can be translated into such a relation where the name is called department and which has three different attributes department id, department name and manager. And also note that the key attribute is retained that is the department id which is the key attribute of this entity here becomes the primary key of the relation that’s formed. 

So this is straight forward that is as long as we have a simple entity type with simple attributes, note that the attributes are also simple, there no multi-valued attributes or composite attributes and so on. And it can be translated in a straight forward fashion to the relational model. What happens if we have a composite attribute? Remember that a composite attribute is something that is made up of sub attributes. A composite attribute is different from a multi-valued attribute that is a multi-valued attribute is something which can have many values for the same attribute, the color of a bird can have several different colors.










 

(Refer Slide Time: 00:18:41)

 

On the other hand a composite attribute is made up of two or more other attributes each having its own domain. For example the slide shows a composite attribute called department id for the same example of a department entity type, so the department id is the attribute here which is a composite attribute which in turn is made up of two other attributes called location id and region number, so on. 

Now a location id could have a different domain let us say location ids are given alphabets like a b c d and so on and region number are given numbers 1 2 3 4 and so on. So both of them may have different domains and they combine to form the attribute called department id which in turn is also the primary key of this department. Therefore we are considering two different aspects here, one is how to deal with composite attributes and the second is what happens if the composite attribute is the key attribute of the entity type. 

So the slide here shows the example of and shows how we can translate this into relational model. Firstly, the name department of the entity type becomes the name of the relation called department and all the other simple attributes are retained. Department name is retained as department name, manager is retained as manager and only here for the composite attribute, the department id never appears here. It’s just that all the simple components of the composite attribute are straight away loaded into this relation. 

That is location id comes here and region number comes here and in fact if either of these two, let us say location id or region number is again a composite attribute and it has some more attributes. Just take all the simple components of the composite attributes. So don't take region number and just take whatever is the simple component of this region number and add it to the relation here. 

And all of these simple components which form the composite attribute which is the key becomes the primary key. That is the primary key here is a composite key made up of two or two or more different attributes which combinedly identify or help in identifying a tuple of the relation.

(Refer Slide Time: 00:21:37)

 

The next example that we are going to see is the example of how to map relationships. First of all, let us look at the following relationship that is shown in the slide here. What characteristics can we ascribe to the relationship that is shown here? Firstly, we notice that the relationship here is a 1:1 relationship that is one employee is associated with one insurance record or rather the other way around in this case. That is one insurance record is associated with one employee and have a look at the association as well. 

The association or the relationship type is an identifying relationship type that means the relationship type called insurance details shown in the slide here is used to identify or provide an identity for the insurance record by associating it with an employee and also the insurance record has a total participation in this relationship that is insurance record has no existence without this relationship. 

Now how do we translate such entity types? So essentially the idea here is what how do we translate insurance record into the relational model? So for weak entity types, the translation is shown here in the slide below, just create a relationship or a just create a relation of the same name as the entity type but since it does not have a key, because weak entity types do not have key attributes. So, since it does not have a key attribute, use the key attribute from employee with which it is associated with and take that key attribute and make it into the key attribute of insurance record. However note that since pan number here is also the primary key for employee, this has to be made as a foreign key of insurance record. 

That means whenever we are updating or altering the table, we have to use the cascade option. Whenever let us say the employee type is updated or deleted that means to say that if the employee relation is deleted from the database, this in turn delete all the insurance record relations from the database itself because insurance records do not have any existence without the employee records. 

So the three steps here in order to translate a weak entity type is to first identify or is to first to locate the identifying relationship and see which entity type is this weak entity type associated with and use the primary key of that entity type as the key for the weak entity type or the record of or the relation for the weak entity type and make it into a foreign key of this entity type and use cascade options whenever updations or deletions are performed on the strong entity type.
 
(Refer Slide Time: 00:25:05)

 

Let us move on with translating relationships. So how do we translate? Let us take the simplest form of relationship again the 1:1 relationship. We saw what happens or how do we translate 1:1 relationships when weak entity types are considered. Now let us consider an entity type which is not weak but still is involved in a total participation that this is shown in the figure here. The figure shows a relationship type called managedby which relates two different entity types that is department and manager and there are attributes, relevant attributes are shown for each of them. 

That is the department has a key attribute called department id and a manager has a key attribute called employee number and the relationship itself has a key attribute called secretary. That is a secretary is assigned for a department that is managed by a manager that is the secretary attribute does not have an existence without an existence of this relationship. That is if a department is not managed by a manager then there is no secretary that is associated with this. 

Now how do we translate this? The translation is again shown in the slide below. So first create an entity type or create a relationship, create a relation called manager. Let me repeat this again. For this relationship, create a relation in the RDBMS model called manager with the following attributes. Now you might be wondering why should we create a relation called manager, why not department. Now let us think about it a little further. See in this slide here that manager is a strong entity type; it is not a weak entity type. However it is involved in a total participation in this relationship type. 

What is the total participation? The total participation is that the entity type does not have any existence without being participating in a relationship type of this kind. So what it essentially means here is that a manager has no existence that is a manager would probably be just an employee. So a manager would have no existence unless here she is associated or is given a department to manage. So it is the entity type that is involved in the total participation is taken as the primary entity type or the base entity type, primary relation called manager and then employee number becomes the key here that is the key for manager and the department id which is the primary key for department becomes a foreign key in manager and whatever attributes are associated with the relationship itself become attributes of this relation here that is of the manager relation here. 

So therefore the manager relation has a primary key called employee number and a foreign key called department id. Note that this makes sense when we note that manager does not have any existence without this relationship. That is, refer to the problem of referential integrity in relational data model. What is the referential integrity stipulate? Whenever a foreign key refers to a tuple in another relation, the tuple should exist that is it should refer to an existing tuple in the other relation. Now if we had made department as the base entity or the base relation here, we cannot use employee number as a foreign key because the manager relation won't even exist before this relation that is managedby is formed. 

On the other hand department has an independent existence without whether or not a manager is associated within. Therefore that forms the rational behind why we choose the entity type which is involved in total participation as a base entity type for the translation.














(Refer Slide Time: 00:29:49)

 

So let us summarize this, the previous slide once again. So in any 1:1 binary relationship between types S and T, choose one of them as a base relation. In case one of them is involved in a total participation, choose that as the base. If neither department nor manager where to be involved in total participation, it doesn’t matter which you are going to choose as the base relation. Include the primary key of the other entity type as a foreign key in the base relation and include any relationship attributes as attributes of the base relation.
 
(Refer Slide Time: 00:30:29)

 

Consider the example shown in the slide here. What happens if in a 1:1 relationship both entity types that is both entity types that are participating in this 1:1 binary relationship are involved in a total participation. Take a look at the slide here. The slide shows two entity types, project and consultant and each project is uniquely identified by a project id and each consultant is uniquely identified by his or her pan number. And there is a relationship called, relationship type called consultation which has its own attribute called secretary and it’s a 1:1 relationship and both of them are involved in a total participation. That is a project has no existence unless it is being consulted by a consultant and a consultant has no existence unless he or she is associated with a project. So, neither of them will have independent existence without the other. 

In such cases we cannot identify any relation as the base relation. If we identify project as a base relation and try to use pan number of the consultant as the foreign key then referential integrity could be violated. It’s the same in the other way around as well. If we use consultant as the base relation and try to use project id as the foreign key, again there is a chance of violating the referential integrity. In such cases the simplest way is to take the relationship type. In this case the consultation has the base relation that is form a relation called consultation and use project id and pan as the primary key of consultation and then all of the attributes from both of them will become attributes of this relation.
 
(Refer Slide Time: 00:32:27)

 

So incase both entities in a 1:1 binary relationship or both in total participation then we merge both of the entity types into one, usually in the name of the relationship. That is in the name of the relationship called consultation in the previous example. Now let us see how do we map 1 to N relationship. What is a 1: N relationship? That is N entities of one of the entity types could be associated with one entity of the other entity types. That is it forms some kind of a tree relationship that is one entity being associated with N different entities of the other type. 

So the slide here shows such an example that is employee works in department that is employee is an entity type, so N different employees can work in one department. That is one department may have several employees but each employee is associated with only one department.
 
(Refer Slide Time: 00:32:48)

 

And of course there are keys called employee number for employee and department id for department. The slide also shows how we can reduce this to a relation. The simplest way is to take the entity type on the N side of a relationship. So in this case the employee, so take this as the base that is translate it into a relation called employee and the primary key of employee becomes the key of the employee entity type here and the department id becomes a foreign key here. So this is as simple as that, that is for each binary 1:N relationships identify the relation S that represents the entity type on the N side. 

Why is this? So because each entity type on the N side uniquely identifies a department that is uniquely identifies the entity type on the other side. Therefore we can use the primary key of the entity type on the other side as a foreign key in the base relation. Therefore use this as the base relation and create a relation including the key of the other entity type as the foreign key.









 
(Refer Slide Time: 00:34:05)

 

(Refer Slide Time: 00:34:36)

 

How do we map M N relationships? Now what is an M N relationship? An M N relationship essentially says that M different entities of the first type can be associated with N different entities of the second type. Therefore there is no unique identification that is given an employee in the previous case, one could uniquely identify the department with which the employee is working in because each employee can work in at most one department.


 
(Refer Slide Time: 00:34:50)

 

On the other hand here let us say a relationship called deputedto. So an employee could be deputedto several departments let us say. So M different employees can be deputedto N different departments and of course employee has employee number as the key and department has department id as the key and so on and deputedto also has an attribute called record number which maintains a record of which employee is deputed where and so on. So in order to translate such relationships, note the steps that are shown in the slide here. There are three different relations that are formed, one is the employee relation that is one of the entity types of this relationship type. So the employee relation is formed with employee number as the primary key and all the other attributes that form the employee entity type. 

Similarly a department relation is found with department id as the primary key and all other relations and then a separate relation is created for the relationship type itself. So deputedto becomes a separate relation by itself and then uses employee number and department ids as foreign keys and also as the primary keys of this relation and whatever attributes that belong to the relation becomes part of or belong to the relationship type becomes part of the relations that is created here. That is the record number attribute becomes one of the attributes of deputedto relation. 

Note that we cannot move this record number that is the attribute of this relation to either the employee or a department because it does not uniquely identify either employee or department. Each employee could be associated with N different record numbers because they could be associated with N different departments and similarly each department could be associated with M different record number because M different employees could be working in that department.

 

(Refer Slide Time: 00:37:43)

 

So this slide shows, summarizes how M is to N relationships are translated. In a M:N binary relationship, it is not possible to collapse the relationship into one of the entity types because neither of the entity types uniquely identifies the other entity type. Therefore a separate relation is required and usually this is in the name of the relationship type itself. 

So separate relation is required in order to complete the mapping and of course the cascade option should be used whenever updates are performed on any of the relations pertaining to the entity types. That is whenever the whenever the entity type called employee or department is updated or deleted then these changes should be cascaded, so that they are reflected in the deputedto relationship as well. That is if employees employee entity type is deleted then of course the deputed to relationship should also be deleted.
 














(Refer Slide Time: 00:38:53)

 

Now one might ask the question is it possible to use the strategy. That is use the relationship type as the base relation rather than any of the participating entity types. Can we use the strategy for mapping 1:1 and 1:N relations as well because M:N is simply a generalization of 1:1 and 1:N relationships. So of course it is possible that is take the example of employee works in department that is N different employees working in one department. 

We can still create a separate relation called works in where it can use the employee number and the department id as the foreign keys of this relation. However it just creates an extra relationship or extra relation in the database that is totally unnecessary but this is sometimes actually attractive to use than collapsing the relationship into one of the relations especially where, especially if you have to avoid null values. That is especially if we have to, especially if we have cases where there are some employees who do not work in any department. If N different, if an employee can be associated with at most one department it means that an employee can also be associated with zero department. 

So in that case the department id field of employee would be null, it does not violate referential integrity because remember that referential integrity says that a foreign key should refer to an existing tuples or else should be null. Therefore it does not violate referential integrity but creates a lot of null values in the database schema, in the database itself. So if we have to avoid null values, it is actually preferable to use the relationship type as the base relation when performing the translation.
 





(Refer Slide Time: 00:40:57)

 

How do we map multi-valued attributes? We have seen how to map composite attributes and simple attributes and keys and so on but what happens if there are multi-valued attributes. Composite attributes are different from multi-valued attributes in the sense that each of them can have several different domains that is it is just a combination of several simple attributes. So we just open up the combination, when we are translating a composite attribute and then include all the simple attributes that form part of the composite attribute. 

On the other hand a multi-valued attribute is not a composition of several sub attributes instead it is an attribute that can take on several values instead of one value and the example, the slide show is that of a bird. So a bird has a multi-valued attribute called color, so what is the color of this bird. A bird could have several colors it need not have just one color and of course there is a primary key called species which identifies each bird uniquely. 

So in order to translate multi-valued attributes, take a look at the lower half of the slide which shows two different relations which make up this translation. The first relation shows a relation called bird with species as a primary key and all other attributes except the color attribute, all other attributes of the entity type called bird. And then a separate relation is created called bird colors where species and color are both included and are both part of the key that is combinedly defined the key of this bird. Therefore we say that bird species eggs has color Y, eggs has color Z, eggs has color A and so on. 

So the color attribute may be repeated in several or several tuples or rather the species attribute may be repeated in several tuples, once for each different color that the bird can take and both of them that is species and color become the primary key for bird colors. 

So for each multi-valued attributes of a given entity type, we have to create a separate relation that has a primary key of S paired with all possible values that the multi-valued attribute can take. And of course the cascade option should be used for referential integrity on the bird relation. That is whenever the bird relation is deleted or updated, the corresponding changes has to be made in bird colors as well. 

(Refer Slide Time: 00:43:20)

 

(Refer Slide Time: 00:43:51)

 

How do we map n-ary relationships? Until now we have been looking at binary relationships. What happens when there are n-ary relationships and different entities forming part of the relationship?
The slide here shows such an example that is the standard example of supplier supplies part to project. So there is a supplier who is uniquely identified by the supplier or the sales tax registration number or something STReg number and there is a project that is uniquely identified with project id and there is a part that is uniquely identified by part id and then the supplies relation which relates all of these three different entity types 

So the simple way of translating this is to use a separate relation called supplies as the base and of course separate relations each for supplier project and part with their corresponding primary keys and the supplies relation which has the primary keys of each of these relations has the foreign key of this relation. 

(Refer Slide Time: 00:45:02)

 

So for each n-ary relationship of any type R where n is greater than 2, we have to create a new relation S to represent this relationship type R. And of course the primary keys of the participating relations become foreign keys in this new relation. And the cascade option should be used for all of the relations that correspond to the entity types that participate in this relationship type.

What happens if one of the relations in an n-ary relationship type is a weak entity type? That is let us say part is a weak entity type, there is no existence for part unless it is associated with a supplier supplying it to some project. In that case we have to first identify the entity type which gives an existence to part and the part relation here has only foreign keys, it does not have any primary keys but the supplies relationship does not change that is it doesn’t have any part id, it just simply has the supplier primary key and the project primary key without the part primary key. Therefore we get two different relations that have foreign keys and they don't have their own primary keys.
 


(Refer Slide Time: 00:46:42)

 

So that brings us to the end of this session that talked about how we can map ER models into relation relational database models using several different rules. And of course this is not a comprehensive set of rules because there are several other sets of rules used for example in derived attributes or enhanced ER models like generalization and specialization which are not covered here. But all of them in totality are used to create the basis for any kind of a tool, software tool that can translate between given ER schema and its corresponding relational schema. 

So, this slide shows a summary of each of this mappings that is it gives a set of thumb rules saying if this is what is given in the ER model then what happens in the relational model. So in an ER model if an entity type is given then a corresponding entity relation that is a relation in the name of the entity type is created. If a 1:1 or a 1:N binary relationship is given then we create corresponding foreign keys from the N side to the one side or from the weak entity type to the strong entity type in this relation. 

So we create a relation and create appropriate foreign keys from them. If a M:N relationship type is given as shown in the slide here then we create a relation with the name of the relationship type. So, within quotes which is shown as relationship relation with two foreign keys that is one for each entity type that participates in this relation. 









(Refer Slide Time: 00:48:44)

 

If an N-ary relationship type is given, it is still the same strategy that is we create a relationship relation with N different foreign keys. That is one for each entity type or rather N different foreign keys as long as these entity types are strong entity types. If there is a simple attribute in a ER model that simply becomes an attribute in one of the relations in the relational model. If it is a composite attribute in the ER model then it becomes a set of simple attributes that is you take the simple part of all composite attributes. That is just go on finding the simple attributes that forms the composite attributes and then make all of them as part of the this relation.

(Refer Slide Time: 00:49:33)

 

If it is a multi-valued attribute then we need to create a separate relation and a foreign key. That is you have to associate the primary key of the base relation with each possible value of the multi-valued attribute. If it is a value set, it becomes a domain, value set in the ER model it becomes a domain. And a key attribute in the ER model will become either a primary or a secondary key in the relational model. So that brings us to the end of this session. 
Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 10

Storage Structures 

Looking at what might be termed as the logical aspects of database design, we looked at different data models, how data can be represented and how the relationships between them can be represented. We saw the ER schema which is meant primarily for human comprehension and we also saw the relational schema which we claimed is meant for the physical schema that is meant for machine comprehension. But when we say physical schema, it is still a kind of misnomer because the relational data model does not say anything about how data is actually stored on computers or storage device like disks or whatever, wherever databases are implemented. 

So in this session and the next few sessions, we are going to actually ripe a part in a sense look inside a DBMS or inside an implementation of a DBMS and see how are these data elements actually stored on computers. And what is it mean when we say that we have stored a table? How is a table actually residing on disks or any other kind of storage device? So that is what we are going to be concerned with in the next few sessions. So let us begin the session on storage structures. In order to be understanding storage structures, one of the first things we need to understand is what might be termed as the memory hierarchy.
 
(Refer Slide Time: 00:02:39)

 

When we talk about storage or storing data, the first question we need to ask is where is data stored. 
There are different kinds of devices which are capable of storing data. You might have obviously come across hard disks, I mean data that are stored in hard disks, floppy disks, CDROM and even the computer RAM that is the random access memory in the computer, the cache memory within the machine and even the registers are within the CPU, all of them are meant for storing data. And all of these can be organized, these kinds of, different kinds of memory devices can be organized in the form of a hierarchy which is termed as a memory hierarchy. 

This slide shows such a memory hierarchy but it divides this hierarchy into two kinds of storage devices what are called as primary storage and what are called as secondary storage. If we were to draw a hierarchy, primary storages devices would appear up in the hierarchy and below them would be the secondary storage devices. What is a difference between the primary and secondary storage devices? Primary storage devices, some examples are shown in the slide here like CPU registers, cache memory, RAM, DRAM, SRAM and so on.  

All of them are extremely fast memory devices, you can address or retrieve data elements extremely fast from these devices. However all of these are volatile memory devices that means once the power is switched off, they no longer can hold data. Data that are stored in primary storage devices cannot be persistent in nature. On the other hand secondary storage devices of which some examples are magnetic disks like your hard disk on your PC, magnetic tape which is primarily used in many locations for archiving data or taking backups of data. 

Then there are CDROMs, there are read-only CDROMs, there are write-once CDROMs, there are even some kinds of read-write CDROMs that are being available today. And there are also what is a more recent phenomenon what is called as the flash memory. Flash memory is a kind of, is made of what are called as EEPROMS that is electrically erasable programmable read only memories which can store data persistently even after the power is switched off and they can perform or they can perform data transfer in a rate that is much faster than existing storage devices like say magnetic disks or tapes and so on. 

So the common theme in secondary storage devices is that data can be stored in these devices in a persistent fashion and usually secondary storage devices are much cheaper than primary storage devices and they can store much more data than can be stored in a primary storage device. However usually secondary storage devices are much slower to access, they inquire much more over heads during access than accessing a primary storage device like say RAM or cache memory or so on.   







(Refer Slide Time: 00:06:06)

 

Now for the most part when implementing a database management system, we shall be concerned mainly with secondary storage devices. We do not concern hassles with what are called as main memory database which are databases that are completely held in main memory. There are not many implementations of main memory databases simply because it’s much more expensive to have large main memories which can implement databases of sizable of a pretty large size. So secondary storage devices have certain kinds, certain characteristics which are important or which influence the kind of storage structure that we are going to use to access and store and access data in these devices. 

We can either categorize secondary storage devices either random access device or a sequential device. Something like the PC hard disk or magnetic disk or random access devices that is you can access any given block of data in a magnetic disk, they are called sectors. So you can access any particular sector directly, it is not purely random access because it does performs some kind of sequential searches. However for most practical purposes or magnetic disk is a random access device where you can address any block directly and move to that block read or write to that block directly.

On the other hand something like a magnetic tape is a sequential access device. If you have to access the hundredth block and the tape is rewound, you have to run through the first 99 block before being able to access the hundredth block. Therefore it’s important what kinds of, how efficient can data access be when we are using a device which is either random access or sequential. For example we cannot implement a storage structure that has to perform a lot of pauses on the data mainly because especially when we are using a sequential device. 

Similarly we can classify devices as either read-write devices, write-once devices or read-only devices. Read-write devices are those where you can read and write data any number of times hard disks, floppy disks, magnetic tapes and so on are examples. 
On the other hand there are write-once devices where you can write data once but you cannot erase it. Once it is written, it becomes a read-only device. So such kinds of devices cannot incorporate data structures which need to be modified during runtime for example. Similarly there are read-only devices where data is stored during the manufacture of the device itself and it can never be altered and so new data cannot be stored on such devices again. 

So there are devices especially in embedded systems which stores small databases within read-only devices and it’s extremely important that such storage is performed correctly the first time because there is no scope for any kind of modifications, once data is written on to these devices. Then there are devices that can be classified as either character devices or block devices. Character devices are those where you have to read data character by character which can be extremely inefficient when we are dealing with large amounts of data. Some kinds of tape devices and so on are character devices. 

On the other hand there are block data access devices where usually the unit of data transfer is a set of characters called a block. So every time any read request is given, a read request reads an entire block of data into memory and writes back an entire block of data on to the device and usually there block access is coupled with what are called as read aheads that means its not just one block, it’s a set of blocks that are read into memory at a given point in time in order to increase data transfer efficiency.
   
(Refer Slide Time: 00:10:40)

 

What are the requirements for storing databases, what kinds of storage requirements do databases pose? Firstly we note that almost all databases required data to be stored permanently or what is called as persistently for longer periods of time. It should not be the case that once the computer is switched off, all the data in the database is lost, it has to be stored permanently or in a persistent fashion. 

Usually databases are far too big to fit in main memory. It is not realistic to be able to search a database by loading the entire database into memory and then use such techniques that are mainly useful for main memory. We have to store the database on disk and involve strategies that can search data on to the disk and use memory for this purpose. Over the years the cost of storage has dropped drastically, in fact there are many claims that the progress in storage has beaten what is called as Moore’s law. That is the amount of storage that can be packed into a given cost, for a given cost or on a given square area of physical dimensions.

Today we can have gigabytes of data stores in a very small device that one can store within your in pockets or on in a very small area. We have gigabytes of data stores that are embedded within watches for example and within pens and so on. So the amount of the data store or the storage available per person has increased dramatically over the years. And one of the byproduct of this is the changing definition of what might be termed as very large databases. In fact the term very large databases when it was coined was meant to refer to databases which are of hundreds of megabytes large. 

Slowly very large databases came to mean several gigabytes of a data and now we have databases that are several hundreds of gigabytes or terabytes which are 10 to the power of 12 bytes and then even petabytes of data. Petabytes are 10 power 15 bytes of data where especially databases that work on web related data like Google or AltaVista or search engines which collect data from all over the web actually work on petabytes of data. So the definition of very large databases has been changing continuously to include more and more data storage requirements.   

Therefore what is being, what has become imperative today is to design extremely agile data structures that can store and manage data between main memory and secondary storage devices in an efficient fashion. When we store data on to secondary memory, we usually distinguish between two kinds of data storage; the one is what is called as primary data storage. The primary data storage talks about how we store the data itself, how data itself is organized on to disks or any storage device and how are they accessed. 

Secondary file structures are those are also called as auxiliary or augmenting file structures are those sets of files that are used to speedup access to these data elements and this is especially important, secondary file organizations become especially important when the size of the database starts growing by leaps and bounds. When we have terabytes or petabytes of data, it is the role of auxiliary files or secondary files that provide pointers that help us in locating the required data element becomes more and more important.  






 
(Refer Slide Time: 00:15:04)

 

Usually when we talk about data storage on secondary devices, we are talking about what is termed as file organizations. Data is stored in logical structures called files on disks. The way files are organized on disk is called the file organizations. Usually files are stored as a sequence of records and a record is analogous to the notion of a tuple in a relation in relational algebra or a row in SQL parlance. So, a file is stored as a sequence of these physical or logical records and are stored in terms of what are called as physical blocks. 

As we saw before in previous slide, the block is the unit of data transfer between main memory and the storage device. So there are two different things here within a file, one is the logical ordering of data which is in the form of records and then the physical storage of data which is in the terms of blocks. There could be 1:1 correspondence between records and blocks which is very rare which means to say that each block is one record or there could be many records per block or many blocks per record depending on how we define our record structure. 

The term file organization refers to the way in which records are stored in terms of blocks and in the way blocks are placed on the storage medium and are interlinked so that they can be accessed from wherever. There are three different kinds of file organizations that we are going to see in this session today. The first one is what is called as the unsorted or the pile file or organization. And the second kind of organization is what is called as the sorted file organization and lastly we are going to look at hashing file organization. 






(Refer Slide Time: 0017:09)

 

Let us briefly look at the notion of record and blocks which is important for us to understand these different kinds of file organizations. A record like we have mentioned earlier represents a tuple in any relation. It is a logical unit of data which of inter related data which is of interest to the user or the database management system. A file is defined as a sequence of records and records could either be fixed length or variable length. Remember in SQL for example you can use variable length strings and variable length integers and so on. So the length of a given record could also be variable or fixed and records comprise of a sequence of different fields and fields is the same as, a field is the same as a column in SQL parlance or in attribute in relational parlance. 

(Refer Slide Time: 00:18:09)

 
Blocks: So blocks like we mentioned earlier is the physical unit of data transfer or data storage in storage devices. They correspond to for example sectors in hard disk or page in virtual memory systems and so on. They store, usually a block stores records from a single file but it need not necessarily be such a case it depends on the file system structure. Blocks are usually of fixed length, blocks cannot be of varying length unlike records and the length of a block is dependent upon physical characteristics of the storage device and also of the operating system and the RAM and so on. 

So many times the database management system itself does not have much control over the size of a block. A storage device is termed to be either defragmented or fragmented depending on whether contiguous sets of blocks on the storage device belong to the same file or to different files. We now define a term called the blocking factor which is important to determine how records are packed within blocks. The blocking factor is the number of records that are stored in a block on a given storage device.
 
(Refer Slide Time: 00:19:19)

 

It is constant across blocks, this blocking factor is constant across blocks if record length is fixed. On the other hand if record length is variable then the blocking factor is also variable because the number of records per block may vary from block to block. Blocking factor is simply defined as the number of blocks divided by or the size of block divided by the size of the total number of records. So bfr as shown in the slide is B divided by R where B is block size and R is record size and this is a floor function that is which takes a lower integer value of this division. 

Since the record size may not exactly divide block size, there would be some amount of wastage which is given by this formula that is in each block there is a wastage of this much amount of bytes that is blocking factor times the record size number of bytes. So how is this wastage managed? There are two kinds of approaches to managing this wasted block area when records are stored within blocks. 
The first is to do nothing that is don't use, let wasted spaces be. Such kinds of techniques are used in what are called as un spanned records that is a record may not span multiple blocks.

(Refer Slide Time: 00:20:41)

 
 
This slide shows such an example. The slide shows one block which spans from here to here comprising of three different records and there is no space for a fourth record. However there is some extra space that is left in the block which is left unused. 

(Refer Slide Time: 00:21:28) 

 

On the other hand there are what are called as spanned records. A spanned record is something that can span across different blocks. This slide shows such an example, this block contains three records and there is not enough space for the fourth record. However part of the fourth record is placed in the remaining space leaving a small amount of space for a pointer to point to the next block in the logical sequence of record. And wherever the next block begins, the remaining part of the fourth record is stored and then the next records are stored here. So such kinds of record organizations are termed to be spanning organizations where a record can span across multiple blocks. I am sure you would have noticed that if record size is bigger than the block size, we have to necessarily use spanning organization for storing records because we cannot store records into blocks otherwise.

 (Refer Slide Time: 00:22:30)

 

So this is what this slide says that is when record size is greater than the block size that is R is greater than B then usage of spanned records is compulsory. When we have variable record sizes or when we use spanned record allocation, we can term what is called as the average number of blocks that are required per or we can compute what is called as the average number of blocks required for storing a collection of records. So this can be, in order to compute this we first simply compute the blocking factor that is the block size divided by the average size of each record.








 
(Refer Slide Time: 00:22:41)

 

Then the following formula where r is the number of records divided by the blocking factor will give the number of blocks that is required for storing a particular database that is particular set of records.

(Refer Slide Time: 00:23:30)  

 

Let us now start with the different kinds of file organization techniques. The first kind of file organization technique that we are going to see today is what is called as the unordered file organization. This is also termed as a pile file where the term meaning that records are just stored as a pile inside the file. This slide shows such an example that is records are coming into the system and they are just being appended to the file record 1, record 2, record 3, record 4 without request to what data that are contained in these records and how they are going to be searched. And of course these data, these records eventually go into different blocks and they are managed in some fashion by the operating system underneath in the machine.

(Refer Slide Time: 00:24:23)  

 

A pile file is the simplest form of file organization, we don't have to do anything for organizing this file and insertion of records is the simplest that is records are inserted in the order of their arrival. And on the other hand if we have to search this file, we usually need some kinds of auxiliary files or we require some kind of help in order to efficiently search these files for a given data element. Therefore insertion is very easy however searching is extremely expensive because we will have to do a linear search, we have to just search through the entire file in order to find the data element that we require.  
















(Refer Slide Time: 00:25:07)

 

What about deletion in pile files? Deletion possesses yet another tricky problem in pile files and which can create certain kinds of fragmentation problems. Have a look at the slide here, the slide shows a pile file containing three records and some part of the block is empty and there are other blocks as well in the file. Now suppose that record two has to be deleted. Now once we delete record two and empty the space, we cannot reclaim back this space because the insertion algorithm for a pile file is not cognizant of this extra space that it can use. 

The insertion algorithm simply inserts records at the end of the file, it just appends records to the file. Therefore such a kind of deletion strategy is inefficient in terms of space usage. When we are using variable length records in a pile file, we encounter another unique problem and this is of record modification. Whenever some data is modified in a record as long as the record is of fixed length, it does not matter we can make the modification in place and write it back into the file. However if we allow for records to have variable size and the modification results in the size of the record to grow, there may not be enough space to write back the record. This slide shows such an example.










 
(Refer Slide Time: 00:26:06)

 

There are three records in this file or rather four records in the file record 1, record 2, record 3 and record 4 and record 2 is modified. Now the modification is such that the size of record 2 increases. Now we cannot write back this record at the same place where it was earlier. Therefore we will have to mark this, mark the earlier record as deleted or unused or something like that and write back record 2 at the end of the file, this is shown in the figure here. 

And of course we need to update any kind of auxiliary data structures that point to record 2, so that it points to the new location in the file. The second kind of file organization that we are going to consider are what are called as sorted files. Sorted files are those files which are physically sorted on the disk based on some field called the ordering field. So the file is actually or rather physically sorted on the disk. So when you read the file on disk in a particular order, it provides or it returns back records which are sorted based on the ordering field. 

Ordering field should be a key field or I should or it is recommended that the ordering key, ordering field should be a key field that is it should be unique for each record and should belong to an ordinal domain.










(Refer Slide Time: 00:27:27)

 

What is an ordinal domain? An ordinal domain is something where you can establish a total order among elements of the domain. For example the set of all integers is an ordinal domain. The set of all names for example is not an ordinal domain, we cannot place one name with respect to the other unless of course we impose some kind of an ordering like say lexical ordering. We say, we order the names as per the lexical rule that is a comes before b, b comes before c and so on. In sorted files insertion and deletion are both expensive because we have to ensure that the file remains sorted at all times. Especially when a new record is inserted with ordering field which has to go somewhere in the middle of the file rather than at the end of the file. And updation of a record may actually involve physical migration of the record, especially if the ordering field is modified. 

However searching in a sorted file is made simpler because we can use what is termed as binary search. What is binary search? We shall not go into too much details of binary search, let me just give a small algorithm of what a binary search looks like. Essentially the binary search technique is a technique where we divide the search space by half that is into in each iteration of the set that is we reduce the search space to half of the previous search space in each iteration.
 









(Refer Slide Time: 00:29:34)

 

So this slide shows a simple algorithm for binary searches. We see in the first step here that we start with two bounds left and or lower and upper bound. The lower bound is at one that is the first element or the first record and the upper bound is said to b that is where b is the number of blocks in the file or the last block in the file and in each iteration we are going to compute the mid point of these bounds that is l plus u divided by 2 is the midpoint.  

Now suppose we have to search for a given key value k, we say we read the records from form block i that is a midpoint and then see if and compare that with the required key attribute. Now there could be three different options that is one is the key attribute is equal to the key that is read from the block in which case we have found the record therefore return, we return a success. 

On the other hand a key attribute could be less than the midpoint in which case we have to search the lower half of our search space. That is we have to search between 1 and i minus 1 that is l and i minus 1. On the other hand if key attribute is greater than the midpoint, we have to search in the upper half of the database that is we have to search between i plus 1 and u. So this series of steps is performed until and unless or as long as u is greater than or equal to l that is the upper bound is greater than or equal to lower bound. 

Whenever they cross that is whenever the upper and lower bounds cross without having found the given record, we are able to conclude that the record does not exist in the file and then we say it is not found. So binary search, we are not going to detailed analysis of binary search here. However one can verify that a binary search technique requires an order of what is termed as log n where n is the number of blocks in the file. A binary search requires an order of log n number of disk accesses where as a linear search which actually searches through the file requires an order of n number of block accesses for searching. That is on an average n by 2 number of block accesses have to done for a linear search whereas only log n to base 2 number of accesses need to be performed for a binary search.
   
(Refer Slide Time: 00:32:45)

 

Let us now look at one more technique by which sorted files can be made more efficient in terms of insertion and updation. Note that whenever a sorted file has, whenever a new record has to be inserted into a sorted file, it is always a problem because the file has to be always sorted physically on the disk. It’s not a logical sorting that is being performed here. Therefore whenever a new record is being inserted as shown in the slide here, let us say record with key one is already in the file, next key value of 3 is already in the file and key value with 7 is already in the file. And now we receive a record whose key value is 4. 

Now what do we do with this record? In fact this record has to appear between record 3 and record 7 in the file physically on the file. That means that we have to physically move record 7 below and then insert record 4 here so that the file remains sorted. This is an extremely expensive operation, especially if lots of insertion operations are taking place. In order to mitigate this problem another technique what is called as the overflow file is used. 

An overflow file is a secondary file or another file where records are stored in an unsorted fashion. Whenever a new record is being added to the database, it is just added to the record or the overflow file in the form of a pile file that is it’s just appended to the overflow file. And periodically that is in a less frequent fashion, let us say once in a month or once in a weak or on a weekends or something like that, the overflow file is merged into the master file or the actual sorted file. 

That is the overflow file is first sorted and there are a number of merge algorithms that can take two sorted files and merge them together in an efficient fashion and using this, the over flow file is merged back into the master file. When such a technique is used for sorted files, searching also becomes a little bit different from a pure binary search. That is whenever a key has to be searched, we can perform a binary search and the master file and incase the key is not found in the master file, we have to perform a linear search in the overflow file. So there are two kinds of searching that has to be done when overflow files are used in sorted file organizations.

(Refer Slide Time: 00:35:35)   

 

So to summarize sorted files, we see that the sorted files are more efficient than pile files especially for key base searches. However they are suitable mainly for random access devices. Note that if we have to perform a binary search and binary search over a tape device, we may have to keep moving back and forth in the tape device quiet often which makes it more, which makes it terrible inefficient. Therefore binary searches are more suitable for random access devices rather than sequential devices. 

It’s a better choice where a database is mostly read only because insertion is always a problem, insertion and updation is a problem. We have to use overflow files or physically move records and merge and so on and mostly queries are key based retrievals. The third kind of file organization that we are going to see today, the third and the last kind of file organization that we are seeing today is what is called as hashing file organization. 

What is meant by hashing? Hashing is a means of providing very fast access to records on certain search conditions. And what are these search conditions? These search conditions are usually the equality condition based on a key field. That is whenever I what to search a record having a particular key attribute, note that it is not something like whose keys are less than a particular key attribute are greater than a particular key attribute and so on. This is useful only when we are searching for records whose key attributes are equal to the attributes attribute that is given in the query.
 
(Refer Slide Time: 00:36:28)

 

Hashing techniques uses what are called as hashing functions or which are also termed as randomizing functions that map particular keys into buckets for hosting records. And just like sorted file techniques, even hashing techniques are primarily suited for random access devices. Before we go into how hashing is performed on disks, let us have a look at what is called as internal hashing. Internal hashing is hashing that is performed entirely in main memory and in most database management systems, hashing is used extensively in main memory in order to quickly access a given data element among a set of data elements that have been loaded onto memory. 

So usually such kinds of hashing techniques uses an internal data structure that has something like static array of M different buckets and they are indexed from 0 to M minus 1 and they are several candidates for such a hashing function. A simple candidate is to just compute the mod or the remainder of the key with M that is the M where M is the number of buckets for the hashing function.
 













(Refer Slide Time: 00:37:39)

 

There are also other hashing algorithms called folding the key where a given key attribute is twisted and folded in different ways in order to come out with a number, that is uniformly randomly distributed across the set of all buckets that form the array static array of hash buckets. There are also other techniques like sampling the key and so on which we are not going to be seeing here. External hashing is the hashing technique that is used for managing data on disks rather than in memory. 

External hashing comprises of blocks on the disks which act as buckets and in turn are augmented by one or more blocks that hold the hashing array itself or the set of buckets that form the hashing array itself. So the figure here shows a typical hashing process given a particular record with a key attribute K, it is first put through a hashing function which is called hash of K here and this hashing function maps it onto a particular index entry in an array of buckets. Each index entry here in turn has an address of one or more blocks which form this bucket. 














(Refer Slide Time: 00:39:02)  

 

Usually one block can be allocated to one bucket or it could also be more than one blocks that are allocated to one bucket. Once the block is identified, this record is just appended to this block. So whenever a search has to be made on this key, we have to make a sequential search within the block. However we can reduce a search space drastically especially when there are large number of records on to a single block or a set of blocks that form a given hash bucket. 

So to summarize external hashing again, external hashing uses two levels of indirection that is hashing into buckets and searching within buckets. A bucket is usually one disk block or a set of contiguous blocks which is also important here that is there is no point having non-contiguous blocks as part of hashing because we again need to store some information on how to access these blocks from one another. A bucket can hold more than one record obviously and also this depends on records size and we have to perform a sequential search within a bucket.  














(Refer Slide Time: 00:40:31)

 

(Refer Slide Time: 00:41:10)

 

Hashing has to content with a contentious issue of what is called as overflows. What happens if we choose a hashing function that tries to hash every key onto a very small number of buckets. It especially, this can especially happen when the dataset itself could be could be skewed even if the hashing function that we have chosen is a reasonable one. That is given a set of keys that are uniformly distributed over a given range, this randomizing function uniformly distributes it over the set of all buckets. 

However if a dataset itself is skewed, we have a large number of key values near a particular value rather than all across the range then the hashing function would also be correspondingly skewed. In such cases what happens is that buckets could get overflowed that is the number of records stored in a bucket could go beyond the capacity of the bucket itself. In such cases there are several techniques that are used for overflow management. There are three different techniques that are primarily used; the first one is what is called as open addressing. Open addressing simply says that if this bucket is full just use the next available bucket which has some space in it. 

So once hashing function hashes on to a particular bucket and then we find that it is full. We start a sequential scan or a linear scan of the bucket space for the next available bucket in which we can store the record. The second kind of technique that is used for managing overflows is what was called chaining. Chaining is the technique of maintaining a link list of different buckets so that when a particular bucket is full, it maintains a pointer to another disk block or another set of disk blocks acting as another bucket which can hold some more data in them and so on and when that becomes full there is another chain and so on. 

However because hashing has to perform sequential searches within buckets, if we encounter or if we end up with a long chain of buckets it becomes terribly inefficient in terms of searching. Then the third kind of technique is what is called as rehashing where we try to use another hashing function. If the first hashing function doesn’t work that is maps to an overflow bucket, we use hashing function two and then see if it works and then hashing function three or whatever and then combine it with something like open addressing or chaining in order to manage overflows.

So this slide here shows the concept of chaining where there are main buckets here which in turn have pointers to overflow buckets and these pointers point to exact records in these overflow buckets and each record here has a next record pointer which points to the next overflow record that are managed by this overflow buckets.   

(Refer Slide Time: 00:43:57)

 
(Refer Slide Time: 00:44:25)

 

Until now we have been looking at kinds of hashing where the bucket space or the number of buckets is fixed. Such kinds of hashing techniques are what are called as static hashing techniques. And we have already seen what is the limitation of a static hashing technique. A given hashing technique might work generally that is a given hashing function might be good enough so that if the sets of keys are uniformly distributed, the hashing is also more or less uniformly distributed. 

However when the set of keys are skewed, when the data itself is skewed using a static hashing might be terrible inefficient because some amounts of buckets could be overflowing while a large number of other buckets could be more or less empty. In order to obviate this need, we use what is called as dynamic hashing. Dynamic hashing is the process where the number of buckets can change dynamically can grow or shrink with time as and when keys are being added or deleted from the database. The overall strategy in dynamic hashing is quite simple and it is shown in the following three steps. 

The first is we just start with a single bucket to begin with and we hash everything on to this bucket. Once the bucket is full we split the bucket into two separate buckets and then continue with the hashing process. This is and then we redistribute the records or the data that is stored within a bucket such that they are more or less uniformly distributed across the two different buckets. This process continuous, the process of splitting continuous whenever there is an overflow and then there is the process of merging that happens whenever there is an underflow that happens that is when a bucket becomes empty. We now look at a simple dynamic hashing technique where, which was how buckets can be split and merged. 



The slide shows one such technique here we see that we have a small diagram here which shows two different kinds of nodes or data structures. This kind of data structure, the circle here is what is called as an internal bucket and the square or the rectangle here is what is called as the leaf bucket or an external bucket. The leaf buckets or those which actually store the data. Initially all data is stored in a given, in a single bucket that is this is the bucket for all records. 

(Refer Slide Time: 00:46:17)

 

(Refer Slide Time: 00:46:19)

 

And assume that now we our keys are made of binary strings and then we are storing all of our data with these keys in these records. 
Now suppose there is an overflow that happens here in this bucket. Now what happens when there is an overflow?
  
(Refer Slide Time: 00:47:19)

 

This slide shows such an technique that is when there is an overflow, the bucket is split into two different buckets and you can notice the labels here for the edges joining these buckets. The first bucket is the set of all records whose keys start with 0 and the second bucket is the set of all records whose key start with 1 assuming that our keys are made of binary strings.  

Now suppose there is again an overflow in this bucket and there is no overflow in the upper bucket here, what happens to the hash table then. The hash table then changes to this following data structure where the overflow bucket is split and a new internal node is created and two different buckets are then created. So this bucket now is the set of all, holds the set of all records whose key start with 1 0 whereas this bucket holds the set of all records whose key start with 1 1. So we can trace that starting from the start node here, so 1 0 takes us to this bucket and 1 1 takes us to this bucket. 

What happens now if one of this bucket encounters an under flow that is it becomes empty when records are deleted from the database. We just have to merge this bucket with its partner so to say that is seen in the diagram here that this is the bucket who whose edge is labeled as one. We have to just merge it with its partner whose edge is labeled zero.





 
(Refer Slide Time: 00:47:56)

 

(Refer Slide Time: 00:48:57)

 

So this takes us back to the previous configuration where we had only two buckets in the hash table. There is another kind of dynamic hashing what is called as extensible hashing which also uses a similar kind of hashing technique in order to grow and shrink buckets.





 
(Refer Slide Time: 00:49:03)

 

Extensible hashing uses what is termed as a global directory of 2 power n number of bucket addresses where n is called the global depth of the directory. And then each bucket is uniquely identified by some set of higher order bits d number of bits which is less than or equal to n which can uniquely identify each bucket. And of course buckets are split and merged whenever they overflow or underflow and correspondingly n is changed that is the global depth is either increased or decreased in a corresponding fashion.
 
(Refer Slide Time: 00:49:50)

 

This slide shows such an example. Here we have a global depth of 3 that is n equal to 3 and there are several different bucket pointers that shows 000, 001 and so on. And there are several different buckets each with differing capacity where here this bucket says that d equal to 2. That means this bucket can be uniquely identified with just the top two bits that is all keys starting with 0 0 can go into this bucket. Here for example d equal to 1 that is all keys starting with 1 go into this bucket but here these two buckets have a large number of data elements that is where d equal to 3 that is this bucket contains all keys starting with 0 1 0 and this one contains all keys starting with 0 1 1.
 
(Refer Slide Time: 00:50:43)

 

Now suppose what happens if the last bucket overflows that is where d equal to 1? This bucket is then split so that d becomes 2 and then instead of instead of just one bit, we have to use two bits in order to uniquely identify this bucket. Therefore the top bucket here, the upper bucket here is the set of all keys which start with 1 0 and the lower bucket here is the set of all keys that start with 1 1.













 
(Refer Slide Time: 00:51:12)

 

So that brings us to the end of this session where we have looked at several kinds of file organizations and for physically managing records on storage devices. Let us quickly summarize what we have learnt in this session. We first looked at different kinds of storage media and what are their characteristics. We can classify storage media into different kind’s volatile, non-volatile, primary, secondary and so on. In fact they can be placed in a hierarchy and then there are different characteristics like random access, sequential access or read-only versus read-write or write-once and so on and then there could be either character devices or block devices and so on. 

Each of them, each of these characteristics impact the kind of data structure that we can use for storing records. We then looked at the concepts of records, blocks and files which are the terminology we use for dealing with data that are physically stored on to disks. Records are the logical unit of data that are stored while blocks are the physical unit of data that is used for data transfer and file is the set of records or a sequential records in which typically a relation is stored. We also saw the notion of spanning and un spanning of records in terms of how they affect the blocking factor or the number of blocks that are required to store a particular file of records. We then saw three different kinds of file organizations, the first one was the pile file organization which is the simplest where we just append records into a file. 

However which poses problems with insertion or rather with deletion and updation and also with search. We then also saw sorted files which are files that are physically sorted on disks based on some ordering attribute. Sorted files are much more efficient for search because we can use binary search on sorted files. However they pose very tricky problems in terms of insertion and updation of records and they become especially tricky when records can be of varying lengths and updations can change the key value of on which it is sorted.

We also saw the last kind of file organization called hash files where hashing function is used in order to identify the block number or the bucket in which particular record is stored. We also saw how or what are the challenges that are faced by hash functions especially when we use static hashing techniques because static hashing techniques cannot work efficiently when the dataset is skewed in which case we have to use dynamic hashing where the number of buckets in the hash table can dynamically grow or shrink as and when data is inserted or deleted. So that brings us to the end of this session. 

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 11

Indexing Techniques Single level

Hello and welcome to another session in database management systems. In our ongoing exploration of dbms, we have a kind of graduated from looking at a data management from a logical perspective to looking at a data management from a physical perspective. We have looked at how we can represent data conceptually using say the ER schema or in a more physical form that is in a way that is more friendly to the computer as in the relational data form and so on.

However all of these models were basically mathematical models that gave us a kind of formalism which told how we can represent data elements and how we can represent relationships among data elements and so on. We graduated from there to see how data is actually stored on the computer, what kinds of overheads do we incur when we use one kind of storage method versus another, which kind of storage method is easier in terms of lets us say insertion, easier in terms of updation, easier in terms of maintenance, easier in terms of searching and so on. 

We looked at three basic kinds of file organizations and compared them in terms of their complexity of insertion, updation, search and so on. And in our session on storage structures, we have also mentioned that when we talk about storage files there are basically two kinds of files that we are concerned about, what is called as a primary file or the data file contains the actual data that is stored in the database system and then set of one or more secondary files are what are called the auxiliary files which contain metadata which help us in accessing data elements as efficiently as possible.
 
In today’s session we are going to be concentrating on these secondary files. these secondary files are called as index files which provide one or more index structures that can help us in accessing whichever data element we need as efficient enough as possible. So let us look into index methods by firstly, briefly surveying or briefly summarizing what we have learnt about storage structures.  










(Refer Slide Time: 03:27)

 

First of all what are the storage requirements of databases, what kinds of requirements are we looking at here? Databases need data to be stored in a permanent fashion or what is termed as persistent fashion for long period of time. It shouldn’t be the case that once power is switched off, all your data is lost. It should be there for much longer period of time than the typical user session on a computer. And usually the amount of data that we are talking about is too large to fit in memory. 

We saw how the definition of very large databases has been changing over the years. Initially the term very large databases was used to mean hundreds of megabytes of data and now we are talking about peta bytes of data which is actually 10 to power 15 bytes of data. This is especially true in databases like web log, I mean web databases as in web search engines like Google, AltaVista or so on. They routinely deal with peta bytes of data and in fact we also saw how the storage technology has been beating Moore’s law in the sense that larger and larger amounts of storage is now possible in a smaller and smaller surface area and also at lower and lower prices.
 
Therefore storage by itself is not a big problem. Storage is cheap, secondary storage is especially is cheap, quite easily available. However the bigger problem now is to search for databases. We have anyway stored peta bytes of data but how do we search the relevant data items or whatever data that we need in as efficient fashion as possible. Imagine how would it be if you used a search engine like say goggle and you gave a web search and it gave you a request to come back after two days to look at your search results. 

It is unthinkable, we are looking at response time that is interactive in nature at most a few seconds before which the user gets bored or the user cannot wait beyond that. Therefore a search engine like this has to search potentially a data space of peta bytes of data before giving results for a given request. So in order to efficiently handle these data structure or this vast amounts of data, what is getting more important now is the set of secondary or auxiliary file structures using which we can try to efficiently access the data that is stored in primary databases. 

(Refer Slide Time: 06:25)

 

Let us briefly go through some of the definitions that we studied in storage structures. Let us review some of them because they are again important when we are talking about indexing methods. First, the notation of a record. A record is the physical counter part of what could be termed a tuple in a relation or a row in sql parlance and on disk a data is stored in terms of files and file is treated as a sequence of record. So, one might analogously say that a file stores a given relation or a given table. Although it need not exactly be the case because sometimes files are used to store more than one relations or relation is sometimes spread across different files due to some physical considerations like maximum file length that is allowed by the operating system and so on.

However in a general sense we can consider a file to be representing or to be storing or to be the physical counter part of a relation. Records could be of either fixed length or of variable length. Fixed length records are easier to handle in terms of quickly finding their location in a file for example finding the offset of a given record in a file. However not all data elements can be amenable to fixed length records especially when we have to store data elements in form of text where a text can range from few words to thousands of words. So if we allocate a large amount of memory for the text field, it would be unduly wasteful when we are using fixed length records in which cases we use variable length records. And records themselves comprise of a sequence of fields. A field is analogous to a column in sql parlance or an attribute in the relational algebra parlance.




(Refer Slide Time: 08:25)

 

We dint saw the concept of blocks. A block is a physical unit of storage in storage devices for example sectors in hard disk or page in virtual memory and so on. They are the smallest unit of data that are been transferred between the storage device and the computer. And usually we deal with block storage devices. When we are talking about databases, we rarely deal with character devices where the unit of information transferred is a single character. Blocks are usually are almost always of fixed length, they are not of variable length and the length of a block is based on several characteristics that are beyond the scope of a typical dbms. For example they are based on a consideration that deal with what is the storage capacity of the storage device that we are talking about, what is the operating system that we are using, what is the size of the data bus in the machine and so on.
 
Therefore the database management system has little or no control over the size of a block and a contiguous block in a storage device may or may not correspond to the same file. If they correspond to the same data file then it is well and good, in the sense that there is lesser overheads in accessing a file. We don’t need too many seeks, seek is the set of operations that is performed on any storage device like disk in order to find the correct block that we want from the device. So if contiguous blocks belong to same file, we do not require too many seek operations when accessing a data file. On other hand if they do not belong to the same file then we may incur some overheads in the seek time of the storage device. So a storage device is said to be defragmented, if contiguous blocks belong to the same file and it said to be fragmented if the blocks are distributed all across the storage device.





(Refer Slide Time: 10:30)

 

When we are taking about blocks, there is an important term that we used the notion of a blocking factor. The blocking factor is simply the number of blocks per record that are stored in the database. That is if I have a record size of R and I have block size of B, blocking factor is simply B divided by R, the floor of this function B divided by R. If B is greater than R then blocking factor is greater than 1, that means there can be more than one records per block. However if R does not divide B that is if B is not a pure multiple of R then there is some extra space that is wasted which is given by the reminder of this division. So how do you deal with this extra space?
 
(Refer Slide Time: 11:23)

 

There are two varieties of dealing with this extra space. We saw that records could be either of the kind of unspanned records or they could be spanned records. Unspanned records are those which do not span across different blocks, in such cases if we have some extra spaces as shown in the slide here they just are unused. We can’t do anything about it, that is we just leave that extra space unused which results in certain amount of wastage of space. However it helps in easy accessing of records from blocks. 

(Refer Slide Time: 12:35)

 

On the other hand we could think of spanned records were records can be split so that they are stored across different blocks. This slide shows such a diagram where three records are stored in their entirety in a given block m and the fourth record is split between block m and block p. And of course at the end of each block we should have some kind of a pointer that points to the next logical block, next block in logical sequence in the disk, so that we can know which block to access next in order to find the remaining part of the fourth record.













 
(Refer Slide Time: 12:38)

 

And of course whenever B is less than R that is whenever record size is greater than the block size then we have to use unspanned record storage.
 
(Refer Slide Time: 12:54)

 

We also saw three different kinds of file organizations and a file organization is simply an organization mechanism by which data is stored in files so that they can be efficiently accessed. We saw three different kinds of file organizations which we termed as unsorted files or pile files, sorted files and hashing files. Unsorted files are those files where you just input records into the file or just append new records at the end of the file without any consideration to the data that is present in the record or in the file. 
Unsorted files are very efficient when it comes to inserting new records. You don’t have to do any kind of searching, you don’t have to do any kind of reorganization, you just have to append it to the end of file. However it is very inefficient when it comes to either deletion or modification of a data or searching, especially in searching of data. In the worst case we may have to search the data file in a sequential fashion. And this can be a tremendous overhead when the file is extremely large in size, when it is giga bytes or tera bytes of data stored in one unsorted file.
 
A sorted file on the other hand is a file organization where the file is physically sorted on the disk based on the some field in a record which is called the ordering field. Therefore a physical sorting of records helps us in easy access of the file. We can use a search technique called binary search that can reduce or search space by half in each iteration so that usually what is termed as log N order of time, we can find whatever record that we are looking at. However the sorted file organization incurs a lot of overhead whenever insertion or deletion happens in the file.

Whenever a new record is inserted, we should ensure that the file remains physically sorted that means we may have to records in order to accommodate the new records in its place. And similarly the case for deletion, in order to remove any kind of fragmentation we have may to move records so that the overall file remains sorted. The last kind of file organization we saw was the hash files. A hash file uses a hashing function which hashes or that is which is a function that can take a value of a key field in a record and transform it into one of several bucket addresses. 

We saw two different kinds of hashing, static hashing and dynamic hashing where in static hashing the number of buckets are fixed and we have to deal with, we have the problem of contending with overflows especially if the data is queued. That is if the data set or the distribution of keys is queued, we may have a small number of buckets overflowing and I have to be dealt with techniques like open addressing or chaining and so on which poses terrible overheads during searching and we could have a large number of buckets that are empty because none of the keys were hashed to those functions. We also saw a remedy to this problem namely the notion of dynamic hashing where the number of buckets could actually grow or shrink in size whenever records are added or deleted from the database.











 
(Refer Slide Time: 16:40)

 

Let us move on to auxiliary file structures, the main topic of concern today, the notion of indexes. Let us first go through a few definitions before we look at actual index structures. Firstly, the notion of an index file. An index file is a secondary or auxiliary file that contains meta data or data that helps us in accessing the required data elements from the database. An index or an access structure is the data structure that is used inside the auxiliary files that helps us in searching for our data elements as efficiently as possible. And of course data structure are augmented with their corresponding search methods in order to search for our data record.

We can think of two kinds of indexes what are called as single level indexes and multi-level indexes. Single level indexes have just one level of index structures that is in addition to the primary file there is just one secondary file and the index file maps directly to block or record addresses in the primary file. A multi-level index on the other hand has multiple levels of indirection where one level of index structure may point to another level of index structure and so on. And finally the last level would point to block addresses or record addresses in the primary file.












(Refer Slide Time: 18:15)

 

Let us look at a few more definitions regarding pertaining to index structures. The notion of an indexing field or an indexing attribute is the field on which the index structure is built. That is searching is efficient whenever a search is given on this field that is on whichever field an index is maintained. For example, we saw the notion of the ordering field which is a field based on which records are ordered on disk in sorted file organizations. An indexing field is analogous in the sense that this is the field on which an index structure is build.
 
Usually the ordering field and the indexing field are the same and usually they are also the primary key that is the primary key is usually the ordering filed and by default an index structures is built on the primary key that is used in the records. A primary index is an index structure that is defined on the ordering key field that is the field that is used to physically order records on file in sorted file organizations. And in many cases the ordering field is the same as the primary key.














(Refer Slide Time: 19:36)

 

We also define a notion of clustering index where these are index structures that are defined on an ordering key field. However in cases where the ordering field is not the primary key or is not even a key field that means the ordering filed need not be unique. Remember that all key fields have the unique constraint, that are posed on them and whenever ordering is performed on a non-key field, it is not unique. So an index structure on such a non-unique field is called a clustering index. We also define the notion of a secondary index which is an index structure that is defined on a non-ordering field and even not necessary the key field.
 
(Refer Slide Time: 20:27)

 

Let us have a look at primary index to begin with. Like we mentioned before a primary index is an index structure that is defined on the ordering field of records and that to when the ordering field is a key field. Usually it is a primary key on which records are ordered on disk. A primary index comprises of an ordered file, note that even a primary index is a sorted file and it comprises of fixed length records and has two fields. These two fields are shown in the slide as k of i as you can see here. That is there is a pair called k of i which is the key for the i th record and a of i is the block address containing the i th record.
 
(Refer Slide Time: 21:21)

 

The figure in this slide shows an example of a primary index. Here on the right hand side of the figure, the primary data file is shown where records are divided into several blocks. So each block begins with a particular field number and of course this is a sorted file organization in the sense that records are physically sorted in these blocks. Now if you can notice here, the first record or the first field in each block for example the first field in the first block is 2000 3 0 1 0 1, the roll number of a student and of the second block is 2000 3 01 21 and of some other block here is 2003 02 21 and so on. The first field in these blocks are indexed in the index file that is they appear in the index file here. These the first field in the index file is K of i which we saw earlier that is the key value that is maintained in the index. The second field contains the pointer to the block or the block address that contains this record.








(Refer Slide Time: 22:39)

 

What is some of the properties of primary indexes? If you have noticed, the number of entries in the index is equal to the number of disk blocks that comprise of the primary file that is the primary or the data file. The first record in each block of the file is indexed that is the first record in each block of the file appears as 1 of k of i’s in the index file. These records are called anchor records because this is the anchor by which other records in the or other key values in the block are accessed. 

Therefore if you are searching for a key value of let us say 2003 01 20 as shown in the slide here, we perform a binary search on the index file and we come to a point where we realize that it has to lie in the first block itself because 2003 01 23 is greater than 2003 01 01 and lesser than 2003 01 21. So we have to find that block address which or that anchor address which is lesser than or lesser than or equal to the given key and the next block address would be greater than the given key. Such an index structure in which not all ordering or key attributes are indexed is called a sparse index. The primary index that we saw here is a sparse index. A sparse index essentially means that not all attributes or not all possible key values are indexed. More specifically in a primary index, we are only indexing one key value per block. On the other hand a dense index or is an index structure where each key or each search key value that appears in the primary data file is indexed in the indexed file.








 
(Refer Slide Time: 24:50)

 

How do we search using a primary index? Search is easy, we saw just a few moments ago that we can search using binary search, so we just have to perform a binary search whenever we have to search using the key value and then find such a record or a key value that is less than or equal to the key value that we are looking for, such that the next key value that appears in the index is greater than the key value that we are looking for. What about insertion and deletion? Insertion and deletion is easy if records are of course one fixed length and they are statically allocated to blocks, without block spanning. What is this mean? This means that suppose I allocate, suppose I know the set of all, the entire range of values of a given set of keys and I statically allocate a given sub range or subset of keys to particular blocks.
  
For example let us consider that a student roll number can range from 0 1 0 1 to 0 1 5 0. 
Let us say there are 150 students who can range from 0 1 0 1 to 0 1 5 0 and then we say that each block contains exactly 20 records. Therefore we store 0 1 0 1 to 0 1 1 9 or 0 1 2 0 rather in the first block and 0 1 2 1 to 0 1 4 0 in the second block and so on. Therefore we statically allocate each record to block address. If we do that insertion and deletion are easy. However they may result in wasted space, especially if not all records of this data set may be available at any given point in time.
 
For example if we have only let us say 0 1 0 1 and 0 1 5 0, we still have to have a number of blocks wasted because each address is stored in a particular block, we cannot store any other address in other blocks. On the other hand if we don’t want this wastage of space then we have to contend with re-computation of blocks that is moving records between blocks and also re-ordering of the index structure whenever insertion and deletion takes place. That is this is because primary index is based on a sorted file that is a file on which the entire data set is sorted and it has to remain sorted whenever insertion and deletion take place.

(Refer Slide Time: 27:37)

 

The next kind of index structure that we look at is the clustering index. Remember the definition of clustering index. A clustering index is an index structure that is used when a sorted file organization is used and the ordering key or the key on which a file is sorted is not the key field. That is ordering field is not a key field. What is the implication of saying that the ordering field is not a key field? The implication is that when a field is not a key field, it means that there is no unique constraint on the field. That means there could be repetitions, that is the same key value in the k of i, a of i model, the given k of i value may point to multiple addresses or multiple block addresses which store data values pertaining to the same key.
 
The structure of a clustering index file is similar to that of a primary index file in the sense that even this stores k of i and a of i addresses except that, except to this small change that only distinct values of the ordering field are stored. That is suppose we are ordering a primary data file based on the student names rather than roll numbers. We just store one index entry for each distinct student name and not every occurrence of student name. And we store the block address of the first occurrence of any given student name that is the first occurrence of the particular ordering field is what we are storing rather than all addresses of a given field.









 
(Refer Slide Time: 29:38)

 

Let us look at an example. The slide in this example shows a data file which is a sorted data file of course and where the ordering field is the department number and there is no unique constraint on the department number. That is department number may repeat over several records. We see here in this example that there are three records pertaining to department number 1 and three records pertaining to department number 2 and two records pertaining number 3 and so on. And we also see that the record pertaining to department number 2 has spanned or this key or this ordering field department number 2 has spanned over to two different blocks that is the first block and second block. The left hand side of the slide shows the clustering index which contains the usual k of i and a of i fields where k of i is the value of the ordering field or the indexed field and a of i is the block address.

Note the first two entries in the indexed file. The first entry says that for the value one, department number 1 look up block number one. The second record says that or the second indexing record says that for the value of department number 2 look up block number 1 again. Why is this so? This is because the value two appears for the first time in block number one. What about the other values of two, how do we search for the other values of two. Note that the file or the primary data file is a sorted file that means to say that if we know the first occurrence, if we know the block number of the first occurrence of a given ordering field, it is sufficient because the file is sorted and we can start looking at the next logical blocks in sequence, until we finish all the set of records having the same ordering key. 

Therefore if we are searching for a particular data record having department number 2, we have to first start from block number 1 and proceed in the next logical block in sequence which in this case is the block number 2 until we exhaust all records having department as 2.
 
(Refer Slide Time: 32:07)

 

A clustering index is also a sparse index. Why is this so? This is because only distinct values that are appearing in the ordering field are indexed. In the previous slide even though department number 1 appeared three times and the same thing would be for department number 2. Only a single entry existed in the clustering index field for cluster indexing file for both of these records. Insertion and deletion in clustering index may cause problems because of the well-known problem of sorted files that is the files have to be sorted and when we have to retain this sorted mechanism of or sorted form of files, this may impact on the clustering index.
 
In the previous example suppose we inserted a new record having department number as 1 then we have to move all the records having department number 2 that is the last record in block number 1 to the next block. That changes the corresponding address, the corresponding a of i value in the clustering index field that is a of i cannot, can no longer point to the first block but instead it has to point to the second block. Therefore we need re-orderings, whenever we perform insertions and deletions. There is an alternative solution for handling this problem of insertions and deletions that is to allocate blocks for each distinct value of the clustering field. This is shown in the example in the next slide.










 
(Refer Slide Time: 33:53)

 

This slide shows an example, where a clustering index is used on a primary file that is ordered on a non-key attribute. However there is a very specific organization of the non key of the primary file in the sense that each distinct value of the non key attribute of the ordering field is allocated a separate block. Have a look at the right hand side of the figure more carefully. In the first block there are three records having department number 1, in case more records are inserted having department number 1, they are not allocated to next logical block in sequence. However they are allocated to separate blocks and a separate pointer is maintained to these blocks, so that we can access more records having department number 1.

Have a look at the third block that is seen in this slide here. There are two records having department number 80 and the block can accommodate three records of this particular size. However even though other records exist for example department number 89, they are not put into to this block; they are given a separate block by themselves. Therefore this kind of block organization results in certain kinds of wasted space because especially since, especially if there are not many repetitions of the non key attribute. However insertion and deletion are much more simpler in such an organization this is because we don’t have to worry about any changes in either the block structures or in the index structure itself.









(Refer Slide Time: 36:02)

 

The next kind of index that we are exploring today is what is called as the secondary index. A secondary structure or a secondary index file is an index file that is used to index fields that are neither ordering fields nor key fields, that is there is no assurance from the primary file that the file is organized or ordered along these fields and they are also not key fields. That is there is no assurance that the values in these fields are unique. There could be many secondary indexes possible on a single file that is depending on how many fields that are there in a given record. A secondary index maintains one index entry for each record in the file that is if you remember the definition of a dense index, a secondary index is a dense index. 

(Refer Slide Time: 37:10)

 
In contrast to a sparse or a non-dense index where not all values of the indexed field are indexed. This slide shows an example of a secondary index. Assume that roll number is no longer the key field and it need not even be unique. So the left hand side of this slide shows a dense index where each and every field or each and every value of the roll number field that appears in the primary data file also appears in the index file. And there is a corresponding pointer from each of these key values to the particular record addresses directly. Note that we don’t have to maintain block addresses here because this is a dense index, we can directly dereference or directly refer to the record address that is the block address and the offset within a block where the record begins. And note that the file also need not to be ordered based on this secondary index and the pointers or the record pointers are arbitrarily shaped when they point to the primary data file.
 
(Refer Slide Time: 38:22)

 

What are some of the properties of this secondary index? Since if I am performing a secondary index on a key field, let us look at some properties of secondary index, maintaining secondary index on a key field. In previous example roll number could be a key field in the sense that it need not be an ordering field but it could be a key field. The data file could be ordered on some other field let us say name or grade something like that. But the index is maintained on the secondary or on the key field which is unique that means that since each field or each key field is unique, there are as many secondary index entries, as there are records in the data file because it is dense index and each index entry or each key field has to be indexed. 

However the data file need not to be sorted on to the disk that is it need not have, there is no need for that because we are directly referencing it’s a dense index and directly pointing to the particular record address. And because it is key field and because key fields are unique, we can maintain or we can be sure that we can use fixed length records for the secondary index because we know the length of the key field and we know the length of the address of a given record. 
Therefore the length of a key field plus the address of the given record in a primary file forms a length of the cluster of the secondary indexes. 

(Refer Slide Time: 40:11)

 

What happens if the secondary index is maintained on a non-key field that is where the field that is being indexed may have duplicate values. That is it need not be unique and it can have many number of value. What is the implication of having duplicate values? As we saw in the clustering index, a given value of the indexed field may point to multiple records or multiple records in the primary file. There are three different techniques for handling the duplicates in secondary index. Note the difficulty that we encounter in secondary index that is not there in a clustering index. In a clustering index we have the assurance that even though the indexing field that we are using is not a key field, the primary file is ordered or is physically sorted according to this ordering field on this non-key field.
 
Therefore if we just know the first occurrence of a given particular value of this field, it is sufficient using which we can access all other values that are present in the database. On the other hand when we are using a secondary index and a secondary index can be used on fields which need not be ordering fields. We don’t have any assurance of that sort that is this is a non-key field, so therefore there could be duplicates and there is no assurance as to how these records are distributed in the primary file itself. So handling duplicates becomes much more difficult in a secondary index rather than in a clustering index. 

There are three different varieties of handling secondary index duplicates, one is to use duplicate index entries. Duplicate index entries means that we use fixed length records however and of course use a sorted file or physically sorted file for the secondary index file and maintain as many index records as there are different values of a given non-key field. 
The second one, the second approach is to use variable length records that is have one value of K of i and many values of a of i that is more than one values of a of i that point or that in turn point to different record addresses or the third approach is to use extra re direction levels which will see in more detail shortly where the first level points to a block of record addresses and so on.
 
(Refer Slide Time: 43:04)

 

The first option is to use duplicate index entries, that is index entries are repeated for each duplicate occurrence of the non key attribute. The example shows here that the term 2003 01 02 is repeated 4 times and 2003 01 03 is repeated twice in the secondary index file. The advantage of such a scheme is that we can still use fixed length records however the searching of this data file becomes a bit more complicated. Binary search becomes more complicated. Why is this so? Because remember how binary search works. Binary search starts with the entire space or the entire set of indexes, as the search space to begin with that is the lower bound for the binary index is to begin with the first record and the upper bound is the last record. And then we compute a mid-point of the lower bound and the upper bound and compare our key, the key that we are searching with the mid-point.
 
Now what happens if we compute a mid-point in a data file or in an index file where key values could be repeating. When we compute a mid-point that is when you compute lower plus upper divided by two, we are not sure that or we cannot say that all records having this particular key will appear either to the left or to the right that is there could be repetitions or there could be duplicate entries of the particular key at the midpoint on either sides of the midpoint. So we have to search both sides in order to retrieve our particular or in order to make the next decision about the next iteration. And of course insertion of records would require restructuring of the index table because the index table is always sorted and maintained in a sorted order.


(Refer Slide Time: 45:10)

 

The second approach to handling secondary indexes was to use variable length records. In a variable length recording record schemes we use a, given K of i value that is if you look at the previous slide here for a given K of i value, the size of the a of i field is not fixed. That is there could be one or several address fields for the given key index. What are the problems or advantages of this approach? One advantage is that binary search, well it is still little bit complicated. However it does not suffer from the complications of repeating multiple keys that is if we know exactly the block addresses and block addresses are stored in a way that we can find out the next block address very quickly, we don’t have to worry about whether a given entry appears on both either side of the mid-point.
 
However it becomes a complicated in the sense that if variable length records are stored in a single index file, the midpoint may not pertain to or may to point to a valid block address or a valid block address of the index file. And insertion and deletion of the records may require restructuring of the index table. So restructuring in the sense that we may have to add more fields in the a of i or we may have add more addresses, add more values a of i field which in turn may affect the next fields that appear in the database. And of course there is also this problem of spanning and non-spanning of records that is we may have to use spanning records in order to in order to allocate them into blocks and associated problems of a searching and retrieval which we saw in the session on storage structures also hold for this kind of indexing scheme.







(Refer Slide Time: 47:41)

 

The third kind of scheme, secondary index scheme in order to handle duplicates or in non-key attributes is to use extra redirection levels. Have a look at the slide shown in this figure. This slide shows three different levels of files. That is the right most part of this slide shows the primary data file which contains blocks which in turn holds one or more records. There is a label id or a lab id field in this file which is the record or which is the field which is being indexed by the secondary index file. The lab id field is neither in sorted form in the primary data file nor is it a key field that is it need not have unique addresses, it can have repetitions. Because it can have repetitions, each repetition that is each distinct record address for a given value is given a separate block.
 
Have a look at the left most part of the slide here. The left most part is the usual secondary index file comprising of K of i and A of i fields where K of i contains distinct values of each of the distinct values that appear for the indexed fields which is a non-key field. Now, because this distinct value can pertain to several records or several records in the database, it is first the A of i address first points to a block that is reserved for storing addresses of this key value or of this index value. This block is of fixed length. Note that this is shown in the second field here. Note that even though a particular is not full, it is not used up for the second address. This is the block for the second value is a completely different block from the block for the first value even if it is not full. 

That is a separate block is allocated for each distinct value of the index field and this block contains the set of addresses by which we can using which we can perform a sequential search or on the particular address that we require or retrieve the set of all records pertaining to the given key value that we are searching for. So in this session we looked at three kinds of indexed files or three kinds of single level index structures namely the primary index, the clustering index and the secondary index. Let us briefly summarize before we end this session and look at what are the different kinds of index structures that we saw in this session today.
(Refer Slide Time: 50:59)

 

First of all let us revisit the extra indirection levels and look at some more properties of extra indirection levels. Before we summarize, the extra indirection level is the most frequently technique for handling duplicate record and the advantage of this is that the index records are of fixed length and it doesn’t suffer from complications, binary search complications that we discussed earlier and we can use the usual binary search in order to search for a particular given address.
 
However there could be wastage of spaces because a complete block is allocated for a given value even though if it is not completely full. And what happens when blocks overflow? That is there are large number of records of the given index, in such cases block overflows are handled by chaining which is the same technique that we saw in the hashing technique and retrieval requires sequential search within blocks. However insertion and deletion of records are straight forward, we don’t have worry about restructuring the index or restructuring the data file. We just have to insert the corresponding entries in the block file whenever insertion and deletion take place. So let us come back and summarize the different kinds of index structures that we saw today.











 
(Refer Slide Time: 52:31)

 

Firstly the type of indexes: If the index structure is on a key field and the ordering field is  or rather if the index structure is on key field and the ordering field which is also an ordering field then such and index structure is called a primary index. That is usually on the primary key and the keys order. So primary index can afford to be sparse and point to and store and index only the anchor records in each blocks. On the other hand if the index is on a non-key field however which is a ordering field then it is called a clustering index. That means the data file or the primary data file is physically sorted based on the ordering field which is not a key field. That means there is no unique constraint for this field, in which case we can use the clustering index where we just store index structures for each distinct value of this field and store the address of the first occurrence of this distinct value. Because it is ordered, we don’t have to worry about accessing the other values that exist in the database because they appear in logical sequence starting from the first occurrence.

If the key field is or if the indexing field is a key field, however it is not an ordering field then we use a secondary index for a key field. We saw that a secondary index on a key field is a dense index with fixed length records on which binary search can be used efficiently. If the field on which indexing is performed is not a key field and is also not a ordering field then we have to use secondary index of the non key variety that means we have to deal with duplicates in one of three different fashions either use duplicate index entries or use variable length records or use the most commonly used technique of extra indirection levels in order to handle duplicate address.





 
(Refer Slide Time: 54:48)

 

And quickly what are some of the properties of the different index structures that we saw? The primary index, what are the number of index entries. The number of index entries for a primary index is the number of disk blocks in the primary file or in the data file. It is a non-dense index or a sparse index. A clustering index is also a sparse index which stores the number of distinct index field values that is the number of distinct values that appear in the index file. And a secondary index on a key attribute contains as many number of index records as there are number of records in the data file itself, it is a dense index.

And a secondary index on a non-key field may or may not be dense, may be either dense or sparse depending on whether the non key field is unique or not that is the repetition in the non key field or not. And the number of records contained in the index file is equal to the number of distinct values that appear in the indexing attribute. So this brings us to the end of this session where we saw different kinds of single level indexes. In the next session we shall be looking at multi level indexes where index structure can have several different auxiliary files.   


Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 12

Indexing Techniques Multi-Level and Dynamic Indexes

Hello and welcome. In the previous session we had looked into different varieties of index structures. Index structures are auxiliary files that are used in the database storage that are used to help in accessing data which are stored in the primary files. Index structures are extremely important especially when database sizes have been growing exponentially in the recent past and the value of index structure is also more important because the main problem in database today is not the storage of the large amounts of data but retrieval of them. Rather in searching data elements based on some certain criteria, key values and so on. We saw different levels of, different kinds of index structures namely primary indexes, clustering indexes, secondary indexes on key attributes and secondary indexes on non-key attributes and so on. Let us briefly summarize them today in this session before we move on to more complex index structures. 

(Refer Slide Time: 02:28)

 

Some key definitions in index structures are shown here. An index file is a secondary or auxiliary file that is used to help speed up data access that is contained in the primary files or the data file. An index or an access structure is the data structure that is used in these secondary files which help in this retrieval process and of course the data structure is associated with its corresponding search methods and algorithms using which we can access these data elements as quickly as possible. 
We said that there are two kinds of index structure primarily, namely this single level index and the multi-level index structures. However until now we have just covered this single level index structures. A single level index structure is a single auxiliary file that directly maps to addresses in the primary file. And these addresses could be either block addresses which stores physical blocks on disk or any other storage medium or they could be recorded results where the address of or a record is directly stored or a record can be directly accessed within a block. That is the block address is augmented with the offset value which gives us the record address.
 
(Refer Slide Time: 03:57)

 

Some more definitions which are again important for looking into multi-level indexes which we are going to be exploring in this session. The indexing field is the field or the attribute on which the index is maintained and usually the field could be either an ordering field or a non-ordering field. It could either be a key field or an non-key field and the corresponding index structure for each of these fields changes depending on what kind of fields or what is the characteristics of the field that we are indexing.
 
A primary index is an index that is defined on the ordering key field of the data element that is the field should not only be an ordering field, it should also be a key field of the data element. What are the properties of an ordering field and a key field? Key field has a property that it is unique, that it has a uniqueness constraint or that no two fields in the database, no two key fields in the database have the same value. Similarly if the field is an ordering field we can have an assurance that the primary data file is physically sorted based on this field. 

Therefore whenever we have a key of a given value i, we know that for all key values greater than i we have to search forward that is we have to search in the forward direction of the file. We don’t have to search the reverse of the file or all the key values until now and so. Therefore these properties the property that the file is ordered based on the ordering field and the field is a key field helps us in building a primary index, there is a sparse index which can help access data in the primary file as efficiently as possible. We also look into clustering index which is the index structure that is used when the field that is to be indexed is an ordering field but not a key field. 

If the clustering field is not a key field then it is no longer constrained by the uniqueness constraints. That is there is no longer requirement that each of these, each element in this key field has to be unique. This poses a particular problem in the sense that a given key value may correspond to more than one addresses. The last kind of index that we saw was the secondary index. A secondary index is an index that is defined over some non-ordering field.
 
(Refer Slide Time: 6:45)

 

That is there is no assurance that the primary data file is ordered based on this field. If the primary data file is not ordered based on this field then it is not possible for us to store a sparse index. This is because we don’t know where to search the next record from. Therefore the index has to be dense index structure. However there is still, there is a further dimension to the secondary index data structure that is, is the key field or is the indexing field on which the secondary index is based upon is it a key field or a non-key field. If it is a key field then we have a particular kind of index structure and if it is a non key field then the index structure changes.








(Refer Slide Time: 07:27)

 

Let us briefly look at some illustrations of the three kinds of index structures that we are covered so far, so that it helps us in understanding the more complex index structures that we are going to cover in this session. The primary index structure is shown in this slide here, this slide shown an index file pointing to a different blocks in the data file. The data file comprises of different blocks that is records that are organized into different blocks and the blocks or the records are sorted within the blocks that is the indexing attribute is not only a key attribute that is it is not only unique, it is also the set of data records are also sorted based on this indexing field.
 
When this is the case, it is enough for us or it is sufficient if we are able to store or if we are able to index just the first attribute or the key value within a given block. This is called the anchoring record if you remember. So we just store the key value of the anchoring record in the index file and maintain a sparse index. The number of entries in this index file is equal to the number of blocks, the physical blocks that make up the primary file or the data file. And this index file is a sparse index because it does not store all attributes or all values of the key and this index file can afford to use fixed length records because the value of the record is, value of the key is known and the value of the address, block address is also known and we don’t need anything else therefore the primary index file can afford to use fixed length records.









(Refer Slide Time: 09:15)

 

This slide shows an illustration of the clustering indexing structure. In a clustering index, the file or the primary data file is ordered based on the clustering field. However this clustering field is a non-key field. If it is a non key field then there is no guarantee or there is no requirement for the field to be unique. So, in this slide there are some records that are shown, ordered on the field called department number. And there are repetitions in the department number that is there are three number of ones for a given department number, three number of two’s and two number of three’s and so on.
 
However since the primary file or the data file is ordered based on this field, it is sufficient for us to know where does the first or where is a first occurrence of a given value and that is what we store in the index file. That is the index file stores a unique values that the ordering field takes up that is shown in the left hand side of the slide that is values like 1 2 3 and so on and a pointer that points to the first occurrence that is to the block that contains the first occurrence of this particular value because the primary file is sorted, this is sufficient for us.
 
However there is a problem with insertion of records which needs, which may need a clustering indexes to be altered and which can be rectified by assigning separate blocks for each distinct value of the ordering field. 









(Refer Slide Time: 11:03)

 

We also know that secondary index on key field attributes, if it is a key field note that secondary index or index structures that are maintained on non ordering fields that is a fields on which, fields which do not contribute to the physical ordering of data records in the primary data file because they do not contribute to the physical order of the records in the primary file, this has to be a dense index because we have to for each record we need to know where this index or where this is actually stored. Therefore all values of the index attribute has to be reflected in the indexed file. 

In the slide here there are two files shown. The index file shown in the left side contains each roll number which is the indexing attribute and which is also the key attribute. That is the roll number is unique because the roll number is unique that is because the attribute is a key attribute, we don’t have to worry about duplicates, we don’t have to worry about repetitions and because repetition is not a problem we can afford to use fixed length record sizes for the indexed records. That is we know the length of the key attribute and we know the length of a record address and therefore we can afford to use fixed length records in the indexing file.

However the indexing file is dense and it contains as many records as there are records in the data file itself. If secondary index is maintained on a non key attribute then we no longer have the luxury of the uniqueness constraint on the attribute. That means this attribute not only, does not contribute to the physical ordering of records in the primary file, it also is not constraint by the uniqueness constraint that is there may be repetitions, there may be several different records having the same key value. If this is the case, then a given key value K of i may correspond to multiple addresses. In clustering index this was not a problem because the indexing attribute was an ordering attribute. That is physically the data records were ordered based on this attribute. 

Therefore it was sufficient for us to know where is the first occurrence of this record. We do not have such a luxury in secondary indexes because this indexing field is not an ordering attribute. In such a case we use extra levels of indirection or extra level levels of redirection in order to reach the data record. In the slide here there are three different kinds of files that are shown.

(Refer Slide Time: 14:02)

 

The left most file is the secondary index file which shows K of i and A of i attributes that is key values each and every distinct key value and a block address for each key value. Disk block address is not the block address of the data file but in fact a block of addresses, a block address containing a block of addresses, many different addresses. So this block contains several addresses, one each for each record having this particular value and they are stored block wise and block overflows are handled by chaining so that each different key value occurs in a separate block by itself.















(Refer Slide Time: 14:53)

 

So let us briefly summarize the different characteristics of a single level indexes and see and motivate a need for multilevel indexes. If the field or the indexing field is a key field and an ordering field, we can use a primary index as shown in the slide here. If the indexing field is a key field and not an ordering field, we can use a secondary index on keys that is a dense index with fixed length records. If the indexing field is a non key field but it is an ordering field then we use the clustering index where we can store just the address of the first occurrence of every unique data value. If on the other hand, if the indexing attribute is neither a key field nor an ordering field then we have to resort to secondary indexes of non key varieties that is we have to use an extra level of indirection. 

(Refer Slide Time: 16:06)

 
There also other properties of the index structures that is primary index is the sparse index where the number of records in the index file is equal to the number of blocks in the primary file. Similarly a clustering index is also a sparse index where the number of records is equal to the number of distinct values that are present for the attribute. A secondary key index is a dense index which contains as many number of records as there are records in the database itself. While the secondary non-key index is either a dense or a sparse index depending on weather there are repetitions in the non key attribute and the number of records in the key attribute is simply the number of distinct values that are present in the indexing attribute in a data file.
 
(Refer Slide Time: 16:46)

 

One of the main advantages of index structures like say primary index or dense secondary indexes and so on. Is that index files are ordered files that is they are ordered on their key values because they are ordered on their key values; it is possible to search them based on binary search. A binary search is a search technique which reduces the search space by half in each iteration. Therefore in the average case or in a ideal case, one can reach the particular key or the address in log n number of times, log n to base 2 where n is the number of records in the file.
 
On the other hand a linear search requires times of the order of n or n by 2 rather, so n different memory accesses or n different record accesses. However we can note that there are three different entities that we are concerned with during physical storage. These are the file itself which is a sequence of logical records, a record which is a logical equivalent of tuple in a relational schema and the block which is purely of physical nature that is which is meant for efficient data transfer between the storage media and the computer and whose size is determined by physical characteristics. 

Now between block and records, we have defined a notion of the term blocking factor where blocking factor is the number of records per block or how many number of records can we store in the block. Now if the blocking factor of an index file that is of blocks storing an index file is greater than two that means if a block, if a disk block can store more than two addresses or more than two index records then we can actually come out with a even better method of searching where the method of searching is of the, reduces by the order of blocking factor rather than by the order of 2 which is constant in binary searches. These are explored in multi level indexes. 

(Refer Slide Time: 19:20)

 

A multi level index is an index file which contains several different levels as the name suggests and each index block at a given level has a factor called the fan out.  A fan out is typically derived from a blocking factor that which depends on the number of records that one can store in a block. So a fan out is the number of different records that or the number of different entries that a given index entry can point to and in a good implementation, block accesses can be reduced from log n or log b to base 2 where b is the number of blocks in the index file, so it can be reduced from log b to base 2 to log b to base fan out. So this is useful if fan out is obviously greater than 2. If fan out is greater than 2, we can reduce the number of block access by a tremendous factor.
 











(Refer Slide Time: 20:31)

 

The figure in this slide shows a two level index structure where index structures are categorized into first level and second level. The first level is called the base level and the second level is called the top level and for the sake of clarity, I have also shown how index records are divided into blocks at the base level. It is divided into two, here the blocking factor is 2 but usually the blocking factor is more than 2. If you can notice the slide carefully, the way the slide or the way the index stores information is that there are different levels in which information is stored. 

At the top level there are just 2 entries k of i entries 2 and 10. This entry says that everything between 2 and the next number can be found in this index file. That is every key value between 2 that is greater than or equal to 2 and less than 10 can be found in this index file. Similarly every key value that is greater than 10 and there is nothing below, therefore which is just greater than 10 can be found in the index file that is pointed to by this pointer. The other, the second level index file is also a replica of the top level index file in the sense that everything greater than or equal to the key value that is specified here and less than the key value of the next record is pointed to by the present address. That is this is an indexing scheme that indexes an ordering field and also a key field. Therefore all records starting from 2 to less than 5 can be found here and all records starting at 5 and less than 10 can be found here and so on.
 








(Refer Slide Time: 22:39)

 

The first level or the base level is a usual primary index that is maintained on a sorted file. The second level is a actually a primary index on the primary index because the index file itself is a sorted file and it is sorted based on the key attribute, we can store another primary index on this index file. And we can continue this process to any number of levels depending on the size of our database. So we could have a third level that stores an index or that stores a primary index on the second level and so on and at each level, the number of entries in the next level is determined by the fan out or the blocking factor. 

Here the fan out was 2 therefore what we ended up seeing was a binary tree structure. However in general the fan out is usually much more, a block could contain many number of records, many number of index entries much more than 2. Therefore at each level from level 2 to level 3 for example fan out number of records that is a blocking factor number of records can be indexed using just one index structure. Therefore the number of entries starts reducing exponentially by a factor of fan out. So this again depicts the same thing that is at each level, the number of index entries is getting reduced by half.
 











(Refer Slide Time: 24:19)

 

In order for a multi level index to be efficient enough or to be more efficient than primary index, there is an important consideration of the multi-level index structure. If you are familiar with data structures, you might have come across a data structure called tree which is a data structure used for representing hierarchies. A multi level index structure is useful only when the tree structure that is specified by the multi level index structure is balanced. What is meant by a balance tree? It is well possible for us to have a multi-level index structure of the kind that is shown in the left hand side of this slide shown in this figure.
 
In the left hand side, as you can see there are several levels to the index structure and in the worst case one has to make 4 different 1 2 3 and 4 different traversal’s of different index files in order to find a given record. On the other hand the right hand side of the figure has the same number of nodes or here each node or each circle here represents an index file and the right side of the figure has the same number of index files, however a smaller number of levels. And the load or the number of index files is more or less evenly balanced across the entire tree. 

In such a case in the average, the average behavior of the such a tree or even the worst case behavior of a balance tree is only log n to base fan out while the worst case behavior of an unbalanced tree is of the order of n different block accesses and which is no different from performing a linear search over these different index files. Therefore an order for multi level indexes to be useful, they have to they have to form a balanced tree.
 





(Refer Slide Time: 26:42)

 

So whenever insertions and deletions happen in a data file containing a multi level index, the balanced property of the index trees should be maintained. And this is especially problematic in multi level indexes because all index files are physically sorted files and we need to make a number of different adjustments at number of different levels, if you are just storing several different primary index structures in order to maintain the balance tree property of the index structure. An approach to overcome this is what is called as dynamic multi-level indexes. That is an index structure changes itself dynamically by as a little a number of operations as possible so that the balanced property of the index tree is maintained. 

(Refer Slide Time: 27:41)

 
The most commonly used dynamic index structures are what are called b-trees and b plus trees. Let us have a look at these two index structures in a little more detail. Here a b tree is an index tree which is of course as the name suggest a tree data structure where each node has a pre determined maximum fan out given by p which is of course, when we are implementing it which would be related to bfr that is the blocking factor. There are several terminologies we use when we are talking about b tree. A given block that is allocated to a b tree is called a node in the b tree. As we will see later, a block corresponds to tree node in the logical tree structure that the b tree forms.
 
A special kind of node called the root node forms the access to the b tree that is it is the top most node in the tree and each node has a maximum of p children that is the fan out number of children and of course each node has a maximum of one parent. I am saying the maximum of one parent because the root node will not have any parent node. So it is either 0 or 1 parent depending on whether it is a root node or a non root node. And there are what are called as leaf nodes that are the lowest level nodes that is nodes which do not have any children. And any node in the tree that is not a root node, that is neither a root node nor a leaf node is called an internal node. Of course we have already defined the notion of parent and children that is when a node points to some other node then it is said to be a parent of the other node and the other node is said to be the child of the node which is pointing to it. 

(Refer Slide Time: 29:41)

 

This slide shows a typical structure of a b tree node. This node is nothing but a block when it is implemented on disk. As you can see here there are several aspects or several fields to this node in a b tree. One can easily notice that there are several pointers in fact there are precisely maximum of p pointers in the b tree. Each pointer which points to a triangle represents a sub tree that can be pointed to by this node. Of course a sub tree can be null if this is a leaf node or if a pointer does not exist. Sub trees are filed in such a way that there are filled leftmost that is you cannot have a left most sub tree as null but some internal sub trees to be filled up and and left most sub tree being null. And in addition to the sub tree pointers or pointers to other nodes, there are several different key and data pointers. That is there are several blocks here that contain a key value under pointer to the record containing this key value. 
 
Similarly there is another key value and and pointer to that record and so on. So there are several key values that are present and pointers to where the key values are present in the primary file in addition to maximum of p pointers to other nodes in the tree. What are the properties of these, what are the properties of such nodes or what are the constraints that a b tree has to adjure to?

(Refer Slide Time: 31:30) 

 

For a node containing p minus 1 keys, note that if a node can point to a maximum of p sub trees, it can contain a maximum of p minus 1 keys because as shown in the previous slide, their keys embed or this pointers embed the keys. That is there is one pointer on the left most side of the left most key and one at the right most side of the right most key and one pointer between every two keys in the node. The keys are always stored in a sorted sequence that is if there are p minus 1 keys then k1 is less than k2 is less than etc until K of p minus 1.
 
For any data element in a sub tree that is pointed to by one of the sub tree pointers, let us say in some sub tree Pi for any data element in that sub tree pi that data element should be less than the data element of the left most key that is K of i minus 1 or rather it should be greater than the left most key, the right most left key that is K of i minus 1 and should be less than the leftmost right key that is K of i.




(Refer Slide Time: 33:02)

 

So more constraints of b tree nodes, each node can have at most p tree pointers of course and each node except the root node and the leaf nodes should have at least sealing of p by 2 tree pointers. What is sealing of p by 2? Divide p by 2 and take the upper integer value of this division. So they must have at least more than half of their pointers to be filled up as part of the tree building procedure. The root node should have at least two tree pointers, unless it is the only node in the tree that is unless root node is also a leaf node, it should have at least two children as part of the tree. 

And all leaf nodes are the same level. What is the level of a node in a tree? The level is simply the distance in terms of the number of hops from the root node. All leaf nodes are maintained at the same level in the tree. So this is the constraint that has to be maintained and suitable algorithms have to be or suitable algorithms are created so that these constraints are maintained.
 














(Refer Slide Time: 34:22)

 

We shall be looking at insertion and deletion algorithm for b trees, after we have a look at the b plus trees. In fact the insertion and deletion algorithms for both of these tree structures are the same except that b plus tree has greater expressiveness or b plus trees allows for different kinds of accessing, different varieties of accessing the data elements in addition to B trees. Therefore let us have a look at some definitions of B plus trees and their constraints before we look at insertion algorithms. B plus trees is a most common index structure that is found in many of the commercial RDBMS.
 
It is very similar to a b tree except that the leaf and non leaf nodes have different structures. In a b tree, there is no difference between a leaf node and a non leaf node both of them have a same structure. That is they have a set of address pointers and the set of key values and data pointers. The leaf nodes form a separate kind of index containing each different key value in a sorted form and pointers to the corresponding data elements. That is leaf nodes are linked together so has to provide ordered access to the data file records. 













(Refer Slide Time: 35:48)

 

A non leaf node of a b plus tree is depicted in the following figure and as you can see it is quite similar to a non leaf node in a or it is quite similar to a node of a b tree. That is it contains a two kinds of entities that is sub tree pointers or block pointers and key values but the only difference is that there are no data values here. That is there are just key values k1 k2 etc, there are no data pointers as part of this node. And of course the same set of constraints hold that is for any x between k1 and k2 the value of all the keys in that sub tree x should be greater than k1 that is the right most left key and less than k2 which is the left most right key.
 
(Refer Slide Time: 36:41)

 

A leaf node in a b plus tree is shown in the following figure where there are no sub tree pointers because there are no children for the leaf nodes and there are only a set of keys and data pointers. That is there is key one and data pointer to the record containing key one, there is key two and the data pointer pointing to record containing key two and so on. And at the end of this block, there is a pointer pointing to next logical leaf node or the left logical block in this sequence. Therefore starting from the left most leaf node, we can access the entire database in a sorted form just by following the leaf nodes and the links to the next leaf node.

(Refer Slide Time: 37:37)

 

What are the properties of leaf nodes? Keys in a leaf node have to be ordered just like the property that we saw in b trees where keys have to be ordered within a leaf node. We should be able to access each leaf node in key sequence that is k1 is less than k2 less than kn if there are n different keys in a leaf node. And just like the nodes in a b tree each leaf node should have at least half of its keys filled up. That is a sealing of p by 2 number of keys should be filled up and all leaf nodes should be at the same level as far as the overall b plus tree is concerned.











 
(Refer Slide Time: 38:23)

 

Let us first look at how we perform search in B trees and B plus trees. We are going to be looking at searches and additions and deletions in B plus trees and the corresponding algorithm for B tree can be derived in an analogous fashion. In fact the algorithm for B trees are little simpler than that of B plus trees. The searching algorithm over B plus trees is a generalization of binary search. Here it is a peary search in the sense that where P is a fan out of each block. So just like binary search we go about with a given key and starting from the root node of the tree, that is given a search key k start form the root node. If the key is present in the root node, in the current node itself then we are successful. That is the key corresponds to sorry, from the key we are able to end up, find the corresponding leaf node from where we can find a corresponding data pointer.

However if the current node is a leaf node in the B plus tree and key is not present then we can be sure that the key is not available in the database itself then we return not available or else what we do is we search for the different pointers such that the key value that we are looking for is embedded between the left and the right most keys. That is if we are searching for the first left, first sub tree that is P1 then our key pointer should be lying between k1 and k2. If we are searching, if we want to search any Pi then we have to search or value, key value should lie between Ki and Ki+1. It is a matter of terminology, the slide shows Ki-1 and Ki it is a matter of terminology whether how we use i minus 1 and i that is we can either say Ki-1 and ki or ki and ki+1. And we continue this search in the left sub tree in a recursive fashion by going back to step two and searching in that node and searching in a sub tree and so on.






(Refer Slide Time: 41:06)

 

What about insertions? This is the main contribution of B plus trees in the sense that we will be able to insert records while maintaining the balanced property of the trees. Now we should be illustrating the process of insertion with an example and we shall not be going into the exact algorithm of insertion which can be referred to in any standard text books. However the illustration serves to help us clear the or help us to clarify the notion of how insertion happens within a B plus tree, the logic behind insertion in a B plus tree. To begin with B plus tree starts with a single node which is the root node and which is also leaf node. It has no parents and it has no children.

And the first key that is inserted into the database goes into the root node and because it is a leaf node, it just points to the corresponding data pointer. As and when nodes fill up that is as and when more and more records are inserted and more and more keys have to be incorporated into the tree, nodes get filled up. As and when they are filled up, nodes are split and this split nodes are made into children of a newly created node and the key values are split or also split correspondingly across these two nodes and the new parent node is updated accordingly. And this split operation is cascaded to levels above so that we end up with just one tree starting from one root node following until the leaf nodes and in a balanced fashion. 










(Refer Slide Time: 43:11)

 

Let us take an example to illustrate our point. Let p equal to 2, that is the fan out factor for just for the sake of simplicity we shall be assuming that the fan out factor is two. that is each node has only 2, has at most 2 children each internal node or root node has at most two children and let us consider a sequence in which records are different keys are inserted and a possible sequence is shown in the slide here that is keys are inserted in sequence 5 8 3 7 2 9 and so on.
 
So they appear in some arbitrary sequence that their sequence need not be ordered and we cannot ascribe any particular property in which keys are inserted. Initially when we insert five, we just have one root node and one key node and data pointer and nothing else in the tree. When we insert 8, it is still just one root node that is shown in the figure here and with two key pointers 5 and 8 and 2 data pointers, two corresponding data pointers. However when then next key is inserted that is key value 3 is inserted, this node overflows that because P equal to 2, we cannot accommodate any more keys in this node. Therefore we require a spilt in this key node. How do we split this node?













(Refer Slide Time: 44:47)

 

Note how nodes have been split in this slide here. Initially we started with a root node which was also leaf node. When you spit this node, we get one more leaf node that is which is shown in right most side of this slide here and another intermediate node that is a node that points to two leaf nodes. In a B plus tree node that the intermediate node has a different structure than the leaf nodes, this has to be incorporated. So 5 and 8 were present in the tree and now key value 3 has to be inserted because 3 is less than 5 and note that the keys always, within any leaf nodes the keys always have to be in sorted form. Therefore the key 3 has to be inserted to the left of 5, to the left of this pointer called 5. 	

Therefore we get two different leaf nodes, one containing 3 and other containing 5 and 8. 
There is a small bug, there is a small error in this slide that is the left most node contains just the key 3 and the right most node contains the pointers 5 and 8 and the non-leaf node or the intermediate node is suitably updated so that 3 appears here that is everything less than or equal to 3 appears in this node and everything greater than 3 appears in this pointer. There is nothing else to be placed here because we don’t have any other keys to begin. Therefore assume that we have got keys in the sequence 3 5 and 8, we would end up with a tree as shown in this figure here.
  
Now suppose 7 has to be inserted. Now, 7 can be inserted into this leaf node without any problem. That is 7 gets inserted here and the nodes and the key values are reordered. Earlier we had just 8 in this node and one 7 was inserted, the key values were reordered so that the keys are always sorted and there is no overflow. However the next key that is key value 2 causes another overflow that is key value 2 has to be inserted at the left most left most side here. This causes causes an overflow and this overflow has to be cascaded up or has to go up the level in the insertion or in the B tree.


(Refer Slide Time: 47:32)

 

Therefore we get a B tree of the following form here that is both key values in the intermediate node now get filled and the intermediate node now points to three different leaf nodes. The insertion of 9 that is the next key value again forms a overflow because nine has to be inserted beyond the last block here. The algorithm tries to insert 9 into the last block which fails and then a new node is created.

(Refer Slide Time: 48:10)

 

Now, because a new node is created and we encounter another overflow. This overflow happens at the level above that is because a new node is created, we need 4 different pointers. However a node can accommodate only 3 different pointers here. Therefore we need to spit even the the node above and and introduce a new level into the tree. Therefore the corresponding tree that gets formed is shown in this figure here that is the next level node is also split so that a third level is created and then the keys are more or less uniformly distributed across the entire tree. As you can notice here the property or the balance property of the tree is maintained as and when the insertion is happening and all leaf nodes are at the same level. That is the height from the root of the tree is maintained at the same level for every leaf node in the index structure.

(Refer Slide Time: 49:16)

 

So that was a brief illustration of how keys are inserted into a B plus tree and we shall not be going into the exact algorithm in order to insert keys in a B plus tree. Deletion of keys have to contend with an analogous problem that is the problem of underflow. In insertion we had the problem of overflow and in deletion we have the problem of underflow. An underflow happens when a node contains less than p by 2, floor of p by 2 keys less than or equal to floor of p by 2 keys. That is note that there was a constraint that alteast half of the keys in a node has to be filled up that is more than half rather. So if it contains less than half then of keys in a node then there is an underflow. Whenever there is a underflow a node is merged with its sibling in order to bring down number of levels in a tree. We shall not be going into deletion algorithms also in detail here.
 
Let us move on to the last aspect of indexing structures, namely how do we deal with index structures on multiple attributes. Until now we have assumed that indexing attributes are the fields on which indexes are maintained or simple indexes that is simple attributes. However the indexes could sometimes be maintained on composite attributes that is a set of attributes forming a key attribute. That is for example department id and employee number, to combine to form a key attribute is a composite attribute. How do we maintain indexes on a composite attribute? There are several different strategies that are variations or extensions of existing indexing structures and we shall briefly summarize some of the main techniques used for indexing multiple attributes. 
(Refer Slide Time: 51:23)

 

One simple way is to index multiple attributes that is maintain a sorted file, a primary index of multiple attributes is to have an ordered index on multiple attributes. That is instead of sorting the file on just one attribute, we sort the file on two attributes that is sort it based on first attribute and among them sort based on second attribute which is the simplest solution possible but which can give us only a primary index as we have seen earlier that is it has to be a ordering attribute and a key attribute. The second strategy for dealing with composite attributes is to use what is called as partition hashing.
  
(Refer Slide Time: 52:13)

 

Partition hashing is a hashing technique which takes a composite attribute that is n different key elements pertaining to this attribute and returns n different bucket numbers. 
We then transform this into a single bucket address by concatenating all these bucket numbers to form the bucket address. So that is another technique for dealing with composite attribute. The third technique that is used especially in applications like data warehouses is to use the notion of grid files.

(Refer Slide Time: 52:50)

 

A grid file is a file that establishes grid structure. What is a grid structure? We can use a grid structure when we know the entire range in which a key value can be spread upon. For example if we know that the roll number of a student ranges form 01 to 150, we know the entire range of the set of all possible roll number. And if you are fairly sure that the distribution of key values in this range is fairly uniform then we can split this range into several different buckets. Now such splitting of key ranges into buckets and combining this different buckets forms a grid structure and these grid structures forms a matrix or a hypercube which can be stored within a single file using several techniques called row major techniques, column major techniques and other techniques called space filling curves and so on, using which they can be stored within a single file. 











(Refer Slide Time: 54:03)

 

This slide shows such an illustration. That is there are two different key values roll number and grade. Roll numbers are ranging from 001 to 125 and they are divided into 5 different ranges, similar 5 different buckets. Similarly grades are divided into 5 different ranges A B C and D and the combinations of this form a grid and each cell in the grid corresponds to a set of bucket address or a set of block addresses which contain records that satisfy both this constraints of keys. So each pool or each cell corresponds to bucket pool.

(Refer Slide Time: 54:42)

 

This brings us to the end of this session. Let us quickly summarize the main topics that we have covered in this session. We covered several kinds of multi level indexes and multi level index has, index structures has several different levels and are usually organized in the form of a tree. A tree contains a root node which is the entry to the multi level index structure, several leaf nodes and many internal nodes that form the tree structure. And for a tree structure to be efficient, it has to be balanced and balancing or self balancing tree structures are what are implemented in dynamic multi level indexes in which we saw B trees and B plus trees. And, we also insertion how insertion and deletions are handled in B plus trees. Lastly, we looked at some strategies by using which we can maintain index structures on multiple attributes that brings us to the end of this session.

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 13

Constraints & Triggers

Hello and welcome to another session in database managements systems. We have covered a quite a bit of ground in the explorations of dbms already. We have looked into logical models of data management from the conceptual perspective, from the physical perspective and so on. We also looked at physical requirements of data storage although we have only looked at very specific kinds of physical data storage and retrieval strategies for databases. We also looked at few kinds of index structures using which we can efficiently access or efficiently search and retrieve for the relevant tuple or the relevant record that we are looking for in a database management system.
 
Usually when we are taking about databases there is sometimes a physiological question that is raised as to whether any collection of data is a database. Especially perhaps in the late 90’s there was this ranching debate about whether the web, the world wide web is just a large database. Now of course one might be tempted to believe that any collection of data is a database and some might argue that is not really true, collection of data is just a collection of data. It is something like the difference between a set, a bag and a relation and so on. All of them are just collections of something’s, a set of tuples or a bag of tuples or relation of tuples all of them are just a collection of tuples.
 
However some of them for example sets or bags impose what are called as additional constraints over this collection. A bag of tuple is simply a collection of tuples and there is no constraint what so ever on what should be the tuple, what kinds of tuple should be there, what should be the size of each tuples, what should go into or what should define the attributes that make up the tuples and what about duplicates in the tuples, how many different kinds of the same, how many different numbers of the same kinds of tuples can be there and so on. There are absolutely no kinds of constraints that are imposed on a bag of tuples. However, when we consider a set of tuples there is an implicit constraint that is imposed that a set is a collection of a things of the same kind. 

Although not always so but usually we talk about a set as a collection of things of the same kind. Therefore when we say a set of tuples, we tend to believe that it is a collection of things or collection of tuples of the same kind that is all of the tuples have the same number of attributes and each attribute or each corresponding attribute of each tuple has the same domain and so on. And there is also an implicit constraint that there are no repetitions in sets that is two or more tuples cannot have the same value for every corresponding attribute that they contain. Therefore there is an implicit constraint of no duplicates that is imposed on a set of tuples and of course in order to implement this, that is in order to implement a bag of tuples and to implement a set of tuples, the kinds of programming that we have to do, the kinds of logical checks that we have to do, changes. 
Implementing a bag of tuples is the easiest. We just maintain one collection of tuples using whatever data structure that we can use for maintaining collections, link list, trees, whatever arrays and so on. Similarly in order to implement a set of tuples we can use the same kind of data structure in order to implement a collection. However we have to keep making checks so that the set property of this tuples are maintained. That is we have to maintain checks that all of the tuples are of the same kind and there are no duplicates that are there in the tuples. Moving on, if we consider a relation of tuples, relation of tuples is also a collection of tuples just like a bag or a set, however it imposes even more constraints on the set of tuples that is we have seen some kinds of relational constraints like the entity constraint, the key constraints and referential constraints, no duplicates and so on.

So a relation also does not allow for duplicates in tuples. It also explicitly states that all tuples have to be of the same kind that is they have adhere to the same schema that is defined by the relation and it explicitly forbids any tuple that does not adhere to the schema and there are strong constraints of about how the values of each attribute should be that is what should be the domains and how each what is the domain of each attribute and where can a value lie and so on. And there are other constraints like key constraints which can uniquely define a tuple and because there are no repetitions in a relation, by default the set of all attributes of a tuple form its super key and so on and so forth. In this session we are going to look at these constraints in much more detail. We are going to be concentrating on the concept of constraints and an associated concept of triggers in database systems. Triggers are concepts that are quite prevalent in what are called as active database systems.

(Refer Slide Time: 07:14)

 

And we are going to be looking at triggers in this context, in this session. So let us look at what are the different kinds of constraints that are imposed or either implicitly or explicitly in a typical database management system. Let us define the term integrity constraints. As the term implies integrity constraints are constraints that strive to enforce the integrity of the database system. What is an integrity of a database system? The integrity of a database system or a system of data elements essentially states that the set of all data elements that are stored in this system is a valid set.
 
(Refer Slide Time: 08:01)

 

Note that a valid set or validity is different from correctness. Validity simply states that or validity is some kind of correctness that is independent of what the user or what the application really considers semantically correct from the usage perspective. For example if I state that the marks obtained by a student can range from 0 to 100 then a value of 200 for the attribute called mark is an invalid number. It is not at all, it is not valid so this pertains to validity. However if I enter a value called let us say 50 for a particular student it may be valid. However it need not be correct, it could be an incorrect entry for that field. The student may have actually obtained 90 marks whereas I would have entered 50 for the student.
 
Therefore it is an incorrect value, however it is a valid value. Integrity constraints as part of the dbms are independent of the application programs. They have no idea about what is the application context or what is the usage scenario in which this particular data element is being used. Therefore they can only talk about or they can only enforce validity of the database values or of the data elements stored in the database. They cannot obviously ensure correctness of the data that is stored in the database. 







(Refer Slide Time: 09:47)

 

One of the first form of key constraints in the relational model, one of the first form of constraints in the relational model is the key constraints. We have seen key constraints when we talked about the relational model and also in sql but let us revisit key constraints here again for the sake of completeness, when we are talking about constraints. A key constraints is a very fundamental form of relational constraint and it manages entity existence and of referential integrity. 

What is meant by entity existence? Entity existence essentially means that we should be able to deference that is we should be able to identify each entity or each tuple in our database uniquely so that essentially means that each entity should have at least one super key using which it can be uniquely identified or it can be uniquely distinguished from the rest of tuples in the database. In the relational model or in the relational algebra since a relation forbids duplicate tuples, the entire tuple in the worst case itself is the super key of the tuple.
 













(Refer Slide Time: 11:03)

 

So the key constraints therefore states that key attributes should be unique across the relation that is there should be no two tuples having the same key attribute that is if I identify a subset of attributes as a key attribute, there should be no two tuples such that the key attributes are the same that is such that they are indistinguishable as far as the key attribute is concerned. The entity constraint stipulates that the key attribute can never be null because all null attributes are the same, no matter where they occur and there is no difference between a null attribute in the name field verses a null attribute in the age field. All null attributes are the same, it means there is no value associated with it. 

The entity constraints therefore states that the primary key relation that is the relation, the subset of attributes that can uniquely identify tuples in a relation may never be null. And the third kind of key constraint is a referential integrity constraint. The referential integrity constraint essentially is a constraint over the foreign keys. Remember what a foreign is. A foreign key is a set of attributes or subset of attributes within a tuple that refer to another tuple in a different relation, mainly the primary key or any key attribute of another tuple in a different relation. A foreign key or the referential key integrity constraint states that a foreign key should either be null that is it should not refer to any other tuple or if it refers that is if it is not null then it should refer to an existing tuple.
 
We cannot refer to a tuple that does not exist; we cannot assign a manger to a department for example which is non-existent. We cannot assign a manager to a non-existent department. On the other hand we can say that a manager is not assigned to any department at all that is set foreign key as null which is quite acceptable as far as a referential integrity constraint is concerned. 




(Refer Slide Time: 13:20)

 

How do we specify key constraints using sql? We have seen this in the session on sql, let us briefly summarize it here again. Note the creation of a table in sql. The following slide shows a table called employee which has two key constraints, one is the employee number which is the primary key which is marked as a primary key and there is a pan number for each employee which is a secondary key. That is it is given some constraints called non-null and unique and so on. And there is also a foreign key that is the field called REPORTSTO which states who is the supervisor or who is the manager to which this employee reports to. 
 
So the foreign key references another employee tuple and references the field called or references the attribute called employee number in another employee tuple or in another employee of another tuple in employee relation. So these are the circles that are shown in this slide depict how the key constraints are identified. The first circle shows that when we stipulate that pan number is not null then we have identified that as a key constraint that is it is one of the keys by which can uniquely identify tuples in this relation. The second circle shows that the employee number is a primary key which implicitly states that employee number may not be null. So therefore the not null constraint for the employee number is actually spurious but nevertheless it states that employee number is another key which is used as the primary key that is which is used to, which is actually used to deference every tuple in this relation.
 
Finally we identify foreign keys using the foreign key constructs that is we identify which, first of all which field in this tuple is the foreign key and it refers to which field in which other relation. That is it references the relation called employee and the field called or the attribute called employee number.
 


(Refer Slide Time: 15:53)

 

How can key constraints be enforced? Now, that we have seen some physical aspects of data storage or data management. Let us go inside the dbms to see how or what could be a possible means by which constrains or key constraints can be enforced. A simple way of enforcing constraints is to create an index structure over the field that form the keys. For example if we create any kind of index structure, let us say primary index on the declared primary key attribute then we can easily enforce the unique constraint. Note that primary key has to be unique, all keys have to be unique. Therefore whenever a new tuple is been inserted into the database system, we just use the index structure to verify whether a key attribute of this value already exist in the database system or not.
 
If it already exists then inserting a new tuple with the same value is going to violate the uniqueness constraint so in which case this can be rejected and the integrity constraint can be enforced. Similarly for all other attributes that is all secondary key attributes, we can maintain secondary indexes such that we can always verify whether before insertion of a tuple whether the corresponding value for that key actually exist in the database or not. 

Referential integrity similarly can be verified by using a primary index over the other table that is the over the table that is being referenced. Therefore whenever I insert a tuple in a relation that contains a foreign key, I first verify whether this tuple in fact really exists in other table whether the foreign key, whether the foreign key references to a tuple that actually exists in the reference table. Only if this is so then I have to allow the insertion of the tuple in the referencing table.




 
(Refer Slide Time: 18:03)

 

So let us probe a little further into enforcing referential integrity. Sql automatically rejects whenever a tuple is being inserted into a relation containing a foreign key such that the foreign key is not null and it refers to a non-existent tuple and this rejection is performed using corresponding index structures. And by default sql uses the restrict option for managing alterations or updates in tables. For example if a tuple is deleted or if a tuple is asked to be deleted in the referenced table that is let us take an example of a manager working in a department. If the department tuple of the corresponding department table is to be deleted then sql rejects such a deletion because there is a foreign key constraint that is from the table called manager which is referencing this tuple.
 
Unless of course we use the cascade option in which case even the corresponding referencing tuple would be deleted that is the corresponding manager tuple which is referencing the department tuple would also be deleted when the department is deleted. Similarly sql rejects any updates to the part to tuples that can affect the foreign key constraints. For example if I try to update the department id in the manager table such that it now points to a non-existent department, sql rejects this update. On the other hand if I try to update the department table and change the department number such that it violates some foreign key constraints that are pointing to it that is it makes some foreign keys dangling then sql would reject such an update.








 
(Refer Slide Time: 20:21)

 

Of course updates and deletes, we can force the dbms to go ahead with the updates and deletes by using the cascade option in which case the overall referential integrity is still maintained. However updates and deletes are cascaded that is every referencing tuple is also updated whenever the reference tuple is being updated.
 
(Refer Slide Time: 20:48)

 

There are also other constructs in sql that instructs the dbms to perform in the different fashion, to act in a different fashion than mere cascade. A cascade simply says that whatever changes is being made in the reference tuple make the corresponding changes in the referencing tuple. 
That is if I change my department id from 50 to 102 change all referencing tuples that are referencing to department number 50 to department number 102 that is simple cascade. However we may want to do some other operations other than simple cascade. There are other options that we can use to instruct the dbms to perform differently. One of such option is a set null option. Have a look at the relation shown in the slide here. The slide creates or the relation creates a table called EMPLOYEE where employee number is the primary key and there is a field called name and there is a foreign key called REPORTSTO which is the employee number of the manager or the other employee to which this particular employee reports to. 

Therefore, foreign key references employee relation and the employee number field and then in the foreign key relation or in the foreign key specification there are two other constructs. The first construct says that on delete set NULL and then the second relation says or the second construct says that on update CASCADE. What do these things mean? The first thing, the first construct says that on delete set NULL. Essentially it means that if the tuple that I am referencing to is deleted then set NULL. That is set this field to be NULL. Note that a foreign key can be NULL without violating referential integrity. Therefore this semantics of this statement means that if the person whom I am reporting to is for some reason deleted from the set of employees. Then set the field as NULL that is I am not reporting to anyone.
 
On the other hand if the person to whom I am reporting to changes his employee number let us say I am reporting to a manager with employee number 102 and employee number is changed to 150 then it says on update cascade. That is cascade this new employee number to and make corresponding modifications in the foreign key construct so that referential integrity is maintained.
 
(Refer Slide Time: 23:43)

 

Sql also supports what are called as deferred constraint checks. A deferred constraint check essentially says or tells the dbms that when inserting a tuple that don’t make a constraint check right now. Why do we require a deferred constraint checks? Deferred constraints checks are especially useful when we have circular constraints or circular referential integrity. Have a look at the figure shown in the slide here. The figure shows two tables with corresponding referential integrity. The table essentially states that the first table is a manager table which contains a primary key called employee number and the name of the manager and the department id that is managed by the manager.
 
The department field is a foreign key into the department relation. The department relation in turn is having a primary key called department id, it has a department name and a field called manager which in turn is a foreign key onto the employee number. Consider that this company which is having a schema like this has recently upgraded its database system and now they have installed a new dbms and they are now porting the set of all data that they have from their old database system to the new database system. Now when they are porting which essentially means that they are adding these tuples in a batch mode, adding a set of tuples in a batch mode. There is no guarantee that or it is very difficult to guarantee that the tuples would be added in the order in which they are required. That is whenever I am adding a department id, the employee the corresponding employee id for the manager already exist and whenever I am adding a manager tuple, the corresponding department id for which he is a manager already exist. This is very difficult if not impossible to sustain or maintain. So in such situations it is useful to have a deferred integrity check.
 
So if we use the deferred or deferrable option in sql then the dbms defers integrity checks till the present transaction is completed. We have not looked into the concept of transaction has yet. However let me give a brief intuition about the notion of transaction. A transaction simply is a logical unit of database operations. For example if I talk about debiting and crediting between two accounts, let us say I am performing wired transfer between my account and my friend’s account. So a transaction in this case is the set of operations that debits a set of, an amount of money from my account and credits it to my friend’s account. This entire set of operations belongs to one semantic entity or semantic transaction so to say and either both of them have to be performed or none of them should be performed. That is either both debit and credit should happen or none of them should happen. 

It shouldn’t be the case that my account is debited but my friend’s account is not credited or the case that my friend’s account is credited with some money from somewhere but my account is not debited. Either both of them should happen or none of them should happen. The deferrable option essentially defers integrity constraints that is until the end of the transaction. In the middle of the transaction we might encounter a situation where the integrity is violated. However that is as far as we have at the end of the transaction, we have perfect integrity that is we have maintained the integrity of the overall database system. This slide shows how or what is the syntax of the deferrable construct. Have a look at the slide here. This slide creates a table called manager which is shown in the previous slide or which was shown in the previous slide and manager has an employee number attribute which is a primary key and name attribute and a department id which is the foreign key. 
 
(Refer Slide Time: 28:32)

 

Of course there is no need to separately specify foreign key if we directly say that department id references some other tuple that is department department id and we say that it is deferrable and also that initially deferred. That is it is initially deferred that means to say that deferrable essentially means that it is possible or it is ok if integrity check on this referential integrity is deferred till the end of the transaction. 

And this second one says that initially deferred means explicitly tells the dbms engine that as soon as a tuple is tuple of type manager is inserted, initially defer the referential integrity checks. Let us to move onto other kinds of constraints from key constraints. The next kind of constraints that we are going to look at are constraints on attributes. Of course we have already seen a few constraints on attributes as some examples are shown in the slide here.













(Refer Slide Time: 29:42)

 

The first constraint that we have seen was the NOT NULL constraint which essentially states that null value for this particular attribute is not a valid value. And the second kind of constraint which is also key constraint but some sense also an attribute constraint is the unique constraint. It essentially checks against duplicate values of the attribute being inserted into the relation.
 
(Refer Slide Time: 30:13)

 

A more general form of attribute constraints in sql is what is called as the check option. We can specify any general kind of attribute constraints using the check option on a relation. There is an example or there are two examples shown in this slide here. The first example defines a field or an attribute called gender and says that it comprises of a single character char 1 and then imposes an integrity check on the value of this field or the value of this attribute. It says that check gender in M or F that is those are the only two valid characters that can be assigned for gender. Even though it is a single character, it is not just a single character but it is a single character in this set of two characters M and F.
 
Similarly the second example shows a field called age that contains an integer of two digits which can contain an integer having two digits and there is a check that states that check that age is greater than 18 especially if you are talking about employee records and so on. There might be a legal integrity constraint that is a legal minimum age for an employee of 18 years. Therefore that can be directly included as part of the check constraint that is check age greater than 18. Note that the first constraint that is check gender in M or F is a physical constraint that is it is a constraint that is given as part of the physical world around us.

And the second kind of check that is used that is check age greater than 18 is the normative constraint. It is a constraint that is imposed by the local laws or the set of norms which define the set of correct behavior within this system. And it can change from system to system in some places probably the minimum working age is let us say 16 or in some other places it could be 21 and so on. So that is a normative constraint while the formal constraint is a physical constraint.
 
(Refer Slide Time: 32:38)

 

When you ask a kind sort of interesting question, can the attribute constraint that is can the check constraint on values of attributes be used to enforce referential integrity. Referential integrity as you know is a key constraint that is it is a integrity constraint across different relations. However have a look at the small declaration here which might seem to say that we can actually use a check as check for enforcing referential integrity. 
The example that shows the declaration of a field called department id and department id shown as integer having 6 digits and then there is a check here. The check says that check department id in select department id from department. Now this is the department id of the manager tuple and when a manager tuple is being inserted there is a check that is made to see that the department id that is being inserted for the manager actually exists in the department relation which looks like or which seems to suggest that we can enforce a referential integrity using, referential integrity using check statement. That is if it try to insert a tuple or if we try to insert or even modify a tuple of the manager relation such that it tries to reference to a non-existent department id, this check fails and the updates or insertion is rejected which seems to suggest that referential integrity can be enforced using check. 

However look at the other way around. What happens if a department existed when the manager tuple was inserted and then at a later point in time, the corresponding department tuple was deleted. There is no way that this check constraint is now enforced because the check doesn’t even know that the corresponding tuple from that is being referenced in the department table is being deleted. Therefore because of this, because such situations cannot be handled by check, we say that we cannot use the check condition for enforcing referential integrity. The check constraint can not only be used on just an attribute basis, it can be used on a tuple wide basis. 

(Refer Slide Time: 35:28)

 

That means a check constraint can actually perform checks on several attributes on a given tuple. The following example shows such a situation where it says age INT of 2 that is the age of an employee let us say is declared as an integer having two digits and a check is performed to see whether age is greater than 18 that is the legal age for a particular employee in a given company setting should be greater than 18 or there could be children employed by the company as long as the job type is based on talent. That is job type is based on encouraging the child’s talent. 
Therefore you can either perform a check on the age field which says age should be greater than 18 or if age is not greater than 18, you can check a separate field or a separate attribute called job type to see whether it is set as talent in this particular tuple.
  
(Refer Slide Time: 36:39)

 

It is possible to give names to constraints to declare in sql so that they can be referenced by their names rather than by their actual conditions. This particular slide shows such an example where the check called the legal age of an employee or the minimum age of an employee, that constraint on this is given a name called legal employee. That is age INT of 2 and name is given called legal employee which is the constraint called check age greater than 18 or job type equal to talent. 
(Refer Slide Time: 37:20)

 
Once we have named tuples, we can alter constraints that is once we have named constraints we can alter constraints that is we can add or delete constraints by referring to them by their names. For example the following slide shows two different alter commands. The first command says that alter table employee drop constraint legal employee. If the particular constraint is no longer valid, we can refer to a constraint by name and then say drop it. So that constraint is no longer enforced in future additions of or future updates to the table.

Similarly this second alter statement shows how a new named constraint can be added to the table at any later point in time. That is alter table manager and add constraints all works that is the constraint name is called allworks and check the department id is not null. That is check to see that for all the employee manager tuples in this manager relation, the department id that is the corresponding department which they have to manage is not null. That is there is no such manager who is not managing a department. 

Now what happens if there are already some fields or already some tuples in the relation whose department id’s are null. In such a case the addition of this constraint fails that is the constraint cannot be added because it cannot be enforced on this table because there are already some managers whose department id’s are null.
 
(Refer Slide Time: 39:11)

 

A more powerful kind of constraint on in sql is what are called as schema level constraints. Until now we have been looking at constraints that acted at the tuple level or at the key level; tuple or attribute or keys or so on. That is which acted on specific instances of a relation. A schema level constraint on the other hand acts at the level of a schema that is which is enforced for all tuples on a database wide basis. One such powerful constraint, a powerful general purpose constraint is what is called as assertions.

Assertions are general purpose checks that can be performed on the entire database and can be enforced on the entire database for the entire time in which the database is in operation. That is if I make up a specific assertion, the assertion must be true when it is made otherwise the assertion fails of course and the assertion must be true when it is made. And it must be true throughout the life time of the database or until the assertion is dropped. That is once an assertion is made, it will be enforced that is every modifications to the database in whatever table, in whatever tuple would be checked to see that the particular or this specific assertion is not being violated.
 
(Refer Slide Time: 40:50)

 

This slide shows an example of an assertion. An assertion can be created using the create assertion command. This slide shows create assertion command that creates an assertion called NoBench. NoBench is a name of an assertion and the assertion essentially checks that is an assertion is a check statement which checks to see that they does not exist any manager whose department id is null. That is there is no manager who is on the bench so to say that means who is not assigned any department to manage and this assertion should be true when this is first asserted. That is when I execute the create assertion statement, all managers in the database that are listed in the database should be associated with a department. And if there are any managers who are not associated with a department then the create assertion fails.
 
The assertion can be created only when either the managers who are not assigned any departments are modified to assign departments or those tuples are deleted from the database. And once an assertion is created that is once such an assertion is created it is enforced or it is maintained whenever there is a database update that is happened. And any update or any addition of a manager tuple whose department id is null is rejected. And this set of assertions holds for the life time of the database or until the assertion is dropped or until the assertion is deleted. Assertions can be dropped using the drop assertion command.
So we just say drop assertion NoBench and then the assertion no longer holds and further on tuples violating this assertion can be inserted into the database. The second kind of schema level constraints that we are going to look at are what are called as ECA rules or triggers. An ECA rule expands to an even condition action rule, this is an integral part of an specific kinds of databases is called active databases. However ECA rules have been incorporated into most commercial databases as of now, that is let us say db, ibm db 2 oracle 9i or 10 g which is the latest all of them incorporate some form of ECA rules. 
 
An ECA rule or a trigger is again a database wide constraint but which differs from an assertion in the sense that there are not always active that is they are not always enforced and ECA rule is enforced or an ECA rule is awakened only when certain specified events occur.
 
(Refer Slide Time: 44:08)

 

That is note that ECA stands for event condition and action. So when an event occurs a corresponding ECA rule is awakened. The rule then performs a or the rule then performs a condition check to check whether the condition holds. That is when the event condition, when the event occurs if the condition holds then the rule performs a given set of actions that is the A part in the ECA rule. And the action could be anything, it could be like preventing the event from proceeding or undoing the event or any other set of database update operations that could be totally unrelated to the event. 

It could actually be something like intimating the user about the event and so on. And the action in turn may generate more events which in turn could trigger more ECA rules and so on. There are several options that sql provides to handle ECA rules. We can either specify that the action part of a ECA rule be executed either before the event occurs or after the event occurs. That is before a particular event is going to happen, make a check for any ECA rule that is waiting on this event or one could check for performing the action after the event occurs. The action part of an ECA rule can refer to both old and new values of data elements that are modified by the event. 

(Refer Slide Time: 45:34)

 

That is sql or several or most commercial dbms systems would maintain the older values of data elements that were modified by certain update events in case they trigger certain ECA rules and the ECA rule would want to refer to the older value of the data element. For example one might specify an ECA rule that says alert the user whenever the current stock price let us say falls below or falls by greater than 5%. 

Therefore we need to know the older and newer value of this particular stock price in order to see whether the change in one update is greater than 5%. If that is so then the user is alerted and the action part of an ECA rule can be either specified to be performed once for each modified tuple or once for all tuples that are modified during the update event that is during the database operation.














(Refer Slide Time: 46:49)

 

This slide shows a particular example of an ECA rule. It says all the keywords are shown here in a highlighted form, the keyword to specify an ECA rule is the term called create trigger. So this statement creates a trigger called overdraft which has to be performed after update that is the action part of this trigger has to be performed after the update. Note that actions could be performed either before or after the updates, so this has to be performed after update on a relation called pre-paid and referencing each new row that is the new row that were modified has nrow and for each row that was modified, it checks whenever that is when balance that is nrow dot balance attribute is less than or equal to 0 then update that row and set block or set the attribute block equal to true. 

(Refer Slide Time: 48:11)

 
That is for every pre-paid account whose balance is less than or equal to 0, the account is automatically marked as blocked as part of this trigger operation. What are some of the properties of ECA rules? ECA rules are automatically triggered by the DBMS. The application program need not even be aware of the corresponding ECA rules that were triggered. It is the rule writers responsibility that is whoever enforces these rules or whoever wrote these rules to enforce these integrity constraint, it is the rule writers responsibility to ensure that the rules terminate. And it is also the rule writers responsibility to prevent cascading triggers that is this action would in turn create another cascading action which in turn would trigger some more rules which in turn would create another cascading action triggering more rules and so on.
 
So it is a rule writers responsibility to prevent such conditions from happening. A rule write can also write a cyclic rule. That is rule A triggers rule B and the action of rule B triggers rule A that is rule A tries to undo something, undo an update and which in turn triggers rule B which tries to redo it back and it can obviously go into infinite rules. In most dbms systems termination and infinite cycles are simply handled by seeing or just maintaining a count of the nesting level that is how many iterations have been made and if this crosses a maximum threshold then the entire set of operations including the update that triggered this infinite rules are rolled back. That is they are all undone and nothing is changed and the database state is taken back to the state before the event that created this infinite looping of rules.
 
(Refer Slide Time: 50:09)

 

The last kind of integrity checks that we are going to be looking at in an sql based database system are the notion of authorization and privileges. Authorization and privileges talk about which user is authorized to do what or which user enjoys what kind of privileges. Users in an sql based system can enjoy certain privileges that grants them certain authorization. What do we mean by authorizations? And this could mean an authorization to read a particular tuple for example is a particular or is a given user authorized to let us say look at the account details of some other user or is a given employee is authorized to look at salary details of some other employee and so on.    

So there could be a read authorization, there could be inset authorization that is a user allowed to insert tuples into the database. There is an update authorization, delete authorization, index authorization that is index essentially means that can a user be allowed to create and delete indexes on a particular table. Why is this security constraint, why is index important, that is why should I use authorize to create and delete indexes this is because what happens if I use a weather intentionally or unintentionally deletes the primary key or the primary index structure on a tuple. Then the entire database or the entire data file becomes extremely inefficient to access. Therefore a user used should be authorized to create or delete indexes then alternation and drop authorizations.
 
(Refer Slide Time: 52:01)

 

What about views, what kinds of authorizations can we impose on views? Most database systems allow authorizations to be specified on views without obtaining the specific authorizations on the base table. That is let us say a hr manager would have a read authorization on a hr based view of the employee records. That is which contains employee salary details, perks and so on. However there may not be or the manager may not enjoy read authorization on the actual employee tuple which may contain some more information like his or her personal details and so on which the manager may not be authorized to access. Therefore even if the corresponding authorization does not exist on the base table, it is possible to obtain certain authorizations on the view. And since views are derived table, whenever a query is given through a view authorization checks should be performed before the query is answered. 




(Refer Side Time: 53:09)

 

The general form of providing authorizations is to use a grant command. The grant command simply says that grant privilege list on a particular relation or view name to a set of users.
 
(Refer Slide Time: 53:26)

 

This slide shows such an example which says GRANT insert update and select privileges on the manager table to public. Public means that all users that is everybody, every user in the database system.


(Refer Slide Time: 53:42)

 

The privilege names used in the grant command or the same names as the sql command that is used for this privilege that is a select privilege gives an authorization for reads, an insert privilege gives an authorization for inserts and so on. An update privilege gives an authorization for modification and so on and all privileges, the keyword called all privileges suppose we say grant all privileges, it means all the above privileges can be granted to the specific user in question.

(Refer Slide Time: 54:16)

 

It is possible to revoke privileges that is by using the revoke command that is revoke privilege list on relation or view name from user list and of course we can use now familiar terms called either restrict or cascade. Restrict basically means that if I revoke a particular privilege from a set of, from a given user all the privileges that were granted by the user to other users will not be revoked, they stay in place. On the other hand a cascade privilege says that if I revoke privilege x from a user and if that privilege X has been passed on or granted by the user to other users, they are also invoked in a cascading fashion.
 
(Refer Slide Time: 55:04)

 

So that brings us to the end of this session where we have looked into the notion of constraints in the database system. Essentially we have seen in different kinds of constraints that a typical dbms system provides and they can be classified as either key constraints that act on the key attributes of a particular relation or they could be attribute constraints which can act on any attribute which checks domains or validity of the attribute value with reference to its domain in any given relation and there are tuple constraints which act on the complete tuple itself that is where the value of certain attribute may depend on the value of other attributes on a tuple wide basis and so on.

And then there are schema level constraints especially assertions which are a general purpose schema level constraint which maintain certain integrity over the entire database for the lifetime of the database or until the assertions are dropped. And finally we looked at triggers or ECA rules that are not valid always but are awakened whenever certain events happen and a certain condition holds which in turn prompts them to perform certain actions. We finally then looked into the notion of privileges and authorization that different users in a database system can hold and they could be granted certain privileges and privileges could be revoked from them and this can happen in a restricted fashion or in a cascading fashion. So that brings us to the end of this session.


Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 14

Query Processing and Optimization

Hello and welcome to another session in database management systems. In one of a previous sessions especially when we are talking about a storage structures and index structures, we talked about what is the biggest challenge facing databases yesterday. The challenge is no longer the problem of storing data or of designing that can store larger and larger amount of data. In fact we have very small devices today that can store huge amounts of data, devices that you can probably put in your pockets are something which you can wear inside your watches and so on, which can store something like 2 giga bytes of data. Therefore the problem today is not primarily of storage of data, storage of data has become much more, the surface area required for storing data has become, has shrunk in tremendous proportions and the cost of storing data is also fallen tremendously over the years.
 
However this fall in cost or this affordability of massive amounts of data storage has resulted in a new problem or a new challenge. The challenge is that of retrieval of data, how efficiently can we retrieve data among the huge amounts of data that we have stored. We also saw how the definition of very large databases has been changing over the years. Today in, 10 years ago very large databases would probably have meant hundreds of megabytes of data, probably of few giga bytes of data. However today when we talk about very large databases, we are talking about databases that are easily into peta bytes of data 10 power 15 bytes of data. So we saw, we had mentioned these things when we are trying to motivate the use of index structures or the auxiliary files. Those are files containing metadata that can quickly point to or that can help the application program or the query to quickly retrieve the required data elements form the database.

There is however another aspect of the story using index structures alone does not help in efficient retrieval. The other crucial element in effective retrieval of data and making a data or making the difference between usability and unusablity of a database is a query execution strategies or the query execution and optimization strategies. This is the topic of this session and the next few sessions that we are going to consider. Query execution and optimization, it goes without saying is an extremely important aspect of database management and this is what is going to determine whether a particular query is going to be useful at all or not.
 
If a particular query execution strategy takes enormous amounts of time to retrieve a particular, to retrieve a given data element that can make the difference between whether the query is interactive in nature or whether the query is batch in nature. That is whether you have to, whether the database will have to say that please come back after two days for your query results and so on. 
So let us look into what makes up a query execution and query processing and optimization.
 
(Refer Slide Time: 05:07)

 

Query processing: As we said before effective query processing or efficient query processing is crucial for good or even effective operations of database. A database can be rendered unusable if a good query execution strategies are not used. Let us do a quick calculation. Suppose I have 1 giga bytes of data and 1 giga byte as you know is 1000 megabytes of data. Even it is safe to assume that for most of the server class pc’s today, we have data transfer rates of something like 1 megabytes per second. It is usually 1 mega bit per second, let us consider that it is 1 megabyte per second or 8 megabits per second. So this, if I have to access or if I have to scan through a relation let us say I have a query that select query which requires me to scan through the entire relation of 1 giga bytes of data. That means it would take about thousand seconds for me to scan the entire relation because it is of a one giga bytes of data.

Now consider a query which is given on two different tables, each of one giga byte of data so there are two giga bytes of data that is there in the database. Now a bad query execution plan would actually try to compute a cartesian product of the two tables before trying to return the results that we required. Now if I have to compute the cartesian product of 1 giga bytes of data times 1 giga bytes of data where each axis of the table is going to take me 1000 seconds then it is going to easily take me 10 power 6 that is one million seconds just to compute this cartesian product which is clearly unusable and which is clearly ineffective as far as an interactive response time is concerned. Therefore efficient query execution or trying to rehash a given query in a more effective form. For example in this, in the example that we just took up, the query might be able to figure out that what the user really wants is a natural join for example or some kind of an equi join rather than a cartesian product. 
If it is able to figure that out then probably you would get a response in a little more than a 1000 seconds which is much better than a million seconds for query execution and query processing depends on a variety of factors and not all of these factors are under the control of dbms. What are this factors that can affect query execution? Let us take a few examples. One of the factors that affect query execution is for example whether the storage media is fragmented or defragmented. If the storage media is let us say fragmented, if you remember what is meant by fragmented storage, in a fragmented storage contiguous block on the storage media belong to different files. Now if I have to access a relation if I have to scan through a relation, let us say in response to a select statement and these blocks are divided or distributed all across the storage medium then the response time would be increased considerably for this query. So, not all query execution or not all factors that affect queries or query execution times or under the control of the dbms. 

The example that we took right now is not purely outside the control of the dbms also. This is because several different, several kinds of database management systems would override the operating system mechanisms and then start to deal with devices directly. For example they create their own file system that can ensure that the file system is never fragmented at all and so on. Therefore some high end database management systems would try to overrule the underlying operating system and try to access the hardware directly in order to speed up query execution and speed up or decrease response times.
 
And as we saw earlier insufficient or incorrect information about factors that can, that affect query plans can lead to vastly ineffective queries. For example if a query execution plan estimates that the size of a table is let us say few kilobytes but the size of the table is actually a few giga bytes then whatever execution plan that it uses for a few kilobytes will not work for the few giga bytes table because the entire strategy changes when the scale of the problem changes. A few megabytes can probably be or few kilobytes can probably be stored in main memory whereas a few giga bytes cannot be stored in main memory also. 

And query executions usually use what is called as catalogs. We shall be looking into catalogs in more detail in a later session. They use what is called as query cataloging that help in estimating several factors or several kinds of information about the database. This can include the size in terms of bytes of a particular table, the size in terms of tuples or the number of tuples in a table and estimate of the number of tuples and an estimate of the number of distinct values in a tuple that can help in building indexes for example.
 
Query catalogs hence play a very crucial role in deciding the query execution plan that is ultimately used on the database management system. How does a typical process or query execution process or query processing, process look like? The typical steps in a query execution process is quite similar to the execution steps in typical compiler, the way a compiler complies a given high level language construct into machine language and executes it. 


(Refer Slide Time: 12:08)

 

We start with the user description of the sql query. The sql query is then read and parsed that is the sql query is then read by a query complier that performs scanning where lexical analysis is performed that is the sql query is read character by character and then tokens out of the characters are recognized. And then a token stream is given to the parser which in turn parses the query, constructs a syntax tree and then the tree is validated for semantic checks for types and interoperability and so on. And once this is done, an intermediate form of the query is generated and which is called the logical query plan tree. 

This intermediate form of the query is usually a relational algebra representation of the sql query. Now once this intermediate query tree is generated, a series of heuristics and cost based searches are used to rewrite this tree or rewrite this query execution tree. In order to make it more optimal or in order to make it in order to use a better equivalent query for whatever execution, whatever query has been requested by the user. 














(Refer Slide Time: 13:33)

 

This intermediate query representation is then given to the query optimizer which in turn generates the query execution plan. That is it rewrites the tree in order to reorder few of the operations and then a final physical query plan tree is created. We shall be looking into a query optimization strategies in a later session. However there are optimization strategies can be broadly divided into either a heuristics based optimization strategy which are essentially some kinds of thumb rules which have been known to yield better strategies for query execution. And then there are what are called as cost based optimization strategies where an estimate of the cost that is required to execute one query plan against the other is used in order to utilize the best, potentially the best query execution plan.
 
The physically query execution plan is written in a separate language not necessarily the machine language but there is a separate language that uses its own construct or its own algebra for representing what are called as internal queries. That is the query that are actually performed on the storage structures on the physical files that are stored onto disks. The query execution plan is then given to the query code generator which either executes the query as it is that is starts giving results directly which is called the interpreted mode of query execution or it generates machine code which is called the complied mode of query execution which can then be used to actually perform the physical operations required to answer the query. 








(Refer Slide Time: 15:.31)

 

The code, the machine code that is generated in a compiled mode of query execution is then given to the run time database processor which executes the code and returns the query results. 

(Refer Slide Time: 15:49)

 

In these steps of query executions, two aspects are important and interesting. These are the intermediate form of the query and the physical query plan. The intermediate form of the query as we mentioned before is usually a relation algebra equivalent of the sql query that the user has given. The intermediate form of the query is in the form of tree structure which is also called as an expression tree where relational algebra operators are on the non-leaf nodes and the actual domains form the leaf nodes. That is the actual relations on which query is executed from the leaf nodes. The tree is then rewritten based on a set of rules that are derived from either heuristics or cost based optimization to generate an equivalent tree which produces the same query but preferably in or hopefully in much lesser time with much lesser overheads. 

The physical query plan is written in a separate language which has its own construct and that is either interpreted or complied into machine code. Let us have a look at what constitutes the physical query execution plans and what are the constructs that make up this physical query execution plan. The logical form or the logical query execution plan or the intermediate form, we shall be looking into greater detail in later session.  

(Refer Slide Time: 17:24)

 

The physical query plan comprises of a basic set of operators that define the language of physical query execution. Now what should this operators be? Obviously the operators at the lower level or the inner query or the internal query should contain all the operators of relational algebra itself. That is if the relational algebra says select on this condition, the internal language should also be able to support an operator that can select a particular tuple based on a set of tuples on this condition. 

However in addition to the relational algebra operators, there are several other operators which talk about a physically accessing tables and iterating through them or several other physical operations. Note that relational algebra itself does not concern itself with the physical implementation of the database. 





(Refer Slide Time: 18:23)

 

Let us have a look at a few candidate physical query plan operators and see how they work that gives us a flavor of how does the physical code actually look like or the query execution code look like. The first operator that you are going to see is the table scan operator. A table scan operator as the name suggests just simply scans a particular given table that is it scans and returns an entire relation R or the operator can be parameterized in the sense that you can give certain conditions to the table scan operator that scans a given relation R and returns only those tuples that satisfy the given condition or the given predicate.
  
The main operations that are performed by table scan is to read all blocks. Note that blocks is the physical component in terms of which the records are stored. So a table scan contains a code or the operator for table scan contains code by which blocks belonging to a particular file or a particular table are read in sequence and the block is and the table is returned. There is also an index scan operator that makes use of an index file in order to read all blocks of a data file in sequence.
 












(Refer Slide Time: 19:50)

 

Another operator that is used usually in the physical query plan language is a sort scan operator. The sort scan operator scans a relation R and sorts these results before returning it to the higher level whichever called it. If the relation is already stored in a sorted form and the sorting is also required on the same ordering attribute, no sorting needs to be done separately by the sort scan operator. And if the relation is small enough to fit in memory then sorting can be done directly in memory. However if the relation is too big to fit in memory then external sorting and external sort merge techniques have to be used in order to sort the given record.
 
(Refer Slide Time: 20:41)

 

The next physical query plan operator that is quite frequently used is what is known as the iterator. Iterator is an important concept in managing or in the physical management of data records. If you have probably let us say programmed in a, done programming  c plus plus let us say using the standard template libraries on unix environments or the active template libraries on Microsoft environments, you would have come across the term iterator in several places. What does iterator do? The iterator is an operator that functions on an operand which is a composite operand.
 
For example an iterator operates on a hash table or a linked list or a tree or something like that, so the iterator operates in a way that iterates through each element that forms the composite operand. That is it starts from the first element and it comprises of GetNext function which can get you the next element, until you reach the end of the operator or the data element.
 
So iterators typically contain three different functions as shown in this slide here. The first function open will open the iterator object that is the composite object on which the iterator function has to be performed. The next function called GetNext is going to get the next logical block that or the next logical record or next logical node or whatever it is in this composite object that has to be returned. And then the last operator called close, closes control on the object.
 
(Refer Slide Time: 22:35)

 

This slide shows an example of the iterator function, example of a table scan iterator that is how we can implement the table scan operator using an iterator. As the slide shows there are three different functions that have to be implemented. Open, GetNext and Close which is shown in the next slide here. The Open function let us say given relation or a given file let us consider relation to be stored in a file or a table to be stored in a file, the open construct initializes two variables, a variable called b which points to the first block of the relation and a variable called t which points to the first tuple in inside b. 
The GetNext function just iterates through this variables that is before we go in to GetNext function, let us try to ask ourselves what should the GetNext function do. Ultimately for the program which is calling the table scan iterator, all that is required is the set of tuples one after the other. The GetNext function however has to deal with two different, two different things the blocks that is the physical data stores and the tuples that is the logical data stores.
 
Now tuples can be iterated within blocks but when a block is exhausted, the blocks themselves have to be iterated across the files. That is the next logical block in the file has to be chosen. So the GetNext function performs precisely this set of function. That is if t is beyond the last tuple in b, that is if the current block b has been exhausted then we increment b that is point b to the next logical block in this sequence in the file. And if b is beyond the last block in the file then you return no more data that is it is exhausted in the record or the file or else the else condition here states, is basically t is beyond the last tuple in b but b is not beyond the last block in the file. That means set t to the next or set t to the first tuple in b that is the next b is been incremented and set t to the first tuple in b or the new block.
 
(Refer Slide Time: 25:10)

 

And then increment t and return the old value of t which t was pointing to. So return, that is we first assign oldt equal to t and then increment t and then return oldt. For close we don’t have to do anything because we have already returned, we already returned from the GetNext operator if we have reached end of the file. Therefore close in this example is redundant, however usually the close function performs some kinds of a cleanup operation where if some data structures were opened during the course of the iterator function, these data structures are closed and the corresponding memories freed and so on. Let us implement or let us look at an example of the iterator function. We shall implement the table scan operator that we saw earlier using the iterator function. 

(Refer Slide Time: 26:16)

 

As we saw before an iterator has three different functions the Open, GetNext and Close. Assume that the table is implemented or is contained within a file and the file is organized as a sequence of blocks. That is there are several blocks that make up the file. So when we open the iterator that is open the table scan iterator, we need to initialize a few things. This is shown in the slide here, that is the slide, the open function initializes two different variables b and t where b points to the first block in the file of the record and t points to the first tuple in the block.
 
The GetNext function, note what the GetNext function should return here. The table scan operator should return tuples after tuples that is the first tuple, second tuple and so on. however at the physical level, we are concerned not only with tuples but also with blocks that is we actually read and write in terms of blocks and in not in terms of tuples. Therefore initially what we do is we first check to see if the tuple is beyond the last tuple in b. 

If this is the case we have to increment b and if b is beyond the last block in the file then we just return no more data. That is we say that is all, there is no more tuples to return or else that is the else is for the inner if, so else t is then set to the first tuple in the next block that is we have incremented b and then we just set t to the first tuple in the next block. And if none of these is the case then we just return the next tuple that means we first copy the corresponding tuple to be returned into a new variable which is called oldt in this example and then we increment t and then return oldt. 






(Refer Slide Time: 28:10)

 

For the Close function in this example, we don’t have to do anything because we have already returned in the GetNext function when we have reached the end of the file. But usually in a Close function, we use the Close function to clean up whatever mess we have created so to say. That is whatever data structures that we have opened, whatever memory we have allocated which are not useful anymore have to freed up and so on. So the Close operator or the Close function is called at the end of the iterator which performs all this cleanup operations. Let us look at another example using iterators on how to compute the Bag union of two relations. What is the Bag union?
 
(Refer Slide Time: 28:58)

 

Remember we have talked about a considering relation as Bags rather than sets. A Bag is just a collection of tuples or collection of elements without regard to whether there are duplicates in the collection. So it is also called a multi set. A multi set union or a Bag union is simply a Bag that is made of two different Bags. That is you just empty contents of one Bag into the other and you have got a multi Bag union or union over Bags. So this is denoted by the operator plus or the disjoint union operator R plus S. 
 
So an iterator for performing the disjoint union, here we are considering that both R and S which represent relations for us are now in the form of iterators themselves. That is we are abstracted away a relation form being a file to being an iterator that is we know it is just a data structure which we can open and call the GetNext function and then close once we are done with the data structures. So as far as we are concerned, both relations are just iterators. So in the open function of our disjoint union iterator, we just open one of the relations. We say R.open and then we point the current relation to be R. In the GetNext function we say that if current relation equal to R then we have to call GetNext on the current relation that is we just say current relation dot GetNext. And if GetNext returns no more data that is if there is no more data that is returned then start or set current relation as S and then call S.Open. And then in the subsequent GetNext operations, you just call S.GetNext instead of R.GetNext. 

(Refer Slide Time: 30:53)

 

So what we have effectively done is we have exhausted one of the records by calling GetNext as many times as possible. That is whenever GetNext is called on us, we call GetNext on the current Rel that is the CurRel relation. So once we exhausted one of the relations, we open the other relation and start calling GetNext on that function. So when S is exhausted it returns no more data which is what should be returned by the GetNext as well and in the Close function we just close both of these iterators. That is we call R.Close and S.Close. Well, R.Close and S.Close don’t do anything which we saw in the previous example but in case they do certain cleanup operation. It is always a good measure to or it is always a good programming practice to call the corresponding Close operators in our Close operator.
 
(Refer Slide Time: 32:13)

 

So we just went through some of the elements of the physical query plan programming language that is it contains elements of table scan, index scans and iterators and so on. Now let us have a look at some algorithms that are built around these data structures or around this constructs of the physical query plan that can help us in understanding how a given relational algebra operator, let us say like select or project or something of that sort
is actually executed inside the database system. 

We can broadly divide algorithms for a data access into one of the three following categories. We call them sorting based methods, hash based methods and index based methods. These methods as you can see here are typically meant or oriented towards increasing the effectiveness of search. In a sorting based method the relations that are scanned using the sort scan operator that is they are sorted as in when they are scanned and because they are sorted the property that the relations are sorted would help in performing certain other relational operators like say join in an efficient fashion.
 
Similarly hash based methods use some kind of a hash function to quickly search for whatever tuple or data element that is being asked for within this relations. An index base methods resort to index structures like trees or balance trees and so on for searching the required data element. We can also divide algorithms for data access based on what kinds of data access requirements that they pose. We can divide the kinds of data access requirements into one of these three kinds of requirements. The first requirement is what is called as a tuple at a time unary operator that means the query requires or requires to contend with one tuple at time.
 
For example select and project operator. Every time select is called, select has to be or the condition for select has to be checked against each tuple in the relation that is tuple after tuple. So at a time one tuple is being accessed and this is a unary operator that is it is just one relation on which a particular tuple is being accessed. Then there are full relation unary operators where the entire relation has to be searched. For example if I have to return something based on or return the value of some relation or if I have to compute let us say set theoretic operations like not of something and so on or any kind of set theoretic operation that are unary in nature. And the last kind of operations are full relation binary operators. These are operators that again have to compute or that again requires a complete relation as their query result and they are not just unary, they are binary that means they have two relations to contend with.
 
(Refer Slide Time: 35:52)

 

That is some examples or something like set theoretic operators like union, intersection and so on which require two relations and the entire relation has to be scanned the entire relation has to be returned.












 
(Refer Slide Time: 36:10)

 

Let us see how each of these kinds of query execution requirements can be met using some algorithmic strategies. In this session we are going to be looking at a kind of strategies what are called as one pass algorithms. What is a one pass algorithm? A one pass algorithm is an algorithm that performs at most one pass over the entire database that is over the entire relation of interest. It does not access the relation multiple times. very important and many times limiting assumption in most of the one pass algorithms that we are going to see here is that it assumes that the relation that we are looking for is small enough to fit in main memory. In many cases this is the reasonable assumptions but in many other cases, it is not a reasonable assumption. That is even a single relation could be so huge that it may not fit into main memory.
 
So how does, how can we use a single pass or a one pass algorithm to perform a tuple at a time unary operation? Let us take some example like select or project as shown in the slide here. let us say select some condition over R or project some condition over R. All we have to do is scan through this R that is use the table scan iterator for scanning through this relation tuple after tuple and store this relations or store this tuples that have been scanned in a input buffer, perform a unary operator and output it to the output buffer. So this is schematically shown in the diagram here.
 
That is this is the relation iterator and this relation iterator returns tuple after tuple which goes into the input buffer and in this case this input buffer can be as small as one tuple long. That is we can allocate just enough memory in the input buffer to store just one tuple. So each tuple, after tuple is put into the input buffer and checked against the unary operator and either discarded or sent to the output buffer, so as simple as that. This is quite simple that is within a simple single pass, we have been able to answer a tuple at a time unary operation.


(Refer Slide Time: 38:49)

 

What about relation at a time unary operations? What are some examples of relation at a time unary operators? One example is that of let us say the unique function. In the select, in the sql statement suppose I say select name from employee or select unique name from employee that means given me the set of all unique employee name’s without repetitions. This as you can see is a unary operator that means it operates on just one relation. However it is a relation at a time operator that is it requires to have the entire relation, knowledge about the entire relation before being able to return the required value.

So the general strategy or a general one pass algorithmic strategy for relation at a time unary operators is shown in the slide here. R is now familiar table scan iterator which returns to a tuple after tuple which goes into the input buffer. Now the input buffer is then read into the unary operator whatever be the unary operator, whether it’s unique or group by, for example group by is another relation at a time unique operator. Now this unique operator will either output this tuple into the output buffer, if it is safe to do so or otherwise will put the tuple into a data structure holding the history of whatever relation has been read until now. 

For example in the unique operator, all we need to do here is have a hash table that contains one entry each for each unique entry that we have found until now in the database. So whenever I read a new name, let us say whenever we read a new tuple into the input buffer and check out the name attribute, we just check the hash table here the data structure holding history. We just check the hash table to see if this name was already encountered, if you are already encountered this thing. If you have already encountered this name then we just discard this new tuple. Otherwise we add this new name into this hash table here and then output the tuple. So a single pass algorithm for a relation at a time is also quite simple except that we need to have an augmenting data structure in the form of usually an index tree or a hash table or something like that can hold the history that is required. Now one more thing that is to be noted here is that suppose the unary operator that we are concerned with is the groupby operator.
 
Now the groupby operator cannot return any output until the entire set of relations or the entire set of tuples in this relation is read and the performing grouping is formed using this data structure. That means this space allocated to this data structure should be large enough to hold the entire relation. Therefore such an algorithm cannot be used for relations that are too large to fit in memory because we are concerned only with one pass algorithms in this session here. We assume that the relation can be held in memory so that the entire relation or the entire history of what we have read can be held in the data structure.
 
(Refer Slide Time: 42:35)

 

Let us look at one pass algorithms for relation at a time binary operators. Now one pass algorithmic strategy is vary depending upon on what is the binary operator that we are looking and almost all of the algorithms for binary operators require that at least one of the relation be read completely into memory before we start reading the other relation. And obviously if we have two relations and one is much smaller than the other, it makes much more sense to read the smaller relation into memory and iterate over the larger relation. So let us look at a few examples and which will make this clear.
 








(Refer Slide Time: 43:19)

 

Let us see what is the strategy what is a one pass algorithmic strategy for computing the set union of two relations R union S. I have explicitly used the word set union here instead of just union that means this is not a Bag union. That means to say that we have to compute R union S without returning any duplicate entries in the result. That means we have to remove all duplicates while returning R union S. Assuming that among R and S, R is the bigger relation, here is a very simple strategy to compute R union S. First read S into memory completely using the iterator on S, retrieve all tuples from S and place it into memory and place it in some kind of data structure like an index or hash table by which we can access each tuple of S as efficient of as possible.
 
Now as and when we are reading S, keep outputting the tuples of S because anyway R union S should contain all tuples of S. Then once S is completely read into to memory and indexed in a data structure, start reading R that is the next relation and for each tuple of R that is read into memory, check whether it already exist in S. If the tuple already exist in S then just discard the tuple because we do not want duplicates in the output result. Otherwise if it does not exist in R then or if does not exist already in the relation then just output the tuple. 
 
Now here we are also making another implicit assumption that R and S are sets themselves and they are not multi set. That means there are no duplicate tuples in R itself. Therefore it is sufficient for us to check for duplicates against S, otherwise we need to also store tuples in R so as to check the duplicates within R itself. If we assume that R and S are sets, the set union operator can be performed using the strategy that we outline just now. The next binary operator that we are going to look at is the intersection operator. The strategy for the set intersection operator is also quite similar to that of the union operator.


(Refer Slide Time: 45:56)

 

Assume that we have to perform the set intersection between R intersection S. And assuming that R is the bigger relation, we first read S into memory and then store tuples of S in an in-memory data structure or in memory index or hash tables structure that can  help us access the data elements of S quite efficiently. Then start reading R into the memory tuple by tuple using the iterator for R, then for each tuple of R if and only if the tuple also exist in S, output the tuple of R in to the output buffer otherwise discard the tuple.

(Refer Slide Time: 46:38)

 

What about set difference? Set difference if you see differs depending on whether we are computing R minus S or S minus R because set difference is not a commutative operation. Now suppose let us say without loss of generality, let us say we are computing R minus S and that R is the bigger relation. So we are computing R minus S and the first relation R is the bigger among the two relations. That means we read S into memory as usual that is read the relation S using the S iterator into memory and put S into an in-memory index structure or a hash structure. And for each tuple of R check to see whether it already exists in S. If it already exist in S then discard the tuple or if it does not exist in S then output the tuple as simple as that. But what happens if we compute S minus R, that is R is the bigger relation and it is right hand side of the difference that is instead of computing R minus S, we are computing S minus R. Because R is the bigger relation it is always more efficient to read S into memory rather than R. 

Now if we read S into memory, how does the algorithm change? Let us have a look at that. So this slide shows set difference S minus R instead of R minus S and assuming that R is the bigger relation. Now if R is the bigger relation, have a look at the steps closely for this slide here. If R is a bigger relation, all we have to do is first read S into memory completely that is read the complete, use a S iterator and read all tuples of S into memory and place them into an index structure or a hash table. Now for each tuple of R, what should we do? That is we are computing S minus R that is S minus R is the set of all tuples in S that are not in R. So for each tuple of R, check to see if it already exists in S. And now what happens if it already exists in S? 

If it already exist in S then these tuples should not be there in the final output. And the tuples that should be in the final output are those tuples of S that are not in R. So what we do here is for each tuple of R, for which we find a matching tuple in S we cancel them out that is we delete the tuple in S from the index structure. That is we delete it from the index structure or the hash table that we have been using. Now once R is exhausted that is once we have finished reading through the relation R and we have deleted all common tuples, whatever is left in the S data structure that we have read into memory is the output. That is that we can push them onto the output buffer. 















(Refer Slide Time: 49:47)

 

What about cross product? R times S. Cross product is simple as far as the algorithm is concerned and quite expensive as far as the performance is concerned. That is again assuming that R is the bigger relation just read S into memory and we don’t need any data structure here, we can just store S in its contiguous sequence of memory locations and for each tuple of R combined with every tuple of S and start returning it, as simple as that. 

(Refer Slide Time: 50:18)

 

The last one that we are going to be looking at is a one pass algorithm for natural join. What is a natural join? A natural join is an equi join on two relations that equates attributes having the same name and domain. Now assume that R, X, Y and S, Y, Z are being subjected to a natural join. That means Y is the common set of attributes or subset of attributes between R and S. now assuming again R is the bigger relation, read S completely into memory and then index or place X in a hash table or an index, so that it can be searched efficiently. Now for each tuple of R what we have to do is search through the hash. Now, this hash table or the indexing structure should be done based on the common attributes. That is based on Y here, even if Y is not the key attribute. So, we perform the indexing or the hashing based on the common set of attributes Y and then using which we can search for every tuple of R that is read whether there is a matching tuple in S. If there is a matching tuple, match the two tuples and output it to the output buffer if not just discard the tuple of R. 

(Refer Slide Time: 51:44)

 

So that was a brief  overview of the one pass algorithms that can be used by using the physical query plan operators like say iterator and sort, table scan, sort scan, etc using which we can develop a strategy for performing relational algebra operations like R union S, R intersection S, select, project, unique, groupby and so on. But what are the constraints of one pass algorithms? One pass algorithms are applicable especially for binary relation at a time binary operators. They are applicable only when one of relation can fit completely into memory and it is not just it fits completely into memory, we should also have extra blocks of memory for the other relation. That is let us say if I have M memory blocks available for me, then the smaller relation can be at most M minus 1 in size, M minus 1 blocks in size. It can’t be M in size because we need at least one more block to perform book keeping for the other relation that we are reading from disc.
 




(Refer Slide Time: 53:00)

 

And one pass algorithms rely to a great extent on correctly estimating the size of relation. If a query execution plan in a dbms engine decides to use a one pass algorithm for performing a particular query, relational query, relational algebra operation then it depends for a large part large part on the estimate of the size of the relation that it has. Now if the size is wrongly estimated then it can for example if we allocate two few buffers thinking that the size would be small then the one pass algorithms will be unusable, the query execution plan is unusable, we can’t use the one pass algorithms at all. 

On the other hand if we allocate too many buffers thinking that the relation size is big then we may end up in a possibility of thrashing where memory blocks have to be swapped on to disk and so on. So it is quite crucial to obtain a good estimation in order to use one pass query execution plans.















(Refer Slide Time: 54:15)

 

So let us summarize what we have learnt in this session. We have kind of a scratch the surface of an important and crucial area of data base management systems called a query processing and optimization. We have seen the different stages in query processing and two important intermediate steps in query processing namely, the logical query plan and the physical query plan and in this session we have concentrated more on the physical query plan. That is a physical query plan is a set of language constructs that perform low level operations using, that performs low level operations directly on the storage media that can physically access files into memory for performing any data base related operations.

We also saw what a physical query plan language would look like or rather what kinds of constructs it would have. It would obviously have relational constructs, in addition it would have constructs like iterators, tables scans, sort scans, etc. We have looked at the variety of one pass algorithms using these physical query plan constructors using which we can perform a variety of relational algebra operations like select, project, unique, groupby and many other set theoretic operations. In the next session we shall be looking into the big question of handling joins in query execution plans and also have a look at the logical query execution plans. So this brings us to the end of this session. Thank you. 

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 15

Query Processing and optimization – II

Hello and welcome to yet another session in database management systems. In the previous session we started looking at a very crucial aspect of dbms design. Note that I am using the term dbms design and not database design. We will look at, we will address this issue in more detail a little later, what is dbms design verses what is database design. We started by looking into a very crucial aspect of dbms design namely that of query processing. We saw that nowadays or in today’s world, the size of the database is no longer a problem.
 
We can have huge amounts of data stored in very small amounts of space and being available at pretty cheap cost. In fact I have with me a small device which I can show it for you. This is the small device which can hold a database of size 256 megabytes of store and this cost roughly about 3000 rupees. So you can store data of the order which was not even envisaged before in very small devices at pretty affordable cost and which occupies very small amount of space. Therefore storage of data is no longer a problem, the problem today is retrieval of data.
 
Retrieval in the sense it is not just retrieving any data, it is retrieving whatever data that is required by the user. Therefore we saw the crucial elements today or the crucial technologies of that is going to impact database in the forth coming years are those techniques that can help in retrieving data elements as quickly as possible from databases. 
We saw that auxiliary files in the physical storage world which comprises of index structures and several kinds of hash structures and so on is one crucial element in making this happen, that is making fast data retrieval happen. The second crucial element is query processing, given a users query can we process the query in such a way as to return the results extremely fast.
 
There are several techniques that are used for making query processing interactive in nature that is remember the example of google. If suppose google were to say that when you give a web search query, suppose it were to say come back after two days for your answer it becomes unusable, nobody would use it if the other alternative that is but a search engine like google can search peta bytes of data or the search space of data is peta bytes of data and it can return you relevant results within a few seconds. Now this is possible, one of the techniques for this is to use very intelligent query processing techniques. Of course there are, if you throw more hardware into your database, if you throw faster processors it becomes faster and so on. However a crucial element is still the algorithm that is used to retrieve data. A bad algorithm can make the difference between a database that is very efficient to a database that is unusable for all practical purposes. 
Let us briefly review what we learnt in the previous session in query processing and move on further today to look at some more aspects of query processing. Efficient query processing is crucial for good dbms design and in fact it can make the difference between a database that is operational or effective and database that is ineffective.
 
(Refer Slide Time: 05:14)

 

A bad query processor can render a database all but useless when the size of the database grows beyond a certain size, beyond a certain limit. Query processing depends on a variety of factors unfortunately and not everything is under the control of the dbms. We saw yesterday that query processing could depend on let us say the memory size or the size of the tuples or the size of the relations that are present in the database and the kind of file system that are stored and whether the file system is fragmented or defragmented and so on. So there are there are variety of factors that impact query performance and not all of these factors are within the preview or within the control of the dbms. Therefore query processing is oriented towards obtaining the best out of whatever is available at the biggest or at the control of the dbms.












 
(Refer Slide Time: 06:20)

 

What are the typical steps that are handled or that are executed in a query execution process. Given a query, given an sql query, the query is fist scanned using a lexical analyzer which in turn takes the string which forms a query and returns a set of tokens and then these token stream is then passed to build a syntactic tree or what is called as a parse tree. And then syntax analysis and so on semantics checks and etc are all performed which comprises the validating phase of the query complier and once this is done an intermediate query representation is generated. This intermediate query representation is also termed as a logical query plan tree.

Now this logical query plan tree is usually a tree data structure that represents the relational algebra equivalent of the sql query. Using this tree data structure, there are several rules that are employed in order to systematically optimize the tree for better performance. The intermediate query representation or the query tree is then given to the query optimizer which generates a query execution plan. The execution plan is also called the physical query plan tree and in contrast to the logical query plan tree that means a physical query plan tree comprises of a query plan written in a language that is usually executed by a dbms interpreter or would be complied into machine code for execution. This physical query plan language comprises of its own constructs some of which we saw in the previous sessions something like table scan constructs and sort scan constructs and iterators and so on, in addition to all relational algebra constructs like select, project, set union, difference and so on.






 
(Refer Slide Time: 08:30)

 

The query execution plan if it is an interpreted dbms would be executed directly and the results would be returned to the user. If it is a complied dbms engine, the query execution plan is then given to the query code generator and then appropriate machine code is generated from the execution plan.
 
(Refer Slide Time: 08:53)
 
 

Of course the last step would be the code if it is generated in a complied mode, it is then given to the operating system run time or the database runtime and then query results are given.
 
(Refer Slide Time: 09:10)

 

Now we are primarily interested when we are taking about query processing in two of these stages in the query execution stages. The first stage that we are interested in is the intermediate form of query representation. In fact that is something that we have deferred to a later section. We in fact went straight onto the second stage that is a physical query plan representation. We saw some constructs of the language that is used to describe the physical query execution plan and constructs that are or the physical execution plan is then either fed through an interpreter for this language which executes the query or is compiled onto machine code.

Now what are some of the operators that we saw yesterday? The physical query execution plan operators are those set of operators that define the basic physical operations that need to be performed in order to answer a query. These are also sometimes called the internal query operators or the operators that make up the internal query language. Sql or relational algebra on other hand is an external query language that is it is the query language used by the users or application programs that are using the database. But the database itself or the dbms itself uses an internal language for answering queries that accesses the file system or the storage structure used by the dbms in order to retrieve tuples in response to a query.
 
This of course comprises of the set of all relational operators plus some additional operators that talk about physical characteristics of retrieval. Some example operators that we saw yesterday was the table scan operator which can be implemented using an iterator operator that is an iterator objects so to say which opens a table and returns a tuple by tuple that is an iterator if you remember has three different functions an Open function, a GetNext function and a Close function. A table scan operator opens a table when the Open function is called and returns the next or the top most tuple on every getindex or getnext invocation and starts incrementing a pointer so that the next tuple is returned and so on.
So the i th GetNext invocation would return us the i th tuple in the table and then the close function would close the table. And then there was index scan that would use an index to get a particular or get the required tuple. Then there is a sort scan operator which not only returns all tuples in a table but also sorts them before returning them. We then saw some set of one parse algorithms that are built on top of these physical query plan constructs or data structures that perform various operations.
 
(Refer Slide Time: 12:22)

 

We divided one pass algorithms into three different kinds of categories. The first category was tuple at a time algorithm. A tuple at a time algorithm is something like select or project which needs to concern or which needs to be concerned only with one tuple at any given point in time. Tuple at a time algorithms can easily be executed with a single parse regardless of the size of the relation because we assume that the tuple is never large enough so that it cannot fit into main memory. We always assume that one tuple of the database can always fit into main memories.
 
Therefore since we are only concerned with single tuple at a time, we can always use a single parse algorithm for tuple at a time functions. On the other hand there are relations at a time algorithms that is there are functions that need to be concerned with the entire relation rather than each tuples independently. For example the function called unique in sql needs to look at the entire relation in its entirety of course rather than each individual tuple in isolation. Relation at a time functions can use one parse algorithms only if the relation can fit completely into memory. And that’s not enough, in fact the relation should not only fit completely into memory but especially for binary relation at a time functions, there should be at least one block left over to read data from the other relation. Hence if a relation requires M memory buffers or rather if M memory buffers are present for the dbms then the maximum size of one of the relations of the smaller relations can be at most M minus 1. It can’t be M, because we need at least one more block for data from the other relations. 
(Refer Slide Time: 14:36)

 

And of course that is the limitation of the one parse algorithm in the sense that you can use them only when you know that at least one of the relations can fit completely into memory. Now how do you know that a relation completely fits into memory? What if you don’t know the size of the relation? In such cases you need to estimate the size of whatever data relation or whatever table that that you are using. Hence one parse algorithms rely to a very large extent on procuring good estimates of relation sizes in terms of the number of tuples. And if the estimation algorithm is wrong that is if too many buffers are allocated that is if the estimation is too high then there is a possibility of thrashing.
  
On the other hand if the estimate is too low then we may not be able to use one parse algorithms at all because the relation won’t fit into memory and the strategies that we studied for one pass algorithms will no longer be applicable. What do we do in such cases that is, what do we do in cases where one or both of the relations or too big to fit in memory by itself. For these we use what are called as multi pass algorithms. 













(Refer Slide Time: 16:03)

 

In multi pass algorithms we shall be studying today mainly about two pass algorithms. Generally many multi pass algorithms are generalizations over two pass algorithms and if you know the general strategy that is used behind two pass algorithms, it is sufficient or it would not be too difficult to generalize it to multi pass algorithms. And multi pass algorithms as the slide shows are used when relations cannot be read into memory in their entirety. Only a part of each relation can be read into memory and they usually have an alternation between reading part of a relation into memory and writing it back onto disk. That is an alternation between computation and intermediate result generation and retrieval of intermediate results.
 
(Refer Slide Time: 16:57)

 
The first kind of two pass algorithms that we are going to be looking at are what are called as sorting based two pass algorithms. We are going to be looking at three different paradigms or three different strategies of two pass algorithms, sorting, hashing and indexed based two parse algorithms. The first one that we are going to be looking at is the simplest which is called the sorting based two pass algorithms. The basic idea behind sorting based two pass algorithms is shown in the slide here. Suppose we are given a relation R. Of course that is, such that the relation R is too big to fit in memory and of course for the dbms we are given a maximum of M blocks of memory. That is M blocks of memory elements are allocated for the dbms, therefore M blocks of data can be read from the relation R by the dbms at any point in time. The sorting based two pass algorithms have the following basic structure.
 
(Refer Slide Time: 18:05)

 

This slide show the basic or the skeletal algorithms for a two pass sorting algorithms. Essentially as we noted before, the algorithm or any multi pass algorithm alternates between reading intermediate results and writing them back. That is reading part of data from the disk and performing computations and writing them back. So the basic idea behind two pass algorithms are based on sorting is as follows. First start by reading the relations one or two relations that is we are going to concerned only with either unary or binary operators for the time being. Of course any nary operators can usually be reduced into one of these forms and it’s a generalization of considering either unary or binary operators.
 
So let us consider that the relation or the pair of relations that we are going to be using are first read block by block. So because we have M blocks that are allocated to us, we can read these relations M blocks at a time. Now M blocks of relations from M blocks of tuples from the relation or pair of relations are read into memory and then they are sorted. Once they are sorted they are returned back into disk. 
This is the alternating phase that is you read part of data from disk sort them, in this case which in this case is the computation and then write them back to disk. Now continue steps 1 and 2 until the relation is exhausted. I am just assuming here that we are dealing with unary operator that is until relation R is exhausted and it can of course mean until the pair of relation is exhausted or until the set of all relations that this query handles are exhausted. And then once these intermediate results are written on to disk that is sets of different M blocks of sorted tuples are written onto disk, use a variety of query merge techniques. Remember what is a merge technique, if we have taken a course on let us say data structures you would have come across an algorithm for merging which is usually seen in relation with sorting.
 
A merging basically means that given two or more lists that are sorted, can I obtain a single list that is also sorted of course as efficient fashion as possible. There exist very efficient algorithms for merging especially when the results or especially when the lists are sorted that can produce results in a linear order of time. So use a variety of these query merge techniques. Note here that in the two parse algorithms, it is no longer just merge techniques it is query merge technique. That is as and when you do the merging, perform your query or answer your query as and when you are doing the merging on this intermediate results. So use a variety of query merge techniques to extract relevant results from all the sorted M blocks of disk.
 
(Refer Slide Time: 21:51)

 

Now let us see some examples of two phase or two pass sortings, two pass sorting based methods for answering different kinds of queries. You can notice that all of these examples follow the same skeleton of the basic idea that we presented in the previous slide. So let us take first the example of duplicate elimination. Now as you can see duplicate elimination is or relation at a time unary operator. We don’t have to worry about tuple at a time operators for two pass algorithms. 
Why? because all tuple at a time operators can be answered using a one pass algorithm because all that we need to be concerned about at any point in time is just a tuple and each tuple can be checked in isolation with the other tuples. So let us take the first relation at a time unary operator namely duplicate elimination that is the unique construct, how do we implement the sql unique construct. Let us assume that it is relation R on which we have to eliminate duplicates. And of course we have to also assume that relation R is too big to fit in memory. Now as we know that because we are given M blocks of memory available to us, start reading R in terms of M blocks of data that is read M blocks of data into memory and sort them and the third step is store the sorted set of M blocks back onto disk.
 
Now continue from step two that is read the next M blocks of data from R, sort them and put them back on to disk in in a separate file name. So keep doing this until R is exhausted, R becomes empty. Then what is the next phase now? We have read R relation, sorted them and put them back into memory or rather put them back into disk. The next step now is to use one of these so called query merge technique on these intermediate results that we have generated. So what is a query here? The query here is the elimination of duplicates that is we do not want any duplicate tuples to appear in the relation, once it is output to the output buffer. So how do we eliminate duplicates from these set of sorted intermediate results? So the query merge technique for eliminating duplicates is quite simple.

(Refer Slide Time: 24:36)

 

Take one block from each sorted sub list. There are several sub list of data that have been generated and each list of data is a maximum of M blocks in size that is because we have read data in terms of M blocks, except the last set of data that we wrote every other data would or every other intermediate result would be of size M blocks. The last data intermediate element that we wrote would have a maximum size of M blocks, it could also be smaller than M blocks in size. 
Now take the top most block from each set of these intermediate results that we have generated and for each tuple that is take the smallest tuple among this. For each tuple just go pass the repetitions of this tuple in the other blocks. Let me illustrate this with an example in the next slide which makes it much more clearer. It is as simple as taking one tuple and then moving or rolling forward all other blocks so that they move pass the present tuple that I have taken that is if they contain duplicate tuples. If they don’t contain duplicate tuples, you don’t have to move them. And of course once they are all moved, put the first tuple into the output buffer.
 
Now this is possible or this is possible to be done in an efficient fashion because the tuples are all sorted. That is if a tuple T appears at one particular stage then all tuples that are in some way greater than T should appear below them. Therefore I just need to search each block until I find a tuple that is greater than T, when I am trying to remove duplicates. Such a kind of merge elimination takes an order of M n b time where M is the number of blocks in a sorted block set that is one of the intermediate result block set that that we have stored and b is the number of tuples in a block, n is the number of such block sets.
 
If you remember merging algorithms, a merging algorithm between two list of size M and N would take an order of M plus N time because we have N different lists of blocks of M different blocks. That is each list comprising of M blocks of data and each block containing of B tuples of data. We just multiply all three of them together so that we get the total time that it takes for eliminating duplicates from. Let us look at duplicate elimination with an example that makes the process much more clearer.
 
(Refer Slide Time: 27:50)

 

Now let us assume that as shown in the slide here, this is the first set of block set that is first set of M blocks of data that is written that is 1, 2 is in the first block, 2,2 is in the second block and 2, 5 is in the third block. Now this whole thing is a set of three blocks that were written onto disk and as you can see here this entire thing is sorted, they are in sorted order. That is 1 is smaller than 2, is less than or equal to 2 and so on until 5. This is the second set of blocks that were written onto disk 2, 3 4, 4  4, 5 and so on. And this is the third set of blocks that were written onto disk. We now start by taking just the first blocks of each block set into memory and then considering just the first tuple in the first such block that was read. 

Now the first tuple says that it is or reads as 1. When I read 1 here, all I have do to is eliminate duplicates is to eliminate 1 from all other block sets. Now if 1 has to appear in all the other set of blocks, they have to appear at the top, they cannot appear somewhere at the bottom, this is because it is sorted in order. Therefore one has to appear in the top. So all we need to do is start from the top of each block set and start moving forward until we find a tuple which has a value or whose key value is greater than 1. So therefore once we read 1 here, we look at the second block set and we read 2 to begin with and we know that 1 does not exist in this block set.
 
When we read the third block set we read a 1 and cancel it out, read the next tuple and this is also 1, we cancel it out and then go on to the next tuple which is a 2. So we know that we have exhausted all 1’s that have occurred in this block set and we can stop this process here and output 1. So after 1 is output, the set of blocks becomes like this that is 2 is in at the front, all the 1’s have gone and the block sets have been left shifted so to say in an appropriate fashion. Now the next tuple that needs to go out is 2. So you see that the first tuple here reads 2 and you have to eliminate all 2’s from everywhere else. That is return this 2 to the output buffer and start moving forward until you have eliminated all 2’s. So therefore you cancel the next three 2’s here and end up at 5. In the second block you cancel the first 2 and end up at 3 and the same thing in the third block set as well that is you cancel the first 2 and end up at 3. 

(Refer Slide Time: 31:06)

   
This is depicted in the second slide that is after the second tuple is put into the output buffer that is after 2 is put into the output buffer, our sets of blocks looks as it is shown here. That is all the 2’s have been eliminated here and 5 has come to the top and all 3’s that is all 2’s are eliminated here and 3 has to come to the top and all 2’s are eliminated here and 3 has come to the top here. 

Now if I start to read 5 here then there is a problem because I have not read 3 as yet; 3 and 4 still exist before 5. So I cannot start rolling this block set until I go beyond 5. The simple answer to obviating this problem is to note that the first set of blocks from all the block sets are in memory. So we just choose the least element from the first set of blocks and then start rolling the blocks. Therefore we now choose 3 as the tuple to be output, to the output buffer and then start rolling each of the block beyond 3. Therefore 3 would be output from the second block set and we will roll it until we find 4 here and same thing three would be output here and we will roll it until we find 5. That is we roll the block set until we end up at 5 and so on. So in this way we can eliminate duplicates by taking the least element of the top most set of buffers of each block set and then rolling blocks until we move beyond the least element.
  
(Refer Slide Time: 33:00)

 

How do we perform set union using sorting? Remember these kinds of algorithms that we looked at yesterday for relation at a time operators. We looked at duplicate elimination, we looked at different kinds of set theoretic operations like union, intersection, set difference and so on. So set union is a binary operation. It is a relation at a time binary operator and because it is a set union that is I have emphasized the word called set, it means that no duplicates in the result. As you might have imagined the strategy for computing set union is very similar to duplicate elimination. When we have to compute the set union between two relations R and R and S, let us say R union S we just have to output all tuples from R and S without duplicates, as simple as that. 

Therefore the set union algorithm is quite similar to that of the previous algorithm where you read blocks from R and S rather than just from single relation, sort them and store them onto disk. Now it does not matter for us whether any given tuple that we have read belongs to R or to S. We can just consider R and S to be a single relation and then start reading tuples from them, sort them and store them back to disk and use the duplicate elimination merging technique that we just looked into in the previous example for eliminating duplicates. So the output of this would be, it is clear to see that the output of this would be R union S.
 
(Refer Slide Time: 34:49)

 

What about set intersection using sorting techniques? Set intersection between two relations R and S essentially has to return the set of tuples that are common between R and S. This set intersection is slightly different from the previous two algorithms. Why is this so? This is because here we need to be concerned about or we need to distinguish between tuples that belong to R and tuples that belong to S. In duplicate elimination as well as in union, all that we were concerned about is eliminating duplicates. That is whenever there are duplicate tuples, you just return it once and then cancel out all others. 
but here you have to return tuples, if and only if they appear in both relation R and relation S. therefore we need to have a mechanism of tagging each tuple as to where it belong, does it belong to the relation R or does it belong to relation S.
 
So the simple strategy for set intersection is shown in this slide here. Given two relations R and S, read them in terms of M blocks rather and store the blocks on disk in sorted order that is read relations R and S and sort them and store then back on disk. Now when reading it, ensure that the intermediate results of R are stored in a separate block set or in a separate buffer pool than the relations of S. So that we can easily distinguish between buffers that belong to R and buffers that belongs to S. And in order to compute the common elements between R and S, we simply do the following. We take each block of R that is from the block set and take the first block in each block set and move them into memory and read the smallest tuple, the first tuple in this means the smallest tuple in the block set that we have just read into memory and try to roll S. That is try to roll relation S beyond this tuple that is beyond the first tuple. 

If any that is if this tuple T existed in S then there should be at least one block which gets rolled that is which gets left shifted. If there is at least one left shift in S then it means that the tuple T is common between R and S, therefore it can be returned to the buffer pool or to the output pool. If no such left shift happens in S then the tuple T is not there in or is not present in S and therefore can be discarded. And note that here we are assuming that R and S are sets that is there are no duplicates in R and S for this algorithm. If there are duplicates then what we have to do is we first have to eliminate duplicates from R before shifting S. that is take the smallest tuple from R, roll all other block list in R until you eliminate duplicates of this tuple and then start looking for occurrences of this tuple in relation S. 

(Refer Slide Time: 38:27)

 

The last algorithm that you would be looking at in the sorting strategy is the natural join function. The natural join as you know is an equijoin that operates on two relations, it is a binary operator and it is an equijoin on attributes having the same name and domain in the two relations. so assume that we have two relations R (X, Y) and S (Y, Z) where X and X, Y and Z attribute list where Y is the common component between R and S and assume that we are computing natural join between R and S. 
 
Now a simple way of computing natural join is shown in this algorithm here. We first read blocks of R into memory that is we first read R in terms of M block and sort them on the Y attribute now. We cannot sort them entire tuple contents, we have sort them on the Y attribute. Because this is the Y attribute is the one that is common between R and S. Now sort them on the Y attribute and store them back on to the disk. And then secondly read relation S block by, that is M blocks by M blocks and again sort it on the Y attribute and store them back on to disk. Now what is that we need to compute the natural join? Now for each relation or each tuple in R that is stored on to disk, we have to find all other tuples of S that are also stored onto disk, that can be combined with this relation. We however encounter a small problem here. What is a problem? There could be situations where a given tuple in R can be joined with every tuple in S. Now if that is the case, especially if we are talking about outer joins where we can tolerate null values for Y then we may have to take a tuple from R and join it with every tuple in S and we may not have, of course we do not have enough memory to store every tuple of S into memory and then start outputting it. So we have to do something else now. That is there could be worst case conditions where even combining the intermediate results cannot be done in memory. That is there isn’t enough ram to perform the intermediate operations.

In order to tackle such issues what we do is we start with a second level of what may be termed as external merging. What we do is for each value of Y that appears at the top of the first blocks of R that is read the first blocks from each block set of R into memory, take the value of Y that appears in the first block of R and for each value of I that appears in the first block of R, note that here we cannot remove duplicates. We should have duplicates in the result unless of course it is specified that there should be no duplicates. So for each value of I that are at the top of the first block of R, start rolling S that is start reading the first blocks of S into memory and start rolling them until they go beyond the value of Y and take all these tuples and store them separately. That is let us say you take tuple T1 from R and a set of tuples Ts from S and put all of them into another file. 

Now once you have put all of them into another file, you just start combining the tuple that you have found in R with each tuple that you have found in S. So of course as you can imagine, this is a pretty, this can be a pretty slow operation because there are several amounts of disk accesses or disk writes that are happening here. The first set of disk writes are reading or the first set of disk write happens when we write the intermediate sorted relations back onto disk. The second set of disk accesses happen when we read each of the blocks, from the block sets of R into memory and then for each tuple we have to read each set of blocks in S and for all matching tuples we have to write them onto some other intermediate file where they can be combined.

Note that we cannot delete the tuples from S, after we have done this operation because there could be another tuple in R having the same value of Y which can also be joined with all these tuples in S. That is for example if we are joining let us say employee and department and let us say the join attribute is the department number that is for each department number attribute in the employee relation, find the department number attribute in the department relation and then join them together. Now there could be a case where two or more employees are working in a same department and have the same department number. So when we have found the first employee with some department number let us say D and assume that an employee relation is R and the department relation is S. Once we have found the first employee who works in a given department D in S, we take the tuple D is S and write them onto disks so that they can be combined. 

However this tuple cannot be deleted from S because there could be another employee who is also working in the same department and whose tuple also has to be combined with the tuple of the department. So therefore natural join using sort is a slightly expensive technique.
 
(Refer Slide Time: 45:35)

 

The next set of algorithms that we are going to see or the next paradigm of algorithms that we are going to see are the hash based algorithms. As the name suggests hash based algorithms use a hash table as their underlying data structure and use hashing as a overall paradigm or the overall strategy in which the algorithms are based. The basic idea behind hash based algorithms are shown in this slide here. We first, given a relation R that is too big to fit in memory of course. We read relation R block by block, we don’t even have to read it M blocks at a time. If it is possible to read it M blocks at a time fine, it makes it even more faster. 

Now for after we read relation R block by block, take each tuple in a block and hash it, move it through a hashing function so that it is hashed onto a bucket in a hash file. Remember what is a bucket in a hash file. A bucket is a set of buffers, is a chain of buffers that contain all tuples that are hashed in the same value for a given key. That is all tuples having the same key would be hashed onto the same bucket. There could be tuples having different key being hashed on to the same bucket and which could well be possible but what we can definitely say is that all tuples that have the same key will be hashed onto the same bucket.
 
So all similar tuples should hash to the same bucket then we examine each bucket in isolation, we don’t have to work across buckets anymore. In sort based techniques note that we had to work across the buffer list that were, sorted buffer list that were generated. 	 
In hash based technique we can examine each bucket in isolation of the other bucket in order to produce the final result.
(Refer Slide Time: 47:41)

 

So this slide is a reminder of what a hash file organization looks like. Given a tuple with a particular value key on which hashing is performed, the key then maps onto a set of buckets. This one shows static hashing and of course there are also dynamic hashing techniques. So the hashing function hashes a key onto a bucket number, the bucket number in turn points to a chain of blocks that form the bucket or where data or tuples that should lie in the bucket are stored. Let us look quickly at our usual algorithms for duplicate elimination and set theoretic operations like union, intersection and so on using hash based techniques, how can they be performed using hash based techniques. 

(Refer Slide Time: 48:57)

 

Let us first look at duplicate elimination using hash based techniques. This is quite simple as shown in the slide here. Given a relation R which contains duplicate tuples and of course which is too big to fit in memory, read the relation block by block or M blocks by M blocks. And take each tuple in the blocks that are read and hash it to a set of buckets of a hash file. And let me reiterate the fact that if there are duplicate tuples in R and important property that we have to note here is that because we are using the same hash function, if there are duplicate tuples they will necessarily be hashed on to the same bucket. So we have already cornered in a sense all duplicates into the same bucket. 

And now our job is much simpler. that is visit each bucket and eliminate duplicates using either a one pass algorithm for duplicate elimination, if the bucket can fit into memory remember how I did one pass algorithm for duplicate elimination. That is we have to maintain a internal hash or a internal index structure in memory and then eliminate all duplicates as and when we read the relation. So if the bucket can fit into memory, you can use a internal data structure or in memory data structure to eliminate duplicates or if the bucket is too big to fit in memory then we can consider this bucket as a small relation and use our previous algorithm, the two pass sort based algorithm for removing the duplicates in the buckets.  

(Refer Slide Time: 50:31)

 

What about set theoretic operations union, intersection, set difference and so on? For set theoretic operations involving two relations R and S we maintain two separate hash files, hash based file organization. However these two hash files will use the same hash function, hash of K and the buckets are also labeled analogously that is bucket number 0 in the first hash file corresponds or is analogous to bucket number 0 in the second hash file and so on. So if tuple t appears in some bucket n in the hash file of R then if it is present in S, it should also appear in bucket number S of the hash file of S. And in addition to that the standard property that all duplicate tuples are always hashed to the same bucket in a given hash file. 
(Refer Slide Time: 51:34)

 

So using this it is quite simple to perform set theoretic operations using hash based techniques. For example if you have to perform union, all we have do is hash each tuple of R into R hash file and hash each tuple of S into S ‘s hash file and take each corresponding buckets that is bucket 0 of R and bucket 0 of S and eliminate duplicates and then output the results. Similarly if you want to perform intersection, we just examine corresponding buckets bucket 0 of R and bucket 0 of S that is bucket I of R and bucket I of S and then return only the common elements and the same thing for set difference. So that is quiet straight forward. The last technique in hashing that we are going to see is what is called as the hash join algorithm.
 
(Refer Slide Time: 52:29)

 
This is perhaps the most widely used technique for performing natural joins because of it is simplicity and efficiency. natural join over, let us compute natural join over relations R of X, Y and S of Y, Z where Y is the common component between R and S. Now all tuples, now we are going to first read R and S and use the Y component as the key to perform the hashing rather than the entire tuple. Now all tuples having the same value of the Y component should hash onto the same bucket if we are using the same hash function. So if the same hash function is used for R and S, we just take the corresponding buckets that is just take bucket number 0 of R’s hash file and for all the tuples that you find, if they can be combined or if they can be joined with any tuple then this tuple has to necessarily exist in bucket number 0 of the S’s hash file and so on.

So given this property, it is enough for us to examine just the corresponding buckets in order to perform the join operation. This makes the join operation extremely efficient because we don’t have to perform too much of look ups when we are performing the we don’t have to do too many look up’s when we are performing the join. Such an algorithm is also called the partition hash join algorithm because the pure hash join algorithm is a slightly different algorithm from this but this is widely used algorithm for computing natural joins in many dbms systems. So let us summarize what we have learned today in the two pass algorithms.
 
(Refer Slide Time: 54:31)

 

So let us summarize query processing in general and then go back to the two pass algorithms that we have studied today. We looked into the different stages of query execution to begin with and then we saw that there are two stages namely that of the logical query execution plan and the physical query execution plan that are especially of interest. And then we saw what are some of the language constructs that make up the physical query execution plans and then we looked into algorithms that can be built to perform this internal queries or physical, to answer this physical queries using this physical query execution plan construct.
And these algorithms could be either single pass algorithms which can be used if the relations are small enough or if the query is tuple at a time query and there are multi pass algorithms which can be used if the relations are big.
 
Among multi pass algorithms or rather in two pass algorithms, we looked at sort based algorithms or sorting based strategies for performing several of these operations and hash based strategies for performing several of these operations. We have not looked at index base strategies which is quite analogous to sorting and hashing base strategies. We shall briefly visit index base strategies in the next session before we take up the logical query execution plans and ways by which queries can be optimized. So that brings us to the end of this session. Thank you. 

Database Management System
Dr. S. Srinath 
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 16

Query Processing and Optimization – III

Hello and welcome. In today’s session we shall be continuing with whatever we have being exploring in the past two sessions namely query processing and optimization issues. As we have seen query processing is a very crucial element in a dbms design that is this is not about database design like I had mentioned in previous session that is database design is the term that is used to denote activities like schema design normalization and so on. That is how to design a database such that a dbms can be used to handle this, the data in the database efficiently as possible.
 
On the other hand query processing issues concern design issues of the dbms itself that is how can we built a dbms that can efficiently process a given user query and even if in many cases, even if the query is not formulated in a form that is the slightly to be the most efficient, can the database or can the dbms detect it and rewrite the query in such a way that the query becomes much more efficient. And we saw that a query processing is so crucial that it can make the difference between usability and un-usability of the dbms. 

Let us briefly have an overview of the different topics that we have studied in query processing before we move on to today’s topic that is of query optimization.
 
(Refer Slide Time: 02:47)

 


A typical query processing, a typical process of query processing takes several different steps. When the user gives an sql query, it is first passed through a scanning and parsing phase where the query is first scanned so that the query becomes or query is divided into a stream of tokens and these sets of tokens are then parsed to build a parse tree or a query parse tree that gives the syntactic structure of the formulated query that is given by the user. 

Now from this parse tree, a logical query plan is generated that is the parse tree is rewritten based on certain rules, heuristic rules and several different rewritings of this parse tree are possible and one of them is chosen based on some criteria like the cost estimation for this particular query tree and so on. So using this, the logical query plan is generated. The logical query plan of course is re written like I said and we optimize, the dbms optimizes the query automatically to a certain extent based on rewriting the query.

Then from this the physical query plan is generated. The physical query plan is a plan written in an intermediate language that is either interpreted that is executed directly by dbms or is compiled into machine code. And in the last two sessions we saw what would be the or what are the typical building blocks of this physical query plan language. There are, the physical plan of course should support all kinds of logical query operations like select, project and so on. 

In addition, it should also support some physical aspects of query processing like how to iterate through different tuples in a given relation or things like sort scan, table scan and index scan and so on. How do we retrieve based on a particular index and so on. We also saw some algorithms that implement the internal queries that is if you remember the internal queries are formulated or all those queries that are formulated by the physical query plan language that is it is the query that the dbms uses to access data from the file system. The external query is the query that the users use or the application program uses to access data using the dbms. We saw different kinds of access algorithms based on the physical query plan constructs. We saw different, we basically divided this algorithm into one parse algorithms and multi parse algorithms. One pass algorithms are those algorithms which make at most or exactly one parse over the required relation but one parse algorithms have a limitation in the sense that if we are using a one parse algorithm for a relation at a time operator.

What is a relation at a time operator? An operator that requires the entire relation to be present in order to answer the question that is being posed by the query. For example operators like removing duplicate that is the unique operator in sql or order by or group by in sql and so on. So all this require the entire relation to be available before the query is answered. Unless the entire relation is processed, even the first tuple of the answer cannot be returned. Even before answering the first or outputting the first tuple of the result, the entire relation must be processed at least once.




(Refer Slide Time: 06:10)

 

In single parse algorithms, we can apply single parse algorithms only if or if we use let us say tuple at a time queries where we need to be concerned only with one tuple at a time rather than the entire relation at a time or we can use it for relation at a time queries as long as at least one of the relations can be fit into memory completely. That is it not only fit into memory but there should be more memory space left over for at least one block of the other relation if we are using any binary operator.

In the previous session we saw what are called as two parse algorithms and we also said that multi pass algorithms are basically generalizations of the two pass algorithms. Two pass algorithms are used when, of course they are primarily used in relation at a time queries because tuple at a time query do not need two pass algorithms. We can just use a one pass algorithm. So a two pass algorithms are used for relation at a time queries where the relation size is too big to be able to fit into memory. And we saw the basic structure of a two pass algorithm. The basic structure of a two pass algorithm has an alternating computation and intermediate storage handling phases. 

That is take one set of blocks from relation, perform some computation on them like sorting or hashing or indexing which we are going to see today and then write them back into disk. And then read back all this intermediate results before producing the final result. So we divide a two pass algorithms into three different strategies or three different paradigms so to say what are called sorting based methods which we saw in the previous session where the computation that is done when a chunk of blocks is read from a relation is the sorting function. That is a chunk of blocks is read from each relation and they are sorted and they are placed on to disk. 

The next kind of algorithms that we saw was the hash based methods where we read a chunk of blocks from the relation and start hashing each tuples based on certain criteria. If it is a natural join for example we hash it on the common attribute or if it is a group by or if it is a unique function then we hash based on the entire tuple and so on. Today we are going to look at the last method or we are going to see just an overview of the last method namely the index based methods. That is where the computation that is used is an indexing function that is adding or searching on a index. And for our purposes, we are going to assume a sorted index like a B plus tree. 

A sorted index is something like where we can use the index structure not only to retrieve  a tuple based on a key value but we can also retrieve tuples based on a sorted order of all the key values that are present in the relation.
 
(Refer Slide Time: 10:24)

 

So what are index based algorithms? Index based algorithms can be contrasted from sorting and hash based algorithms in the sense that they use a index instead of sorting and hashing for the computation phase. And they are useful when tuples are to be extracted on attributes that have been indexed and they are especially useful for selection functions. Especially of course the, that is when the attributes that are being searched for index. And they are fairly effective for joins and other binary operations. As you will see there is another join function called zig zag join which is also quite popular as hash join and in terms of efficiently computing joins between two relations. 










(Refer Slide Time: 11:19)

 

Let us look at the first algorithm in index based methods. How do we perform select or how do we perform a select operation using an index? We are of course assuming here that the select operation involves a condition that is over an attribute that has been indexed. Of course, if the select is over a condition which is not been indexed then we cannot use index base selection, we should either use one of the other methods that we have seen. Now it is fairly straight forward if the selection condition is an equality condition on an attribute because that is what indexes are meant for. Let us say searching a B tree based on the value of a key is simply saying that select key equal to this value from the relation on which the B tree is maintained. 

So the simple algorithm is to search for the index for the required set of tuple or tuples. In case of inequality conditions that is something like a less than or equal to 10, we have to retrieve a sub tree from the B plus tree index that is we cannot retrieve just one node of the index. And we are also making another assumption here that the equality and inequality condition involves the key under constant and not another attribute that is we are saying a equal to 10 or a less than or equal to 10. We are not saying something like a equal to b where b is another attribute which has not been indexed but and we have to search for all tuples in such a case where the index indexing indexed attribute is equal to some other attribute that has not been indexed.
 








(Refer Slide Time: 13:15)

 

We shall not be covering all the other algorithms say set theoretic algorithms on index based methods. After we have seen the general pattern of how set base operators like union, intersection and set difference are performed using sorting and hashing, it is fairly straight forward to design algorithms using index based methods also. Let us look at the last query that we looked at the other algorithms namely joins or natural joins. How do we perform natural joins using index based algorithms? Now consider a natural join as usual between two relations R (X, Y) and S (Y, Z) where Y is a set of attributes that are common between R and S. That is Y is the attribute over which the natural join is going to be performed.
 
Now assume that suppose, that is suppose Y is not only indexed that is the Y attribute is not only indexed, it is also the primary key in S. Assume that S has as index over its Y component. If it is a primary key then it becomes even more simpler but even if it is not, it is just an indexing attribute or set of attributes we can use the following algorithm. We just start reading R that is the first relation in this chunk of blocks that is chunk of M blocks. So read R block by block and for each tuple that we have read from R extract its Y component and search relation S based on the Y component. 

That is we start sequentially reading one of the relations and for each block that we read we do a index search and because index search is much faster, we find the set of all relations let us say if it is a primary key then we find exactly one relation and if it is not a primary key we have to use some kind of clustering index and we find a set of tuples that may contain this value of Y that we are looking for. Now all that we have to do is join this tuple with how many ever tuples that we have found in S and we have computed the natural join over R and S. So if a corresponding tuple is found then perform the join and push this to the output buffer. Using index based methods we can also perform another kind of natural join which is called a zig zag join.
 
(Refer Slide Time: 16:05)

 

The zig zag join can be performed when there is a sorting index that is available for both R and S. That is assume that we are performing a natural join over two relations R and S where R is (X, Y) and S is (Y, Z) where Y is the common set of attributes between R and S. Now also suppose not only that Y is indexed over S, it is also indexed over R and this index is a sorting index that is it is a something like a B plus tree where I can access all keys available in the relation in a sorted form. 

Now all that we have to do here for performing such a join is shown in the set of three steps in this slide. We just use or we just keep calling GetNext function, note that we can represent relations R and S as iterators. So we just open both relations in iterators and let us say we use R and start calling GetNext function on the iterator, R iterator.

Now this get next function get the next logical key or the next sorted key in R and for each key that is found in R, we start searching for corresponding tuples or matching tuples in S. And if matching tuples are found, perform the join and append the output to buffer. Such an algorithm is called a zig zag join. This is because even though logically we are going in a sorted form physically, the control would actually be going in a zig zag fashion over on a storage disk. That is we don’t know or we are not sure that the sorted form in which we are accessing the keys is indeed the same form in which records or tuples are stored on disk. So such an algorithm is called a zig zag join. However it is quiet an efficient algorithm because we are using index on both set of attributes that is both R and S. 






(Refer Slide Time: 18:24)

 

Let us move on to the main topic of today that of query optimization. Until now we have been looking at the last step in a query execution process that of managing the physical query plan languages. That is how do we perform internal query answering using physical query plan languages. Let us move on to the next or the upper level before this that is of logical query plans that to primarily that of parse trees that are generated after a query is scanned and parsed by the query complier. 

We are going to see whether or how or whether we can re write a parse tree in a sense that we can make the query answer in a much more efficient fashion than it was formulated by the user or the application program. So query optimization techniques that are independent of the semantics of the query or the semantics of the application is what we are going to be looking at here. And these techniques are based on rewriting the parse tree representing a relational algebra expression of the query. That is the parse tree would actually convert a given sql query into a relational algebra and then represent it in the form of expression tree which can be optimized. And there are two kinds of optimization, what might be termed as heuristics based optimization and cost based optimization.

A heuristics based optimization is essentially a set of thumb rules using which we make assertions that the resulting parse tree is a better or more efficient than the original parse tree. However given a parse tree usually there might be more than one parse trees that could be generated based on which heuristics are applied and in what order they are applied. Now among these different parse trees, we might have to choose one of the parse trees for a particular query instance. And we can perform this choosing by assigning trying to estimate cost that each of the parse tree incurs. So usually in practice a combination of heuristics and cost based optimization are used before a query execution plan is finally chosen for execution.

 
(Refer Slide Time: 21:04)

 

Now we have been talking about parse trees and rewriting parse trees. So what are parse trees? A parse trees as you might have come across especially in the context of compliers or syntactic structures that of most programming languages that can be expressed in the form of a tree structure or tree data structure. A tree data structure represents a hierarchical structure. This is also called a syntax tree or a parse tree and execution of a syntactic structure is usually done by what is called as a post order traversal of the parse tree. That is a post order traversal simply says that given a tree having a root or 2 or more sub trees or 1 or more sub trees. Execute the sub tree first and then execute the root. And the same rule recursively applies to all of the sub trees. We shall not be going into details of how to build a parse tree and a traversals and so on but we shall be concentrating on how to modify a parse tree. That is in the context of relational algebra that is context of queries that are expressed relational algebra.
 
This slide shows an example of a parse tree. the left hand side of the slide shows a small relational algebra query which says project department name from select salary greater than 3 lakhs from again manager join department where the join condition is manager dot D number equal to department dot D number. That is it is a natural join between the manager and department relations. And from this natural join we are selecting the set of tuples where the salary field is greater than 3 lakhs and then projecting the department name. That is we are looking, we are querying for all departments who or which pay their managers a salary greater than 3 lakhs. 







(Refer Slide Time: 22:20)

 

The corresponding query can be represented in the form of a tree data structure that is shown in the right hand side of the slide. As the tree shows the top most operation project becomes a root node of the tree. Now the project is a unary operations therefore it has one child which is the select query. So the select query becomes the child of the project query. The select again is unary operations which is being performed on the join operator here. 

So the join operator becomes a child of select and the join operator is a binary operator where it has two children and these two children are its two arguments that is manager and department relations. So, logically as you can see, the queries expressed the hierarchical structure that is inherit in the query is made explicitly by using the parse tree. Once a parse tree is generated by a query complier, there are several kinds of checks that are performed on the parse tree before it is optimized. some of these checks are shown in this slide here.
 













(Refer Slide Time: 24:31)

 

Syntactic check: The syntactic check simply says that is the syntax of every operator correct. For example we cannot have two children on a project operator, project and select are unary operators. and similarly we cannot have a single child on a join operator, it is a binary operator and so on. Entity checks: entity checks basically check whether every relation name that is specified in this query actually exists. That is, is it there on disk, that is, is there a relation called manager and department on disk and so on. In some cases a relations may not, a relation that is named in a query may not actually exist on disk but it could be derived that is it may not be a base table but it could be a derived view that is it could be a view that is there in the schema. In such a case the complier would expand the view. Remember, a view is again another query and the contents of the view are not actually on the database. 

So the view is again expanded and the parse tree for the view is joined or is hooked to the parse tree of the overall query wherever the view name appears. So this is called view expansion and then there are attributes checks that is it does every attribute name refer to valid attributes and then there are type checks. That is, does each attribute participating in an expression have a proper type that is we cannot say something like salary less than cats and so on.

I mean it has to be, if salary is numeric then the other attribute should also be numeric and so on. So there should be type compatibility between attributes within an expression. Once these checks are performed, parse trees are rewritten based on certain heuristics. So these heuristics are also called as rewrite rules and which specify several conditions and corresponding actions that need to be performed when these conditions hold. and a parse tree should be expanded to its maximum extent before rewriting that is for example view should be, view should be replaced by the relevant parse trees when the parse trees is being, before the parse tree can be optimized.
And some rewrite rules could be situation specific and we are not going to be looking at such rewrite rules here anyway. And they work only if certain conditions hold on the data set that is they make certain assumption about the dataset based on which the parse trees can be rewritten.
 
(Refer Slide Time: 27:35)

 

The first rewrite rule that we are going to see today is what is called as pushing selects. The pushing selects basically says that try to push a select operation as low down in a tree as possible without altering the semantics of the parse tree or without altering the correctness of the parse tree. Why is this important or why is this beneficial? Remember a select or the cardinality of a select operation is less than or equal to the cardinality of the input relation for the select. That is a select basically removes certain tuples from a relation before returning the output. And one thing to note here is that whenever we talk about relations, always think of very relations that is megabytes or probably giga bytes of tuples present in a relation. And if a select operation is such that it requires only a small fraction of the tuples there is a great amount of optimization that is performed already. That is huge number of data need not be handled after the select is over and we can start working with a much smaller dataset.

So, select based optimizations are the most common optimizations that are used in query rewriting. So the slide here shows an example of pushing selects based optimization. let us look back at the parse tree that we just generated previously that is a project department name from select salary greater than 3 lakhs from natural join between manager and department. Now here you can see that the select operator here that is salary greater than 3000 is being performed after all possible managers or join with all possible departments. Now if we are going to anyway look at a managers whose salary is greater than 3 lakhs, we don’t have to consider joining all possible manager tuples with all possible department tuples. 

You might as well say that we are going to join tuples of manager where the salary field is greater than 3 lakh. So that is what is performed in this tree here. 

(Refer Slide Time: 30:06)

 

That is the select operation is pushed down in the tree so that first we select the set of all manager tuples where salary is greater than 3000 and then join this with a greater than 3 lakhs and join this with the corresponding tuples of the department relation and then project just the department name. so if, let say if there are about 1000 managers in a company and only about 50 mangers have a salary greater than 3 lakhs then instead of joining or instead of looking at 1000 different manager tuples, we need to content ourselves with only 50 manager tuples.
(Refer Slide Time: 30:58)

 
The second form of rewriting rules is also related to pushing selects and which is called the cascading select or conjunctive selects. That is a select which has a conjunction can be split and cascaded into several different select operations and which can progressively start reducing the search space as we go along. This slide shows a small example which illustrates this point. 

Suppose we have a select operation that says select C and D over R that is select where C and D here are some logical condition. Now we are selecting those sets of tuples from R where both condition C and condition D holds. If this is just the relation here, now this could be, the conjunction could be even more that is it could be C and D and E and  F and so on. There could be many more AND conditions here. now if we leave this as it is, then the entire set of relations are searched for this condition that is the entire set of conditions are matched on the entire set of relations.

On the other hand we can see that the right hand side of the relation or the right hand side of this equivalence condition says that select C from select D from R. that is it is a cascading select that is shown in the slide here where instead of using C and D over R, we first say select D over R that is this could be much more efficient if D is an indexed attribute over R. So if D is an indexed attribute over R or rather D is an condition over an indexed attribute over R, we can quickly or efficiently retrieve all those tuples that match the condition D. From this much smaller set or hopefully much smaller set of tuples that have been extracted, we then perform a select C or we then look for the condition C. So the search space for C is much smaller than the search space of D and if D is a condition over an indexed attribute, the search space of D or the inner select can also be very fast that is because we are retrieving based on index searches.
 
(Refer Slide Time: 33:41)

 

There is an exception however to the pushing select thumb rule that is push selects as far down a relation as possible thumb rule and this exception occurs whenever there are views that are there in a relation. When a view is expanded, it might be necessary sometimes that selects are actually pushed up beyond the view before they can be pushed down. Let us look at an example to illustrate this. This slide shows an example having two relations movie and Starsin. Movie is a relation that has the following attributes,

The title of the movie, the year of the release of the movie, the director of the movie and the language in which the movie is made. And then Starsin is an attribute that says which film stars acted in which movie. It says it has an attribute title which is the title of the movie the year which is the year in which the movie was released, the name of the film actor and the language of the movie. And now of course we can see that among the two relations title, year and language are common. That is we can perform a natural join using the three different attributes. And now we create view called BengaliMovies where which just says select star from movie where language equal to Bengali. That is we are interested only in those tuples of the movie where language equal to Bengali. 

(Refer Slide Time: 35:27)

 

Now let us see if we give a particular query and what happens. Let us say we have a query which says which filmstar worked under which director in Bengali movies that is we have to bear each film star with a director as long as the language of the movie is Bengali. So how do we go about or how do we express this query. This is quite simple from a sql point of view that is we just say select starname and director from BengaliMovies NATURAL JOIN Starsin. 

That is we join we perform a natural join between BengaliMovies and Starsin. note that Bengali movies also have the same structure as the relation called movies. That is you can join based on title, year and language attributes. So the corresponding parse tree for this is also shown in the slide here that is we are projecting starname and director from a natural join between BengaliMovies and Starsin. However this relation Bengali movies is a view.
So we have also expanded the view here. The view for BengaliMovies is basically select language equal to Bengali from movie. So this view is expanded and Starsin is a base table which is kept as it is in the parse tree. Now, if you see here even though the left hand side of this tree that is the parse tree that talks about Bengali movies contains tuples where the language field is Bengali. It does not contain tuples having any other language field. However while performing the natural join we are still considering all tuples in Starsin even if it does not contain the language called Bengali. 

That is we take a particular actor and we look at the set of all his or her movies regardless of whether the language is Bengali or not. This is clearly wasteful. Because after all we are only interested in which star worked under which director in Bengali movies. So, one way to optimize this is to put a select operator here. That is we need to select those tuples from Starsin where language equal to Bengali. Therefore this select operation here which is in a view or which defines the view should first be taken up before it is brought down. This is what is shown in the next slide here.
 
(Refer Slide Time: 38:08)

 

That is all tuples of Starsin are selected even if they are joined with tuples having language equal to Bengali. So here we just select all tuples which is not really necessary. Therefore we need to take this select tuple that is the left hand side of the relation selects a particular or performs a particular selection condition which is also applicable to the right hand side of this sub tree. So we move up, we move this selection relation up as much as possible before moving it down and to bring it here. That is so this would be the rewritten parse tree. That is there are two separate select statements one for movie and one for Starsin which finally forms the query.



 
(Refer Slide Time: 38:58)

 

Some more thumb rules involving selects. Some times in a join function we perform a join blindly and then select a set of tuples from this joined set of tuples based on a condition that applies to only one of the relations. We have actually seen such a example earlier but let us revisit again in order to make this, bring out this rule. We didn’t apply this rule however because we applied some other rule. Now consider the following query over the movies database. Again the same movies database comprising of movie Starsin and Bengali movie relations. 

Now the query is we are only looking for which actors or which stars acted under the director Satyajit Ray in Bengali movies. We are not just pairing up all sets of actors with all sets of directors under Bengali movies, we are just looking for who all acted under the direction of Satyajit Ray in Bengali movies. So the sql statement for this again is quite simple. We just say select starname from BengaliMovies NATURAL JOIN Starsin WHERE director equal to Satyajit Ray. 














(Refer Slide Time: 40:30)

 

The corresponding parse tree if you build the corresponding parse tree here this would look like this. At the lowest level is the join operator between BengaliMovies which is expanded here that is shown in yellow and the Starsin relation. Now after these two relations are join, we select for director equal to Satyajit Ray and then project starname and director name. Of course we need we need to only star name, there is no need to project director. 

So what we are doing here is that we are joining two relations and even if let us say that even if we move this select up and bring it here, we are still joining two relations based on just the factor that language equal to Bengali. However we can note that in the second relation here, we are finally interested in those tuples where the director field equal to Satyajit Ray. We are not interested in any other directors whose records are available in this relation here. So we can as well move this director equal to Satyajit Ray select below the join here that is which is above the join here is now below the join function. And of course in addition to this we can also perform the previous optimization which is not shown here that is move this language equal to Bengali up and bring it down so that it comes before Starsin. So there are two different optimizations that are possible in this particular query tree.









 
(Refer Slide Time: 42:17)

 

The next kind of thumb rule we are going to be, that we going to consider is what is called as inserting projects. Now note that if the output of a query involves projection of just a few attributes over a large relation containing let us say tens of attributes or different or tens or even sometimes of hundreds of attributes, a large tuple containing many different attributes, there is no need to work with so many tuples or so many attributes when all we require is just what is given by the overall projects and the conditions that of select operations that are given in the query. 

Have a look at the examples shown in the slide here. the left hand side of the slide shows a query tree which says going back to our manager and department example, project department name from that is select salary greater than 3 lakhs from natural join between manager and department. That is this is also optimized that is the select operator which is here is now brought here so that it is optimized. 

Now if we notice the output of this tuple is just the department name, the output of this query is just the department name. The user or the application program is not concerned about any other field of the database. That is anything like the department id or the number of people in the department, the location or whatever else that is there in the department relation that is not really important. 

However we cannot just throw away all other attributes other than whatever is requested because these attributes may be required for some conditions. Now which are the other attributes that are required for some conditions. Here that is department dot dnumber equal to manager dot dnumber is another condition that is required for the natural join that is shown here. So, the only set of attributes from department that we are concerned about here are the department name and dnumber that is the department number. 

Similarly the only set of attributes that we are concerned about for the manager relation is just the salary and the department number. we are not concerned about the name of the manager, the age of the manager, the date of birth, the employee number, the address, nothing. So what we can do is we might as well insert extra projects down in the query tree so that the output relation becomes smaller and more focused to what is required for as part of this query. 

That is we have inserted a project dnumber and salary before manager because those are all the fields that we require above and similarly we have inserted a project dnumber and department name because those are all the attributes that are required for these query from the department relation.
 
(Refer Slide Time: 45:42)

 

So that was the brief overview of some of the thumb rules that can be used to rewrite  query parse trees in order to make them work better. Let us look at the other aspect of query optimization that of cost based optimization. A cost based optimization is essentially used to choose between one or more or choose between two or more different candidate parse trees. that is given a query parse tree if I have used the thumb rules and generated several other candidate parse trees, each of which claim to be more efficient which is the one that I have to use. 

In order to do that we assign a cost for each of the different components or a cost estimate for each of the different components of a parse tree. And the overall cost of a parse tree is an algebraic expression over each of these cost estimates. It is either a sum of all these cost components or multiplication or whatever depending on what exactly is the tree. Now what are the factors that affect execution cost? This slide shows some possible candidates that affect execution cost, say access cost is second storage. 

How costly it is to access data from disk or any other storage medium that we are using. then what is the storage cost, how much storage do we need especially for storing the intermediate files and what is the computation cost, how much of processor time that we require and so on. What is the memory usage cost, how much memory, primary memory or ram does it take. And how what is a communication cost especially between the dbms server and the client, if they are situated on different machines. And communication cost becomes very profound, if we are considering distributed database that is how many communication sequences are required between the different dbms servers that form this distributed databases. So all of these factors affect the overall query execution cost.

(Refer Slide Time: 47:59)

 

A database catalog is a set of value estimates or some kind of meta data or meta information that are stored in a dbms that are useful for cost estimation.
 















(Refer Slide Time: 48:13)

 

And catalogs are metadata that could be either say table specific metadata like an estimate of the number of tuples in table, the size of the table and the number of blocks that are occupied by table and so on. And they could be field specific like the number of distinct values of a particular field, an estimate of those numbers of different distinct values of a field and so on or they could be database wide tools or they could be index specific information and so on. So let us look at some typical kinds of information that is stored in a catalog and see how we can estimate the cost of different operators.
 
(Refer Slide Time: 48:57)

 

Some typical information that are stored or depicted here let us say B of R is the notation used for the number of blocks that are taken up by a relation R. Similarly T of R the number of tuples that exist in a relation R and similarly V of R, a is a field specific attribute that is it is an estimate of the number of distinct values that attributes a has in relation R. And of course for example if attribute a is the gender then the number of distinct values is only 2. On the other hand other hand if attribute a is something like distance between something and something else, it could take on a range of several other values. And of course V of R, a1 till an is the set of is the number of distinct values of the combined tuple or that is formed by a1 to an. 

(Refer Slide Time: 49:57)

 

Let us look at some simple cost estimation techniques and there are many more cost estimation techniques other than this but we are only looking at examples that show the bigger picture behind this, the representative example. So suppose we have an equality selection that is select, that is s equal to select A equal to C from R where A is an attribute name and C is a constant. Now A is an attribute name and C is a given constant and we have an estimate of the number of distinct values that the attribute can take which is given by V of R, A then the probability of A equal to C is simply 1 over V of R, A and because the number of tuples is T of R, the slide here shows the estimate that is T of A is equal to T of R divided by V of R, A which is an estimate for the number of tuples that are there in S. And this is a good estimate if all values of A have fairly uniform probability or that is in the selection query. Now if the dataset is queued then it may not really be a good enough estimate. 






(Refer Slide Time: 51:27)

 

Consider the inequality condition that is select A not equal to C from R which is assigned to S and C is again a constant. now this is again quite simple because suppose the number of distinct values of A is given by V of R, A then the probability that A is not equal to C is simply V of R, A minus 1 divided by V of R, A that is 1 over 1 minus 1 over V of R comma A. Now multiply this with T of R which gives us an estimate of the number of tuples that could be in S. What about a composite condition something like select C or D that is condition C or condition D over R and assign it to S.

Now let us  first, suppose we have estimated that P tuples in the relation satisfy condition D condition C and q tuples satisfy condition D and there are n number of tuples in the relation. now the probability that a given tuple will match C or D is shown here that is 1 minus, the multiplication of 1 minus P over n and 1 minus q over n. That is this is the probability of  a tuple not satisfying C and this is the probability of tuple not satisfying D and the multiplication of this is the probability of tuple not satisfying both C and D and 1 over this or 1 minus this is the complement of this. This is typically the De Morgan’s law of which says, which gives us the probability of tuples that satisfy C or D. Now multiply this with N or the number of tuples which gives us the size estimate of the query.










 
(Refer Slide Time: 53:29)

 

The last estimate that we are going to look at is trying to estimate the size of a natural join. Consider a natural join between R of X, Y and S of Y, Z. Initially for simplicity let us assume that Y is a single attribute, all though X and Z could be composite attributes that is their sets. Now also let us assume that V of R, Y that is the number of distinct values of Y that are in R is less than the number of distinct values of Y that are in S. Now each tuple in R can be combined with corresponding tuple in S with the probability of 1 over V of S, R because that is the number of values that is S, Y rather than there is a small bug here, V of 1 over S, Y. Now that is the probability of finding given value of Y. 

Since there are T of S tuples combining with T of R tuples, the size estimate of the total of this join would be T of r times T of S divided by V of S, Y here. Now this is true if the smaller relation is R that is the V of R, Y is less than V of S, Y.
 















(Refer Slide Time: 55:04)

 

If the opposite is true then V of R, Y appears in the denominator. Therefore we just take the maximum of V of R, Y and V of X, Y in the denominator and the numerator remains the same T of R times T of S which gives us the good enough estimate of the number of tuples in the natural join that is if Y is a simple attribute. If Y is a composite attribute that is it contains of many different attributes then we have to consider the max of each corresponding attribute that is common between R and S. We shall not be looking into this in more detain here.
 
(Refer Slide Time: 55:40)

 

So that brings us to the end of this session and also the end of the topic on query processing and optimization that we have studied. We have in some sense just scratch the surface of this vast and crucial aspect of dbms design especially, namely query processing and optimization. So in this session we looked at index based algorithms for physical query plans. And we also looked at different query optimization techniques based on pushing selects, cascading selects and pulling selects out of views and inserting extra project operations. 

We also looked at several kinds of cost estimation techniques which can be used in combination with heuristics of query rewriting in order to select the best or in order to select what is considered to be the best query execution plan. That brings us to the end of this session. Thank you 

Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 17

Transaction Processing Concepts

In today’s class we are going to look at transaction processing concepts. Any doubts in this particular class, you can e mail your doubts to the e mail address that is shown in the slide here djram@iitm.ac.in.
 
(Refer Slide Time: 1:40)

 

Now we will look at what is the meaning of a transaction in databases.















(Refer Slide Time: 1:52)

 

A transaction is essentially an atomic unit of access which is either completely executed or not executed at all. Typically databases store date of interest and we have applications trying to access the database and modify the data that is stored in the database. Now one example is banking database where you wish to transfer funds or withdraw amounts from your account in the bank database or you want to debit or credit certain amounts at transfer, amount from one account into another account. When the database is operated upon by the applications, we have certain instruction that will be executed and these instructions are not just normal instructions as executed in an operating system but they need to obey something more than the normal instructions. 

What we are going to see is we are going to look at instructions and can see how these instructions need to be taken care of or what properties need to be enforced on these instructions which we call as transactions.















(Refer Slide Time: 03:26)

 

Now here are some examples of, typical examples of transaction. Many enterprises use databases to store information about their state. Now any occurrence of a real world event that changes the enterprise state requires the execution of a program that changes the database state in a corresponding way. For example balance must be updated when you deposit into a banking database. When you withdraw an amount, basically need to update again your account by modifying the balance correctly. So typically what we say is the transaction is a program that accesses the database in response to real world events. They basically going to modify the state of the database. Transaction essentially modifies the state of the database.
 
(Refer Slide Time: 04:24)

 
Now here is the very simple example of a database transaction. As you can see here it’s a debit transaction which tries to withdraw a particular amount from a banking database. If we show that there is a debit, an account number and debit amount is given to the debit transaction. Now the begin transaction shows that this is the start of the transaction, execution of the transaction. Now there are three instructions which are part of this transaction. The first one is read the account number and the balance that is there in the database, in the banking database. 

Now as part of the read instruction, we are going to see later, what are all the other instructions that need to be executed when this read instructions has to be executed by the transaction. Now after this read instruction succeeds, you will have your account balance in the variable called balance and now this balance has to be updated correspondingly with the amount that is going to be withdrawn from the account. As you can see later after the withdrawal amount, the new balance has to be computed by changing the balance and then the new balance has to be written on to the database back. And finally you signal that this transaction is finished by giving an end transaction. Now what we basically see here is the three instructions id 1, id 2 and id 3 shown in the slide here together constitute what we call as a transaction. 

Now normal programs we don’t basically distinguish by grouping instructions together and saying that they together constitute a transaction. Now we will go further deep analyze why these instruction put together will be called as transaction in this particular case. 
 
(Refer Slide Time: 06:41)

 

Another example could be a credit transaction as you can see here again we have credit transaction being supplied with an account number and the amount that you would like to credit. As in the other case you have a begin transaction and end transaction signaling that all the instruction in between constitute together a transaction. Now as done in the earlier case the read instruction or read instruction is going to fetch the balance from your bank database, given the account number that particular account number, the balance will be read from the backend database. Now the new balance is computed by adding the amount that you crediting into the account and now you have to write back the new balance back into the database. Now these are very simple examples of what is the debit transaction and credit transaction in the case of a banking database.
 
(Refer Slide Time: 07:43)

 

Now we can see how these transactions in reality operate on the database. For example it is shown here in terms of the credit process which is shown in the slide shows the credit part of the transaction which we saw earlier. Now if you look at the debit part of the transaction you can see that the debit process executes the id 1, id 2, id 3 instructions shown earlier. As shown in this figure the backend database which is stored on the disc contains the information or the data relating to the bank customers. 

Now the credit or debit transaction needs to access the database and retrieve the information and correspondingly modify this information and after modification, they need to write the information back on to the database. Now let us understand this little more carefully here. Now we have has part of the debit transaction id 1, id 2 and id 3, id 1 is a read instruction. Now this read instruction has to go to the backend database management system which takes care of actually now finding out the corresponding data on the disc and move the data back to the local variables of the process, debit process. 

That is now after the id 1 is executed, the balance variable will have correct data relating to the current balance that is there in the account number. Now since it is a debit transaction, the balance is going to be updated here by withdrawing the amount from the current balance and the new value will be computed which will be stored in the balance variable. 
Now at the end of the transaction, debit transaction the value has to be written back onto the backend database and that what signals the end of the transaction. Now at the beginning of the transaction, the value is read from the bank database and at the end of the transaction, the new value is written back on the database this is what we mean by a transaction. A transaction essentially is reading some data from the database processing that data according to the semantics of the transaction and the new changed values are being written back onto the database at the end of the transaction. The same thing actually happens in the case of the credit transaction that is shown here except that the new balance is now added with the amount supplied. 

Now we are going to see when the transactions are operating on the database, we need to have certain properties that needs to be enforced on these instructions so that the state of the database is in a consistent fashion. Now let us understand what are those properties and what happens really when the transactions are executing on the database.

(Refer Slide Time: 11:27)

 

Now here is the case where we are explaining what happens when the transactions have to read data from the database. As shown earlier in the figure, we have to first find the address of the disk block that contains the data item x, x in this particular case can be a balance, it can be an account number. So it is basically the data that needs to be fetched from the backend database. 

Now once you actually found the disk block, you have to copy the disk block information into a buffer in the main memory that is the local variable that is shown in the figure earlier. Now once the item has been copied into the buffer, the value of the disk block is copied into the buffer then you have to copy that value into the item x to show that the program variable named x now contains the value that is fetched from the disk block. This is what the read operation which is shown in the transaction signifies.

(Refer Slide Time: 12:43)

 

Now we have correspondingly a write operation which shows how the write is executed. In this particular case you can see that find the address of the disk block that contains x. Now in this particular case you not only fetch the value of the data, you are going to modify the data and then the updated value is going to be written back onto the buffer as shown here. We can understand here, the last step is different from the earlier read operation. Store the updated block from the buffer back to the disk that is extra instruction that is going to be executed in this particular case. 

Now essentially read and write are the two operations that are going to be used by the database transactions. And we have seen how the read and write fetch the required information for the database, fetch the required information for the transaction from the database, that’s what was explained right now. This is similar to the normal process which would have read the value from a backend disk file. This is essentially same concept as of the process, trying to open a file and read the information from a file. 

What more has to be done here is it is not just a set of file operation that are been performed by a process but we also need to enforce certain conditions on this instruction so that the database state is always maintained consistently. And that is what we mean by a transaction and the transaction has to enforce these properties on the instruction it is executing.







 
(Refer Slide Time: 14:48)

 

One of the things that will happen for a normal process is when failures occur, the process can leave the files that it has opened in a inconsistent state. For example if you understand the operating system and open, if your process opens files and the operating system crashes due to power failure or other reasons, what really happens is the files that are open could be left in an inconsistent fashion. Similarly if a file is been operated simultaneously by more than one user, again the chance of corruption exist on the file because there is no guarantees on how the file is been accessed simultaneously by multiple users. And this has to be prevented in a database because the data that is being stored in a database needs to be in a consistent fashion always that is to be maintained. For example if you consider a banking database, whatever happens you wish that you should not lose your money that you are depositing into the bank. 

If the bank come and tells, there is a power failure and we have lost your 1 million rupees that you deposited, you are going to say that this quiet unacceptable and you want the bank to ensure that whatever happens, the data that is stored in the banking database is consistent all the time. This is an essential difference between database and file systems. File systems could be, they may not be any guarantees associated with file system in operating systems whereas when you take the database transaction there is certain level of guarantee that is given to you regarding the state of the database at the end of the execution of the transactions.

Now let us look at what kind of failures can happen in a system and what are the consequences of those failures. They could be different kinds of failures, we have actually listed a few failures here and we will start discussing them in more detail as we go along. Now you can see here the first kind of failure that can happen is a hardware or software error in the computer during transaction executing. This means it is possible that there is a problem in the hardware or the software. For example it could be a operating system bug or it could a hardware bug that could have made the computer fail which means that when you are executing this set of instruction that is shown earlier as a transaction, the failure can occur. The other kind of failure that can occur is internal to the transaction. Failure caused by an operation in the transaction, for example you are actually dividing by zero, divide by zero will cause the program to crash.
 
(Refer Slide Time: 18:23)

 

So this is another kind of error that can happen in the system. The other kind of errors is condition that cause translation of a transaction. For example data needed for the transaction not found, you are trying to transfer funds from one account to another account then you recognize that the other account doesn’t exist. This will result in the transaction to be aborted because the fund transfer is not happening correctly. The account into which you should be transferring the funds is not found in this particular case. 

The other important issue is concurrency control enforcement which is to be done when multiple transactions are simultaneously operating on the database. For example if you really see how people can operate with the accounts, it’s possible that two, you could be withdrawing some amount of money using your atm machine but at the same time there could be another transaction trying to transfer funds from your account to another account. 

When this happens it is possible that the state of the database could be corrupted unless there is some kind of a concurrency control that is enforced to ensure that the system is in consistent state. We will go and look at the subsequent lectures in detail how the concurrency control is enforced by the database management system on transactions so that the database is in the consistent fashion. Now it is also possible because of the concurrency control that is enforced, a transaction is aborted because the transaction is started executing and the concurrency control mechanism found that transaction cannot proceed anymore. Then it may also abort the transaction. So this is another reason why a transaction could fail. 

(Refer Slide Time: 20:37)

 

They could be other reasons as well, something like loss of data in the disk blocks during a transaction, due to let us say the disk head has crashed. So then it is possible that you are not able to retrieve the data correctly from the disk. This is a disk failure. This occurs when disk hard disk has failed. There could be other catastrophic reasons for you when you deal with databases, things like power failure, fire and other kinds of catastrophes like earthquake which could destroy the data and they are beyond the human control. 

And one of the things you must realize is all these kinds of failures are possible and in the event of this failures, the database till should ensure that the data that is stored in the database is consistent and it is available by other means. That is you are able to retrieve the data back, even when failures of this nature occur. Now a part of this lecture we will explore, how you can handle this situation when failures occur when transactions are executing.












(Refer Slide Time: 22:06)

 

Now what we are going to look at is we will intuitively understand the concept of transaction to start with and we will see what can happen to a transaction in the event of failures. We in fact look at several kinds of failures, starting from a transactional error to a disk failure, to a power failure, to a more catastrophic failure. So what we would like to see is what happens if this failures occur, when transactions are in progress. Imagine you are withdrawing money from your bank account and the power fails.  

Now what happens? Is your bank account still shows correct balance or is it going to show that you have already withdrawn when you have not taken your amount. What is the state in which your bank database will be left when the failures occurs, when your withdrawing money from a account. Now here is a case where it is shown more precisely to say what kind of scenarios can prevail and how those scenarios have to be addressed. 

Look at the scenario one. What happens if the credit transaction fails after executing ic 1 and before executing ic 2? Remember ic 1 is a read account balance instruction and ic 2 is when it is actually modifying that balance local which still has not written that value back onto the database because the right has to be done in the end of it, ic 3 has to be done to write the balance back on to the backend database. 

Now what happens if the credit transaction fails after executing ic 1 but before executing ic 2? Now in a normal scenario, if you don’t really take care of this situation, it is possible that the database is left in an unknown and undeterministic condition when the failure occurs. But you have to actually prevent this from happening, by saying that it will bring back the database to a consist fashion if the failure occurs. In this particular case, you have to ensure that all the instructions ic 1, ic 2 and ic 3 are either executed or not executed at all.  

This is a very important property that needs to be ensured for database transactions. We will go to see this property in more detail, this property is called the atomicity property of the transactions. That is all the instruction put together have to be executed either in full or none of them should executed at all. In fact if you carefully look at the initial example where we had proceeded the three instructions ic, 1 ic 2 and ic 3 with begin transaction and end transaction. All the instructions between begin and end have to obey this property called the atomicity property that either all the instructions are executed in full or none of them are executed. 

This is what we see has scenario one. What can happen if the credit transaction fails after executing ic 1 and before executing ic 2. Let us move to scenario two. What happens in scenario two? 

(Refer Slide Time: 26:01)

 

If the credit and debit transaction executes simultaneously, what are the likely things that can happen? In fact shown a case where ic 1 is executed then followed by id 1 is executed then ic 2 is executed then id 2 is executed, then ic 3 and followed by id 3. If you carefully look at the way it was written here, both the credit and the debit instructions have been interleaved. ic 1 is basically a read account number and the balance, id 1 is also a read instruction on the database except that this is the debit instruction, the earlier is the credit instruction. Now this will also read the balance in the account number. 

Now if you say that both are operating at the same account number, they are reading the balance at the same time. now imagine ic 1 has the same balance, the value that is currently let us say the account has a balance of 500 rupees in your account then both ic 1 and id 1 read the value as 500. Now imagine that your depositing 200 rupees and withdrawing 100 rupees. Now ic 2 will say that 500 plus 200 which is actually 700 and ic 3 will try writing 700 back into the database whereas id 2 will try to reduce the balance from 500 by 400 and id 3 will write the value has 400. 
Now you can see, you have lost some amount in the process because the credit amount is completely lost because both the credit and the debit transactions are simultaneously operating and only the debit is shown here, the credit is lost. The credit that is done into the database is lost in this particular case. So this is what we see as scenario two. When transaction operate concurrently on the database items, it is possible that the database state is left in an inconsistent fashion as shown in this particular example. Now we have to prevent this from happening and this is what we call as the consistency property of the transaction. 

Now what we mean by consistency here is when the transactions are operating concurrently, simultaneously we need to enforce the condition that the transaction in effect have executed one after the other rather than simultaneously. This is in some sense we need to prevent if there is conflict between transactions. They operating simultaneously on the database items has to be prevented and this is achieved by what we earlier called as concurrency control mechanisms 

So we need concurrency control mechanisms for making sure that the database, when it is operated upon simultaneously by multiple transactions is not left in an inconsistent state. this is what we see has a scenario two and scenario two gives the property of consistency whereas scenario one gives the property of atomicity to the transaction.
 
(Refer Slide Time: 30:00)

 

Now let us move on to the third scenario. Scenario three where basically it is possible for one transaction to see the values of the other transaction before it is actually finished, its full operation. Now that what is actually stated here. What happens if the result of the credit transaction are visible to debit transaction, before it is actually written onto the database? What does this mean? This is elaborated further by saying that ic 2 writes the balance. The earlier case if you are actually depositing 200 rupees when your initial balance is 500, ic 2 will write the value of 700. 
Now the debit can read this value of 700, even before the credit has actually committed its value to the backend database. Now we can, debit can now go and then withdraw the money from the new balance even before it is written back on to the backend database. If this happens, the results of one transaction are visible. In this particular case, the credit transaction are visible to the debit transaction before it is actually finished execution. Now this results in what we call as an isolation property because for some reason if the credit transaction fails later, for various other reasons if the credit transaction fails and if its results are already visible for debit transaction, you need to abort the debit transaction also because it has read the values of a transaction that is aborted. This is what we mean by causing cascading aborts. 

If a transaction values or results are available for some other transaction before it is committed, it could lead to cascading aborts. To avoid this, what we have to do is we have to enforce the property called isolation. Isolation ensures that the transaction results, the values which the transaction has changed, the values of the data items which a transaction has changed are not available for other transactions till the transaction has actually committed. Actually concurrency control protocols and commit protocols which go together, ensure the consistency of the database in the presence of multiple transactions executing simultaneously on the database. Now let us move on to the forth scenario which leads to the final property, forth property of the transaction.

(Refer Slide Time: 33:17)

 

Now this scenario four tells what happens if the database server crashes before the changed data is written onto a stable storage. One could imagine several situations where the database values have been written, the transaction have committed but their final values have not been written onto the database for various reasons. Now whatever happens after the transactions says it has committed, its value should be preserved. The value that the transaction has changed should never be altered after the transaction has been committed. 
So you have no way of saying that the results of the transaction is lost after it has committed. This is what we mean by the property of durability. All results of the committed transaction are preserved after that point once the transaction has committed. You have to guarantee this inspite of any other kind of failure that may happen to a database.

(Refer Slide Time: 34:39)

 

These four properties are very important properties when we talk about transactions. Now to just repeat this properties, atomicty ensures which we actually derived from scenario one ensures that all the instructions of a transaction are executed in full or none. So the first question of some part of the instruction being executed and some part of the transaction instruction not being executed doesn’t arise at all. Because we ensure that all the instruction of the transaction are executed in full or none.
















(Refer Slide Time: 35:23)

 

The second property which we discussed is a consistency property. When multiple transactions are accessing data simultaneously, the access is protected through concurrency control mechanisms to ensure that the updates which are done by the concurrently executing transactions are not lost on the database. This is what we actually mean by the property of consistency. And we also mention that the consistency is ensured in database management systems by using a set of concurrency control protocols and we are going to study this concurrency control protocols in depth during this lectures.
 
(Refer Slide Time: 36:10)

 

Now the third property is isolation. The isolation property ensures that the results of one transaction will not be visible to the other transaction till the transaction commits. This ensures that there are no problems relating to partial results being available for other transactions. We also mention that when this happens cascading aborts takes place, when one transaction results have been read by other transaction and the earlier transaction has to be aborted. And to prevent this cascading aborts, we enforce the property of isolation on the transactions.
 
(Refer Slide Time: 36:58)

 

The forth property is the durability property and it says that the effects of the committed transactions are not lost after commitment. For example if you have deposited some amount into your bank and you want to ensure that it’s never lost after you have actually deposited the money into the bank. It will never be lost, that is basically the durability property.
 














(Refer Slide Time: 37:26)

 

Now all these four properties put together are nicely known as the acid properties of the transaction as show here. It is the summarization of the four properties that we have been so far discussing. A stands for atomicity, C stands for consistency, I stands for isolation and D stands for durability. So these four properties put together are called as the acid properties of the transaction. And normal process will not obey this acid properties whereas the transactions in the database context will obey this acid properties. Now one of the things that we are going to look at through this lectures is see how this acid properties are realized by the database management system when we are actually executing transactions in the database.
 
(Refer Slide Time: 38:27)

 
Now we will also further elaborate this acid properties littler more formally by actually taking what happens and how this acid properties are ensured. Now as you can see here in the case of the debit transaction, all the instructions starting from the begin transaction to the end transaction will be executed in full or none which actually means that id 1, id 2, id 3 have to be executed in full. Now one of the things we are going to do is when there is a begin transaction, we record the state of the database. Now whatever happens after the transaction starts executing, if there is a failure we will ensure that you get back to that state by restoring the state to the original state if the instructions are not executed in full. 

For example if your original balance, starting balance is 500 rupees and for some reason the debit transaction cannot be executed, all the instruction restore the balance back to 500. This is what we mean by undoing a transaction. The transaction, all the instructions which were executed partially, till completion of the transaction are rolled back which means all those instruction will be nullified. We actually rollback on those transactions, so that effect on those instruction is nullified. So this is what we mean by atomicity property. We ensure that either all the instructions are either executed together or none of them are executed.
  
(Refer Slide Time: 40:17)

 

Now again, to stress again what really we were talking about consistency in case of both debit transaction and credit transaction access the balance data simultaneously, we will protect them through the concurrency control mechanisms. A simple mechanism that we are going to use is we lock the database items and allow only transaction which acquires this locks to change the values of those data items. And only when the transaction releases the locks on those data items, other transaction will be allowed to use those data items, this is a very simple technique. 
The most sophisticated techniques that can be used for enforcing concurrency control mechanism but this is what we would like to do to ensure that the consistency property is enforced or realized on the database.
 
(Refer Slide Time: 41:14)

 

Now when you go to the isolation property, you are going to look at either debit or credit transaction results will not be available unless they are committed. in one way these transaction have to hold on to this locks when they require and should not release those locks for other transactions till they are committed to ensure the values that they are modify are not available to other transactions till they finished execution, till they reach the state of end transaction which means that now they have committed their values and after that only those results will be visible for other transactions.
 
















(Refer Slide Time: 41:56)

 

Now there are several ways in which we can ensure the durability property. the durability property will ensure that you have backups sufficient backups, you have written all your logs committed transaction locks and there are various ways in which the effects of this are preserved to make sure that all the committed transaction values can always be obtained by using the backups and the transaction logs. You are going to look at this property and how this is realized in detail as we go along.
 
(Refer Slide Time: 42:31)

 

Now we come into more details of what really happens with transactions. For this actually we will introduce certain terminology to start with. 
The idea is to get more formal with the transaction concepts, see them in more detail as we progress. So far we have been looking at the properties very intuitively, trying to understand them in a very intuitive fashion. Now we will try to understand the concepts in the more formal way.

Now there are two case in which the transactions can enter into. one is a commit state which actually means when the transaction has completely executed all its instructions, it can enter into a commit state which actually means that all the reads and writes of the data items which is actually read can now be written back and there been safely written back of the database in which case we say that the transaction has committed itself. Now for some reason, the transaction has started executing but it cannot commit the values of the data item that it has changed which it has read from the database then we say that the database has entered the state of abort which actually means that all the effects of the transaction will be nullified and database state will be left when the transaction start executed.
 
That is equivalent to saying that I actually had a begin transaction. The state of the database when I started executing this transaction which is the begin transaction and I actually keep the state back to that initial state when the transaction started executing. That is called the abort state. So we have commit and abort. A transaction could be either committing or aborting. When it is says it has committed, it is writing all the values that it has read and changed back onto the database. When it is says it is aborting, it is not committing any of the values that it has changed.
 
(Refer Slide Time: 44:46)

 

So to emphasis the transactions are not just ordinary programs instructions, all our discussion today highlights the fact that additional requirements are placed on transactions to ensure that this acid properties atomicity, consistency and isolation are realized with the transactions.
(Refer Slide Time: 45:14)

 

Now here is the case where we can quickly see with respect to commit and abort, what really happens for the atomicity property. Now atomicity property says that a real world event either happens or does not happen. Now if you take the case where a person either books or does not book his tickets which actually means that when he is actually book his tickets, the transaction has committed the values. When he says he didn’t book the ticket, it means that the transaction has aborted. And again to give the state the think that it is not true of ordinary programs, a hardware or a software failure could leave files partially updated which is not the case in the case of transactions.

When you say I have booked my ticket, it means that you book your ticket that means the transaction booking tickets as committed and when it says it is not book the ticket means that it is not committed, it is aborted.
















(Refer Slide Time: 46:26)

 

Now coming to the consistency property, you have to ensure that the set of integrity constraints that are specified by the transaction are all enforced when the transaction has executed. Now what we mean by this is transaction designer must ensure that assuming the database was in a state that is satisfied all integrity constraints when execution of a transaction got started. Then when the transaction has actually completed execution, we need to ensure that all the integrity constraints are once again satisfied. In a simple way, the consistency can be ensured by saying that the transaction execution results in a serializable execution. That is the transaction is executed as if all the operation have been executed in a serial fashion.
 
(Refer Slide Time: 47:19)

 
We are going to look at that particular property right now in the next few minutes in the little more detail and see what does it mean by consistency preservation. And isolation essentially will avoid cascaded abort as explained earlier.

(Refer Slide Time: 47:36)

 

Here is a simple case where isolation was given in a more detailed fashion. It relates to when multiple transaction execute concurrently and you want to actually ensure that the final execution thus preserves the consistency by ensuring that one transaction values are not read by the other transaction till it has finished.

(Refer Slide Time: 48:09)

 

Now we will see this property of concurrency and isolation together by taking a simple example as given in this particular case. As explained in this diagram, as you can see this here t1 has two operations op 1 and op 1 2 and t2 has two operations operation 2 1 and 2 2. Now it’s possible that these sequence of operations can be interleaved in multiple ways on your database. As you can see here one possible sequence is operation 1 1 executed first of transaction 1 and then operation 2 1 of transaction of transaction 2 then operation 2 2 is executed of transaction 2 and operation 1 2 of first transaction is executed. As you can see here from the execution sequence of this, we may not be able to say that transaction t1 completed all its operations before transaction 2 is executed but on the other hand if there is a way, you can ensure that all the instructions of transaction 1 are finished before transaction 2. It is equivalent to saying that the set of instructions that are executed are all in a serial fashion.
 
Now one of the requirements of operation 1 1 and operation 1 2 to be serializable in this context is they are not operating on the same account. It is possible that two people are withdrawing money from two different accounts. Since there is no conflict in this particular case between the two operations, it doesn’t really matter even if o1 2 is executed later. we can always say interchanged the operations since there is no conflict in  this particular case and rewrite the set of operations as if they have been executed as o1 2, o1 1, o1 2 and then o2 1 and o2 2. When operation don’t conflict, it is possible for us to interchange operations and then ensure what we see the property of serializability. That is all the operations of t1 have finished before t2, that is what we mean by serializability or in other sense what we going to say is all the operations of t1 since they are finished, t1 precedes t2 in terms of the operations have been executed on the database. This is called serializability.
 
However let us say that the operation 1 1 and operation 2 1 conflict with each other in the sense that they access the same database item. In this particular case, we can say that they are accessing the same account and the same balance and they are trying to modify the same balance. They are not just reading but writing values, modified values onto the database in which case we say there is a conflict. Now operations will conflict if they operate on a same data item and one of them is right that is what we mean by conflicting operations. When transaction is conflict we need to serialize the transactions and this is what we mean by conflict serializability. the conflicting operations should be executed in such a way that we know that the conflicting operations are executed in a serial fashion which actually means that we can ensure that the operations on the database in this particular case if o1 1 and o2 1 have conflicting there have been executed one after the other and that decides how the transaction precede with each other.
 







(Refer Slide Time: 52:33)

 

Now let us see the following scenario where on o1 1 and o2 1 which are two conflicting operations, we say that t1 preceded t2 and let us say o1 2 and o2 2 are also conflicting. Now let us say as far as those operations are concerned, t2 precedes t1. Now this is a scenario that will result in the transactions t1 and t2 not being serializable because as far as the conflicting operations o1 1 and o2 1 are concerned, the t1 is preceding t2. And in the case of o1 2 and o2 2 which are again conflicting the transactions are preceding in the other direction, t2 is preceding t1. So we can’t say as far as the conflicting operations are concerned t1 is executed before t2, one case t1 executed before t2 in the other case t2 is executed before t1. This is a very interesting thing which we going to study in detail in the next lecture.
 
We see how transactions need to preserve the property of conflict serializability. Only when transactions execute and they are serializable, conflicts serializable we say that the database is, the transactions have executed in the correct fashion on the database. We are going to further study this property in tomorrow’s lecture in detail as concurrency control mechanisms. The concurrency control mechanisms are expected to provide the property of conflict serializability. They ensure that when transactions are executing concurrently, we can serialize them in a, transactions are serializable and that’s the property that is ensured by concurrency control mechanisms. We are going to study the concurrency control mechanisms in detail in tomorrow’s lecture.                                     

Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 18

Transaction Processing and Database Manager

In the previous lecture we have looked at the basic properties of transactions namely the acid properties atomicity, concurrency, isolation and durability. In today’s lecture we are going to see how these properties will be realized by the transaction processing system within the database manager. We will take a few simple examples and through this examples we will illustrate how the transaction processing system will ensure the acid properties of the transaction.
 
[Refer Slide Time: 02:04]

 

Here is a very simple example shown in the slides here. There are two transactions here which are shown T1 and T2. T1 is a account transfer transaction, transfer of money from one account to other account. Now 1000 rupees had been transferred from account A to account B by transaction T1, transaction two is an interest payment transaction. So it is actually crediting into each account a 5 % interest into each of the accounts. 

Now what is shown here is this T1 and T2 operating simultaneously on the banking system. Now what we will do is we will try to understand these two transactions in terms of the various operations performed by these transactions. What I am going to do here is I will write T1 as performing several transactions or several operations. Now the first operation that is performed by T1 is to actually take the account A and read the value of the balance that is there in this account. So it is basically a read operation of the account. The second operation is essentially to add 1000 rupees into this account and the third operation is going to be writing the value back. The same thing is going to be done for the account B. So I will actually show that as the fourth operation but the subsequent operations will not be shown here. 

They are self-explanatory one can understand after that, the other operations. Now the suffix here shows that this is the first operation and this gives the transaction id. One is the transaction id and the one here indicates that it is the first operation. So operation 1 1 indicates that this is the first instruction of transaction. This indicates that it is the second instruction of transaction 1 like that it is shown here. 

Now if you basically take a operation Oi j, it indicates that this is j th operation of i the transaction. This is the notation that we will be using. Now as you can see here this is basically a read operation on A and this is basically a write operation on A. So we have between the processing we have the reads and the writes happening on the data items. Now we can also understand the transaction T2 also has shown in the slide as trying to do the following operations. 

O21 is going to be a read of A and then the O22 operation is going to be an update on the value of the data item and then O22 O23 is going to be an item again A the rest of the operations as shown in the earlier case for the B, operation B.
  
[Refer Slide Time: 06:17]

 

Now what we are going to show you is what happens when these transactions are executed simultaneously on the database. Now a list of actions form a set of transactions as seen by the dbms. As you can see here O11 O12 O13 O14 are set of transaction, set of operations constituting T1. Similarly we have O21 O22 O23 constituting the set of instructions constituting transaction T2.

 
[Refer Slide Time: 06.52]

 

Now it is possible for these operations to get interleaved in the sense that it is possible for these transactions, operations of the transactions to execute in an interleaved fashion. Now when this gets executed in an interleaved fashion, we basically call that as schedule. A schedule is nothing but a series of operations as executed by the database management system. 

Now you can see here it is possible for these two transactions to execute concurrently, a set of interleavings that were shown is the operation T1 are constituting A equals to A plus hundred is executed here. Then T2 is executed which is equivalent to saying A equals to this operation is executed here, followed by the T2 of B then T1 again B equals to B minus. This is one schedule which is called a possible schedule, we can call this is as SC1 as one possible schedule. As you can see here these two constitute transaction T1 these two operations, these two operations constitute T2.
 














[Refer Slide Time: 08.29]

 

Now there is another possible schedule also shown in the slide there in which case T1 and T2 are executed as shown here. But the other two operations are interchanged. This is executed before the other operation. Now when we actually have the schedules, one of the important criterion for this schedules to be valid is to see that these schedules produce proper consistent results at the end of the execution.

[Refer Slide Time: 09.07]

 

Now this schedule one can be seen as shown in the slide as read and the writes on the various data items. T1 is basically is reading A and then writing A. It is reading the old value of the bank account balance of account A and writing the new value for the balance here. T2 is also doing a read of A and write of A because you are computing the new value of A by calculating the interest that is payable for this account. Similar way we can also write for write B and then read B and write B and read B and then again write B. What we are going to show you through this example is what happens when these reads and writes are interleaved from the consistency point of view. 

Now we can see here the notion of what is correct from the execution point of view is shown here. Suddenly we don’t want transactions to execute one after the other because then the throughput of the system will come down drastically. You want as many operations as possible should be executed in a concurrent fashion to actually increase the throughput of the system. Now when concurrently executed transactions at the end of it whether they can be translated into what is called a serial execution.

Now in this particular case, what you would like to say is T1 executed completely after T2 this is one possibility or T2 executed after T1. As long as this is possible for you say, we call this kind of schedule as a serial schedule. And that is what is actually shown in the diagram here. A schedule is which is equivalent to one of the serial schedules is equivalent to saying that either T1 executed before T2 or T2 executed before T1 that is one of these should be possible.
 
[Refer Slide Time: 11.28]

 

Now to see what really happens when the execution is not serial that means when finally you are not able to detect saying that the execution of the transactions is not as per the serial schedule.




 
[Refer Slide Time: 11.51]

 

I will take a very simple example and show how exactly will deduce the serial schedules. Let us take a case of a transaction T1 with operation x and then 1 2 which is actually a write of y. Now we can take another transaction T2 where basically it is a read of y and then a write of x. Now no matter how these operations are executed. As long as it possible for you to say that all the operations at T1 have been executed before T2 which is equivalent to saying that if there is a schedule that says O11(x) and O12 (y) has actually finished before O21 (y) and O22 (x) are executed by the database manager. This is equivalent to saying that T1 finished before T2. This is what actually we mean by a serial execution of the transactions.

[Refer Slide Time: 13.26]

 
That is T1 finished execution before transaction T2 started executing. Now it is also possible for you to also have the reverse order where all the operations of T2 have been executed before T1. This is a very simple and straight forward case where we can easily save on all the operations of T1 and T2 are executed in this particular fashion very easy to see that T1 has actually finished all the operations before T2 stated executing. The only case where you will have problems is when some operations of T1 have been executed in such a way that they are interleaved with the execution of the operations of T2.
 
[Refer Slide Time: 14.12]

 

Then the problem of deciding whether the schedule is equivalent to a serial schedule that all the operations of one transaction finish before the other becomes a important requirement. And that is what actually we are going to look at how that can be done. In this particular case what we are going to say is all that will be required for us as a criterion whereas schedule is produced is two operations are said to conflict, we say the notion of a conflicting operation is when one of them is a write operation. One of the operations is a write operation.
 












[Refer Slide Time: 15.15]

 

Now to give you the little more simplistic view, let us say T1 is trying to read a data item x and T2 is actually trying to write the data item x. These two operations O1 of some i, O2 of some j are said to conflicting because they are operating on the same data item and they are conflicting with each other. Here as you can see here, T1 is reading the data item x, T2 is writing the same data item x. Since both transactions are reading the same data item and one of the operations is a write operation, we say that these two operations are called conflicting.

Now whenever we have conflicting operations like this, the first inference is here O1 i and O2 j are conflicting. Now in transactions, if there is conflicting operations and there is a way this conflicting operations have been executed, let us say the conflicting operation in this particular case is executed is such a way that this is the order in a schedule. Now this order actually determines that T1 actually preceded T2 because it is conflicting on data item x and T1 has been executed before T2.
 













[Refer Slide Time: 17.02]

 

And now this order should be preserved, no matter what happens with respect to other operations. And as long as you preserve that order with respect to all other operations, we say that the operations have been executed in a serial fashion or the schedule is reducible to a serial schedule. This is the concept of serializibilty. Now this is the important notion here is when transactions are executing concurrently. We need to ensure that the conflicting operations are serializable, all the conflicting operations are serializable. 

Now here is a very simple case shown in the slide where it shows that where it is not possible to serialize, we need to actually abort the transaction. Now in this particular case it is shown that T1 actually is reading as we go to the earlier case, T¬1 is actually reading the data item A and then writing data item A. Now if you look at the T2 is also reading data item A and then writing data item A. Now the other part of T2 is read B and write B. Now as you can see here with respect to data item A, the order between T1 and T2 is T1 is before T2. 

Now if you look at the data item B, it is coming in the revere direction which actually means that as you can see here, on the data item B as far as the conflicting data item A is concerned, T1 is before T2, as far as B is concerned it is T2 before T1. As you can see this is on data item B and this is on data item A.
  








[Refer Slide Time: 19.18]

 

Now from this it is not possible for us to say whether T1 actually has finished before T2 or T2 has finished before T1. Since we can’t now decipher which one has actually finishing before the other, this schedule is non serializable schedule. And this is what should be avoided. A non serialzable schedule shows that the execution of the operations will lead to inconsistency. The database will be in an inconsistent state when we have the operations executed in a non serlizable way. 

Now in this particular case it is shown here that T2 can commit but T1 has to abort. That’s what was shown in the slide saying that only B can come, transaction T2 can commit but T1 has to abort because T1 trying to commit here will produce a non serialzable schedule. This is also shown, as if you go back to the slide you can see that this is shown as dirty reads, that is write and read conflicts. As you can see B has been, T1 has read A and B values but it is reading the, writing the value at a much later stage. And now the one of the later updates will be lost if you allow T1 and T2 to execute in this particular way. One of the operations will be lost and that’s the reason why the schedule is not allowed. We look at other kinds of conflicts that can arise when transactions are executing. First kind of conflict that we saw is a write read conflict.











[Refer Slide Time: 21.08]

 

We can also have a read write conflict and a write write conflict. what a read write conflict shows is the example shown here is transaction T1 is reading the data item A and if its value is more than zero then it decrements. T2 is actually reading A and it is decrementing the value of A. Now when T1 and T2 are operating concurrently, T1 is actually reading data item whereas T2 is actually writing on to the data item. This is what we mean by a read write conflict on the data item A, with respect to data item A both transaction T1 and T2 are conflicting in terms of T1 reading the data item and T2 modifying the data item. And this is what we mean by a read write conflict.

Again you can see that in this particular case the value read by T1, if it is before the value is changed will be inconsistent. Let us say right now the value of A is zero and T1 reads it as zero, then it is unlikely to decrement that because it has read the value as zero. Now if T2 also read the value at the same time as zero and reads it as zero then it decrements it as minus 1. So that value is actually in conflict. The actual value that is read by T1 and T2 are not correct. 

Another example for conflict is the write write conflict where two transactions, both of them access the data items and tries to modify the value of the data item. This is a case where it is shown here as two people simultaneously trying to credit into the same account is shown as a example of a write write conflict. As can be seen here, T one is actually writing the value into the account, T two is also writing a value in the account A as well as into B and T1 at a later stage is trying to write the value into B. Now as you can see in this particular case, the writes on the same data item will be conflicting leading to non serializable schedule. So typically this is what we mean by a write write conflict. 

To summarize what we are actually seeing, I will give a simple example of an operation and show how exactly is the conflicts serializability is to be achieved. If you take a transaction Ti and say Ti as a operation oip and data item x and you have a transaction T j which has an operation q on x. now we say that these operations are conflicting if one them is write. Now when you say this is write and this is a read, you have between oip x and oj q you basically have a read write conflict. The x data item is read by transaction T i and it is been modified by T j. So this is basically a read write conflict. Now if you say that this is a write write, typically what you see is between operation o i p x and o j q x you see a write write conflict.
 
[Refer Slide Time: 25.01]

 

Now when there is a conflict between the two operations, whether it is a read write or a write write conflict, you need to actually serialize the operations by actually saying that they are executed in a serial order which is what we mean by conflict serializablitiy. Now whenever there is a conflicting operations, we need to actually serialize the two operations which is known as the conflict serializability. 

In this particular case oi p (x) and oj q (x) are the conflicting operations and they need to be serialized in a particular fashion and this is what we mean by conflict serializability. We are going to see in later lectures, how the transactions are executed by the transaction manager to ensure that conflicting operations are serialized or serial schedules are produced by the transaction manager. 

One of the simple technique that is used is what we see as a two phase locking and we are going to study that two phase locking as a technique for achieving conflict serailizability later in our lectures. Now here is actually what is shown as how exactly the transaction manager achieves some of the properties that we have been discussing.




 
[Refer Slide Time: 26.52]

 

The dbms ensure that a transaction either completes and its results are permanently written. This is what we mean by committing a transaction or no effect at all on the database, this is equivalent to saying that the transaction has been aborted. So we have two states for the transaction, either a commit state or an abort state. In the case of a commit state, all the operations of the transaction or executed in full and then they are committed. In the case of abort, no effect at all on the database as far as that transaction is concerned. Now the idea of transaction manager is it controls the execution of the transactions. As we saw in this particular case, it controls the execution of the transactions in such a way that the operations of the transactions are serializable. 

Similarly if you take the recovery manager, recovery manager is responsible for undoing the actions of transaction which do not commit. This is a equivalent to saying that the recovery manger is responsible for ensuring the property of atomicity. All actions of the committed transactions survive any kind of hardware or software failures. This is actually known as writing the committed transactions on to a stable storage. What we are going to do is we are going to look at little further into how the recovery manager ensures that properties of atomicity and durability. 











[Refer Slide Time: 28.28]

 

Now what are issues involved in ensuring atomicity and durability? The following errors  can occur when a transaction is executing. First is it could relate to logical errors. For example you are trying to withdraw some money from a bank account, it is possible that the account itself doesn’t exist or the account doesn’t has sufficient funds. In all this cases, transaction cannot proceed any further. This is what we mean by logical errors. The transaction may have to abort because of logical errors. There could be system errors. For example it is possible that there are problems of network, there are problems of system failures, temporary failure or power failure in which case when the power comes back, you need to know what really happened for your transaction with respect to already started transactions.

A simple example could be, you go to a atm and try to withdraw money from the atm and the power, when you actually press the button for withdrawing the money, the atm stops functioning. atm failed due to various reasons. You would like to know whether their system is actually debited the amount from the bank, from your balance or not. That is basically system errors. There could be crash, system crashes, there could be a hard disk failure, the disk head could have been corrupted. So there could be various reasons why the system didn’t’ perform, it could be a system crash. So all these errors are possible. 

When the system actually goes into any of these errors, you want to understand how exactly the atomicity and the durability properties can be maintained. A simple example trying to illustrate this point will be something like a file which all of us open on a windows machine or any of our unix machines. Now here when you open a file in a editor mode and tries to edit your file, there is no guarantee in terms of what happens when a power fails because the file could be in a very corrupted state. there is no guarantee for you in terms of the state of the file which all of us know we keep repeatedly saving the file, when we actually entering or writing some document, we try actually saving the document as many times as possible, so that when the power goes off or something else happens we still save the portion of the work we have actually done.

We don’t lose the file because of power failure all the work that we have done. Now the same thing cannot happen in the case of database systems because here the more critical data that is been in saved in the file. So we need to ensure that whatever happens when any of these failures happen, the system is still in a predictable state. That is the difference between ordinary file systems implemented by an operating system and a database implemented by commercial systems. They ensure that whenever these things happen, still the properties, the save properties for the transaction, the acid properties of the transactions are retained.
 
[Refer Slide Time: 32.11]

 

To explain how these properties are retained by the system, we need to also understand the different storage types that are available in a computer system. A simple volatile storage, we basically look at a simple volatile storage, this does not survive system crashes that means when the system actually crashes the storage is lost, the storage is volatile it is lost the minute the system crashes. When you talk about non volatile storage, the system actually survives these crashes. That means the storage is the, whatever you write into the storage is not lost when the system crash occurs.

A simple case is whatever is there in the main memory is lost when the power goes whereas if you have written it onto your hard disk, it survives a power failure because it is written into a more non volatile storage. Now we also have a concept of a stable storage which is an abstraction of maintaining replicated copies in multiple non volatile storage media, so that whenever higher disasters occur we still have a way of getting our data back and that is we mean by the concept of a stable storage. 

Now what we are going to see is how this concepts are used for actually achieving the atomicity and durability properties in the transaction manager.
 
[Refer Slide Time: 33.55]

 

What we are going to show here in this particular case is what really happens when transactions have to roll back. The rolling of the transaction has to happen mainly because of logical errors or the system crashes and hence it has to be restored back to a previous state. Transaction abort requires roll back which means undoing all the actions of that particular transaction. 

Now to ensure that roll back of the transaction occurs properly, what we have to do is all the writes of the transaction have to be properly recorded in what is called as a log file. The log file retains all the information relating to the writes of the transaction and this will be used when the roll back has to occur. Now if the system crashes, all active transactions during the crash need to be aborted when the system comes back. This is equivalent to saying that they will all be rolled back and the information that is there in the system in the log file will be used to properly undo the transaction activities, whatever the transactions are being doing. What we are going to do is again in this particular case, we will take a very simple example and see how exactly this happens.
 









[Refer Slide Time: 35.22]

 

Now here is the case where case where the logs are maintained and how this logs are helpful in ensuring the atomicity property or the how the atomicity property will be realized by the database manager. What is shown here is a simple case of writing the logs before the transactions starts executing and making sure that it is carried over whenever the data item is being written or a new value for the data item is being written.

What you can see here is the first log that will be written as far as the transaction T one is concerned is what is called is the begin log, transacting begin log. Now what we have is basically a relating to a transaction, every transaction is preceded by the begin transaction as a keyword. Now this begin transaction actually tells the database manager to write what we see as a log, this is the transaction begin log. 

Now this is a, the begin log has to be written onto the transaction. Now in between the transaction does various operations. As we have seen their, it’s possible that there are several operations which are done by the transaction in between and then we have and end transaction. Now this is actually the last instruction that is executed by this transaction, so we will have a what is called a commit log indicating that the transaction has actually committed which is equivalent to saying that all these operation have been successfully executed. So between begin and end, at any point of time when there is a crash, we need to actually recover back to the starting point. And this is what we mean by actually roll back. What we are calling as a roll back is basically rolling back all the things that a transaction is done to the beginning. This is what is meant as a roll back. Now what we do is we actually ensure that whatever the transaction is doing, is written on to this log and this log will be used for rolling back the transaction whenever a crash occurs.
 


[Refer Slide Time: 37.57]

 

For example you can see that in the slide it is shown that T i writes a data item. Now as you can see the first one transaction starts, there is a transaction log T i start. Now the second thing that you notice is T i writes a data item. now there is a old value and a new value old value is the old value of the data item and the new value is the new value of the data item x. so there is a log that is written there which shows that T i x old value and a new value is shown here, this is how actually the log is written. 

Whenever there is a change in the data item, we basically write the log and now this log shows what was the old value and what is the new value now. When you come to the last transaction, basically you have a commit log that is a T i commit log. So as shown here, we have a begin log and a commit log and in between whatever is happening is being recorded there as shown in the slide there. So one of the things that we are going to look at now is how this logs can be used for recovery purposes. How exactly this logs can be used by the database manager to ensure that whenever those kind of failures that we are talking earlier occurs how the system will recover back from those failures.













[Refer Slide Time: 39.54]

 

Now one of the things when writing this logs is one of the things that one should remember is the id’s of the transactions are stored. So that we know to this logs pertain to which transactions, so transaction id’s are appended when the logs records to identify the transactions for which this have logs been produced. Logs are replicated and stored in a stable storage. This is also very important because in the logs themselves are lost then there is no way you can recover back. 

Logs only assuming that the logs are written on to a stable storage, you can ensure that the transaction can be made to recover. But if the logs themselves are subjected to failure then you will not be able to recover back and hence logs are replicated, one of the assumption we make it logs are replicated and they are stored in a stable storage. So when we say a log is written, we assume that the values relating to the log have been written on a stable storage and it is possible for us to recover this information at any point of time.
 













[Refer Slide Time: 41.10]

 

Now as you can see here we are also showing how this log entries will also be ordered, ordering of the log entries. We say a transaction T are can commit only if the log entry relating to that is saved on a stable storage. This is equivalent to saying as you can see here, when you write this commit log this commit log is actually written on to a stable storage then we say the transaction is committed. This is the point where it is possible for the transaction now to say that it is committed. Now before this is actually written, this log is written all the other entries before this pertaining to this transaction should have also been written onto the stable storage. 

You should never write the commit log before all the other log entries relating to this transaction have been saved on the stable storage. Now only after the writing the entries relating to the logs, you should write the data items themselves after this point onto the stable storage. This is very important, these steps are very important because if you perform them in any other order, you will have problems in terms of recovering back. First requirement is all the log entries relating to this transaction should have written onto the stable storage in the first instance, before you are writing the commit log. Only after writing the commit log, the data item values pertaining to the transaction can themselves can be written onto the stable storage. 

The reason for this is simple. If you don’t write the log values first on to the stable storage, there is no way if something happens to recover from that particular failure. for example if you have written the data value onto the stable storage, now something happens there is now way of finding out what is the state in which the transaction is unless the logs are written properly. So logs are the bases for the database manager to find out what is the state in which the transaction is when a failure is occurred. And hence it is important for you to first write all the logs relating to the transaction then write the commit log and then write all the data items onto the stable storage. This is how one needs to order or write the various things relating to the transaction. 
[Refer Slide Time: 44.09]

 

We will take a very simple example. A simple example in this case to see how the two transactions can really execute writing their logs. In this particular case, it is shown T one and T two T one is actually reading certain data items and writing certain data items. Similarly T two is also reading and writing certain data items. For completeness sake, we also have shown the initial values that are there in the database when this transactions T one and T two start executing.  

As can be seen in the slide, the initial values of A are A is 100, B is 300, c is 5, D is 60 and E is 80. Now T1 when starts executing, it is going to read the values of A will increment by 50 then read the value of B increment it by 100 then write the value of B back into the system. Then will read the value of C. Now C value is inc is double then the value of C is again written. At the end of it A is recomputed as A plus B plus C and the value of A is written. 

If you look at the transaction T2, T2 is actually reading the value of A. It is incrementing it by 10 then it is actually reading the value of D, it is decrementing it by 10 then actually reading the value of E and reading the value of B. Then E value is computed by adding E plus B and then writing the value of E then D is recomputed as D plus E then finally the value of E is written. 

What we wish to covey with this is the two transactions are simultaneously reading several data items and trying to modify. It is equivalent to saying that there is set of operations which are going simultaneously in terms of reads and writes between these two transactions.



 
[Refer Slide Time: 46.21]

 

Now let us understand what really would have been the logs that would have been produced when this transaction T1 and T2 start executing. Now when T1 starts executing you can see that there is an initial log given there as T1 start. This actually shows that T one started executing that is the start log for the transaction T. T1 is the id of the transaction. Now when actually T1 tried writing the value of A, the old vale and the new value are actually stored in the database, in the log. 

As you can see in this particular case, this log shows that A’s old value is 100 and the new computed value is 150. Similarly when B has actually been recomputed, we have a old value for B as 300 and the new value has 400. Similarly for the value of C, it is 5 and 10 and finally when the last computation for A actually took place. A is 150 and then old value of A is 150 and then the new value is 560. It is at this point of time actually the T1 finished executing all its instructions and it is ready for commit.

And that is the time the commit log is written, T1 commit log is produced at that point of time. A similar thing is shown for T2, as you can see there is a start log for T2 and then we have various new values and old values for the data item that are done by T2 are also shown here and finally commit log is shown for T2.









 
[Refer Slide Time: 48:08]

 

Now what really happens is once a failure is occurred, undo all transactions for which there is a start entry in the log but there is no corresponding commit entry. This is equivalent to saying that I will just go back to the previous slide to show what really we are we are saying. Now we can say if T when you actually a system crash has occurred, you can see there is a start log for T1 but corresponding commit log is not present. Then all that we have to do is you have to actually undo whatever has been done by the transaction. 

For example in this particular case, if T1 has actually modified the values of A B C then those values have to be reset back to the old values from the new values. That is equivalent to actually saying that we have undone the transaction because it has not reached the commit state. Now the other case is redo all transactions for which there are both start and commit entries because these transactions have already gone to the finish stage. We are going to redo the transaction for all those which commit, a start and the commit entries are there.












 
[Refer Slide Time: 49.26]

 

And also if typically we have, if the data items are not yet written this is what we mean by actually looking at whether the, not just the log entries but the data item entries are not still written then we need to redo. But if the data item entries are also been stored on the stable storage, there would have been along with commit log there would have been a complete log and the log book showing that the transaction has completed. All the operations relating to it, in which case the redo need not be done and this is typically achieved by what is called a checkpoint record. 

[Refer Slide Time: 50.04]

 

If the checkpoint record is entered into the log, whenever the system writes the data item values onto the database. The effect of the write operations are all committed on the transactions. Now all transactions whose commit log entry appears before the check point entry need not be redone in case of a system crash. So the checkpoint is a place where you can decide whether when a commit log exists whether you have redo or need not redo those transactions. This is how exactly the database manager will ensure, the transactions are executed atomically and they satisfy the property of durability. What we are going to see in the next class is how the concurrency control properties of the transactions are realized by the transaction manager.  

Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 19

Foundation for Concurrency Control

In the last class we have been looking at the basics of transaction processing systems. What we have done is to look at the basic properties of transactions when they execute in the database system. We looked at the essential properties of transactions in database systems and we have been looking at the four properties that are essential for programs, transactions atomicity, consistency, isolation and durability.
 
[Refer Slide Time: 1.56]

 

Atomicity property is required to ensure that all the instructions in the transaction is either executed or none of them are executed. So the property of atomicity ensures that all the instructions either get executed or none of them will be executed. The property of consistency ensures that when more than one transaction operates on the database, the consistent state of the database is maintained. That is they don’t really malign the values written on to the database. 

The property of isolation provides that the effects in one transaction are not visible to other transaction till it has committed all its values to the transaction. The property of durability ensures that the transaction is returned its value on the database, it will be stored permanently on the database. That’s the property of durability. We have been looking at the four properties and trying to understand more on how the consistency property is realized in database systems. 

What we are going to do in today’s class is to look at the foundations for the consistency management which is also known as the concurrency control mechanisms in database systems. What we are going to look at it is the basic concepts of transactions from the view of concurrency control and this is what we call as foundations of a concurrency control in databases. What I am going to do in today’s class is show the basic properties of, basic foundations for concurrency control in databases by first introducing the notion of a schedule. What is a schedule and what are the different things that we can understand by the concept of a schedule. 

The second thing that we are going to look at as part of the schedule is we look at how exactly we can produce what are called serializable schedules. This is basically the basic notion of ensuring consistency in databases. So we are going to use these two things, one we are going to look at what is a schedule, what is a meaning of a schedule and how exactly serializable schedules can be produced. After understanding these two things what we are going to do is in the next class, we are going to see some protocols that are their in the database systems that ensure that this schedules produced by the database are serializable schedules. So we look at the protocols in the next class but in today’s class, we will understand what we mean by a schedule and what we mean by a serializable schedule. 

[Refer Slide Time: 05.28]

 

Now let us go and understand the meaning of what we mean by scheduling. Now the meaning of a schedule in database transaction system is, it is essentially a set of operations performed by the transactions on the database. We will consider a very simple schedule let us say Sa and then here what will do is we will write a series of operations performed by the database, by the transactions on the database. We are going to understand first the notion of what are these different operation before we write the actual schedule. What I am going to do is I will say essentially the database operations could be either a read operation which means that the read an item from the database. Read x shows that this is an operation trying to read a data item, read an item x from the database. 

What this means is if this item is not in the main memory, this item will be fetched from the disc and will be transferred into the main memory of the database. And after that this will be used as a program variable by the program to do some operations that means the value is available, now it can perform some computation using the variable. The other operation that the database transaction can do is what we see as write x. write x essentially means that write value of x back to database which means that the updated value of the transaction has to be now returned onto the database. We will also have two extra operations which are performed either a commit or an abort at the end of the transaction execution.

Commit means we are essentially committing all the things that you have done. This is at the end of the execution of the operations. A transaction can issue what is called a commit command, commit actually means do all the changes. And then abort means essentially discard all the changes. So typically the transaction decides at the end of the execution whether to store the values back on to the database or discard them. These are essentially the operations that we are interested when we are dealing with a database. A schedule essentially consists of read operations, write operations, a commit and a abort command.
 
[Refer Slide Time: 08.44]

 

Now what will do is we will further define the schedule much more clearly by taking a set of transactions that are operating on the database simultaneously. For example take the case where we have the reservation system where passengers are trying to book their ticket simultaneously. They could be one passenger who is trying to travel to delhi by rajdhani express and he will try to book his ticket for that particular train. 
 
There could be another passenger simultaneously trying to book for the same train for going from chennai to delhi which actually means again both of them, both these operations simultaneously operating on the database. Now both of them should get different seat numbers when this operates simultaneously. If both of them get the same seats, we actually leaving the database in the inconsistent state because two passengers cannot get the same berth to travel from chennai to delhi. So that is what we mean by looking at, i know when transactions are operating simultaneously on a database, how exactly we can produce consistent results on the database. 

Now in this particular case what we can see is there could be the first transaction T1 which reads the current reservation information from the database. We will just abbreviate as read as an r symbol so and we will give an prefix r one here to indicate this is being done by transaction one. This is looking at the rajdhani express availability which is given by r one x. now after actually checking this, it will try to write the value back onto the database saying that it wants the reservation for this. 

In some cases the passenger finds that he doesn’t have the money, at this point of time he can discard the changes which means that the transaction can get into an abort state after reading the availability but nor booking the final one. It is possible for various reasons. The passenger didn’t have a lower berth as he desires, so in all this cases the transaction after started could abort the operation of writing this values back on the system which means that essentially the passenger is not interested in booking the berth for himself after reading the values. Now let us say this basically transaction commits at the end of it which means that you have got the berth and your writing this value back. This is what all the operations that could be performed by the transaction T1.

Now if you take basically another transaction T2, it could also be reading the value of this rajdhani express availability for a berth and then writing the value back and then later committing. Now if you basically look at, these are all the operations of the second transaction. It’s possible that these two operations of the transactions can be interleaved in any order when they are executed on the database. What does that mean? It means that it’s possible that, I could execute r1 x of T1 then r2 x of T2 then w1 of x and w2 of x then say I commit c1 and c2. This could be one possible execution sequence because these can be interleaved in whatever fashion that is possible. 

This is basically what we mean by a schedule Sa or some schedule. a schedule is nothing but a sequence of operations that we are performing on the database from transaction T one to T n some n transactions. Now the transactions keep continuously executing on the database which means that as the transactions are coming, we are executing the read and write operations relating to this transactions and either committing or aborting the transactions at the end of what they perform and this process continues. Now as it’s happening, we need to ensure that whatever operations are performed by the database is actually leaves the database in a consistent fashion. 

For example we can look at the schedule that we have just produced and see what would have happen if the sequence of instructions are executed as shown in a schedule S, Sa in this particular case. Now as you can see here the value of x, let us say at this point of time is basically 10 at the start of the execution so read x read one of x would have resulted in reading the value of x as 10, read two x will also result in reading the value of x as 10 then a write would actually resulted whatever computations that was done and writing the value. 

Let us say actually after this computation, I actually write a value of x equals to 5, I subtract 5 form 10 and let us say the w to actually adds 5 to 10 which means that it results in 15 and after that c1 commits which means the value of x will be written as 5, when c2 commits value of x will be written as 15. As you can see here, if the transactions have been executed one after the other, the end result would have been different x, the end value of x would have been different from what was actually produced here. 

[Refer Slide Time: 14.38]

 

If you carefully notice the effect of the first transaction are last on the database if because the second transaction also read the same value of x, not produced by the first transaction and hence this will result in an inconsistent operation of writing the values onto the database. We will characterize this consistency more carefully by looking at schedule and then trying to characterize the schedules in terms of how they basically, whether they are consistent schedules or whether they produce what we see as consistent results on the database. 

Now a simple case is where the transaction T1 is written assuming that this is the only program that is operating on the database. Let us assume that T2 is also written assuming that this is the only transaction that is executing on the database. This requires that T1 is consistent as long as it is executed from start to finish. Consistent from start that is start to end, all the instructions are executed without any other transaction seeing the values used by T1. 

Similarly the same thing is true with actually T2 which actually means that it will also assume that the start to end is executed as far as T2 is concerned without being interpreted. What this means is either you execute T1 completely before T2 or you execute T2 before T1. This is a very important notion here of saying that I have what is called a serial schedule. A serial schedule is one where the transactions are executed in such a way that new transaction is executed only after finishing the earlier transaction. So in this particular case, if you say a serial schedule all the transactions should be executed one after the other.
 
For example if I have n transactions, there should be a mechanism by which I categorize T i less than T j less than T n like this which actually produces a serial schedule. The only problem with the serial schedule is this is very limiting because it’s possible that these transactions can be executed concurrently, simultaneously still actually producing correct results. for example, let us assume that T1 is booking for rajdhani express and then let us say T2 is trying to book the reservation for let us say Trivandrum mail. Now there is no conflict between these two which actually means that even when these two execute concurrently, there is not going to be any problem in terms of the end results because they are not conflicting with each other.
 
[Refer Slide Time: 18.20]

 

So by unduly restraining that I know the transactions should be executing one after the other would only affect the database performance. We can, when they are not conflicting suddenly we can execute them in a parallel way and get better performance from the database rather than enforcing a serial order. This is the first important concept of trying to look at a serial schedule. Now what we will try to do is how exactly one can think of a serial schedule and produce a serial equivalence schedule. Not exactly serial schedule but equivalent schedule to a serial schedule. Now what we do is for this, we will define the notion of equivalent schedules. What this means is two schedules can be seen to be equivalent under certain conditions. 
For example, let us take a schedule Sa and a schedule Sb and define what we mean by an equivalent schedule. Two schedules are equivalent if basically all the operations which appear in Sa also appear in Sb. For example for transactions T one to T n, all the operations will define all the operations appear in both schedules. Now after this point to define equivalent schedule, we need the property of saying what kind of equivalence is this between the two schedules. one is to say as i have actually shown in the last slide that when they are actually not conflicting, it doesn’t really matter how the operations actually appear in the schedule S a and S b. For this what we define is what we call as conflict serial ability which actually means that only when transactions are conflicting with each other, those operations alone need to taken care, other operations need not be, no they can be executed in any possible order. What this actually means that you need to focus between the two schedules S a and S b on what we see is the conflicting operations and ensure this two conflicting operations are done in the same order in S a and S b.
  
[Refer Slide Time: 21.22]

 

Now for this, I will define what it means to say two operations in transactions conflict. conflicting operations are the following. Now one of the operations of the transaction is basically a read operation. Let us say read one of x and the other operation is essentially a write operation. This is conflicting because the transaction T1 and T2 are operating on the same data item X and one of the operations is a write in which case we say these two operations are conflicting with each other. There are also other probability where the first operation is a write and the second operation is also a write which essentially means that T1 and T2 again will be conflicting with each other with respect to this write operation.
 





[Refer Slide Time: 22.39]

 

So from this we can infer that, if one of the operations is a write and two transactions are operating on the same data item in this case X and one of the operations is a write then we say that these operations are conflicting. Now all the executions that a transaction does need to worry about how these conflicting operations are executed in a schedule. Now to give this notion what we say is all that we will be worried about is conflict serializability. That means you don’t need to really concern yourself about serializing all the operations but you have to actually do what is called the conflict serializability. That means when operations are conflicting, you have to do what is called the conflicts serializability.

Now in this particular case, let us say the two operations r one w as shown in the last slide which means that r one x and w two x, if they have performed this operations one after the other, it essentially means that T one has executed before T two as for data item x is concerned. Now it could be the other way round also, depends on how this is done in the schedule, how exactly this conflicting operation is performed. But suddenly if r one x is performed before w two x, this is the order as far as data item x is concerned. 

Now if the transactions are also conflicting on another data item, let us say y and on y database has actually performed operations such that w two y occurred before. Let us say w one y on the database item y if you actually want to order this transactions. Then it is going to be T2 before T1 as far as y is concerned. Now if these two operation occur in this order in a schedule it essentially means that there is no fixed order as far as T1 and T2 is concerned because as far as x is concerned T1 is before T2. As far as y is concerned it is the other way around T2 is before T1 which exactly means that I no longer can infer from this T1 occurred before T2 or T2 occurred before T1 which essentially means that on the conflicting operations, there is no way to actually serialize the transactions by saying T1 before T2 or T2 before T1 in which case such an execution is not obeying conflict serializability because the conflicting operations are not serializable in the schedule. 
[Refer Slide Time: 25.35]

 

Now to actually give the notion how exactly this can be further looked at. two schedules S a and S b can be seen to be equivalent, if the conflicting operations appear in the same order between these two schedules which means let us say I have a schedule where there is a set of operation that are performed in this particular fashion on schedule a. If they actually are performed in the same order in S b, there could be some other operations interleaved but then as long as the final order that I see between these two conflicting orders is the same then I say these two are equivalent schedules. Now this won’t be equivalent if the order in which they appear here is different from each other. 

For example if w x in the other schedule comes before r one x then they are not equivalent schedules. If the operations are not conflicting, it doesn’t really matter in what order they are appearing. For example let us say there is a data item here in this schedule x on which actually transaction one reads this here and then there is another data item which the transaction two is actually writing which is z in this particular case. Now these are not conflicting operations. now even if you change the order of this operations, it still doesn’t matter because they are not conflicting operations which is equivalent to saying that even if you now transfer w2 to before to r1 x, it’s still is okay for me because these operations are not conflicting and hence we don’t really care to actually worry about the order of non-conflicting operations. 

Since in the first one, r one x we just recap what we are trying to do here. Two schedules S a and S b are equivalent as long as the conflicting operations appear in the same order between the two schedules. in that sense the two schedules, one and two as shown here three cases where S a is you know some random order where r one x appears before w2 x, r1 x indicates that this is a read of transaction one on x. This is read write of transaction two on x since these two are conflicting, the way in which the schedule a there appearing is the read x is before the write x. If it appears in the same order in S b also then we say they are equivalence schedules that all conflicting operations appear in the same order in the two schedules. We say they are conflict equivalent conflict it terms of equivalence, they are in terms of conflicting operations they are equivalent schedules whereas you can see they are not conflicting, it doesn’t really matter in what order they appear.
 
[Refer Slide Time: 29.22]

 

Now to introduce the notion of how we use this conflict equivalence in actually deciphering whether schedules are schedules can be use to decipher whether they produce consistent results. We say a serial schedule is always a consistent schedule. This is the benchmark to say that I produce serial schedule then it is consistent. The simple reason here is T1, all operations of T1 executed before all operations of T2. I see this as consistent execution because I am able to execute all operations of T1 before T2 and hence the database is consistent.

What it is doing is consistent. Now when I have a schedule which is not equivalent to a serial schedule, I will try to change the operations which appear in the schedule S a. I transform now this to S a dash but this is an equivalent schedule. As long as the conflicting operations are same between S a and S a dash, this is still an equivalent schedule. This can further be transformed to S a double dash and finally this S a double dash becomes a serial schedule which actually means that I have been able to transform a schedule S a into a serial schedule. 

Now we use this notion to say that S a is a serializable schedule, not serial schedule but it is a serializable schedule. In terms of the conflicting operations, the way i see executed this conflicting operations is same as S a double dash a and hence S a is basically a serializable schedule. Now what we are interested in is producing the serializable schedule because serializable schedules can be reduced by swapping the non-conflicting operations in whatever way you want into a serial schedule.
 

[Refer Slide Time: 31.36]

 

This is in effect conflict serializable schedules, conflict serializable schedules. what we are, as long as a schedule is conflict serializable and at the end of the  execution you are able to show that this schedule is equivalent to a serial schedule, S a is a consistent schedule or the operation of S a is consistent and that is what we are interested as far as serializability is concerned. This is a very important notion of actually being able to serialize the transactions produce serializable schedules.

We will try to look at now is how exactly the notion of serializability will be used by database transactions to produce consistent schedules. Now for that what I will show is to start with, as the database is operating, transactions will be coming in at any given point of time into the database. Imagine for example, this is a railway reservation system which means that the passengers keep coming and keep reserving the tickets at any given point of time which means that there is this set of schedules, this set of transactions T1 T2 T n which keep generated at different points of time. Now as they keep arriving into the database, some operations of T1, T2, Tn will be executed here which actually means that to just produce this we will say O1 x is the operation of transaction one, Oi j is a general operation on a data item y on the database. And this is how this operations are executed as far as the database is concerned. This is what we actually mean by a schedule.  

Now since the database will be operating continuously, these transactions keep coming regularly into the database. It is not possible at any given point of time to actually close the transaction, close the schedule and say I have actually looking at a particular schedule. What this requires is at a given point of time if you want to analyze, you need to put a break point and say I will actually take what is called a projection of this for the complete schedule which means that all those transactions which have actually committed or aborted as far as the schedule is concerned whose operations are all performed that will be included in this complete schedule. 
Let us say up to T some r, I have been able to now do the all the transaction execution then I will say I will execute from T1. Probably I will do a slightly change here and i say T r to just make sure that we get the thing right, T r comes later which actually means that up to T1 to T n these are completed transactions. That means the complete projection of schedule S will include up to T1 to T n which means that my schedule S which is on a partial schedule of all the transactions coming in which includes the T r here. This T r actually goes into this schedule here and then this from this actually, I am actually projecting from this set here to complete S.
 
[Refer Slide Time: 35.20]

 

Now which actually means that T1 at a given point of time the T1 to T r are the total set of transactions that I can consider but T1 to T n is a completed set. And I am actually looking at complete schedule which means that all the operations of these transactions have been included in the complete schedule. Now when you actually take the C of S as the projection then we want to apply the notion of whether this schedule produced is a correct schedule or not. This is when we are going to apply the equivalence this schedule let us say is called S a. Now I apply on S a and then see whether this S a is reducible to some serial schedule.  

A simple check of seeing whether a serial schedule is being produced or not, what we can see is a simple algorithm which constructs a graph showing how the transactions are executed in your system. 







[Refer Slide Time: 36.33]

 

Assume that actually T1 came into the transaction system, now you try to actually see T1 put it as a point in the graph. Now let us say another transaction actually T2 comes into the system. Now assume that T1 and T2 actually conflict on a data item x and then this conflicting operations on data item x are executed in such a way that T1 appears the operation of T1 appears T2. Then provide arc showing that there is a precedence relationship between T1 and T2 showing that T1 comes before T2 as far as this operation is concerned. 

Now we assume that there is another transaction which came T3 and this is conflicting on let us say an item z and this is the operation as far as this is conflict is concerned. Let us assume now between T3 and T1 there is a relationship in terms of conflict on Y and if in this case T3 is executed before T1 we have essentially a cycle in this graph which means that the conflicting operations actually in terms of the graph of forming a cycle. 

What is the meaning of this cycle? Assume now T1 is less than T2 as far as first arc is concerned T2 is less than T3 as far as second arc is concerned. As far as the third is concerned T3 before T1 which shows that it is not possible form this to actually say any particular order in which these three transactions have executed. The last one will be wrong because by a transitive relationship T1 should have finished before T3.









[Refer Slide Time: 39.03]

 

And this is what exactly is done to show whether a schedule is a serial schedule or not. There should be any cycle if there is a cycle it shows that it is a non serializable schedule. The presence of the cycle in the transaction graph is shown to produce non serializable schedule because a cycle prevents you from coming with a order in which the transactions are put in a particular order of one being finished before the other and hence this will not produce a serial schedule. I think this is very important as far as the concept is concerned because we use the transaction graph to understand whether a protocol is basically produces serial schedule or not. 

As a later lecture towards the next lecture, what we are going to build is several protocols for actually building the or executing the transactions. Essentially these protocols will try to construct the transaction graph in such a way that an incoming transaction is put in the correct order as far as the sequence is concerned. All that you want to do is you don’t want a cycle in the graph, you are not trying to produce a cycle in the graph and the protocol has to ensure that there is no cycle in this particular graph and that’s what exactly we can understand. For example imagine that there is currently a current transaction graph looks something like this. Let us say I have three transactions active in my database and this is the current sequence as far as the conflicting operations are concerned.
 
Let us say a T4 now comes into the database at this point of time. I have several options of where I can put this to avoid a cycle from coming in into this particular graph. It is possible for me where the protocol always allow the transaction graph to grow only in the forward direction which actually means that it is possible for me to keep this T4 here that is one possibility or I can put T4 here or I can put T4 here which actually means that the graph goes only in the forward direction and when it goes in the forward direction it prevents any cycle from occurring because you are not going to but a backward arc. As long as you don’t put a backward arc, you let the transaction graph only in forward direction.
 
[Refer Slide Time: 41.38]

 

It is possible for you to avoid a cycle in the graph. The other possibility is it is possible for the protocol to decide to put it even before which actually means that if it can be made to read the value, let us say there is a write x here and I let this transaction read the value of x before this is modified then it is possible for T4 to be put before T1 even when it is coming after the graph. In that sense it is possible that T4 before T1 also doesn’t produce a cycle and hence this is also a correct schedule. So it depends on how exactly the transaction graph can be allowed grow by these protocols. 

Essentially the concept is a graph is constructed and a cycle is prevented from happening in the graph. So I think we understood now the basics concept of how exactly the serializable schedules is used by the transaction system. Now what I am going to show in the next few minutes is to look at other kinds of consistency requirements. An interesting thing to understand as far as consistency is concerned is to look at basically T1 executing before T2 is a correct thing to happen. But this need not be the case for example if you look at debit and a credit transaction it is possible that any number of debits and credits can be interleaved as long as the debit occurs as one unit and credit occurs as one unit. 

Now this is very interesting because we can start looking at what is a operational semantics and try looking at whether the way the transaction execute is consistent or not. To give a more deeper treatment of this we will take a simple example and then see what exactly we mean by this understanding the semantics and seeing whether the execution is right or wrong as far as the database transaction execution is concerned.

For example imagine I am actually having account and I do a debit on my transaction which actually means that this is basically withdrawal and I actually add some numbers after that means I credit into my account which is equivalent to saying that I read the value of x here and then I basically add some number here. As long as the read of x is consistent with respect to the write, it does produce consistent results. 

What this means is there is basically a write that is happening on x before the read is happening on x. Now this write can be by any transaction, let us say this is by the transaction i and this is by another transaction j. This is the relationship between the two transaction in terms of, I am actually reading the value let us say the j is reading the value produced by i that means T i has actually produced the value of x which is being read by T j.
 
[Refer Slide Time: 45.38]

 

Now as long as this relationship is maintained between transactions in terms of how exactly they read the values of the previous transactions and this is actually maintained between the two schedules, we say that two schedules are equivalent in terms of views. This is called the as appose to actual conflict serializability, this is called the view equivalence of schedules. 

Now what this means is two schedules S a and S b are view equivalent as appose to conflict operations equivalent schedules, they are view equivalent schedules if the way actually read operations and the write operations are related is they actually read between the two schedules, the operations are actually the same in terms of the way it has been produced and read. Now if this order is changed between the two schedules then it is basically not, they are not view equivalent. 

The final writes between the two schedules also have to be, this is first requirement. The second requirement is the final write operations are same between the two schedules. The same in both schedules these are the two conditions that need to be satisfied for two schedules to be view equivalent.
[Refer Slide Time: 47.06]

 

It is possible that view equivalence can also be seen as producing consistent results. For example if you look at the typical case of what we considered as debit and credit transactions you know occurring simultaneously. It is possible to see that view equivalence is will produce correct results as appose to the conflict serializability. 

Now, this is interesting because we will start realizing that it is possible to enforce correctness by understanding what is happening with the transaction semantics. For example; it is possible to look at semantics of operations, finally we can look at semantics of an operation and then see whether a particular execution of this operation can be correct.  

What I am going to do is I will take a very simple example to show how semantics can be applied for understanding the consistency criterion. It is possible to say that I have I will take as slightly different example here to show what is semantics of an operation. We can take a simple queue as shown in this particular figure. Now, the queue will have what we say as a front pointer and a rear pointer and it will basically have two methods which can be executed which is basically, an add and a delete.

Now, if you can carefully look at how exactly the queue can be left in a consistent condition when adds and deletes are happening simultaneously. Now, you can see that basically an add will will happen at the rear end and a delete will happen at the front end. 

Now, adds and deletes can suddenly be concurrent assuming that the queue is not full, the queue is not empty; under those conditions, adds and deletes can occur concurrently because add is actually trying to manipulate the rear  pointer, delete is actually manipulating the front pointer.


[Refer Slide Time: 49.28]

 

This is very important to look at little more deeper. For example; let us say, there is T1 here and you are actually saying Q dot add. Now, there is a T2 which is actually saying, Q dot delete. Now, we know that from the semantics of add and delete, T1 and T2 can happen concurrently and still produce correct results. 

This is basically semantics, knowing the semantics of the operations; I am able to say that these two produce consistent results. Now, if you basically further say that two adds can also happen simultaneously and I have a mechanism for producing, know two adds working simultaneously, it is possible because all that you need is lock the rear pointer and if you allow the rear pointer to be obtained by each add separately, then you need to lock only the rear pointer and ensure that this two adds at the add level can be concurrent. But at the rear level, they will be blocking each other. 

That means the access to rear will be may consistent; but at the add level, they can be still working parallely or concurrently.
 












[Refer Slide Time: 50.54]

 

And, this is basically understanding the operation of the transaction and applying what we call as semantic consistency. Since you know what is semantic actually means that, you know the meaning of the operation and apply the meaning of the operation to decide whether something is consistent or not. And, that is very interesting because it is possible to apply a much greater level of consistency criterions by understanding the meaning of the operations. 

To just recap what we have done in this particular class and then give you some indicators of how exactly the to go on further reading in this particular subject, I typically covered the idea of what is basically a schedule in this particular class and what I have also done in this particular case is I have actually produced equivalence schedules and this equivalence schedules are from different aspects.
 
Two schedules are shown to be equivalent from a conflict operation point of view by saying that if the conflicting operations are executed between the two schedules in a particular way, the same order is maintained between the schedules; we call that as conflict equivalence. We also showed view equivalence which actually means that the writes produced by one schedule. The writes and the reads, the way they occur on operations are same between the two schedules. We call that as a view equivalent schedule.

Finally, we also showed what is called semantics and based on semantics, how the schedules can be seemed to be equivalent. You can do the commutative operations as long as they are parallel, whatever order they appear still the schedule is right as long as you commute the commutative operations are performed in any order is still will be producing consistent schedules and so we actually shown How exactly we look at equivalence. 

[Refer Slide Time: 53.15]

 

And, what we have further shown in this particular class is typically, how simple case of conflict serializability can be achieved by constructing a transaction graph. A transaction graph is constructed by producing before and after relationships on the various transactions and that is how actually the conflict serializability is achieved by constructing the transaction graph.

Finally, we have actually shown, how exactly the protocols, various protocols will be used, will be designed to produce the serializable schedule. The criterion for this is will be designed to produce conflict serializability. What I am going to do is in the next class, I am going to discuss a series of protocols which actually produce conflict serializability.
[Refer Slide Time: 54.43]

 
We are going to look at a set of protocols. We start with a most popular protocol of two phase locking and show how two phase locking will produce conflict serializability and also go on to show other kinds of protocols that exploit the property of the constructing the transaction graph without any cycle. That is the essential property is here, there should not any cycle as far as the transaction graph is concerned and the protocols exploit this property of trying to construct the cycle and we essentially can divide the protocols as being optimistic or pessimistic on how actually they construct the transaction graph.

We are going to take this in the next class of looking at the protocols and seeing how different protocols can be constructed for producing serializable schedules. As a thing of further reading on this, you can typically look at there is a book by Burnstien on actually concurrency control in databases. This is an excellent Burnstien and others. Basically a   book on concurrency control and you can have a look at this the book as a further reference. 

I have also used basic the foundation thing was used by the book on Fundamentals of Database Systems by Elmasri and Navathe, Elmasri and Sham Navathe.
 
[Refer Slide Time: 56.39]

 

I have used the chapter from this book while doing this particular; foundations on concurrency control. What, I am going to also do as part of next lecture is while while doing the protocols at the end of the next lecture, I am going to introduce a few problems and try solving them at the end of the next lecture. We will stop here for this lecture. 


Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 20

Concurrency Control – Part -1

Foundations for concurrency control in particular what we did was we looked at the problem of serializability when transactions are executing concurrently. For example there are a set of transactions and all these transactions are executing concurrently. We were looking at how this can be interpreted as a serial execution of the transactions. For example some order in which they could be seen to be executed is what we looked at a theory for correct execution of the transactions.

[Refer Slide Time: 02.21]

 

We also looked at two kinds of a serializabilty, the conflict serializability where the transactions which are executing on the same data item when they conflict, how they can be reduced to serializable transactions. That is what we meant by conflict serializability. We also looked at other forms of serializability like view serializability. What we are going to in this todays class is we are going to look at specific concurrency control protocols. And to begin with what we are going to do is we are going to look at what are the different types of algorithms that will allow concurrent execution of transactions to be serializable that means which produce correct execution of the transactions in a database.    

So what we are going to do now is we are going to look at what are the algorithms that we use for achieving serializability and these algorithms are called concurrency control algorithms. 
Now there are two broad classes of these concurrency control algorithms based on how they actually view the system. Some view the system in an optimistic fashion and some view the system in a pessimistic fashion. What we mean by optimistic and pessimistic, I just explain with a simple example. Now if you basically look at a narrow pass bridge where vehicles are crossing this bridge and this is relatively a very very low traffic bridge. We don’t really look at, the vehicles will hit each other when they are passing through this narrow bridge which in effect means that there are actually vehicles coming from opposite ends.

So you actually assume that relatively, the conflicts are very low which actually means that a scenario where the conflicts are extremely low is one situation. There could be another situation where the conflicts could be extremely high which means that this is a very high traffic bridge and hence you assume the possibility of two vehicles finding themselves passing through this narrow bridge at the same time is likely to be high. In either case what we have to see is the two scenarios are different system configuration. So what we will do is we will broadly classify conflicts being extremely low and conflicts being extremely high. 

Now if you typically look at the way we would see when the conflict are extremely high is we basically will put some kind of a traffic signal here and we will ensure only one of the vehicles is into the narrow pass bridge. That means we in effect will ensure that only one of them is going to be inside. If we don’t put the traffic lights her, very often what will happen is two vehicles will get into the bridge and realize they are in conflict and one of them has to back track.

For example if we assume that one vehicle V1 has passed up to this point and V2 has passed up to this point and you find there is a conflict here. One of them has to backtrack and this basically involves lot of hard work and hence when the conflicts are very high, it is probably not advisable to actually allow the vehicles to get in without any kind of a control. The situation where the conflicts are low is seem to optimistic that means you actually believing that they conflict are less which is an optimistic approach or you view the system in an optimistic way, the second one where the conflicts are very high, you look at it has pessimistic. So this is broadly classified in the two scenarios optimistic and pessimistic scenarios.











 
[Refer Slide Time: 06.47]

 

Now what will do is we will see algorithms that operate on the database system assuming that the system is an optimistic scenario. And those algorithm which actually operate assuming that they are in the pessimistic scenario. For example if you know chennai for example in the mount road, if you assume optimistically no vehicles are going to hit each other and remove the traffic lights, everything is going to be chaos there. But on the other hand if you know IIT Madras at gagendra circle, you are not going to install a traffic light because we don’t have that much traffic, it is going to be cumbersome. 

So in some sense if the conflicts are rare and if you apply the other algorithm, it is going to be overkill. So typically we have to understand, what is the scenario in which the system is in whether it is in optimistic scenario or in the pessimistic scenario and based on that you should be applying this algorithms. Now broadly what we are going to look is these two classes of algorithms which are actually categorized, two classes of cc algorithms but it is possible that there are more algorithms than these two classes. 

Now, the one class which we are going to look at is the lock based algorithms which all assume pessimistic scenario which assume that the transactions are going to conflict with each other often. The other basically a time stamp based algorithms. The time stamp based algorithms assume that the system is in the optimistic fashion and hence conflicts are there. What we are going to look at is one lock based algorithm, the most popular algorithm called the two phase locking algorithm and we are going to look at a basic time stamping algorithm for the time stamp basic time for the time stamp based algorithms, we look at the basic time stamp based algorithm.



 

[Refer Slide Time: 09.16]

 

Both these classes will tell us that one belongs to the pessimistic class and other belongs to the optimistic class. Now what I am going to do in the rest of the lecture is look at the lock based algorithm called the two phase algorithm in detail and explain what are the properties of this two phase algorithm and we will study this algorithm in detail and this is the most popular algorithm implemented in practice as well. So we are going to spend some time looking at two phase locking algorithm used for concurrency control in database systems.

As the name suggests, this actually is a locking based algorithm. The lock is very important here because what we are trying to do here is when transactions conflict on data items; we are in essential going to allow these transactions to lock the data items. This is equivalent to saying that once the transactions is locked the data item, it is not accessible for other transactions and in effect that protects the data item from being wrongly manipulated by other transactions. So locking is a concept that is used in this particular case. 

Now to explain the concepts of locking in more detail, we have a problem which we are also familiar in operating systems called the mutual exclusion problem. What actually the mutual exclusion problem will try to do is it actually tells that if there is one process P1 which is trying to be in a critical section. The critical section is nothing but as piece of code that is trying to address a share data structure between P1 and P2. 

Let us say there is a shared data structure here between P1 and P2 and whenever P1 is in the critical section, it means that it is trying to manipulate the share data structure here. Now only one of them can be allowed to be in the critical section for manipulating. This is basically the read write operations that they might perform on the shared data structure. 

So what in a sense the mutual exclusion problem does is it allows only one of the processes to be in the critical section. If one process is in the critical section, it excludes the other process from being in the critical section. This is what we understand in operating systems as a mutual exclusion problem.

Now this is achieved in the operating system by using two operators called the P and the V operators. P is essentially a lock operator. So when you actually apply a p operation, the process actually uses the P to lock the critical section and uses v to unlock the critical section. This is like I know two people cannot be in the room simultaneously then what the use is they use a lock and once somebody acquires this lock, gets into the room unless he actually unlocks it and comes out of it, the other person cannot enter into the room. That is what actually prevented by using this P and V operators in the operating system context.
 
[Refer Slide Time: 13.00]

 

Now in the case of databases, it is slightly different than being just mutually exclusion problem because what we are actually doing in the database context is we are actually assuming that there are different types of locks, there could be a read lock. Whereas in the case of a mutual exclusion problem, we have only one kind a of lock but here there are different types of locks that we introduce. Read lock actually says that transaction is trying to read the data item, is trying to read. This is very important that means it is not going to right on the data item is only going to read the data item. 

And if you say there is a write lock, the write lock means that it is trying to write, the transaction wants to write onto the data item. Transaction is trying to write on the data item. It is also you can say read lock, I will abbreviate here afterwards read lock with an RL. An RL shows that the transaction only wants to read the data item and this is also called a shared lock because it’s possible that more number of transactions can start reading the item at the same time.
It is shared, that means there won’t be any violation if more than one transaction tries to read the same data item. But at any given point of time, only one transaction can write on the data item. So when we abbreviate write lock as WL, this also is called exclusive lock that means this lock will be granted only to one transaction whereas the shared lock can be given to multiple transactions at the same time.
 
[Refer Slide Time: 15.05]

 

By actually distinguishing the kind of locks, we will in sense that try to increase the concurrency because whenever a transaction acquires a read lock, it is possible for other transactions also to acquire this lock but whereas if one transaction is given the write lock, the other transaction cannot be given an extra write lock on the same data item unless the work was finished by the first transaction.

So in other words as opposed to looking at the operating system context where we have only one lock, we actually distinguish the semantics of the lock here whether it is read lock or whether it is write lock, based on the database systems tries to optimize the concurrency that is possible when transactions are simultaneously executing, to also understand the granularity of the lock. This is also important issue when you actually look at locking algorithms. The granularity actually means that how the size of the data items that are locked by the database by the transaction. 
 
Now it is possible that the transaction only locks a simple data item which actually means that in a database table one single data item, for example in student in the case of a student record, it is possible that we are locking only a we are locking only a student name or we are locking only the students cgpa. We are not locking any of the other information of the student record. The other one could be the entire tuple, tuple means all the things relating to the students, one particular student getting locked that is called a tuple. 

This is equivalent to also a row in a database. The entire row is being locked. The other one is the table which actually means that all student records are getting locked. That means that the entire table is getting locked, this is very important to understand the granularity. As you reduce granularity, the transactions only locks that small item but if you start increasing the granularity it starts locking a large number of items. So consequently as you increase a granularity of the lock, you correspondingly reduce the concurrency that is available in the system for transactions to execute.  

One simple example is for example if you provide for the entire IIT, access at the in gate where there is there is going to be a lock. There is going to be only one person entering into the entire into the campus at any given point of time and after he goes out the lock is given to the next person, it drastically reduces the number of people who can get into the campus at any given point of time. But on the other hand if the access is controlled at a room level where when they enter into the IIT, when they reach the particular room you want only one person to be entering into the room at aty given point of time and that is the point where the room is locked. 

A particular room is locked then the amount of concurrency that is available in the system is extremely high but it depends on where the concurrency is to be provided but we suddenly should lock as small as a granularity item as possible, keeping the consistent criteria into account.

[Refer Slide Time: 19.01]

 

Now given the understanding of the lock, let us go on to see how a simple two phase locking protocol will be working in the database context. As the term two phase locking explains, there are two phases in these particular algorithm. The first phase is what we call as a growing phase of the transaction in which the transaction tries to acquire all the locks and there is a second phase which is called a shrinking phase where the transaction tries to release the locks.
So if you basically look at the execution in terms of time, let us say this is the time access. Now the transaction starts executing at some point T1 and what it actually does is as it starts executing, let us say it starts to actually reach execution points where it needs some data items. For example let us take a simple transaction where it is accessing bank card database as shown in the earlier examples.

I will basically try to read the balance in a bank account. Now the first thing that will happen in this case is I try to acquire a read lock on the balance data item which could mean that I might actually lock the entire account of a particular person and that shows that I in effect reached a point where I have locked, I have asked for a lock. Now it is up to the database manager to see that if nobody else is actually has a lock on this at this point of time or if the other transactions own only the read blocks on this data item, I can be granted the lock request.

Now as I proceed like this, I might keep asking for a locks on other items as well. So this is basically the growing phase where the transaction is actually asking for locks. This is the first phase of the transaction execution which is called the growing phase. The growing phase, the transaction is trying to get all the locks it needs to do the work that it has to do. For example imagine that you have to move some items form one room to another item, another room then you first step before moving the items from one room to the other room or manipulating the items in a particular room, you try to acquire locks on all the rooms that is the first phase of the growing which is called the growing phase.

After you have got all the locks, you do the operation that you have supposed to do and after that you basically start releasing the locks. This is basically called the shrinking phase. In the shrinking phase, the transaction is actually releasing the locks. This is lock acquisition and this is lock release. In this side it is basically releasing the locks. Now the important condition of two phase locking is once a transaction is released one lock, it cannot ask for any more locks. This is a very important condition and a subtle condition that enforces serializability.
 
We are going to examine this little more deeply little later but right now what we understand is the condition between these two phases says once the transaction has reached this point which is called the lock point. This is called the lock point. When the lock point has been reached let us say at time T, the transaction is not going to ask for any more locks and it only releases the locks. This we will put it, so the first phase is the growing phase and the second phase is the shrinking phase. Now in terms of the two phases, the condition is you are not going to this lock point, you are not going to ask for any more locks after you have reached actually the lock point. 

This is a very important condition that is enforced. Now let us examine how exactly the two phase protocol works for a transaction and understand deeply what are the consequences of applying a two phase locking protocol.



[Refer Slide Time: 23.58]

 

Now I will take a simple transaction and show what exactly would have happened, if I had actually applied a two phase locking algorithm. I will take a simple case of a transaction 1 which is actually reading an item Rx and actually writing an item data item x and then again it is basically trying to acquire or read an item y and then write an item y. There is basically another transaction T2 which is also doing exactly the same work of reading a data item x, writing a data item x, reading a data item y and writing a data item y. 

Now if you basically look at how two phase locking protocol would have worked in this particular case, when both these transactions at some point of time come into the system. Now let us assume that actually T2 has actually come into the system at some point of time, the first thing it will start doing is as its starts, it requests for a read lock on x, read lock on x. 

Now when the read lock on x is granted, it is going to ask for an up gradation of this read lock to a write lock. Now it still cannot release this lock on x, though it knows that at this point of time it is actually finished working with x. Now it is going to work with y but still it cannot release the lock on x because if it has released the lock on x, it cannot ask for the lock on y. This is the condition for two phase locking. If you have released one lock, you cannot ask for any more locks. So the transaction T2 will hold the lock x and ask for now read lock on y and it will upgrade it to the write lock on y and it this point of time, it will release all the locks.
 





[Refer Slide Time: 26.05]

 

Now only when it releases the locks on x and y, can the transaction T1 start executing which actually means that the locking will ensure if T2 has acquired a lock on x, it effectively prevents T1 from acquiring the same lock on x and hence when there are actually two conflicting transactions on a data item, they will be serialized based on who has acquired the lock first. This is how exactly two phase locking protocol will work. Now the meaning of acquiring a lock is it subsequently prevents any other transactions from getting the lock on the data item as long as this transaction is using it.

This is typically what is achieved as part of the execution, what you are achieving here is what is called conflict serializability. Here in fact making sure that the transactions execute when they are conflicting on data items in a serializable order. I am going to show a small proof to show that a two phase locking in effect produces serializable order of transactions. Before I do that, I am going to look at some more properties of two phase locking in terms what it can be doing. Since the first thing that is going to happen here is since transaction has to wait, if the transaction has been locked by some other transaction. If a data item is locked by some other transaction, it is possible that transactions could be waiting for each other. 

Now this could result in what we call as the problem of dead locks. This is one of the problems of two phase locking. What we mean by dead lock is let us say T1 has actually acquired a lock on, T1 has acquired a lock on data item x and T2 has actually acquired now a lock on data item y. It is possible that now requesting a lock on y and this is requesting a lock on x. Both cannot proceed any further because they have reached this point but they are not going to release this x or y till they reach the point of lock point where this is the lock point. So both will, they are at this point right now and there is no way both these transactions can reach their point but unless they reach the lock point there is no way they are going to release these locks. 

So it is possible in which case T1 is actually waiting for T2 to release a lock and T2 is actually waiting for T1 to release a lock and both will keep on waiting for each other because they are in a continuous loop here. This in effect means that there is a dead lock because neither of them will be able to proceed any further and they will be waiting for each other in this particular context. This is what we mean by dead lock. This is one of the problems of using a pessimistic kind of an algorithm because the algorithms which make the which make the system wait for each other, will transactions to wait for each other can result in this dead locks. 

The dead locks will basically bring bring the system performance and the throughput of the system drastically which means that the system time, the system throughput, the number of transaction that are executed by the system can drastically get affected when there is a dead lock condition. Problems of deadlocks are they need to be detected when they happen and the system has to recover back from this deadlock. In this particular case either of this transactions have to be aborted when a deadlock occurs and make sure they release their locks so that the other transaction can proceed. For example to break the dead lock, one of the transactions has to be aborted to make sure that T1 can proceed. So deadlocks needs detection and subsequently resolving this deadlock require that you abort one of the transactions that is involved in the deadlocks so that the system can proceed further.
 
[Refer Slide Time: 30.06]

 

This is one of the consequences of two phase locking algorithm. Two phase locking algorithm also doesn’t produce optimal schedules and we are going to look at the problem of what is an optimal schedule at a later point of time because the reason for this is it basically ensures, two phase locking ensures that a transaction always proceeds after it gets acquires locks to the finish. 

It never grants a lock and later actually aborts the transaction at a later point of time because there is a conflict. Whereas in the case of optimistic algorithms it is the other way. They let the algorithm, they let the transactions proceed to execute up till some point of time and resolve the deadlocks by resolve the conflicts at a later point of time by looking at what they have operated upon and seeing at a later point of time, if the operate on a data items in a conflicting fashion, they will get aborted at a later point of time. 

So in that sense optimal schedules may not be possible in the case of two phase locking. We are also going to see a small example and show how time stamping produces optimal schedules compared to two phase locking algorithm. I explain this concept in the slightly different way by taking what we are actually done in the earlier class of looking at a serialization graph. For example if you look at as the transactions T1 comes into the system, in effect you actually creating this graph which shows how the transactions have executed in the system one before the other. For example in this case let us say there are three transactions and then after that basically one more transaction has been executing. 

Now we are basically, what I am doing here is I am saying that T1 executed before T2, T2 executed before T4. This is required only when there is a conflict. For example they conflict on a data item x, I need to know how exactly they have executed one before the other, otherwise it doesn’t make. In this particular case T2 and T3 doesn’t have any conflict directly and hence there is no need for me to actually put a arc here because they are not operating on common data items. 

Now let us say there is an incoming transaction Ti into the system at this point of time. Now what the two phase locking in effect does is if there is a lock, for example let us say there is a z item on which T3 is actually locked in. Now the only way Ti can come into the system is only after T3, it can never be. For example if it is conflicting on this z data item with respect to any of the existing transactions, the only way you can allow T i to execute is by being after T3. There is no way this transaction which arrived now after T3 has locked to come before. This is prevented which means that the transaction graph, the TG graph here can grow only in the forward direction that is the only way the two phase locking allows the graph to grow.
 













[Refer Slide Time: 34.00]

 

It doesn’t let the graph to grow in any other direction but let us understand this problem of this graph growing in other directions as well. This is very interesting for various reasons because that allows a better schedules optimal schedules to be created when you are actually executing the transactions. Now to explain this, I will take a simple example of two transactions T1 and T2 executing on two data items x and y.

Now it is possible that I actually allow the read and the write things to proceed in a slightly different way and ensure that as the transactions are operating on this, either they read a pre copy or a later version which is modified by T1. Now as we saw earlier there is a read x and write x that is happening on x and there is a read y and write y happening on the... Now let us say T2 exactly does the other way of reading y, writing y and then reading x and then writing x. Now it’s possible that when the transaction is actually modified the value of x, you can allow still this transaction to read a pre value which actually means that it is still has not written the value of its modification on to the database. This is the database storage.

Since it has not yet modified the value on the database storage, it is possible to allow the T2 to read a value before modification which means that there are three possibilities depending on what you allow T2 to do after T1 has actually started executing. If you allow the before value that is pre x to be read then the transaction T2 is actually coming before T1 because it is actually reading a value that is actually modified or that is not modified by T1. If you allow T2 to read a modified value then in affect it is coming after that is this is this is write x, this is after written, T1 has actually written the value on x you allow T to read the value.
 



[Refer Slide Time: 36.30]

 

Now two phase locking will not allow the transactions to read pre x which in effect means that it will prevent this from happening. More advanced algorithms allow the transactions to be placed anywhere in the transaction graph, even they can come before a particular transaction or after the transaction graph. In effect there is algorithm called the 5 color protocol which allows transaction graph to grow in all directions. These are actually 5 different, the 5 colors here denote 5 different kinds of locks. 5 kinds of locks that a transaction can acquire besides just acquiring a read lock, a write lock, the other kinds of lock with a transaction can be granted when it tries to acquire a read or write data items which were before or after. Depending upon the pre and post whether the transactions data items have been modified before or after, it is possible for the transaction to acquire the locks. And this allows the transaction graph to grow in all directions and this produces sort of, it produces more optimal schedules than what we see in the case of two phase locking.
 














[Refer Slide Time: 38.12]

 

The other interesting problem that you see if you actually look at two phase locking is presence of long live transactions which actually means that the transactions are executing for a longer time. For example, the example for a long live transaction is a transaction that is trying to compute annual interest for every account in the bank. What this does is every year ending time, the savings is taken and the rate, interest need to be paid for each account is calculated. Now what this long live transaction, it runs for a very long time. 

Normal transactions run only for a few milliseconds, this runs for a few hours and then it tries to acquire locks on almost all the data items, acquires locks on a large number of data items. Now, what this means is this is property one and this is property two, large number of data items. This in effect introduces lot of problems because this blocks a large number of short transactions, what we actually have is in case short live transactions. Now in the presence of long live transactions, short transactions will have problems of execution because they don’t be able to run. 

Their response time is going to get drastically effected, when there are long live transactions, if you apply a two phase locking algorithm because the long live transaction will in effect lock all the data items and will not release the locks on other data items till it finishes because that is one of the conditions of the two phase locking.
 







[Refer Slide Time: 40.09]

 

So if you apply a two phase locking, for long live transactions which are also there along with short live transactions, the performance of the short live transactions will drastically get effected, they won’t be able to execute. The response time is going to be really bad when you actually apply two phase locking. This is another very important issue when you actually look at two phase locking algorithm. 

There are lot of protocols which modify the two phase locking. There is not strictly two phase locking, they modify the two phase locking condition to actually allow long live transactions to execute along with short live transactions and there is a whole large number of protocols available for making long live transactions execute along with short live transactions. 

We will not go into details of that but then there is i will I am going to give references at the end of it pointing out a papers which actually give this algorithms, a host of this algorithms. Now what I am going to now touch up on is the other aspect of how a concurrency control algorithm has to be integrated with a commit protocol. Now what we understand by commit protocol and concurrency control algorithm has to be made little more clear. Now as you can see here, it is only after the transactions commits. If you remember the way we actually explained earlier, there is the transaction begins execution by actually saying begin transaction. 

When it actually comes to the end of the transaction, you are actually that is the time when you are writing all the values that the transaction is modified back onto the database. It is still that point the values modified by the transaction are not actually written onto the database. That is what we mean by the commit protocol. There is inter relationship between concurrency and commit, in the sense that as the transaction modifies the values, this is this the place where transaction is actually modifying the values, transaction modifies.
And this is the point only after commit point it is actually visible, the modifications are visible only after this point. So there is going to be some kind of an interaction between concurrency protocols and the commit protocols and what we need to understand is how exactly the concurrency control algorithms get integrated with commit protocols because both together can only provide correctness. And if you basically see that the transaction has it modified it is visible then it is going to create difficulties in terms of the other transactions reading the values which are not yet committed by the transaction on the database. 

So both concurrency and commit protocols have to work together. And we are going to explain in the next few minutes, how exactly the commit protocols works along with concurrency control algorithms. What I am going to look at it is how the 2 PL algorithm gets integrated with a commit protocol.
 
[Refer Slide Time: 43.42]

 

We are going to look at several ways, this can be done with two phase. I will take this simple graph that we have actually taken earlier to see how the commit protocol gets integrated with this. This is a familiar thing that we have actually, now the T1 is the start of the execution of the transaction. This is basically the end of the transaction and this is the lock point of the transaction. Now what we are actually looking at here is, it is possible once the transaction is reached the commit point, it is possible as it is releasing the locks they are immediately made available for some other transaction.
 
I will take only a simple case of data items x on which the transaction has a write lock. Now let us say it actually finished and releases the lock release write lock on x which actually means that it is possible that this x is available for some other transaction T2 now to actually start working. 

Now when T2 actually is trying to now acquire lock on this same data item x, remember that this transaction has till not reached the commit because the commit point is here. The transaction is actually committing itself here not before this. So it actually means that we are allowing the transactions to be executed transactions to release locks before they actually reached the commit points. 

This is where the interface between the locking protocols, that is the concurrency control protocols and the commit protocols come into picture because the commit protocols start operating at this point whereas the before that we have applied the concurrency control protocol. Now if this protocol releases the lock then the data item is visible for other transactions because they effectively can acquire the lock. But the value that they are going to read is not the value that actually is produced by this transaction because it is still not committed. 

But if it is reading the value produced by, but our understanding is since it is released the lock any transaction T2 acquiring the lock is after and hence there is a relationship between these two that T1 executed before T2 and hence this is to be preserved because the value T2 should read is now the value modified by T1. Now what is going to happen in this case is if the lock has been released by a transaction before it is actually committed, it’s possible at a later point of time at the commit stage, the commit protocol issues an abort which actually means that this transaction T1 effectively has actually aborted.
   
[Refer Slide Time: 46.33]

 

Now this requires that T2 is also actually aborted. This is what we mean by if you if you allow the transactions to release the locks before you basically result in cascading aborts. T1 actually modified a value on x, T2 has actually read this modified value but now T1 actually aborted for various reasons. Now T2 should also abort and this is what really will happen if you apply the transactions to release the locks before they are actually committed. 

Now we are modification to the two phase locking algorithms taking this into the account to avoid cascading aborts will require that the transactions actually start acquiring the locks. This is actually the lock point but actually none of them effectively will release their locks till they reach the commit point which actually means that the key point holding on to the locks till they reach the commit point and all the locks are released only after the commit actually happens, which actually means that the transactions commit that means they write their values whatever values they have actually got, they will write these values back on to the system and then they allow or they release all their logs.

The logs are not released before committing, this basically shows that the transaction effectively starts holding, starts holding the logs till it actually reach reaches the commit point. This ensures that the concurrency control and the commit protocols work correctly by actually integrating the concurrency control protocols with the commit protocols. Now what we are going to look at it is all the protocols will require some kind of a modifications when we req when we look at how they integrate with the concurrency control protocols.

[Refer Slide Time: 48.25]

 

I will show you typically the three properties of, if you if you remember for a transaction, we are basically looking at three properties of atomicity which actually ensures that all or none of the actions of the transactions are written. Then we are basically looking at concurrency control. Typically this is consistency when they basically operating together, they basically produce consistent results then we are actually looking at the property of isolation. 
 
Isolation means that one transaction results are not visible for other transaction till the transaction has committed then we are talking about durability. This durability is the transaction values are permanently written on the database. So this together what constitutes what we actually called as the acid properties of the transactions. 

Now they together have to hold for every transaction. What we are looking at here is basically the concurrency control aspects. Now they have to get integrated with the isolation properties, this is where the commit protocols are coming into to picture. This is where the integration has to take place between concurrency control protocols and the commit protocols. When we discuss the time stamping algorithms also which operate in a slightly different way of actually ensuring that properties of serializability are actually enforced at the end of the transaction execution not before. 

We have to see how that actually integrates with the commits protocols. It is going to be interesting to see how time stamping protocols ensure commit protocols integrate together in a proper way. When we discuss the time stamping algorithms, we are going to look at how commit and concurrency control protocols integrate with each other in that particular context. Typically the atomicity properties and durability properties are achieved by what we say as recoverability properties which are actually ensured using the logs. Typically the logs are maintained to make sure at any point of time, the transaction can redo or undo its actions and that is achieved using the logs.
 
[Refer Slide Time: 51.24]

 

Logs plus the concurrency and commit protocols together ensure that the transaction acid properties are realized. And to just give you complete picture, what we are basically looking at the lowest tier is the database items. These are nothing but the tables that are stored in the database. Now at the other end, this basically the applications which are trying to modify the database items. 

Now all the properties algorithms that we are talking about now come in the tier which is which sits between the applications and the database tables and this is what we mean by the database management system. Now in this case the dbms has various other things, this has to ensure among other things. A part of the subsystem has to deal with the transactions and that is basically what we mean by the transaction manager. Now the transaction manager is part of the dbms and this transaction manager is the one which actually ensures that the as the applications are executing, they serializablity condition is actually enforced using the transaction manager. Now to give an idea of what exactly happens when a transaction T1 starts executing, when it is actually putting a lock request, let us say it is actually requesting a read lock, this is actually given to the transaction manager.
 
[Refer Slide Time: 52.44]

 

It is up to the transaction manager now to grant this request or disallow this request at this point of time. So, all the transactions in effect will make the request to the transaction manager. The transaction manager when it locks the data items for example there is a table here the student record table, now typically it allows a particular tuple to be locked then this lock will be granted based on the request that is basically requested by the transactions.










[Refer Slide Time: 53.43]

 

So all the lock requests are coordinated by the transaction manager and it knows which transaction holds what locks at the given point of time and ensures that the lock request are properly coordinated among the various transactions that are executing. It also ensures that along with the, for example if there is a commit protocol that needs to be operated before the lock is released, the transaction manager ensures that the transaction locks are not released till the commit point of the transaction is reached. 

This is how exactly the concurrency control protocols work in the case of two phase locking. What we are going to do in the next few minutes is sum up and lead to the next set of algorithms which are typically time stamp based algorithms. Now what I am going to show you in this particular case is a simple execution of a transaction and show how two phase locking may not be a best way of executing the transactions and how one can think of in effect producing more optimal way of executing the transactions. This is the very simple example, I will with this example I will lead lead to the next set of protocols that I will be talking in the next lecture which are called the time stamp based protocols. 

In effect looking at the execution of two transaction in T1 and T2, if you typically look at a read x, a write x and a read y and a write y by transaction x and T1 and T2 let us say just repeats the same kind of execution. You can in effect see that, it’s possible for T1 and T2 to execute in different possible directions. Now one way the two phase locking ensures that T1 executes after T2 is both locks of T1. 

Let us say the write lock on x and write lock on y will be granted for T1 which in affect prevents T2 from start executing till T1 has actually finished which means that the schedule that will be possible in this particular case is read x write x of one, read y write y of the other this is the point where the transaction would have committed T1 and all the execution of T2 proceeds after this point.
 
[Refer Slide Time: 56.31]

 

But it is possible for, if you carefully notice what is possible here is this is not the best possible execution of the transactions. It is possible for you to say once T1 has actually finished working on data item one, data item x, it is possible at the point of time for T2 to start executing on the data item x because T1 no longer needs that data item on x but the whole set of problems will come if you let this happen. 

First thing as we discussed, we will be sacrificing on the isolation property because you are letting transaction T2 read the values before T1 has actually committed. So this will sacrifice isolation property of the transitions. So how exactly if you want actually optimize the execution of the transactions, probably two phase locking is not the best possible way of executing but two phase locking is by far the best way of or a simple way of executing the transactions. I think I will stop here and going to come back in the next lecture. 

Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 21

Concurrency Control – Part -2
          
We are continuing discussion on execution of transactions in database systems. We in fact looked at various things like how the execution of the transactions to be controlled in the database systems to produce consistency results. In fact transaction execution is one of the most important things in database system because it affects the performance of the system. If you typically notice during the results time when cbse results or state board results are announced, now lot of people try to access the results through the net. The results is stored more or else on a on a database system. What you will realize is a large number of people simultaneously trying to access these results at the same time that is very important, at the same time because everybody wants to know his result or watch result at the same time as somebody else because everybody is curious to know about the results.
 
Now this is the time when there are simultaneous hits on the database system, the large number of people were trying to access these systems at the same time. Now normally we notice that it is at this point of time the system fails, no that’s the time when it cannot respond to so many requests at the same time which is equivalent to saying that the throughput of the system, the number of simultaneous users it can cater to is going to come down drastically. That’s the time the system will be challenged in terms of how many number of transactions it can process in a given point of time that is per second how many transactions you are able to execute on your system.
 
Another very interesting example is our railway information system. If you have noticed in summer there is a lot of people trying to book the railway tickets online or go to the counters and book their tickets. You can notice that the number of users trying to book the tickets in the last you know couple of days, this being summer seems to be peeking at 30000 per day which is you know, the system should be able to handle that many number of peoples simultaneously coming and try to book their railway reservation simultaneously. So database systems, one of the very critical things in database systems is how many number of transactions the database is able to execute without losing consistency.
 
Now it’s very important that if one user has booked his ticket, somebody else simultaneously accessing the ticket overwrites all the details and the same seat is allotted to two people at the same time, the same birth is given to two people. Then the system is in an inconsistent state. When one user is trying to book and he gets the birth, it should be guaranteed that nobody else is going to get the same birth that’s were basically we were trying to understand how the database actually solves the problem of trying to give as much as throughput as possible and at the same time maintaining the consistency of system state. This is the most important requirement of database systems. 

What we did in the last class was we were looking at two phase locking, a very populated protocol that is employed by database systems to ensure that when simultaneous transactions are executing they log the data items. For example in this case, if you what we are discussing few minutes back on the railway reservation case the birth will be for particular train if it is locked by one transaction, it will not be allowed to be locked by another transaction till this transaction finishes operating on that particular item. That is how exactly the lock coordination is done by the transaction manager to ensure that more transactions can be executed but at the same time consistencies ensured.
 
Unfortunately what we have observed in the two locking phase is beside it is prone to be dead locks because when you locked its possible two different transactions could have locked themselves in a different way and that could have resulted in a deadlock scenario. And it is also possible that since the locks are released only after the commit point, it is possible that more throughput cannot be achieved by using two phase locking that is were basically we see that two phase locking needs lot of you know, those kind of algorithms need change to ensure a better throughput. Two phase locking suddenly doesn’t gives that much throughput and so if you actually want to improve the transaction throughput, possibly you should explore other algorithms as well. 

Now one of the things that we did in the last class was to actually classify the algorithms into optimistic algorithms and pessimistic algorithm and we showed that the optimistic class of algorithms, we basically use what is called the time stamp based algorithms whereas in the pessimistic we have basically the conflicts are more, we said we basically using locking algorithms. This is what we actually did in the last class and we said, we will continue to look optimistic based algorithms in this class. 

(Refer Slide Time: 7.11) 

 
And we are going to look at typically the time based algorithms, they are very popular algorithms in terms of producing a better throughput. I am going to compare both optimistic and pessimistic algorithms in a little more detailed fashion for a few minutes now and then get down to look at time stamp based algorithms in much more detail. But before we go in to time based algorithms, let us understand essentially the difference between pessimistic based algorithms and optimistic based algorithms. 

(Refer Slide Time: 8.28)

 

Now really what happens in the case of pessimistic and optimistic based algorithms case is they are in two different spectrums in terms of what actually they do for checking the consistency of the database transactions. Notice that when actually we admit the transactions into the databases if you admit only those transactions that are going to be consistent. Then you have actually doing the admission control for consistency in the beginning of the transaction execution.















(Refer Slide Time: 8.49)

 

Now it’s possible for you to let the transaction execute till the last point and start doing for the consistency check for the transactions at the end of the execution. So it’s possible for you to actually do the consistency check at the start of execution versus the end of the execution. Now if you basically do the check at the end of execution, you typically waste the execution time of the transactions. For example it’s possible that a transaction is actually executed up to it finish and it is at the point of time, it is told that the execution is not consistent with respect to other transaction execution and that transaction is aborted. Then it is like saying that you did lot of work but some body says at the end of the work, whatever you have done is not consistent, so please come back again and redo whatever you have done.
(Refer Slide Time: 9.11)

 
So that is one way of ensuring that you know the consistencies is applied at the end of the execution. On the other hand everything you do from the beginning, a teacher is by your side and looking over your shoulder and looking at every step what your doing and if you actually took a value that is not actually a correct value, he actually stops you at that point and says, wait till you have the correct value and then you are executing then basically that is what we actually see it as a very conservative way of executing the transactions. 
 
The other way of execution is let people proceed and let them submit to you at the end of the execution, whatever they have done and now check whether what they have done is correct or not when two people are simultaneously working, they assume wrong values tell them that your thing is wrong because I have already accepted somebody who has actually done it before. Now you go back and redo assuming that this is the correct value. So it depends on were exactly this consistency criteria is actually applied. But if you assume lots of times, they are not going to be conflicts. The people are going to operate on different set of data items, it’s possible for you to actually assume that the execution can be allowed to proceed independently and you check for the consistencies at the end of execution that is one approach.

The other approach is to actually look at these two ends of spectrum, seeing that one end of spectrum is what we are marking as the optimistic end of the spectrum and the other end is what we have marked as pessimistic end of the spectrum. Now we have a number of protocols which lie in between these spectrums, this full spectrum of things that we have. What we have actually seen is 2 PL in grade detail and this falls under the pessimistic side of the things.

(Refer Slide Time: 11.45)

 

We have a variety of time stamping protocols; some of them can be seem to fall fully optimistic time stamping, fully optimistic protocols. There are protocols which are time stamp based algorithms which are not completely optimistic, they fall in between the pessimistic and the optimistic protocol. What we are going to do is we are going to look at protocol, called the basic time stamping protocol and then we will see how this protocol is different from the 2 PL locking kind of the algorithms that we have seen earlier.

(Refer Slide Time: 13.05)

 

What we will do when we study the basic time stamping algorithm is to understand how time stamp based protocols work in the concurrency control. Then we will modify this basic time stamping protocol to produce what we see as a fully optimistic time version of the time stamping protocol. Also other protocols more notably the multi version protocol, it’s possible to map it somewhere here the multi version protocols. What they basically do is they produce multi versions of the data item when reads and writes are going on which means that they keep multiple copies of the data item being manipulated in the database and consequently you have what are called the multi version multi version protocols, concurrency control protocols.














(Refer Slide Time: 14.06)

 

One modification of the 2 PL, 2 phase locking protocol for multi version is the other thing that we are going to look at which is the multi version 2 PL is another algorithm that we are going to look at as part of the multi version protocols.

(Refer Slide Time: 14.46)

 

We will look at basic version multi protocol and we are also going to look at two version locking protocol which is called the modified 2 PL for multi version protocols. What this actually shows is there is whole gamet of algorithms as shown in this picture. If you basically look at this spectrum is quite wide in terms of number of protocols that can be put in between the pessimistic and optimistic kind of algorithms. It depends to a large extend to what side the algorithm should be applied, depends to a large extent on the system configuration.

(Refer Slide Time: 15.29)

 

For example if you assume that they are going to be too many conflicts, the data items are going to be having many conflicts for particular data items then it is not worth actually, putting optimistic kind of protocols. On the other hand where there are not likelihood of lot of conflicts and you start mapping it to the optimistic side pessimistic side then you are likely to have the throughput drastically comes down and it’s not worth actually mapping or putting the pessimistic kind of algorithms into your database systems. 

(Refer Slide Time: 16.20)

 
A good example here again is to look at what we see as, typically when a new movie is released, a large number of conflicts you know large number of people try to book for the same movie you know on a particular theatre. And also you going to see that there is a choice for a particular you know set of seats because there are more preferred seats in the theatre compared to other kinds of seats in the theatre. If you have visited the theatre many times you know which is a convenient place to sit and view your movie. In effect mean says a large number of people, when the new movie is released we try to book the tickets and many of them will start asking if you ask the preferences, will start asking for those set of seats that’s basically high conflict data items.
 
If you basically look at the data items, you viewed the data items you can see a large number of people trying to access or trying to modify or manipulate a small set of data items and this is what mean by conflicts being very high for the small percentage of data items. Now this suddenly requires some kind of concurrency control. Now in this particular case if you actually apply a pessimistic algorithm which ensures from the beginning that the transactions operate in a consistent way works well because ultimately a single seat can be booked by only one customer. 

You cannot have multiple people trying to book for the same seat whereas 10 people competed for the 1 ticket and ultimately 9 have to be aborted even if they have all gone ahead and then did whatever manipulations they have to do. But at the end, the database is going to say only one of them is going to get the ticket which means that 9 of them abort after proceeding taking all the information they abort at the end, whereas the pessimistic concurrency control would have actually aborted all the other, would not have allowed the 9 to proceed in the beginning which means that the transaction would have been blocked from actually trying to manipulate the data item once it is actually booked. That’s essentially the difference between optimistic and pessimistic kind of an algorithms. What we are going to look at is look at a little deeper in this sense and understand what exactly is the way, this consistency is enforced by the pessimistic and optimistic algorithms as far as the transactions are concerned.
















(Refer Slide Time: 19.24)

 

Now what we will do is we will actually take a sample graph and start showing how these graph actually has grown, if you typically looked at the two cases of optimistic and pessimistic concurrency control. The idea of actually looking at this graph is to understand in a more deeper way how the consistency checks at different points will really help you in terms of resolving the conflicts. As you see in the diagram there is typically at this point of the time, there are set of transactions which are executing in the time sense that T1 which is actually before T2 and now which is basically before T3 an order in which the transactions are trying to execute one after the other.

(Refer Slide Time: 20.32)

 

If you keep a new transaction an incoming transaction T4, now there are two points which we are talking in the graph. One is the entry point and other is the end point. Now T4, if you allow T4 to execute without really looking at, what it is trying to do will be consistent or in consistent. 

(Refer Slide Time: 21.00)

 

What would have happened is T4 would have proceeded to execute and when it comes at the end of the execution, then you try figuring out whether whatever the transaction is trying to do makes sense or makes consistency, whether it falls under the consistency criterion whether it satisfies the consistency criterion. 

(Refer Slide Time: 21.30)

 
Now all that it means is if this has to be satisfied, if there is lightly to be an arc you know something which shows that the T4 at the end of the thing has to actually, T4 at the end of the thing will have a precedence relationship which are shown here carefully follow the diagram to understand what we actually trying to discuss here.

(Refer Slide Time: 21.51)

 

There is a T4 that entered after I have this graph. Now where do I replace this T4? If I am actually saying that T4 comes after T3 this is fine because this is really doesn’t disturb the order.

(Refer Slide Time: 22.29)

 

Because T1 comes before T2, T2 comes before T3, T3 comes before T4. So there is specific order in which things are happening for you. 

(Refer Slide Time: 22.49)

 

And this is correct order because one after the other the relationship is maintained but on some other conflicting data items if T4 has to come before T1 which means that now I want to actually force the relationship on the graph something like this, you wont be able to insert this relationship in to the sequence of actions that you’re doing and this actually violates the consistency which means that it wont be possible for me to no longer say this because from this the relation that I get is T1 is before T4 by the transitive relationship. 

(Refer Slide Time: 23.05)

 
This is like that I ate breakfast then I ate lunch then I actually ate my evening snacks. Now I can’t say suddenly my dinner comes after evening snacks but actually my breakfast which is the first event that I have actually performed occurs after my dinner. That is what exactly is happening here which is a violative because by this relationship, the breakfast should be coming before the dinner event but whereas I am saying that my dinner event comes before my breakfast event and this violates the consistency criterion because one after the other as you have actually able to see here, one after the other the relation is correct as long as the future relations doesn’t violate whatever order I have been able to come with.

(Refer Slide Time: 24.23)

 

If you really understand what really we are trying to say in this graph is the following. I have a set of transactions and when actually I write this relationship that I have here, all that is being talked about now is if there is a cycle in this graph, it is violative of the consistency criterion because a cycle, you can’t break the cycle one after other. What essentially this is showing is this is before relationship. Now what this is saying that T1 is before T2, T2 is before T3, T3 is before T4. Now suddenly I am saying T4 is before T1 and this actually is what introduces the inconsistency in to the execution of the transactions as one after the other.










(Refer Slide Time: 24.45)

 

Essentially this cycle in the transition graph is to be avoided, if you want to produce consistent execution of the transactions. Now understand what we will try to understand here is how does a pessimistic kind of algorithms will really try to solve this problem versus how optimistic concurrency control algorithms tries to solve this problem. I will try use this same transaction graph to illustrate this point that an incoming transaction, we first map in to the transaction graph by the 2 PL which means that it will never allow a transaction to be executed unless it’s position in the transaction graph is fixed by the algorithm which means that there is no way T4 could have executed in the 2 PL case, once it starts executing this condition would have checked.

If this condition is not possible, what 2 PL does is it essentially makes T4 execution impossible. T4 would have not executed, T4 cannot execute which means that I prevent T4 from execution from the beginning. This is what we call as basically not allowing disallow, disallow the transaction from the beginning and this is what 2 PL would have done. Now what the optimistic concurrency control algorithms will do is T4 is allowed to execute, allow to execute but then it would have failed when it wants to commit. T4 cannot commit because at before commitment we basically check whether what T4 is done consistent or not which is the two spectrum that we are talking. This spectrum is where you don’t allow in the beginning itself, in the other case you allow the T4 to execute and stop it from committing after it has executed, it still cannot commit because its violating the restriction. Now let us go deeper and understand this because this is basis for further discussion when we go on to timestamp algorithm also.






(Refer Slide Time: 26.46) 

 

What we will try to understand is how the 2 PL will disallow the transactions from executing from beginning itself, the consistency check will be done at the beginning itself. And as I do this I will also try to informally prove this 2 PL actually produces serializable schedules. Now let us look at a set of conflicting operations ultimately a transaction transaction boils down to a set of operations performed by their transaction.
 
For example if you typically look a transaction T1, there is a set of operations this transaction performs and that can be O1 to On. To get the correct subscript what I am going to do is I will actually make this order, this operation with the subscript which is the transaction number in this particular case and under that I will typically look at the further subscript which is the operation number. What this means is an operation i of j means that this is the ith transcation and this is the jth operation. Since this is the first transaction that I am talking about. What I am going to do is I will try to make this just the 1 of j and this becomes operation 1 of n. And we will also to complete the notation what I will basically indicate also is the data item on which this operation being performed.

For example you can indicate here this actually accesses or does something on x or y or whatever it is. Now the meaning of this is operation of first transaction, this is the jth operation, this is the manipulation this is the xth data item. This is the operation n of transaction 1 manipulating data item y. Now what we will basically look at it is when this is executing in the context of a 2 PL, we need to acquire a lock on this data item before you actually proceed on x or y because that is how 2 PL actually ensures that you are not operating unless you acquire a lock. Now in this particular case, we are typically looking at two transactions that are executing simultaneously to understand how they could be manipulating. Now let us say these are the sample transactions of 1 and 2, transactions T1 and T2. 

(Refer Slide Time: 28.54)

 

Now we will say that an operation will be conflicting if it is actually operating on the same data item. What does this actually mean? If in a case of banking transactions if I go and withdraw cash from my account and somebody else also simultaneously withdrawing cash from his account, not my account. We both are essentially operating on our own individual accounts. There is no conflict in this particular case because he is operating on his account and I am operating on my account. The minute both of us go to the bank and try to withdraw the amount from the same account then we will be conflicting. 

Again if we both are looking at the balance amount that is available in the account, we still not conflicting because both of us simultaneously can view, what is the current balance in the account. Without really withdrawing if you are just looking at the balance, still you are not producing any inconsistent results. So a simple read, though it is on the same data item it is still not conflicting. If either of us are withdrawing or both of us are withdrawing which means that if one of the operation is a right operation, if both are operating on the same data item in this particular case you can see both of these are actually operating on the same data item x and if one of them is a right operation then only there will be a conflict because if I am withdrawing and he is looking at the current balance that is available in the account. Now it depends on whether I read it after I withdraw it or before, it starts now becoming a conflict operation if one of it is actually right.








(Refer Slide Time: 33.00)

 

So this is what we mean by two operations being conflicting. Now only when operations are conflicting, we need to worry about the order in which these operations have executed. If the operations are not conflicting, for example if two of us are withdrawing the money from two different accounts, it doesn’t really matter because we are just operating on two different accounts but the minute actually we started operating on the same account, we need to know what exactly happen with respect to that account, who has first seen that account, how much has been withdraw by a person x, how was it actually added later by somebody else all these details one after other needs to be there. The after relation is important, otherwise there is going to be inconsistencies in the final result that you see as far the account is concerned. 

So when we actually look at T1 and T2, what we are interested in is if there is conflict between T1 and T2 then only we are interested in finding out the relationship between T1 before T2 or T2 before T1. And this relation essentially boils down to looking at some operation of 1 and some operation of 2 and saying how this two conflicting operations have actually been executed with respect to each other. This is what we actually, at the end of it interested to see T1 before or T1 after.











(Refer Slide Time: 34.32)

 

For example if one of it is a right operation then we say that it’s conflicting as we just explained. Now look at operation O1j and operation 2j of T1 and T2. Since they are conflicting now, if they are not conflicting we can’t write this order because they can execute in any order they actually wish, without really producing inconsistent results.

(Refer Slide Time: 35.30)

 

If they are conflicting, we have to understand which operation executed before the other. Based on that we are going to say that the transaction T1 is coming before T2 or T2 is coming before T1. 

Now what as an end of the series of execution, if you typically look at now I basically will take only the one transaction suffix, I will drop the actual operation suffix, I will say two conflicting operations O1 and O2 belong to T1 and T2 are executed in this particular order which means that there is an order of T1 before and T2. 

(Refer Slide Time: 36.00)

 

Now imagine I have a conflicting O2 versus O3 for the third transactions which means that T2 executed before T2 executed before T3. Now if you look at another, this is what we have actually looked at, O4 this means that T3 is executed before T4. Now look at the discussion that we actually had a while ago on the transaction graph. This is what exactly happened in our transaction graph.  
(Refer Slide Time: 36.32)

 
We are just proceeding one after the other but when we actually came to T4, what is really happening was we are trying to say O4 also has a conflicting operation with O1 but these were executed in this particular way which actually means that this is the order that would have happened and which is what actually produces the inconsistent results. Now if you basically look at 2 PL, should this execution would have happened if I consider 2 PL. Now O1 would have actually locked the data item, let us say this is the conflicting operation is defined here on some data item x that I will indicate it here which means that there is a lock on data item x which was obtained by O1. Now, let say there is a conflicting operation between O2 and O3 on data item y which means that in this particular case, the lock was obtained by O2 before O3. 

Now let us say this is a conflicting operation Z here and on which actually we have got a lock on jet for O3 because if the lock was not obtained this sequence is not possible because the transaction will never execute in the case of 2 PL unless the lock was granted. So T1 would not have been able to execute unless it has obtained a lock on x because between O1 and O2 conflicting, T1 has got the lock before T2 that’s the reason why this relationship is possible, otherwise this relationship is impossible. You can’t have the relation as shown in this particular equation here. Now let us say there is between T4 and T1, let us say there is an item, data item a this is doesn’t fit into regular sequence that’s why I am using different xyz this is a sequence, a is out of the sequence. Now between O4 and O1, there is a conflicting operation being performed on a right. Now O4 is saying that it locked, it has got the lock on a before O1. 
 
Now for a minute think if this is possible in 2 PL. What O1, what transaction T1 would have done is it actually requires a lock on x, a lock on a, right before its start executed. Now what this set of equations I have here says is I have got a lock on x, now O2 would not have got, transaction t would not have got unless I released this lock to it which means that I should have released a lock on x before getting the lock on a because O2 saying that it has actually got a lock on y before O3. 

Similarly O3 is saying I have got a lock on Z before O4 but O4 is saying that now I have got lock on a before O1. If you carefully understand this unless this lock is released by T1, T2 would not have got that lock, unless T2 has got that lock it would not have actually proceeded to get the other lock on y. So actually if you use this, equation we are writing here is inconsistent because 2 PL prevents this by saying if I have a lock I won’t release that lock till I get all the other locks. 










(Refer Slide Time: 37.18)

 

Remember the condition that was imposed by 2 PL which says that unless all the locks are obtained, you are not going to release the previous lock that is what we meant by lock point right. All the locks will be obtained that 2 PL finishes, the transaction finishes the execution then it basically releases the locks. If you release one lock you are not supposed to ask any more locks. With that condition if I have released lock x, I would not have asked for lock on a which means that transaction T2 would not have been able to execute unless I finished all the executions. Since T2 would not have got a lock till I finish, there is no way T4 can say it has actually has come before me and obtained lock a lock on a if this is the way it’s supposed to execute. 

(Refer Slide Time: 41.55)

 
This is intuitively what happens with 2 PL. And that is essentially the reason why this important condition is put in 2 PL saying that if you actually release one lock, you are not excepted to ask for any more locks. If that is not followed, you would have ended up actually having this problem. The cycle in the transaction graph would have happened if I allow this condition that x could be released but still you can ask for a. That is what actually is prevented in 2 PL saying that once you have a lock on x, you have to ask for lock a before you release any of the locks that you have required earlier because if you release one lock you cannot ask for any more locks. This essentially prevents the cycle in the transaction graph. This explanation makes you understand how 2 PL checks for the consistency at the entrance, when the transaction is start to executing how 2 PL ensures the consistency requirement. 

(Refer Slide Time: 42.42)

 

Once the transaction starts executing and its start getting the locks, there is no way you can say the transaction is inconsistent. It will never get in to an inconsistent state of execution. What really happens in this particular context is the transaction can get blocked, when its get blocked this results in a scenario of a transaction either permanently waiting if there is a deadlock kind of a scenario or waiting for sufficiently long to acquire a lock but the transaction will never start execution unless it is in an inconsistent state.
 
There are certain issues which relate to what is the overhead of this kind of the algorithm which we will discuss towards the end time stamping based algorithms. The other issue is when you actually move to the other end of the execution and model check for the consistency, how exactly that is going to be done. Now we will look at typically the scenario there and see what are the possibilities of that being done and how that will be that will be done in case of optimistic scenario.  
 
Now what really happens in the case of and optimistic algorithm is we basically take a transaction data items on which it is basically operated. This set of data items which are actually manipulated, in this particular case it is going to be a set, so this set going to be x and a. in the earlier case this is the set which transaction T1 has actually manipulated. T2 manipulated a set of data items relating to y and probably some other set. If you take T4, this is the set which we get here a and probably some other set. Now when T4 wants to commit, T4 can just execute whatever it wants to do, T4 executes.
 
Now it says I want to commit that is basically when it is trying to write the values of a back on the database. This is at the commit point, what your going to do is it check for the values of a and say whether whatever previous things done by the other transactions are consistent with respect to this. If this is basically what we say as the validation check. the validation check is done at the n and if T4 passes the validation then I actually allow T4 to commit. What happens in the case of a purely optimistic kind of scenario? Every transaction is allowed to proceed, what really happens in the case is the transactions takes values, does it in a local copy of the data item and manipulates whatever it needs to do on the local copy and when it actually finally wants to write you perform this validation at the end of it. I will give only an intuitive explanation at this stage to allow you to understand what’s happening here but more detailed discussion is going to follow when we actually take up the time stamping algorithms which are the optimistic case of the time stamping algorithm. 

(Refer Slide Time: 44.58)

 

We will go in depth, see how exactly this works. To give to an intuitive explanation of the optimistic scenario, what is done in this case is let us say 4 people are trying to do simultaneously the something. Each one will be given a local copy. This is like 4 people, 4 people in this case as you know you can see the transaction T1, T2, T3 and T4. Now all the 4 transactions here will be allowed to proceed by taking a local copy from the database. Now 2 PL would have said I am going to order all of you and then you have to get in with the with respect to the tickets I have given you. T1 is number 1, T2 is number 2, T3 is number 3 and T4 is number 4. So that’s the way they are going to execute.
(Refer Slide Time: 47.38)

 

Now I basically will not bother to give them a ticket when they actually arrived my place. What I will say is you actually try doing whatever you want to do that means each one will get a local copy of the data item. In this particular case as you can see T3 T1 and T4 will get a copy of a. In this case it’s going to be a and x, this is going to be y, this is going to be z and other values. Now when you actually finish doing something, for example let us say all the values are produced by the each one of them, at end of it submit to me. When they finish execution, they submit whatever they have done to me.
 
Now I will look at what is called validate. This validate phase is going to look at, if this problem of a being done in an inconsistent way by T1 and T4 will be detected at this stage. For example this will show me that there is something which T1 done did on a which is inconsistent with T4. How can that happen? For example what we will say here is T1 has actually manipulated the value of a with some particular let us say 1 is the manipulated value.
 
T1 actually manipulate the value of x to 2 which actually has been used by T2 and T2 has actually manipulated the value and produced the value 3 which is actually used by… And now T3 manipulated this value to 4 which is used by… Now this is actually manipulated this is to 3 and it’s actually to be used by T1. This is where exactly this problem of, I have actually should not have used this and manipulated because I am coming after but with respect to a, T1 actually has taken T4 value and manipulated it to 1. Please understand this little more carefully. I actually did something and produced a value for x equals to 2 and that value is used by T2. This is like I did something in my paper and passed it on to the next person T2. T2 use that value and produced a value for y and this y value is passed to T3 and we used that value to produce a value for Z and that is given to T4 but T4 actually given a slip to T1 before this for a which actually means that there is a cycle here. Who is now took whose result, it is difficult to say because T4 seems to be a1, a1 seems to be affects to t2 like this, if you basically look at it.
(Refer Slide Time: 48.24)

 

This is what actually produces the inconsistency and this will be what will be detected when they submit their values to me. In this case of 2 PL they will not be allowed to proceed to execute and produce this values but in the case of optimistic algorithms, they will be allowed to proceed, they will be allowed to do this manipulation but when they give the values to me, when I look at them I can see that this is violating the property of consistency which is essentially the cycle in this particular graph. 

What the validate phase does in the case of optimistic algorithms is essentially check this dependency at the end of execution and say that now T4 should be aborted, T4 has actually produced a value so, I basically abort all those transactions which has actually produced inconsistent results. And I will say that T1 and T4 should restart the execution and again they will take the new values from the database and start executing. This is what will happen in the optimistic scenario. Now the problem here is as you can see in the case of a pure pessimistic kind of an algorithm, T4 would not have wasted its time computing something because I have used some value and I tried computing so I actually sat down and worked out everything but at the end of it I submitted my value and I was told you are inconsistent because you used inputs which are not correct. So you go back and redo what you have done.

So this is basically a loss of execution time. You have actually unnecessarily executed T4 and found out that at the end of the day T4 didn’t use the correct value, so it has to abort and it has to restart where this would never have happened at all in the case of 2 PL pessimistic kind of algorithm because they would have been asked to read all these values in the correct way before they start executing the transaction.




(Refer Slide Time: 50.55) 

 

So in that sense the pure pessimistic kind of algorithm would not have allowed this scenario where the aborts are required. Whereas in the case of an optimistic let things proceed but then at the end of the thing you check whether what you have done is correct or not, both have as you can see here both have the plus and the minus. If you except T1 T2 T3 T4 in this particular case, this diagram you can see they generally operate. For example the scenario what we actually assume here is that a large number of transactions generally operate on different data items not on the same data items. That means this is going to be x, this is going to be y, this is going to be z and this is going to be p and there is completely disjoint sets. 

(Refer Slide Time: 54.11)

 
Then you can see that at the end of it they can all go very simply you know without really locking anything because the locking overhead is reduced here. At the end of validation phase they all proceed very smoothly because they are not actually conflicting. This is one case where all of them are using different doors, different keys so all of them pass the validation phase. This is what you will see if you apply a optimistic scenario.

(Refer Slide Time: 55.00)

 

Try to apply a purely pessimistic scenario in this particular case to see what would have happened. Let us say most of the time, all of these transactions operate only on one data item which actually means that it is possible for only one of them to be able to proceed if they actually conflictive which means that all of them will tend to do the same thing data item but only one of them will be able to go, whereas in the other case all of them will be able to go which in effect means that you have to go back and then start reworking out for T2 T3 T4. There is going to be larger number of aborts. If you apply, where there are large number of conflicts, if you apply for pessimistic algorithm there it is going to be a large number of aborts of the transaction and this results in wasted time. In this particular case you can see only one of them have the chance of proceeding because all of them are conflicting. Now if you actually applied a pessimistic kind of scenario, you know that one of them can succeed in getting a lock which means that you would have serialized them one after the other with respect to this conflicting data item and they all coming here and trying to rush through this door would not have happened.








(Refer Slide Time: 55.17)

 

They will get the key one after the other. This is the case of actually, they are actually trying to hit each other. Now if you basically apply the locking, there is going to be an organized lock on this data item x which is something like this lock is going to be, this is the key for the for the data item, this key is going to be used, passed to one after the other and you can see they all can use this data item one after the other and there is going to be not going to be a conflict after they start executing. This is the other case of applying the pessimistic scenario. 

(Refer Slide Time: 56.20)

 

This lecture what we did is we essentially looked in depth, the difference between optimistic kind of algorithm and pessimistic kind of algorithm. Both have their place in terms of in terms of applications where it should be, were there are contentions are very low, there is no point to apply pessimistic algorithms where there are large number of conflicts there is no point to apply an optimistic algorithm. So, both actually have their place. In fact what we are going to see in the next few lectures is whole gamete of algorithms that fall in-between which also produce excellent results when they are applied to transaction processing system. In the next lecture we are going to take basic time stamping scheme and look at in detail. 
Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. 22

Concurrency Control – Part -3

We will continue our discussion on Concurrency Control algorithms in this class. Last few lectures just to recap what we have doing. We looked at a class of algorithms try classify them into two broad classes: pessimistic algorithm and optimistic algorithm. In the last class in particular, we have seen in depth where exactly this difference between optimistic and pessimistic kind of algorithm come. Class before we have seen how 2 PL works in the context of pessimistic algorithm and also we mention that. This class we are going to looked at one optimistic concurrency control algorithm before we actually look at we also going to look at what we understand as time stamp based algorithms as supposed to locking we will look at time stamp based concurrency algorithms. 

Now at all time stamp based algorithms are optimistic algorithms that you have to understand time stamping algorithms essentially used the notion of time for serialiazation that is what they do. Time stamp algorithms need not necessarily be optimistic algorithms but one can formulate optimistic algorithm and really what we are meaning optimistic algorithms? We were saying that the validity of the operations done by the transactions is checked at the end of the execution of transaction that’s what we actually mean by the optimistic algorithm. Now it is not necessary that all time stamp based algorithm check for the consistency of the results consistency of operation and by the transaction at the end of the execution. 

They may do something in between they do not need to necessarily do it only at the end of the execution of the transaction it is possible to do it. So what we will do in this class is we will try to understand in a simple way, which we call as basic time stamp algorithm try to understand how time stamp based algorithms work in the context of concurrency control. What we are going to do is? Look at know how you apply time stamp based algorithms how the recovery properties commit properties and recovery properties are integrated in to the time stamp based concurrency algorithm.

If you remember when earlier lecture on two phase locking we discussed how locking algorithm are integrated with a commit protocol. So we are going to do the same exercise of looking at time stamp based concurrency algorithm and also look at how time stamp based algorithms will integrate with commit based algorithms? So that’s what we are going to do in lecture. Now we will proceed to look at basic time stamp based algorithm. So what we will do is we will explain the time stamp based algorithms.




(Refer Slide Time: 5:10)

 

By taking a simple case of a basic graph of basic algorithm which is called the basic time stamp based algorithm, concurrency control algorithm. 

(Refer Slide Time: 5:40)

 

The algorithm has several basic things that’s done like any other concurrency control algorithm. The first thing the done is typically in this particular case every transaction is given a timestamp. 



(Refer Slide Time: 6:18)

 

This is if you typically look at you know very good example in real life of getting a timestamp and then executing what you want to do, the darshan of lord Venkataeswara know Tirumalai Tirupathi devasathanam as this thing called they give the time band you know. When you actually want to have darshan of the lord they say that you have to obtain a time band. You know what exactly the time band means? They give you the times sticker and says that if you actually go there know before the time or above just the time, you will be able to see the lord. So basically in other words, the darshan of the lord is to be serialized because you can see one after other, the queue one has to move to see the lord. So basically a serializable will be a problem there. 

Now one of the ways we solve this in real life by saying that you need this band you know this band is generated. What is the band means there? Essentially getting a time stamp time band and says that all the people what is the time band have to proceed the order of time that they have got that means they will be viewing the lord in terms of the time band. In a similar way, if you look at how the transactions are going to execute, each transactions when it enters the system, it get the similar time band that is what called time band. How exactly this time stamp is given to the transaction? Later we will look at the step but the idea of giving the time stamp to the transaction is that essentially time stamp serializes the order in which transactions are executed. All transactions are executed as in the increasing order of the time. 

If a transaction has got a time stamp, its order in terms of execution is fixed because all those transactions which got time stamp before this transaction will commit before this transaction. All those which got a which got a time stamp higher than this can proceed and are going to commit after this transaction is committed. So in that sense in serialize order time the notion of the time here serializes the transaction execution. This is a very important concept. This notion of time and I am using the notion of time is a very important concept. As we go further down, we are going to look at distributed transactions were a single transaction is split into sub transactions and execute on multiple sides. This is like not now using single time stamp. For example; for each person could be carrying is own wrist watch. So you get into additional problems of how do I actually take this different times and now put them all into one zone. For example; now London will be in one zone, Chennai will be in one time zone.
 
So if you are actually generating transactions in different time zone, you get into problems. Now all of us using the single clock, automatically everything will be serializable but if they use multiple time zones then you get additional problem of making sure that the events across multiple time zone are again serialized. We are going to look at the notion, when we go on look at the transaction and see how this notion of this time becomes critical when you want to address the concurrency control problems in the case of distributed databases. So to understand at you know very simple level, what really happening is when transactions are coming in into the system giving them a time stamp and you are excepting the transactions are executed with respect to the time and essentially produces the serializability. To look at the basic steps in little more detail. 

As I was trying to do here every transaction start with will be given a time stamp. This timestamp which will be used subsequently for checking whether the transactions are executing in a consistent fashion or not. Now all read and write operations of the transactions are tagged with the time stamp. Read operations and write operations the transactions at tagged with these time stamp, write operations of the transactions are tagged with these keys will call these time stamp as TS. So all the read and write operations are actually tagged with this time stamp that’s the second thing.

(Refer Slide Time: 12:06)

 

So when the transaction issues are read or write operations, then let us say transaction TI has got a time stamp of ten twenty just understand or at this point of time is eleven thirty. We will say the transaction got a time stamp of eleven thirty. Now subsequently whatever the read or write operations that are being performed by transaction will all get the same number that’s one time stamp we are going to use. For example, in a later point of time eleven thirty five or eleven forty, you are doing something but all belong to same transaction. They are not giving the time stamp this is very important. 

All the read and write operations of the transaction are tagged with the same time stamp which is given to the start of the execution of the transaction. Now what do we do after this point of time? We also have to actually know for every data item in the data base okay. For every data item X, we need actually what is the read time stamp of the what is the read time stamp of X and read time stamp of X shows the time stamp of the okay the time stamp of the transaction okay that has read the value of read the read the value of X. What is this mean? 

This means let us say this is the data item X and there is the transaction which read this data item and time stamp. Let us say eleven fifty and say that is the highest transaction. There could be transaction time stamp lower than this which also read this data item. But we are going to consider the highest time stamp of the transactions that has read the value of x will be the RTS of X. Similarly the write time stamp of x is the time stamp, the highest time stamp. If you want to actually qualify it the highest time stamp or TS of the transaction that wrote the value of X the transaction that wrote the value of X okay. So for every data item, you are going to do have two time stamps corresponding to the transaction highest time stamp value of the transaction that has read the v value and 

(Refer Slide Time: 15:25)

 

also the value and also the highest time stamp of the transaction which wrote the value. Why do I need this tool? because this essentially tells with respect to this data item. What are the earlier transactions that I have actually done whether the read or write operation are performed by the transaction on this particular data item. Now we will use this time stamp to see the validity criterion when a new transaction comes tries to do some manipulation on this data item. We are going to look at it and say which are all those values which can be allowed by the transaction manager to proceed and which needs to be aborted. Now this is the preliminary thing in terms of how exactly the algorithm maintains the data. Now in terms of actual execution, what I does is, whenever a transaction okay; now consider the case when a transaction issue. A transaction issues the read operation the read operation. Now this read will be tagged with the time stamp of the transaction. So basically, essentially this is TS of the read operation okay, time stamp of the read operation. Now these TS has to be compared with the data item time stamps. Now let us say, 

(Refer Slide Time: 17:14)

 

The read time stamp of X and write time stamp of read operation we will say on X a transcation with time stamp TS issues a read operations on X. Now the RTS and now WTS the value of X are the following. Now what is the condition? Now this read operation can be allowed to proceed. Now the read operation can be allowed to go further as long as the condition that the time stamp of X that the current transaction is greater than the write time stamp of X. Please remember the reads can be shared.












(Refer Slide Time: 18:00)

 

After I write a value any number of people can read that value, but if you assume that only if I am actually reading a value after somebody actually written a value on X, which means that the my time stamp value is lesser than what actually has been produced. Then I need to be little careful and avoid such operations. Understand the condition where TS is less than WTS of X, If TS time stamp of the transaction is less than time stamp of this current transaction is less than right time stamp of X. What does it actually implies that means a later transaction actually return the value. A later transaction than me has written the value.
 
(Refer Slide Time: 19:08)

 

A later transaction has written the value of x. Now in terms of time stamp order if you want to execute the transactions strictly in the time stamp order, this should not have happen.

(Refer Slide Time: 19:20)

 

Because now I am actually reading a value produced by somebody who is coming later than me. This is like saying that know if you look at senior junior relationship you know. Now if you essentially says that I am actually I am passing the batch which supposed to passed in 2004 says that its actually passing after the 2003 batch there is a violation, because 2003 batch has already gone, which means that the value return by x is by transaction later than me or older than its me. Now I cannot come back and say to read its value okay. So unless I had actually finished the later transaction would not be able to come and do whatever is trying to do. So in that sense, now this is the case transaction T is to be the issuing transaction has to be aborted the transaction which is actually issuing. If this is the case the transaction issuing the transaction which issues which is sure which should read the operation needs to be aborted okay and it is restarted okay and restarted with the higher order time stamp restart we are going to add higher values time restarted with a larger time stamp okay. Now understand what I am saying just going back.  











(Refer Slide Time: 21:30)

 

A transaction issues the read operation TS on X. Now the transaction time stamp is TS here as shown here and the current values in the database as far as RTS and WTS concerned are shown here. RTS shows the transaction larger time stamp of transaction that has read the value of x. WTS shows the time stamp largest time stamp of transaction which is return a value on x. Now my time stamp is less than WTS. I do not need to compare this RTS, RTS value is higher since reads can be shareable. They need not be exclusive I am still not violating any consistency thing only write I have to read something earlier return by transaction. This is earlier than me rather than a transaction later than me. This essentially shows that a later transaction has written the value of x.

(Refer Slide Time: 22:39)

 
The transaction which is issued the read operation needs to aborted in such case and restarted with the larger time stamp and when you restarted a chance of distractions succeeding his higher because now it will take this and proceed further and chance of the transaction succeeding later is very high.

(Refer Slide Time: 22:55)

 


This is what we will be done as far as the read operation is concerned. Now if you look at the write operation, a transaction let us say with the write operation, transaction with the time stamp, with the time stamp okay TS issues a issues write operation okay. Now if issue a write operation you have to check now. Let us say there are again similar case of RTS is the current value, WTS is the current value of the data item X. Write operation will say on X okay, in which case the RTS and WTS are the current value. Now you need to check if either of these conditions are true, TS is less than the RTS RTS thats the lead time stamp or TS less than the write time stamp of x then what does this implies? This implies that I am writing a value into X which was read by the later transaction or return by the later transaction. In both cases, what I am saying is, now I am trying to manipulate the value of data item but somebody comes later than me actually read as value return this value.









 
(Refer Slide Time: 24:55)

 

In either of the case, this results in an inconsistent situation because who is somebody coming later than me as actually read. I am actually modifying something would have read some stale value okay. Similarly I have read value that value will be last come then and start modifying it. So in that sense both these conditions the write has to be rejected okay the write operation has to be rejected write on X has to be rejected. Now what are the reject means? Same as the earlier case that, the issuing transaction has to be okay. The issuing transaction needs to be aborted and restarted with a higher stamp. Transaction needs to be aborted and restarted with a higher time stamp. Now this essentially explains their basic steps of the time stamping algorithm okay.

(Refer Slide Time: 26:17)

 
Now what we are trying to do in this particular case is, we are trying to say the algorithm tries to provide a basis for know checking. We give transaction a time stamp no we did not still discussed. How the transaction will be given a time stamp know several ways it which can be done. One way is actually you can use a counter. For example; first transaction will be given a counter value of one. The second transaction will be given higher number than this. Since all I am interested is logically showing that the transactions comes one after the other, a counter is good enough for me. Only thing is the counter eventually might become infinitely large means that means the counter value will become so large that at the end of it, somewhere I have to reach at the counter.
 
So at some point of time when there is more brief period of time like you know. This is period when everybody goes for lunch know like that when the when there are no no transaction for brief period know in the system you can just forget the entire earlier history. You can just set the counter zero and then start doing it again from one that point of time okay that’s one way of actually giving time stamp to the transaction. This is basically a logical time stamp because all that doing saying is transaction t 2 which got a counter value of two is higher than a transaction which got a counter value of one. So in terms of actually looking at t 1 and t 2 all that that you have decide in terms of time order is the counter value okay, then are various other ways of actually giving the time stamps for the transaction. The other way is actually to use a physical clock okay, physical clock that gives the values. Physical clock that gives okay this is like saying

(Refer Slide Time: 28:37)

 

I will use the clock time of machine okay the clock time and then I will basically say that every clock take I generate okay, every clock take is used. Clock take is used for generating the time stamp. Now there are interesting, this is important because every clock take can generate only one time stamp. 


(Refer Slide Time: 29:22)

 

So, which means that the number of transactions that can come limited now, because they are limited by the number of clock takes because the time stamp will be generated only one time stamp can be generated in a clock take because two transactions need to get two different time stamp. They cannot get the same time stamp. If they get it then you will have problems of validating them at the end seeing how they will basically know you need to order them. So, you would not be able to order them if both of them get the same time stamp. Then interesting algorithm for generation of time stamps, in fact there is a very interesting algorithm which does not generate time stamps with respect to the starting point.
 
He generates with respect to the time when the transaction is excepted to finish. This is an extremely interesting way of looking at the problem in an entirely different perspective. There is like saying when somebody entering in to IIT, it is at that point I give a tick to him or I give a band to him which means that is excepted now to follow everywhere else with that band. So the band was given with respect to when actually came in to the system. The other thing is when is excepted to leave the IIT gate. I actually put a band there, saying that this is the time you were excepted to leave the system. 

If somebody close up at the gate know after his time over you basically know abort that particular thing. This is like saying that is excepted to leave the system within that particular time I come to the gate at ten o clock. Now one way of saying that is I get the ten o clock at the time stamp the other way interest when we saying is I am supposed to leave this system by twelve o clock. So I give twelve o clock as a time stamp which means that ending time is given that means all the transaction if the transaction dint finish within that particular time period. It will be aborted. So possible for know for the algorithms to generate this time stamp in extremely interesting ways that used for um some algorithms to actually fixed a band. A band of time during which the transaction is excepted to finish the execution. So one can intelligently use generation of time to see how the transaction execution can be controlled. Now has to just come back to the point of how the algorithm works and then see for further issues with respect to the algorithm and how this algorithm is different from locking kind of an algorithm. What we do in this particular algorithm is? 

(Refer Slide Time: 32:20)

 

We actually giving a time stamp TS to the transaction. Every transaction is getting a time stamp and then the read and write operations of the transactions read or write checked again the RTS or the WTS. One of the things I have actually forgotten to mention is when you accept a read operation or the write operation, immediately you need to actually update the time stamp of the data item.


















(Refer Slide Time: 32:54)

 

For example: I showed only in the earlier case how the RTS and WTS will be used to abort a transaction. For example; if those conditions are not met you are going to abort the transaction, but if you say that you gone to accept the reads and writes on the data item. For example; let us say. Now these read or write on TS accepted. If the operation is accepted, if the operation is accepted if the read or write accepted the abort conditions we actually saw earliest. So I will just not mention them here. If the read or write accepted  then the RTX corresponding RTX.
 
(Refer Slide Time: 33:52)

 

to be updated RTS of X okay to TS okay highest of this is what should be made equal to know. Now similarly we are going to look at the write time stamp of X and then the TS okay. The highest value is set to the highest of RTX, TS is set to RTS of X similarly for the WTX the highest of WTS X and TS when a write operation is accepted the corresponding value is basically updated. What this means is? The RTS and WTS always reflect the time stamp the highest time stamp of a transaction which either read or written on the data item X.  

(Refer Slide Time: 34:52)

 

That’s what actually happens by making these counters. Typically RTS and WTS are counters, data counters. They stored the value of the transactions that have highest transaction value that either read or written on this data items. Now let us go little more deeper and look at how exactly the basic time stamping algorithm is different from the two PL kind of an algorithm. I will take a simple example and show you where exactly the difference is going to come when you apply a basic time stamping algorithm verses the locking algorithm. If you remember write, we have actually taken a simple case of  













(Refer Slide Time: 35:50)

 

a transaction T 1 reading the value of x writing the value of x and then subsequently doing a read of y and then a write of y. Here basically x and y are the data items. Now I actually read the value of x, write the value of x then read the value of y and write the value of y. Now let us say there is another transaction T 2 which exactly does the same thing as done by T 1. Now what would have happen? If actually I applied a two phase locking kind of an algorithm. Now T 1 needs to acquire a lock on x and lock on y and after that it does whatever, then execute then commit. After that release x and y lock on x and lock on y. This is done at the end of the execution of the commitment. So you going to release the lock on x and lock on y after the execution that means only at this point of time T 2 can acquire lock on x, lock on y and then execute and this is only way T 1 and T 2 can proceed. So unless T 1 completely finishes releases its lock T 2 cannot produce. 

















(Refer Slide Time: 37:58)

 

This is very important thing that happen if I applied the locking criteria right but if you carefully observe after this initial point of  T 1 trying to do Rx and Wx, it is that point you can see there is no more use of x for transaction T 1 which means that it is possible for T 2 to start executing from this point not at the later point of time. Now there will be a read y write y here and then there will be read y write y. This is an optimal execution of this scenario, but this wont be possible apply the locking strategy, because locking would have required to two phase locking would have required you actually lock both these data item which means that is only at this point of time the lock on x will be released that means this will be shifted up to this point and you basically start executing the second transaction T 2 from this point.

So if you mark this is T 1 and T 2 you can see the overlap a significantly come down because you are not able to execute now. This is the zone I could have shifted the execution of Rx and Wx to this point but this wont be possible if I apply two phase locking and let us see if it is possible for me to actually to do this if I apply time stamping based algorithm. 












(Refer Slide Time: 39:38)

 

Now what would have happen? Is T 1 would have got a time stamp of TS 1 and T 2 would have got a time stamp of TS 2. Now all that condition that we have is TS 1 is less than TS 2 which means that we have a case where transaction T 1 has been able to get time stamp which is lower than transaction T 2. Now when they start executing at the end of at the end of execution of the first transaction T 1 it will write the value of time stamp on data items x. It is at this point of time the transaction T 2 will try attempting accessing data item x. Since the time stamp of TS 2 is greater than TS 1 as can be see here it is possible for T 2 to access data item x at this point of time. 
 
So what really will happen after this point is, you basically will have transaction T 2 accessing the data item x and then writing. Similarly here at this point will have y being accessed by transaction T 1, now the T 2 can start accessing y after this point of time. Now what can be seen here is that the overlap that we were talking earlier. To recollect what  we have been talking earlier is that its possible for the transaction T 2 to overlap with T 1 when transaction T 1 is finished with accessing x and now tries to manipulate y. Now this effectively prevented in two phase locking because T 2 can access lock only after it is released by T 1 and T 1 will not release lock on x till it actually reaches this point is basically the lock point for the transactions. So unless it reaches the lock point its not going to release the lock on x and hence T 2 will not be able to start executing till this point of time. 








(Refer Slide Time: 42:27)

 

But whereas effectively, it can start executing from this point onwards that is what we actually looking at when we look at the time stamping algorithm all that matters here is the TS 1 is less than TS 2 and that is the order in which it will be allowed. Both x and y will be allowed to be accessed by the transactions manager.

(Refer Slide Time: 42:45)

 

So in other words, both T 1 and T 2 will be executing in the time stamp order and this permits. In some cases more concurrency then what we seen in the two phase locking. but remember this is not straightly a optimistic kind of an execution because we still looking at how the transaction should execute by looking at the transaction Time stamps which were given at the beginning of the execution of the transaction not at the end right. So it is not fully optimistic in that sense. In a fully optimistic scenario, this would have been done by the transaction at the end of the execution.
 
For example: T 1 would have written all its value. T 2 would have written all its value and I will be checking which one should be sort of committing at the end of the execution that means both will execute to their finish and then I will actually use a validation point here and say which one of them will pass the validation and make that transaction commit whereas here, I am using the time stamp and using the time stamp to order these transactions in the beginning itself. I know the T 1 has the time stamp TS 1 and T 2 has got the time stamp TS 2 when it started executing and now if TS 1 is less than TS 2 that is order in which T 1 and T 2 will commit. If it is other way around, then the commitment is going to be T 2 before T 1. 

(Refer Slide Time: 44:32)

 

Now this explains what we see as a basic time stamping algorithm? How time stamps are used for concurrency control? Now one of the things that we still did not understand here is how this get integrated how the concurrency control gets integrated with the commit protocols. If you remember, we did this exercise even for two phase locking, when we integrating the two phase locking with commit protocol and that is the reason why we actually modify the two phase locking saying that the locks will not be released by transactions till the transaction commits because if it releases earlier the other transaction can look at the value and this will create cascading abort and other problems.
 
Now similar thing happens in the case of even time stamping algorithms. We need to see how the time stamping algorithms get integrated with the commit protocols. Now what   will do is, we look at a simple mechanism by which the time stamping algorithms get integrated with the commit protocols. A simple exercise here will be to just look at not just how the transactions writes its value to explain the problem. For example, you can you can see here that there is a write x, there is read x followed by write x in the case of transactions x transactions one. Now if you take the transactions has returned actually the values at this point of time that is, it is actually continuing to execute the other things before it actually reaches the commit point.
 
Now if you understand the right x here the modified value of x is written this stage. The modified value of x is written is strictly not correct because the transaction is not still reached the commit point here. Now let us say at this stage of commit for some reason, this has to be rolled back which means that whatever the value that has actually been written here still needs to be undone which means that any body who is actually reading this modified.

(Refer Slide Time: 47:00)

 

Transaction coming after this would be reading the value that is written by this T 1 and that potentially creates a problem in terms of how the transactions depend on each other. In affect we will be relaxing the concept of isolating one transactions affects on the other and that’s what causes this difficulty of relaxing this. What we do in this case is? We will actually replaced this write in what we call as a pre write, that means every transactions to start with will issue not a write but a pre write that means this write instruction that we write seeing here will be a pre write and after exactly it wants to commit and it reaches this last point here. This is the point, it should use a write transaction that means the pre write on x and this is a write on x, the pre writes are not exactly written on to the database, not written on to the permanent storage but they are buffered. 






(Refer Slide Time: 48:05)

 

Pre writes are buffered and we will actually validate this pre writes and  make sure that the pre writes once accepted are not rejected at a later point of time from consistency point of you and when a write is actually issued by the transaction, updating its pre write its never rejected its always accepted, but the pre writes the transactions can still not commit a pre write which means that the commit stage it does not issue a pre write it doesn’t issue a write its possible that the transactions pre writes will all be rolled back which means that there is not going to be any affect on this pre writes on the actual database. What we will see is now a modification taking pre write in to account, how the pre writes will try to solve the problem for integrating commit protocols, with concurrency protocols.

















 
(Refer Slide Time: 49:07)

 

Now if you remember we are actually two checks, when a transactions issues. Now let us say T issues a pre write. Now this pre write has to be checked for pre write on a data item x. 

(Refer Slide Time: 49:24)

 

Now this has to be checked for on the database. Items on the database times stamp now this will be checked again is the read time stamp of data item x and also, write time stamp of data item x. Now if the pre write is pre write time stamp of the transactions is less than either RTS or WTS of x that means it is actually less than the write time stamp or the read time stamp, then T is aborted. 
(Refer Slide Time: 50:24)

 

Otherwise we pre write is buffered. Pre write is actually not written but what is done here is pre write x is buffered with its corresponding time stamp buffered with TS. TS has its time stamp okay as its time stamp okay. Now what is this mean? This means essentially the following, all the time stamps that we are talking about here are, with respect to this buffered item here x. For example; if x is actually pre write, pre write on x is buffered with time stamp TS. Now any subsequence reads that we issue here needs to be checked again this pre write.

(Refer Slide Time: 51:24)

 


Now at a later point, this pre write becomes a write on x, then it is at this point of time the actual WTS of x is updated to the corresponding TS okay that means this will stay like this for a while in the buffer when this actually comes write comes on x it is at this point of time it will be updated to a write time stamp of x. Now as these pre writes are buffered as the reads comes in to the system reads of a transaction come in to the system, we still needs to check those reads again as any of the pre writes that are already buffered in the system. 

(Refer Slide Time: 52:15)

 

Now let us understand how the reads. Needs to be modified in this particular case; now if you typically look at read of x issued by a transactions T with a time stamp TS is how we will read this. Now this read x time stamp is TS. Now if you remember earlier, this will be checked the write time stamp of x and if the write stamp of x is less than the TS will allow the read to proceed because any number of reads can be done on the data item. Does not really matter, as long as the read time stamp is higher than the write time stamp. 














(Refer Slide Time: 52:57)

 

because reads can all be done concurrently where as the writes have to be exclusive. Now in that sense it does not really required to be checked again the read time stamp what you need to do is you need to check only the time stamp of x in this particular case again as the write time stamp of the x. Now if you only do write time stamp of x is less than TS. I think I will put other way around which will make things, if write time stamp of x is greater than TS then basically this read has to be rejected. For obvious reasons, because we are actually read after some other transactions which came after you will be actually produces the write value the read issuing transaction is aborted transaction is aborted.

This is same as condition of the earlier one but other case read is allowed in the in this case the read is still not allowed when you actually have a pre write buffered okay. Now what is this pre write buffered means the pre writes buffered means if i have typically there is a pre write time stamp on x with a TS. Now I have to check this time stamp. Let us say it is TSP to just indicate that now this TSP is actually less than the TS.














(Refer Slide Time: 54:45)

 


The TS is the time stamp that I am actually trying to read what does this indicate? This condition indicates that there is a pre write buffered and that please remember that pre write will never going to be rejected when the actual writes comes in. So I actually need to buffered in such a case we need to buffered we need to buffered the read transaction. Read transaction that means you actually postponed read transaction and allow read only after the write has been committed. Allow read to happen after the write happen after the write actually comes write on x comes. This ensures that this typically will ensure that the read of the transaction happens after the writes affects. 



















(Refer slide time 56.02)

 

Now the other conditions were the read on the transaction TS is less than the TSP that’s going to be the other condition, then you can allow the read to happen because this in this particular case there is no pre buffered writes on the transaction and hence this TS can be allowed to proceed. 

(Refer Slide Time: 56:14)

 

read will be allowed to proceed will be allowed to proceed. Now what this shows is? It requires just a minor modification in terms of how we handle when we want to integrate both commit and the time stamping protocols.

(Refer Slide Time: 56:38)

 

All that we have to do additionally do is, we need to actually make sure that the writes are handled properly in this case by ensuring that they do not actually write on to the database to start with and produce the pre write. What we will do in the next lecture is, we will look at host of other protocols which are most optimistic than the time stamp based protocols in the next lecture. 
 
Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. 23

Concurrency Control – Part -4

In the last lecture, we have been looking at time stamp based concurrency control techniques. What we have been looking at in more particular is, how the basic time stamp mechanism works in terms of ordering the transactions, in terms of its time stamps. We also looked at how the basic time stamp protocol can be integrated with commit protocol. Towards the end of lecture, we saw how the basic time stamping techniques can be integrated with the commit protocol. We just recap what we have been doing there.
 
We introduced instead of write, a pre write and then ask the transaction to issue a pre write instead of write to start with, and pre write is going to be buffered and any read transactions incoming read transactions will be checked against this pre write to see if they need to be buffered or they can be they can be satisfied, the read transactions can be satisfied. So what we have done in the last lecture? We also mentioned that in the last lecture, basic time stamp mechanism is not truly optimistic concurrency control algorithm because we are not actually looking at looking at validation. At the end of transaction execution, what we are still doing is we are ordering the transaction as they enter in to the system rather than looking at the end of the execution whether what they have done is truly satisfies the consistency requirements.

What we are going to do in today’s lecturer is, to further continue looking at these models of concurrency control and see a slightly different kind of algorithms which include the truly optimistic version of a time stamping algorithm. To start with, what i will do is, i will look at a time stamping algorithm, time stamping based protocol that is truly optimistic in the sense that the validation will be done at the end of the execution of the transaction and we are going to look at in depth that particular protocol and also we will look at another different approach to concurrency control which is actually the multi version concurrency control algorithms.

The multi version solves the problems of concurrency control by producing a new version of the data item, each time you write the value which means that the old value is still preserved when you are actually producing a new value further data item and that’s solves the concurrency control  in a completely different way. For example; actually each time you try to do something since preserving a old value a large extent a problem of concurrency control is elevated by maintaining a multiple copies of the data item, but a consequent problem is that you will end up paying a huge overhead for this space. A completely degenerated case is where you not only maintain the new version but also the time at which this was done.
 
This typically comes down what to see as a temporal database because we actually record when a value is changed not only the changed value, the new version of the changed value but also the time when it actually happened and that becomes what we call as temporal database. Concepts of temporal database are suddenly beyond the scope of this particular series of lecturers, but I encourage you to read the material on your own to understand. After i finish this I will encourage you to read a little bit more on your own on temporal database which constitutes a very important and interesting aspect of databases by itself. As we go further down, what we are going to do is, we will start our discussion by first looking at a truly optimistic time stamp based protocol to start with and then proceed on multi version protocols.
 
I am also going to look at multi version based concurrency control algorithm and figures them, version of the multi version two version protocols in the context of two phase locking and we were going to look at in depth. In the next class what i am going to do is, i am going to review some of these things that I have done in last lecturers with set of questions and then giving more explanation of what was actually done, by looking at a series of questions review questions that we can attempt on the last seven lectures. In the seven lectures, we are going to have review questions. In the next class, we can be prepared on the things we have done so far. So that, you will be able to look at the review questions more carefully at the end of the seventh lecture.

Now we will start with the optimistic protocol. Now a truly optimistic protocol as we were taking about we have the approach to looking at the problem of concurrency  control, the CC algorithm will have the approach of actually doing the checking at the end of the execution of the transactions. Now every transaction in this case, can be thought of as having several faces. The first phase can be the read phase which means that the transaction typically reads whatever is needed by it.

(Refer Slide Time: 7:56)

 

All the data items that are actually required by it and then it also manipulates them, but only one thing is does not actually write them back on to them database till such point actually the transaction gets validation. So you have an extra phase here which is called the validation phase. Now the validation phase make sure that different transactions if they are conflicting with each other enter in to the validation phase and they get validated and subsequently enter after validation phase in to their write phase, which actually means that we have these three phases; a read phase in which the transaction reads all the values and then you have the subsequent validation phase after the transaction finishes all its required things, it gets in to the validation phase.
 
In the validation phase, any conflicts are detected and make sure that if the transaction passes the validation phase, it is in a consistent fashion whatever is trying to do and after it enters the write phase it is allowed to write the values of the transactions on the database. Now if the transaction does not pass through the validation phase, it is actually the transaction does not pass through the validation phase is actually aborted.

(Refer Slide Time: 9:20)

 

The transaction that fails validation, validation is aborted and restarted. Now it can be seen in this particular case.










(Refer Slide Time: 9:54)

 

What is happening really is the transactions just enter the systems, they read in the read phase they take all the required data items. They do whatever they need to do and they come in to the validation phase. When they come in to the validation phase, the system checks for any possible conflicts between the various transactions and once the transactions passes the validation phase, you enter in to the right phase into the right value. Now the important requirement here is validation.

How does the transactions gets validated when they are conflicting with each other? To ensure this, what we will do is we will essentially look at the read sets and the write sets of the transactions. Read sets of the transaction will agree with Tr and the write sets of the transaction. Now this is the important requirement because at the end of the day, at the end of the transaction execution, what we essentially do is, we look at the read sets 
















(Refer Slide Time: 11:09)

 

and the write sets and decide whether there is any conflict and based on that we are actually going to decide whether the transactions validate against the each other or there is a possible inconsistency produced by the value transaction and that is how exactly, the transactions are allowed to commit or mean to abort come back and do the redo whatever they done in earlier. This is truly optimistic, because what we are doing in this particular case as you can see here is, we are actually allowing the transactions to go through this read phase irrespective of whether they validate or they do not validate only when they come to the validation phase, we perform this check.
 
Once they actually validation is performed and it passes through the validation phase and it actually enter in to the write phase depending on the answer to this yes or no. If it is yes basically the transaction enters the write phase and tries to write the value on the database. So this is how exactly as you can see the validation, since it is performed in the end of the execution. This is a truly optimistic concurrency control to this how exactly the system proceeds to execute. On the other hand, as we saw earlier the pessimistic case the validation is done at the beginning of the transaction execution, not at the end of the transaction execution.

For example: Even before the transaction is allowed to read it needs to acquire to corresponding the logs. Only then, it will be allowed to proceeding to other phases which means that it will be blocked at the beginning. It will never be asked to abort after it is actually got the logs for other than actually deadlock or something else happens in the transaction aborted for those reasons. 





(Refer Slide Time: 13:03)

 

Otherwise the transactions once it get the locks, will proceed to execute finish its results write the values and then only it releases the logs. When in this particular case, the transaction starts executing to start with and it get validated at the end of its execution to see what it has done is correct or not and it is aborted if it has produced wrong results and it is made to redo the things again. So we have actually a completely two different approaches to solving the same problem. Now let us understand what exactly happens, if you apply the true optimistic concurrency control. Now one of the things that we will be doing here is we need to record though we have looked at three phases. 

(Refer Slide Time: 13:54)

 

We will be needing lot more information to see whether the transactions validate or not or they do not validate. To explain this what we will what i will do is i will give intuitively give you what happens when we see algorithm and then go into details of the algorithm physically. Now as you can see if there is no read if read sets of the transaction do not conflict with each other in which case, it does not really matter how they actually went about doing their activities. If there is a write set conflict, write conflict or read write conflict, then I need to worry about how to order these transactions. Now what I look at is. For example: you imagine, there is somebody in front of me who has actually done something database. This is what we called as the committed set.

Now this committed set has gone ahead something on the database before actually I came try to do something. Now I need to worry about, what is the thing this committed set has done and see. If I actually conflict them respect to the committed set which is equivalent to say that this is Tj which has got committed. Now you have to realize that I have to maintain information about the current committed transactions, what all the data sets? What is that they have done on the database? Now i will take a look at the committed sets of the database and see with which are those committed sets? I am actually conflicting to see if I can validate again as well. Now if there is conflict between the committed set and my set.
 
I am the current transaction which is trying to validate. Now the committed set has to see whether there is an overlap between the read and write sets and there is no overlap between the read sets or write sets depending on that you can decide now what exactly has happened before or after and based on that you can say whether i validated against them or not. A simple case is, if there is a conflict in the sets you have to make sure that a strict ordering is actually ensured between the two transactions that conflicting against each other and you progressively relaxing this requirement of the phases one before or one after depending on how much conflict is really existing between these two transactions. Now we will start explaining now with this background what exactly is done to see where these conflicts are coming, how we can look at validating the transactions. Now to give this explanation we will take simple example will start explaining how the algorithm works. As I just explain what I am going to do is I will see I will maintain. For example; Tj is the committed transaction. We will take this as a committed transaction. Now Ti is the transaction trying to validate, transaction trying to validate now okay. 












(Refer Slide time: 17:53)

 

Now what I am going to look at is how exactly the Tj and Ti sets u know read and write sets of Ti and Tj really conflict with each other and based on that we have to see whether Tj should come before or Ti should come. In this case it is already clear that Tj has come before, so the order is fixed. Since the order is fixed what i am going to do now is, since this order got fixed because Ti Tj is already committed. Ti’s all operations should be enforcing this requirement that it comes after Tj. Now to what level Ti should be coming after depends on the conflict. For example: if there is non-conflict, it does not really matter how Ti actually work with respect to read and write phases, but as you start seeing that is more and more conflicts in the system. More and more conflicts in the system, you have to worry about how exactly this ordering is done.
 

















(Refer Slide Time: 18:27)

 

Let us see now, how exactly this how can be further, there is read sets and there are the write sets. The read and the write sets of each of these transactions have to be now looked at, this is the first thing. Now the requirement that I need to know read sets and write sets of the transaction is the strict one.

(Refer Slide Time: 19:30)

 




In fact, this often causes problem because this is the restriction that requires that we know a priory what are the read sets and write sets of the transactions. Some level of pre- processing will be needed for us to be generating the read sets and write sets. This is the first thing that happened in the optimistic concurrency control. Some algorithms other than optimistic algorithms also require that I know the read sets and write sets of the transaction is a strict one, that will be the assumption that means for example, if i write my t begin and at this stage i actually gave what are the read sets and write sets of the transaction. Now one way is, the user gives the read sets and write sets that were the transaction writer grammar gives the reads and write sets. 

(Refer Slide Time: 20:31)

 

The other one is the compiler can look at and actually generate these reads sets and write sets. If the compiler is generating the read sets and write sets, suddenly it is going to be superset of the reads and writes that actually might get executed when the transaction is executed. I will not be knowing which are all the data item that I will be needing I do a particular transaction. So I parse it actually a start it time, I might indicate a superset of the set of data items i might actually access, when I go to the, when I actually execute my transactions.  That is what see in terms of the reads sets and the write sets. The other thing that we need to maintain as part of this is what we see as the write phase. When it has actually starts and end of the write phase. Start and end of the write phase. Similarly in terms of read phase, we have to see the start and the end. These are the times which we need to maintain each for these phases because based on this we are going to now say 







(Refer Slide Time: 21:58)

 

whether the validation of Tj coming before Ti is true or not and that is the reason why we need to maintain for each transaction, the write phase for each transaction we need to do this. Each Tr will be waited as transaction, we need to look at the write phase and read phase starting time and the ending time for both the read and write phase. Given this, now let us look at what really we will we need to do in terms of validating these transactions. 

(Refer Slide Time: 22:16)

 



Now every transaction, as it is explained will have three phases. Every transaction will get into three phases here. Every transaction has a read phase. Now a read phase is marked by a start of the read phase, end of the read phase, there is a start time and the end time for the read phase. Then it enters the validation phase and then we have the write phase. Again we have the starting time and the ending time for each of the transactions. Now this is maintained as part of the execution. 

(Refer Slide Time: 23:20)

 

 
So for every transaction when the read has actually started, when the write has actually started when the read has actually ended, when the write has ended. Now if you understand now the criterion that Tj should be coming before Ti. Now, if there is a true conflict between Tj and Ti which means that there is basically some data item which are produced by Tj have to be read by Ti, which means that data items produced. What we mean by data items produced is, the value of this is return by Tj produced by Tj or read by or consumed by Ti consumed by Ti. Now it is very clear that unless the write phase of j is finished, the read phase of i should not have started.










 

(Refer Slide Time: 24:14)

 

If it starts early, then it means that this equation. The equation we are looking at that Tj should have should have lesser you know in terms of time order should be coming before Ti would not be valid because Ti would have read values not produced by Tj but by some other transactions. This is very important to understand. So, for this to be validated conflict data item between Ti and Tj. In terms of Tj write some values Ti reads some values which equivalent to conflict.

(Refer Slide Time: 24:52)

 

There are common items between the write item of Tj and the read set of Ti. In which case, we need to ensure that this criteria that is enforced is write phase of write phase of write phase Ti of Tj ended before Ti read phase started, before Ti read phase started. What I am trying to explain here the meaning of this is, Ti read phase would have read the values, Tj write phase would have written the values which means that the write phase of Tj ending before read phase of Ti ensures that Ti read the correct value produced by Tj. 

(Refer Slide Time: 25:45)

 

Since Tj is already committed. This ensures that Ti is valid in the case where Tj is committed and Ti is read a value before Tj is committed that means the write phase not ended which means that Tj still not return the value but Ti start reading those values which means that conflict has not been resolved properly. This will not be valid in which case, this condition that Tj is coming before Ti cannot be valid if this is not true. To what we will ensure is first condition, what we will ensure is that as I said, 
















(Refer Slide Time: 25:57)

 

The write phase of this is the true Tj ended. This ensures that before the read phase of Ti. This ensures that Tj is strictly before Ti because Tj has finished all its work, then Ti is coming. The other condition where there are common data items between the two write sets. Progressively we can relax at the end of the thing we probably can say that there are no common data items. There are no common data items before data items between Tj and Ti. What does this actually mean? This means ‘Tj’ is working on a separate set of data item. 

(Refer Slide Time: 27:41)

 

Ti is working on two separate sets of data items. In which case, for the condition Tj comes before Tj will be notionally correct all that i need to do is their read phase of  Tj read phase of Tj know start, start read phase of Tj the starting time is before the Ti staring time. Ti read phase staring time read phase starting time okay. What does this actually mean? I will try to slightly rewrite it for you so that, you understand the meaning of this. Read phase always has two times. Remember that read phase has a start time and an end time. Now that all I compare to that is start time of Tj and then i need to actually compare the Tj’s read phase.
 
Again this will again compare have start time and the end time. Now all the I have to do is Tj read start time. This is what i am looking at is less than Tis read start time read phase start time, because since they do not conflict, it does not really matter, how they actually got executed but Tj to come before Ti all should it done is, it read the data items from the database before Ti. Take this condition satisfies, then it find with me that Tj is committed coming after that but all my reads happened after Tj has actually finished its read phase. Now this is what is exactly done in terms of you just recap what we have done in this algorithm. All that we are doing is, we are ensuring that every transaction goes to three phases.

(Refer Slide Time: 29:29)

 


The read phase, validation phase and a write phase and in the validation phase, we are actually looking for this transactions conflicts with any previously committed transactions and the validation phase ensures that whatever has been done has validates again and regerates reads and writes gets validate again the committed transactions and that ensures that whatever once the validation phase is crossed the transaction, it can safely go and commit. All other future transactions that come now can validate as transaction right. Now the most interesting fact here is, for example: imagine there are two vehicles coming in to the IIT. If you know, how the IIT Chennai is organized. There is only one IN gate to which all four wheelers can get in to the campus. Now let us say all of them are raising know to go in to some program in our open air theatre or student activities center. Here there will be a limited parking lot. There only limited space in the parking lot. Now where do we enforce know in terms of who will actually win, in terms of putting his car in the parking lot. 
 
One thing is to say that, I know my parking lot will take only two hundred cars, then I say that when I enter to the IIT IN gate, I start giving the numbers one two three four five like up to two hundred say that, beyond two hundred cars will not enter which means that the cars will be turned back the minute the two hundred and one car actually tries to enter in to the IN gate. That means it is actually at the IN gate level, I am actually enforcing the concurrency control. I do not even let cars inside my system. Once I know I cannot handle them but then I have the counter there which makes sure this criterion is ensured before it enters in to the system.
 
The other one is I do not enforce this rule. There will let cars go in in to my through IN gate but when they actually reach the parking lot, it is at that point i actually see which of them can get in to my parking lot. Now the one which probably came later at the IN gate raised passed the car which was in between which means that though at the in gate level, it is not really first car enter in to that, but by the time it reach the parking lot, it is faster than the other car. Then I let that car to get it which means that the transaction is raised with each other for committing. Now when they raised with each other, they committed at the end of it that can be an altogether different transactions that are committing. If the car is entering into parking lot are not necessarily the cars that came in the same order at the IN gate.
 
If you remember a basic time stamping ordering would have been would have given a time stamp for each one of them and allowed them to enter in to the parking lot as the time stamp. So it is not truly optimistic in that sense, a locking would have also worked in a similar way except in a slightly different way where it have actually ensure that for each one of them specific lot somebody would have got a token and that token would have been used for enter into the parking lot. 

So in some sense, truly optimistic algorithm will allow. It is possible for example: a car which entered but tries not to get in the parking lot or park for some reason it get struck, then there is no point actually trying to reserve the lot for you at the end of the day because you let these who ever comes in you let them get them in and then assume that going to be less than 200 cars at any given point of time using a strict ordering at the IN gate does not really makes sense because you are given at point of time assuming that the conflicts are very rare. As they enter in there, at the end of the day all the cars enter in your gate automatically committed for finding a parking lot, but when you start finding there is going to be large number of cars that will be coming in allowing them inside the system does not make the sense because many of them have to return back. 

This is what would have been done in the case of an optimistic algorithm. Optimistic algorithm would have actually allowed all these cars in to parking lot. So let us say there are only two hundred cars can be parked in the parking lot but then if you actually allow a large number of cars to get in there, many of them go back spending all the fuel of coming up to that point and going back whereas you know that you can only take only two hundred cars inside. 

So, in the case where there is high conflict, high contention it is good to actually apply pessimistic approaches like two phase locking because they ensure that you do not really have lot of aborted transactions but whereas when you assume that there are going to be ten or fifteen cars are going anyway come inside putting a large restriction on them ask them to take token all that does not really is worth, that kind of approach does not really worth because at the end of the day all these find their value true in to the parking lot in which case optimistic concurrency control is quiet good. To just sum up, we see is optimistic concurrency control allows the transactions to proceed but there will be wastage when they actually abort, because they have to redo all the work they have done. Pessimistic approaches block the transactions and allow them to proceed only when the road is clear, when the road ahead is clear.

So it make sure that the transactions proceed and they never abort. When start proceeding, they never abort. So both have placed in terms of where there applicable and where there can be used. Now this sort of things comes up our basic concurrency control algorithm discussion where put up in broad perspective the class of algorithms belonging to pessimistic broad class of algorithms belonging to optimistic algorithms. What I am going to do in the next fifteen twenty minutes is to see a completely different set of algorithms.

They actually work in a different approach and different way to just give perspective on is this two classes but a whole set of classes that lie in between that’s what we are going to do in the next few minutes we have. To start with, what I am going to do is, i am going to look at a class of algorithms that are termed as multi version protocol. Now these multi version protocols are different because they tend to actually not you know overwrite the value but what they do is, for example if the meaning of the multi version, if you understand here is, every data item X will have multiple versions. Now obviously for example: the X value is ten here. Now when you rewrite this X value at later point of time what you actually get is twenty, then this is going to be a new version of X. It is called X version 1 assuming that this is X version gnome.
  
So you typically tend to produce what we see is several versions. Each time you actually change a value. For example: now return it the ith time. Now this is going to be i th value that you are producing. Now in the traditional case what we do is, we actually overwrite this value. 






(Refer Slide Time: 38:26)

 

Since we overwrite the value, the old value is completely lost that is where basically the problem of what i have done is correct or not, is multiple people are simultaneously writing this value is possibly what I have written what written by that is where we have problem concurrency control coming in to picture. Now the problem becomes completely different, if you say I maintain multiple versions. For example: you assume in the case of banking database where when i actually, simultaneously two people are withdrawing from the same account or depositing in the same account. You have to ensure that one finishes, one finishes writing the value then other comes and does its part of it. Otherwise, one of it writes going to be loss, but if you actually maintain multiple versions the problem is entirely different.
 
For example: I take one value produce a new version is take another value produces a new version is that all i need to worry is that versions produces a consistent. There new versions being produced and these versions are consistent the problems becomes severe the older value is always with respect to that older value only producing the new value and if somebody takes the new value and again produces the other value, it becomes a new version of the old value. Now a completely degenerated case of this is, what we are talking in the beginning of the lecture which is called the time based or database or temporal databases. Now temporal databases are also sometimes called historical databases. They preserve the history of what actually is being done.
 
For example: X value at ten at time t okay. So this X value twenty at time a different time. So you basically record the time as an event as a couple. The time becomes one of the elements of the database values. So value plus the time records at what value is this time and the value. This is very important. 



(Refer Slide Time: 40:51)

 

If you look at some kind of applications like the stock prices for example: The stock price at this point of time will be different from the same stock price, stock value. For example: if you take a particular company like TCS, its stock value is going to be at a particular point of time, its stock value is something, but at different point of time its stock value is something else. So the database if you just give the stock value. It is not really useful because you also need to know the current time at which this was actually done and that is what we mean by purely temporal database the temporal here means component the time component value is also stored.
 
If you start recording the entire history that is completely a degenerative case of multi version database. In the case of multi version database, a multi version concurrency control you maintain certain number of versions. Not all the versions that can be two versions which means that you keep only the current value and the previous value. You are not keeping value that are beyond that time and that becomes a two version database. It is possible that you maintain n number of versions which means that all the values previous values of the data item is always stored. Now what we will see is, set of simple multi version protocols, concurrency control protocols. 











(Refer Slide Time: 42:26)

 

To see how exactly the multi version protocol works and I will also see that as a specific case, a two version two phase locking protocol which have been dealing earlier. Extension of this two version protocol to the case of a two phase locking protocol. Now let me explain how a general multi version protocol will be working and later explain how it extended for the two version two phase locking what we will do is, for every item data Xi we basically going to do record. This is the meaning of this is the ith version of x ith version of data item X. Now ith version would have been produced by some transactions. Now what I will record here is which is the transaction time stamp? This is the time stamp of the transaction that is actually produced value time stamp of the transaction that has actually return. This transaction producing this value, producing xi.  


















(Refer Slide Time: 43:53)

 

Now, if some other transactions actually try taking the value of X and producing this then it would have become Xi plus one. Each time you write a value remember that you producing a new value new version of the value which means that it becomes if somebody wants to write on X Xi it becomes Xi plus one is no longer Xi. Now the other time stamp that i will be interested in since this is the write time stamp, the other time stamp I will be interested in is a read time stamp. Now it is remembered, this has to be the highest because they could be remembered several transactions leading Xi. Among them, i am interested in the highest time stamp of a transaction that read the value of this highest time stamp of a transaction that read the value of X. I am making it very simple for you. You can read at the end of it, i will give you some reference which you can read more of this. So there are two things we are doing as shown in the earlier case, the write time stamp which gives the time stamp of the transaction that produced. The read time stamp, highest time stamp that read the value of X. 















(Refer Slide Time: 45:16) 

 

Now what we are going to do now is, for example: You look at the write operation on a data item at the end of it, the transaction issues and now as it issues, a write operation on a data item X. Now as it issues, the write operations on X, I have to find out the X ith version which is basically the highest version produced highest version of X with time stamp, with write time stamp less than this transaction time stamp or equal to almost that means, i look at the latest value of xi which is the current value which I should be taking for Xi to be now operating upon. Now for this particular value, i should see the write time stamp. Please remember, write time stamp is the highest time stamp of Xi read by another transaction. For example; there is already a transaction that read the Xi value. Now i am interested in finding out who read this value of Xi. 


















(Refer Slide Time: 46:53) 

 


Now if this transaction time stamp which has read is greater than TS, is current transaction time stamp, it means potentially what I will be doing we will be wrong in this particular case, because I am trying to produce a new version of this particular Xi. Xi plus one, but this is already read by somebody who is coming later than me which means that I will be violating, I should not be producing a value which already read by somebody who should be coming later than me show potentially in this particular case, transaction should be aborted. Transaction is aborted and restarted.

(Refer Slide Time: 47:29) 

 

If this condition is not true, what i am going to do is, if the condition is not true that nobody has read this value. If the condition is not true, what i am going to do is, if the condition is not true that nobody has else read the value, Xi is now taken and then a new version of this value will be produced which is going to be X (i plus 1) for which the read write time stamp will be said to TS and the read time stamp will also be set to TS that is the current transaction which has actually produced. This is how exactly a new version of the data item will be produced, if the earlier condition is satisfied. 

(Refer Slide Time: 48:07) 

 

What in effect is saying is, if i essentially look at when i am trying to write the latest value which I should be using to produce this value that I have here. What i am going to look at is, the write time stamp is suddenly less than the TS, then I look at the read time stamp of this and make sure that the read time stamp is no more than the Xi time stamp which i am setting and if this is correct, then i will basically proceed and produce the new version. The condition is here this is less than TS, is also less than TS in which case actually we produce the new version of the version of the value.
 
Now read operations are quite simple compared to this. Read will essentially what it will do is, it will look at read operation of the transaction on X, it will actually first look at all the sizes and make sure that Xi satisfying the RTS, read time stamp highest RTS is less than the TS, that means this is the latest value of Xi which is less than the TS which this time stamp can read, which this transaction can read. After this, the TS value will be set to the time stamp of the current read operation that means the Xi, the highest Xi will be taken always you read the latest value of Xi and when you reach that latest value of Xi, then make sure that you are actually setting the read time stamp of this to the current time stamp which means now the RTS the highest read time stamp of Xi will be equivalent to the current transaction which read the operation. That is how the read operations will be performed on the transaction. This explains, how the basic multi version protocols execute.
(Refer Slide Time: 50:15) 

 

In the case of multi version protocol, I did not keep any specific limit on the number of versions that are produced by the multi-version protocol. What this means is, the ‘I’ can be any number. Now often the problem will be that, i will be ending in highest cost in this particular case because i have to store a large number of data items because not just the current value, but the previous values all have to be stored in the database which means that the database storage overhead is going to be somewhat higher in this particular case and that is one of the tricky issues of the multi version protocol. 

That is where i actually paying a larger cost a more specific is actually the two version protocol which means that you keep only two versions of the data items. Two version protocol and we will look at these two version protocol extension to two phase locking. If you remember in the case of two phase locking, we actually use two locks: one is the read lock and other is the write lock. Read lock is basically a shared lock and write lock is the basically an exclusive lock. 














(Refer Slide Time: 51:57) 

 

Now what we have done is in the case of two phase locking every data item is either locked in the read lock or the write lock more and in the presence of a read lock another read lock can be allowed. There is a write lock another write lock another read lock will not be allowed. If you look at typically the matrix of what will be allowed, in what case you end up actually having, let us say this read write lock which is the current transaction holding, then you have a requested ones which are read or  write. These are the requested and these are the holding, currently holding.

Now it is possible at the end of the day, if it is basically a read lock in the presence of another read lock is yes. If there is a read lock on a data item X, you can also grant another read lock. If there is a read lock, this is going to be ‘No’. If there is a write lock, read will be disallowed. If there is a write, another write will be disallowed which means a data item on which a read clock is currently there, another transaction cannot ask for write lock, but it can ask for read lock since read is shared. 














(Refer Slide Time: 53:10) 

 

If the transaction is holding write lock on a particular data item, no longer you can ask for on particular data item. This is how two phase locking prevents on a particular data item things being pretend in an inconsistent way. Now, what we will do in the multi version extension of the protocol is we will introduce a new lock called the certified lock. Now what the certified lock will do is? It will allow in the phase of write also reads which means that you can read a previous value of the data item when somebody still holds the write lock, because it is still not return a value on the database. 

(Refer Slide Time: 53:59) 

 


For example: Remember X read lock data value item value of, this is the read lock that i have got and another transaction has actually got a write lock on this, but it still not written the value of this. This is what we will do here is, we will say this is the question mark here which means that it still not return the value. Now it is possible for me to actually allow this write lock. In the presence of write lock, a read lock has to be taken, but then when actually this transaction wants to write, it will upgrade this to certified lock. Now when a certified lock comes, it is going to write the value and both write and read value are going to be disallowed in the presence of a certified lock. Now this becomes a two value in an extension of two version multi version protocol because there are two versions. Old value which will be allowed to read in the presence of write lock, but when actually the value is return that is the time, it will be updated in the earlier it will become somebody can use. 

(Refer Slide Time: 55:06) 

 

So, this is basically two version extension of a multi version protocol that we see. Now to give better explanation what really we gain by doing this will again produce the matrix which we produce earlier. By saying that, there are three locks. Now read write and certified lock and similarly will have a read, write and certified lock. Now, in the presence of a read lock you can still grant a read lock. In the presence of write lock, you can still grant a write lock, yes but then certifies it is no. Now in the case of a somebody holding a read lock, you still will be able to grant a write lock here, because it can be on previous value, write value is basically no and all other cases it is going to be no, because in the presence of certified lock you can get any other locks. 






(Refer Slide Time: 56:12) 

 


Now the advantage of this is, when somebody has actually got a write lock, please understand intuitively what is the meaning of what we are trying to do here? what we are trying to do here is, when somebody is trying to hold a write lock, it is only intentional lock. I am intending to write. I am actually not writing. For example: if the transaction executes for a sufficiently long time, he need not actually block others from even reading. The others can read the previous value commit and go as long as i have not trying to commit. He commits again my old value. So it comes before me finishes everything. I do not need to like somebody trying to write in the railway reservation case. Takes the form, tries to fill the form, but while he is trying to fill, there is no point trying to block everybody else from trying to commit or trying to come before him and trying to finish his transaction that when he actually writes it and gives it, that is the time it is going to be blocked, not before that. This allows a higher level concurrency, but at the cost of actually an additional lock that i will be holding, when i actually reach that particular point.
 
This is an extension because i am only trying to have two values of a particular data item and allow with respect to this two values, who can come before me or after me that’s what exactly is achieved when you use the two phase locking extension to the two version protocol and that is how it actually gives more concurrency as compared to earlier two phase locking. This is the interesting extension to two phase locking using the concept of multi version protocol. What we are going to do in the next class is take a few examples for all the lectures i have done and do review questions on the topic we have done so far. 

Thank you. 

Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. 24

                                          Distributed Transaction Models

We have been considering transactions in the data executing on a single system. Implicitly this is assumption that we been making do we did not explicitly state that underline assumption of the model transaction model is that the transactions are executing on the node and the data is also fully resident on same node. What we mean by node is the node is a single computer system on which there is an operating system image and on which the database is running and now the transaction which are part of applications are all running on the same system, that’s the assumption which we are making when we are actually explaining all the transactions models.
 
Now consider an example where for example, railway reservation system where sitting in Chennai and your trying to make booking for train leaving out of Delhi which could mean that you are actually trying to access the database that is resident physically on a computer system in Delhi sitting in Chennai. Now, it could mean several things. It could mean just that you are actually trying to access the database to a physical connectivity which could be a telephone line, a lease line, a some kind of connectivity between your interface which could be just display interface to the system in Delhi which means that you are still physically working on the computer system that is located in Delhi that’s could be one model. But, this will not be one model which could be present when you work with systems which are dispersed and which are connected by wide area network, local area network or this is what we called as distributed system. 

When computer systems are connected by a network they could logically present a single node kind of abstraction for the end user. This is what we mean by the distributed system. Now when the data is distributed and the transactions can be executed on different nodes of the distributed system, we called the scenario as distributed transactions  that means we are no longer assuming that, the transactions execute on a single system computer system but they could be executed on different nodes of a distributed system. To explain this scenario what I will do is, I will take a very example of a banking system and explain how the scenario look like. 

Let us say that we have a bank in Chennai SBI branch. Now I have also a branch in let us say Mumbai again SBI branch could be a different bank also and it is possible that i have an account one here and have an account two here and i am actually doing a fund transfer from account one to account two, which means that i am going to do a debate here from account one by let us say two hundred rupees. Now it is possible for me to credit the following thing on to the other thing. Now this is data for account one is resident on node 1 which is basically node okay and the account to which is the other account data that is resident on node two which is in Mumbai. Now this means that physically the data is distributed in two locations one in Chennai branch and the other in Mumbai branch. 

Now if you assume that these two are connected by some kind of a network does not really matter what is the network but you can you can assume that physically these two nodes are connected by an underlined network. This network could be an atm network okay or could it be a fiber optic network or it could be variety of even a satellite network is present between these two nodes. So we are actually assuming that some kind of connectivity exist between these two nodes which affectively means that the information from one node to another node can be accessed or the data can be from  one node can be accessed. The thing in another node otherwise vice versa it is possible. 

(Refer Slide Time:  7:15)

 

Now supposed to these if you are actually seeing a single case where both account one and account two present on a single node. Both are present in Chennai. This is what the case we actually look at account one and account two and all that we had begin transaction end transaction as part of the transaction here and all this will be executed in the same branch of one system, that means physically this entire transaction  is executed on a single node. Now all the things that we so far have been talking belong to this scenario where both account one and account two are present on a single node and the entire transaction. Here TR is executed on a single node. So TR is executed on node which is the single node here and account one and account two are present on the node. So there is no remote access. This TR does not access any remote information both accounts are present on the same node. This is what ideally we are talking about. 





(Refer Slide Time: 9:00)

 

When we are talking about one single node of distributed system. As opposed to this, if you consider this particular case, we are talking about account 1 and debiting some other two hundred rupees from this account here and account 2 which is present on different node and adding 200 rupees here. Now imagine what could happen if these are executed on two different nodes. Now it is possible in other case, a power has failed in the whole thing is proceeding in the single system case it is possible that power could fail or other things could happen when the transaction is executing.

(Refer Slide Time: 9:25)

 


This is what we consider the acid properties of the transaction. Now if the transaction debiting and crediting from one account to another account you should ensure that the properties are completely preserved as acid properties are completely preserved as far as the transaction is concerned. Now in a single node now when the power fails the log records could be used to make sure that the transaction will always be an in consistent state. For example: for part of a transaction executed that is credit part is executed the debit part is not executed.
 
This can be figure out from the log logs of the transaction and either you can do undo or redo the transaction depending upon the state of the current transaction logs record that that is preserved. Now the scenario becomes quite complicated if you essentially look at distributed system. Now you imagine account 1 debiting is happening in one node account 2 crediting is happening in another node. Now it is possible that, part of the transaction is got executed part of the transaction is not executed for various other reasons. Firstly, to know that they both got executed itself is an important issue which is like two friends separated in two different locations try to ascertain whether the other friend exactly the other than excepting to do. 

(Refer Slide Time: 11:07)

 

It requires a few phone calls and also making sure if the phone rings, the person is present. So many other issues get involved when people are physically separated and they have to coordinate and do some activity, but on the other hand, if they are present in the same node, in the same room, it becomes lot more easier when they have to coordinate. The minute they are physically separated when phone rings, somebody does not pick up does it mean they are busy with something that is why he did not pick up the phone. You have to make assumptions really relating to behavior to other node with respect to your node and also if you put a telegram it could be a different mode of communication compared to ringing using a telephone and trying to reach him. So what is the mode of communication between these two systems between node 1 and node 2? What is the kind of communication primitives that are present between these two systems that also makes an important requirement when you study this model.

(Refer Slide Time: 12:10)

 

What you are trying to say is, if you assume the transactions execute on a multiple nodes of distributed system, the whole approach to concurrency control and commit protocols takes an extra dimension because you have to consider here the possibilities of distributed system. What can happen in distributed system? Node can fail, network can partition, network can fail with results in the system getting partition. For example: there is a temporary network failure between node one and node two, the node two may not be reachable from node one. So all these are becoming important.
 
Now the transactions take these in to account and they have to ensure the acid properties when these things happen that’s the most important thing. So we will actually consider this model as a distributed transaction model and we will study some concepts relating to distributed transaction model starting from this lecture. 











 

(Refer Slide Time: 13:46)

 


Now I will explain at very high level, the distributed transaction model and get start more details of some of more transaction models to explain the essential difference between centralized system database system and distributed systems with respect to the transaction models. We will take a very simple example and illustrate with respect to the simple example how exactly we can build a distributed transaction model on top of distributed database systems. Now continuing the discussion we had on credit debit transaction, what we will see essentially in the case of distributed transaction is a transaction TR which we have seen there will have now, two portions which is the debit portion of the transaction and the credit portion of the transaction. Now, debit portion of the transaction is executing on a different account 1 which is on a node 1 which is Chennai node. This is resident on a different node and this resident on node 2. Now we call this in credit and debit two sub transactions of this root transactions TR. Now TR becomes the root transaction.
 













(Refer Slide Time: 15:10)

 

So there is a root transaction which is the complete transaction. Now what this root transaction will do is it will actually spawn two agents which are nothing but two sub transactions agent 1 and agent 2. This agent 1 is actually sub transaction of the main transaction agent 2 is another sub transaction 2. Now a single transaction is split into two sub transactions. So that, these two sub transactions are executed on two different nodes of the system. For example: the root is going to be on one node, agent 1 is going to be in another node. Agent 2 is going to be in another node. Now agent 1 is going to execute the debit portion of the transaction and agent 2 will execute only the credit portion of the transaction that means, it will access account one here and make sure in the account two hundred rupees is debited and here it will access account two and make sure two hundred is added here. Now while doing this, all the acid properties have to be preserved as part of this which means that atomicity, concurrency, isolation, durability all the acid properties independently have to be preserved as part of this particular exercise. 















(Refer Slide Time: 16:58)

 

You do not want if power goes off in the middle of transaction you do not want to be unsure what really happen to your account on node one. It should be the preserver the property that two hundred rupees detected by this transaction atomically you know when accessing this account, other transactions are not allowed to access this account. When it is modifying, somebody not to allow the results before it finish its computation. Now after finish it executing it is completely preserved on the database. All these in an atomicity concurrency, isolation, durability properties. Now to achieve this issue at the highest level, this is the model what we have is the agent. The root agent spawning other agents okay agent one and agent one on different nodes. These agent 1 and 2 are responsible for executing the transactions on the local node and together they form a complete transaction. 

















(Refer Slide Time 18:19)

 

For example: additionally we have to worry about if A 2 alone is done, A 1 is not done, 
then again we have the inconsistency because A 1 is debit, A 1 is debit A 2 is credit. So you do not want the debit being done, not credit or vice versa or both have to be done simultaneously. When this is done, this also done this is also not done this is also done that’s what we mean by atomicity either the whole thing is executed or none of its executed. Now to ensure that this kind of agreement is reached between the various sub portions or sub transactions that are executing on different nodes. We need what is called an exercise a module that i actually does this which make sure that transaction properties which includes the commit property that all of them either commit or none of them commit. 
 
The transaction the transaction end all these to preserve these we will have an all nodes manager running which is called the distributed transaction manager. This is the responsibility of the distributed transaction manager. This DTM stands for distributed transaction manager distributed transaction manager. Now that DTM on every node ensures that the transaction properties are obtained on each one of these nodes when they are participating in the execution of the transaction. Now each local execution which needs to again maintain transaction properties will come under what we will see in LTM, LTM is nothing but the local transaction manager and if you see, LTM stands for local transaction manager. 








(Refer Slide Time: 20:24)

 

If you see exactly what we have said in the earlier slide, the credit and debit here which are executed by agent 1 and agent 2 needs to be preserved by the transaction property and this is obtained by the acid by the preserved acid property for debit transaction and credit credit transaction. This is ensured for agent 1 and agent 2 by the LTM. LTM preservers the logs and all the related things done in the centralized database context. The LTM make sure al the things are executed by the agent or as per the acid properties and that is what is achieved by actually having the LTM. 

(Refer Slide Time: 21:41)

 


On each of the nodes to explain this is the model, this is the distributed transaction model. What you see at the highest level to just recap what we have been telling along the complete transaction. Now become the root transaction as part of the root transaction when you will have the sub transaction which needs to be started on different nodes of the distributed system. Now here agent1 gets started node 1 and agent 2 started on node 2, agent 1 indicates a sub transaction the root transaction. Agent 2 indicates the other two sub transaction of the root transaction.
 
Now for staring this agents and also making sure that the coordinate among themselves when they are committing, corresponding properties are achieved by the distributed transaction manager which is called the DTM. Now the DTM is responsible for preserving the transaction properties across the various nodes. Now, when the agent starts on a local machine whatever it gets executed in the local node, it preserve by this local transaction manager that means the local transaction manager is responsible for ensuring whatever is executed in the local node obeys the transactional properties and this is the complete model of the distributed transaction model. 

(Refer Slide Time: 23:17)

 

What we are going to look at is, we are going to look at with respect to this model, how the concurrency control and model the commit protocols are going to be executed in this context and suddenly this becomes little more complicated than what we have studied in this centralized scenario. For example: commit in the centralized scenario is just writing the logs and making sure all the values modified by the transactions are return back on to the database and then if your using two phase locking, then make sure that the locks are released so that other transaction can make use of now the data items modified by the current transaction. So commit becomes a simple exercise of just writing the logs and writing the modified values back on to the database.
 
Now if you consider the distributed scenario where there are multiple agents running on the multiple nodes of the distributed system, the commit no longer becomes that simple. Now you need to worry about whether node 1 and node 2. For example: some reason node 2 wants to commit each part of its transaction and then you need to know node1 should roll back. You need some kind of agreement to be reached among the various nodes before they actually commit the values on to the database system. So this is basically called the commit protocol, commit protocol is the most specialized protocol than a general agreement problem.
 
Now all that you have to agree in the case of commit protocols is either to commit or abort. There is no other agreement that you try to reach in the case of commit protocol because one could mean i want to agree to commit my part of the transaction. Zero could mean that i could abort a part of the transaction. So all the nodes at the end of this transaction that agents involved in distributed transaction have to agree for one or zero depending on one actually means that all together to commit zero means they do not want to commit. So we typically we going to look at commit protocols and study various commit protocols for achieving consciences. This is the first part of the distributed transaction model, what kind of commit protocols exist and how do they work with respect to various scenario like node failure, network failure, partitions.  

(Refer Slide Time: 26:15)

 









So we are going to look at various scenarios what could happen in the distributed system with respect to this how these commit protocols really work that’s going to be the first part of the lecture, going to focus on the commit protocols. We are going to essentially look at two kinds of commit protocols. One is the two phase commit protocol which has the name suggest does it two phases that means the first phase which participations are prepared, in the second phase actual commitment take place and that is what we  mean by the two phase commit protocol and this two phase commit protocol naturally integrates

(Refer Slide Time: 26:59)

 

with the two phase locking protocol that we have studied earlier in the case of a centralized database system. So we will study the two phase commit protocol and study extension of the two phase commit protocol in to what we see as the three phase commit protocol. Now the idea of this three phase commit protocol is when sudden things happen in the two phase protocol which we are going to see the protocol will get lock that means the protocol any way to proceed further till the recovery take place in other words certain kind of failures occur, the two phase commit actually locks the protocol blocks the system whereas actually modifying it in three phase commit protocol, we will able to avoid the wait for. 











(Refer Slide Time: 28:24)

 

It actually becomes the wait free protocol in some sense, when some kind of failure occurs wait free protocol are extremely important because when failures occurs if the system is not blocked, because of the failure it is a good property of the system, because it allows the system to actually go forward and not blocked by the failure. So wait free protocols are very interesting and important in distributed systems. Now from two phase to three phase commit protocol, what we see is, you can avoid certain kind of failures and make the system resilient for this kind of failures. 

The system becomes more robust under three phase commit protocol as opposed to two phase commit protocol. We are going to study the two phase commit protocol in detail and then going to study the three phase commit protocol in the context of distributed transactions to start with. In this particular class, we are going to focus on two phase commit protocol and study the two phase commit protocol in detail and see what under circumstances two phase commit protocol works correctly. Now after looking at the commit protocols, they are going to look at concurrency control protocol in the context of distributed transactions, 












(Refer Slide Time: 29:38)

 

is going to be later part of the talk we will focus on the concurrency control mechanism as applied to the distributed transaction systems. Now the rest of the lecture from this point will concentrate on the two phase commit protocol and we will try to explain in  detail the two phase commit protocol. Now let us look what kind of scenario will be required when you actually looking at the two phase commit protocol. Now to explain what really commit protocol means?

(Refer Slide Time: 30:14)

 


We have actually taken a root transaction which actually started executing on this is basically the begin transaction. This is the begin transaction. Now I have basically the agent 1 which is the debit portion of the transaction. This is debit on node 1 debit on node 1. This is basically the sub transactions agent 1 agent 2 is the credit on node 2. This is particularly operating on account 1 and this is operating on account 2. Assume that some point of time, this is the end transaction. Now when the actual agent 1 and agent 2 are started on different nodes of the distributed system and they start executing on the two different nodes at some point of time when i reach the end of the transaction. I am going to look at this part and at this point of time i have to decide whether i will be committing my transaction. 
 
I have reached this point now and when i reach this point, i need to understand now whether agent 1 and agent 2 are willing to now go with writing or doing the debit and credit completely. For various reasons its possible that agent 1 or agent 2 may not be willing to complete part of the transaction. One reason could be account 1 is not present on node 1 or the account has been closed for various reasons which means that account 1 is not present on node 1 which means the debit cannot produce logically further in which case the agent 1 whatever agent 2 wants to do agent 1 will say i am going to abort my part of the transactions because i cant complete my part of the deal.

(Refer Slide Time: 32:33)

 

I cannot finish debit transactions on my node, because account 1 has been closed. The other case is account 1 have sufficient funds. Let us say it has no balance and you try to withdraw two hundred rupees from the account, when there is no balance in the account. Again agent is to say look i am not going to commit my part of the transaction because there is no way I can commit this transactions. So in all these cases both nodes have to agree both agent 1 and agent 2 have to agree to commit their transaction for this whole thing to go through. 

(Refer Slide Time: 33:13)

 

Otherwise, there is no point to trying through have one part of the transaction executed and the other part not been executed. Now to ascertain this, this commit protocol gets started ideally at the end transaction. For example: when you reach the end of the transaction, you have to actually now start this commit protocol to ascertain whether this whole transaction can be committed or not which means that at this point of time, all the agents involved in the transaction have to participate in this commit protocol to ensure whether they are committing the transaction or not committing the transaction that is the part of the commit protocol.

(Refer Slide Time: 33:58)

 

Now let me explain this by taking what we actually call as a coordinator of the transaction, which means that we will first fix a coordinator for the commit protocol which means that there is one single node, there is a node in the distributed system that is going to act as a coordinator for the commit protocol and ideally this is the one where the transaction has been submitted which will try to coordinate transaction has been that’s what we mean by coordinator. Now all the agents where this transaction is being executed we call them actually participants. They are all participating in the commit protocol. So they are called participants. So all agents where the sub transaction is executed is called the participants where call the sub transactions is executed is called the participants. 

(Refer Slide Time: 35:37)

 

So we have actually two things in the two phase commit. One the first coordinator and number of participants. Now the responsibility of the coordinator is to initiate the commit protocol and make sure some response from all the participants and based on to that the coordinator take a decision again inform all the participants and properly terminate the commit protocol at the end of the execution. So this is what we will call as a coordinator. Each participant has a responsibility for its own local transaction it should replying on behalf of the local agent it should ensure that certain things are done by the local agent before the participant reply back to the coordinator with a reply. Now we are going look at full detail what are step that we will be done by the coordinator and what are the steps done by the coordinator participant in the case of two phase commit protocol. 







(Refer Slide Time: 36:37)

 

Now what we do in the case of two phase commit protocol is the coordinator initiates the commit protocol. At the end of the transaction he initiates the commit protocol. By what we mean by imitating is he can send certain messages two various participants and make sure he get some response from this participant when he is ensuring when he is actually executing this commit protocol. Now this is the responsibility of the coordinator, to start the commit protocol at the end of the transaction. This he does by sending what he is called a prepare message to the entire prepared message. This message is a special message which is called the prepared message. He prepares the participants for this prepare message is sent to all the participants. 

(Refer Slide Time: 38:09)

 
Now after sending the prepare message, the coordinator enters what we call as a the wait state because still now decided on the final outcome of this particular weight state till all replies are received or some of this were basically the problem come. If there is a problem in terms of some participants not responding because there is a failure either of the network. 

(Refer Slide Time: 38:42)

 

or the node then they are not going to respond if they do not respond what is going to happen in the coordinator. So the coordinator basically puts a or time out period actually starts a time out period immediately after sending the prepare message. Now he will wait for the time out period and if replies do not come within this time out period he is going to decide based on what replies he got because he has not got the reply.

















(Refer Slide Time: 39:18)

 


He will ensure that you know based on this, he will make a decision which actually make sure that even in that case we can still make a decision but probably he might make a decision on the negative side saying that i will abort which is the safer decision to make because when the participant comes at lateral point of time, it is always possible to ask the participants to abort rather than to commit. So you will wait for a time out period and after the time out period, he is going to make a decision relating to whether he should be committing or abort, always when time out has reached and replies are not come is going to make a decision relating to abort of the transactions rather than the commit of the transaction.


















(Refer Slide Time: 40:16)

 

Now as far as the participant is concerned, participant actually decides now waits for the commit protocol that means to start with is in an undecided state wait for prepare message from the coordinator. Now when you get a prepared message from the coordinator. Now we have to apply the two things which participant can do, participant can actually reply with a ready message. 

(Refer Slide Time: 41: 04)

 

What ready message means is that the participant is willing to commit his part of a transaction that’s what actually means by the ready message. The ready message indicates that the participant is willing to commit his transaction that means he has executed successfully his part of the transaction and he is willing to actually commit his sub transaction that’s what it mean by the ready message. Now, before sending the ready message, if he is willing and before sending the ready message, he needs to actually ensure that before sending the ready message what should be he doing? He should be ensuring that all the logs are return because if he does not write the locks and he replies with the ready message.

(Refer Slide Time: 41:55)

 

The coordinator assumes that whatever may happen later, the participant is willing to go with the commitment of the transaction. Tomorrow you cannot come back and say look i am not going to commit. Once you have replied with ready message, you are committing your self to commit this transaction. So it is important for you write all the logs and also if you are using locking protocol, you are not going to release the locks till you know the decision of the coordinator because if the coordinator now says please come in, you need to write all the values on to the database and then release your locks. So before sending the ready message, the participant should write all the required locks. Participant writes the required locks. 












(Refer Slide Time: 43:16)

 

 	
Now, if you carefully observe all this writing the locks preserving all the different properties of the transactions is achieved by the local transaction manager. LTM ensures that all this local transactions properties are maintained using the LTM. Now once the participant it’s a ready message the coordinator can now decide if he receives all the ready messages he can now decide, what needs to be done. Up to this point, we call this is phase 1 that means phase 1 is preparing the participants for commit. 

(Refer Slide Time: 43:41)

 


You are preparing this is actually prepared phase. Actually, you are preparing the participants for committing only in the next phase, phase 2 you actually commit.
 
(Refer Slide Time: 43:52)

 

So there are two separate phases, thats why called two phase commit protocol. In the first phase, you are preparing all the participants for committing. In the second phase you are actually committing the transaction. Now it is possible in phase 1, you reply back for some reason the participant is not willing to commit because as we explained earlier, there was no sufficient fund. All these could be reasons while the participant may not to commit his transactions. In all these cases, what the participant does is, he actually sends an abort answer message which means that he sense saying that i am aborting. His answer for prepare is abort answer message. 

















(Refer Slide Time: 44:46)

 

When a participant is actually giving an abort message, participant is not willing to commit his part as the transaction, participant is not willing to commit not willing to commit when coordinator receives this abort answer message, he will be forced to take only in the second phase we are going to take that commit decision. So phase 2 once the coordinator enters phase 2, he is actually making the decision on commit. He is deciding now on decision commit he is made. Now what are the different decisions? It is possible in the time out period coordinator receive all the coordinator received all ready messages from all the participants all the ready messages from all the participants.

(Refer Slide Time: 46:01)

 

Then now, the coordinator in this particular case, we will take a decision to commit the transaction. That is obvious because all the participants in the transaction willing to all the agent transaction are willing to commit their part of the transaction. Hence it is logical for the coordinator to take the decision to commit in the entire transaction. But before actually we taking the commit decision okay, before sending the commit message the agent has coordinator has to write decision on a stable storage. So that whatever may happen later, the coordinator can recover back see what is the decision that was made by him, with respect to this particular transaction that is the logical thing which he does. 

(Refer Slide Time: 47:10)

 

So, we except in this particular case the coordinator receives all ready message commit decision is taken commit message is sent to commit message is sent to all participants. Before sending the commit message, before sending these are the steps that the coordinator has to follow before sending the commit message, he needs to actually write the commit log. The commit message he needs to write a stable log is written a stable log. This is important because whatever may happen to the coordinator, he should be able to now tell the rest of the participants at later point of time, what was the decision that was taken on this particular transaction. 











(Refer Slide Time: 48:03)

 


When the participant receives now, at the second phase, phase 2 when the participant receives participant receives the commit message receives CM message commit message, the transaction is committed and an acknowledgement is sent to the transaction is committed and the ack is sent acknowledgement is sent an ack is sent is sent to the coordinator

(Refer Slide Time: 48:50)

 

Now the coordinator, when he receives in the second phase all the acknowledgement the coordinator on receiving the coordinator on receiving on receiving all ack that is acknowledgement will write complete log will write the complete log. Now at this point of this time the transaction is completed and it can be closed, successfully by the coordinator. Now there could be various cases in between. 

(Refer Slide Time: 49:41)

 


This was talking about only a successful completion on the transaction by the coordinator as it is proceeding from one phase to another phase. To recap what we have done phase 1, this is actually in the phase 1 coordinator imitates the commit protocol and here he actually sends the prepare message is sent to the all participants. This is in the phase 1 first action of the coordinator and enters in to the wait state for a time out period for replies from all the participants. Now the participant who is waiting for the prepared message can actually respond with the ready message. Before sending the ready message, participants make sure he write all the stable logs and enters into a now a wait state of the decision of the coordinator.  














(Refer Slide Time: 50:28)

 

Now the coordinator after this point enter phase two the phase two is entered by the coordinator when he receives all the replies from the no all the ready messages from the participant or a time out period as a write. Now in either case it takes a decision if all the messages are received ready messages are received he commits the transactions writes the commit log and sends the commit messages to all the participants. Now when the participant in the second phase and receive the commit message, they actually write all the transactions are committed and an acknowledgement is sent back to the coordinator. 

(Refer Slide Time: 51:25)

 


So that now he can write the complete log and close the transaction. Now it is possible that in the first phase. We did not discuss the case were its possible for the participant to reply with an abort message. Now when the participant replies with an abort message or coordinator waits for an time out period, in either of its cases the coordinator is going to take a abort decision and this will be communicated to all the participants and when receive the abort decision, the participants are abort the transaction. This is what actually happens with the two phase commit.

Now before we going to in details of looking at what are the different things can happen what are the different failures scenario that can be presented in this case and how this two phase protocol is resilient that kind of failure. We will try first looking  at what was the state transition diagram for the coordinator and the participant, that explains this whole scenario in lot more detail and after that we are going to look in more details, how the two phase commit protocol is resilient to certain kind of failures when the commit protocols is executed. 

Now, to just explain this, what we are going to take is, this is actually the coordinator state transition diagram. Coordinator state transition diagram. I am writing the state diagram of the coordinator to start with. To start with, the coordinator is based on the initial state and from the initial state what the coordinator does is he actually sends prepare message to all the participants. So this actually we will have some level of undecided state. This is the state where the decision was not taken by the enters its wait state which is the undecided state. Now this is basically the prepare message is sent by the coordinator and he is waiting for the decisions. Now its possible that actually he waits from this state possible for the coordinator from this undecided state that he receives the all the ready messages this is what we actually seen. Now when he gets all the ready messages, he is going to take when the input to the coordinator is ready messages.

(Refer Slide Time: 54:27)

 
All the ready messages have been sent to the coordinator, then you are going to get the commit command commit message from the coordinator and the state in which actually the coordinator reaches the state the commit state. Now it is possible that the coordinator actually got abort answer message from the participants when he got an actually abort answer message and he is going to taken an abort command message that means is going to send an abort command to all the participants and that is typically when an abort decision is taken by the coordinator. Now it is possible that, when from the initial state the coordinator can reach the abort decision if there are no sufficient replies, that is the time out period when a time out period is reached.

When the time out period is reached its possible for the actually by sending the prepare message and gets to the undecided state. So strictly speaking, this arrow should come from the undecided state here which means that the this state from which this arrow will come, there is a time out period and after the time out period, the coordinator has to take an abort command message that he waits for the time out period and after the time out period he actually takes the decision to abort the message.

(Refer Slide Time: 56:09)

 

Now if you look at the participants’ state transition diagram, it looks something like this. To start with actually the participant is in initial state. Now the participants receives the prepare message and the answers with the ready message then he basically enters the ready state here. Now from the ready state, it is possible for the participant to reach when he reach when he receives the command message, he could be receiving the he could be sending an acknowledgement message and could be reaching the commit state. If he actually receives an abort message, abort command message, again he will acknowledge, but he will reach the abort state. 



(Refer Slide Time: 57:09)

 

It is possible for the participant from the initial stage itself. By actually responding with an abort answer message he could be reaching this state. He actually sends an abort answer message, with which he actually reaches the abort message. 

(Refer Slide Time: 57:27)

 

This sort of summarizes what we are doing now in two phase commit protocol. What we are going to do in the next class is, take these two transition diagram and then explain how the two phase commit protocol is resilient for failures. 

Database management system
Prof. D. Janakiram
Department of Computer Science and Engineering
        Indian Institute of Technology, Madras

Lecture – 25
 Basic 2-phase & 3-phase Commit protocol

In the last lecture, we have just looked at basic two phase commit protocol. Just stopped at that point where we just had the basic protocol and how it works. Actually two parts are there. One part is the participant protocol and other is the coordinator protocol. How the coordinator protocol working is, the coordinators starts with sending a prepare message and once he sends a prepare message from the initial state actually he reaches the state where he is now undecided. 

So the prepare message is sent to all the participants. Once the prepare message has been received by the participants, they will reply back with the ready message to the coordinator. If the coordinator actually receives all the ready messages, then he will take the commit and he will send the commit message then the coordinator will reach the commit state and if he receives any of the participant actually says he wants to abort that means for the prepare message he gives an abort answer message, then you are going to take an actually abort command message and this state. 

Actually the coordinator reaches the abort state, it also possible for the coordinator to reach the abort state. If he times out, that means he does not receive the message from the participant, then one of the participant failed. Since one of the participants is failed you are bound to actually taken abort message. Right this is the basic part of the coordinator. As far as the participant is concerned there is each participant site for the two phase commit is concerned, each participant start with an initial state now he is going to get a prepare message. In response to the prepare message, he can basically give a ready message. 

Now he gives the ready message, he is in the basically ready state then he is in the ready state. Now wait for the coordinator’s decision. If the coordinator actually gives the abort command message, then you are going to acknowledge and reach the abort state. Even after actually you reply with the ready message, it is possible for coordinator can still take a abort decision because in that particular case, the other participants may not have replied back unless he get all the ready message, he cannot commit the transaction. 

Now in the other case, all the participants replied with ready message. The coordinator is going to send the commit message in which case the participant actually commits, then sends an acknowledgement that means he has written all the values on to the database. This is the basic two phase commit protocol that we have seen in the last lecture and were we have stopped is, just explain this protocol and then said that what happens in terms of different failure scenarios, we thought that we looked at in the last class that’s why we stopped in the last class. 
Now that several things can happen in this basic protocol. For example: coordinator can fail in the first phase or second phase participant can fail in the first phase or in second phase network could fail which means that its possible for the various nodes to get partition, some nodes in one partition and some nodes in another partition. So, all these are possible scenarios as far as this protocol is concerned. Now one of the things we looked at what are the cases were this protocol is resilient which means that which are the failure scenarios were this will still work correctly. 

(Refer Slide Time 5.58)
 
 

Now let us first take the case were you know we are going to look at different cases of failures. Now the first kind of failure is a participant fails in phase 1. What happen if the participant fails in the two phase commit? We are talking about the first phase. 

















(Refer Slide Time 6.39)

 


Now what happens in this case is the coordinator will time out and then once the coordinator times out is going to take the decision of aborting or sending an abort message here. For example: here the coordinator is going to timeout and he is going to in that response is going to take an abort message, abort command message in the minute actually one of the participants has not replied. 

(Refer Slide Time: 7:02)

 

So, i think in that particular case, the coordinator will time out and then and take an abort decision. Co-coordinator will timeout. As a result he is going to take an abort command message that’s what going to happen when participant fails. So obviously two phase commit is resilient for participant failures in the first phase. Now what happens if the participant fails in the second phase of the Participant fails in the second phase that means in the first phase, the participant has replied with the ready message. It means that he actually replied the ready message and now he is waiting for the coordinator’s decision. Coordinator can take either an abort decision or a commit decision. 

(Refer Slide Time: 08:09)

 

Now, if you say that the coordinators have taken an abort decision or commit decision? After recovery you have to check with the coordinator and accordingly you have to do. As far as you have concerned, you have replied with the ready message that means you actually now all the logs and you cant release any of the locks or any of the resources till such time you know the decision of the coordinator. Right in this particular case, you are going to wait for the decision of the wait for the decision. 

Now since you are waiting for the decision, since you have failed for the decision all that you have to do is after recovery that means after you recover from failure. After recovery, the participant should check with the coordinator. Participant checks with the coordinator for the decision. What happened about the transaction he has to check with the coordinator and accordingly he has to terminate the transaction that means based on what the coordinator tells now. Coordinator need not wait for the participant because it already replied with replied message.







(Refer Slide Time: 9:19)

 

So, in this particular case, the coordinator is feed to choose a decision depending on read message or if he get all the ready messages then he take the decision to commit the message, commit the transaction in which case the commit decision would have been taken. So, that participant has to find out what happens to this transaction he has to do accordingly. One of the things is neither other participants will be blocked because of this participant failure the coordinator will be blocked. 

All the coordinator has to do is to read the decision when the participant comes back, he has to tell the participant about his decision and when he get the acknowledgement from participant he is going to write the final log on this transaction, it has been successfully completed. That’s the thing that needs to be done in this particular case. Now, it is possible also for the failures to happen for the coordinator at different phases and also possible that both coordinator and participant also fail. Need not be just failures of participant alone coordinator alone. But right now we looked at what happens if the participant fails, one or more participants fail in the first phase and in the second phase. 













(Refer Slide Time: 10:30)

 

So, if you want to look at what happens to the coordinator failure? Suddenly you have to now understand in which phase the coordinator has failed. Again we can see the case were coordinator fail in the first phase if the coordinator fails in the first phase, that means he sends the prepare message and after that has failed. He has not actually taken any from this transaction.

(Refer Slide Time: 11:36)

 

Then he fails before the ready messages actually arrived at the coordinator. So the coordinator has not reached the decision. But after sending the prepare message if he fail the coordinator has failed. Now the way to recover is simple. All that the participants have to do is, they have to elect the new coordinator and then restart the protocol. That means the new coordinator will send the prepare message and correspondingly he will take the decision about the transaction by running by the two phase commit protocol. So in this particular case, a new coordinator has elected assuming that there are no failures, new coordinator is elected here and then the transaction is restarted. 

(Refer Slide Time 12.12)

 

Two phase commit of the transaction is restarted protocol and then based on that the commit protocol will be executed based on that. One of the assumptions that is making here is that all the participants are live.

(Refer Slide Time 12.24)

 
They are not dead but if they are if it is not the case also, what will happen is, the new coordinator will be elected and he will be time out when he starts the protocol. So, he will make an abort decision. So in that sense, the protocol become reentered means any number of times, any number fails failures is occurred. For example: you elected a new coordinator. Now that coordinator will fail again in the first phase again you are going to again have the election algorithm to elect a new coordinator. This is basically protocol is very simple. 

Now in the other case, if the coordinator actually fails in the second phase. This is an interesting problem. Actually the fact you are saying that the coordinator actually fail in the second phase means that taken a decision. Right now, it means that decision has been taken decision of aborting or committing has been taken. Now one of the conditions that can happen here is as this decision known by another partition which we can say now there is a case were at least one participant knows the decision coordinator before he fail. 

(Refer Slide Time: 13:53)

 


So, the scenario one is here one of the participants received. One of the live participants, which means that he is actually live and then he actually receives the coordinator of decision live participants. Received the decision which actually means that all that you need to do in this particular case is, since the decision is known the same decision can be conveyed to everybody. 







(Refer Slide Time 14.27)

 

If it is a commit decision everybody will commit, if it is an abort decision everybody will abort. Right it is very simple case where the decision of the coordinator is already known the coordinator fails some participant does not receives the decision.

(Refer Slide Time 14.47)

 

All that they have to do now is, find out if any of the participants has received the decision of the coordinator. If they have then will become simple scenario were that decision can be implemented by other participant also. In this particular case, it becomes non-blocking. That means that normal that means is nobody actually block. This is very important you are not waiting for somebody to recover back right.
(Refer Slide Time: 15:25)

 


For example: if shooting is happening, our recording person decides to go out for tea or something, and then we are all blocked. Since the time backs know that is a blocking protocol. You do not want to think blocked. You wont actually even it goes and put in automatic modes and off then we are not blocked. We can continue our job. Similar way, even one of the coordinators fails and it still wait for the participant to recover back from the failure. It is a very nice thing. It is a very simple case that is how we have to the model of the protocol we do not want to know any other participants suffer. Because of this failure suffer means have to wait now is you do not know when that recover is going to happen is you get indefinitely blocked if such a thing happen. So one of all the cases we have seen so far two phase commit protocol is non-blocked. It does not block anybody. 
















(Refer Slide Time: 16:44)

 

Now there is an interesting case that is going to come now where we can see the case were the scenario 2 were the coordinator fail. None of the live participants know about the decision of the coordinator. But again there going to be two sub cases here. Now we can say decision of the coordinator is not known. In this case, it is possible that you know all the participants are live, that means there is no other participant failure. That means only coordinator has failed. The decision of the coordinator is not known. But all the case where this is only first case is not the second case.

First case, were all the participants are live that means there is no participant failure. All the participants are live. In this particular case, since all the participants are live, a simple thing they can do is they can restart the two phase commit protocol by electing a new coordinator. In this particular case, you will elect a new coordinator and restart the two phase commit protocol. 















(Refer Slide Time 17.54)

 

(Refer Slide Time 18.20)

 

You are going to see is, the case where if you say that the two phase commit the participant also failed along with the coordinator right. So, there is not only a coordinator failure, but there is a participant failure which means that, now the rest of the people the interesting case is here. 





(Refer Slide Time 18.51)

 

Imagine that the coordinator took a decision of committing and that is only known to the participant and he actually wrote all the values of the database and then he actually fail. Now, if the remaining participant elect the leader, coordinator and then terminate the protocol, it might contradict. If the earlier coordinator already taken a decision and that is know to this partition. There is no way to recover back from this. 

So basically this is the case were all the other participants in this particular case, all the live participants are blocked. All the live participants are blocked till such time the coordinator recovers. You cannot do anything except to wait for the coordinator to recover wait for coordinator to recover. Now often a coordinator failure also means a participant failure, because the transactions were it is started normally coordinator is one of the participant sides.
















(Refer Slide Time 20.09)

 

For example: if you basically look at how the transaction is started one of the sub transactions were it is getting executed.  He also takes up the responsibility of being a coordinator. Normally coordinator failure will be equivalent to a participant failure and that is the scenario were two phase commit will not be able to recover back and this is an important case. Normally you do not have a coordinator failure. A coordinator failure normally means a participant failure. Only thing is, if it is fail in the first phase then you are bit lucky because in the particular case, the remaining participant can elect the coordinator can make a decision but if you actually failed.

In the second phase, phase two after actually sending the ready message, you discover that the coordinator actually fail you will be in trouble because you would not know whether the coordinator is taken a decision or not take the decision. So that is the case were the participant is forced to wait till the coordinator recovers back from the failure. Now have explained for this, what we are going to see is how this protocol gets modified to also allow protocol to recover in the case where the coordinator fails in the second phase along with the participant and that modification is what we see as the three phase commit protocol.
 
What the three phase commit protocol does is, intuitively to understand what the three phase commit protocol does is, when the coordinator actually fail you would not know what the state it state is, and this is very important distributed system. This is the typical problem you will face in the distributed system. You need to know the state of other node when things have actually failed or when the system crash. If you know the state of the system is easy for you to recover back.

But now if you do not know the state of the system, then you are in trouble recovering back from that failure of the particular system. Now, in this particular case when the coordinator actually fails you won’t know whether it is commit decision or an abort decision. Now what do you want to decide now is or want to see is, what is the state in which it is? To get out of this ambiguity, for introducing a three phase that means the coordinator will not directly to get in to the phase of committing. If you carefully observe the problem is in terms of moving um coordinator moving directly when it gets the ready message in to the commit state. If you actually record in between a state where the coordinator says that, now I have received all the ready message. 

(Refer Slide Time 23.32)

 


Now i am prepared to commit, it is still not committed. But, then now you introduce a prepare to the commit state for the coordinator which means that now you know whether the coordinator has prepared to reach the commit state or not. If it is not reached to prepare commit state, then no harm done because he failed in the second phase but only in the third phase actual commitment would have taken place.
 
Right basically you need to know the state of the coordinator when he actually die or  when he fails. So in that you remove that you need to introduce a third phase that’s were you basically look at the three phase commit protocol. What were we are going to do now is see how it get modified for the three phase commit protocol or how the three phase commit protocol works and also look at what kind of failure scenarios the three phase commit protocol can tolerate or it is resilient to what kind of failure. 








(Refer Slide Time 24.49)

 


Now what we are going to do is, for both the participant as well as the coordinator will introduce a new state.  A new state were they have to first get in to the before commit state or prepared to commit state before they actually commit. They cannot directly get in to the commit state from the ready state. Now the coordinator part is going to look at something like this will start with an initial state and now as usual send a prepare message to all the sides. Then i basically reach the undecided state here because I have still not decided on the state. Now if i basically get all the ready messages then I will actually enter what i call as a before commit state i have still not committed.
 
So, i basically reads a before commit state here. Now once i reached the state before commit state and i receive acknowledgement from all the participants. Then i take the commit message, that means i get an all those participants then i will issue the commit message and then i reach the commit state after receiving ok from all the participants. Now it is possible for me for various reasons to move from before commit state to an abort state. If i do not receive okay from all the sides, i can still end up in an abort state.
 
As usual i can also come to the abort state if there is an abort acknowledgement message from one of the participant. Then in response to this, as usual will take the abort command message. It also possible to a timeout message i might reach the abort message. Now this is the diagram for the coordinator. State diagram for the coordinator look something like this participant state diagram. The three phase commit would look something like this. Participant will be initial in the initial state when it receives the prepare message it will reach the ready state, if it is willing to commit the transaction in that case the participant will reach the ready state. In case, it is not willing to commit the message. Then it will answer with an abort answer message and will reach the abort state. 


From the ready state also, it is possible for the participant to reach the abort state. It receives the abort command from the coordinator which is possible for the participant to actually reach this state. If there is an abort command message and then the participant reaches that state through the ready state. If the other participants are not willing to commit, the coordinator can take the decision to abort the transaction. In which case, participant will move from the debit state to abort state. If all the participants are ready to commit the transaction which means that, all of them have actually responded ready message then it is for possible for the coordinator to take a commit decision.
 
In which case in the three phase commit he will initially enter the state called the prepare to commit state, which means that he will prepare to commit message to the participant. 
When the participant receives the prepare to commit message, he will reach prepare to commit state. Now he reaches the prepare to commit state is not still commit the transaction is waiting for the final decision coordinator.

When the coordinator receives for all prepared to commit messages, then he will actually issue the commit message. It is a commit instruction in which case the participant will reach the commit state. This additional state is needed because is possible that if the coordinator fails know after one coordinator and one participant fails after reaching the prepare commit state is still possible for the participant to elect a new coordinator in which case it possible that decision to abort has been taken by the new coordinator. In which case after coming back after recovery from failure, it is possible for the participant to move from prepare to commit abort state. Now this is the participant state diagram. We looked at the actual failure scenarios here. 

(Refer Slide Time 31.06)

 


Now the case where the failures can occur as usual participant failing in the first phase second phase are similar. Similarly the coordinator failing in the first phase is also similar. Only the case were we need to look at the second case where the coordinator actually fails that is what we recorded there that means there is a coordinator failure in the second phase and there is also one participant failure or also we have a participant failure. This is the case were we have a problem earlier. Now let us examine this case and see what happens in this particular case in terms of how the recovery takes place in this particular case as the coordinator fails in the second phase along with the participant.
 
Now in this particular case, there is going to be third phase obviously, because the second phase is not the last phase. Now if the coordinator nobody receives the decision from the coordinator obviously at most, the coordinator is likely to be in the second phase which means that the participant here could have been in the prepared to commit state prepared to commit state it has not yet actually committed. So at most, this state can be prepared to commit, it cannot be commit state. The coordinator would also been this is for participant. This is the state of the participant can be this. Now the state of the coordinator can be also before commit state. Now if these two are in this state, the rest of the participants can still now elect a new coordinator 

(Refer Slide Time 33.20)

 

and still go ahead with the protocol, because they have not still committed yet. They have not committed the transaction. Now, in a simple case were the coordinator actually passes the second phase, now you imagine this is going to be a sub case in this particular case where the coordinator has passed in to the third phase means it fail in the third phase not in the second phase. Now this becomes very simple case. Coordinator fails in third phase. 






(Refer Slide Time 34.06)

 

This is the simple case because the coordinator actually taken a decision to commit and this is actually conveyed to all the participants. Otherwise, you would not have got him in the third phase. Because unless everybody replied for the prepared to the commit state, he will not reach to the second phase in the commit message which means even one of the participant have received it. Then only, it is cleared that actually moved in to the third phase. Otherwise he is actually not moved in to the third phase. So in this particular case is easy for you just commit the message because you know the decision of the coordinator that it has been actually committed. 

(Refer Slide Time 34.58)

 

So this ambiguity of not knowing the coordinators state when it actually failed is taken off in the case were the state of the coordinator is known by introducing an extra phase was the participant and the coordinator directly do not commit. But they actually reach before state before commit or before prepared to commit state before actually committing and that is how this ambiguity is actually resolved. That is how the failure is handled by introducing an extra phase for commitment. Now one of the interesting thing is, how the election is going to happen because every time, actually seeing that if the coordinator is failing these equivalent to actually some kind of a leader election algorithm Because you are actually electing a leader whenever there is a coordinator failure, there doing a leader election.

In a simpler way, the leader election can be done by a signing in terms of each participant for each node an id and these ids could be in an ascending order. Obviously every node is going to get the unique id here that means there is a unique id node is going to get and you can actually assign at any given point of time, simple way assign the highest of the you can use the node id or highest node id. So basically you can assign the live node with least id as the leader. However, this is not simple because, now you actually find out you are you should take as the leader. Because, somebody has to dictate that the coordinator has actually failed that means what really is going to happen is, if a participant discovers it is going to be only the participant who is going to discover that there is the coordinator failure.

Because he is waiting for some decision from the coordinator that is how the participant discovers that there is the coordinator failure. So what happen is, if participant discovers it can be anybody. A participant discovers that the coordinator has failed. Now what he does is, he has to do an extra. If he basically discovers that, there was the failure of the coordinator, then he has the option of finding out or he has to do this process of finding out, who is the next highest or least node that is still live.


















(Refer Slide Time 38.04)

 


So, he starts now looking for the possible highest least node, live node and tries to know make that node as the coordinator. But then, that node also discovers that the coordinator would have fail participant. So basically all that need is, it has to see whether it has a least node, live node and if it so basically it now elects himself as the coordinator starts running the recovery protocol. 

(Refer Slide Time 39.00)

 


So in this particular way, any number of coordinator failures are tolerated by this protocol because all that protocol has to do in this particular case is, make a participant whenever it discovers that there is a coordinator failure, figure out the next highest or least node that is supposed to take over and its turn to take over. It will basically become the coordinator tries to run the protocol. Now, there are two ways. This can be further achieved in terms of how this protocol can be run by the coordinators. One of the things the properties has to ensure that protocol is a reentrancy of a protocol that means the protocol can be run any number of times that means a coordinator fails then a new coordinator takes over.
 
Now, while running the algorithm protocol that coordinator could also have failed which means that another coordinator has to be elected. It has to take over this should become reentrance. That means any number of such failures should be possible when you are actually recovering from the failures in which case basically the protocol is called reentrant protocol. That means it is tolerant to multiple failures of the coordinator any number of times. So, it becomes a reentrant protocol. Now there are two ways the reenter protocol can be configured. One is a proactive way of actually making it move forward the other is basically you know a pessimistic way of reentrance.

(Refer Slide Time 40.35)

 

This simple way what the coordinator can take as the decision is, if the coordinator is in a state of before commit state, he will basically whatever the state in he basically enforces that on the other people. Let us understand this point a little carefully the coordinator has failed in the first phase which means that node decision has been taken by the coordinator. Now a new coordinator is in place. How this new coordinator is to start the protocol. Now a new coordinator realizes that obviously the new coordinator is not likely to be any other state other than the first phase. He has not received any of the coordinator. So he is going to run the entire protocol staring from the first phase. 

(Refer Slide Time 41.45)

 


Now he actually imitates the prepare message to the other participants receives the ready message then get in to the second phase then goes in to the third phase things like that. 
Now the new coordinator can be further improve the protocol is, whatever he is state he is in he can actually find out if there is any other participant who is in a before commit state which actually means that now he can actually need not restart the protocol, but then he can knows that actually the decision of earlier coordinator is actually to commit the message. 

(Refer Slide Time 42.22)

 

Now you could actually when there is a before commit state, it is possible for the protocol to terminate as an abort because the new coordinator now ignore this decision  but then just conducts the sends a message to the all the participants. Now some of them not going to reply back to him then he can actually enter the abort decision is quite possible for the new coordinator to find out if one of the participant has reach the commit state. In which case, actually it can force the protocol to a commit state.

(Refer Slide Time: 42:51)

 


To just explain this a little more probably in a using our diagram, you can see here the coordinator actually the new coordinator who has elected. 


















(Refer Slide Time 43.30)

 

Now if you basically dint receive the prepare to commit message which means that he will be in a before state he is still not received this message which could have made the whole protocol terminated the abort decision. Now, if you actually checks with the other live participant and at least one of them has actually receive this PCM state, but is not the new coordinator. This some other has been elected as the coordinator now. Then, it still can take a commit decision because everybody would have reached a state.
 
So it is possible for unless all of them replied with a ready message. This state would have not reached. Right, now the new coordinator can take a decision to commit a transaction and then make it know. Basically, all that required for the elected coordinator in this case is not only check its state, but then checks the state of all live participant based on that restart the protocol at an appropriate point. 
















(Refer Slide Time 44.29)

 


It is done like that the reentrant protocol terminates in a forward direction in terms of committing wherever there is a possibility of a protocol to commit it will commit rather than always sending out of the abort state, depending on the state of the new elected coordinator.

(Refer Slide Time 44.47)

 


So these two approaches are possible in terms of recovery from the failures as far as the coordinator is concerned.  I think what we are going to do in the next two minutes is, not only look at this scenarios of failures but then also introduces now, the concept of what happens with the network failure, because you are now talking about only node failures so far, we are not talking about network failures as part of the recovery process. Now in terms of network failures as far as the protocol is concerned, the two ways of actually interpreting the network failure: 

(Refer Slide Time 44.51)

 

Network failure actually means that the nodes are not reachable. So let us take for example a node here. This node could be a Chennai node and this other node could be Delhi node. Now all that you are looking at in terms of this system is, both of them are connected by the timings of some, let us say back bone network that could be several network. It involved connectivity with been these two nodes. All that you have is basically a network, which is actually providing this connectivity.
 
Now when the network actually fails, in a simple case this actually surfaces as the time out on the other side that is how actually you are going to discover. You are waiting for a reply and this scenario actually come as the timeout in our diagram. As we saw earlier and that timeout could be possibility. 











(Refer Slide Time: 47:00)

 

Because the network has actually failed not the node has fail. In either case it wont be able to detect the difference between a network failure and a node failure because the node as far as for example for this Chennai node is concerned it does not receive the reply in time from the Delhi node it could be network failure it need not be a node failure. The node can be still be kicking doing some work there, but when it actually sends a message, let us say ready message.
 
(Refer Slide Time: 47:24)

 


Chennai node does not receive the ready message. This is equivalent to actually a node failure that means a network failure in some sense actually translates to a node failure. The simple case this translates to a node failure. 

(Refer Slide Time 48.00)

 


But then, it is probably not appropriate. Just consider a network failure is just a node failure. In our case it still works, because when the timeout actually reads out the other side all that you are assuming now is, the decision of the node is to abort. So nothing no harm is done in this particular case but could have done positive thing is ends up in the negative side because you could have still commit the node has still responded with a ready message.
 
But since it does not reach the other node and it decided that when it did not have any information that best thing it can do is abort the message or abort the transaction that’s what actually assumes then abort. So basically a network failure and node failure in a normal case both appear to be treated in this same way right and that is how this protocol both two phase commit and three phase commit are resilient to network failures in terms of translating them to the participant failure.
 
But one has to be careful when the network actually fails, but then that failure is not just to one failure of one node but it can result in not able to reach to multiple nodes. So typically this scenario is a very simple case where it has been translated to a timeout case but then assume that a network failure actually results in multiple nodes not being reachable.




(Refer Slide Time 49.43)

 

Now, let us imagine we have node in Chennai. Let us say, this is Chennai node and then we have actually Bangalore node and then we also have Mumbai node here and then we have one of Delhi. Now it is possible that a network actually connecting them. Let us assume that complicated network here which actually connects all the nodes with one another. But then, if there is actually some kind of hub that connects the southern nodes together and the northern nodes together, it is quite possible that this hub actually fail which means that these nodes are can still each other in this direction and these nodes can reach each other in this direction.
 
Now assume that Chennai is the coordinator. Now as far as Chennai is concerned, the whole problem translates into failure of both Mumbai and Delhi, because it still reachable. Bangalore is still reachable and let us say, it replied with whatever required messages. So, this basically results in partitioning of the whole set of nodes. Now you have actually two partitions. There is one partition, one here which consisting of Mumbai and Delhi and there is another partition two consisting of Chennai and Bangalore. 













(Refer Slide Time 51.40)
	
 

(Refer Slide Time 51.56)

 


Now it if you carefully looked at this scenario and try to understand what could have happen in this, it is possible that Chennai and Bangalore could start detecting that there is participant failure which means that two participants have failed and two phase commit is resilient to participant failure. So what basically they do is depending on which phase you are in, you might end up actually making the transaction. So basically these two might right to run this protocol saying that there are two participants failure. 


(Refer Slide Time 52.29)

 
	
Now as far as Mumbai and Delhi are concerned, for them it translates to one coordinator failure one participant failure. They also try recovering back from this problem by saying that now they will have a new coordinator depending on what phase they are in and how they should be handling this problem. They might actually try recovering back from this failure. Now in this case, it’s possible that the decision of two partitions need not be consistent and this is happening mainly because the nodes are delivering it is a network failure. They are seeing them as failures of the participant nodes. 

(Refer Slide Time 52.58)

 


The participating nodes are still live and they have working. So for each one of them scenario is different unless this is discovered that is network failure not a node failure you would not be actually making an distinction network failure or the node failure and this really results in a complicated situation were even if you use a three phase commit protocol were the network actually partitions the nodes into multiple partitions, you wont be able to recover back from the problem. 

In other words, you are making an assumption here is there is no partitioning that is happening this is the case no partitioning of the node and this is an assumption with your actually working on the two phase commit protocol. If the network partition you have the problem of recovering back from the failure because all the case which discuss which we looked at are actually taking them participant failure and not really participant failures and it is a network failure basically will have a difficulty.

(Refer Slide Time 54.18)

 

An interesting assignment could be what are those cases were the network partitioning network partitioning can lead to problem which is the case in which the protocols the states of the coordinator and the participants in which case the partitioning could really be a problem. The other thing is to discover if you are the majority partition. Probably if you are the majority partition, probably you can still go out do something which means that you have to discover that the system has actually partition and whether you are the majority partition and based on that probably can recover out of the failures. It is still a very open ended problem in terms of how one can recover from network partitioning. It is all interest in see how one can make a distinction between network partitions and node failure.
 
For example: one way is to actually, if you just do a thing you only know node whether the node is live or not, that’s still remain the interesting problem in terms of seeing how one can make a distinction between a node failure and a network failure in a distributed system. Right now, what we have seen is the basic commit protocol in the next class what we are going to see is, how this commit protocol can be integrated with concurrency protocol. For example: how the two phase locking can be integrated with two phase commit protocol that’s going to continue with the next class. 
Database Management System
Prof. D. Janakiram
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. 26

                            Concurrency control for Distributed Transactions 

In the earlier lecture, we just looked at how to face commit and three phase commit protocols work what we are going to look at in the lecturer is the concurrency control protocols for distributed system. How this concurrency protocol can be integrated with the commit protocol. In the first few minutes we are going to look at is how the two phase locking protocol can be integrated with the two phase commit protocol for the distributed system. Now what we have is basically in the two phase locking, there are two phases of execution which we have seen earlier.
 
There is basically a growing phase in which the transactions acquires the lock and then there is an actual point were they start releasing the locks and the rule here is that in terms of time, this is the time axis. Now in terms of the time axis, what you are looking at here is a particular phase were this is basically called the growing phase where the transaction is acquiring locks and this is the shrinking phase were it is releasing the locks. Now one of the condition is this is basically called the lock point. So if you have actually released one lock, you cannot ask more locks that is how basically ensures that transaction is always executing in a serializable order. Now, this is basically transaction execution point. For example: this is the start point and this is the end point. Now, if you release a lock for a transaction, then somebody else in between can read the values of this data item that we have actually modified. 

(Refer slide time: 3:30)

 

For example: Assume that even before you have finished you released a lock on a data item which actually means that somebody else can now read the value of that data item. Assume that there is a data item x and now you actually have acquired a write lock on x during the growing phase and then release this lock during the shrinking phase. Now it is possible that when you have actually modified the value of x. Let us say x is now modified from earlier value is twenty, now it got modified itself to thirty. Now during this phase when you release the lock, somebody can acquire this lock, which means that they can read the value of x which you have modified before you have actually committed which means that if you now want to abort, then the transactions which read this value earlier all have to abort.

(Refer Slide Time: 4:32)

 

This will result in cascading aborts. If you allow transactions to read the values other transaction to read the values of the data item before the transaction is actually committed. So in general, this execution of the two phase locking is not correct because you cannot release the locks before you have actually committed. So what modification that we are going to get when you integrate with two phase locking commit is you will basically acquire all the logs you will come to the commit point but your lock point and you are going to keep this locks till the end of this transaction, execution, plus the commit phase. Only after the commit phase, after you have committed that this is the point were you have actually commit point your going to release your locks in terms of the time scale.
 
This is the number of locks that you have. So the number of locks is actually increasing with time. The transactions actually started at some point here and it is started executing it acquired all the locks it required the point actually where it reached it required all the logs, then it continued execution, somewhere here it actually finished execution that means this is the execution phase. Now after the execution is over the commit protocol is going to be started now because then it has to decide to commit, then this is the zone which the commit protocol gets executed and after the commit protocol is executed, the transaction actually finish execution, then the locks are released at the same time. 

(Refer Slide Time: 6:46)

 

If you release the locks earlier, this will result in the isolation property being sacrificed and other transaction able to read the values of still uncommitted, modified value of this transaction. So you are not going to release the locks as shown in the earlier diagram but this integrates both the locking and the commit protocols together and after execution is over, you actually run the commit protocol and then release your locks. This is how the two phase locking is integrated with the commit protocol. 

Now if you want to see in the distributed setting, how the distributed two phase locking will work. It is actually a simple extension of the two phase locking, because what we now going to see is distributed two PL plus its integration with the two phase commit.















(Refer Slide Time: 7:46)

 

Two phase commit is for distributed transactions, because commit protocol makes sense only in the case when you have where your transaction is executed. Otherwise, there is no question of involving two phase commit there because you have actually finding out the participants can all commit their transaction. There is only single node that nodes knows know already reaches the commit phase all that it has to now decide is whether its committing the transaction or not. Now, in the case of distributed two PL how you are going to execute the transactions is you basically have a coordinator which we have shown earlier. 

This is basically the route transaction and the route transactions have actually spawn sub transactions which means you actually have the Tr 1 is here and there is a Tr 2 here. Now the Tr 1 actually executes a two PL which actually means that it acquires all the locks that it actually needs and this other transaction also acquires all the locks it needs in terms of the two PL.














(Refer Slide Time: 9:10)

 

Now each one of them when they reach the commit phase have to decide whether they have are committing or not committing. When the actual commit message has been taken by the coordinator that is the time they will commit and release their locks. Imagine that there is actually a data item X on which this transaction is operating, T1 is operating. There is a data item Y on which the transaction two is operating. Now T 1 will acquire locks on x, T 2 will acquire locks on Y. They finish their part of the execution. 

(Refer Slide Time: 9:56)

 

Now at the end of the execution, each one is going to decide whether it is going to commit its part of the transaction that is were two phase commit will start know execution out two phase commit protocol is run you figure out whether both of these transaction are willing to commit. Now based on that, based on the commit protocol outcome, they will commit the transaction and then release the locks. Now the important thing is what we discuss is the earlier lecturer is what happens when failure occurs then, what is the problem of blocking? Now you assume that T 2 gets blocked. Transaction 2 get blocked which actually means that is not going to release the lock is because it acquired. Since such time it actually made a decision on committing or abort.
  
The locks are not released till the commit protocol has actually finished which means that the other transactions which try to acquire locks now on these data items will be blocked from acquiring the locks. So basically blocking in that sense is not good, for the simple reason that the resources are blocked. In this case, data items are blocked from access by other nodes. What i am going to show you is a slight extension of two phase locking protocol. The basic two phase locking is used on the actual nodes that means the each node will run a two phase locking for the data items it is actually accessing but this two

(Refer Slide Time: 11:44)

 

phase locking is used for local data items. You are not further looking at Tr 1 accessing a data item that is remote to that particular node and if you basically want to look at that model, you will realize that there could be problems of applying two phase locking where each sub transaction could access both local data items as well as remote data items.








(Refer Slide Time: 12:06)

 

Assume for example, you have a root transaction here. This is the root transaction and then you basically have two sub transaction T 1 and T 2 here correspond on two different nodes, n1 and n2. Now let us say n one as actually data items x y and let us say this is having p and q. Now assume that T 1 actually wants to acquire a lock on x a read lock on x and then also want to acquire a read lock on let us say p, remote lock on the now which means it has the send request lock to the other node n 2. Now for example this is already locked by T 2. You have actually known T 2 has actually acquired a write lock already on p. 

(Refer Slide Time: 13:22)

 
Now unless you have understand T 1 and T 2 are the part of the same transaction T 2 will be blocking T 1 from acquiring a lock on p. Normally, what you do in the distributed transaction model is to access the local data items only you will actually spawn a sub transaction here. You would not actually allow a sub transaction running on one node. 

(Refer Slide Time: 13:49)

 

You acquire locks on basically any time you required to access a data item on a different node you spawn an agent there which is basically a sub transaction of your main transaction. Basically, if you want to provide this extension, there is an extension that is possible for a simple two phase locking which is the nested two PL. 



















(Refer Slide Time: 14:29)

 

The extension two PL takes care of the extension of the two PL to the scenario of distributed transactions where all this transaction sub transactions are seen as children of the root transaction. Now any lock requires that basically is needed by the transaction is the lock request is sent to the root node and it is basically the root node which acquires the locks on behalf of the children node. 

(Refer Slide Time: 15:01)

 


This could mean that for example: the T 2 actually wants a lock on P 2 P actually. This lock is acquired in terms of the root node and then inherited by the T 2. Now when T 2 actually finishes, this lock is not released but then given back to the root node. Now if T 1 actually wants the lock, it gets it from the root node. So, all data items the locks for the data items are with the root node. Now any time you actually require a lock you are making a request to your root node and acquiring a lock and when you actually finish you actually locking the root. The root acquires this lock back. Now if these are actually the now till the same transaction the locks are held by the root. So, it basically it does not conflict the two transactions child transactions of the same root node does not basically conflict with each other. 

(Refer Slide Time: 16:29)

 

The conflict is relived because they are children of the same node. Now the way this execution is going to be done in the nested two PL is there are two kind of models that one can think of. One is called the open nested two PL and other is the closed nested two PL. Now in the case of open nested two PL, we are actually assuming that in the tree structure that you have, these are all actually sub transactions that are possible. Now, this is basically the root, can take this is as T 1 and this is T 2 and there actually two sub transactions for T 1 and they actually two more for T 21 T 22. So basically you have a tree in terms of how showing the transactions are spawn.











(Refer Slide Time: 17:29)

 

Now it is possible for only the child nodes to acquire the locks in terms of actually operating on the data item, that means T 1 will never do anything except saying that there are T11 T12 which basically access or modify the data item that means only the leaf nodes,

(Refer slide time: 18:00)

 

One possibility is to say only the leaf nodes are allowed to modify or process data item and if you allow these intermediate nodes also to do this processing, then it becomes open nested 2 PL and if you put the restriction saying that only the leaf nodes can do the processing on the data items, it becomes a closed nested 2 PL.
(Refer Slide Time: 18:20)

 

Typically, what the nested transactions are meaning here is T 1 consist of T 11 and T 22 and it is possible for T11 to further go down and have T 11 and then this can be you can further have T 112. This is the sub transaction of T 11. You further have nesting which means that you can actually make this nesting, further nesting which means that you can always divide the main transaction in to a sub transaction.
 
Now in terms of an open nested transaction, it is possible to actually covert an open nested transaction model in to a closed nested transaction model, because if there is processing being done at T 1, you can typically make that. For example: This can be now here spawn separately as a sub transaction of  T 1 dash which means that the processing is never done at the intermediate node. 

















(Refer Slide Time: 20:15)

 

If there is a processing involved, you spawn a sub transaction to do it that means in terms of the root other than the leaf nodes, everybody else actually is spawning sub transactions and whatever the sub transaction do that result is being aggregated at the higher nodes. They do not do any processing by themselves. A simple extension of nested two PL two PL to nested two PL requires that, whatever locks are acquired by the children for doing processing are passed on to the parent when they finish execution of the transaction and it is easy to see why nested two PL will obey the serializability condition. All that that has nested two PL has is that the children never release the locks but they are inherited by the parent when the transaction finishes. Now, it is possible for another child to acquire these locks from its parents that means when you need a log, you have to first see whether your parent has the lock, then you get the lock from the parent.

















(Refer Slide Time: 21:28)

 

You are inheriting from the parent the lock. We assume that all the locks are with the parent. So the only way you can acquire the lock is by making a request to your parent. When you actually finish your execution, you are returning the lock back to your parent. Now you can see why this simple nested two PL will ensure serializability by looking at intuitively what is happening here. Now, if you basically see that there are two branches of the tree and there is a lock acquired by any of this children here, there is now an inherent no serializability at the higher level because unless let me put in terms of simple tree structure nodes. One, two, three, four, five, six, seven. Now you can see that the inherent order in which this execution can proceed is, if three requires a lock or six requires a lock that is currently held by any of the other nodes the only way it can be obtained is parents which actually means that unless two finishes. 

















(Refer Slide Time: 22:51)

 

One cannot acquire the lock that means at the higher level the execution is always proceeds in terms of this tree, the nodes of finishing one node is which is earlier till finishes and later the other node can finish. Otherwise, there is no way other node than finish. So inherently that tree structure will be ordering the way the transactions are executed and inherently provides a serializability condition. Now you can say that, that means the only execution sequence that is possible in the case is four after that five if there is a conflict, then six and seven. 

(Refer Slide Time: 23:38)

 


If there is no conflict, it is possible for this tree structure to be executed. For example: where the leaf nodes are all. So that the same tree structure that we saw earlier it is possible that, i can have execution where if there is no conflict, because if there is no conflict, it does not really matter how six and seven are actually executed. If there is a conflict that means, if five is asking for lock on data item x and six is also asking for the data item x, this wont be possible because this lock would have been held by node unless it finishes execution, this would not have happen and hence this will be prevented. You can see in a simple case, a nested two PL is an extension of a simple two PL where a transaction actually spawns a sub transaction. 

(Refer slide time 24.43)

 

and now that sub transaction can further spawn other sub transactions and then each sub transaction when it finishes gives a lock to the parent and that is the way to execution can proceed and that inherently ensure is the serializability requirement. This is an extension of simple 2 PL to nested 2 PL for distributed systems. This is one possible model. You also seen a number of actually time stamping scheme as part of our earlier discussion on transaction models. Now it is worth actually seeing what exactly happens with time stamping schemes in their distributed scenario.
 
Now what we have is, the basic time stamping scheme which shows how exactly transactions can be executed by having a basic time stamp being given to both the transactions as well as to the data items. Now in the basic stamping scheme every transaction is given a time stamp at the beginning of the execution. Now this time stamp could be just the logical lock value that we saw in the distributed systems. 





(Refer Slide Time: 26:47)

 

which means that this is a local clock that is maintained by the distributed system which in affect means that this is to be either synchronized clock or some kind of logical synchronization happens these clocks such that the time stamp reflect the total order among the transaction that have actually executed in the distributed system. One way to do this is the last two bits, know least significant bits a part of each node and the higher bit is actually a synchronized bit with the other clocks of the other nodes.
 
It is possible for you to give a time stamp for each of the time transaction originating in each of the nodes of the distributed system. Now, if you assume that every transaction entering the distributed system can be executed on any node of a distributed system, so this time stamp corresponds to the logical time that we saw in terms of the different node synchronizing their logical clocks. One extension is a simple case of mutual exclusion algorithm that we had implemented in the case of lamport’s clock. The same scheme is applicable here except that instead of mutual exclusion is going to have a read lock or write lock that you acquired on the data item that means in affect this is equal to the time stamp being used for writing on the data item or reading on the data item. 

Now different nodes will decide the order of the in which they can access this data item by using the time stamp value. The basic time stamping scheme what you are essentially doing is you are actually giving time stamp at the start of the execution of the transaction. Now as the transaction is start executing your actually looking at its read set and you are seeing its write set and then you are deciding in terms of the read set and the write set, what are the time stamps of these data items and making sure that the transactions are executing in terms of the time stamp order. Now, assume that every read and write data items in the data base have given the time stamp value.



(Refer Slide Time: 29:54)

 

For example: Let us take a simple case of actually looking at a data item x which is in the database. Now each data item will have two time stamps corresponding to it. One is the read time stamp that means this gives basically the read time stamp. What the read time stamp will you give is the value of the timestamp, the highest value of the timestamp of the transaction that has read x that means it tells, which is the highest time stamp transaction that has read its value. Now similarly we will have one of write time stamp X which tells correspondingly the write time stamp of the transaction.
 
This is basically the write time stamp. The explanation for the write stamp is similar except that instead of what we have put there as read. Now this is the highest value of transaction which has actually written on the data item X. The value of the time stamp of the transactions, that has return on X. 
















(Refer Slide Time: 32:04)

 

Now the use of the read of time stamp and the write time stamp is for serializability, you always allow the transactions to only do in terms of increasing values that means if there is new transaction that comes in whose time stamp is lower than the write time stamp and if you want allow to read the value you obviously will have problem right. For example: If the write time stamp of the data item is greater than the transaction time stamp, then you should prevent it from reading the x value that is basically you to abort the transaction and then ask it to come again with the higher time stamp. So that you are preventing the access to the data item in terms of always the data items should be access by the transaction in terms of increasing time stamp. If you ensure that you are automatically achieving the serializability condition. 


















(Refer Slide Time: 33:20)

 

Now in a simple way in a distributed setting you are going to extend the basic time stamp distributed transaction model by ensuring that these time stamps are generated in a consistent way by the nodes of the distributed system. If each one of them generate the time stamps  the transaction time stamp are generated in a global way even when the data items are distributed you are still ensuring that globally all the reads and writes are ordered according to the time stamps of the transaction. Since the time stamps of the transactions are generated in a consistent way in the global system, we are essentially ensuring a serializability order for your transaction coming in the distributed execution. Basically that is an extension of basic time stamp scheme to the distributed scenario the extension is quite simple and straight forward again in this particular case ensuring that the logical clocks of the distributed system are synchronized and executed properly as per that order.
 
Now one of the interesting extensions that one can see is, a completely optimistic extension of the time stamping scheme were the data is fully replicated on all the nodes which means that now every node has a data item available locally. So what you are going to do now is, you are going to look at each node is going to send its read and the   write set to everybody and now everybody can vote on the reads and the writes and if whatever reads and writes you have appropriate number of votes then you will be able to view the modification which means that the majority of copies are always consistent and always proceed in the same order as being done by the nodes. 

Let us look at that extension and is often called the optimistic time stamping scheme and i will actually summarize the different time stamping schemes which have done earlier with respect to the distributed scenario. The optimistic scheme is interesting extension because what you have is here a fully replicated database that means the data items are fully replicated on all nodes. Now take a simple case were there is node n 1 and there are two data items x and y and take another node n 2. What is going to happen now is, it will also have x and y which is essentially means that all that a transaction is going to do now is going to produce you known a list of data items that is willing to modify along with the transaction time stamp which is trying to do. 

(Refer Slide Time: 37:00)

 

Now what is going to do now is, it needs to send the information to all the other nodes and other nodes have to now say whether they are willing to accept the list sent by another node, that means every transactions modification what is read what is to modify will be send to all the other nodes and now those nodes can decide whether that update list is consistent as far as they are concerned. If they say yes, then if enough number of yes votes are come for a transaction then obviously can commit. It does not get that it does not basically commit which means all the processing always going to happen locally but then you have to send your read list and write list for voting to other nodes. 

Now the other nodes can decide to vote yes in this particular data item. Let us take a little more detailed way and then see what really happens in this particular case and how the transaction execution is going to proceed. In this particular case, what we going to see is let us say i have a transaction t 1 which started at let us say ten twenty here with respect to its time stamp. Now its basically trying to read x and write y. So that means its read set consist of x data item and it is basically write set consist of y. Now it will modify whatever it wants to modify as far as this transaction is concerned and produces this list for other transaction to vote, that means it will communicate this list to node n 1 saying that as far as this t one is concerned, it is at time ten twenty it is modifying x and y and that list is being sent to the other node. 

Now assume that, there is basically a read time stamp on x which actually shows again, what is the highest value of the transaction that has actually read x and similarly the write transaction on y. Now what you are going to see is, if this list is actually non conflicting as far as the other node is concerned, what we mean by non conflicting is basically there is set of transactions want to execute on other node as well. Now what you have to check on the other node is before the modification is done whether this modification is consistent as far as the other node is concerned and the consistency criterion is here is, this x and y whatever this transaction is trying to do now is later than whatever has been done already in the other node.  

(Refer Slide Time: 40:22)

 

which means that the corresponding RTS and WTS tools are applied on the data item and if it is consistent as far as that is concerned then you have vote saying that, this is okay for me because as far as i am concerned, there is no conflict of this transaction for serializability and if it is basically conflicting and this condition does not hold good the RST or the WTS not being know in the case of write transaction, it has to be more than both RTS and WTS. In the case of read transaction, it should be more than the write transaction WTS. So, in that sense we apply the consistency rules of RTS and WTS on the other node and decide whether this list is consistent.
 
Now that would not be enough because this is only a proposal to modify. It is still not modify. They could be several such list come to me. Now it is possible that there is a pending list with me and i have to actually compare these values with the pending list also, what is pending now with me? Now let us say in the pending list, none of it actually conflicts with what i have then the rule is simple. Then I will be basically give a ‘yes’ vote for it. 







(Refer Slide Time: 42:02)

 

If there is a conflict in the pending list, which means that there is a list which came to me which i has actually voted. Now let us say there is a conflicting list with me and in the conflicting list, i have voted and yes for the conflicting list. Now, there are two things that can happen with the conflicting list. Let us say there is another transaction whose time stamp value is let us say ten twenty now. There is a conflicting T dash direction here whose value is let us say ten o clock time stamp value is ten o clock and let us say for this obviously I have voted and as earlier that is why it is actually my pending list. I have still not heard about this transaction. 

(Refer Slide Time: 43:08)

 

Now one of the things, i can do in this particular case is since i have already voted for t 1 as ‘yes’ T dash as ‘yes’. The best thing i can do for t 1 is say ‘no’ because it is conflicting and an earlier transaction already said yes. Since i have actually said yes to an earlier conflicting transaction which means that this t dash has actually ‘yes’ for my case. Now i actually another transaction at t 1 which is actually conflicting with my pending list. Let us say this x value trying to modify and when it is trying to modify the value of x probably if we take a specific example here to make this little more clearer. 

(Refer Slide Time: 44:03)

 

Basically what you are saying is this node n 1 when it got a list, it is voting ‘yes’ or ‘no’ right. Now let us say there is basically let us take node n 1 the execution sequence, the node n 2 the execution sequence. It received an update list of Tr dash at ten o clock and this is the list with x and y let us say, it receives this is the update list. Now, it did not conflict with anything by actually voted for yes. It still pending on my side because i have not heard the final decision on this, because the one who has actually sent me the list if it get enough yes votes then it is going to get committed and send it finally to me as 













(Refer Slide Time: 45:01)

 

an update list, then will apply the updates to the actual that is my update. But, in this particular case this is the pending list. I have voted but then the decision on this still pending. There is no decision on this particular list. Now what happened? There is another transaction t 1 that came by and it is actually ten twenty and then now if it is not conflicting with my pending list, there is no problem what so ever then i basically saying ‘yes’. Now let us say the one which came which came at ten twenty as a pending with my conflict pending list. Now for that pending list, I have actually voted as yes. 

Now one way for me to do is, this ten twenty i would not reply immediately but wait i say okay till i get the decision on ten o clock, i am not going to vote ten twenty which means that differ making a decision on that is one possibility. That means this list is not voted by me. I am not going to say yes for it a ‘no’ does not create a problem because a ‘no’ is already been. I have actually said i am not willing to go with that pending list. 















(Refer Slide Time: 46:31)

 

So even if it conflicts no really does not bother you because you have actually voted but some other reason you actually not willing to go with that transaction. Only when you say yes to something, then the current one is conflicting with it, then you have you have a problem now whether will this create the problem or will not create this problem because based on that you have to take decision. Now assume a scenario where this is this scenario 1. This scenario 2 could be same T 1 the same T 1 arrived at my node and now it is conflicting. Now here also let us say, i take the decision in to differ making a decision in this case.

Now both scenario 1 and scenario 2 differ, then imagine a possibility were none of the list the likely would have getting a majority may not be there because everybody is waiting for the list to be voted and there could be the case were none of them might  receive enough votes and everybody is waiting for everybody else. Suddenly, it looks like differing voting on both cases is not a good idea. Now the next case which case i better vote no and which case i should better vote ‘yes’. Now, if you basically looked at i have already voted at for that ten o clock case is yes.
 
The best bet for me is that ten twenty can be differing because it is the later transaction. Suddenly if i have done something at ten o clock and somebody is trying to come to do at ten twenty it is still alright with me, but if it is actually nine fifty, i better in this particular case vote as no because i have already this any way will be only one chance it would have succeeded is the case were ten o clock did not get enough votes. In that particular case, the T 1 has a chance. 

Otherwise it does not have a chance right. So i can wait. I can always wait both cases, then decide only when i know about the know fate of ten o clock transaction because i have voted for it and make any future things that have coming in. Till the pending list is clear i would make any other decision but what happened in the case is the possibility of all the transactions waiting for each other pending list and result in no body progressive. So you better vote the case where it is nine fifty as you vote for this as a no to avoid this confusion. 

(Refer Slide Time: 50:14)

 

Now at the end of it, the majority votes if a pending list actually majority vote that basically sent as an update list for all the nodes, that means that list is now has to be updated by all the nodes. Now the very fact any node has got enough votes any pending list has got enough votes that becomes a permanent write on it and you can easily show that no to conflicting things can get. 

Both cannot get majority vote at the same time because only one of them can win if the two transactions are conflicting, only one of them in the end will be able to achieve the majority vote and that is the one which will go through and the other is going to be restarted. If you basically look at it here, it is a completely optimistic scenario because you are executing all the transactions and you are never checking the consistency before start execution of the transaction.

What you are doing is you are executing the transaction and at the end of it, you are deciding by sending this updates to all other nodes. You are seeing whether you can go through with this execution or not. So this is typically the pessimistic scenario verses the optimistic scenario. Pessimistic as we told earlier, the consistency check is made at the beginning of the execution and then this is the begin that means actually you do this checking a locking would have ensure that the check is done in the beginning unless you acquire the locks will not proceeding on execution. 

On the other hand, this check is made at the end which actually means that we are actually executed in transaction fully and then applying the consistency check. Now in that sense, basically having the two spectrums. One is the locking based algorithms which fall in this spectrum and then a fully optimistic time stamping scheme which for in the scheme were the consistency check is only applied at the end of the execution sequence. A variety of you know models are possible for as the transactions a models are consistent based on the time stamp scheme. 

(Refer Slide Time: 53:10)

 

One of the most interesting scheme is to give what we have seen in the case of example:  Visiting Tirupathi temple. You have the model were the band of time is given for you; you have to go and then see you know thing at the point of time. Basically, you are scheduling transaction which can be executed in the future. For example: you can say as far as t 1 is concerned, you can give a band of time which it can really execute which means that when it start executing you can say that, this becomes the time before you which commits it is alright for you.

This is like telling, if it comes at the gate at ten twenty. This is not the start time, virtual time of the commit as far as the transaction is concerned, if it commits at let us say ten o clock its fine with me yours telling when it commit and it can actually finishes it execution. So you can actually give an order in which, this is basically end finish time. This is not at the start time that you are trying to schedule. You can actually do this at the end. For example: it is possible for you to tell even a band. 

Let us say, this transaction I can allow between 10 and 10.15 to execute, finish its execution. If i generate this band such that, conflicting transactions will not finish in the same band I have actually know it is getting the serializability condition. For example, there could be another. For example: if the band that is given to these two transactions is different, if they are conflicting. 



(Refer Slide Time: 55:01)

 

Let us say, i give a band of 10.15 to 10.30 for this. It actually means that, suddenly they are not you know the serializabililty condition will not be violated here because one is finishing at 10.15 and other is finishing. So i actually gave the band of time in which they can finish their execution that is other model i can give. Completely different kind of model is also possible were you can say that I will basically do what is called only transaction time stamps, but not data time stamps. For example: all the schemes which i have seen so far have two kinds of time stamps. One is time stamp to the transaction and i am giving the time stamp to the data item. You could do a scheme were all that you are going to do is as the transaction is executing you can give time stamps to different points of execution. 

For example: you can give start of the transaction finish of the transaction, what is the time and used that for actually doing the consistency check but as long as one transaction finish after other their conflicting and that you are able to ensuring at your clocks times that you have actually happened. You are actually able to ensure the consistency condition.  In a simple way what we are talking about is, let us say T 1. There is a start time and there is a finish time of T 1 and similarly T 2. 

There is a start time and finish time and these times are actually given by using a clock here. Now, as long as the conflicting transactions are all as they are executing these times are given, you can ensure that the serializability condition as far as the operation is concerned. By making sure that, the start and the finish time of the properly synchronized as far as the transaction execution concerned. By doing this, we actually do not need to maintain time stamp on the data item. 




(Refer Slide Time: 57:27)

 

That is a saving in terms of not needing to maintain the time stamp. A variety of schemes are possible we have seen couple of scheme in this particular lecture. 

Database Management System
Dr. S. Srinath
Department Of Computer Science and Engineering
Indian Institute of Technology Madras

Lecture No. 27
Introduction to Transaction Recovery

Hello and welcome. In this session today we shall be staring with a new topic namely that of recovery that is how to recover data or how do we bring back database into what we called as a consistent state in the phase of any kind of failures. Failures could be of any kinds. Let us say disk crashes, power shutdown or network connection failure, many different kinds of failures and we are going to be somewhat specific when we say recovery. Obviously we cannot recover data that involved that were being processed in main memory during ram. Main memory during the crash but only we can recover whatever has been returned on to persistent storage like disc. But what is written on the disk? Good recovery or somewhat semantics associated to what is written on to the disk and do we have to do something more when in order to recover from crash schedule? 

In order to answer this question we need to know the concept of a transaction in a database processing environment. Therefore, the title of session is called introduction to transaction recovery. In fact we are going to define the notion of correct recovery from wrong recovery based on the concept of transactions. We are not going to study about transactions in detail here. They are covered in a separate topic under itself transaction processing itself is a vast topic with several different aspects to it and we shall be concerned mainly with the recovery aspect here when it comes to and we are using the transactions to help us guide in deciding which kind of recovery is correct recovery which is incorrect recovery. 

Let us first define the term OLTP. I am sure you might you heard of the term OLTP in several different contexts. It stands for online transaction processing and environment that is the database system plus an application program plus any other associated accessories like networks and so on that goes into form an environment that is meant for online transaction processing.












(Refer Slide Time: 3:22)

 

What is meant by online transaction processing? As the name suggests, it is a processing environment that can interactively process database transactions. We will define these terms in a much more accurate fashion later on. 

(Refer Slide Time: 3:53)

 

But, let us first look at some environments that can be classified as online transaction processing environment. Some examples are like airline or railway reservation systems. What are the characteristics of such system? One of the main characteristic that you can straight away see in Railway Reservation Systems, for example:

(Refer Slide Time: 4:29)

 

Is that there are several numbers of users who are accessing the database system simultaneously that is you might have experienced it if you have tried booking a railway ticket over the internet. Suppose a train is getting almost full and if you delay in booking your ticket given by let us say some times even by few minutes, then never you may not get a conform ticket at all might go in to waiting list. 

So that means, at that particular instance of time when your checking the status of tickets, there were several other people accessing the same database throughout the country. That is they could be accessing via the internet, could be accessing via let us say some kind of Queuing system that is across the booth or whatever from, they are all accessing the same database and several different transactions are happening at the same time and similar examples are of that of banking systems and especially how cash dispensed in atm’s or wire transfer mechanisms and so on. Another over tip environments require super market checkout systems and where customers come in with their baggage of whatever things that they have bought, they have to be checked out praised and build and so on.  













(Refer Slide Time: 5:52)

 

There are hotels and hospital system, trading and breakage system where buying and selling of the shares keep happening continuously whenever trading is on and several such sessions are happening simultaneously. Now let us take scenario which helps us understand what should be the properties of these transactions that go on in OLTP environment. Now consider a small banking example where different accounts are maintained in a bank. There could be on different databases or within the same database or different locations or same location, it does not matter. Let us just consider that there are different accounts and this database is being accessed simultaneously by several users performing several transactions. Let us say that one percent using net banking to transfer some money from his account to somebody else account. 


















(Refer Slide Time: 7:14)  

 

At the same time, other person is using an atm to withdraw some money or deposit some cheque or some cheque is getting enchased some did is getting enchased or withdraw whatever several things are happening simultaneously in the bank. Now, among this let us consider a small one particular example that account number 2,565 sense 2,500 rupees to another account number 165. 

So let us say at time t equal to 0 however we define our time, a transaction begins or set of database operations begin the application program that is making this wired transfer will initiate database operations which will first read the balance of 2565. Now it will read the read the balance amount and because it is withdraw, it is a withdrawal from account 2500 will be deducted from this balance and let us say we are using concurrent applications which can have different threads of execution which can run at the same time. 

So when this balance is being deducted here at the same time the balance of second account is being read of account number 165 and due to some reason, this thread process gets swapped of into disk by the operating system. Some other process is running because note that operating system of the scheduling processes to and some other processes running at the time and by the time, this process comes back. It is time number 4, time t equal to 4. At this time, balance amount of 2,500 is added to this account. 

Now meanwhile let us say there is another transaction. Let us say, the account holder of this of this account number 165 is meanwhile standing in an atm and depositing certain cash certain amount of cash to the atm or may be he has sent a cheque and that cheque is getting clear. So, why this person with the account 2565 performing wires transfer? The person having the account 165 is also depositing amount of 3,000 rupees and it is so happens that, the way processes are scheduled this set of operation that is reading the balance of account number 165 here and adding 3000 rupees is done before the previous transaction finished.

That is before the balance of before amount of 2,500 is deducted from that is based on the previous transaction. You can see what happens what has happened now? The previous transaction has read the old balance amount and added. Actually this should be added. I am sorry. There is a small bug here. This should be plus equal to that it is taken. It is taken previous balance amount and added to 2500 rupees here. While before it could that the previous balance was read, 3000 rupees was added by the customer who is depositing his cheque from the atm.

Now what has happened here in that, this entire transaction is lost this entire serious of operation is lost after time t equal to 4 because suppose this person had 50,000 rupees in his account it will be now 52,500 rupees rather than 55,500 rupees. 3000 plus 2500 rupees. 

(Refer Slide Time: 11:12)

 

So 3000 rupees is just gone, it is just lost. Now, as you can see this is not unrealistic situation especially we just saw today, when we have facilities like net banking or atm or  booking train tickets over the web or using telephone calls or using sms from mobiles and so on. This is not an unrealistic situation because concurrency is concurrent activities are happening at the same time. I mean a concurrent activity is happening all the while and if you are not careful, such kind of activities can result in an inconsistent database. The entire transaction of depositing 3,000 rupees is lost in this example.
 
So, if you are not careful what is the clear you have to take what is that you need to remember when we are dealing with situations like this. The thing that you need to remember here is the first two activities. Let us say the first activity of wired transfer between account 2 5 6 5 1 6 5 is a completely different or is a conceptually separate or distinct activity from the second activity of depositing 3000 rupees.

(Refer Slide Time: 12:47)

 

So the first activity is a conceptually or logically separate activity or functionally than the second activity. Such kind of logical units of works are called transactions and transaction activities in a transaction, the actual database activities transaction should be schedule in such a way such that these kinds of anonyms do not occur. Let us try to formulate these things in a little bit in much more detail. Now what is a transaction? A transaction is a logical unit of program execution. 

(Refer Slide Time: 13:35)

 
As we saw the entire activity of withdrawing money from A’s account and depositing that money in to the B’s account constitutes one logical unit of operation and it is a combination of database updates which have been performed together. They cannot be independent of one another. The withdrawal from a’s account is not independent of deposit into b’s account and vice versa. There are several different transactions depending on where and how it is being used. 

Let us have a brief look at the different definitions of transactions which makes it clear what are the different fests to handling transactions or recovering from transactions one. Firstly we can define a transaction as a logical unit of work that is meaningful in the user’s environment. As we can see here, the wire transfer that is withdrawal of money from A’s account and depositing in to B’s account is a meaningful semantic activity as part of the users environment because that constitutes a semantic process in the users environment, a wire transfer. Similarly depositing 3,000 rupees; In order to deposit 3,000 rupees, there were two database operations. Network done that is reading previous balance and updating the balance. So both these activities of reading and updation is one semantic activity that, it constitutes one meaningful activity namely that of depositing certain amount of money in to an account.
 
Now one can define even transaction as a logical unit of work with respect to concurrency control and recovery. Not necessarily semantic activity in terms of the user’s environment.  Many cases users depending on what granularity you are looking at. Users may not be concerned with water considered transaction set at the database level. However, we might have to club or we might have to combine certain database activities in to transactions in order to maintain consistency in the phase of concurrency control and recovery process. Transactions are also called atomic unit of work. Instead of calling it as logical unit of work, much more stringent definition is to say that it is an atomic unit of work with respect to concurrency control and recovery. Atomic unit of work is a more stringent requirement than saying logical unit of work. An atomic unit of work basically means that it cannot be subdivided in to smaller works.
 
Either all of the activities of a transaction are performed or none of them are performed. You cannot perform half a transaction and leave it at that or you cannot perform 90 percent transaction and leave it. Either you have to perform the entire set of activities of a transaction or nothing at all or another definition that it is generally used that is transaction is an atomic unit of work that will apply to consistent database returns another consistent database. 









(Refer Slide Time: 16:58)

 

That is an atomic unit of work that is meaningful, that could be meaningful that is the atomic unit of work with respect to concurrency control recovery. However, not all atomic units of work that can be managed for concurrency and recovery could be transactions because some of them could take it take the database to inconsistent state. So the transactions sometimes defined as in those only units of work make transactions to a valid state of database. Now what are the properties that a transaction should satisfy that,  we have seen we have motivated the need for transaction in several different from angles. 

We first saw an example application an example OLTP application where transaction processing incorrect transaction lead to anonymous and also we saw different definitions that look at the notion of transactions from different levels. Now how can we consolidate them together and synthesis what are the basic properties the transaction should hold?  So the basic properties that a transaction should hold are called as the acid properties of a transaction. Acid stands for atomicity, consistency, isolation and durability that is shown in the slide here. So, what is atomicity in the acid property? Atomicity we just saw in the previous slide that a transaction should be viewed as an indivisible unit of work. 












(Refer Slide Time: 18:46)

 

That means either all activities in a transaction should be performed or none of them. We cannot perform half a transaction and leave it. Consistency: Consistency of a transaction basically means that of the database is consistent transaction should be consistent after a transaction. The transaction the atomic unit of work should not lead the database in to an inconsistent state. What is meant by an inconsistent state? Any state that violate the integrity constraint of the database. 

(Refer Slide Time: 19:17)

 



We saw how to specify integrity constraints and how they are enforced in a database system. The third property of transactions is isolation. We saw an example of transaction violating isolations in our banking example that we saw before. Isolation essentially means that even though activities in the database are happening concurrently that is the readings and updates and reads and writes operation whatever is happening on to the database level are all happening in a simultaneous fashion. 

(Refer Slide Time: 19:53)

 

The net effect in an OLTP environment, the net effect should be as though the transactions have been executed in some serial order. It does not really matter to what should be the serial order as long as we can establish equivalence between the way in which database activities are performed to a serial sequence of execution. That is it should be as though that transaction ‘a’ was completed before transaction ‘b’ begin.  ‘b’ was completed before transaction ‘c’ begin and so on and the last property of the transaction is called durability, that is once the transaction finishes are rather we used the term commit here, that is once the transaction says now i have done all my work and you can commit whatever changes have made in to the database. Once it is committed, the changes are persistent. 











(Refer Slide Time: 20:52)

 

You cannot rollback or you cannot undo the changes that are being made by the transaction after it is being committed to the database that is commit is something like in order to understand the notion of commit, it is something like a physical activity. For example: dispensing money from an atm is a commit operation. Once it is committed, once money is dispensed, you cannot rollback. You cannot expect the user to say no no no, we did something wrong. We have to put back money that i gave you because some other transaction is conflicting. So once commit operations is performed, it is durable that is the transaction cannot be rolled back and we have to do something else in order to undo the operations of transaction. 

Once you dispense the money from an atm you have to do something else. We chased the person who withdrew the money and get back from him if required and so on. So we cannot undo the transactions within per view the database. Let us look at the examples that specify each of these acid properties of the transaction. Have a look at slide here. Let us say the transaction involves again a wire transfer from account a to account b that is, the transaction should be like this. Account a dot balance minus equal to amount whatever amount has to be transferred and account b dot balance plus equal to amount. Let us suppose that the balance that is amount number of rupees has been debited from a’s account and for some reason the databases crashes.
 
Let say there is disk crash or a network failure whatever or operating system crash or whatever. Now once the system is brought up again that is once there is a recovery process, this transaction has not completed. Therefore in order to make it atomic, we have to roll back. We have to roll back the changes that you have made since the beginning of the transaction that is, we have to put back this amount back into a’s account and then restart the transaction once again. Otherwise, it would not be an atomic operation. 


(Refer Slide Time: 23:17) 

 

This amount to if it did not perform the recovery operation this would amount to performing half a transaction and we saw that performing half a transaction and we saw that half a transaction is not an atomic transaction. What about consistency? Have a look at the example here again. This is again the wire transfer example from account ‘a’ to account ‘b’ that is the same series of operations have to be performed that is A dot balance minus equal to amount and B dot equal to B dot balance equal to amount. Now let us say that the query that has to be performed. These two things have been that is the query planned has been performed and these two operations are given to two different threads in the operating system.

(Refer Slide Time: 24:05)

 
And it so happens that the thread performing changes on B’s balance is scheduled first before that of ‘A’. Let us say first ‘B’s balance amount is crediting. Let us say ‘A’ is sending 2500 rupees to ‘B’. So we know the amount 2500. So these balances added by a value of 2500 and however when trying to recover, remove 2500 rupees from A’s account. We see that A has zero balance in his amount. He cannot make this payment. So  this transaction fails. We cannot make this transaction. So in order to keep this transaction consistent, we have to deduct whatever credit we made in to the account of B in order to bring back the consistency in the database systems. 

(Refer Slide Time: 25:06)

 

Note that here there is no crash or anything of that sort. Here there is the normative failure. The normative or failure with respect to nor. The failure which violated the integrity constraint. We can think of an integrity constraint that says odd raff are not allowed that is the balance amount in users account may never be negative. So when we try to do this operation that is when you try to debit 2500 rupees from ‘A’s account, we found that the balance is becoming negative and it violate the integrity constraint which in turn cause the transaction to roll back that is in order to maintain the consistencies in the database systems.

The third property is that of isolation and isolation like you said before deals with concurrency that is what how do we handle concurrent operations being performed from two or more transactions simultaneously. So again consider the case of wired transfer another case of wired transferred that is account A’s is transferring some account to B. At the same time, account the person holding account ‘A’ is also withdrawing some money that is the person holding account A has given a check at some time which is getting process now and if the same time, the account holder is withdrawing some  money. Let us say the transaction t1 is reading account A’s balance debiting the amount and crediting the amount account B.
Let us say that A’s balance will become zero after debiting this amount. Let us say it 2500 rupees and let us say the same amount is also being withdrawn by being as by account holder for withdrawal. The net effect of running these two transactions should not be the case that both of them read the database or if the balance, there is 2500 rupees and then go ahead independently debiting them debiting the account because that would be in correct because we would have debited more than 2500 rupees.

From these two transactions where it is not could not be reflected. So the net effect should be T 1 precedes T 2 that is T 2 begins only operation after T 2 completed is in effect that that should be the case. Which case T 2 will fail or Tone begins operations after T 2 completed? It does not matter which is the serializable schedule which is the serialize schedule that we want is it T 2 after Tone after T 2.So in either case none of the 2 transaction will fail that is either the withdrawal will fail or the wired transfer will fail.

(Refer Slide Time: 28:02)

 

And durability like we said is the commit operation that is one thing committed, then it is not change we gave an example of let us say money dispense information from atm. 












(Refer Slide Time: 28:15)

 

Once the commit operation is performed, it is safe to dispense money from the atm and we cannot roll back the transaction once the commit is performed. What are the different states in which a transaction is in and this is important to know when we are trying to recover from a failure of a transaction? Now transaction is set to be in several different states depending on what has happened since it begin. It is said to be in active set which is the initial set when the transaction is executed. When the last statement has finish execution and it is ready to commit, the transaction is said to be partially committed. 

When the transaction discovers that it no longer proceeds with normal execution because something else has happened, some crash or some violation of an integrity constraint or some violation of an isolation requirement and so on. When it discover something like that then it is said to be in the fail state and once roll once the transaction is rolled back, it is said to be aborted and if the transaction successfully completes that its operation that is an atm successfully dispenses money. It is said to be committed and either committed or roll back or aborted state is called terminated state. 













(Refer Slide Time: 28:30)	

 

So this slide schematically depicts the different states in a transaction and also shows from which state you can go to which other state that is from the active state, you can go to either a partially commit state or a failed state and also you can reach fail state from a partially committed state that is after performing a few operations and from a partially committed state, if everything okay then you can go to committed state or if the things are not okay you can go in to the fail state turns take in to an aborted state. 

(Refer Slide Time: 30:14)

 

Let us have a simple look at how these acid properties can be maintained or what it takes to maintain these acid properties and we are going to look at simple example called simple technique called a shadow copy. Shadow copy is extremely simple extremely inefficient and it is not used in practice. Several more sophisticated techniques for handling are maintaining acid properties or taken up in much more detail when we take up the topic of transaction processing itself. Here this is just to illustrate the concept of what it takes to perform, to maintain certain properties of a transaction.

(Refer Slide Time: 31:06)

 

Shadow copy transaction assumes the database to be a single file and assumes that ther is only one transaction that is active at any time. Note that it can only provide ACD that is Atomicity Consistency and Durability and not isolation. 

(Refer Slide Time: 31:22)

 
So shadow copy is simply like this. Suppose you have database in a file and you have to perform your transaction. Now before performing your transaction, make a copy of the database that is copy in to entire file. The file and make your changes on the copy of the databases. Now if your changes succeed, that is it does not violate any integrity constraint and it is consistent and it is safe to commit and so on, then simply you delete the original database and then you keep the new updated copy of the database. Incase you have to abort your transaction, then you just delete the copy that you have created and let the original database be in its place as simple as that, that is you make the entire database to in to shadow.
 
(Refer Slide Time: 32:22)

 

Copy the entire database into another file and make changes on it and if it is safe to commit the changes, then delete the original file or if it is unsafe delete the new file and let the original file be as it is. Of course, how it is interactional and inefficient but of course later does it satisfy these acid properties of a database. Let us look at atomicity. If i see that i cannot do all operations in a transaction such that atomicity needs to be met then i just delete the new transaction that is i just delete the new file. It is all are nothing. 












(Refer Slide Time: 33:10)

 


When all operations are committed, all operations are performed in the new file, will i delete the old file. So, therefore it is all or nothing. No operations have been performed. Consistency: If any consistency, if any integrity constraint is violated in the new database is deleted. So assign that old database is consistent, we are still left in a consistent state. Isolation obviously not supported because when two or more transactions are copying making different shadow copies cannot we cannot support isolation here and durability. At any point in time, once the transaction commits, it just ensures that either the old file or the new file remains that is once the transaction terminates it is either commit or abort if it commits, then the new file remains. If it aborts, old file remains. So it is durable what are changes made are persistent in the database.
 
Let us have look at concept of serializability which is again very important, when it comes to recovering from failed transactions. Like we mentioned before, in the previous shadow copy example, isolation was not supported and in order to support isolation we should ensure the notion serializability in our transaction processing environment. This serializability simply says that, if i set of activities from two or more concurrent transaction taking place, they should they should schedule in such a fashion as though transaction were executed in some serial order. So have a look at the slide here. Slide shows two transactions here T1 and T 2 and transaction. Tone is a wire transfer that is taking fifty rupees from A’s account putting in to B’s account. Transaction T 2 is also a wired transfer that is taking ten percent of whatever amount is their in A’s account and crediting in to the B’s account. 





(Refer Slide Time: 33:25)

 

Now suppose i have to perform all activities of T1and then start with all the activities of T 2 obviously it is a serialize schedule. Such a schedule is called a serial schedule that is performing all activities of one of transaction before starting first activity of the second transaction. So this equivalent to performing T1 followed by T2. 

(Refer Slide Time: 35:27)

 




If i perform all activities of T 2 and then start with the first activity of T1 and then perform all activities of T 1, this is also a serial schedule, this is also correct schedule and this is equivalent to T 2 followed by T1. However serial schedules do not have does not necessarily mean that all activities pertaining to given to transaction completed before the first activity of the next transaction T1 is taken up.
 
For example: this one this slide shows how activities from T1 and T 2 are interleaved. 
The color activities here belong to T 2 that is read a T equal to eight times “point one” A equal to A minus t write A and then the transaction T2 has not yet completed. But transaction T1 is already begin. Read ‘A’ equal to A minus 50 and so on and then transaction T2 continues here and transaction T1 also continues here. However, if you notice even this schedule is a serialize schedule or it is a serial schedule. This schedule is equivalent to performing T 2 followed by T1.

Why is this so? Have a look here. Have a look at how the activities of T 2 and T1 are interleaved? All activities performing are regarding updation of data element A is completed of the transaction T2 or from the transaction T2 before first operation involved involving data element A is even performed from transaction T1. Same thing with respect to B and we can actually see that, we can rewrite it that is we can take these elements of B back here and put this back here without changing the semantics that is without changing the overall semantics of this serialized schedule that is once this schedule finishes, it is equivalent to as though T2 was executed first followed by T1 that is although activities of T2 before the first activities of T1 ever started. 

(Refer Slide Time: 38:15)

 

This brings to some definitions of how we can enforce serializability over a set of database activities. We define the term conflict serilazability by first defining. The term called conflict between database activities then we say that then we say that particular schedule is conflict serial able. If there is no conflicts or with respect to or when it is being transformed to a serialize schedule. A schedule in which, all transactions all activities of one transaction is performed before all activities of the second transaction. Now consider 2 activities i and g belong to two different transactions is T1and T2.

Now i and j can be swapped in their execution order if they refer to different data element because it does not matter. One is referring to element A other is referring to B. It does not matter. We can perform them in any order and i and j can be swapped in their execution order even if they refer to same data element, however all that they are doing is reading all the contents of the data element. Even both are reading the same data element does not matter who is reading first and who is reading second. As long as nothing else in between them that is and they are said to conflict that, i and j cannot be swapped in their execution order if atleast one of them is a write operation. If at least i is trying to write and j is trying to read. We cannot perform i should be read before j, we cannot have j read the database before i writes it and so on. 

(Refer Slide Time: 40:04)

 


The same thing is true when both are write operations. Given a schedule ‘S’. A schedule is something like what we saw in this slide here, it is a schedule. Now suppose we are given a schedule like this that is the activities of T2 will perform will like this and activities of T1 are performed and activities of T2 continues and so on. Now given a schedule: If we in order to determine whether it is safe or not, whether it is serialazable or not, we can identify this, if we can swap or if we make one or more swapping of activities database schedule activities and bring them to a serialize schedule were all activities of one transaction are performed before all activities of the second transactions without encountering any conflicts as a way defined in the previous slide. 



(Refer Slide Time: 40:12)

 


Then this kind of schedule is said to be a conflict equivalent schedule and it is also said to be a conflict serilizable schedule that is, it can be serialized or it can be equivalent to serialized schedule where the equivalent criteria is conflict equivalence that is conflict serializable.  

(Refer Slide Time 41.29)

 



There is an alternate weaker notion of serializability called view serializability. Conflict serializablity is quite strong and in many cases we do not need the stringent property of conflict serializability. The view serializability simply says the following. Suppose for each data item Q, suppose there are set of transactions that are happening in a dbms system. Now for each data element Q, suppose it was transaction ‘Ti’ which reads the initial value of Q in a serialized schedule that is in a serialized schedule ‘S’ that is in any other schedule which is view serializable.  

(Refer Slide Time: 42:20)

 

It should also be the case that, the same transaction is the first transaction to be reading   this data element S or this data element Q and similarly in the given schedule, for each data item Q if T j precedes or T j writes to Q before Ti that is before Ti reads, then the same dependency should be maintained in any other schedule that is anybody writing to a data element before somebody else is reading it, this kind of dependency should be maintained in whatever if the schedule has to be view serializable. Similarly, the last operation that is whoever performing the final write in a serializable schedule should be the same transaction who performs the final write in whichever schedule is view serializable. 

 









(Refer Slide Time: 42:38)

 


Let us take an example: there are 3 transactions shown in this figure. The first transaction T1 reads data element queue and writes it back to disk after performing some operations which is not relevant here. It is only the reads and writes which we have concerned about it and T 2 just writes some something in to queue and T 3 alone it is writes something into Q. Now the following schedule is a view equivalent schedule or a view serializable schedule. Note that it is not a pure serialize schedule. The activities of T1 and T2 are being interleaved here. All activities of T1 are not completed before activities are performed.

However, if you see who is reading the first, who is the first transaction to read the data element Q. Suppose we take a serializable schedule that is T1is followed byT 2 that is T 2 followed by T3. If in that serialize schedule, the first transaction to be reading data element Q is T1, that is the same thing in the schedule as well. Now is there any read before write dependency? For example: T1 is reading before T2 is writing  or rather  T1 is reading before T3 is writing. Is that dependency maintained here? that is read before writes write before writes either of those dependencies are maintained here and who is the last transaction writing to Q that is T3 that is same thing here that is T3 is the last transaction that is writing to Q. Therefore this is the view equivalent schedule that is it corresponds to T1 happening before T2 happening before T3, because the first data element to read was T1read Q was read one last element write in to Q was T3. 

So when the T 3 finishes, then there is no difference between saying it was performed as T1T 2 T 3 or it performed in this fashion. However note that, this schedule is not conflict serializable. If we try to swap updates here, that is if he try to swap the activities here in order to get serialize schedule, then we encounter a conflict that is take a look at the second and third activities here. Now in order to bring in a serialize schedule, we have to swap the second activities with the third activity. So that T1comes here and T 2 comes here. However both of them are writes and we saw that when both of them are write operations on the same data element and belong to two different transactions, then you cannot swap them, they are in conflict. Therefore this schedule is not conflict serializable. However it is view serializable that is, as far as database view is concerned, it remains the same whenever we look at before T1 or after T3. It is being the same. 

(Refer Slide Time: 46:34)

 

So every conflict serializable schedule is also view serializable. However, the converse is not true which was the example we saw in the previous slide. Usually, this thing happens  that is usually, we find view serializable schedule that are not conflict serializable whenever what are called as blind write. A blind write is something that we saw in the previous slide here. 

That is transaction T 2 T 3 shown in slide contains no read operation. They just write in to the database, without any read operation. So such it is a hall mark of blind writes which bring in schedules that are view serializable, but not conflict serializable. 













(Refer Slide Time: 47:21)

 

Let us look at the last concept of what are called as recoverable schedule and in order to understand what are the requirements of database recovery? Consider the set of following set of transactions as shown in the slide here. There are two transactions here T8 and T9. T8 reads a data element ‘A’ does some modifications and writes set and then goes about reading about some other read data element and so on.  After it writes here transaction T9 reads the data element A and possibly let us say displace it and it does not perform any writes.  So, as you can see this is the conflict equivalent schedule That is i can swap read A with read B which will give me serialize schedule that is T8 followed by T9 and swapping by read B and read A it is it is still conflict equivalent. So, therefore performing read A write A read A of T9 and read B of T8 is conflict equivalent. 


















(Refer Slide Time 48:27)

 

However, suppose let us say T9 that is read A and displayed that is display the latest value of this stock price or whatever. Suppose this T9 commits and displays the value of the value of k. But T8 is not committed, that is T8 is not completed still and it sees that it cannot commit because some problem somewhere and it has to rollback. Now if it rolls back then T9 also has to be roll backed because it read value of A, after it has been written by T8.  

However we cannot rollback T9 because it is already committed and committed is not the durability condition here, that is we have already made some commitment in some sense that is we have displayed the new value of the stock or whatever.  So in such a situation, it is impossible to recover from the failure of T8 because we cannot the rolling back of   T8 will also require rolling back of T9 it is impossible. So this is in an example of a non-recoverable schedule. 















(Refer Slide Time: 49:35)

 

So serializability or conflict serializability alone is not enough. We need to also look at recoverability of a particular schedule of transaction events if we have to be recover from a database crash. So database system requires a recoverable schedule and finally let us have look at the problem of cascading roll back which is also quite important when it comes to recovery. Even if a schedule is a recoverable, to recover from a failure of a transaction in some times, there is need to roll back several transactions. The previous example was also an example of cascading roll back that is supposes transaction T9 not committed and transaction T8 roll back, then T9 also has to roll back. So in order to make it recoverable we have to defer the commit of T9 until after T8 has committed. 

So that will make recoverable. However, it still contains the problem of cascading roll back. So this example also shows cascading rollback situation where there are three transactions T T1 and T 2 and has read a value of A and written it and whatever value is return by T is being read by T1 and T2. Now T1 and T2 cannot commit that is cannot display the new value of A until T has committed. Otherwise, it will become non recoverable.  Now even if they do not commit and suppose T has to roll back, it has a cascading effect in T1 and T 2 that is in fact T 2 is dependent on T1 and T1 is dependent on T. So a roll back of T will cause a rollback of T1 which in turn will cause a roll back of T2. So such cascading roll backs will lead to an undoing of large amount of work from several different transactions in case of any database crash or system failure. Cascading rollback is an undesirable thing to happen and leads to an undoing of lot of work. 







(Refer Slide Time: 52:05)

 

So when we are looking at schedules of operations that are performed by OLTP environments, they have not to be only serializable. They have to be recoverable cascade list as far as possible we should try to avoid cascading roll backs.

(Refer Slide Time: 52:27)

 

So that brings us to end of the introduction session on database recovery where we have said the ground for all the issues that make up that are concerned or that we have  concerned ourselves with whenever we are dealing with database recovery.


(Refer Slide Time: 52:46)

 

For example: We saw the notion of the transaction that is when we are recovering from the database crash, we have to ensure that we does not leave any transaction in a half or partially committed state. It should either be fully committed or no operation should have performed that is all are nothing. Atomicity transitions have to be obtained whenever we  are recovering from database crash or the system crash and in order of that in happen we have recoverable schedules and it is not sufficient for schedules to be serializable and also whether it is conflict or view serializability, it is not sufficient for schedules to be just serializable. 

They should also be recoverable schedules and as far as possible they should be cascade list schedule that is crash or rollback of one transaction should not automatically mean that several other transactions or several other work that has been partially completed has to be rollback. It is not even partially completed, even though they are completed just waiting for the commits which what we saw in the operations in the previous slide that is even though transactions T1 and T2 are completed they have read and written a value of a. They are just waiting for the original transaction to commit and because of some problem the original transaction may crash and because of that even all the operations that have been completed have to be rollback without any reason by themselves. So this brings to the end of this session.  

Database Management System
Dr.S.Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 28

Recovery Mechanisms II

Hello and welcome. In the previous session we started looking into recovery mechanisms in databases, especially we looked into the back ground of transactions and the idea of a transaction and how recovery should maintain consistency in terms of different transactions. That is it should not leave a transaction in a unatomic form that is when a database is consistent or either all transaction is performed completely or they have not been performed and it should not violate an integrity constraints and it should be serializable and so on. 

Today’s session we are going to look at some mechanisms for recovery. Specifically we are looking into what are called as log based recovery. As the name suggests log based recovery means that recovery mechanisms for the database are performed using transaction logs. That is whenever transactions happen certain elements of the transactions are logged into log files and using these log files, we can try to recover the database into a consistent state in case of any kinds of failures. 

(Refer Slide Time: 2:37)

 

So let us briefly summarize whatever we have learnt about the transactional requirement of databases before we look into recovery mechanisms or log based recovery mechanisms. Firstly, why recovery or in what situations do we talk about recovery mechanisms? Recovery is pertinent in the phase of failures and given database system can be subject to different kinds of failures there could be system failures, the power could just go off database, there could be media crashes that is the disc crash or something of that sort. There could be communication failures that is network has failed and transaction which was partially submitted or especially if you are having distributed databases, transactions which was started on other machines failed communication between the two machines failed and so on. And there are of course transaction failures, that is transaction could fail for a variety of reasons including the above kinds of failures that we are talking about. The transaction could fail because they violated integrity constraints, transaction could fail because they could not, there is no serializable schedule for the set of activities from these transactions or they could fail because whatever schedule that’s being currently performed has led to a dead locker something of that sort.

So in many of these failures we need a recovery mechanisms. In the last case that is transaction failure usually its automatic that is the system is still functional the dbms, the database everything is still functional so it is just a matter of re submitting the appropriate transactions after waiting for a while and hoping that it succeeds this time rather than fail or if the transaction has violated an integrity constraints, it involves something like raising an exception or intimating the application program saying your transaction is wrong or I cant perform your transaction because for example your account doesn’t have enough money to withdraw so much amount that you have asked something like that. 

So leaving aside the last point here, in most of the other cases the dbms or the databases has crashed and it has to be booted up, it has to be brought up again. And once it is brought up again there is no guarantee that what ever data that’s there in the database is consistent and it will be un safe to just start the database and have it running from where ever it is left off because any amount of data that was there in the volatile memory in the ram would have been lost and we don’t know how we can set these things right. 

(Refer Slide Time: 5:30)

 

So what are the properties of transactions that we have to assure when we are providing recovery mechanisms? The property of transactions as you know is called the acid property of transaction that is atomicity, consistency, isolation and durability properties. So let us briefly summarize what is meant by the acid properties and what do they require. 

Atomicity means either all updates that are performed by the transactions are performed that is either all updates that are required by the transactions are performed or none of them are performed. We can’t have a transaction that has performed half of the updates that were made for it that is we cannot have a transaction that has debited my account in a wire transfer transaction and has not credited the other account and the money is lost. So it should be either all or none kind of operation. 

Similarly consistency requirements is that when a transaction has to finish or if a transaction has to successfully complete, it should not violate any integrity constraints of the database. That is given a consistent database a valid transaction should leave the database in another consistent state. It need not be the same state, it could be another state but it should be a consistent state. If it violates any kind of integrity constraints on the way then you have to roll back, you can’t complete the transactions and you cant leave it there as well because then we were violating the atomicity requirement of the transaction. 

Isolation constraint that is the I in the acid property states that whenever there are multiple transactions that are happening on in a dbms, the net effect of all the transactional updates should be such that or should be equivalent to a schedule in which all updates of one transactions are performed before the first update of the next transaction is taken up. That is it should be as though the transactions have run in some serial order, it need not actually be run in serial order that’s what we saw in the previous session. Activities can be interleaved as long as this interleaving is safe. That is we saw notion of what is meant by safe that is the notion of conflict serializability that is we should be able to conflict or view serializability which we saw that is we should be … (Refer Slide Time: 08:13) into a serialize schedule without encountering any kinds of conflicts. 

And the last property is that of durability. That is once a transaction commits it cannot be rolled back. The changes that are made after a transaction commits are durable, it is persistent and not only that the changes are made inside the database that is on to disk, the changes could also entile performing some kind of physical operation like we gave the example yesterday of dispensing money from an atm. that is if once a transaction for withdrawing money succeeds and it commits then the atm dispenses the required amount of money that was asked by the customer for withdrawal. 

Now once the money is dispensed you cannot roll back the transaction. the problem … (Refer Slide Time: 09:13) if it is found that there was some error and the money shouldn’t have been dispensed, you have to look at solutions that go beyond the database systems, you cant ask the database to just roll back this transaction and leave it at that. 

(Refer Slide Time: 10:31)

 

What are the different states in which transaction lies? The first state is the active state that is whenever a transaction becomes active and it is executing, it is said to be in an active state. Once a transaction has performed all its updates and it is ready to commit that is it has finished its executions and it is ready to commit then it is called a partially committed state. And once a transaction discovers that it cannot commit, mainly for example it has violated an integrity constraints or its schedule cannot be serialized and so on then it is said to be in a failed state. And once a transaction has rolled back from its failed state that is it has undone whatever it had done already then it is said to be in an aborted state. And if the transaction has committed successfully then it is in a committed state and either aborted or committed state is called a terminated state. 

Now let us look at these states of a transactions in terms of recovery. That is what kinds of states require recovery of a transaction. now if a transaction is terminated, its either aborted or committed then we wouldn’t have lost atomicity as a part of the transaction that is if the database crashes after a transaction has committed or aborted, it should be such that atleast the dbms should be designed such that these transactions should not be executed again. That is we should not submit this transaction again to the dbms. 

For example if the user has requested for withdrawal of say 1000 rupees from an atm and the transaction has aborted or rather the transaction has committed and 1000 rupees has been dispensed from the atm and right after commit, the system crashes then the dbms should be designed such that whether or not this data is there on or has been updated on the dbms for a variety of reasons which will see shortly. Whether or not this data is updated on the dbms this transactions should not be run again. That is the user should not be given another set of 1000 rupees after the system comes back because the transaction has already run and the operation has already been performed whatever operation has been asked for. 

On the other hand if a failure occurs during let us say active or partially committed state then we may have to, in some cases undo whatever has been done by the transactions, whatever operations has been done by the transactions and then probably resubmit the transactions. That is re run the transaction once again from the start.

(Refer Slide Time: 13:54)

 

This slide shows the state diagram or state transition diagram for the different sets of a transactions. That is we start from the active state and the active state can go to either partially committed state or a fail state depending on whether all operation in the transactions have been executed successfully or whether there have been some problems. And even in a partially committed state, there is a chance of failure if the transaction finds out that it cannot commit. For example if the transaction is dependent on some other transaction to commit in case of and the other transaction rolls back and this transaction is subjected to a cascading roll back. So in that case even if all the operations have been… (Refer Slide Time: 13:27) there is still a chance of failure even from the partially committed state. And if nothing goes wrong then we can go ahead and go to the committed state or once we reached a failed state then the transaction goes in to the aborted state. That is it rolls back whatever been done, so it undoes whatever operations has been done and it goes back into an aborted state. 










(Refer Slide Time: 16:14)

 

Let us look at the concept of serializability again where we talked about what is meant by serial schedule and how do we know whether a serial schedule is valid or not. In order to determine whether a serial schedule of transactional activities that are interleaved between one another, in order to know whether this is valid or not we have introduced the notion of conflict serializability. As, if you remember conflict serializability is a mechanism which defines the notion of conflicting database activities. 

What are conflicting activities? Consider two activities I and J belonging to two different transactions T1 and T2. Now I and J can be executed in any order that is I before j or j before I doesn’t matter if I and j refer to different data elements because they don’t affect one another. And I and j could still be executed in any order, if they refer to the same data elements as long as both of them are just read operations. So both of them are just reading the given data elements, so it doesn’t really matter whether I read the data data elements first or j reads the data elements first. On the other hand if either I or j or both contain a right operation on the same data element and both of them of course refer to the same data element then they are said to be conflicting. So we cannot swap the execution of I and j and expect that the swapping is an equivalent schedule to the earlier schedule. 

So if I have a schedule of operations, database update operations I can verify whether the schedule is safe or not by seeing whether it is conflict equivalent. That is can I keep rearranging the operations of this schedule so that I eventually end up in a serialize schedule that is all activities of one transaction are performed before the activities of the second transaction. So I end up in a serial schedule without encountering any conflicts during the way. 





(Refer Slide Time: 16:25)

 

So if a schedule can be transformed in such a way or if it is conflict equivalent to a serialize schedule then it is said to be a conflict serialize schedule. 

(Refer Slide Time: 19:07)

 

We also saw the notion of non recoverable schedules that is in what cases, we can never recover from a crash and so on. So the slide here shows an example of a non-recoverable schedule. There is a transaction T8 which is reading a data element A and writing something back on to A. That is it has performed some computation say as long as when we are concerned about recovery, we are not really concerned about what kind of operation it makes. As long as there is some right operation we assume that some update has taken place, may be there was no update that is it has just read the data elements and return it back for whatever reason but at the level of recovery mechanism… (Refer Slide Time: 17:15) some change or there is some modification that has happened. So this transaction T8 has read a data element A and it has returned it back onto the data base.

Now after it has returned it back onto the data base, another transaction T9 read that element of A and of course did something and then committed. So this commit operation could probably involves some kind of physical operation like say displaying the data element may be it is the new stock prize or whatever, it just displayed the data element. 

However this transaction T8 try to do something more and crashed. Now because transaction T8 has crashed, it has to be rolled back that is whatever operations that are performed by T8 has to be rolled back but we cannot roll it back because transaction T9 which has already read the changed data element has already committed. And whatever data that is returned is already out in the open and it’s been displayed. So such a schedule is a non-recoverable schedule. And how do we prevent non recoverable schedules from occurring? 

Simple way of preventing non recoverable schedule is from this example is to note that transaction T9 cannot commit until transaction T8 has committed. That is if a transaction is reading a data element that is written by some other transaction then it cannot commit until the previous transaction has committed. So in that way transaction T9 cannot display the value of data element A and until and unless transaction T8 has successfully completed. 

(Refer Slide Time: 20:36)

 



But even then that is even if we stipulate that transaction T9 cannot commit until transaction T8 has committed… (Refer Slide Time: 19:18) cascading roll backs. That is shown in this slide here. That is there are 3 transactions T, T1 and T2 and this transaction T has read data element A and modified it and returned it back into the database. 

Now this modified data element now is read by T1 and then T1 in turn modified it again and wrote it back in to the database. And T2 then read this second modified database, second modified element that is the data that was modified by T1 and then probably try to display it or something. And of course, because we have ensured that none of them can commit until T can commit, they are just ready and waiting for performing whatever their commit operation tells them to do that is whether you display it or dispense money or whatever. 

Now transaction T instead of committing crashes for whatever reason. Now because transaction T has crashed, transactions T1 and T2 even though they have completed successfully have no option but to roll back. So this is the problem of cascading roll back. so even if the schedule is recoverable, sometimes suppose transaction T is a long running transaction, it runs for several minutes or probably sometimes even several hours transaction T1 and T2 are short transactions and there are several such transactions are waiting on transaction T2, for transaction T to commit. 

Now for whatever reason if the T transaction crashes whether it is a transaction failure or a system failure or a media crash or whatever, we have to roll back and all these transactions that are been waiting on transaction T.   

(Refer Slide Time: 21:17)

 

So let us look at how to tackle all these problems in a systematic fashion, so the concept of recovery. Recovery from transaction failure is a process of restoring the database to the… and where do we restore it? We restore it to the most consistent state that was there before the failure. And there are two kinds of recovery strategies that we are going to be seeing today which are called the deferred update strategies and immediate update strategies. Deferred update essentially means that the database or updations to the data base are deferred until after sometime which will formalize later on. And immediate update techniques update the database as and when transactions are running. The database is physically changed as and when transactions are running. 

(Refer Slide Time: 22:20)

 

So this slide defines both of these techniques. Again the database is not modified until a transaction reaches it commit point in deferred update techniques and in immediate update techniques database is updated as and when transaction progresses. However transaction fails in immediate update techniques, you have to undo this operation that is you have to change this. Therefore they have to be logged whatever update were made to the database have to be logged before the updates are made. Obviously you can’t log the updates after making the updates because what happens if the system crashes, once you have made an update and before writing the log. 

On the other hand if you have written a log and the system crashes before making the update, it is still not a so much of a problem as we will see later. 
 









(Refer Slide Time: 23:15)

 

Before we go on to recovery techniques, there are two things that we have to define and we have to be careful about how these impacts recovery techniques. The first issue is that of cache management. You might have studied in an operating systems course that most operating system use what is called as buffer caches. And what are buffer caches? Buffer caches are some buffered areas in memory that act as a cache for data that are present on disk. That is whenever a disk block or a disk sector is accessed or is sort by the operating system instead of just reading one disk block, usually operating systems perform what is called as read ahead that it reads a set of blocks into main memory. 

And all writes that are performed on to disk sectors are initially performed just on the main memory and not onto the disk. And its only once in while these cache or this buffer cache is flushed onto disk. This is done in the interest of performance that is why for example in most operating systems, you need to perform some kind of disk sanity checks if the operating systems crashed midway because not all blocks that have been modified would have actually been written on to disk. 

Now this buffer cache is an operating system construct that is it is in the control of the operating system. And application programs or user level programs that run in an operating systems usually don’t have control for this buffer cache but for database recovery we need to have control over this cache because we can’t assume that the operating systems has written something onto disk after we have said write because operating systems in turn has its own mechanisms that might defer writings on to disk and which may impact a recovery process. 

Therefore typically in many database management systems, what is done is a part of the buffer cache that is maintained by the operating systems is given to the dbms. That is the dbms is given control of this buffer cache so that it can, that is the dbms can control … (Refer Slide Time: 25:48) into the buffer cache and so on.
And such kinds of cache pages which are given to the dbms are called dbms caches. And of course there is also the problem of what happens if the system crashes when the cache has being written on to disk. That is I have written something onto cache and now I am flushing this buffer cache but during this buffer cache flush, the system crashed and what do we do. The cache is partially written and the data could be inconsistent and so on. 

For that a technique called shadow paging is used which we are going to study in the next session on database recovery technique. The shadow paging technique that is used for data base recovery can also be used for maintaining or recovering cache contents in the case of crashes. 

(Refer Slide Time: 26:50)

 

The second issue that we are going to be concerned about is the concept of … (Refer Slide Time: 26:55) log. Now we have mentioned in passing that in order to aid the process of recovery from databases we maintain logs that is whatever updates are made to the database are all logged in some log file. Now this log file keeps on growing because every update that is made to the database, the information about this update is kept in this log files.

Now this log files keeps on growing and we don’t know when a crash would occur and how much of history information we need and so on. So how do we prevent this log from growing forever and probably becoming bigger than the database itself. So the answer to this is the notion of a check point. A check point in a log records a state where all transactions that have been committed until this point in time have been physically modified on the disk in the database. That is the database has been updated and everything is fine for all the committed transactions until a check point. 

So all committed transactions that have been, information about whom have been stored in the log until a check point can be thrown away. That is at a check point we can throw away data about all the committed transactions that have happened before the check point. And at what frequencies do we have to check point the log? That is check pointing rather that is the process of introducing a check point in a log as you might have imagined is a separate process by itself that is we have to decide at what intervals at or at what frequencies are we going to introduce check points into the log and what should be done when a check point is being introduced. 

And check pointing actually involves suspension of all activities of the database, all transaction activities of the database temporarily until we know for sure that all the that this check pointing criteria is made that is all the committed transactions have been successfully updated onto the disk. 

(Refer Slide Time: 29:10)

 

The algorithm for check pointing is quite simple but quite costly in terms of operations that is in order to take a check point, we first suspend all transactional activities temporarily because we don’t want more data to be written when we are handling this check pointing. And then we force write that is we flush all main memory buffers that have been modified to disk that is whatever has been, whatever data that had to be updated onto the database we force write all of these committed … (Refer Slide Time: 29:47) and then we write a check point note in the log file and and also of course force write this log on to disk. 

The fact that we have written, the fact that we have encountered a check point should also be recorded persistently onto disk because once we have thrown away information about other transactions, we can’t lose the fact that we have performed a check pointing operation and then we resume transaction activities.



(Refer Slide Time: 30:20)

 

There is also a notion of fussy check points where that are more, slightly more efficient than the usual check pointing techniques. Note that check pointing requires suspension of all transaction activities and if this is done too frequently then it impacts database performance itself. 

Now let us see where is the biggest overhead during a check point and seek can we do something about making this check pointing faster. The main overhead in check pointing and I am sure you would have imagined that is the flushing of all the buffer cache onto the disk. That is each buffer cache contains set of disk blocks and so all of these disk blocks have to be physically flushed on to disk and this is what is going to take the most time. 

Now in fussy check pointing what happens is that transaction activities resume after writing the check point entry into the log, even though flushing has not been completed. That is even the check pointing, check point entry could be in the cache and all the flushing actives activities are still going on but transaction activities, the transaction activities resume. However the previous checkpoint is not released that is the older log entries are not deleted until after the new check point entry has been flushed onto the disk. 

So it’s a background operation where until we are sure that the new check point operation has been written on to disk, this can be written onto disk only after all the buffer cache buffer has been flushed on to disk. So until we are sure this has been done, the previous set of log entries are not deleted.
 



(Refer Slide Time: 32:11)

 

So let us look at the first kind of recovery technique which we called as the deferred updates recovery. So what is the notion of a deferred update recovery? As the name suggests deferred updates means that the updates to the database are deferred until transaction commits that is until transaction has reached a ready to commit state. 

The overall strategy for a deferred updates recovery is simply this thing. That is a transaction cannot change the database. So even if a transaction has run half or 90% or 95% or whatever, it has not made any changes on to the data base as far as until it has committed. So until it reaches the commit point, the database is not updated. And a transaction does not reach its commit point until all its update operations are logged and the log is force written on to disk. 

That is the transaction does not say am not ready to commit until all its operations that have been done have been logged and this log is available on the disk. Therefore even if the transaction crashes now and even if the database is not updated, we have the log entries which says that these are the changes that were made and the transaction is now ready to commit and it can commit. 











(Refer Slide Time: 33: 42)

 

So using this let us see how we can perform recovery. We first look at recovery in a single user environment. That is it is a sequential database engine where transaction… (Refer Slide Time: 33:52) transaction is performed one after the other. This is a simplistic case but it helps in understanding how the deferred update technique works. So deferred update techniques uses two lists of transactions. That is one is the list of all committed transactions, when does it use the two lists of transactions. That is after, it is after the disk that is after the system has been brought up following a crash and the system is asked to recover to a consistent state. 

Now once the system is asked to recover to a consistent state, the recovery process starts by using two lists of transactions. One is the list of all committed transactions since the last check point and the list of all active … (Refer Slide Time: 34:42). Since it’s a single user operation there would be utmost one such transaction which would have failed in an active state. And a logs that are maintained for these transactions are maintained in the following form which is shown here. That is the first element here says that this is a write item that is something has been written on to disk.

Note that as far as recovery is concerned, we are interested only in these write items, logs can be used for a variety of purposes something like to understand the behaviour of transaction, to profile the performance of the data base and so on and so forth. But as far as recovery is concerned, we are just interested in what changes have been made to the database. Therefore we are interested in only these write items. So it says this is a write item belonging to transaction T involving data element x and this is the new value that was written on to the x or that has to be written on to the database for element x. 




(Refer Slide Time: 35:49)

 

Now once these two lists are maintained that is the list of all committed transactions since the previous check points and the list of all active transactions. we first start by re doing all the committed transactions, that is we take the set of all committed transaction since the previous check points and then go about looking at the logs and see what values they had written on to what data elements and we start writing those values once again. 

We don’t care what is the semantics of these values or whether these values were read or anything because these transactions were already committed, the cash has been dispensed already. so we don’t have to do any physical operations, we don’t have to even tell the application program that we are doing these things because these were already committed and we know what are the values that has to go into the database, we just write those values. we just start from the previous check points and start, data element x has to have a value of 10, data elements y has to have a value of abc or whatever and we just start writing those values back in to the database.  

And then for all the active transactions which crashed midway … (Refer Slide Time: 37:12). During execution we just have to restart all those active transactions because none of the active transactions are physically modified the database that is what the deferred update technique all about. That is transactions don’t modify the database until they are ready to commit and they won’t be ready to commit until they have made all the log entries on to disk. 

After they have made all the log entries on to disk and said that they have been committed then they have been treated as a committed transaction and the first… (Refer Slide Time: 37:48) and all the redo operations from the transaction is performed. If for any case the transaction that is the ready to commit tag does not go in to the log or whatever has been written by the transaction is not flushed on to the log and the system crashes then it is treated as an active transaction and it is just started once more. And it runs once again and makes changes on to the logs and if everything goes well that is the log is flushed on to disk then it is ready to commit and the data base is updated.

(Refer Slide Time: 38:28)

 

What about updates if the database is actually a multi user environment and there are several concurrent transactions that are running at the same time. In a single user environment, we don’t have to worry about concurrency control that is how transactions are serialized that is they run in a serialized fashion by default. But in a multi user environment there is the problem of serialization of transactions. Now serialization of transactions is not the problem of the recovery part of the database, it’s usually handled by the concurrency manager that is whatever concurrency control techniques are used for managing concurrency. 

Now we assume that for deferred update techniques, we require that concurrency control uses what is called as a strict two phase locking. You might have heard of the notion of locking in several contexts in operating systems and probably even in systems design and so on where locking essentially means that if a transaction is performing some some updates on a database or some data elements, it obtains a lock. That is it locks the data elements so that nobody else can read or write to the data element or can access the data element. Of course there are two kinds of locks that is read locks and write locks so read locks can be shared but write locks or exclusive locks cannot be shared that is once a transaction has obtained a lock on a data element that it is going to modify, no other transaction can even read the element until the lock is released by the transaction. 

So in strict two phase locking all the locks that are held by a database or by a transaction are not released until the transaction reaches its commit point. Now what does it mean that it reaches its commit point? That is all the updates it has made are logged that is are written on to logs and the log is flushed on to disk. now the transaction is sure that somewhere whatever updates it has made is persistently stored that is the updates it has made is safe somewhere, its not just lost once if the system crashes only then it will release its locks. So only after it releases its locks can other transactions read the data element, read the corresponding data element. 

(Refer Slide Time: 41:08)

 

So assuming that we have strict two pl that is strict two phase locking, the recovery process can follow the technique shown here. Firstly make two lists of transaction, once after a database has crashed and it has restarted and the recovery algorithm is begun and it is starting to recover, make two lists of transactions. That is first is the set of all transactions that have committed since the previous check point and the list of all active transactions. 

Now for all the set of transactions that have committed, we have to redo the operations. This is the same thing we have done in the single user environment. However here we have to ensure, we have to explicitly state that redo of the operations are performed in the same order as they appear in the log. We cannot try to optimize this redo operations by introducing some concurrency there because they may violate some kind of serializability conditions if they are performed in some other order … (Refer Slide Time: 42:24) between two or more updates such that they cannot be swapped in their ordering. 

And once the redo operations are performed that is once all the committed transactions have been persistently written on to disks, we then restart all the active transactions and before that we release all the locks that have been held by this active transactions and again the ground is free so that whoever wants the locks can now hold those locks. So we release all those locks and resubmit all the active transactions once again. 




(Refer Slide Time: 43:01)

 

Let us take an example of different transactions and see what happens in a multi user environment. So this slide show a set of transactions and two different events, one is a check pointing event that happens here and one is the system crash that happens here. Now there are several transactions in a database here. Transaction T1 has already committed before the check pointing event happened. Therefore after the check pointing data about T1 are thrown out, we don’t even need data about T1 anymore. However T3 and T4 have started before the check pointing operation but they have not completed, therefore we cannot throw away these data about T3 and T4 even if check pointing is performed. And transaction T2 has started only after the previous check point but has committed its operation before the system crash. 

While T4 and T5 have not yet committed, T4 is really a long transaction that is taking place and they have not committed when the system has crashed. So what happens in the update operation here that is during the deferred update operation? T1 is not concerned at all, it doesn’t figure in to the picture at all because there is no data about T1 so at the time of the updates that is happening here … (Refer Slide Time: 44:31) committed but the data is still there because it’s occurring after the check point. And because it has committed transaction T2 will undergo redo operation that is all its updates are logged into log file and using the log file, the database is updated. Transaction T3 has also committed before the system crash, it has started before the check point therefore its data will not be thrown away and because it has committed before the system crash this is also re done. That is redo operation will be performed on transaction T3. 

And transaction T4 is still active at the time of system crash as this transaction T5. Therefore both T4 and T5 have to be resubmitted to the dbms that is after releasing all their locks that they have held, they have to resubmitted back to the dbms. 


(Refer Slide Time: 45:27)

 

So that’s what this slide says that is data about T1 can be safely removed after check point, it doesn’t even figure during system crash. That is it doesn’t even figure during updates recovery rather. And data about T3 and T4 should not be deleted at check point because they are not committed and transaction T1 is unaffected during the recovery process. T2 and T3 are subjected to redo operations and T4 and T5 were aborted and resubmitted back into the dbms. 

(Refer Slide Time: 46:00)

 

There is some efficiency issues about redo operations. small thing we can notice, if a data element x has been written (Refer Slide Time: 46:15) it is enough if we just write the last update on to the database because anyways if we write a previous update, it is going to be over written by the next updates. 

Hence for redo operations we can start from the end of the log and start making updates moving backwards in the log. And we should not write a data element on to the data base if it has already been written once during redo because we have already written the latest value during redo.

(Refer Slide Time: 46:45) 

 

What are some of the properties of deferred updates? There is no undo that is required. If you have noticed we have only talked about redo, there is no undo operations that are required. Why? Because the database is not modified at all, it’s only the transaction logs that are modified. And since all locks are released in strict two pl, all locks are released only after commit. A transaction can read a data element that is being modified that is no transaction can read a data element that is being modified by another transaction. Therefore there is no possibility of cascading roll backs because one transaction has already read a data element that is been modified by some other transaction and waiting for it to commit. So there is no such possibility, so there is no possibility of cascading roll backs. However potentially there is a large amount of disk operations during commit because enormous amounts of updates in especially large transactions that have been written on to logs have to be written back on to the disk. 








(Refer Slide Time: 47:53)

 

The next technique that we are going to look at is what is called the immediate update techniques. In immediate update techniques, the database is updated as and when transactions execute. However for the sake of recovery, database updates are performed after the updates are recorded in the log and the log is written on to disk. That is only after we know that is there is still some kind of deferred updates happening here, that is updates are deferred only after we know that the log is written on to disk and before which we modify the database. 
 
As and when we know that a particular log entry has been written on to disk, all those corresponding entries can be modified. And however in the phase of a crash, we have to undo all un finished transactions since the last check point. We need an undo operation here which is not required in the deferred updates. And we still need the redo operations for redoing all the activities of the committed transactions since the last check point. 















(Refer Slide Time: 49:03)

 

So let us first look at the undo operations that is what should be done for all the un finished transactions. Now in order to perform undo, we need an extra element in the transaction logs. This is shown here that is a transaction log contains this write element, write item element and and transaction t that is this element belongs to transaction t involving data element x and it says that the old value of x was this and the new value was this. Therefore when we are undoing it, we have to replace x by its old value and because we dint have an undo, we dint have to store old value in deferred updates. 

And of course undo operations have to be performed in the reverse order obviously because the oldest values have to remain on the database. And these log entries, the way these log entries are created is such that undo and redo operations are what are called as idempotent operations. What is an idempotent operation? (Refer Slide Time: 50:11) idempotent operation is something where it does not matter how many times you perform the operations. 

For example some problems during undo and undo couldn’t finish, we can just restart this undo process from the beginning once again and then run it again. And it doesn’t matter because it’s just re writing the old values, its not performing any computation. It’s not saying that value of x was changed by 5% so reduce it by 5% or something like that. So it’s not performing any computation, it’s just re writing back on to disk. 








(Refer Slide Time: 50:44)

 

So how do you perform recovery in single user environment using immediate updates? Again like deferred updates we use two lists of transactions, list of all committed transaction since the last check point and list of all active transaction since the last check point. And first we start by undoing the activities of the set of all active transaction using the undo policy that we just saw in the previous slide. And then we perform the redo operation of all the transaction that have been committed since the last check point and then we submit all the active transaction back to the dbms so that can execute once again. 

(Refer Slide Time: 51:32)

 

And how do we perform recovery in multi user environments using immediate update techniques. It’s the same, it’s quite similar to that of the deferred update techniques where we use strict two phase locking so that none of the locks are released until the transaction is committed. Therefore there is no possibility of a cascading roll backs. And as before use two list of transaction that is the list of all committed transactions and the list of all active transactions since the last check point. Then undo the writes of all active transactions using the undo policy and redo the write operations of all the committed transactions using the redo policy. And then just release all the locks that have been held by the active transactions and give the transactions back to the dbms that is resubmit the transactions. 

(Refer Slide Time: 52:28)

 

So what are the properties of immediate update recovery? As you can see database updates can happen as and when logs are written on to disk. That is the operating rather the dbms should only keep track of when the buffer cache logs are written on to disks. So as and when the buffer cache logs are written on to disks, the appropriate database update can also start happening. So because of this database updates, the load on database updates is uniformly distributed they are not burstive as in deferred update transaction, deferred update techniques where all updates happen at the commit point. 

But note that any physical activities that is being performed by the application program like say dispensing money or launching a missile or whatever, so any physical activities that is to be performed by the upper application program can be performed only after the database is updated that is only after we know that… (Refer Slide Time: 53:37) and the database is updated. Why? This is because even if transaction is committed, the fact that it has been committed might still be in the buffer cache, it may not be written on to disk and the system could crash. and once the system comes back again, it is treated as an un committed transaction and it is run once again …(Refer Slide Time: 54:05) even if in memory the dbms knows that it has been committed, it cannot or it should not tell the application program saying everything is okay, everything is still not okay. It will be okay only when the the disk is updated that is once they are flushed on to disk, so commits can be performed only either the database or the logs are updated. That is force writing logs on to disk whenever a commit happens. 

(Refer Slide Time: 54:34)

 

So that brings us to the end of this session on transaction recovery using log based recovery techniques. Here we saw several different issues; we started with several different issues concerning recovery in database systems. We looked at the different kinds of failures that can happen and what it means to recover from a system crash or some kind of a failure. 

And there are two issues that affect recovery processes. One is cache management that is for the sake of efficiency disk blocks are usually cached into ram in the buffer cache. And because we are talking about recovery, this can impact recovery process because we are not really sure whether something that has been written on to disk has actually been written on to disk. 

So some part of the buffer cache is usually controlled by the dbms and which is called the dbms cache. And then we looked at the concept of check pointing which allows us to throw away, safely throw away historical information that is stored in logs. And then we looked at two kinds of log based recovery techniques deferred updates and immediate updates both of which use what are called as idempotent, undo and redo operations. There is no undo operation in deferred updates but there are undo and redo operations in immediate updates. And then we also categorized this log based recovery into two different kinds, single user environments and multi user environments. And in multi user environments we have a requirement that we have to use strict two phase locking in order to prevent cascading roll backs in case of a crash recovery. So that brings us to the end of this session. 
Database Management System
Dr.S.Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 29

Recovery Mechanisms III

Hello and welcome. In the previous session we have been talking about recovery techniques in database management systems. How do we recover a database especially when the database was running and the system was subjected to some kind of a failure. And we saw that there are different varieties of recovery techniques each having their own advantages and disadvantages. Let us continue with this topic further today and bring it to a logical conclusion by looking at all the other issues that entails database recovery. 

(Refer Slide Time: 02:02)

 

Let us briefly summarize what we have studied until now. Whenever you are thinking … (Refer Slide Time: 2:00) given computer system whether it is databases or otherwise is subjected to frequent or frequent or non-frequent kind of failures, different kinds of failures. And we could classify failures into different types like say system failures or media crashes or communication failures or transaction failures. 

System failures and transaction failures are by far the most frequent or the most common kinds of failures. What is a system failure? System failure is something like suppose you are running your dbms on a machine and let’s say the power goes off and your machine crashes. The characteristics of such a system failure is that all data that are there in the volatile memory like ram are lost. However data that are present on persistent storage like disk are still retained. So whatever has made it to the disk can still be retained, however whatever was still in the ram is lost. 

Similarly there are failures like transaction failures which are again quite common. Transactions failures entail failures where for a variety of reasons given transaction is not able to complete its operation. This reasons could be something like too many processes running in this system or not enough privileges for running this transaction or the transaction trying to do illegal … (Refer Slide Time: 03:34) transaction and so on and so forth. 

And these are again quite frequent kind of failures and here again the characteristic is that whatever the transaction was doing and whatever data that the transaction had in the main memory is lost, however the data that are present on persistent storage like disk are still retained.

Then there are communication failures where especially if your database is distributed and you had to communicate between two or more different geographically distributed databases then you might encounter communication failures which need to abort a transaction or leave a transaction midway and there are somewhat infrequent failures called media crashes like a disk crash, like a disk developing bad sectors or some kind of a damage, physical damage to the disk where not only data that are there in the physical memories, in the primary memory is lost but the data that are present on the disk are also lost. That is you … (Refer Slide Time: 04:47). 

(Refer Slide Time: 4:52)

 

Now what is the main property that we have to consider or that we have to maintain when we are talking about recovery techniques. As we all know transactions in databases follow this well-known property of acid which stands for atomicity, consistency, isolation and durability. So atomicity essentially says that either all updates that are to be performed by a transaction are performed or none of them are performed. This is obvious because suppose you are transferring money between two accounts, let us say you have a debit in one account and a credit in another account either both of them have to be performed or none of them have to be performed. So either the transaction should run fully or it should not run at all and so on. It’s not sufficient if money is debited but not credited or vice versa.  

Similarly, consistency. A transaction when it completes cannot leave the database in an inconsistent state. We have seen some integrity constraints and notions of triggers where   suppose data database consistency is violated, automatically triggers are enforced which will cause the transactions to roll in many commercial databases. So if the database starts from a consistent state, at the end of a transaction it should remain in a consistent state. 

And of course the third property is of isolation where when multiple transactions are executed simultaneously, for efficiency reasons they are executed in a concurrent fashion that is the updates made by each of the transaction are performed concurrently. However the net effect that is not all possible concurrency is okay, essentially the kind of concurrency that is permitted is that the net effect of these multiple transaction should be such that it should be as though the transactions have been executed in some kind of serial fashion. 

And the last property is about durability where once a transaction commits, once a transaction says that I have committed then whatever changes that it has made is persistent. After commit it should never be case that let us say system crashes after a transaction has committed, it should never be case that the transaction is now aborted or rolled back. Once the transaction is committed, it means that it has performed some kind of physical operation that is some, it has performed some kind of an operation that is beyond the purview of the database itself. 

For example when in an atm transaction, let us say in an automatic teller machine transaction once a transaction commits it means that it is safe for the atm to dispense money… (Refer Slide Time: 07:49) after the atm has dispensed money we decide to actually abort the transaction and then say okay it has been not committed and so on. So once changes that are made by transactions are committed they have to be durable that is it has to be persistent. 











(Refer Slide Time: 8:07)

 

And we also saw that transactions can be in different states like active or partially committed or failed, aborted, committed or terminated. So an active state is the initial state when the transaction is executing, it’s performing some operation, it feds some data elements and made some changes and so on. And partially completed is a state when it has performed all its operations and it is ready to commit, it’s not yet committed. 

And once it discovers that it is not able to proceed further, whether it’s not able to commit whether it’s not able to calculate or perform its computation further we say that the transaction has failed. Once a transaction has failed it has to roll back any changes that it has made to the database has to be reverted, has to be undone and once a roll back is complete we say that the transaction has aborted. And on the other hand if the transaction successfully commits, it’s a committed transaction and a terminated transaction which has something either aborted or committed.















(Refer Slide Time: 9:15)

 

So this slide which is now familiar shows the different states of a transaction in a schematic fashion. That is an active transaction can become either partially committed or failed transaction depending on what it has, whether it has been able to perform all its operations or it’s not able to perform all its operations. And from a partially committed transaction, you can still go to a failed state when you see that it’s not possible to commit the transaction. But once you see that it is possible to commit the transaction then you go into a committed state and once you come to a fail state, you perform a roll back operation where you go to the aborted state in the transaction.  

(Refer Slide Time: 9:57)

 

What is meant by transaction recovery once again? A transaction recovery is a process by which we recover the database system which is performing several different transactions into the most recent consistent state. That is at any given point in time, any … (Refer Slide Time: 10:21) forming several transactions simultaneously. Now during such a time when there are some kinds of system crashes let us say some kind of failures like say system crashes or transaction failures or media crashes or whatever, it basically leaves the system in an consistent state, half of some transactions have been performed some of them have been committed, some of them have just started and some of them were shown to be committed but may not have been committed and so on and so forth. 

It’s essential that when we recover from this system crash, we should recover to a state that is obviously consistent. And it has to be not just any consistent state, it has to be the most recent consistent state that was there before the crash. and all active transactions around the time of the crash that is when the crash happened, there were let us say 10 different active transactions that were running, once we recover from the crash of all these active transactions there should be either in a committed state or an aborted state after recovery. That is either all the operations that are performed by the transactions are persistently stored in the database or they are rolled back so that they can be invoked once again buy the dbms system. 

(Refer Slide Time: 11:46)

 

We also saw that there are certain pre-requisites for recovery to happen. That is there are certain kinds of schedules by which transaction activities are performed which are not recoverable at all. For example this slide shows two different transactions T8 and … (Refer Slide Time: 12:07) following operations T8 performs a right operation on an element called A. So it reads an element A and make some changes and writes it back to the element. 

Now let us say T9 reads the new element of A which has been return back by T8 and commits that means let us say displace. So let us say something like you have just updated the bank account in your bank or updated your stock prize or something like that and there is another transaction which has read the new stock prize or the new balance account and displayed it or printed a statement let us say, commit is a some kind of physical operation. So let us say it printed a statement.

However the transaction T8 has not yet completed here and while it is still going on, it encounters a system crash. And once we recover we say that T8 has to be rolled back or has to be aborted and restarted again. But what has happened here is that this transaction t nine has already said that the new value of A is so and so and it has already printed the statement or displayed it or done something of that sort. So such a kind of schedule is not a recoverable schedule that is we will not be able to recover from such a schedule. 

(Refer Slide Time: 13:29)

 

There is also another kind of problem with recovery namely that of cascading roll backs. This slide shows such a example here that is even when a schedule is recoverable,   sometimes whenever we roll back a transaction there might be several other transaction that are waiting on it that have to be rolled back. And all of them have to roll back one after the other which causes a cascade of different roll backs. 

So here there are three different such transactions T, T1 and T2 where T has read element A and return something back to here. Now whatever has been returned back by TA or by T is now read by T1 and its return back and whatever has been return back by T1 is now read by T2. But transaction T has not yet committed here and it’s still going on and then it crashes. Now after we recover from the crash, suppose we decide that transaction T has to be aborted, it means that transactions T1 as well as transaction T2 have to be aborted because they have read some changes or they are basically depending upon the changes that are made by T itself. So this causes the problem of cascading roll backs. 
(Refer Slide Time: 14:49)

 

So we saw how to prevent cascading roll backs and how to prevent non recoverable schedules in the previous session which I will not go into more detail here. Essentially in order to prevent non recoverable schedules, you need to ensure that no transaction reads a data element that is written by another transaction unless it has committed, only after committing a particular operation another transaction can read this transaction. We also saw two kinds of log based recovery. What is a log based recovery? Log based recovery uses what are called as write ahead logging or what are also abbreviated as wla write ahead logging addresses, wal rather write ahead logging. 

Now what is a write ahead logging? Write ahead logging essentially means that before performing any operation on the database, you perform a corresponding operation on to a log file and then perform the operation on to the database. Now whenever the database crashes, you have to use your log file in order to be able to recover from your system failures. We saw essentially two kinds of techniques based on write ahead logging. What are called as … (Refer Slide Time: 16:16). In deferred update techniques database is not modified until after the transaction reaches its commit point. that is whatever changes that a transaction makes to the database is first just returned to the log and only when the transaction is ready to commit, will the log be return on to the or the changes be return onto the database itself.  

So, until after the commit point transaction or the database is not modified and once the commit succeeds, you also enter a commit or committed entry into the log. So essentially whenever the system crash, you just need to redo your transaction so that it reaches the point where it has been. And there are other kinds of techniques called immediate update techniques where databases updated as and when the transaction progresses. However as the name write ahead logging suggests that the updates are first written onto the log before they are written onto the database. And immediate update techniques requires both undo and redo operation that is you need to undo all active transactions which have not committed and you have to redo all transactions which have committed actually. 

(Refer Slide Time: 17:41)

 

The general idea behind recovery using deferred updates is shown in these slides and such kinds of … (Refer Slide Time: 17:48) that is there is no undo operation as part of this database, as a part of this recovery technique however there is a redo operations. Because, why is there no undo operations? Because the database is not  touched at all that is the database is not modified at all until the transaction is ready to …(Refer Slide Time: 18:12) multi user system. In a multi user database system, you have to perform let’s say some kinds of two phase locking in order to prevent   … (Refer Slide Time: 18:28) in order to enforce isolation constraints. And all locks that is whenever you are holding such a lock on a database item all such locks are released only after the commit. 

Therefore no transaction can read any other, can read the values written by any other transaction before it has committed. Therefore there is no possibility of cascading roll backs and also there is no possibility of a non-recoverable schedule occurring as part of this recovery mechanism. 

However the problem with deferred update techniques is that suppose a transaction performs let us say some 500 different operations. Now none of these 500 different operations are written on to the database until after the transaction is ready to commit. therefore once a transaction is ready to commit, suddenly there is a huge spurt of activity, suddenly there is all 500 different operations have to be performed on to the database at one instance of time rather than being distributed over the entire life span of the transaction. Therefore there could be potentially heavy disk operations during commits which could bring down performance or which could cause the system to thrash or lead to some other such problems. 

(Refer Slide Time: 19:53)

 

With immediate update techniques that is where locks or where the database is updated as and when the transactions are written onto logs, we require both undo and redo operations. So such kinds of recovery techniques are also called undo slash redo transaction or undo slash redo recovery techniques. Now what is the general idea behind immediate recovery techniques? The general idea is that as and when the transaction is   executing, the database is updated even before committed which is very important here. That is the database is updated as and when the transaction is executing and the transaction may not commit at all, the transaction may become a fail transaction it may abort, it may have to roll back and so on. However as and when the transaction is executing the database is getting updated. 

What it means is suppose the transaction cannot commit, suppose the transaction is not able to successfully perform its commit operation then we need to roll back the transaction. Now because we are changing the database as and when we are performing the transaction, we have to perform corresponding undo operation that is we have to undo whatever the changes that the transaction has made to the database. 

So database updates are perform after the updates are recorded in the log and the logs are written onto disks. And of course it may not necessarily be force written onto disk that is you just write it onto log and then write it to the database. Now once you know that the log has been written to disks, it’s safe for you to write to the database. Now suppose there is crash at any point in time and … (Refer Slide Time: 21:46) to do two different operations, one is you have to undo all unfinished transactions since the last check point and you have to redo all committed transactions since (Refer Slide Time: 21:55) as you know our different stages in transaction logs where we say that it is safe to throw away all other historical data that is there in the log. 

Because, the log is a monotonically increasing file where every operation keeps on adding to the log. Suppose we don’t truncate the log, it could well be the case that the log becomes far more bigger than the database itself and becomes far more difficult to manage than the database itself. Therefore it is important for us to be able to conclusively decide, when at time what parts of the logs can be safely deleted or can be safely thrown away. So that is where the notion of check points also comes in to the picture. 

(Refer Slide Time: 22:44)

 

There is a third kind of recovery techniques which we have not seen in detail but I am including this here for the sake of completeness which are called undo based of transaction, undo variety of transaction recovery protocols, we saw that is undo slash no redo. we saw that there was the deferred update technique was no redo slash undo and the immediate tech update techniques were undo slash  redo and this is undo slash no redo. 

The general idea behind undo based transactions is that it’s a recovery technique that is based on pure undo operation. what this means is all committed transactions are flushed onto disks during check points that is at every check point in order to safely throw away certain transactions, we flush all committed transactions … (Refer Slide Time: 23:46) transactions are maintained that is the chain of all activities by an active transaction is maintained by what is called as backward chaining in the log. 

That is whenever I enter an entry for a particular transaction in the log, I also maintain a pointer to the previous operation that it performed. therefore the way that these log entries are maintained are what are called as compensating log records or also called as CLRs where for every update operation, you perform you store what is called as a compensating operation. For example if you let us say debit 5000 rupees from an account A, the compensating transaction or the compensating operation rather would be to credit 1000 rupees or 5000 rupees to this particular account. 

So for every kind of update operation let us say you deleted a particular tuple, the compensating transaction would be to insert the same tuple back to the database, the compensating operation. So you maintain the log of compensating operations and at every check point you see which are the transactions that are committed and flush them onto disk so that you don’t have to undo them once again. And at any time the system crashes or we encounter a system crash, what we need to do is simply start from the state where the system has crashed and follow the backward chain of logs and perform all the compensating log records that is compensating operations that are performed. 

So this is the pure undo based operation where there is no need to redo any transaction   or there is no need to redo any transaction or there is no need to redo any particular transaction activity. The redo is performed or the flushing of transactions data are performed only at check points and not after any system crashes 

(Refer Slide Time: 25:53)

 

Now let us look into the notion of check points itself in a little more detail. As we said before, a check point is some point in a log where we say that it is safe to throw away everything before this check point. That is all committed transactions until this check point have been are now durable that means they have been physically committed to the database. Therefore the records of all committed transaction need no longer be stored as part of this transaction log. And it is safe to throw away all the data about committed transactions. and the frequency at which we take a check point decides or some kind of a balancing factor between what is the amount of log that you are going to store versus what is the performance over head that you are going to incur. 

Because at every check … (Refer Slide Time: 26:55) you need to bring the database to what is called as the quiescent state that means you have to suspend all the activities all transactional … (Refer Slide Time: 27:05) transaction that have committed and ensure that they are physically modified in the database and … (Refer Slide Time: 27:13) restart the database system that is restart all the transaction activities. 

Therefore if check pointing is done at two frequent an interval, if check pointing is done too frequently then it impedes performance while if check pointing is done at too widen interval then it starts increasing the amount of log records that you have to store. 

(Refer Slide Time: 27:45)

 

So what is the general algorithm for taking a check point? We have to first suspend all transaction activities temporarily that is we need to take the dbms to a quiescent state and then we have to force write or flush all main memory buffers that have been modified of committed transactions actually on to disk. And then write a check point record into the log and force write the log onto disk as well. And then now it is safe to resume transaction activities. 















(Refer Slide Time: 28:18)

 

But then as we saw that bringing a database system to a quiescent state is a costly operation just imagine a large database system like let us say the database system for railway reservation let us say for a given railway zone like say southern railway zone. At any given point in time there are possibly hundreds or even thousands of transactions that are performing, so many requests coming from different places for reservations and cancellations  or some other kinds of activities not just reservations and cancellations let’s say rescheduling and so on. 

There are several different transactions that are running at any given point of time and even if we stop the database system let us say for a few seconds or even if or sometimes even a few minutes because the size of the database is so huge, it actually impedes or impairs the activities of a large number of requests that is there could be a large number of people waiting in different reservation centers waiting to reserve their tickets and bringing the dbms to a quiescent state will actually stall all of them and so on. 

So there are other technique that are used which can try to obviate the need for bringing the database on to a quiescent state whenever a check point is being taken. Remember taking a checkpoint is quite important because otherwise the transaction log itself becomes too difficult to handle but … (Refer Slide Time: 30:00) it doesn’t impedes the performance too much. 

That is where the notion of fuzzy check point comes into the picture. That is fuzzy check points essentially does not require, normal check pointing requires suspension of all transaction activities but fuzzy check point does not require in bringing the database onto a quiescent state. Essentially what we do in fuzzy check pointing is that we write the check point entry in to the log and we resume operations but we keep watching the disk itself that is at the physical layer. 

For example note that I mean note that when we write something onto disk, the operating system may not actually physically write it on to disk. It would actually write it onto a buffer cache within the main memory, so it’s still volatile and it will write it onto disk at some later point in time. So we wait until that point where the check point log actually comes onto the disk and then we start removing all the old entries that is whatever has to be safely removed are then removed. 

(Refer Slide Time: 31:21)

 

A second kind of check pointing which does not require the database to be brought to a quiescent state is also called a non-quiescent check pointing operation. Here unlike fuzzy check pointing, there is also no need to watch the physical disk sector or physical disk block to see whether the check point entry has actually appeared. But the idea here in non-quiescent check pointing is that active and non-active transactions at the time of the check point are handled separately. 















(Refer Slide Time: 31:56)

 

Let us look at how non quiescent check pointing actually performs. There are essentially three different steps in this non quiescent check pointing. At any time if I have to take a check point, what I do is I first determine which are all the active transactions at this point in time. That is these are all the active uncommitted transactions that are currently running and then make an entry called start check point with even to Tk which are all the active transactions running at this point. 

Now this entry will go into the buffer cache and so on and so forth and finally at some point in time it will reach the disk. Now wait until and we are not concerned when they are going to reach the disk and we are not going to watch the disk at all, we just wait until each of this transaction either commit or abort. But we do not prevent other transaction from starting that is other activities could be resuming by themselves but we simply have to wait until any of these commit or abort. 

When all of them are done that is they are finished or terminated, we write a end check point entry into the log and then force write the log onto disk. That is we flush the log onto disk. Now what is the use of such a check pointing? That is how can we recover from such a check pointing? 










(Refer Slide Time: 33:18)

 

Recovery from such a non quiescent check pointing is also quite simple. Let us say at some point in time there is a crash and we recover from such a crash or we have started with the recovery process after the crash and we start to read the log backwards that is from the last entry onwards. Now suppose we meet a end check point entry first, what does that means that the previous check pointing operation has successfully completed. That is we know all incomplete transactions that were there at the time of the check point have been completed. Therefore there is no need to do anything that is it is safe, we just have to restart whatever transactions were running after this time. 

But suppose we reach a start check point without an end check point. What does this mean? This means that the crash has occurred during the check pointing operation itself. That is these were the set of transactions that were running and all of them have not yet terminated, they have not yet committed or aborted and the crash has happened. So therefore what is that we need to do? We have to undo all these operations that is all the transactions, all the operations that have been performed by these transactions. 

There again we can use some kind of backward like we saw in the undo only kind of recovery algorithm by which we undo all of these operations. And until how far behind in the log we need to go? We need to go just until the previous check point because we know that the previous check point would not have completed until all the active transactions at that point in time have not yet completed. 

So therefore we never need to go beyond one check point that is beyond the previous checkpoint. That is we can safely throw everything else beyond the previous check point and we just have to trace them through the log till the previous check point and undo all these operations. So that’s another kind of check pointing operations that are used in addition to or in complement to the normal quiescent based check pointing where you need to suspend all activities of the database system. 
(Refer Slide Time: 35:41)

 

There is another… (Refer Slide Time: 35:42) which is called as shadow paging, it was … (Refer Slide Time: 35:50) Part of sys … (Refer Slide Time: 35:53) in this but where the technique of recovery is not based on logging but on maintaining different copies of database pages. That is the database is managed in terms of different pages or logical entities of disk of operation which pages could be just let’s say disk blocks or disk sectors or set of sectors and so on. 

And we basically make different copies of these pages in order to maintain the log. So there is no need for write ahead logging and all updated data are kept in a different location from the original data, they are not kept on the main or on the same data set. And recovery basically entails that we discard the new page whatever is the updated page and revert back to the shadow original page. That is the shadow of the original page which was still there that is we don’t delete the original page until it is safe to do so which we will see shortly. 

Shadow paging is quite efficient in terms, in the sense that there is no need to perform multiple write operations. In log based recovery you need to first do a write ahead logging and then write on to the database system. So there is a need to perform multiple write operations for every given multiple update operations for every update operation that is involved. But in shadow paging there is no need to do that however shadow paging suffers from fragmentation of pages over an extended period of time and usually we need to defragment the set of database pages by an offline operation after a number of recovery techniques have been executed. 

 

 

(Refer Slide Time: 37:54)

 

So how does the shadow paging technique work? Here is a small illustration. We usually keep a page table that is each page has a particular address where each page can correspond to a set of tuples for a given table for example and each page is given a particular address. And there two different point of for each page address what is called as the current page pointer and the shadow page pointer.  

Initially we just have the current page pointer here and which is pointing to a particular page. Now let us say some tuple in this page has to be modified. So there is a modification request for this page. now we see that this page has no shadow page. So therefore what we do is we create a copy of this page and point the current page pointer to the copy. The original will now become the shadow that is the shadow of the original page that existed. 

And we are perform the modifications, whatever update operations that we need to do will be done on this page here. Now suppose somewhere down the line when we are using this page there is a system crash. Now when is a system crash all that we need to do is to discard this page and go back to the shadow original page. And then change the current page pointer to the shadow pointer here and set the shadow pointer to null. So we just discard whatever was there and we have gone back to a consistent state. Now you might be wondering at what time are we going to start, how do we know that this is consistent that is how do we know at what time do we start creating a shadow page. 

 





(Refer Slide Time: 39:41)

 

Essentially shadow pages are updated with current pages at every check point. That is whenever there is any check point or check pointing operation not just crash recovery operation, we perform this following operation, we perform the following set of activities. That is we replace the current page with a shadow page, make the shadow page pointer to null and the current page pointer to the previous shadow page pointer. So at every check point we are flushing the shadow page or we are updating the shadow page with the new current page. 

And every first modification after a check point requires creation of a new current page. And as this kind of evident here, the check pointing operation that has to be used here should be quiescent mode check pointing operation that is we need to suspend a database activities for performing such a update operation that is replacing the shadow page with the new consistent current page.















(Refer Slide Time: 40:46)

 

There are also other issues during that we have to content with for crash recovery. What happens that is the first, the main issue is what happens if there is crash during the recovery process itself. That is there is a system crash and we are trying to recover the database system and then there is again a crash. 

Now in order to be able to handle such issues that is so that crashes during recovery do not affect the database, we need to be or we need to ensure that the undo and redo operations are idempotent. Remember even in the previous session we had used this word idempotent. Idempotent essentially means that no matter how many times you perform the operation, the net effect would be as though you have performed the operation only once. And it doesn’t matter how many times you perform this operation that is let us say copying an element a to element b is idempotent. No matter, how many times you copy it, it is equivalent to saying that we have copied it only once 

But there is a problem with idempotent operations as well. That is if you have to make undo and redo operations idempotent then we have to ensure that multiple transactions or multiple active transactions do not access the same element at the same time. That is one transaction has to wait until previous transaction has committed before it is safe for it to access a data element and that actually impedes performance in terms of performance. 

And ofcourse in shadow paging technique there is other problem which occurs when there is a crash during recovery, especially when we are manipulating pointers. That is suppose you have discarded the new current page and we are changing the pointers to the   to the old shadow page and then there is a system crash. It may so happen that all pointers are lost and the page has just become garbage that is there is no way to access the page and basically place it in the context of the larger database. 


(Refer Slide Time: 43:14)

 

Until now we have essentially talked about system crashes or transaction failures where the fundamental criteria was that we can lose or we may lose data in the main memory but not in the persistent storage. What happens if there are media failures that is there is a disk crash, let us say or development of bad sectors. In this case we cannot even rely upon the data to be safe in the persistent storage as well that is even the data are there in the persistent storage are gone. Fortunately media failures are much more … (Refer Slide Time: 43:53) disk crashes happen far more in frequent fashion than let us say power offer or operating system hanging and so on. 

(Refer Slide Time: 44:06)

 
And … (Refer Slide Time: 44:08) crashes what are called as archiving or taking backups and I am sure you know the concept of taking backups. It’s simply taking a copy of a entire set of database on to another offline media which is stored physically in a different location. And however when you are talking about large database systems, again when we talk about database system always imagine a large database system because most of the problems of database management systems come from their size and not from anything else. 

So imagine a large database system like a railway reservation or a bank or whatever. Now the problem is … (Refer Slide Time: 44:55) itself takes a huge amount of time. Suppose I have one giga byte of data in my database which is quite common, in fact it could sometimes we even have more than one giga byte of data, we have several giga bytes of data in any operational database and possibly even tera bytes of data. Now copying them to a separate let us say tape media or optical storage takes huge amount of time in the order of hours or possibly sometimes even days to copy the entire set of data elements onto backup storage and therefore there is a need to perform back up operations in an online fashion. 

That is as and when database activities are going on, we cannot obviously suspend all railway reservations let us say for one day. We cannot say tomorrow there is you cannot reserve any train tickets and so on. It has to be on everyday 24 hours a day or 7 days a week, so everyday or every instance of time the reservation activities are going on. You can book train tickets over the net and you can book them over the counter so on and so forth. 

(Refer Slide Time: 46:14)

 

So there is a need to perform online backup operations. So let us briefly see what is the basic idea behind online operations and which can help us appreciate the need and the complexities involved in a online backup operations. Before we do that we need to take up certain definitions, we distinguish between different levels of archivals. So what to call as level zero archivals and level i archival. Level zero archival or what is called as a full dump is an entire archival or a copy of the entire database onto the archival storage. And a level … (Refer Slide Time: 47:04) incremental dump is essentially copying of only the changes that the database has under gone before the, rather after the last archival. That is only changes since the last i minus 1 level i minus 1 are archived.

(Refer Slide Time: 47:25)

 

Now the basic idea behind online archival is shown in this figure. Between main memory and disk there is this check pointing operation. As you might have noticed similarity between archival and check pointing. Check pointing requires that the dbms system to be brought in to a quiescent state and archival requires a suspension of all database activities   in order to copy them onto the archive. 

So check pointing gets data from memory onto disk and whenever we need to recover, we recover based on logs. And the archival process gets data from disk onto archive and because it is unrealistic to say that we suspend all dbms activities during archival. We usually allow that the dbms to keep updating its database operations as and when the archival process is going on. 

However in addition to the database, we also store the log from the previous check point until the previous check point in addition to the database onto the archive. Therefore whenever they need to recover, we need to recover from archive plus log to bring back the database onto the last consistent state before a media crash and the database is being modified as and when the dump operation is running.




(Refer Slide Time: 48:47)

 

So a simple algorithm for online archival is given here. We first begin by writing a begin dump record to the log that is starting from the logging operation itself we begin the archival process. We then perform a check point usually a quiescent check point but not necessarily and then we flush the log. That is we have begin the dump and began the check point that is we have flushed all committed transactions onto the database and we have a log of all the active transactions here. Then perform the dump, full or incremental or whatever kind of dump from the disk on to the archival data. Now once all data are transferred onto archive including the check point that is there in the log then enter end dump in the log. 

















(Refer Slide Time: 49:47)

 

So what is the advantage? The advantage is shown in this slide that is during recovery that is then we have to restore the database. Now the recovery from media crashes is called restore rather than recovery that is we are restoring the database from the archive. So when we restore the database from the archive, we find the most recent full dump and reconstruct the archive based on all the incremental dumps and then we write it back on to the database. 

And then we take the database log that is the surviving log entry that was also archived in the archives, in the archival storage and there we have a check point which shows what all has been done to the database and whatever was there after the check point we start redoing those database. That is whatever kind of logging method is used that is suppose we have an undo logs then we perform the corresponding undo operations and suppose there were it was a redo log we perform the corresponding redo operations depending upon lets say it’s a deferred updates or an immediate kind of logging operations. 

So that kinds of summarize … (Refer Slide Time: 51:05) there are number of other topics in database recovery which we have not touched upon, especially perhaps most significantly the kind of recovery techniques that are used in many of the commercial dbms systems. Most of the commercial dbms systems today use a combination of undo and redo logging operation that is this is to get the right tradeoff between performance and recovery over rates because in a pure redo operations you have a performance overhead in the sense that there is a huge spurt of activity during commits for every transaction.   

Therefore they use a combination of these undo and redo operations and there is a kind of a famous series of protocols called aries, a r I e s which can possibly search on the internet which was proposed it IBM unverdant research centre which was kind of used quite, which is kind of very popular and used in several different commercial database systems. 

(Refer Slide Time: 52:23)

 

We have not covered the details of the aries protocols here for the reasons of brevity but most of that concepts used in aries depend upon or based upon several of the concepts that we have studied here like deferred updates or immediate updates and check pointing and undo and redo operations and idempotent operations and so on and so forth. 

So let us briefly summarize what we studied in database recovery under the topic of database recovery. We looked into the idea of transaction states and what are the different states that a particular transaction can exist and the acid criteria for transactions which determines how or which determines how our recovery protocol should run. And we looked at different kinds of failures like say system crashes media crashes transaction failures and so on. 

And for the most frequent kind of failures like say system crashes or transaction failures we saw three different kinds of recovery mechanisms. That is no undo slash redo operations that is deferred update recovery and undo slash redo which is immediate update and there is undo slash no redo operations which are based on compensating log records or clr’s.

We also saw the notion of shadow paging where you don’t have the need for a log therefore it’s much more faster, the recovery mechanisms or the dbms itself is much more faster because there are no multiple write operation overheads that are involved during every update but which suffers from a possible fragmentation. That is so many pages created at different places in the disk are possibly when suppose there is system crash during recovery itself, there is a possibility of encountering garbage system pages as part of the disk itself. So shadow paging has its own advantages and disadvantages with respect to write ahead logging. 

We also saw the notion of archiving or the idea of taking online backups for handling media failures where we take a incremental backup and we recover or we restore from backup based on not just the archives but based also on the log that … (Refer Slide Time: 54:57). That brings us to the end of the session.

Database Management System
Dr.S.Srinath
Department of Computer Science and Engineering
Indian Institute of Technology Madras
Lecture – 30

Introduction to Data Warehousing & OLAP

Hello and welcome. In today’s session, we are going to be looking at a slightly different topic from the conventional idea of databases and such a change in topic occurs simply because the kind of users that the database is going to be is meant for is going to change. Until now, we have a kind we have had an implicit assumptions that the users of who are going to be using the database are in some sense if I can put it this way or in some sense   clerical users. In the sense that, they are most interested in adding and retrieving data elements into the database as efficiently as possible that means, the users who are using this database are involved in the operational aspects of a larger system.

For example: if you are thinking of, let us say railway reservation we are talking about how best we can design a database system for reserving a ticket. Therefore, what it means is whenever a new ticket entry is made it has to be efficiently entered into the database and suppose if there are any modifications for a given ticket entry, it has to be efficiently modified that means, it should be able to efficiently search the given entry and make modifications at just one place if possible and not many more places and keep that overall consistency of the database in time and that is what is meant by operational aspects of a database system or operational aspects of a system is a day to day operation. Somebody comes you gives a request for a reservation, you enter the request, you reserve a seat for him and give him the ticket and or cancel it something or ask for concessions or whatever and so on.

But, there are other kinds of users who use the database system as well and specifically we are talking about users who take strategic decisions in addition to or in contrast to the tactical decisions that are taken by the operational people that is the folk sitting in front of a let us say a ticket counter take very tactical decisions. How efficiently can you perform your operations?  But then, there are a variety of strategic decisions which is the best way which is the best location for me to place my next railway reservation counter. Which part of the city has the most people traveling by trains or which part of the city has the most people traveling by first class ac or which part of  which time of year is the best for me to offer concessions on second class sleeper, something like that. 

So these kinds of strategic decisions are of a qualitatively different nature than the tactical decisions that are taken for operational issues of any system but if you watch closely it is the same ……(Refer Slide Time: 4:20) or fed and retrieved in operational situations is the same data that is required for making strategic decisions as well. If you have to make strategic decisions about let us say which is the best location for me to open my next open the next reservation ………(Refer Slide Time: 4:40) 

For that we need a lot of information about what is the, from what addresses are people coming and booking tickets were right. If somebody comes from area ‘a’ and books tickets in area ‘b’ and assuming that area ‘b’ is the nearest reservation counter and the address information will in turn show me that because there are a lot of such people coming from here to there, it probably makes more sense for me to open a next counter here and so on. So we will see in the next few sessions how the whole aspect of data base design changes when the usage scenario changes from an operational data usage to a strategic decision making usage. So that brings us to the topic of data warehousing. 

Data warehousing as you might understand the term warehousing, a warehouse is where you keep your inventory stocks right that is where you have stocks from several different sources are going to several different sources and you are essentially talking about a large number of stocks that are maintained in the warehouse and a warehouse is typically of strategic importance. If you take up, let us say some kind of civil engineering project, the location of your warehouse is of prime importance depending upon because there are which depends upon several factors like say what is the cost of transportation, logistics coordination and so on.

Suppose you are having you are handling a large infrastructure project somewhere. Let’s say building a flyover something like that, having the location of your warehouse in a in a way that is easily accessible with possible cost constraints is probably one of the most crucial strategic decisions that can be taken. In an analogous sense, a data warehouse is a warehouse of data elements that have been captured from different operational data sources. So that this whole set of data elements becomes is of strategic importance. You can take strategic decisions based on the data elements that you have gathered and which leads us to the next term in the slide here which is called the OLAP or OnLine Analytical Processing that means we are looking for requests or queries that are of an analytical nature rather than an operational nature or what you called as a transactional nature. We have talked about transactions quite a bit. Now we are looking into analytical queries where queries in turn help in strategic decision…….(Refer Slide Time:07:44)
















[Refer Slide Time 7:49]

 


………….. by itself is a vast subject and that several research papers that have been written in several commercial implementations that have been implemented and the huge amount of  interest in data warehouses. However, in this course we shall probably be touching just a small part of this vast ocean of data warehouses and the way we have partitioned this, our exploration into data warehouses are in three parts as shown in the slide here. In the first part, we shall be looking into an important difference between OLAP that is Online Analytical Processing versus the Online Transaction Processing that traditional data bases are geared towards ….. in data warehousing namely that of data cleaning and data integration. 

The next two parts deal with the deal with the warehouse core itself where we are talking about the data models that that go inside the data warehouse and what are some kinds of the thumb rules which go towards data warehouse design. We will also look at some kind of index structures for data warehouse based on the data models that we studied and see that, how they differ from (Refer Slide Time :9:13) data bases like say b plus trees or b trees or hash based indexes or something like that.











[Refer Slide Time 9:26]

 

When we are talking about data, we can essentially divide data into two kinds of data. Essentially I have called them as operational data and the next slide calls it as historical data, that is operational data can also be considered as data that works within quotes that is data that is involved in the operation of a particular system. For example: if you want to withdraw money from your account, you need data about your account, your account number, your pin number if you are using an atm your account balance and so on. 

All these data elements are all operational data elements because they are crucial, they are necessary for performing the operation of withdrawal of money right and what kinds or what are the characteristics of operational data? Operational data are subjected to frequent updates and queries. You could be withdrawing money almost every day or some kind of operations would be happening on your account almost on a daily basis so there should be, there would be some kind of queries or updates happening quite frequently to the database that is maintaining your account information. 

Therefore we saw in the sessions on normalizations and functional dependencies. We saw that in order to make the process of updates and queries efficient, we need to normalize the set of data elements that is we need to normalize what is to prevent any kind of redundancies occurring in the data and also to prevent any kind of update anomalies. Suppose update records of in some place and there are redundant copies of the same record of some fields, then I will have to update all occurrences of this field. 

So in order to minimize update anomalies we essentially normalize the set of data elements and a set of tables that are normalized are essentially fragmented, because given data elements. Let us say your account information may require several information something like your account number, name, address, branch location, balance, transaction history and type of account so on and so forth and because they are normalized, the set of all tables are fragmented in different phrases and the operational data is usually of local relevance which is kind of emphasized in the should kind of emphasized that means it is very unlikely to expect. Let us say that you have opened an account in Chennai and you go to Delhi and you want to access your account. It is unlikely to expect the account information that you have opened in your bank in Chennai to also be present in Delhi. It has to be queried in Chennai and your request has to be routed through Chennai and it has to be queried here and the result sent back to Delhi. 

So, operational data is usually of local relevance, wherever the data is it is relevant there in in the geographic location and the kind of queries over operational data is also what are called as point queries. What is a point query? A point query is something like asking something about asking query about some individual tuple. For example: what is the balance in your account with account number so and so or what was the last set of ten transactions that your account had. Therefore, what is happening here is that, there is there is a particular point or a specific tuple is the key where your account number in this case where which is being used to access all relevant information for the operationalization.

[Refer Slide Time: 13:38]

 

On the other hand, there are you can think of another kind of data called as historical data or also called as archival data that is data which are archived over a long period of time over a long period of different operational data sets. It is a long set of operational data sets and it is these kinds of data which tell us something that is it is also called as data that tell us something about the overall trends of operationalization that is happening, that is suppose I have a data collected over from all railway reservation centers for the past ten years it can tell us about trends as to which are the peak times in which people travel what kind of people travel? In what kind of or make what kinds of railway reservations and so on. I mean people having what kind of salary range travel in what kind of classes? Sleeper classes AC or whatever and so on. 

So it is that data that tells us something about the larger system which is using the data set and as you can see here, the kind of updates that, this data sets undergoes is quite infrequent in nature. It is not like every day that you are going to get historical data. You have to get historical data over a period of time. Therefore, you collect historical data let us say once in six months or once in a year or something of that order. 

So it is a very infrequent updates but what is more important here is, that updates are not all that important that is they happen very infrequently. So, we can take care of them whenever they happen, but it is the queries that are more important, that is analytical queries require huge amounts of aggregation. What is the average age of the person traveling in second class sleeper? If you want to calculate that, it needs huge amount of aggregation over a large data set based on all different second class sleeper tickets that have been sold and this is an integrated data set with a global relevance. 

We are not it usually it does not make sense to look at trends for trends in a given operational data source. It does not make sense to say what is the trend in this particular reservation center? I mean it is it usually would be very small compared to the larger set of all different operational data sources and the performance issues in managing historical data occurs mainly in query times and not in update times because of course, updates are far infrequent or far more infrequent than queries and queries need to access a large amount of data and you have to perform a number of aggregate operations before being able to return the query and they have to return it in an online fashion, that is why it is called online analytical processing. You have to return it in a return query results in an interactive response time that is the user should be sitting in front of a terminal and the interactive response time is usually of the order of maximum a few seconds and you cannot expect the user to be waiting for a large period of time after giving the query.

[Refer Slide Time 17:22]

 

Let us look at some examples of a transactional data and historical data. A transactional data or operational data are those kinds of data limits that are handled by systems that are called as OLTP systems or Online Transactional Processing Systems that is these systems which maintain transactions and handle different transactional activities in the system. Now what are some examples of operational queries, something like say what is the salary of Mr.Misra? It is some it is a point query. As you can see here that is, you find out the employee number of Mr.Misra and then get the salary field and that is it or who is the what is the address and phone number of the person in charge of the supplies department? 

Again it is a point query that is just a question of following different references find the supplies department and look up the set of managers given the suppliers department id look up who is the manager managers id and then given the managers id, find the address and phone number of the manager and so on. So these are some kind of typical, analytical queries, typical transactional queries which we have seen quite often. Now that is we have seen how to specify these queries in SQL, how to optimize these queries, how to create transactions around them and so and so queries shown in this slide.

[Refer Slide Time: 18:58]

 

How is the employee attrition scene changing over the years across the company? As you can see, this kind of query is qualitatively different. It is qualitatively in a different caliber than the kinds of queries which we saw in the previous slide. If you want to answer this query, let us say how the employee attrition scene is changing over the years and across the company. It is not a question of accessing one particular record or one particular tuple. You need to look at employee attrition information across the company and across the years and then look for trends saying it is increasing, it is decreasing, and so on. That is you have to find out some aggregate employee attrition information across the company and then plot it against the years and see how it is changing or something like, is it financially viable to continue our manufacturing unit in Taiwan. 

It is a pretty vague query right. What do we mean by financial viability? What do we how do we calculate financial viability profits against cost and so on? So, how do we know that we are incurring more profits than costs or we are gaining more profits than the cost that we are incurring. It is not quite easy to find that out, that is you need to be able to aggregate a number of different information sources and then say profits are generally more than the cost and so on right. So, these are the kinds of queries which are of analytical nature and typically for interest for strategic decision, making managers who perform strategic decision making. 

[Refer Slide Time: 20:54]

 

A data warehouse is an infrastructure to manage such kinds of historical data and it is designed to support OLAP queries involving very gratuitous use aggregations aggregated queries and so on and it is not just a queries, there are also a number of post retrieval processing also called as reporting which is just as complex or possibly more complex than the query retrieval itself, that is I have gathered huge amount of information about all possible profits and costs from all possible centers and so on. How do I project this information? How do I give out all these information to the decision maker which is again a quite involved in itself?











[Refer Slide Time 21:44]

 

So these slide shows the schematic diagram of the oral architecture of a data warehouse. A data warehouse as we saw in the previous slide is meant for managing historical data and where do you get the historical data from or how do you first of all come out with historical data? Historical data you essentially obtain from all the operational data sources or the OLTP units. So you have several OLTP units for your organization. Whenever you are thinking about an organization here, think of a large organization something like say the life insurance corporation of India or the Indian railways or something like that where they have a number of different units each of them having their own databases.

Each of them handling their own accounts, each of them handling their own reservations and transactions and so on and from each of these OLTP units, we obtain huge number of operational data elements everyday right everyday there are number of people traveling in trains and from number of different and they would have booked their tickets from number of different sources. Now all of these operational data are then subjected to a data cleaning and integration process. We shall see shortly what is meant by data cleaning and integration? There are huge number of possible inconsistencies or possible sources of what we called as dirty data that can exist in the OLTP sources and we will see what these sources of dirty data that can exist and how we can clean, what is also called as data scrubbing and so on before we are able to present it as historical data. Now once operational data is subjected to data cleaning and are integrated across all of these OLTP sources, we present them as historical data into a data warehouse that is, a data warehouse would already exist and you just integrate this set of data into the data warehouse. You just update the data warehouse with these set of data. 





[Refer Slide Time: 24:04]

 

There is also a notion of data marts which is probably of interest here at least look at the definition. Whenever we talk about data warehouses, we look at then as a collection of data marts that is a data mart is a historical data about one specific kind of one specific segment or one specific OLTP segment. Suppose you are considering let us say again take the example of Indian railways. Let us say, we have one segment called express train reservations that is one data mart. Suppose, we are interested in strategic decision making only about express trains or may be let us say Rajdhani express or something like that all the super-fast trains. So all of the historical data that we gathered about Rajdhani express would go into that data mart called Rajdhani express that is of all Rajdhani expresses across the railways and over a period of time and so on. So and the data warehouse is usually seen as the collection of this different data marts that feeds into the data warehouse and data warehouse data marts are also seen as small warehouses where you can in in some way you can support all activities that you support on a data warehouse also on a data mart and but these are within a given segment. 














[Refer Slide Time 25:45]

 

Now let us expand this system or this box and data cleaning and integration and see what happens inside that? As we mentioned earlier, we obtain data from different OLTP data sources, operational data sources that feed into the data warehouse. Now can we or what kind of (Refer Slide Time: 26:12)………… Before we are able to integrate operational data sources or before we are able to populate the data warehouse from the operational data sources. Operational data sources as we have seen are mainly meant for the data that works that is as long as the data is sufficient for the operation to be performed. It is good enough. It is(Refer Slide Time:26:49) ……… characteristic of operational data is that operational data is of local relevance. It is usually relevant, that is data in an operational data source is relevant only locally. 

However, when we are talking about only historical data, in a data warehouse there are two different things that you have to (Refer Slide Time: 27:11)……… one is that because we are using different kinds of data sources, we should have a uniform standard of data representation and semantics across all of these different data sources and secondly we should remove any kind of duplicate information that are present in these data sources.

Suppose you have opened an account. Let us say, let us again take the running example of the Indian railways. Now suppose you have opened an account for booking tickets over the web. Now you have booked a few tickets over the web and then you have not booked tickets for quite significant period of time in which time you forgot your user name and password and you forgot that you have ever opened an account here and then you go back and open another account for buying tickets over the web. Now as you can see if these two accounts the behavior in these two accounts are considered different are considered to be from different users, then it will give us erroneous information or erroneous aggregate information for strategic decision making.

It is important that for historical data that these two accounts should be clubbed or we should recognize that these two accounts belong to the same user and then club them together whenever we are performing any kind of aggregate queries. So, let us see what does it entails to do all these or what are the kinds of complexities that we encounter in performing these kinds of data integration and cleaning and so on and so forth. 

The general model of data integration and cleaning is shown in the slide here. As we saw in the previous slide, these are the different OLTP data sources which feed into the data cleaning and integration unit and usually the (Refer Slide Time: 29:26)………… we have opened the box which was a black box here. Now we have opened the box and you see that there are usually two different units here. The data cleaning unit and the data integration unit and the process by which data flows through this unit is not a unidirectional process that is you garner a data from this OLTP data sources and pass them through data cleaning, pass them through data integration and also perform some kind of a back flushing that is (Refer Slide Time: 30: 02)…………… that you have back into the OLTP database itself. 

We will look at some examples which tell us what it means actually and once the data is integrated it is fed into the data warehouse and based on queries in the data warehouse or updates in the data warehouse you get a feedback from the warehouse itself which tells how we said that you should perform your cleaning? What is important for the data warehouse? Cleaning essentially means that we have to change whatever is important for OLTP? We have to change representations from whatever is important for OLTP to representations where whatever is important for the data warehouse. 

[Refer Slide Time: 30:51]

 

So, let us look at data cleaning or the dcu in a little bit more detail. As the name suggests, data cleaning performs cleaning operation on data sets that is data sets that contain dirty data. What is meant by dirty data that is, what are the different sources of dirt? I am talking about dirt within quotes. So that one can encounter for given operational data source. There are several different possible sources of dirt that requires cleaning to be performed. The first kind of dirt is what called as lack of standardization. For example: because you have different branches, different reservation centers or different cities. There could be multiple encodings. One of them could be using ASCII base systems, one of them could be using Unicode or one of them could be using head side or something or the other. 

So there could be multiple encodings, multiple locales multiple languages in which data is represented and you should be able on to standard encoding or language or whatever. There could be spurious abbreviations. Somewhere somebody writes Mahatma Gandhi road in part of an address and somebody else writes M.G road. Now we need to be able to recognize that M.G road is same as the Mahatma Gandhi road. So when we are talking about, when we are answering a query like how many people comes from in and around Mahatma Gandhi road or live in and around Mahatma Gandhi road who buy tickets. 

We should be able to say that, we should be able to search both M.G road and Mahatma Gandhi road further because they are the same and similarly there could be semantic equivalence, M.G road and Mahatma Gandhi road probably you can write an intelligent algorithm that will see whether mahatma Gandhi road is an expansion of M.G road and so on and you could kind a well fairly find out this is a spurious abbreviations and so on. However what if there are semantic equivalence? Chennai and madras. Some people may use Madras for the city name and some people may use Chennai and because we are talking about historical data, note that which is important here that is at one point in time officially Chennai was called as madras. So even in official documents, you would be using the name madras but then again it would have become Chennai. 

Now you should be able to query later or say that these two are the same and knowing though intelligent algorithm can do that. You need extra knowledge in addition to the data sources. You need knowledge about the overall environment, the governmental policies, the standards and so on in order to be able to identify these duplications. Similarly there could be multiple standards. There could be one data source which could be using the metric systems, while somebody else could be using miles and feet and so on and so forth. So somebody might in someone data source might have said 1.6 well 1.6 kilometers is the same as one mile. So 1.6 kilometers and somebody else might say one mile and so on. 



	






[Refer Slide Time: 34:30]

 

There could be other sources of dirty data like missing spurious or duplicate data. For example: the age field of an employee could be missing which will hamper my query of what is the average age of an employee or something like that, what is the average or what is the correlation between the age of an employee and the attrition and the probability of attrition and so on and so forth or there could be incorrectly entered sales values or incorrectly or some typographical errors and so on or there could be duplication of data sets and duplication appearing in different forms. 

For example: take look at the last example. A person called L. Bala Sundar would have registered in one, would have bought a ticket in one reservation center sighting his name as L. Bala Sundar the same person could have gone to a different reservation center and bought another ticket at some other point in time, sighting his name as B L Sunder and with the same address and the same phone numbers and so on and so forth and we should be able to detect such kind of semantic duplication. That is, thee is duplicate data that is occurring at different data sources, but not in the same form that is the name has changed or something has changed, but semantically they are still duplicate. 












[Refer Slide Time: 35:56]

 

There could be other sources of dirty data like inconsistencies or incorrect or inconsistent use of codes. Let us say, in one of reservation center the gender of the person who is reserving a ticket is maintained as a character either m or f or and in other reservation centre, it is maintained as 0 or 1. It is just the different standards that each data base designer has designed for each center and this is especially true if this has evolved over time. Let us say that reservation center, number one would be using historical data base that was designed which is not changed because of cost considerations and the new reservation centers using a new data base system that has been redesigned which is termed to be more efficient and so on and so forth but which uses different codes for the same data element and there could be codes with some outdated meaning. 

Let us say third class sleeper because we are talking about historical data. It is quite possible that we encounter some data about third class sleepers which no longer exists in railways today and so on and there could be inconsistent duplicate data that is, there could be two data sets that are found to belong to the same persons. We have reasons to believe that it is the same person. But, he has given two different addresses in these two sources and so on how do we detect such things and other kinds of inconsistencies like inconsistent associations that is, let us say one department provides particular kinds of  says that the sales have been so on and so on. But, it does not add up to the total sales figure that is also been provided. So, associations across different data sources may be inconsistent and there could be semantic inconsistencies. Somebody might have typed February 31st in a date. 

So what is that referred to? Is it February 31st or March 1st or March 31st or what or integrative violations in referential integrity like referential inconsistency. There is, let us say 10 lakh rupees sales reported from a unit that has actually closed down which is not even there and so on. So several kinds of these sources of dirty data crop up in practice and perhaps cleaning or data cleaning is perhaps the single biggest research problem that is still kind of open in the field of data warehouses, because there is no single thumb rule for data cleaning. We cannot just say all data is pass through one kind of data cleaner and it will be cleaned, because there is several sources of dirty data that can occur and there is no single way of cleaning them. 

[Refer Slide Time: 39:10]

 

So what are the issues in data cleaning? It (Refer Slide Time:39:45) …………is automated. It is quite difficult to automate for the computer to learn by itself that let us say Chennai was called Madras or Burma was called Myanmar and so on and so forth and the thing we don’t even know whether the data cleaner is performing correctly which is what is called as GIGO or garbage in or garbage out. Suppose you give some garbage and we don’t have correct rules. We just give some junk data and it just gives some junk outputs we don’t know whether it has actually cleaned or whether the output is correct or not and it is its quite hard to verify the cleanliness of the output data.

And data cleaning requires considerable knowledge that is tacit For example, one kilometer or one point six kilometer is equal to one mile which we know but it is kind of a tacit knowledge it is not exquisite knowledge and which goes beyond the purview of the database like Chennai was called madras. So you should know data about governmental policies you should know data about geography should know some rather some knowledge about geography, governmental policies, metrics and so on and so forth which has to go into the data cleaning unit.

So, it is not possible to design a data cleaning unit in a way which kind of paraphrase does one size fits all or which says that this is the data cleaning unit and this will work for any kind of data in any context what so ever and so on and the data cleaning complexity increases as we increase the number of data sources or they increase the history span that we have taken up for cleaning. 
 
[Refer Slide Time 41:10]

 

What are some of the steps in data cleaning? How does typical data cleaning process looks like? Essentially, there are five different steps that go into a data cleaning process. Firstly, we will start with data analysis. Given the set of OLTP data sources; we start by analyzing them and look for certain kind of metadata that is what we can learn about the data that we have and then give all those metadata back to the user or the data cleaning.
	
Now the user in turn will now specify a set of transformation rules that is if this is the kind of dirty data this how we have to transform it into clean data and so on. So the user specifies the set of transformation rules that that are performed either the scheme or data level which transforms dirty data into clean data. We then verify the rule by running them on test data sets that is some kind of sample data sets and then we incorporate the transformation rules into the data cleaner and then perform the data cleaning process So once the data cleaning process is performed we usually also perform what is called as a back flow that is we repopulate the data sources with cleaned data. For example: Suppose we have seen that Chennai and madras are used interchangeably in in the in the OLTP data source, we see to it that or we perform a back flow, so that it is all either Chennai or madras in the OLTP sources that is, there is some kind of standard in the OLTP sources itself which is called as back flow of data. 










[Refer Slide Time 42:55]

 

We shall not be looking into great detail into how each of these transformation rules is performed. But let us have a look at some specific or some typical examples of how dirty data can be cleaned. For example: how do we search for illegal values within what is the strategy for illegal values. We could use some kind of a max min or a mean deviation or cardinality criterion that is slide a window through your data source that contains a max min limit for the particular data value. 

Whenever a given data value is lesser than the mean or greater than the max, you know that it is an illegal value. So, similarly mean and deviation or cardinality and so on and so forth and spelling mistakes. How would you look for spelling mistakes? There are some techniques what are called as n gram outliers an n gram is essentially a collection or a sequence of n letters that form a different words. For example: suppose I take a three gram, let us say hashing I perform a three gram transformation on hashing, then I get several different three grams. ‘has’ is a three gram, ‘ash’ is a three gram and ‘she’ is a three gram and ‘him’ is a three gram and so on. 

So I get different three grams, then what we do is we cluster all these three grams based on their occurrences and we see if (Refer Slide Time: 44:36)…………… layers in the clusters that is, are there any n grams which standalone without belonging to any cluster. First slightly, these outliers are going to be spelling mistakes and that is the strategy that is generally used for checking spelling mistakes. Similarly, lack of standards that is compare values sets from a given column across different tables and see whether they are using the same standard or so on and duplicate and missing values that is compare the number of rows with the cardinality of this particular column. So, you find out if the number rows do not match the cardinality you see that there could be some kind of missing values or null values say in this. 

[Refer Slide Time 45:31]

 


So, let us look at one or two algorithms in slightly more detail which can give you an appreciation of the kind of the complexity that it takes or the kind of techniques that are used for data training. Simple algorithm for duplicate elimination what is called as the hash merge algorithm. If you remember the storage structures session that we that we covered you know what is meant by hashing right that is given a particular tuple. Run it through a hashing function so that it is mapped on to a given bucket. 

Now what can we say about hashing across different tuples? Duplicate tuples that is tuples with duplicate (Refer Slide Time:46: 21)…………… therefore elimination of duplicates now reduces to searching within a bucket for looking at any duplicate values and then eliminating them.














	

[Refer Slide Time: 46:36]

	 


So here is an illustration of the hash merge algorithm. Let us say, there are four different records like this and this is the hash key that is the name and address of the person that is we want to eliminate duplicates as for as person information goes. So as you can see, K J amit and 50 Lvl road, both of them map onto the same bucket and these map onto different buckets and so on. So you just compare within a bucket and then eliminate duplicates.
 
[Refer Slide Time: 47:07] 

 

There is another technique called sorted neighborhood techniques. This is mainly used for misspelling or detecting misspellings in data sets and sorted neighborhood techniques is given by the following algorithm shown in this slide here. First of all, identify what is the key? As in the previous case, we identified that name and address is the key so identify what is the key within a given tuple. Then sort the table with (Refer Slide Time:47:41)…………… the table based on the key you can see that all duplicates will cluster together right. Similarly all misspellings, there is a high likelihood. It is not that there will be but there is a high likelihood that all misspellings will also be clustered together that is will be quite close to the actual spelling that has to exist that is for example if K.J Amit become let us say K.J Aman or something like that a m a n. 

So it is Aman is still quite close to Amit that is assuming that we are sorting tens of thousands of records, misspellings would be quite close to the real spelling. Then, you slide a window that is take a set of n rows and keeps sliding it through the data base to see if there are any misspellings and then merge the misspellings and we have to make multiple passes until there are no more merges of records. 

[Refer Slide Time: 48:55]

 


So here is an example again where this is used for duplicate elimination in addition to misspelling detection where given this table here it is sorted like this here and given a window of size three we see that we see that there are duplicates within this window and we start eliminating them and the last algorithms that we are going to see for duplicate 






[Refer Slide Time 49:15]

 


elimination and also kind of and also for misspelling detection and so on what is called as the graph based transitive closure for (Refer Slide Time: 49:32) ………. this algorithm is more of improvement over the sorted neighborhood algorithm where it basically reduces the number of passes. The idea behind the algorithm is based on the notion of transitivity in an equivalence relation. For example: Let us say records R1 and R2. We see that R1 and R2 are duplicates. 

So we establish a relationship that, R1 is a duplicate of R2 and so on and then we find that R2 is a duplicate of R3. We can definitely infer that R1 is a duplicate of R3 as well, because the duplicate is duplicate of relationship is a transitive relationship if a is a duplicate of b and b is a duplicate of c the then a should be duplicate of c what this means is when sliding the window once we established that a and b are duplicates, there is no need for us to compare a and c and so on. So that is the essential idea behind graph based transitive closure. 













[Refer Slide Time 50:57]

 

This slide here shows the schematic example where there are five different records and in a naive sliding window sorted neighborhood protocol, we start by comparing R1 R2 and R3, then R2 R3 and R4 and R3 and R4 R5. We are just sliding it with a window size of three. However when we encounter when we slide R1 R2 and R3. Suppose, we see that R1 R2 and R3 are duplicates. So R1 is a duplicate of R3 and R2 is a duplicate of R3 and so on. Now, there is no need to compare R1 and R2 with R4 and R5. It is sufficient if we just compare R3 with R4 and R5. So that is the general idea behind this algorithm which can reduce the number of passes in eliminating duplicates. 

[Refer Slide Time 51:52]

 

The next topic that we come in in the data cleaning and integration is the issue of integration itself that is, there are different OLTP data sources from which we are getting data. Most of which have different kinds of inconsistencies, sources of dirty data and so on and then we have to clean them and so on. But, after we have clean them we have to integrate them under a under a common banner and this integration can occur at two different levels which are shown here as data integration and schema integration. That is schema integration entails forming an integrated schematic structure based on the set of all desperate data sources that is you have data sources from counter A counter B counter (Refer Slide Time: 52:50)……… under one common schematic structure and data integration essentially entails cleaning and merging data from these different sources that is eliminating duplicates and merging all of them under this schematic structure. 

[Refer Slide Time 53:08]

 


Let us briefly look at what are the issues or what is the complexity now in schema integration as well and it is not as simple as it looks like. Consider the following schemata that is shown here or there is a schema that let us say the one of the OLTP sources used which is called which uses a table called cars and the car table has different fields like serial number, model, color, stereo glass tint so on and so forth. Everything is in one table. On the other hand, another retail center let us say uses two different tables for the same information and look at how these tables could be different. First of all, there are two tables rather than just one table then the names are different all these names are in German and these are in English. So cars are called autos and serial numbers are called as serine numeral or something color is called farbe and so on. So, it is not just the structure is different, the name of each of these fields could be different. The data types could be different and so on. 



There are several different challenges that we have to face when performing schema integration that is there are naming differences, there are structural differences, data type differences, semantic differences, missing fields and so on and so forth. 

[Refer Slide Time 54:42]

 

The generic architecture of schema integrator is shown in the slide here where it basically consists of two specific stages. One is the lower most stage is what is called a wrapper or an extractor. That is given a particular schema, the extractor maps all given schema into a standard schema set that is whether it is color or whether it is cars or autos or whatever it maps it onto a common schema set and then the mediator constructs the overall schema set or rather it looks at the federation of different data sources having the same schema and then handles query based on or constructs the data warehouse or handles query or whatever. 
















[Refer Slide Time 55:35]

 

	
So the difference between a wrapper and an extractor is that an extractor essentially physically extracts data, creates a schema and physically extracts data from the data sources according to the schema, whereas the wrapper is just a logical wrapper. Similarly mediator is a logical mediator while a constructor physically constructs a data warehouses from all the extracted data sources. 

[Refer Slide Time 56:04]

 

So there are different tools for data cleaning and integration and as I said there is no there is no common tool that can fit all possible requirements, but there are several tools that can make the life easier for sink. So, here are some examples d f power or eti star and SSA name and so on and so forth right. So let us summarize what we studied in this session. We looked at the important differences between OLAP and OLTP queries. What are the difference between analytical queries and transactional queries? What are the characteristics of OLAP (Refer Slide Time:) ………… of a data warehousing system where you take data from different OLTP fields and take them through data cleaning and integration phases before populating the data warehouse and then of course, there is a feed back and back flush and so on and this itself is a major issue with the machine. 

So that brings us to the end of the session.

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 31

Introduction to Data Warehousing and OLAP – Part 2

Hello and welcome back. In the previous session, we were looking into specific kind of databases kind of any sort or any kind of databases mainly meant for managing historical data or archival data, namely what are called as data warehouses. That is we are looking at how the database design changes if the application domain is changed from an operational data management that is operational data management to data management for strategic purposes that is asking queries that are of a strategic nature.

(Refer Slide Time: 01:41)

 

And we saw that, we saw the overall architecture for a data warehouse were you have to populate a data warehouse from different, several different data sources mainly the OLTP data sources and these data from these sources are routed or source through a data cleaning and integration engine where we saw what is really meant by data cleaning. That is there are several different sources of dirty data being creeping in different kinds of inconsistency, different kinds of missing values, spurious abbreviations and so on. And we need to clean those data elements before populating it on to the data warehouse and of course the data elements are to be integrated under a common schematic structure before the database can be populated.

And we have of course things like back flushing of clean data back in to the OLTP databases and we have feedback from the data warehouse to the cleaning engine and so on.
Let us move forward in this exploration of data warehouses to see what forms the core of the data warehouse. What kinds of data models are used to the core, what kinds of index structures or what kind of storage structures and such things are actually applied to the core of the data warehouse.

(Refer Slide Time: 03:56)

 

So in this session we shall be concentrating on data models and warehouse design. To begin with let us have a look at some examples OLAP queries like we saw in the previous session. Have a look at this OLAP queries something like which says how is the employee attrition scene changing over the years across the company or is it financially viable to continue our manufacturing unit in Taiwan. Now the thing is a characteristic nature of these queries is that most of them involve huge amounts of aggregation. That is we need to span or we need to read huge amount of data elements before we are able to conclusively answer a query. 

Look at the first example. How is the employee attrition scene changing over the years across the company? See we need to detect the trend here. That is we need to detect the trend or employees more likely to quit the company now or employees more likely to stay or they happy or they unhappy or they finding better opportunities away outside and so on. Now in order to answer this question, we need to take employee data from across the company that is different data sources how many ever branches I have, we need to collapse such data from across the company and across the period of time. It’s not sufficient if I just look 1 year data, 2 years data. We need to collect data over a period of time may be 5 years 10 years or something like that and aggregate it across employee behavior across the entire company and then we get a result something which something of the form which says that employee attrition seen or employee attrition rate is so and so percent.

That is given whenever an employee joins, there is so much of a probability that he is going to leave in say 1 year or 2 years or 3 years and so on and so forth. So just to answer this small query that is just to get the small set of percentages, you see we need to actually go through the huge amount of data that’s present in the data warehouse. So what are some of the typical characteristics? One, we saw of course aggregation that is we have to aggregate over a large set of data and summarize results over a large data set. 

We need to also often cluster data based on different parameters, is there a correlation for example in the previous slide we saw some queries talking about correlation. So suppose you want to find out correlation then we want to, we may actually want to find out clusters along a particular dimension and or along a particular set of dimensions to see whether there is actually some kind of correlation for some values in the dimension to some kind of behavior that we are searching for.

(Refer Slide Time: 06:03)

 

There is also trend detection that it is common in OLAP queries where you need to, you need to in some sense integrate over time. That is you need to start from the earliest source and start working your way through the through till the latest source of data and project the temporal behavior of the data elements along certain dimensions and say this is the trend that the trend is increasing towards this thing, the trend is decreasing, the trend is fluctuating and so on and so forth.

Then there are what are called as multi-dimensional projections that is a behavior could be attributed to several different dimensions and we may want to ask how each dimension contributes to certain behavior. How is the total sales dependent upon say the location of my branch, let us say I am having several branches, a retail branch across the country and I want to see how sales are correlated to branches or how sales are correlated to different events like say some festivals and some kinds of national holidays and so on or the weather is it, the sales is more in summer or winter or what and so on.
So you may have to actually project these facts, this fact called sales across several different dimensions and this has to be performed as efficiently as possible no matter what the dimension is. This slide shows the typical design of data warehouse at the core of the data warehouse say as you can see in this slide, there is a kind of something typical about this slide. That is there is a core here and a lot of other surrounding elements here. The core is what is called as the hyper cube of or also called as the data cube and several other names which maintain facts that are sorted across several different dimensions. 

(Refer Slide Time: 08:29)

 

For example I may want to store sales data across several different dimensions like say the branch dimension, the year dimension, the event dimension, the product dimension, which product has the most sales or which branch has the most sales or which year has the most sales or which event brings in the most sales or which, whatever I mean product or the category of products and so on and so forth. So we will look shortly in to how this hypercube itself looks like or how we can design such hyper cubes and that basically stores facts that are contained in the data warehouse.

However maintaining purely just the hypercube would make the data warehouse pretty inefficient. This is because again the good old as aggregation problem that is even if there are facts that are stored efficiently in a hypercube. We still require to access let us say tens or thousands or probably millions of different facts before we are able to answer a query, before we are able to answer a query like is it financially viable to continue this branch or whatever. We need to look at several different facts before we will be able to come out with the response. 

So in order to speed up such processing, there are number of what are called as materialized views. Remember the notion of views from traditional data base. What is a view? View is basically a logical substructure of the overall schematic structure. That is all of these materialized views are in some sense sub cubes of this hypercube but in traditional databases, views are essentially virtual tables. That is you store the view as a query not as a base table that is not physically in the database. So whenever you’re querying a view, you first have to execute the view that is generate the view first on the fly and then query the view. But, here that completely defeats the purpose of what we are looking at essentially speed up in performance that is query response times. 

So these kinds of materialized views are actually physical views that is views which are physically computed from the core, from the hypercube core and stored on to disc. These could be something like, some kind of partial aggregate information, something like let us say I have sales data across all branches. Now I might want to store sales data for a particular product. Suppose I have a retail store, I may want to store sales data for let us say some kind of vanaspati oil or whatever   across all different branches. I may want to store partial aggregate information like sales data across quarters, across different quarters in a year and so on and so forth. So such kinds of such kinds of data which are queried often are materialized from the hypercube core and stored as materialized piece. 

Now I am sure you might be wondering, why this would make sense, why don’t we do that in traditional databases. Because, we cannot do that in traditional databases because it hampers the concept of normalization. That is materialized view is essentially creating extra functional dependencies which over and above whatever is present in the core itself. Now what happens if there are extra functional dependencies? Maintenance is a big problem, that is whenever an update happens because there is redundant information or derivable information or maintaining all those derivatives of information becomes a huge problem.

For example if I have a particular amount of sales data for a particular product in a particular branch and let us say tomorrow I update this data element with a new number and this automatically affects all aggregate information that I am storing here. What is the aggregate information for all products in this branch or for this product in all branches and so on and so forth. So maintenance of materialized views is a problem. That is whenever the databases or data warehouse is updated the materialized view should also be updated.

However this is not too much of an issue in data warehouses mainly because I am sure you would have got, what is the reason because the number of updates is far less frequent than in a typical OLTP set up. In an OLTP set up that is in an operational database there are frequent updates and queries. The number of updates is comparable to the number of queries but here the number of queries is far more than the number of updates. The number of updates is very infrequent, possibly of the order of once a month or once in 6 months sometimes and so on where you can actually afford to bring down the warehouse system, the OLAP system, update the OLAP engine with the new data it has come in and then compute all the materialized views.




(Refer Slide Time: 14:32)

 

So this cube, this slide essentially summarizes what we have just talked about that is there is a hypercube core that makes of the core of the data warehouse. And the core manages what are called as atomic data elements that is data elements which cannot be sub divided into other kinds of data elements. And it’s the global schematic structure for the entire warehouse. Remember this global schematic structure is what we have derived from the different OLTP sources. And this is based on the multidimensional data model and we will see mechanisms by which we can actually implement it in practice.

And materialized views are physical views for faster aggregate query answering and it is, it essentially amongst to de normalization of the core. And this de normalization is performed to increase performance, is done to increase performance rather than updation and updation is an issue which but because updation is relatively infrequent, it’s not too much of a problem. So let us look at the hypercube core again in a little more detail. This slide here shows a particular hypercube having three different dimensions. Of course the reason why we are calling it as a hypercube is that it can have any n different dimensions need not just be 3, it could be 5, 10, 25 whatever. So it could be any different dimensions.












(Refer Slide Time: 15:27)

 

Now as you can see here each dimension talks about specific aspect of the data that we are storing. Let us say this is the sales hyper cube, the amount of sales that has happened. Now this dimension stores the amount of sales across each product and this is across each week and this is for each branch. Therefore each cell in this hyper cube refers to sales of a particular product in a particular branch in a particular week. So in this hyper cube, the constituent of the cells that is the sales data is what is called as the fact that is stored by the hyper cube.

(Refer Slide Time: 16:21)

 

So the hyper cube manages the fact called sales and branch, product and week, along which the sales are projected, are what are called as dimensions. So the sales fact is projected across three different dimensions. In addition to normal relational database operators or rather in lieu of usual relational databases operators, there are several other operators that are defined on the hyper cube itself. Here are some of the operators that are commonly used for data warehouses. 

Pivoting: pivoting essentially means, what do you understand by the terms pivoting? Essentially it is rotating the cube given a pivot that is suppose you want to see the sales data projected across some 3 different dimensions or some 4 different dimensions or whatever. We choose this set of dimensions so that these can be displayed on the visualization engine. Data warehouses or OLAP engines usually come with a, usually have a huge visualization counterpart which helps the user to visualize the inherent knowledge contained in the database or in the data warehouse in different ways. That is one way is that of pivoting the warehouse across different dimension. 

Then there are slicing and dicing operators that is selection of some sub set of the cube. You can think of it as slicing the cube across different dimensions or dicing that is forming a sub cube out of the larger hyper cube and so on. 

(Refer Slide Time: 17:16)

 

Usually data cubes also supports this role up operator were you can aggregate information on one or more dimension. For example we may want to roll up the weeks data into months data that is in the previous example (Refer Slide Time: 18:28) we have data per week. Now instead of data per week, we might want to collapse certain sets of dimensions like this in order to form month wise data. So that means we have rolled up in the week dimension across one aggregate level so that the week has now become months and so on.
Similarly the opposite operation of roll up is a drill down operation were given an aggregated dimension, you open it up in order to reveal more details. That is open up months to reveal week by week information or open up weeks to reveal day by day information and so on and so forth provided that exists in the database, in the data warehouse. 

How are these hyper cubes implemented in practice? In practice there are two major approaches towards implementation of hyper cubes, what are called as ROLAP and MOLAP approaches. ROLAP essentially means the relational OLAP that is where the multi-dimensional hyper cube is transformed by certain set of transformation rules to the relational model and vice versa. That is the relational model is transformed back in to hyper cubes for answering questions. So every hyper cube queries are mapped on to relational database queries and vice versa. 

(Refer Slide Time: 19:23)

 

And the other kind of implementation strategy is called MOLAP or naive or native multidimensional model where the storage structures and index structures are redesigned for storage of multi-dimensional data itself and which has got nothing to do with the relational model in itself. So when you provide a data warehouse query, a MOLAP query it is directly translated on to disc accesses and further such low level query primitives and answer directly.

So there are, this slide also shows some examples like say true relational OLAP, it’s from micro strategy which is an example of a ROLAP and Arbor Essbase which is an example of a MOLAP system. Let us look at ROLAP systems in a little bit more detail that is how are ROLAP is actually implemented, how is a hyper cube implemented, how is a collection of hyper cubes implemented and so on. 

The most fundamental form of implementing a hyper cube using a set of tables is what is called as the star schema structure or the star schema architecture. The star schema architecture is shown in this slide here. As you can see the name star schema comes from the way that the schema is organized. That is the schema has the central fact table and the fact table obviously stores facts. That is their sales data for which is the fact which is stored for all possible combinations of branches, branch products and weeks.

(Refer Slide Time: 22:16)

 

That is branch one, product one, week one sales so and so. Branch one, product two, week one sales and so on. And each of these columns here that is branch, product and week contain ids which are foreign keys in to the respective dimension tables. That is there is a branch dimension table which contains information about branches. So if branch number one is stated here, this would be a foreign key here which would give details about what is branch number one that is the name of the branch, the address, the manager, whatever and so forth. And similarly product, each product is a foreign key here which gives details about each of this product and so on. So this is the most simplest form of representing multi-dimensional hyper cube in a relational data model. 

Now let us briefly summarize what a star schema is about. There is a central fact table and there are set of supporting dimension tables that is in the previous slide, the green table is the fact table and the other three brown tables are the dimension tables. And the advantages of a star schema, it’s pretty simple to comprehend and also to design and there’s very small amount of meta data that’s required, small numbers of foreign key relationships that have to be maintained and its very quick for query responses as long as the queries are on the facts that are stored in the database.




(Refer Slide Time: 23:38)

 

However there are certain limitations as well. That is its not robust towards changes especially in the dimension tables. Suppose if I want to add something to the dimension table, it’s quite difficult and there is an enormous amount of redundancy in dimension table data because dimension tables need not be normalized and we may just be repeating information in the dimension table. 

The next kind of relational model that is used is what is called as the snowflake schema. The slide here shows an example of the snowflake schema. Well, it doesn’t look like a snowflake in the slide but you can imagine why it is called as snowflake schema. That is it is a star schema that is augmented with something more. That is the branch dimension here itself consists of 3 different tables. The product dimension here consists of 3 different tables and the week dimension remains as it is.

What are these three different tables? These are basically normalized data structures or normalized versions of the branch dimension table. So if the branch contains several information the manger information, the location information, the turnover information and so on and so forth, you may want to essentially break it up into different table so that accessing data about branches becomes more efficient, similarly for other products. 










(Refer Slide Time: 24:14)

 

And the result is that each such dimension here has sub dimensions which kind of gives the gives an overall structure like a snowflake where there is a central fact table and branching out dimensions and so on. So the features of a snowflake schema detail here that is like always there is a central fact table as in the star schema. And however rather than just dimension tables, in storing de normalized or un normalized data it’s not of de normalized it’s more of un normalized data.

In each of these dimension tables, its actually dime normalized data that is stored in the dimension tables. And what are the advantages of this thing? The query responses are faster, especially on the dimension tables. We don’t have to work search through the dimension tables for searching on a specific queries because it’s normalized and you have several foreign key relationships and index structures and so on. And it’s more easier to update especially on the dimension table but there are certain limitations as well. That is there is large amount of meta data in fact the because we are talking about aggregated data that is archival data, we might very well end up with a large amount of tables especially when we try to normalize the dimension tables. 












(Refer Slide Time: 25:27)

 

So a given branch information may create possibly hundreds of different normalize tables, if we have to normalize it to say let us say fourth normal form and it becomes as a result it becomes much harder to comprehend when we are looking at it manually, when we are looking at the overall schema manually.

The third kind of ROLAP model or the relational schema model for implementing multi-dimensional structures is what is called as the constellation and this is perhaps the most used model, most practical model because it contains multiple fact table not just one fact table. 

(Refer Slide Time: 26:40)

 
And in a data warehouse, we usually require to have multiple fact tables that is let us say each fact table corresponds to one data mart. So one data mart stores information about sales, the another stores information about salaries, the other stores information or facts about procurements and so on and so forth or cause and so on. So when we have several such fact tables, we may want to actually, we may want to actually have several stars schemata intermingling among one another. And in addition, suppose whenever we have some kind of materialized view for example let us say this is a sales data and this is some kind of a materialized view that talks about aggregated sales data across different or it just talks about the discounted sales that is sales which have been made whenever discounts were offered and so on. 

So it is some kind of a materialized view that has been materialized from the main fact table. So as a result what happens here is this fact table here shares some of the dimension tables from the main fact table, from the sales fact table. So as a result you have a dimension table actually having foreign keys in 2 or more fact tables and 2 or more fact tables having foreign keys in to this dimension tables.

(Refer Slide Time: 28:47)

 

So the result looks like a constellation of different stars that is the different stars and they are staying together so that it forms a kind of a constellation. So the constellation is the most commonly used architecture and used when multiple fact tables are required. And usually not and that is even when multiple fact tables are not necessary or constellation schema is used in order to maintain materialized views in addition to the central fact table. That is it usually has a main fact table and several auxiliary fact tables which are materialized views over the main fact table. It helps in faster query answering for frequently asked queries, however it is costlier to update than a snowflake schema which is kind of obvious in this. 
 
What are some of the issues in data cubes? There are certain especially managing data cubes in a ROLAP architecture that is using a relational data model there are certain issues which in some sense refuse to go away.

(Refer Slide Time: 30:20)

 

That is they can be sometimes so severe that it may render, it may render the data warehouse useless. That is it may not able to provide interactive response times which is the most important factor for a data warehouse. One of, probably the main such challenge is what is called as the curse of high dimensionality. There is several known index structures by which we can index on different tables or on different dimensions as efficiently as possible but most of such known index structures degrade in performance, degrade to almost linear search in performance when the number of dimensions become high. 

So with a large number of dimensions, searching efficiently for a particular data element becomes really difficult. Then they are what are called as categorical dimensions. It may not be possible to order the elements of dimensions across a given scale. For example week it is easy to order. Let us say I have a week information, so I can say week 1, week 2, week 3, week 4, week 5 and so on and there is an implicit ordering that is we know that week 3 should come after week 2 and before week 4 and so on. It’s not possible for week 3 to appear after week 5 and so on. 

So there is an implicit ordering information. Now this ordering information is sometimes exploited for faster retrieval and computation of clusters and so on. But then there are certain kinds of dimensions which are categorical in nature, not ordinal. For example products or say branches, products let us say vanaspati oil, rice and ghee and so on and so forth. 

Now which comes first and which comes second? Should oil come before ghee and should ghee come before rice or the other way around. There is no such ordering that is given for the elements in this dimension. So, categorical dimensions becomes difficult to index. We look at some index structures later on where we will see that why it becomes difficult to index categorical dimensions. We cannot identify a region and say this data element falls between this and this region because there is no ordinal information about the dimensions. And even when there is ordinal information about dimensions, sometimes ordinal classes vary whenever we aggregate. For example this slide shows a given example here (Refer Slide Time:32:38), let us say at the lowest level, we are having names about different sets of students that have come through a given university and passed out and so on. 

So we have ordered the set of names, student names according to lexicographical order. However when the set of students are aggregated into classes, let us say batch 1, batch 2, batch 3, batch 4 and so on. Then the ordering becomes, the ordering is now based on their graduation year, the batch 1 graduated in 94, batch 2 graduated in 95 and so on and so forth. Now it’s no longer lexicographical ordering. So the same index structure that is used for the lower level dimension cannot be used when the data cube is rolled up. That is when we have aggregated the set of students to a set of batches and so on.

(Refer Slide Time: 33:56)

 

When we are designing a data warehouse there is another, there is a very important dimension called the time dimension which probably needs to be addressed separately and that’s why there is a separate slide on the time dimension because there is certain properties about time which other dimensions may not really adhere to, may not really hold.

The time dimension as you see is usually mandatory in most warehouse applications otherwise that the whole idea of archival data or historical data becomes meaningless if there is no time dimension associated with it. And if you are talking about trend detection and so on, we cannot do it without time dimension. And the time dimension has several different meanings based and for ROLAP techniques depending on the application context. that is suppose we rollup weeks into months or months into year, should we follow the simple calendar based rollup or should we follow the physical calendar, let us say if it is a company or should we follow the academic calendar if it is a university and so on. 

So it has several different meanings when we are talking about rollup. And we need to also, in time we need to not only just order based on time, we need to also index some special events like graduation date or releases or some festivals or so on and so forth which actually affects the facts like the sales or whatever it is that we are storing. And we have to we have to timestamp these events with specific whatever time scale that we are using whenever we are having the time dimension. And lastly the order of traversal of the time dimension is important.

If we have let us say a set of all students or set of all products and we want to find out some aggregate information, it does not matter whether we traverse this dimension front to back or back to front but when we are talking about time and when we are talking about trends, is the trend changing over time, the order of traversal becomes extremely important. And let us look back at the materialized views. Coming back to the materialized views, how do we know when to create a materialized view?  	

As you know materialized views are summary tables that actually create physical views of the fact table. And creating a materialized view is a tradeoff between faster query answering and increased complexity during updates. That is whenever I update the data warehouse, there is a question of view maintenance, materialized view maintenance. So how do we know when to materialize? A good measure for making such a decision is what is called as the RSS ratio or the result to search space ratio. The result to search space ratio is a simple ratio which is shown in the slide here.
















(Refer Slide Time: 36:03)

 

That is how many number of rows of results are returned divided by the number of rows that are scanned for a given query. For example if I say what is an average age of what is the average age of the employee or how was the average age of the employee change over the years. So let us say every year we have some 4000 employees, employee records and we have 10 years of data. So the number of rows that are returned are just 10 for each year the average age.

However the number of rows that are scanned are 4000 times 10, about 40000. So that gives us the RSS ratio. So, we decide to summarize if the RSS ratio is too small and the query is too frequent. That is if 10 by 40000 is considered too small given the frequency of the query, we say we have to materialize this. That is we have to physically create a separate table for this.
















(Refer Slide Time: 38:11)

 

There is also an other kind of  table in ROLAP structures which are again useful, which are called as revision history tables and which are again quite crucial when answering different queries. Remember in data warehouses, we are storing archival data that is data over a period of history. Now archival data essentially means that there is more of a probability that some kinds of data elements are revised over time. For example turnover per employee as shown in the slide here that is what is the turnover that the company is getting per employee, that is the total turnover divided by the number of employees. Now it keeps changing every year. 

Now let us say, suppose we just have to, we don’t have to, we don’t want to store it as historical data that is we don’t want to store it as how turnover is changing over time but rather what is the turnover per employee. So the revision history table essentially stores the different values that this turnover per employee has taken up over each different updation. That is the first updation, the turnover per employee was this and the updation was on this date. Whenever this value was updated the second time, the value is this and the updation date is this and so on. So this information is used based on what time frame is the query addressing, when it is asking a question. For example I might say what was the turnover per employee in the last decade, then I might want to take up some value that was updated in the last decade rather than this decade and so on. So that’s where revision history tables become important.  

So let us quickly look at what are the different steps that it takes while designing a data warehouse. See when we are designing a data warehouse, we are looking at a warehouse from a managerial perspective. That is we are essentially starting out with an enterprise model that is we have an enterprise model of how our enterprise works like whether it is a company or whether it is a large organization or university or whatever. There is certain kind of enterprise model, a university is divided into departments and labs and faculties and so on and so forth and so on. 
(Refer Slide Time: 40:00)
	
 

Now based on the enterprise logical model, we built the logical data warehouse model or the data warehouse logical model and from the data warehouse logical model, we come down to the data warehouse physical model which is the star schema and snowflake schema or constellation and so on and so forth. 

Now this part is handled by the data warehouse administrator or the database administrator along with the end user. The database administrator has to clearly understand the end user requirements before building the logical model. And here it’s usually a question of the database administrator using one or more automated tools that can translate are given logical model on to a physical model. There are some design for the most pattern art where how to get good designer, how to calibrate design is actually quite a tough problem to answer. 

However there are some thumb rules that can help the designer in designing good data warehouse models. And this slide summarizes some of the thumb rules, things like the warehouse logical model should closely resemble the enterprise model and not let say the operational model. The enterprise model is the overall strategic model of the enterprise while the operational model is the model of let us say given particular operation. Let us say withdrawal from an ATM has a particular operational model. But this operational model rarely figures in the overall enterprise model of the bank itself.








(Refer Slide Time: 41:49)

 

So the warehouse logical model should resemble the enterprise model rather than the different operational models. And note that the OLTP sources are based around the operational models rather than the enterprise model. And there are some transformation rules which are usually necessary from enterprise to warehouse models.

(Refer Slide Time: 42:44)

 

And special planning is required for managing time dimensions and revision history which we saw a special tables that we saw in the previous slides. So how do we, let us say how do we design this data integration that is from OLTP sources to data warehouses. What it is entitled for creating or integrating schemata from OLTP sources to data warehouses. So OLTP to OLAP or OLTP to warehouse models are also based around certain sets of design principles which again have certain kinds of thumb rules again like there is no simple way of or there is no algorithmic procedure for designing schema. 

However this thumb rules help in creating a good design or evaluating different designs. For example several OLTP schemata has a number of operational data fields like counter sales table that is the cash register number or the cashier identifier and so on. Such operational data fields are usually not useful for strategic decisions something that talks about what is the trend in sales and so on. We usually don’t care which cashier did what and so on. So we have to remove all such operational data fields, it’s important to identify which data fields are purely operational and which data fields have some kind of strategic importance.

(Refer Slide Time: 43:20)

 

And of course we have to add a time element. Time element may be implicit in the operational model but here we have to add them explicitly for the data warehouse model and also of course version elements if necessary. And we have to decide on the derived data or the materialized views as early as possible and usually commonly used materialized view is what is called as the ALL view. That is for example if I have product wise information let us say product is a dimension, so we have product 1, product 2, product 3 as different values in the dimension and the last value will be ALL. 

So, ALL basically says what is the sales value for all products. Similarly ALL value in the branch dimension would say what is the sales value across all branches and so on. So that was about ROLAP structures and let us briefly look at what kinds of strategies are used for MOLAP structure that is native multi dimensional databases or hyper cube maintenance and what kinds of index structures are used for handling such multi-dimensional retrievals.

(Refer Slide Time: 45:09)

 

So, just to summarize we have different classes of dimensions, there could be categorical dimensions, there could be ordinal dimensions. Dimensions could be sparse where there are small number of data points per value or dimensions could be dense where there are large number of data points per value.

(Refer Slide Time: 45:45)

 

Now if we are talking about indexing note that when we are talking about MOLAP, I directly went into indexing because this is the main indexing in addition to the storage structures are what constitutes the native support for MOLAP. 

So when we are talking about indexing, indexes are usually based around ordinal classes but there are categorical indexes as well, as we will see and the performance may depend on the storage structure for any given data set.

(Refer Slide Time: 46:22)

 

So what kinds of storage structures are used for multi dimensional data or how do we store multi dimensional data or hyper cubes on to disc.

(Refer Slide Time: 46:44)

 

Remember discs are just stored in blocks and sectors and so on. So, one way of storing is simply multi dimensional sorting that is let us say there are 3 dimensions and there is a fact. So you just sort the table along all 3 dimensions, first along dimension 1 then along dimension 2 then along dimension 3 and so on, so as simple as this. So, it just becomes a table one long table, the hyper cube is just one long table and this is very simple to implement and searching is very fast, if the dominant attribute that is in this case dimension one is part of the query, else it becomes fragmented and it becomes slower. Another kind of storage structure that is commonly used are what are called as space filling curves. Have a look at the slide here carefully. As if you see in the slide, this slide shows a two dimensional space discrete two dimensional space that is 0 1 2 3 4 like this, 0 1 2 3 4 like this and there is a curve that has gone through the space like this.

(Refer Slide Time: 47:16)

 

That is it has covered the entire space without revisiting any point again or without revisiting any point twice. So it is quite simple to calculate the position of a data element whenever a space filling curve is used like this. That is the first data element 0, 0 is here and 0, 1 is the second data element and 1, 0 is the third data element and 2, 0 is the fourth data element and so on. So, it is very easy to compute the location of a data point.

However, it suffers whenever the number of dimensions are high or especially when there are sparse dimension that is when there are large number of cells which have no data points associated with them. Then there are, so from these storage structures let us move on to some kinds of index structures that is how do we index elements based on this storage structures. 

The simplest kind of index is the ordered index on multiple attributes that is instead of when you have to index based on multiple dimensions, index on each dimension separately.



(Refer Slide Time: 48:47)

 

So suppose you want to index on n different dimensions k1 to kn, index on k1 separately and k2 separately and k3 separately and so on and ordered index files are maintained by ordering each such key in sequence. But of course ordered dimensions suffer from several short comings especially when the number of dimensions is high or when the number of data points is high, you need to compute intersections across these different dimensions which becomes a lot of overhead. There is also what are called as partitioned hashing where given a composite key like this that is n different dimensions, a partitioned hashing returns n different bucket numbers and the hash bucket is simply the concatenation of the n different bucket numbers.

(Refer Slide Time: 49:42)

		 
And other kinds of multi dimensional indexes include the grid files. We have seen the grid files when we are talking about index structures for normal databases.

(Refer Slide Time: 50:00)

 

So, we shall not be covering grid files now in great detail except to state that we can think of set of buckets forming a grid and a multi dimensional index would in turn map on to a set of buckets in this grid are what is called as the bucket pool and then we essentially find out what is the or find out the corresponding data points.

(Refer Slide Time: 50:18)

 

Then there are what are called as the bit map indexes which are quite again, quite commonly used and these are used on fields that are sparse that is where fields have only a small number of values. For example gender has only 2 values or grades in a university setting may be has 5 values a b c d e f, a b c d e and so on.

(Refer Slide Time: 50:55)

 

And essentially what we do is we store a corresponding bit vector that enumerates all possible values and sets the corresponding bit for each data element. And it’s much more compact than other index structures because we would be able to answer composite queries like this product and this branch and this week and so on and so forth, quite efficiently. This slide shows a particular example. Let us say there are 3 subjects database, artificial intelligence and say parallel distributed systems and 6 different grades a b c d e f.
















(Refer Slide Time: 51:21)

 

Now each grade is denoted by a specific bit that is set in a 6 bit vector that is there are 6 vectors in this and each grade has a particular bit set and no values, all zeros. Similarly there are 3 different subjects, so there is a 3 bit vector here and corresponding bit is set. Suppose we have to search for all students who have scored a in databases and AI. What is that we need to do? We just need to compute the logical and between databases and AI and grade A and that would be the logical intersection of the corresponding buckets or the corresponding sets that are stored in this values. Another kind of multi dimensional index is what is called as KD trees which is again used for efficiently answering point queries in a multi dimensional data space.



















(Refer Slide Time: 52:25)

 

We shall not be looking into KD trees in more detail. Let me just explain KD trees by an example.

(Refer Slide Time: 52:37)

 

You might of heard of binary search trees were given a particular data element, all data elements having a key greater than this particular data element is in the right sub tree and all keys lesser than this given keys in the left sub tree. The same strategy here is expanded to k dimensions that’s where the KD trees come from k dimensional tree. 


(Refer Slide Time: 53:38)

		 

So let us say this is an example data set we have. There are 2 dimensions x and y, so representing let’s say salary and age and these are the set of data points we have. The way we build a KD tree is simply like this. The first element becomes the root, for the second element we start by comparing the x dimension, the first dimension here now because 5000 is greater than 25000 it comes here. For the third one we start by comparing the x dimension here the first dimension, so 4500 is greater than 25000 comes here. 

Now here we compare the second dimension that is 32 with 28 and because 28 is lesser than 32, it comes to the left of this tree and so on. So as you can see here, all the odd number of places corresponds to dimension 1 and all the even number of places correspond to comparing it dimension 2. So it’s a simple extension of the usual binary search tree data structure for k different dimensions but of course there is an other way of looking at KD trees as where each data point here is found to bisect a given space into two different sub spaces. We shall not go into much more details here because KD trees in itself is a vast subject by itself.













(Refer Slide Time: 54:15)

 

(Refer Slide Time: 54:25)

 

And there are other kinds of data structures which I shall not be going through like quad trees and R trees which is again a very commonly used data structures where we manage regions and there are certain user specified regions and there are certain virtual regions. 






(Refer Slide Time: 54:30)

 

(Refer Slide Time: 54:36)

 










(Refer Slide Time: 54:37)

 

And so each data point is also represented as a region of zero area and so on. So a region is supposed to consider, is supposed to contain other regions within it and it is supposed to have a particular capacity. Whenever a region exceeds its capacity, it is split to form 2 different regions. So the slide here schematically shows an R tree where this is the root node which is the overall region and which has two children having two virtual regions.

(Refer Slide Time: 55:45)

 

And each of these two children have two other children that is children that is child 1 and child 2 and there are to be a virtual region here, so child 1 and child 2 and so on and it is split correspondingly. And R trees are suitable for range searches, neighborhood searches, nearness searches and so on.

(Refer Slide Time: 55:47)

 

But however all of this suffer from the curse of high dimensionality and let’s not try to prove this here but it’s obvious when we analyze this data structures more closely. Lastly let us look at one data structure for indexing categorical data. Here as you can see until now we have been looking mainly at ordinal data whether it is K tree, KD trees or R trees it is important that there is an ordinal information associated with each dimension.

(Refer Slide Time: 56:08)

 

That is this is lesser than this and this is lesser than this and so on but here in categorical data we use what are called as signatures or bit map indexes by which we can say if the set of all ingredients whichever appear in a particular element is set to 1 here and all other elements are set to 0. 

(Refer Slide Time: 56:38)

 

(Refer Slide Time: 56:43)

 

So which basically forms what is called as the signature tree for categorical data. And there are other kinds of extensible signature hashing and so on and so forth which we are not going to cover in more detail here.

(Refer Slide Time: 56:56)

 

(Refer Slide Time: 56:58)

 










(Refer Slide Time: 57:10)

 

So let us summarize what we learnt in the data model section of data warehouses. We saw that a data warehouse contains the OLAP hyper cube at its core and having different materialized views and then we also saw different kinds of ROLAP and MOLAP implementations of this materialized views and different kinds of index structures, so that brings us to the end of this session. 


Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 32

Case Study: MYSQL

Hello and welcome to another session in database management systems. In this lecture today, we will be looking at a case study of a real world DBMS. We have learned so many concepts in database management systems like especially the relational database, I mean how relational tables are stored, what is the relational algebra and what is the query processing engine do and what are the different kinds of indexing methods and so on and so forth. Let us see how it is actually implemented in some real world context. 

In this lecture today, we are taking up a pretty interesting real world database system namely MySQL database system.

(Refer Slide Time: 01:24)

 

What is very interesting about this database system is that it is a phenomenon or it is an outcome of a very remarkable phenomenon of recent times namely the open source and the free software phenomenon. If it were not for the open source or the free software phenomenon, I would contain that several different innovations that we see in the realm of algorithms and software design would not have been possible. Today it is possible for anyone around the world to be able to freely download this database here which we are going to, this DBMS here which we are going to see today. And you can download it for free of charge that is it’s not only you can download it for free for charge, you can also have access to the source code that made up the MySQL database and you can make your own changes to the source code of the DBMS.
And it’s completely free in that sense, free as in freedom like which is generally used, which is the term generally used by the free software foundation people who allocate free software around the world. 

(Refer Slide Time: 03:37)

 

So let us look at MySQL in more detail. MySQL is a very popular open source database system and it is licensed under what is called as GPL or the GNU public license. Of course looking in to the inter cases of GPL is definitely not within the scope of our lecture here but essentially the gist of GPL essentially says that the software provided is free in the sense that you are free as in freedom that you are free to make any modifications to the source code. You can free to redistribute the source code, you can free to resell the source code if you wish to however you need to also give the same freedom to whoever is going to next use your software and so on.

And there are different varieties of GPL and you should check up what exactly is the specific variety of GPL when you download MySQL. And it’s a widely used database management system and it is used in the wide variety of systems like embedded systems to large scale information systems and it’s largely written in C and C plus plus and its source code is available like I said and it has been ported to many different operating systems and operating platforms. And the website on which it is available is shown here in the slide namely www.mysql.org.

Let us have a brief look at the history of MySQL, how it came in to being and what are its different features. MySQL is a relatively recent phenomenon in database management systems in fact the genesis of MySQL stems from another small SQL engine called mSQL, small m and SQL as shown in the slide here.



(Refer Slide Time: 04:49)

 

mSQL or I guess it stood for mini SQL was a simple SQL engine that allowed users to write SQL queries and maintain very simple databases with a small set of tables here. Now some set of people in Sweden actually who try to tweak mSQL, mSQL also was free software and they try to tweak mSQL and out of this endeavor arose a completely new set of database system which is now called MySQL. 

Now MySQL has now, the people who formed MySQL has now formed their own company MySQL AB which is a Swedish company and run essentially by the three people who formed this database system in the first phase. And of course there are also commercial versions of MySQL which are sold for a price and which come with lot more support from the MySQL team who can actually install and tune your database system and so on and so forth. But for the free version of the software, there is no support and there is no warranty as such. 

MySQL has several different features, there are several APIs or application programming interface that are available for many languages like C, C plus plus, Eiffel, Java, Perl, PHP and so on and so forth. This APIs allow programs written in these languages to directly send SQL commands to MySQL. So you can embed your database system or rather your database client within another application program. It uses B tree based disk tables and MySQL team themselves developed new storage structure is called MyISAM and also performs index compression, so that the shadow of the index on the disk is quite small. 

And there are storage engines that are available which support both transactional and non-transactional operations. Remember what is a requirement of a transactional operation essentially the acid semantics, that it has to provide atomicity and isolation and consistency in durability and so on. Durability also in a sense implies recovery that is once a commit has happened, you should be able to, it should be reflected in the database even if there is a crash later on and so on. 
MySQL is a multi-threaded engine and if your operating system supports threads at the kernel level, MySQL uses that and if your operating system and your hardware of course support multiple processors, MySQL automatically uses multiple processors.

(Refer Slide Time: 08:44)

 

So it is scalable to many processors on a multi-processor machine and is aware of the fact that there are many processors that are running and you can utilize them. The kind of memory allocation system that it uses is a thread based memory allocation system rather than a process based where each thread which is a light weight process manages its own memory. It also has other nice features like fast joins which is computed by an optimized algorithm which just uses one pass operation over the tables when performing the joins. 

We saw several different kinds of join techniques merge join, hash join and so on, so which essentially forms the gist of the kind of algorithms that go into these techniques here that computes fast joints. And whenever temporary tables are required, say virtual tables or tables as a result of a query in a nested SQL query and so on, they are implemented using in memory hash tables and they are extremely fast to access and so on. 

MySQL server has these double features that it can run as a standalone server as well as it can run as an embedded server that is server that is actually embedded within some other application. So your application actually runs as the server and it looks as though your application is supporting database system in itself.  






(Refer Slide Time: 11:27)

 

MySQL supports several different data types, several different atomic data types. We saw some kinds of data types when we are talking about SQL for example integers and date type and character strings and characters and enumer or the set data type where you provide the enumerated set of values or the set of all possible values that a particular data element can take and so on. 

Similarly in MySQL, there are several different atomic types that are supported, signed and unsigned integers and there are even different lengths of integers they are called as tiny int, small int, integers and big int and so on and so forth. So there could be 1 byte or 2 bytes or 4 or 8 bytes long and there are floating point numbers, double precision numbers, characters, varchar essentially is a string which can, variable number of characters and text and blob is a binary large object which could be something like any media object or audio visual file or anything like that date, time and so on. It even supports variable length records, your record length need not be fixed you can have a combination of fixed and variable length records within a single database. And MySQL supports functions in both from and where clauses of an SQL statement. 

Remember an SQL statement has three different parts where you say select something from something else where the condition. So you say select and you say set of field names or attribute names and which says what to select but here in fact you can say select and give a function. So you can say select average of the values of this lets say a total marks taken by or scored by students and so on.






(Refer Slide Time: 17:17)

 

So you can say select average of marks from student where marks equal to whatever and so on, where mark is greater than 60% or whatever and so on. So you can have functions where average here is a function that’s actually in the from clause, not exactly in the where clause. We saw in the session on SQL that functions being supported in the from clause is only a recent addition to the SQL standard and it was not primarily in the initial SQL standards. It also supports, MySQL also supports outer joins both left and right outer joins. Remember what is an outer join, when you are joining two different tables, you basically join let’s say you are computing an equijoin, so you basically join based on the value of the certain attribute. 

Therefore join happens where, join happens by matching values different values that this attributes takes between the two tables. Now it might so happen that for a particular value in the first table, there may be no corresponding value in the second table. Such records are generally ignored if you are computing an inner join or a usual conventional join operator. But in the outer join operator, you compute even those you don’t throw away such records and embed them with nulls essentially. 

So this example essentially is an example of left outer join and the corresponding or the dual operation of that were a given attribute in the second table has no corresponding attribute in the first table would form a right outer join. MySQL also supports aliases on tables and columns, so you can refer to the same table and column using different names. And in recent versions of MySQL, especially in MySQL version 3.2.2 and later you can actually provide an SQL query across different databases itself. So where, what is a database here? Databases essentially is a set of tables that are stored under one heading saying this is the student database, this is an employee database and this is the some other database and so on. Here you can actually provide, you can actually give SQL queries that can span across different databases within a single query. 
There are also many features like scalability and performance issues and so on. MySQL has been tested on very large databases up to databases which have up to 60000 tables and 5 billion rows in the table. It can allow up to 64 indexes per table that is you can index up to 64 attributes in a table and up to 16 columns as part of an index. Essentially, 16 columns as part of the key that makes with the index. So your key can be up to 16 attributes long and it also supports prefix based indexing. What is a prefix based indexing? Essentially on data types like varchar which is a string or text, you might want to ask queries like show me all students whose name starts with some pra or whatever something like that. So you might want to search for similar strings or you might want to search for prefix match strings and so on. So such kinds of search can be efficiently supported using prefix based indexes rather than indexing on the complete string as such. 

MySQL also supports user privileges and password based authentication by users and also host based authentication. So if the same user or if an authenticated user logs in from a different host, here she would have go through a different set of authentication privileges. Here she may not be allowed access from different host and only from particular host as such. 

(Refer Slide Time: 17:29)

 

Now what kinds of standards thus MySQL comply to? Note that when we talk about standards, we don’t mean something that is set in time. When we say SQL standard, the standard called SQL itself has been evolving over time. They almost every several years or so new features are added to the SQL standard and which becomes the new version of the standard. In addition to this evolution in standards, MySQL or any other database system would itself be evolving that is when the DBMS is implemented there would be a need that is felt for some more features to be included and so on. So there is a different set of evolution that is happening as part of the database itself or as part of the DBMS itself. 

So there are two different evolutionary streams, one of the evolution of the standard and one of the evolution of the DBMS itself. And when we say it supports or it complies to a particular standard, we should be clear about which version of the DBMS supports which version of the standard because both are evolving in its own different trajectory. So MySQL actually has several different modes or it can operate in several different modes that can support several different standards in SQL.   

For example MySQL versions greater than 4.1 can apply different modes to different clients itself that is let’s say client one connects to MySQL and says I want to use the ANSI standard of SQL. And second client connects to the same MySQL server that is running and says I want to use the IBM db2 standard of SQL and not the ANSI standard SQL and so on. 

So the default mode of operation for a client can be set with this option, it is shown here (Refer Slide Time: 19:27) minus minus SQL mode equal to a given mode, when we are starting mysqld that is for the server rather not for the client. That is you can set up, this is the default standard or this is the default mode of operation for MySQL.  

And ANSI SQL can also be selected using the minus minus ANSI option, when you are starting up mysqld that is in the command line itself. And you can set the mode for any particular client at any time using the set command that is set SQL mode equal to some particular mode. So what are the different modes that it supports? The ANSI mode for example where set mode equal to ANSI. So when mode is set to ANSI, there are certain implications in the way MySQL treats given SQL query. For example a real suppose a attribute is named as real then it is by default treat it as double, that is double precision real number by MySQL but in ANSI standard SQL it is treated as a floating point number, the single precision floating point number.
 
(Refer Slide Time: 22:00)

 
And similarly you can use double quotes for identifiers rather than strings. That is when you are identifying column name, you can actually use double quotes rather than when you are representing the string data which is part of a column. Similarly this double pipe operator that is shown here is treated as a concatenation operator rather than the logical or operator, the two vertical lines that you see here (Refer Slide Time: 21:14). And when I give a function let us say average, it ignores spaces between a function and the first parenthesis. Suppose this is the function here, now it ignores all the spaces between the function and the first parenthesis here.

(Refer Slide Time: 23:17)

 

Now what is the implication of this? the implication of this is that I cannot use the same function name as part of a let us say a column name or a variable name or something like that because this would clash with the function name because there is no way to, there is no way for the SQL server to distinguish whether this is a function call or a variable name or anything or a column name during parsing. So by default MySQL does not ignore this thing that is you should not have a space between a function name and the first parentheses so that function names can be repeated or function names need not be unique that is you can use the same function name for your column names or variable names. 

Similarly there are other SQL modes for example you can say mode equal to db2 or mode equal to oracle which supports some kinds of parsing, which has some kinds of parsing implications that are compatible with the way IBM db2 works or the way oracle database work. For example both of this modes have more or less the same implication for example you have to treat double pipes as a concatenation operator rather than a logical or operator and allow double quotes for identifier quotes and ignore space between function name and parentheses that is there is you cannot use the same names between the function and a column name. 

There also other SQL modes, we shall not go into the details here but let us just enumerate what are all the different SQL modes.

(Refer Slide Time: 23:31)

 

You can set mode equal to MAXDB, MAXDB is the variant of MySQL which is primarily oriented towards enterprise application. So it’s a very, it’s a transactional, it supports mainly transactional semantics and you can build large enterprise applications around the MAXDB. And then you can set mode equal to MSSQL which is Microsoft SQL or postgres SQL and MySQL different version like 3.2.3 or MySQL 40.0 and so on. There are other SQL extensions that MySQL supports which are different from the ANSI standard. These include the following that are shown here (Refer Slide Time: 24:12).

For example you can insert a comment c like comment, c like comment means one which starts with slash star and ends with star slash. You can include a comment within a SQL query, anywhere within a SQL query. Similarly you can perform some kind of macro operations or pre-processing operations from which you can bring about selective execution of an SQL query depending on which version of MySQL server this query is being executed on. For example look at this statement here, it says select and within comments it says exclamation mark or bank 3 2 3 distinct name from employee. Now what it means is that if my SQL server or MySQL server which is executing this query is of version 3.2.3, then you execute whatever is there within this comment otherwise comment out this, that is remove this commented part from the query. 







(Refer Slide Time: 24:52)

 

Therefore if I am running it on let us say MySQL server 4 then the outcome would be select name from employee and only in 3.2.3 it would say select distinct name from employee. Now why would one want to do something like that because there are certain kinds of nuances or certain kinds of changes in the SQL semantics between different versions of MySQL and you can write a single SQL query that can work on any version of MySQL by selectively masking or enabling certain parts of the query.  

Let us look at how data is organized on the disk in MySQL. MySQL does not create its own file system, in fact it uses the file system whatever file system is provided by the operating system. And each database is stored in a separate directory in an existing file system. What are the implications of doing this, that is what are the implications of using an existing file system and the fact that you are using, you are keeping one directory per database.















(Refer Slide Time: 26:32)

 

Some of the implications are shown here that is the names of tables can be either case sensitive or case insensitive depending on the operating system. For example if you are using a Unix based system or a Linux based system, it would be case sensitive. So if you say name with n as capital, this would be different from name with all small letters, so they would be two different tables having these two different names. However on an operating system like windows it is case insensitive, where both of them would point to the same table. And tables can be renamed and dropped using operating system commands not necessarily with in MySQL.

For example you can just say move or rename a given file and then the table actually gets renamed. so as shown here to rename a particular table, you just have to rename files with the following extensions .MYD, .MYI and .frm. For example if you have a table called employee you will see that and in a database called employees database, you will see that under the directory called employees database you will have employee.MYD, employee.MYI and employee.frm.

now suppose you want to change employee to something else say contractor whatever, you just have to change or you just have to move these files to some other files names using operating system commands and this change will be automatically reflected within the database itself. What kinds of storage engines does MySQL use and what is the storage engine in the first place.







(Refer Slide Time: 28:39)

 

A storage engine essentially is the engine that determines how tables are organized within files and what is the updation policy and so on.

(Refer Slide Time: 29:33)

 

And storage engines also differ with respect to their support for transaction semantics that is does it support atomic updates, does it support isolated updates, does it support recovery and so on and so forth. The different storage engines that MySQL supports, the primary storage engine which many of the earlier versions of MySQL where shift with was the ISAM storage engine. As we saw earlier it is called the MyISAM storage engine. ISAM essentially stands for indexed sequential access mode and provides indexed sequential access and does not provide any transactional support. There are also other kinds of storage engines like the heap storage engine which is used for managing tables that are in memory that is temporary tables or virtual tables and so on. This is also called the memory storage engine. 

There is an other kind of storage engine called merge storage engine which can treat several ISAM tables as a single merge table that is it can efficiently merge different tables that have been created using the MyISAM storage engine. And in recent versions of MySQL there are storage engines like InnoDB, BDB which provides support for transaction safe updates that means it provide support for acid semantics. They are available in versions only later than 3.23.34a. So to be very safe and also you should also be aware of the fact that in these versions or if this is the first version in which this storage engine was released then it would be available only in source code form and not in pre compile binaries.

(Refer Slide Time: 30:45)

 

Therefore if you want to use transaction semantics on MySQL, it would make a lot of sense to download a later version something like MySQL 4 or MySQL 5 which is the latest as of today. And the BDB storage engine is not supported on all operating platforms and so you have to ensure that for whatever platform that you are using, the BDB storage engine is actually supported. 

There is also the NDB clusters storage engine where you can implement MySQL over a computational cluster. What is a computational cluster? It is a collection of different machines on a local area network which can provide what is called as the single system interface. That is which can provide the combined power or processing power of all the processors of all the machines. 

Now NDB clusters is a storage engine that exploits this capability of a cluster, so it can support tables that are actually physically partitioned over different machines in a cluster. And of course NDB cluster is also very recent storage engine that is supported by MySQL and it is available in versions 4.1.2 and later on. And like we had noted above that in 4.1.2 it’s only available in source form that is you have to compile this engine yourself but in later versions, the binary forms or pre compiled versions are also available.

(Refer Slide Time: 32:34)

 

What about transaction semantics in MySQL? Transaction semantics have been supported in MySQL only after version 3.23 max and above and so on but before that MySQL actually supported what are called as atomic updates. We will look into atomic updates shortly and what are the relative advantages and disadvantages of atomic updates versus acid semantics and so on. But if you want specific acid semantics, suppose you are using your MySQL engine for let us say performing some debit credit updates in any kind of whether banking or any kind of money transaction that is going on which has to be recorded on this. 

It’s better to use MySQL which supports the Inno DB or the BDB storage engines because they have support for full transaction semantics. And other non-transactional storage engines like the earlier versions of MySQL and so on support what are called as atomic updates or atomic operations. And this can be simulated in a Inno DB for example by setting auto commit equal to 1 that is commit is automatic and it’s not explicitly stated in the SQL query.

These non-transactional atomic updates are actually faster than the transactional storage engines and because of course you don’t have to worry about locking and isolation and other isolation based semantics nor do you have to worry about logging and so on where the first was locking and the second is logging and logging in order to support log based recovery mechanisms that is in order to be able to support durability aspect of acid semantics.

Now because all of this are not required and only atomic updates are required, it’s much faster. and in many cases this is all that is required that is suppose you know that there are no concurrent operation that are happening or suppose you are relatively sure of the uptime of your DBMS then it makes much more sense to use non transactional storage engines which support only atomic updates because they are many times faster than acid, I mean storage engines which support acid updates. And a very interesting feature of MySQL is that you can use storage engines on a per table basis. So you are actually save only for this table use Inno DB and for every other tables use let us say MyISAM and so on. So that you use atomic updates and only for the critical table, you say that I want full acid semantics. 

The next topic that we will see are stored procedures. What are stored procedures? stored procedures is essentially a set of SQL statements that can be actually be stored on the server and some kinds of SQL statements which are used pretty often in different queries. So you don’t have to keep writing it as part of the query again and again. And you can actually refer to these stored procedures as part of a given query. There are two kinds of, two ways in which you can store SQL statements as what are called as stored procedures and stored functions. A procedure essentially performs a set of operations and may or may not return a value or may even return multiple values when it is called.

(Refer Slide Time: 36:11)

 

But a function has a specific semantics that is it can be embedded within any SQL statement and it returns back exactly one value as part of its execution. So stored procedures support is a very recent phenomenon in MySQL and it is supported in MySQL versions 5.0 and above and the latest version is actually as of now is actually 5.0 and whatever later versions would still support stored procedures.
And a stored procedures as you know increases the performance in terms of communication complexity, especially if you are using a large number of stored procedures in your query, you can save on a large amount of communication complexity between the client and the server. so you don’t have to send all the query from the client to the server for execution but the flip side of stored procedure is that if several different clients are connecting to the server and calling several different stored procedures then the sever gets a huge load in trying to execute all this different stored procedures. 
 
And MySQL also allows libraries or a function to be used as a stored procedure within the server. That is these libraries need not actually be SQL statements but they could be pre compiled libraries written in some other application specific language like C or C plus plus. And stored procedures are created using the CREATE PROCEDURE   construct in SQL and functions can be created using the CREATE FUNCTION constructs and procedures are invoked using the CALL constructs, you can say CALL procedure 1 or procedure 2 and so on, while functions can be embedded within any SQL statement like we saw select average of whatever or select anything and so on. 

(Refer Slide Time: 37:41)

 

So you can actually embed function within an SQL statement, however the function has to return one specific value. So functions return a value while procedures need not, it’s not actually do not but it need not return a value. In fact a procedure can actually return multiple values through the input parameters itself and a function is distinguished from a procedure by this RETURNS statement or the RETURNS clause. So function says returns this thing, returns a particular value when you are declaring a stored function. And a stored procedure or a function is associated with the specific database. What are the implications of saying that a stored procedure is associated with a specific database?

See you can say for example I have an employee database and there is a particular kind of query that is invoked on this database for example what is the average, what is the average working hours per week in this month and so on. Now you can actually store that as a stored procedure in the employee’s database. Now let us say you are using some other database, let us say salaries database or some other completely different database itself not some other table but you can still use or invoke a stored procedure on the employee’s database. Note that in SQL, in MySQL now you can actually give an SQL statement that spans over multiple databases.

(Refer Slide Time: 38:54)

 

So when you invoke a stored procedure on a given database and implicit command called USE database is called. USE database is basically something like connect to the database and start using tables in this database.


















(Refer Slide Time: 41:31)

 

And this USE statement is terminated when the procedure terminates and the procedure now has access to all tables of the database within which it is written. And of course you can call stored procedure of a specific database by prepending its name with a database name. For example here you say CALL EmployeeRecords.UpdateSalary where UpdateSalary is a procedure that performs updates on salary and employee records is the database within which this stored procedure is stored. And another important implication of the fact that stored procedures are associated with the database is that when a database is dropped or it is deleted then all stored procedures and functions that are associated with it are also dropped.

(Refer Slide Time: 41:37)

 
Support for triggers. Triggers, as you know we covered a session on constraints and triggers. Triggers as you know are essentially special kind of stored procedures that is they are written using ECA rules as we saw in one of the previous sessions. That is they are triggered automatically when an event happens and a given conditions is true. Now support for triggers is not very comprehensive in MySQL as of now at least. That is there is only preliminary support for triggers that is available in MySQL version 5.0. And triggers are essentially stored procedures and which are actually called automatically in response to certain events. 

MySQL also supports views, as we saw view can be created either as a virtual table or as the materialized view. That is view can be stored as a query or can be actually physically materialized depending on what kind of application that you are using. In transactional applications views are usually virtual tables whereas let us say in data warehousing and other analytical applications, views are actually materialized. MySQL supports views as virtual tables and they are supported from MySQL version 5.0 and above.

(Refer Slide Time: 42:31)

 

And they are treated as real tables for all practical purposes that means each table has what is called as an access privilege system that is which user has what privileges for a particular table. For example you can say this user has a read privilege but no write privilege or this user has a read privilege write privilege but no drop privilege that is you can read and write to the table but he cannot drop the table, he cannot delete the table and so on.

And view updation is automatic because only virtual views or virtual tables are supported. And views are created using CREATE VIEW command or there is also a variant of this which says CREATE or REPLACE VIEW. So if the OR REPLACE construct is present then a view would be updated if another view of the same name exists. And how does MySQL handle constraints and of course triggers and so on? 
In MySQL the notion of triggers or calling stored procedures is separate from that of handling constraints. The kinds of constraints for example data specific constraints like say primary key constraint or unique constraint, if they say violation on the primary key constraint essentially that primary key has to be unique and it can’t be null.

(Refer Slide Time: 44:20)

 

So if there is a violation then MySQL will roll back, if a transactional storage engine is used that is it just performs the transaction rollback and if a non-transactional engine is used then it just stops execution on the offending row that is it does update the row and from there on it does not update any other row in the engine. So therefore it is not an atomic operation that is it could have updated some rows prior to it but it will stop execution at that point in a non-transactional storage engine. And many of this insert or update queries also support a keyword called ignore essentially that means that constraint violations are ignored. 

Similarly if there are violations in NOT NULL constructs and say default constructs, MySQL automatically inserts the default value. for example if you say that some attribute should be not null and the default value is say zero then whenever that attribute value is null then zero is inserted automatically. Now the reason that MySQL does not stop in this case is that you will be able to catch such violations only after the query parsing stage has been finished. And it would make it quite in efficient to stop execution at this point and get back to the user for a different query. So MySQL automatically, because it already has a default value it automatically inserts the default value.

And similarly for queries that try to insert invalid values, for example if it tries to insert  string into a numerical field or some kind of a numerical overflow happens during insertion and so on. MySQL automatically inserts the best value that is zero for numbers or null string for strings and so on. 

(Refer Slide Time: 45:32)

 

And similarly in an enumerated data type, what is an enumerated data type where you say this data type can or this column can take only these set of values. So one can say the column called gender can only take m or f or column called grade let us say, can only take values a b c or d and so on. So if an invalid value is entered in to an enumerated type then it is automatically replaced by the first element in the enumerated type that is the element number zero. Let us see now how you can actually download and install MySQL on a Linux based environment and get it running. 

Now of course in order to download MySQL, we already saw where to visit in the internet www.mysql.org and from that you can actually download MySQL on to your machine. Now there are several different sources and binary versions of MySQL that are available and the pre compiled binaries that are available for different linux distribution. So there are pre compiled binaries for say SUSE Linux or Mandrax Linux or Red Hat Linux and DBAN and so on so forth.














(Refer Slide Time: 48:15)

 

The easiest way of installation is to use an appropriate binary RPM package. RPM essentially is the Red Hat package manager which is the default way in which the packages are managed across different Linux distributions and binary versions of MySQL are complied with static option which uses static libraries. So it’s quite stable that is there is very little possibility or no possibility of linking violations or run time error occurring after the MySQL server has started running.

(Refer Slide Time: 48:51)

 

So when you install MySQL on your system, you need to install at least MySQL server and MySQL client packages in order to get a complete functional DBMS on your system. That is you should have a both server part and a client part. You should also install this package called MySQL shared compat for backward compatibility especially if you are upgrading to a newer version of MySQL. MySQL servers are by default installed in to the in to this directory var lib MySQL, so after you install you can actually visit the directory and see what all it is installed and a new user called MySQL is also created and which is the owner of the MySQL demon or the MySQL server that is running and it’s also possible to make MySQL server start up automatically whenever you boot up your machine.

(Refer Slide Time: 50:03)

 

Let us see how we can do all of these things. In order to install MySQL on your machine, you have to use this rpm minus i command or the MySQL or the install command which installs the server and the client and of the appropriate version number and after installation of MySQL it is necessary to allocate certain areas where databases can be created. And this is automatically perform by the MySQL install db program and in some kinds of installation procedures, this is, this program is automatically called after installation. So you don’t have to even worry about calling this program. And MySQL install db creates a directory structure under which databases are created where each database as you know is a directory in itself.










(Refer Slide Time: 50:40)

 

And by default it creates two databases MySQL and test. The MySQL database is the database of databases that is it’s a database holding information about other databases in this installation and the test database is a sample database that is provided to the user. So let us look at how MySQL database looks like. Once you start let us say, once you have run your MySQL install db there is something called MySQL show which will show the contents of a given database.

(Refer Slide Time: 51:16)

 

So when you say MySQL show MySQL, you see that MySQL database has several different tables columns underscore prev db func host tables underscore priv user and so on. So, which holds different information’s about all the databases that are stored on this machine. For example this holds user information which are all the valid users and their passwords and their privileges and so on and so forth. And tables privileges, that is per table privilege, this table is authorized to which user for doing what and so on. Then there are host based privileges and functions that are stored as part of this database and db is information about the other databases that are stored on this machine and this is a column based privilege information.

(Refer Slide Time: 52:46)

 

And you can, like we had noted earlier you can make MySQL to start and stop automatically or start automatically on boot up, whenever you boot up your machine. For that you need to go to /etc/rc.star where star is either dot 3 or dot 5 or whatever in which you start your computer by default and those files should be changed in order to start up MySQL server by default. And the MySQL server can be stared at any time using this command MySQL.server start and it can be stopped at any time from the shell by MySQL.server and then stop.













(Refer Slide Time: 53:22)

 

There are other options for MySQL server that can be added in a global configuration file and this is available in /etc/my.cnf and this is the typical global configuration file which is saying which is the database directory, which is the socket under which MySQL is connected, which is the port on which the MySQL is listening and who is the user owning the MySQL and so on.

(Refer Slide Time: 53:41)

 

And the MySQL client is the client program using which you connect on to the MySQL server. So MySQL client can be started by MySQL minus h host minus u user that is connect to the server on this host using this user id and then it ask for a password and then you get in to the MySQL prompt.

(Refer Slide Time: 54:17)

 

And SQL commands can be issued from the MySQL prompt and it works within a user session that is you can say use particular database which is the same as connect to a database in several other DBMS system. And then you can start issuing SQL commands and you can quit the client using the quit command.

(Refer Slide Time: 54:35)

 
In addition to SQL using the MySQL client itself, MySQL provides support for several application programming interfaces that is you can embed your MySQL client within another application program like C, C plus plus, perl, java and so on and so forth. 

Let us have a brief look at what kinds of C APIs that does MySQL provides. In order to use the C API in your C program, you have to include the MySQL client library and once you include the MySQL client library there are several data structures that are available to your program.

(Refer Slide Time: 54:53)

 

For example there is a data structure called MySQL which is a structure representing, which is the struct, structure is essentially the struct data type in C that represents a handle to a db connection or to a database connection that you have presently opened.

Similarly MySQL underscore RES represents the result of a query and so on and so forth. So there are several different such data structures which you can access. Similarly there are several different functions that you can also use as part of your program. For example you can say MySQL underscore init which gets or initializes the MySQL structure that is it obtains the handle to a data base connection.  

Similarly you can say MySQL real connect which actually connects to the server and you can issue a query using MySQL underscore query and once you get the query results, you can say MySQL fetch row which fetches the results of this query in a row by row fashion, so the first row and the next row and so on so forth.





(Refer Slide Time: 55:47)
	
 

So using this you can actually embed your SQL semantics or database semantics in to your larger application program itself. 

(Refer Slide Time: 56:41)

 

So let us summarize what we have learnt about MySQL today. So it’s a very interesting database, it’s a very popular open source DBMS that is the source code is available to you and as a result MySQL has or changes to MySQL has been contributed by several people across the world.

In addition to the MySQL ab people, who are in MySQL ab and it provides several different storage structures and its scalable to number of clients and data sizes and it also has transaction support and limited support for triggers and stored procedures. And best of all, there are several different, it has been ported on several different platforms you have MySQL for window, Mac and Linux and so on. And it has several different APIs for application programming or embedding application programs in to MySQL. So that brings us to the end of this session.

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 33

Case Study
ORACLE & Microsoft Access

Hello and welcome. In this lecture today let us look at one more case study of a commercial DBMS this time.

(Refer Slide Time: 01:19)

 

In fact we are going to look at case studies, the well-known oracle DBMS and the Microsoft Access DBMS which is shipped with Microsoft office suite of a product whenever you buy them. And in the previous session, we talked about MySQL which is an open source or free software while the once that we are talking about or that we are reviewing here are both commercial software that are proprietary and you have to buy it off and which can be bought off the self and so on.










(Refer Slide Time: 02:06)

 

Now let us have a look at the main structures of this, firstly the oracle DBMS. In this lecture we are not looking at any specific version of oracle. Oracle is, the DBMS oracle is manufactured by Oracle Corporation headed by of course Larry Wall and who has been a pretty early player in the whole field of commercial database management systems. And there have been several versions of oracle that have been evolved over the years and the latest version at least as of today is oracle 10g, the g here stands for data grids where the database can sense that it is part of a larger data grid and operate accordingly. That is a grid is a collection of different databases which could be distributed widely probably across the world and while functioning as a single data source or a database. And it’s been touted or it’s been kind of targeted towards the data warehousing and business intelligence kinds of applications. 

What are the basic elements of or the building blocks of an oracle database? When we are talking about oracle databases, there are some terminologies that we need to be aware of. Many of these terminologies as we noted earlier, oracle has been an early player in the field of commercial DBMS systems and while terminologies have been invented and evolved over time in the academic and research community for database management systems. Similar kinds of terminologies and evolutions have happened in the commercial field as well.









(Refer Slide Time: 03:18)

 

So sometimes it may so happen that the terminology used, that we have used for example in this class might differ from the terminology used in oracle or let’s say db 2 or some other commercial database engine. And so it makes a lot of sense to clear what each of these terminology mean or what kinds of terminology mismatches can we have between, what we have been using and what oracle for example uses. When we talk about an oracle server in the database management system, the oracle server is actually the DBMS server that we know very well. That is it’s the DBMS engine that serves one or more clients. And when you talk about the oracle database, it is more than just the set of data files that form the database but oracle considers its database as a collection of all its stored data including log files and control files and other such auxiliary files that make up the database. 

And usually in the academic community when you say database, we usually talk about only the data files and the index files, when we talk about for example the storage structure. Then when we say an oracle instance, this refers to the set of processes that is the set of all processes including oracle system processes which with oracle server runs and user processes created for a specific instance of the database operation. What kinds of languages does oracle support? Oracle of course supports the ANSI SQL standard with some extensions of itself that was specific to oracle.









(Refer Slide Time: 05:43)

 

And in addition to the well-known ANSI SQL, oracle has its own query language called PL/SQL. PL/SQL is a procedural query language, on top of SQL that is say for example SQL language is in some sense a pure query language in the sense that it does not have other programmatic constructs like say using variables or well it has certain, you can use certain kinds of variables by aliasing and so on. But generally it doesn’t have the generic structure of any procedural language like C or Perl or some other kinds of language. 

PL/SQL on the other hand is a complete procedural language where you can actually define variables, you can define control flow, if then else constructs and looping constructs and so on and so forth and embed SQL statement within these procedural constructs. When we talk about the database structure in oracle, we usually refer to two different kinds of structures what are called as the physical structure and the logical structure of the DBMS. The physical structure refers to the storage structure that is a physical organization of the data on the database. While the logical structure corresponds to what is a conceptual schema of the database that oracle is maintaining, I mean logical structure here is the logical structure of the way oracle manages data not that of a particular database itself. 











(Refer Slide Time: 07:13)

 

So the way oracle manages data itself can be treated as a database which comprises of several other databases and which comprises of a larger logical structure and update rules and associations and so on and so forth. 

Let us look at the physical structure in a little bit more detail. We shall also briefly look at the logical structure as well. When it comes to the physical structure of the database, the way oracle stores data on disk, it basically stores data in several different files on disk and very unlike let’s say MySQL oracle uses its own buffer management policies for writing data into files. What is meant by buffer management policies for writing data?   DBMS like MySQL uses for example, if you are taking a Unix based implementation or Linux based implementation of and we are comparing MySQL and oracle, MySQL uses what are called as high level system calls for writing data into disk. That means high level system call essentially says, gives a block of data to the operating system kernel and lets the kernel take care of writing the data on to the disk by itself. 

Now what the kernel actually does is in order to speed up the process, it doesn’t immediately write your data block onto disk instead it keeps it in its own memory, in its own cache that is in main memory and then flushes it on to the disk at some point in time. This cache is called the buffer cache. We saw buffer cache and how buffer caches are come into play or affect DBMS operations when we are talking about database recovery processes.







(Refer Slide Time: 08:08)

 

So when we write something onto disk and we use a high level system call, there is no guarantee that whatever you have written on to, whatever you have written has actually been recorded on to disk. Now if there is suddenly a system failure then even though our write has returned us a successful operation and we said we have committed some transaction, it may not actually have been reflected on to disk.

On the other hand oracle uses low level system calls that means oracle manages its own buffer cache and oracle decides for itself when it has to flush the buffer cache and when it has to write to files and so on. However it still uses files that are visible from the file system interface of the operating system. So let us look at the files that oracle uses. So when we are looking at the files here that oracle uses, we have to keep in mind that the way that files are updated in oracle is very different from the way that files are updated in say MySQL. So these are some of the files (Refer Slide Time: 10:45) that oracle uses for maintaining its databases. 

Firstly the data files of course. The data files are the files that contain the actual data and in addition to the data files, a database is associated with a set of redo log files. Remember what is a redo log file, redo log files are the set of log files that are used for redo based recovery. We looked at a set of log based recovery mechanisms where you first write data on to the log file and then after flushing the log file, you write data on to the actual database.

Now whenever there is a system crash, you just have to redo based starting from the last check point in your log file all the operations that have been performed until the system crash happens. So oracle does something like that, so it maintains the set of redo log files as part of database itself. Then there are a set of control files which contain different kinds of control information like the database name or the different file names and the locations of this different files and so on. These control files are also used in the process of recovery, sometimes they are useful in the process of recovery and in addition to control files there are other files like what are called as trace files and alert logs. Trace files are essentially files that track certain background processes that oracle runs.

Oracle basically runs several different background processes that for example, the server is a background processes and there are several system monitors that, oracle runs to monitor events and call triggers or enable triggers and so on and so forth. Now trace files essentially track this background files and logs them on to files and alert logs maintains a log of the major events that have happened from these background processes and from anywhere else in the system. The log files and control files as you can see are the files that are essential for the process of database recovery. Therefore oracle also supports mechanism by which log files and control files can be multiplexed onto different devices.

(Refer Slide Time: 13:15)

 

So when you write a log file, you can configure oracle so that it actually writes it in parallel to multiple devices. The advantages of this is even if there is, in a system crash even if there is let us say a media failure and the log file itself is lost, we can use these backup copies of the log files from actual devices, from different devices to recover back. So the resilience of the recovery process is increased as part of this.

So let us look at the logical database organization now. That is how is database organized in oracle, what are the building blocks that make up a database in a logical sense and   what are the different roles of these different building blocks and so on.  

When we are talking about an oracle database, in the physical sense there are only files, everything is a file and file is managed by low level system call operation. But from a logical sense, each of these files means certain different things and they are looked at as objects rather than files. Now what are the different kinds of objects that an oracle database contains, there are what are called as schema objects. 
A schema object contains definitions of some relevant entity in the database. For example it might contain definitions of tables, it can contain definitions of views or sequences or stored procedures, indexes, clusters or links to other database and so on.

(Refer Slide Time: 14:09)

 

So everything is treated as an object here and the object is essentially in a sense serialized and stored on to disk. So an object for using the object orientation methodology comes with its own set of procedures or methods which handles or which provides a set of services based on the objects and definitions. So when we say a schema object represents a table, it also provides methods by which we can access or perform different kinds of table accesses, access a table row wise, access or perform any perform any index search on table and so on. And in addition to the schema object, there is what is called as a data dictionary which is essentially the system catalog and it maintains different kinds of cataloging information for oracle.

So it contains information like user names and security information that is access privileges and what kinds of users or what kinds of privileges on which schema object and information about schema objects themselves, the modification date and the creation date and any other relevant information for any given schema object. Then integrity constraints which in turn trigger all the stored procedures or which in turn trigger certain schema objects which are stored procedures and then certain kinds of statistics essentially for example that help in performing good query execution plans. We saw the role of cataloging information when we were looking into how do we manage or how do we process a query and optimize a query. 

Cataloging information contain certain, I mean the statistics here in the cataloging information contain certain estimates on let us say how many rows are there in a table, how many number of distinct values of are there for a particular column and which rows is accessed how many number of times or whatever.
All of these information go into performing a or forming a good query execution plan while making the query. And then there are audit trails that is auditing information about the different aspects of the database.

(Refer Slide Time: 17:46)

 

The next aspect of the logical schema or logical organization of the oracle databases is what is termed as the table space. A table space is again an important concept in oracle which basically describes the physical storage structures of a given set of tables in a database and it basically governs how the physical space of a given database is used. We look into how the tables space is organized, they basically contain different segments and which in turn manages the pages or the disk blocks or that make up a particular database. 

So earlier we mentioned the concept of an oracle instance. An oracle instance like we said has a specific definition that is in some sense it is a snapshot of the entire oracle system. That is it is the set comprising of all processes that is oracle processes and user processes and all of which comprises one instance of a servers operation.













(Refer Slide Time: 18:41)

 

Now an instance itself can be logically seen as comprising of several different parts. So an instance is logically divided into what is called as a system global area. What is the system global area? The system global area is essentially the area which in some sense to give an analogy to operating systems, it is the memory area that is used by a kernel for example in contrast to that of areas used by user processes. That is all the resources and memory locations that are used by the kernel is a system wide and globally relevant. 

So the system global area is in some sense the kernel of the oracle system. It contains the database buffer cache which we saw is maintained by oracle itself in order to flush buffers on to disk and in order to basically control when buffers are flushed on to disk. Then there is the redo log buffer which is the buffer cache for the redo log. So whenever a redo log is returned, it is actually first return into the redo log buffer and which in turn is flushed on to the disk at periodic intervals. Then there is shared pool of other resources which we will see. Then in addition to the system global area, when we talk about an oracle instance we also talk about the user processes. 

An oracle instance again is like I said is something like a snapshot of the entire system. So you might also take an analogy to operating systems where we say an instance of the operating system is the set of all processes comprising of the kernel processes and all user processes at any given instance of time. In addition to user processes there is what is called as a program global area in contrast to the system global area. 







(Refer Slide Time: 21:23)

 

A program global area is a memory buffer that contains a data and control information for server processes. That is where, this is the global area again there is no specific analogy as such to operating systems here but you can think of it as a global area which is code specific that is for the  data and control information for the global processes that are happening in the database. In addition there are certain system processes itself which are the oracle processes. So oracle itself runs several processes in the background which comprises the server instance or the oracle instance at any given point in time.

So oracle processes may comprise of the server process itself or the server process is depending upon whether it is single threaded server or its concurrent server where it can actually serve several different clients concurrently. And then there are other background processes like audit trails and system monitors and so on which belong to oracle. So here is a schematic diagram of the system global area. Of course this of an oracle instance rather and of course this diagram is in some sense leaves out a lot of things that is for example the program global area is not explicitly shown here and the system processes is not explicitly shown but generally all of this form the oracle instance. 












(Refer Slide Time: 22:54)

 

So the system global area which comprises of the buffer cache and the redo log buffer they interact directly with the database files and user processes and of course the server processes they interact with the database actually through the SGR or through this system global area. So there are user processes which in turn which interact with the oracle server processes and oracle processes which could be server processes and possibly other processes. And these in turn rather than interacting directly with the database file, to be more correct this diagram should be, the arrow here should come through this thing. That is it interacts with the database or with a disk through the system global area.

On the other hand if we are talking about database files, this logical arrow is correct which says that logically the server processes deal with the database files. So let us look at oracle processes again in little bit more detail. Oracle processes can be categorized into different forms namely server processes. Server processes are those which actually handle request from user processes or oracle client or other application programs and so on which send SQL queries to the server processes.













(Refer Slide Time: 24:24) 

 

There are two kinds of ways in which you can configure your oracle server, you can either configure it as a dedicated server or as a multithreaded server. And dedicated server essentially is a server that is dedicated to a particular client, so it can take handle one client at a time whereas a multithreaded server is concurrent in the sense that it can handle many client connections at the same time by spawning different threads or processes if your kernel doesn’t support threads and so on.

In addition to server processes there are what are called as background processes. And there are different functionalities for each of these background processes and they are created for each instance of oracle and one of the main functionalities of these background processes are to perform these asynchronous I/O. That is there this process is that mediate between the buffer cache in the system global area and the physical disk. So these processes essentially perform what are called as read ahead or delayed write operation. So when you write on to a disk, your data goes into the buffer cache and in some point in time the background processes wakes up and asynchronous late at some point in time the background processes wakes up and then flushes all the buffer cache onto disk. 

Similarly when you give a read command, your data is read the relevant data blocks are read from the disk but at the same time a background process is awakened which in turn reads several other data blocks also from the disk into the buffer cache, again for performance considerations. So the main functionality or the main requirement of the background process is to provide parallelism for better performance and reliability.





(Refer Slide Time: 27:25)

 

So there are different kinds of background processes which take on different roles that perform these asynchronous operations and usually it is good to distinguish between these kinds of background process. For example the database writer process, so this is the process that writes the buffer cache of the data blocks from the buffer cache to the data files on to the disk. Then there is the log writer background process which mediates between the log or the redo log buffer cache and the log files on the disk. 

So data processes are managed separately from the log flushing process and then there is the check point background process. A check point as we saw in sessions on recovery based techniques, this basically refers to a event in which all modified buffers are returned to the data files. That is any log data or any data about transactions that have been committed before the check points can now be safely discarded. So we saw what are check points. So this check points as we saw in the session on database recovery have to be taken at some points in time and there is a tradeoff between the speed or the overhead introduced by the check pointing process versus how much background data or how much historical data that you need to store in order to perform a recovery in the phase of failures. 

So this check point background process is a process that runs once in a while to perform this check pointing operation. And as we saw in our sessions on database recovery, check pointing is a costly operation because you need to bring the database onto a quiescent state when you are performing a check point. But there are other techniques by which, there are other check pointing techniques which can obviate this problem so that you don’t have to stop all database operation when taking a check point.




(Refer Slide Time: 28:55)

 

Then there are other process like the system monitor which performs recovery of an instance that is it identifies that when the system comes up, it identifies that the system had crashed and performs redo based log recovery then it manages storage areas and it also manages any kinds of recovers transactions that were skipped during the recovery process. Then there is the process monitor background process, it performs a recovery operations whenever a user process fails. 

Note that failure of a user process can also leave the database in an inconsistent state especially when it is in the midst of the transaction and especially even more when transaction data return directly onto disk. So if the user process fails to complete the transaction then the database is still in the inconsistent state and then you need to recover but then this is different from a system monitor in the sense that the entire system has not crashed, so other transactions are running, so this recovery process has to be handled separately. 

There is also a process called the archival process which archives the online log files on to archival storage especially, essentially for back up purposes. We also saw some mechanisms of archival when we are talking about the database recovery techniques. Then there are recoverer processes which essentially are useful when oracle is used in a distributed database setting, we have not as yet looked into issues pertaining to the distributed databases. But there are several issues when database is distributed across several different clusters especially to detect that a particular transaction that is spanning across different machines has actually crashed or it is spending or something of that sort.





(Refer Slide Time: 31:38)

 

Then there are other processes like dispatchers and lock processes where dispatchers in multithreaded configurations, they route request from user processes to the appropriate or the available server processes and then there are locking process which are in some sense again monitors that monitors the locks across different instances when oracle runs in parallel server mode.

(Refer Slide Time: 31:47)

 

Now let us look at SQL in oracle. What kinds of SQL statements are handled? As we noted earlier oracle supports ANSI SQL. In addition to ANSI SQL, it has its own extension to SQL namely the PL/SQL which helps the user to embed SQL statement into procedural language but as far as the support for SQL itself is concerned, oracle supports all kinds of standards data definition statements and data manipulation statements and it supports transactional semantics, full acid semantics then other kinds of constructs like session control semantics and even embedded SQL statements. 

In addition to support for SQL, oracle comes with an elaborate mechanism for methods and triggers and these utilities where added to oracle as part of its object relational extension. So oracle essentially is an object relational database where we have not again as yet seen in to what constitutes an object relational database but essentially a database that uses object oriented semantics like method calls, triggers and triggers are also some kinds of method calls and inheritances, associations and so on. So a method is actually a function that is part of the definition of a user defined data type and its slightly different from the stored procedure in the sense that program essentially that’s a user program invokes the method by referring to an object of its associated type.

(Refer Slide Time: 34:03)

 

But stored procedures are actually called from SQL statements independent of any particular object as such. It’s only the database whose context is necessary for stored procedures. And oracle methods have access to attributes of their associated objects and information about their types. So a method for example, an oracle method for example would be a method in one of the schema object. Let us say schema object represents a table. A method would be some code which says how do we access the next row in a table or how do we get the primary key field of a table and so on. So get primary key or get next row or query or something of that sort which abstracts away the implementation or the storage structure of the table from the database engine as well.

So this is different slightly different from the stored procedure concept because stored procedures are visible to user programs or SQL constructs while methods are essentially, I mean the methods are visible to user programs only in an object relational context and not in a pure relational context. And triggers are those methods which have active rule capability that is which are called automatically in response to events and conditions. 

Let us have some more deeper look into the storage organization of the oracle database. We noted earlier that storage is managed by what is called as the table space. Now table space is the space that manages the physical organization or physical allocation of memory or disk blocks for a given database.  

(Refer Slide Time: 35:33)

 

So a database is divided into several different table spaces and among these we can distinguish between what are called as system table space and the user table spaces. A system table space maintains essentially all data that are visible only to the oracle system while the user table space maintains data that are accessible by user programs. And data files are stored or managed by the table space itself. Every oracle database contains a unique system table space and this is labeled as SYSTEM in caps here. And the data dictionary objects and other system wide or information are stored in this system table space or managed by this system table space.

Now the physical storage itself I mean how does the table space manages the physical storage. The physical storage itself is divided into three different kinds of storage what are called as data block or and extents and segments. A data block corresponds to what is called as a page in or data page in several other database systems. It is a smallest level of granularity in which data is stored.




 

(Refer Slide Time: 37:22)

 

Data blocks usually have a one to one correspondence with a disk blocks or the granularity with which your disk accepts data. Most of the or at least all of the disks accept data in terms of one sector size of the disk but there are some disks with enhanced capability that can actually read and write multiple contiguous sectors at the same time.

Now this multiple contiguous sector form one data block. And then extent, the term extents is used to refer to a specific number of contiguous data blocks, so where data block is the minimum unit of storage. Now a set of segments that are allocated to a specific data structure like a table or an index or something like that is called as a segment. So data blocks again each of these, let’s look at each of these different elements in a little bit more detail. 

A data block whenever a particular data block is stored, it contains the following information. It contains the some header and table directory information which talks about which table it belongs to and pointers to other tables and so on. And there is a row directory information which talks about which are the specific rows that are stored in this data block and the data that pertains to a row and any free space information, if there is any free space information, if there is any free space left out in these data blocks.
 









(Refer Slide Time: 38:34)

 

And an extent as we said earlier is a set of contiguous data blocks. So when whenever you create a table, oracle allocates an extent to it and an extent is extended in a sense that is an extent is, more extents are added to the table as and when extents become full.   

(Refer Slide Time: 39:10)

 

And these extents are managed by segment that is we saw that a segment is a set of different extents. And extents can be allocated for any given data structure like tables or indexes and so on. 

So if I allocate an extent to an index, the extent remains allocated as long as the index exists and is automatically freed when the index is deleted. And a collection of extent is what is called as the segments. So there are different kinds of segments, what might be termed as data segments and index segments. And data segments are those which store data or which handle elements that are stored in the data files.

(Refer Slide Time: 40:04)

 

And data segments belongs to each what is called as a non-cluster table and to each cluster. What is a non-cluster table? As we noted earlier, a cluster is a collection of machines on a lan which gives us the single system interface. Now a cluster table is something that is spread across the cluster that is a part of my table is in one machine and part of it is in another machine and so on and so forth. 

Now for every such part in a given machine, a given data segment is allocated. And index segments are allocated for every index structures that’s been created on these tables, so every create index command will allocate a index segment. There are also other segments called as temporary segments which are used as temporary work areas especially to store intermediate results or to allocate data for virtual tables or results of an intermediate query and so on.










(Refer Slide Time: 41:17)

 

And then there are rollback segments that are used for undoing transactions. So like MySQL or any other DBMS, oracle servers are usually expose several different APIs, so that user processes or application programs can directly talk to the oracle DBMS.

(Refer Slide Time: 41:53)

 

So the sql support or embedded sql support is provided for different languages like COBOL, C, Pascal and so on and in addition oracle has its own procedural language called the PL/SQL.


(Refer Slide Time: 42:47)

 

Let us have a brief look at how PL/SQL looks like and what kinds of operations does it provide over and above plane vennila SQL. PL/SQL as we noted earlier provides procedural constructs within which SQL constructs can be embedded. A PL/SQL block can be divided into three different parts what are called as the declaration part, executable part and the exception part. As you know, if you notice correctly this is very similar to how a typical procedural language is also ordered except for this exception part and out of these three parts it’s only the executable part that is mandatory. 

So the declaration part is where you declare variables or and any other characteristics or functions that PL/SQL uses and the set of statements or the set of executions is embedded within a begin and a end construct here where you have the set of statements as well as the exception handlers. That is the executable part as well as the exception part written within this block that is within this begin and end construct.















(Refer Slide Time: 43:23)

 

So the declaration part like we noted is used to declare variables and this could be variables that are used in both SQL and PL/SQL data types. And the SQL and the executable part contain both SQL commands and PL/SQL constructs like if then else or for loop and while loop and so on.

(Refer Slide Time: 43:41)

 

So you can say if some condition then send this SQL statement, else send this other SQL statement and so on. And the exception part handles error conditions whenever an exception is raised and errors could be either system errors that are flagged by the database itself or they could be user defined errors that are flagged by the violation of for example some integrity constraint. 

(Refer Slide Time: 44:54)

 

So let us summarize what we have seen in oracle before having a brief look at MS Access as well. Oracle is a very popular commercial database management systems. It’s a commercial one and it’s in contrast to the MySQL which is an open source DBMS and the latest version 10g supports data grids as part of oracle. And oracle actually supports not just relational, it also supports object relational databases, spatial databases and database of sequences and so on. So different kinds of data can be stored in oracle rather than the pure relational database and it supports procedural constructs over SQL in the form of PL/SQL where you can write control flow constructs if then else and while loop and for loop and so on. 

And oracle supports transactions and log based recovery, it also supports clustering where you can have a data file that is clustered over or a table that is distributed over several machines in a cluster and several other features. Let us now briefly look into the second commercial database whose case study that we are seen which is the Microsoft Access. Microsoft Access we have we have included Microsoft Access as part of this case study mainly because it’s a commercial database which is kind of tailor made for ease of use for a non-technical user and where everything can be performed using graphical interfaces. So the set up time and the learning curve for this database is much smaller than any other commercial database like oracle or other database like DB 2 and even probably MySQL.





So Microsoft Access provides the database engine and a GUI based that’s the graphical user interface based mechanism for almost anything that is for data definition, data manipulation, queries and reporting and almost anything to do with a database can be performed using graphical user interfaces.

(Refer Slide Time: 47:18)

 

But it’s not that, it’s only through the GUI that MS Access can be accessed. You can also access the database using basic that is visual basic and several other macros that you can write to form of your own procedures and so on. And one feature of Ms Access is it provides hyperlinks that is url as a native data type which is generally not supported by other databases where we can treat a url as a data type by itself and not as a string. 

So what are the different components of the Microsoft Access database. It’s called the engine of the of the Microsoft Access database. All data for a given application are stored in a single file which has a suffix called dot mdb but even then, you can access these mdb files through different ways using the open database connectivity mechanisms. We have not talked about ODBC as yet but generally ODBC is essentially is a common interface by which you can access several different databases as though they were data sources using a single common interface. And MS Access provides the support for data validation and concurrency control using logs but not full acid semantics and it also provides some amount of query optimization. What kinds of GUIs are available? We have some screen shorts which we will also see of these GUIs later on.







(Refer Slide Time: 47:47)

 

There are GUIs that are available for specifying the structure of a table, formatting of the of the field layout, any kinds of masks which talks about invalid inputs and validation rules, default values, data types, index structures and anything to do with database design can be specified using graphical user interfaces.

(Refer Slide Time: 49:07)

 

You can also use GUIs for defining foreign key relationships and you can also enable what is called as an automatic inference of relationships that is whenever we use the primary key field of one table as an attribute of another table, there is automatically MS Access would infer that there is a foreign key relationship.
(Refer Slide Time: 49:43)

 

So it also provides this facility of an automatic inference of relationships between different fields. Then queries can be graphically posed through a what is called as a query by example interface that is you can formulate or you can graphically show how your query should look like, that is I should have these tables and these tables and these tables with these associations and so on as part of my query result and then say now give me the query.

(Refer Slide Time: 50:19)

 

So there is a QBE interface or query by example interface and as and when you build your query as part of the QBE interface, programmatically there is an SQL statement that is being created to which also you can switch to and change your query whenever you can. And you can perform joins by drag and drop operations between tables and you can form addition or and deletion of attributes to tables using drag and drop. Then there are what are called as expression builders where in which you can specify that the constructs of the WHERE clause of your SQL query. 

(Refer Slide Time: 51:50)

 

Reports generation again can be performed in a GUI fashion that is it’s an integral part of the access database. The access database doesn’t just return a query and leave it at that, you can actually specify how the return data should look like in the form of a report, let us say in a page where different elements of your query go into different parts of the page. So it becomes a reporting mechanism. And the reporting, whatever reporting mechanisms that you have created are tightly bound to the underlying database tables and queries. So there are different report generation wizards which help you in showing how a database report can be created and different styles in which report can be created. 

So many other additional features what are called as cross tab queries which wherein you can perform group by on specific values within a column and aggregating within the group that you perform the group by on and the tables that you have generated or your database are available as OLE objects. OLE is the well-known object linking and embedding mechanism that are used across different Microsoft applications, therefore for example you can actually use your database table as an embedded table in a word document for example or in a power point presentation and so on.






(Refer Slide Time: 52:08)

 

And there is a user level security based on login and password that is based around the NT server model. And it also performs multi user, it also supports multi user operations and concurrent clients and so on. So to summarize what we learnt on MS Access, MS Access is a DBMS system that is meant primarily for the non-technical end user where the learning curve is much smaller than in using say oracle or MySQL or any other larger commercial database system. 

It is shipped with the MS Office with, so whenever you buy the MS Office suite of packages MS Access usually comes shipped with it. And the main feature of MS Access is the GUI or the graphical user interface which can help you perform almost any database operation whether it is specifying your database or formulating queries or reporting or any other kind of database operations.

So it gives support for graphical query specification and also report generation and these databases are available as DSNs or data source names through an ODBC interface that is any database client that is compatible with ODBC can access MS Access databases as easily as it can access other ODBC comply databases. Oracle also is also ODBC comply and several most widely available databases are ODBC comply.










(Refer Slide Time: 53:02)

 

And these tables or database objects in MS Access are available as OLE objects that can be linked or embedded within other MS Office or within other Microsoft applications like say MS Word or EXCEL or POWER POINT or so on.
 
(Refer Slide Time: 55:27)

 

So let us have a look at some of these screen shots here that show how a typical MS Access or working with MS Access looks like. So when you start a database, this is what you get that is you can start a database where you can specify your database design using different views, there what is called as a design view or you can create tables using a wizard where it will automatically fill up certain well known fields in a table or suggest you for certain structures for your database or you can even create tables by actually entering data using a form.
  
(Refer Slide Time: 56:03)

 

This screen shot here shows an actual form by which data can be populated. So here there are two different forms for two different tables. There is table called mailing list and there is a table called assets and these are the different fields that go into this list. So you basically fill up this form and the data is actually stored on to the database.

(Refer Slide Time: 56:21)

 

The last screen shot here shows how you can graphically build a query using the QBE interface or the query by interface. The window here essentially says that or the user who is formulating the query has said that the result of this query should contain these 4 different tables, out of these 4 different tables these are the fields that are required and the user can specify and basically there is a show which can be clicked or unclicked which will say what to show in the table. And the criteria can be specified here which will say how to formulate the query. So that is the beauty of this MS Access databases. So with that we come to the end of the second case study.

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 34

Data Mining and Knowledge Discovery

Hello and welcome. In this session today we are going to look at very interesting aspect of or interesting application in which database technologies are used namely the field of data mining and knowledge discovery. In fact in recent years data mining has become an extremely or fields that eliciting an extremely large amount of interest not just from researchers but also from commercial domain. I mean the commercial utility of data mining is probably of more interest than or at least as much interest as the research interest that lies in data mining.

(Refer Slide Time: 01:25)

 

And in addition to commercial interest, there is also number of public debates that data mining has started which range from topics like legalities and ethics and the rights to certain information and the rights to non-disclosure of information or the rights to privacy and so on and so forth. So data mining actually is in some sense has opened a pan door as box in and only time will tell whether the technology has given, has been on an overall sense completely beneficial or destructive in nature. 
	
But then there is nothing beneficial or destructive about technology per say it’s how we use it, how we use technology which is what matters. So any way in this session, we shall be concentrating mostly on the technical aspects of data mining obviously. 


(Refer Slide Time: 03:59)

 

And we shall look at the basic algorithms and concepts that make up data mining and what exactly is meant by data mining and how does it differ from the traditional operations of databases or traditional way in which databases are used. So the overview of this or this set of two sessions would be as follows. Let us first motivate the need for data mining that is why data mining and what are some of the basic underlying concepts in data mining, what are the building blocks of data mining concepts. Then we look at data mining algorithms and several classes of this data mining algorithms. 

We will start with tabular mining as in mining relational tables and we will look at classification and clustering approaches and we will also look at mining of other kinds of data like sequence data mining or mining of streaming data and so on. And data warehousing concepts would be covered as a different session all together. First of all, why data mining from a managerial perspective. Let’s first look at what a data mining has for the commercial world first before we go in to looking at the technical aspects of data mining. 













(Refer Slide Time: 04:33)

 

If you were to let us say give an internet search or talk to a manager, let us say about why he or she would invest in data mining, you would encounter a variety of answers. One would say something like strategic decision making that is I look for some kinds of some ways or some patterns in or mind for certain nuggets of knowledge to understand something about strategic decision making or to help in strategic decision making. Somebody would say well it is very useful for something called wealth generation although there is no precise definition of the term wealth generation and you would say that data mining would help me in understanding or making the right decisions that can help me increase my financial portfolio or whatever. 

Somebody would say well I would use data mining for analyzing trends, analyzing how my customers behave or analyzing how particular market is behaving and so on and so forth. And more recently data mining has been used extensively for security purposes especially mining network logs or network streaming data in order to look for abnormal behavioral patterns or patterns that might be potentially linked to abnormal activity in the network or in the system and so on. So, security is now relatively recent and very important application area of data mining.

So, what is this data mining all about and why is this so controversial and why is it so interesting from a technical perspective at the same time. Data mining is the generic term used to look for hidden patterns in data or hidden pattern and trends in data that are not immediately apparent by just summarizing the data. So if I want to look for certain patterns, let us say if I have set of all students and their grades if I want to look for certain patterns on how are the students performing over time or what is the is there some kind of relation between subject A and subject B I mean if a student does well in subject A, he or she does badly in subject B or so on and so forth.
 

(Refer Slide Time: 06:33)

 

Such things cannot be discovered by just aggregating the data, by just saying what is the average or what is the summation or whatever. Besides, such things also cannot be discovered by, I mean such things in a sense cannot be within quotes discovered if we have to give queries that finds out these aggregations. That is if we already knew what it is that we are looking for then it’s not a hidden pattern any more. We know that such a pattern exists that is students performing in subject A will not perform well in subject B, we know that such a correlation exists and there is nothing hidden in the pattern anyway.

So data mining essentially has no query that is if you are performing a data mining on a on a database, we do not talk of any data mining query. In fact it is the mining algorithm that should give us something which we don’t know. Now how do we say something which we don’t know, which is putting it in a very broad sense I mean which is making things so vague. So data mining is actually controlled by what are called as interestingness criteria and we just specify to the database that this is what we understand by an interesting pattern. 

Let us say correlation between performances in subject A and subject B or some kinds of trends over a period of time. This is what is interesting for us. Now find me something or find me everything which I don’t know about or which are interesting according to this criteria.








(Refer Slide Time: 09:24)

 

So when we talk about data mining, we have a set of data to begin with that is we have a database and then we give one or more interestingness criteria and the output of which will be one or more hidden patterns which we didn’t know exists in the first place. Now   given this model, we should say now when we say patterns then the obvious question to ask is what type of patterns, what do you mean by patterns or what do you mean that this is or when do you say that something is a pattern and something is not a pattern. 

If we have to answer that we have to ask two further questions that is what is the type of data that we are looking at, what kind of data set is it that we are looking at and what is the type of interestingness criteria that we are looking. What do we mean by interestingness, is it correlation between something, what exactly do we mean by interestingness.

So let us look at the different type of data that we encounter in different situations. The most common kind of data is the tabular data or the relational database which is in the form of set of tables or now slightly different multi-dimensional form of database. And it’s very common that any kind of transaction data that is let us say data array coming out from the database from an ATM for example or the data coming out from the transactional database at a railway reservation counter or at a bank or any place like that are all tabular in nature. So it’s a most common form of data and which is a rich source of data to be in mine.







(Refer Slide Time: 10:16)

 

In addition to tabular data, there are spatial data for example where data is represented in the form of either points or regions which have been encoded with certain coordinates X Y Z coordinates. So each point in addition to having certain attributes also has certain coordinates and mining in this context also requires us to know what is the importance of the coordinates system.

In addition to spatial data there are other kinds of data like say temporal data, temporal data in the sense that were each data element has a time tag associated with it. So temporal data could be for example streaming data where network traffic or set of all packets that are flowing through a network forms streaming data which just flows fast and where each packet can be allocated some kind of a time stamp or something like activity logs, your database activity log is a temporal data. There could be also be spatio temporal data that is data that are tagged both by time and coordinates. And other kinds of data like tree data which for example XML databases or graph data where especially bio molecular data or volvoid web is a big graph data and so on.

Then there are sequence data like data about genes and DNAs and so on and again activity, I mean sequence is a kind of temporal data where timestamp need not be explicit in sequence then text data, the arbitrary text or multimedia and so on and so forth. So, the several different kinds of data that can be the source from which we can extract or mine for unknown nuggets of knowledge. 







(Refer Slide Time: 13:24)

 

Similarly when we talk about interestingness criteria, several things could be interesting. If certain pattern of events or certain patterns of data keep occurring frequently then it might be of interest to us, something that happens very frequently. So frequency by itself is an interestingness criteria or interestingness or a criteria on which interestingness can be based.

Similarly rarity, if something happens very rarely and we don’t know about it or let us say rarity is again a very interesting pattern to be searched for when we are looking at say abnormal behavior of any system or abnormal behavior of network traffic and so on. So something that happens rarely that is away from the norm is again an interestingness pattern. Correlation between two or more elements and if the correlation being more than a threshold is again interesting or length of occurrence in the case of sequence or temporal data and so on.	 

And consistent occurrence, consistency that is consistency is different from frequency in the sense that overall in the set of all databases, overall for the entire database a given pattern may not be frequent enough. For example there could be one particular behavior pattern, let us say one particular customer comes to a bank every month at the tenth of each month. So if we are looking for frequently banking customers, let us say this customer would not figure out in this algorithm because this customer comes only once a month whereas other customers could be coming many times a month. However if we are looking for consistency in behavior then this customers behavior is far more consistent than someone who comes let us say arbitrarily 10 times the first month and once the second month and 50 times the third month and so on and so forth. So in terms of consistency in his behavioral pattern across different months, this pattern is interesting even though it’s not frequent. 

Then repeating or periodicity is slightly similar to consistency except that a periodicity is I mean consistency is across the entire set, across the entire set of months if you have divided our database into months but periodicity, the time interval could vary in in a periodicity of a pattern. If a customer comes let us say a 5 times to the bank every 6 month, we may not be able to catch it as part of a consistent pattern analysis but if we use an algorithm that detects periodicity of several occurrence of events, we will be able to detect it. And similarly there are several other patterns of interestingness that which one could think of.

Now when we talk about data mining, usually there is sometimes a misconception and not completely but usually there is a contention that data mining is the same as statistical inference. For many cases it is yes, the answer is true that is several concepts from statistics have been incorporated in to data mining and data mining software uses statistical concepts or many kinds of statistical algorithms comprehensively. However there is a fundamental difference between statistical inference and data mining which is perhaps the reason for the renewed interest in data mining algorithms. And here is the general idea behind the data mining versus statistical inference. 

(Refer Slide Time: 17:30)

 

What do we do when we talk about statistical inference? Statistical inference in techniques, essentially have the following three steps as is shown in this slide here. In statistical inference, we start out with the conceptual model or what is called as the null hypothesis. That is we first of all present ourselves or perform a hypothesis about the system in concern. That is we make a hypothesis that if some something to the effect that if exams are held in the month of march then there would be I mean then the turnout would be higher than if it is held in the month of June or something like that. Now based on this hypothesis, we perform what is called as sampling of the data set or of the system.

Now sampling is a very important step in a statistical inferencing process. There is huge amount of literature in to what is meant by correct sampling or what is called as a representative sample and so on. Now based on the sampling of data set from the system, we either prove or refute our hypothesis. That is we show a proof saying, yes this hypothesis is true because statistical sampling of the system has shown that this is true otherwise it’s false. 

Now, when we sample for example if you are performing a statistical inference about user preferences or let’s say some kind of market analysis, we present questioner to different users based on our null hypothesis or based on our conceptual model. Now it is this set of questioner, now this questioner has been created by our conceptual model. So this questioner already knows what to look for and the proof or the answers will either prove or refute the hypothesis but data mining on the other hand is a completely different process or rather it’s the opposite process.

(Refer Slide Time: 19:57)

 

In data mining we just have a huge data set and we don’t know what is it that we are looking for. We don’t have any hypothesis, we don’t have any null hypothesis to begin with. We just have a huge data set and we just have some notions of interestingness. Now we use this interestingness criteria to mine this data set and usually there is no sampling that is performed on the data set that is the entire data set is scanned at least once by the data mining algorithm in order to look for patterns. So there is no question of sampling and there is no null hypothesis to begin with. So we just have a weighed notion of an interestingness based on which we present an algorithm, data mining algorithm over the data set. Out of this comes out certain patterns, certain interesting patterns which form the basis for forming a hypothesis. So it’s sometimes also called hypothesis discovery. Obviously, of course we cannot discover complete hypothesis using just data mining but we too discover patterns using which we can formulate a hypothesis. So in a sense it’s an opposite process of statistical inference. 
Let us look at some data mining concepts. Two fundamental concepts are of interest in data mining especially in the core algorithms of data mining especially the apriori based algorithms. These are what are called as associations and items sets. An association, when we say an association it is a rule of the form if X then Y as shown in this slide here and it’s denoted as X right arrow Y.

(Refer Slide Time: 21:23)

 

For example if India wins in cricket sales of sweets goes up, if India wins in cricket then sales of sweets goes up. So here X is India wins in cricket and Y is the predicate that sales of sweets go up. So we say that we discover such a rule if we are able to conclusively say based on analyzing the data that whenever India wins in cricket, the sales of sweets go up. And on other hand suppose if there is any rule of this form that is if X then Y then I can imply that if Y then X. (Refer Slide Time: 21:27) That is the ordering of this rules is not important. If India wins in cricket then sales of sweets go up, if sales of sweets go up then India has won in cricket and so on which may be true or may not be true but if that is the case then it is called an interesting item set. That is it’s just a set of item. For example people buying school uniforms in june also buy school bags or you can also say people buying school bags in june also buy school uniforms. So it’s just a item set that is school uniforms and school bags are a set of items which are interesting by themselves. 

Once we define the notion of a association rule and an item set, we now come to the concept of support and confidence. That is how do we discover a rule to be interesting. We say that a rule is interesting in the sense of frequent occurrences of a particular rule, if the support for that rule is high enough. That is the support for a given rule R is the ratio of the number of occurrences of R given all occurrences of all rules. So we look into the exact or we will illustrate the notion of support in the next slide with an example where it will become more clear.

(Refer Slide Time: 22:41)

 

And when we say the confidence of a rule, suppose I have a rule if X then Y then the confidence of the rule is suppose I know that X is true, the ratio of all occurrences when Y is also true versus when for all other occurrences when X is true and something else is here (Refer Slide Time: 23:46). So that is it’s a ratio of the number of occurrences of Y given X among all other occurrences given X. So if I know that X is true with what confidence, with what percentage of confidence can I say that Y is also going to be true?

Let us look at some examples here (Refer Slide Time: 24:04). Let us say these are some item sets let us say these are data that have been distilled from purchases of different consumers over a period of time over, in a given month let us say. So the first consumer has bought a bag, a uniform and a set of crayons, the second consumer has bought books and bag and uniform, the third one has bought bag uniform and pencil and so on and so forth. Now suppose I take the item set bag and uniform, (Bag, Uniform) what is the support for this item set. Now the support for this item set is look at all the transactions or the rows here in which bag and uniform occur 1 2 3 4 and 5 uniform and bag. Out of a total of 10 rows, 5 of them have bag and uniform occurring in that.












(Refer Slide Time: 24:34)

 

Therefore the support for bag and uniform is 5 divided by 10 which is 0.5 that is with a this dataset supports the assertion that bag and uniform will be bought together with 50% support that is 0.5 as its support. What is the confidence that, what is the confidence for the rule if bag then uniform? That is what is the confidence by which we say whenever somebody buys a bag, they also buy uniform. For this we have to look at the set of all item sets or the set of all transactions or rows here in which bag and uniform, bag occurs rather not just uniform in which bag occurs.

So bag occurs in 1 2 3 4 5 6 7 8 different rows, out of which bag and uniform have occurred in 5 different rows. Therefore the confidence for this assertion or this association rule is 5 divided by 8 which is about 62%. That means if some consumer has bought a bag then with 62% of confidence or 62.5 % of confidence, we can say that the consumer will also buy a uniform, a school uniform along with this. So the question now is how do we mine or how do we find out the set of all interesting item sets and the set of all interesting association tools. 

Now have a look at this previous slide (Refer Slide Time: 26:50) once again. Now the association rule, when we talk about association rules we have just or rather when we talk about item sets first we just saw a single item set having two different elements here but that need not be the case, bag by itself could be an item set a single element item set, uniform by itself could be a single element item set, crayons could be a single element item set or let us say bag, uniform and crayons could be a three element item set and so on. So item sets could be of any size size 1, size 2, size 3, size n any set of elements. Now we have to find the set of all item sets that is the set of all items that are bought together and that have been together frequently as part of this transaction log here.



(Refer Slide Time: 26:48)

 

Now how do we do that? Now there is a very famous algorithm called the apriori algorithm which performs such a discovery process that is a discovery process for all frequent item sets in a very efficient manner. The simple idea behind apriori algorithm, it is shown in this slide here. However let us not go through the slide in a lot of detail, since it will be more easier to explain apriori through an example. 

The idea behind apriori algorithm is that, the essential idea behind an apriori algorithm is that suppose I have any n element item set. Let us say suppose I have any 5 element item set, that is interesting or that is frequent. So if this 5 element item set is frequent then all sub sets of this item should also be frequent. This seems obvious but this is a very important conclusion or it’s a very important observation in the apriori algorithm. That is if I discover the set of all one frequent item sets that is the set of all item sets of size 1 which are frequent then there is no need for me to look at other item sets when I am looking for two frequent item sets. That is the set of all item sets of size 2 which are frequent will be made up of combinations of set of all item sets of size 1 which are frequent.

So let us illustrate the process of apriori with an example. Let us take our consumer database again, the previous consumer database again where we have consumers buying several school utilities like bags and school bags and school uniforms and crayons and pencils and books and so on and so forth.







(Refer Slide Time:  29:22)

 

Now suppose we set when we say or when we ask the apriori miner to mine for all interesting item sets, we have to the interestingness criteria here is frequency that is frequent occurrence. Now frequency is or interestingness here is parameterized by a threshold parameter which is called the minimum support or min sup. So let us say minimum support is 0.3 that is we term an item set to be interesting if its support is at least 0.3 or greater. 

Now given this what are all the interesting one element item sets? What is that mean to say what are all the interesting one element item set, which one element item sets occur at least at a rate of 30% or more. Now this database here or this data set here has a total of 10 rows therefore we have to look at all one element item sets which occur 3 or more times. So given this we see that all of these are interesting that is bag, uniform, crayons, pencil and books. Bag occurs much more than three times, uniform also occurs more than three times, crayons also occur more than three times and so on. So all of these elements here occur more than thrice which therefore all of this one element item sets have a minimum support of 30% or more. 

Now from this, suppose we have to look at the set of all interesting two element item sets. Now how do we build the set of all interesting two element item sets? We just look at all possible combinations between one element item sets, therefore we have bag uniform, bag crayons, bag pencil, bag books, uniform crayons, uniform pencil uniform books and so on and so forth. Now out of this for each such two element item set that have been created, we have to see how many times they occur in this data set. Now we see that it’s only these set of combinations which have a minimum support of 0.3 or more. So for example bag uniform, bag crayons, bag pencil and bag books all of them along with bag are interesting. 

However let us say uniform and book is not interesting that is it doesn’t occur more than thrice. So let us see how many times uniform and book occur? Uniform and books occur once and second one twice here, so they occur only twice but we need a minimum support of three times so that’s not interesting. Similarly a pencil and uniform, so uniform and pencil is again is not interested. So therefore we have filtered away or we have thrown away certain item sets from our exploration here and identified only a smaller subset of the set of all possible combinations of one element item set. 

Now from this if we have to look for all three element item sets, we have to generate the set of all candidate three element item sets. What are the candidate three element item sets? Perform a union across all possible combinations of these interesting two element item sets to create all possible distinct three element item sets and then look for those three element item sets which occur at least three times or more in this database. Given that we see that there is only one three element item set that is bag, uniform and crayons that is interesting that is that occur at least three times or more or that has at least, that has support of at least 30% in this in this data set. 

(Refer Slide Time: 33:56)

 

So as you can see the apriori algorithm, you can visualize the apriori algorithm in the form of let us say an iceberg. Such queries are also called as iceberg queries when given on to databases that is at the base there are large number of one element item sets. But once we start combining them together, we start getting smaller and smaller numbers of combinations and we peak out at a very small of large item sets which are frequent. So the beauty of the apriori algorithm is that for every parse, it does not need to go through the entire data set. It does not have to parse through the entire data set, it only needs to consult results of the previous iteration or item sets that are of one element one lesser than the present iteration in order to construct candidates for the present iteration. 

So given this algorithm here let us go back and look at the apriori algorithm. Given the explanation here with an example let us go back and look at the apriori algorithm which will now be a little more easier to understand. Initially we start with a given minimum required support s as the interestingness criteria. now given minimum support s as the interestingness criterion, first we e search for all individual elements that is one element item sets that have a minimum support of s. Now we start, we go into a loop where we start looking for item sets of sizes higher greater than 1.

So from the results of the previous search for i element item sets, search for all i plus 1 element item sets that have a minimum support of s. This in turn is done by first generating a candidate set of i plus 1 item sets and then choosing only those among them which have a minimum support of s. Now this becomes the set of all frequent i plus element item sets that are interesting. So this loop is repeated until the item set size reaches the maximum. That is there no more candidate elements to be generated for the next item set or there are no more frequent item sets in the current iteration.  

Now that was about item sets. A property of item sets is that there is no, I mean you basically consider item sets as one entity that is there is no ordering between the item sets. that is it does not matter if somebody buys a bag first or a uniform first or a crayon first or whatever, as long as the, only thing that we are going that we infer from this is that the item set bags, uniforms and crayons are quite lightly to be bought together in in in one piece. 

Therefore if I am let us say a super market vendor, I mean someone having a super market then it would make sense for me to place bags and school uniforms and crayons next to each other. So because there is a higher probability that all three of them are bought together. But when we are looking for association rules we are also concerned about the direction of association that is there is a sense of direction saying if A then B is different from if B then A. So association rule mining requires two different threshold, the minimum support as in the item sets and the minimum confidence with which we can talk about a, with which we can say or determine that a given association rule is interesting.














(Refer Slide Time: 37:22)

 

So how do we mine association rules using apriori. Again we shall do the same thing like we did in the past. We shall come back to this algorithm or the general procedure after we have illustrated an example by which we can mine apriori, using apriori algorithm by which we can mine association rules.

(Refer Slide Time: 38:47)

 

Now the main idea is the following. Now use the apriori algorithm and generate the set of all frequent item sets. So let us say we have generated a frequent item set of size 3 which is namely bag, uniform and crayons with a min sup or of 0.3 that is a minimum support threshold of 30%. Now this bag, uniform and crayons can be divided into the following rules. If bag then uniform and crayons or if bag and uniform then crayons or if bag and crayons then uniform and so on so forth. 

(Refer Slide Time: 39:38)

 

Now what is this thing mean? this thing means that when a customer buys a bag then the customer also buys uniform and crayons and this rule means that if a customer has bought a bag and a school uniform then the customer will also buy a set of crayons or if a customer has bought a bag and a set of crayons then the customer will also buy a school uniform and so on. 

Now we have got all of these different association rules. Now each of these association rule has a certain confidence based on this data set. Now what is the confidence for each of these rules? What is the confidence for the rule if bag then uniform and crayon. That is if a customer buys a school bag then here she will also buy a school uniform and a set of crayons. In order to calculate the confidence of this, we have to first look at which are all the item sets here that have bags that is where the customer has bought a bag. So, there are 1 2 3 4 5 6 7 8 different entries where customer has bought a school bag. 

Now among these 8 entries, in how many different entries did the customer also buy uniform and crayons? 1 and 2 3, so there are 3 different entries, 3 different instances out of 8 instances where this rule holds. Therefore whenever a customer buys a bag, one can say with 3 by 8 or 37.5% of confidence that the customer is also going to buy a set of school uniform and crayons. Similarly we can calculate the confidence for each of these other association rules like this is 0.6, 0.75, 0.428 and so on and so forth. 

Now, given a minimum confidence as a second threshold and suppose we say that the minimum confidence is 0.7 then whichever the rules that we have discovered, every rule that has confidence of at least 70% or more.

That means we have discovered the following three rules, bag if bag crayons then uniform, uniform crayons then bag and crayons then bag and uniform. What is that mean in plain English? It means that people who buy a school bag and a set of crayons are likely to buy a school uniform as well that is bag and crayons implies uniform.
 
(Refer Slide Time: 40:47)

 

Similarly people who buy a school uniform and a set of crayons are also likely to buy a school bag that is here, somebody buys uniform and a set of crayons then they are also likely to buy a school bag. Similarly if somebody buys a set of crayons then they are very likely to buy a school bag and a school uniform as well.

(Refer Slide Time: 43:09)

 
So that is here, that is somebody buys crayons then with 75% confidence one can say that they also buy bags and school uniforms. So again it’s a question of direct marketing or whatever. If somebody is interested in crayons then you might be reasonably sure that they are also interested in a bag and a school uniforms so on. Now so let us look at look back at the algorithm here (Refer Slide Time: 43:41) for mining association rules.

Simple mechanism for mining association rules is first of all use apriori to generate different item sets of different sizes and at each iteration, we can divide each item sets in to two parts an LHS part and an RHS part, the left hand side part and the antecedent and precedent that is the right hand side part. 

So this represents a rule of the form LHS implies RHS. Then the confidence of such a rule is support of LHS divided by that is support of the entire thing divided by the support of LHS. That is support of LHS implies RHS divided by support of LHS will give us confidence of this rule. And then we discard all rules whose confidence is less than minconf.

So now let us look in to the question of how do we generate or how do we prepare a tabular data for association rule mining or let us say item set mining and so on. Now because we use let us say relational data set, relational database you might have observed that or you might have got a little doubt when we have been considering a data set like this. There is something peculiar about this data set. What is peculiar about this data set here? The peculiarity is that it looks like every consumer coming to this store is buying exactly three items which is very unlikely. 

In fact what is more practical is that this set, this data set contains records of variable length. That is one customer may have bought just two different items whereas some other customer may have bought 10 different items whereas a third customer may have bought only 5 different items and fourth customer may have bought only one item and so on and so forth.
















(Refer Slide Time: 46:34)

 

So it is not possible to represent this item set like a table, like a well form table like this because basically it is a set of all items of different lengths. In fact the best way to represent this would be in a normalized form let us say in a database where for example the same bill number here 15563 15563, both of this refer to the same customer. That is it’s the same customer who has bought books and crayons and this is not completely normalized because date is not really necessary here but nevertheless here all of these records are of uniform length, if you order this based on the set of bill numbers then we get the set of all different transactions. 

Now depending on what we are looking for this, this ordering might make a difference. How does this ordering make a difference here when we are looking at data set like this? Suppose given a dataset like this, here performing group by’s on different fields will yield as different kinds of behavior data sets.















(Refer Slide Time: 00:47:19)

 

So what does it mean? Suppose let us say we perform a group by based on the bill number. 

(Refer Slide Time: 47:37)

 

So suppose we perform a group by on the bill number on this table then each group will represent the behavior of one particular customer that is one bill represents one or one bill number represents one particular customer or one particular transaction. So suppose we group by based on bill numbers and then perform apriori across these different groups then we would be getting frequent patterns across different customers.

On the other hand suppose we group by over date, so rather than bill number. So all transactions happening on a given date will come in to one group and all transactions happening on another date will come in to another group but a given date may have transactions from several different customers but all of them are now grouped in to one single group. And suppose we run apriori over this set, over this different groups then we would actually be looking for frequent patterns across different days that is across the different dates. So we have to interpret what we mean by something that is frequent based on how we have ordered the data. If we have ordered the data over different customers then it would show aggregate behavior over the set of all consumers with whom you are interacting with.

On the other hand if you are running apriori or if you have performed group by over dates then it would show you aggregated behavior over a given time period rather than over the set of all customers. Well, it also includes the set of all customers but what is more important here is that how does the behavior or how has the behavior changed over time. So if something is frequent over time, it means that it is uniformly or in some sense consistent over this entire period of time.

So let us summarize what we have learnt in this session. We started with the notion of data mining and like I said in the beginning, data mining is a very interesting sub field of databases which has elucidated a lot of interest not just from researchers or and not just from the technology perspective but from several other perspectives like defense perspective or security perspective, commerce that is business perspective and so on. And there are several debate that have raged on whether it is right to use data mining to look for certain behavior pattern. 

for example would it be right, if a government uses data mining over let us say the set of all different activities of people and find out the behavior pattern of any particular individual and so on. And their pros and cons on both sides of the debate, one would say for security reasons it is right to look for behavior patterns and one would say well for privacy reasons it’s not right to look for behavior patterns and so on and so forth. so it’s a topic which is very much pertinent and has spond a huge amount of interest from several different areas.

And data mining is in some sense, I called it as sub field of databases but that’s not entirely true in a sense that data mining and knowledge discovery many would claim is a field in itself. That is it relies on database concepts as well as several other concepts like learning theory or statistical inference and several other concepts in order to perform data mine. So don’t be really surprised if one would say that a data mining is a complete field in itself and its only associated with databases not really sub field of databases.

but anyway data mining as we said is the process of discovery of previously unknown patterns in the sense that we have not really sure what is it that database is going to give us or what new pattern or what new nugget of knowledge so to say is we are going to learn as part of the data mining process. As a result there is no query as part of a data mining process that is a data mining algorithm is based around one or more interestingness criteria rather than a given query.

(Refer Slide Time: 50:11)

 

And we saw that in conceptually, it is in some way the opposite of statistical inference where we start with a null hypothesis and either refute or prove or hypothesis by sampling, statistical sampling of the population. While here we don’t start with a hypothesis but the end result of the data mining process is the set of patterns which can help us in formulating a hypothesis. We also saw the notion of association rules and item sets as well and the concepts of support and confidence and two different algorithms the apriori algorithm for mining frequent item sets and from which we also saw the apriori algorithm for mining association rules. In the next session on data mining, we are going to look at several other algorithms like say classification or discovery. So that’s brings us to the end of this session. Thank you. 

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 35

Data Mining & Knowledge Discovery
Part II

Hello and welcome to this second session in data mining. In the previous session we saw what this concept of data mining was all about and we saw some very fundamental concepts of item sets and association rules and how do you discover particular patterns in an item set. That is how do you discover something that you don’t know from a data set using the concept of support and confidence and so on.

(Refer Slide Time: 02:06)

 

So, essentially you give a particular interestingness criteria and then you start distilling out certain patterns from the data set. Let us move on further in this session where we will briefly look into some fundamental algorithms or some very simple algorithms on different kinds of data mining activities namely in discovering classification trees or discovering clusters of properties of data and mining sequence data, the data of different sequences or stream data mining and so on. Let us briefly summarize what data mining was all about.






(Refer Slide Time: 02:23)

 

Data mining essentially is the concept of or is the idea of looking for hidden patterns and trends in data that’s not immediately apparent by just summarizing the data. So when we say hidden patterns, its essentially means that something that we don’t know about. There is nothing hidden if you already knew such a pattern existed in the data base. So in a data mining setting there is no query but we use the concept of an interestingness criteria. That is we use let us say frequency or consistency or rarity or whatever be the interestingness criteria and certain parameters define each of these interestingness criteria like frequencies is parameterized by support and confidence for association rules and just support for item sets and so on. And again there are different kinds of data we can think of tabular data, spatial data, temporal data, tree data, graph data and so on and so forth.


















(Refer Slide Time: 03:24)

 

So today or in this session we shall look at specifically at sequence data mining and streaming or mining streaming data and in addition to other mining algorithms. And of course type of interestingness itself could be varied that we could talk of frequency as frequent patterns as being interesting or rare patterns being interesting and so on.

(Refer Slide Time: 03:48)

 

Now let us move further from here and look at the concept of classification and clustering that is discovering classification tree and discovering clusters within a given data set.


(Refer Slide Time: 04:03)

 

Now what is the difference between classification and clustering? Intuitively they both seem do the same thing. That is when you classify a given data set into different classes or whether you cluster a given data set into different clusters but essentially few observe closely classification maps data elements to one of a different set of pre-determined classes based on the differences between data elements. That is if data element a and data element b belong to different classes if they are different enough. 

On the other hand clustering groups data elements into different groups based on similarity between elements within a single group and sometimes it’s also the case that in a classification we know the classes apriori. We know what are all the different classes into which data can be classified into and sometimes in clustering, we don’t know how many clusters we are going to get before the clustering process begins. Let us look at mining in relation to classification techniques rather; we are not interested here in the idea of classification itself but we are interested in the idea of discovering classification. What is it meant by discovering classification? Discovering a decision tree or which decides how to classify data sets into different classes.
 











(Refer Slide Time: 05:19)

 

Let us take a small example. Discovering this algorithm is best represented by an example. So let us take a small example and see how we can discover a classification tree. Let us say that we have data about different cricket matches that have been played over the last several years. Now we have a, let us say in a given city. Now the question is this city is notorious for it rains for its rains and its unpredictable weather. Now in the past several times, play had to be abandoned that is play were to be continued or was abandoned and so on. 

Now we have data like this from different data sets. When it was sunny and the temperature was 30 degrees, play was continued. When it was overcast and the temperature was 15 degrees play wasn’t continued, when it was sunny and temperature was 16 degrees play was still continued and so on. So in some times play was continued and sometimes play was discontinued, its no. Now what is the classification problem is can I classify weather conditions which is a combination of the outlook and the temperature into one of two classification classes that is whether we are going to play or play is going to be discontinued. That is what is the criteria, when play was discontinued and what was the weather criteria when play was continued.











(Refer Slide Time: 07:32)

 

So there is a well-known algorithm called Hunt’s method for identification of decision trees and like before let us first look at an example of how we identify a decision tree before looking at the algorithm itself. 

(Refer Slide Time: 07:53)

 
	
The way of identifying decision tree is quite simple. First of all because this temperature field here (Refer Slide Time: 08:04) is a numeric value, it could take several different values and which might be of no interest to us. So let us perform a hand classification of this numerical values into different classes. 

(Refer Slide Time: 08:19)

 

So what we have done here is that temperature is now classified into three different   classes warm, chilly and pleasant. So whether the temperature was warm whether the temperature was chilly or whether the temperature was pleasant based on dividing the set of temperatures into different classes. Now first of all because there are two values here that is there are two fields here outlook and temperature, both of them both of them will affect the decision on whether we are going to play. 

So how do we know what is the best or how do each how do each parameter affects the decision whether to play or not. Let us start by looking at one parameter after another. First let us look at sunny. Now if you see here that whenever the outlook was sunny, the cricket match was played it was not abandoned. It is sunny only twice here and in both cases cricket matches played. Therefore we can directly conclude that if the weather is sunny regardless of whether the temperature is warm or whether the temperature whether the temperature is chilly or whatever, we can conclude that play will continue, the play is not going to be stop. 

On the other hand let us look at cloudy here. Now when it is cloudy here play was continued in one case or rather in two cases and when it was cloudy here, play was discontinued in one case. So from cloudy we are still in a what is called as a bivalent state that is it is still yes or no may be or whatever, may be yes may be no, we still don’t know. Similarly when the outlook was overcast, let us say here it was overcast and they didn’t play. Here once when it was overcast they actually played and once more, when it was overcast then they didn’t played. So, from overcast we still say yes or no, we don’t know whether they are going to continue play or not.


 

(Refer Slide Time: 10:38)

 

So what we can do now is we can safely remove the first rule from our process that is this is a rule that we have already discovered that is when it is sunny they are going to play. So now let us remove this rule from our from consideration and take these two rules. Now because from cloudy and over cast, we are still in a bivalence state we have to ultimately reach to a state where we can remove this bivalence that is we can either conclude yes or no conclusively. So we will try to we will try to now introduce the second parameter temperature into this state here to see whether we can remove this uncertainty about yes or no. The first case the uncertainty is already removed, so there is nothing we need to do any more. 

(Refer Slide Time: 11:32)

 
So we have introduced let us say here (Refer Slide Time: 11:41) for cloudy, we have introduced all three possible cases warm, chilly and pleasant, similarly for overcast warm, chilly and pleasant. So let us take cloudy and warm. So, whenever it was cloudy and warm there is only one case here play was continued, yes. So basically we have removed the bivalency that is we have conclusively stated that whenever it is cloudy but the temperature is warm, play is going to continue, we are not going to abandon play.

On the other hand whenever it was cloudy and chilly, there is only one case here where play was discontinued. So again there is the bivalency is removed that is cloudy and chilly means no. So we can again conclusively state that the play is going to be abandoned if the outlook is cloudy and the temperature is chilly. 

Similarly when it is cloudy and pleasant, cloudy and pleasant is here and there is only one case here, cloudy and pleasant is yes. So when the outlook is cloudy but the temperature is pleasant, we can still conclude that they are going to continue play. Similarly overcast and warm there is no entry at all, so we don’t know there we can’t decide anything. So overcast and warm remains as it is and overcast and chilly gives us no, that is play is going to be abandon. Similarly, overcast and pleasant gives us yes.

(Refer Slide Time: 12:53)

 

So effectively we have removed this bivalency that existed here (Refer Slide Time:  13:20) when it was cloudy and overcast and decided or came to know when, under what conditions play is going to be continued when it is cloudy and under what conditions play is going to be discontinued when it is cloudy and the same thing for overcast. So therefore what we have actually done is we have discovered this decision tree. So initially we were in a bivalent state that is we don’t know play is going to be continued or discontinued. Now in this bivalent state we were told that the outlook is sunny then we can immediately conclude yes we are going to play today. 

(Refer Slide Time: 13:53)

 

On the other hand if you are in this bivalent state here, if you are told that the outlook is cloudy, we will still be in a bivalent state we still don’t know whether they are going to, whether the play is going to be continued or not. So we ask for more information and then when you find out that the temperature is pleasant, let us say for example then we say that yes the play is going to continue. On the other hand if the temperature is chilly then we have reasons to believe that play is not continued that is the data set tells us that play is going to be abandon and so on.

So what we have got here is a tree data structure where from a bivalent state, we eventually go into a univalent state that is a state were the uncertainty is removed and then we have concluded or we have classified his this play into two different classes that is yes or no that is play is going to be continued or play is going to be abandon. So let us look back (Refer Slide Time: 15:11) at the algorithm little bit as how to go about this. Suppose we are given n different elements. 

In our case in the example that we right now saw, n was equal to 2 that is outlook and temperature. so suppose we are given n different element types and m different decision classes, in this case again m was two that is yes and no. so what we do in this loop here, for each of the different element types we keep progressively adding element i to the i minus oneth element item sets from the previous iteration. And then whenever and then we see whether we can decide, identify the set of all decision classes for each such item set.

If the item set has only one decision class that means we have already decided. so this is done, removed that item set from subsequent iterations otherwise keep continuing until you finish all your element types. 

And of course it could well be the case that even after finishing all my n different item sets, I may not be able to reach a conclusive decision.

(Refer Slide Time: 16:07)

 

So it might well be the case that when it is over cast and chilly. Sometimes they actually play and sometimes they didn’t play and so on. so that again, there are several methods to deal with such kinds of indecisiveness for example to use probabilities that is this is going to or some kind of fuzzy classification where we say that outlook is overcast and temperature is pleasant then they are going to play with a probability of 90% or something like that. 

So let us look further into what are some clustering techniques (Refer Slide Time: 17:03). Now what is meant by clustering or how does it differ from classification? We saw earlier that there is a philosophical difference between classification and clustering, probably not in the n result but philosophically there is a difference. Of course even in the end result there are differences but the most marked difference is philosophically. That is   classification is based on amplifying the differences between different elements so as to make them belong to different classes. 











(Refer Slide Time: 17:48)

 

On the other hand clustering is based on amplifying the similarities between elements so as to form them into different clusters. So clustering essentially partitions the data sets into several clusters one or more clusters or equivalence classes. And what is the property of a cluster or an equivalence class? Essentially the property here is that the similarity among members of a given class in a cluster is much more than similarity among members across clusters. 

So members belonging to the same cluster are much more similar to one another than they are to some members belonging to some other clusters. And there are several measures of similarities and most of which are reduced to geometric similarity by projecting these data sets into hyper cubes or n dimensional spaces and then use some kind of Euclidian distance or other kinds of distance measures like Manhattan distance and so on and several distance measures to compute the similarity. 















(Refer Slide Time: 18:58)

 

Let us look at the first kind of clustering algorithm which is called the nearest neighbor clustering algorithm. This is quite simple that is this clustering algorithm takes a parameter called threshold or the minimum distance or the maximum distance t between members of a given cluster. So given n elements that is x1, x2 to xn and given a threshold t which is a maximum distance that can exist between elements of a cluster, we can find clusters in a very simple process. Initially the set of clusters is a null set. Then for each element let us say j equal to 1 here and j goes to, until j plus one here for each element find the nearest neighbor of xj. 

Now let the nearest neighbor be in some cluster if it is already in a cluster, if it is not in a cluster then fine you can just create another cluster by yourself. So suppose the nearest neighbor is in cluster m. now if the distance to nearest neighbor is greater than t that is if it is greater than threshold then we know that there is no other element that is nearer to me with a distance less than t. therefore I should belong to a new cluster so then create a new cluster and increment the number of clusters else assign it to the cluster m were the nearest neighbor of it existed. So, as simple as that. That is given a small threshold, you basically start partitioning your set of elements into different clusters based on which is the nearest neighbor to a given element. If the nearest neighbor is within this threshold distance then I join the cluster, otherwise I belong to a new cluster. 









(Refer Slide Time: 21:03)

 

There is another kind of clustering techniques which is again quite popular which is called as the iterative partitional clustering. This is another clustering technique where this differs from the nearest neighbor technique in the sense that here the number of clusters are fixed apriori. In the nearest neighbor technique or in the nearest neighbor clustering techniques, the number of clusters are not fixed apriori that means you don’t know how many clusters you are going to get, given a particular threshold and a data set. 

So this is very much unlike classification where we know the classification, where we know the classes under which data can be classified into. In iterative partitional clustering, the number of clusters are already known apriori and then we are trying to rearrange the clusters that is but that is we don’t know how many or what elements belong to which clusters. So, given n different elements and k different clusters, each with a center. What do we mean by a center here? It’s the centroid in the statistical sense, for example it could be the first centroid. That means if a cluster has several features, the average of all these features along all different dimensions will form the centroid of a given data set. 

So let us say we have k clusters each with a center. Now assign for each element, assign it to the closest cluster center. So each clusters has a cluster or a centroid. For each element, find out which is its closest cluster center and assign it to that cluster. After all assignments have been made, compute the cluster centroids for each of the cluster. That is compute the average of all the points that made up this cluster and possibly this will shift the centroid to a different to a different location. So once this centroid is shifted to a different location, the nearest centroid or the nearest cluster center will now differ for each element. 

Therefore we keep repeating these two steps, until the new centroid I mean with a new centroids that are formed until the algorithm converges. That is until the algorithm stabilizes so that the centroids will stop shifting and then we know that we have found the exact or we have found the best centroids for each of the clusters, each of the k clusters. so iterative partitional clustering essentially is a technique were something like saying, suppose I have a data set and I say that suppose I want to create 10 different clusters out of this data set, where would these clusters lie and so on. 

On the other hand, a nearest neighbor clustering technique would say suppose I have this data set and suppose I have a maximum distance, a threshold distance of 5 between   elements that can lie within a data set then how many clusters will I find. whereas in the in the iterative clustering algorithm, we are interested in where the clusters are going to be, where are the cluster centroids of these 10 different clusters that are going to be formed. 

(Refer Slide Time: 24:52)

 

Let us now move on further and look at different other kinds of data sets. We have been   looking into, until now we have been looking into let us say the tabular data as in apripri or association rule mining or some kind of multi-dimensional data. Tabular data can be treated as multi-dimensional data as long as they belong to certain ordinal classes which is of course beyond the scope of this session here that is how do we convert a tabular data into multi-dimensional data. But any way as long as the data can be converted to multi-dimensional form, we can use clustering techniques for clustering them into different clusters. 

Similarly tabular data can be used to also infer classification trees. Let us now move on to different kind of data what is called as sequence data. What do we understand by the term sequence? Sequence is essentially a collection of data elements wherein it’s not just the collection, it’s an ordered collection that is where in the ordering matters. 

That is in a sequence each item in a sequence has an index associated with it. That is some kind of a subscripted element, each element is a subscripted element. So this is the first element, this is the second element and so on. So when we say we have a k sequence, it means that we have a sequence of length k that is there are k different elements in a particular order in this.

there are different kinds of sequence data like for example any kind of transaction log over a period of time or let us say some kind of web browsing logs, http logs or DNA sequences or the patient history, the medical history of a patient over time that is how is the history changing or what kinds of events happened and so on. So all of these are sequence data. 

(Refer Slide Time: 27:13)

 

So let us look at some definitions in mining sequence data and which help us in formulating algorithm for looking at patterns in sequence data. First of all when we talk of a sequence, a sequence is essentially a list of item sets of finite length that is each element in a sequence need not be atomic, it could actually be a set, it could actually be a different set of items. So for example this is the sequence. The first element here is pencil, pen, ink or pen, pencil, ink. The second element here is pencil, ink. The third element is eraser, ink and so on and the fourth element is ruler, pencil and so on. 

So this sequence essentially for example could be denoting the purchases of single customer over time in this particular store or whatever. So let us say the customer came in the first month and purchase these three things, the second month you purchase these two and the third month you purchase these two and so on in some stationary store. 

Now the order of items within an item set here does not matter but the order of item sets itself matters. That is this is the first month, this is the second month, this is the third month, so the position of this item set matters but the position of items within an item set doesn’t matters. So whether I read this as pencil, ink or ink, pencil it doesn’t matter. And we define the term sub sequence, as any sequence with some item sets deleted from it. So, some more definitions. Suppose I take a sequence a1, a2 until am, this is actually a sequence it’s not a set, so this curly braces should actually be a, it should not be there.

(Refer Slide Time: 28:49)

 

So suppose I take a sequence s prime a1 a2 until am. we say that s prime is set to be contained within another sequence s, if s contains a sub sequence of the form b1 b2 etc bm that is m different elements such that each corresponding element is a subset, a1 subset of b1 subset equal to rather and a2 subset equal to b2 and so on. So, hence for example this sequence pen, pencil and ruler pencil is contained in this sequence. That is pen is a subset of this, pencil is a subset of this and suppose you take this out and create this sub sequence pen, these three as a subsequence then ruler pencil is a subset of this one. 

So, let us look at the apriori algorithm. I think called the apriori gen algorithm or whatever apriori all algorithm where it is applied for sequence data rather than item sets or association rules.












(Refer Slide Time:  30:06)

 

The apriori algorithm for sequences looks very similar to the apriori algorithm for item sets as well. How does the apriori algorithm look? First of all we set, we generate L1 that is the set of all interesting one sequences. What is the one sequence? A sequence containing just one element. And then when Lk is not empty when k equal to 1, we generate all candidate k plus 1 sequences and out of these, we take only the set of all interesting k plus 1 sequences. 

What is interesting k plus 1 sequence here? It is simply the set of all k plus 1 sequence which have at least the minimum support that we have specified and so on. Now the main question here lies in this statement here 3.1, that is how do we generate or what is the candidate generation algorithm? How do we generate all candidate k plus 1 sequences? 

















(Refer Slide Time: 31:14)

 

So how do we generate all candidate algorithms? Now given let us say different interesting sequences that is L1 L2 until Lk, candidate sequences of Lk+1 are generated simply by concatenating all sequences in Lk with all new one sequences found while generating Lk-1. What is this mean? Let us illustrate this with an example. 

(Refer Slide Time: 31:46) 

 

Let us say this is my data set and this data set let us say denotes, let us say I have a website and this data set denotes which are all the different pages that have been visited by users in different usage sessions. So one user a went from, one user went from page a to b to c to d to e and so on. Another user came from b and went to d and a and e and so on like this. 

So we have different sequences and of course as you can see here that an element can repeat in a sequence that is this user has requested for the page a 4 times one after the other and same thing here (Refer Slide Time:  32:55) that is after b, a is requested three times and so on for whatever reason. Now from here in order to look at, in order to mind for all interesting sub sequences that is what will be visited before what in this data set, let us start with the set of all interesting one sequences. Now we have set a minsub as 0.5 that is at least 50% of support. Now let us look at the set of all interesting one sequences. What is it mean to say interesting one sequences? Essentially it means that which all sequence of length one have appeared at least 5 times or more. So a has appeared 1 2 3 4 5 6 7 8 times in 8 different sequences, b has appeared 1 2 3 4 5 6 7 8 9 different times and so on. So a b d and e are interesting one sequences, c for example has appeared just once here, so therefore it is not interesting at all as a one sequence. 

Now we generate all possible candidate two sequences that is it is now rather than a combination, it’s a permutation that is where the order matters. So aa and rather it’s not a permutation, it is a concatenation rather that is concatenation of all possible concatenations that are possible between elements of this one. So ab is different from ba and ad is different from da and so on. So these are the set of all candidate two sequences. Now we just see which of these candidate two sequences have minimum support. 

(Refer Slide Time: 34:34)

 

Now among these you see that only ab and bd have a minimum support of 0.5. That is all others aa for example has the minimum support of 1 2 3 that’s it, not 0.5. That is one is here rather 4, 1 2 3 and 4, ab also has minimum support less than 5 and so on. 

So the only set of interesting two sequences are ab and bd in this case. So we have got the set of all interesting two sequences. Now how do we generate the set of all interesting three sequences that is candidate three sequences? We concatenate ab and bd with all the interesting one sequences found in the previous iteration. So the previous iteration here is still the one sequence here ab d and e. therefore we concatenate both of this with a b d and e like this and then we see that there are no interesting three sequences at all and then the process stops. 

Otherwise we would have filtered out few more elements here and then out of these, again we would have concatenated with all possible interesting one sequences that we found in the previous iteration. So here the interesting one sequences that we have found in the second iterations are a b and d. So for level 4 there is no need to concatenate it with let us say e, so it’s enough if we just concatenate with a b and d. 

(Refer Slide Time: 36:34)

 

With sequence data there is an other kind of interesting mining problem that occurs, when we look at a sequence data as a behavioral pattern. See for example when we say this is the way that users behave in a data, user behave in a website. The user here comes to page a then goes to page b then goes to page c, d and e and so on. Now we are encountered with a question as to can we model the behavior of the user. What would be a model that would explain me how users behave on my website?

So what this means is that we have to find out, suppose these are all the different strings   of a given hypothetical machine, we have to find out some machine which can generate all of these strings and possibly other strings that belong to the same class in whatever sense that is. So the question here is that given different sequences, treat this different sequences as strings that are generated by a particular machine. The simplest kind of machine that we can generate is the state machine or the deterministic finite automate or the finite state machine or whatever.
Now but that doesn’t mean that everything can be modeled by a finite state machine but it’s purely because of complexity considerations or practical considerations that we assume that the model representing user behavior is given by a finite state machine. So given a set of input sequences, we have to find out what is the finite state machine that recognizes this class of input sequences. This also called as language inference that is given the strings of a language, you are trying to infer the grammar of the language or you are trying to infer the structure of the language. Now what is the problem in language inference? What is the big, where is the trickiest problem that occurs in language inference?
 
(Refer Slide Time: 38:39)

 

Take a look at these strings. Let us say I have these four strings abc, aabc, aabbc, abbc so on. Now if I want to give you these four strings and tell you that create a state machine that will recognize these four strings. It is quite obvious that one would come out with the state machine like this which says which accepts these fours strings and exactly these four strings, so abbc, abc and aabbc and so on. So which accepts exactly these four strings.

On the other hand, one can also write a machine like this comprising of a single state which leads on to itself and accepts all strings like this. So this is a most general state machine that is this state machine is also correct in a sense that it accepts these four strings but it also accepts anything else made of a b and c in addition to these four strings, while this is a most specific state machine. That is this is a state machine that accepts these four strings and these four strings only and nothing else.

Now the challenge or now the trickiest problem in language inference is to find the right kind of generalization. That is if we make something into a most specific state machine, it will be of no use, while we make something into a most general state machine, it will be useless as well.
So when we discover or when we try to discover a model of user behavior, we should discover a model which is not too specific and is neither too general, it has to have the right kind of generalization. How do we do that? There are several different algorithms that try to generalize a little bit and not too much and not be too specific and so on.

(Refer Slide Time: 40:34)

 

We will just look at one specific algorithm which might be termed as the shortest run generalization that is generalize based on behaviors by using what is called as a shortest run technique of this thing. Now as we did for the previous algorithms, let us first look at the example and then come back to the algorithm. 



















(Refer Slide Time: 41:21)

 

Now the way shortest run generalization works is shown in this state machine here. Now let us say that we encountered different strings. Now let us say this is the first string that we encounter aabcb. Now there is no other string therefore we just build a state machine like this which accepts only aabcb and we haven’t seen anything else, so we can’t generalize anything else. Now second we encounter the string aac. So what this means is this state machine should accept not only aabcb but also accept aac. What does this mean? This means that start from aa and after aa if I get a c I can go directly to the end state, so it has to accept not just aabcb but also aac. 

Now let us say that I get the third string, even here i won’t be able to generalize anything. This is the state machine that accepts aabcb or aac, so we still haven’t generalize anything. Now let us say I encounter one more string of the form aabc. Now what is this mean? This means that aabc that is this string, that is this is a prefix of this thing. That is this is the substring of this thing, this is the prefix of string of the first one. So aabc this state itself should be a end state. So basically we come like this here and abc this becomes the end state.

Now what we do is we merge both of these end states, so b comes back like this. When we merge these end states, note that we have performed a specific particular generalization here. Now what is this machine recognize? This machine recognizes aabc b star that means any number of b’s after aabc. So essentially what it sees is that or any number of b’s after aac as well. That means it has seen a b appear after aac that is this substring aa and c with or without b included, it has seen that b may appear or not appear. And it generalized to the fact that any number of b’s may appear, including 0 number of b’s which may or may not be right that means to say that there might be an implicit, there might be some more hidden variables that says that at most 3 b’s can appear let us say 0 1 2 or 3 b’s can appear not 4 b’s but we don’t have that information here as such. 
So basically the state machine generalize to the fact that after aabc or after aac zero or more b’s can appear and we still lie in the end state but then we also see that when we look at the end state here, we look at the tails of all the edges coming into the end state. so there is a tail here which says c and there is a tail here which says c. now whenever from the end state it finds that there are two or more tails having the same suffix, these two the corresponding states are also merged. 

So what we finally get is aa b star c b star so that means what the machine generally is actually saying is that this language has to have two a’s to begin with, so it has two a’s and it can have 0 or more b’s following two a’s and then it should have a c and then it can have 0 or more b’s and so on. So because it has found 0 or 1 b’s between a and c and it has found 0 or 1 b’s after this c, it has performed this generalization. So this is one way of performing or trying to discover the behavior that is exemplified by a set of sequences. 

Let us look at the last kind of data set for this session namely streaming data. Streaming data has been of relatively newer interest among the data mining community and   especially since the streaming data or mining on streaming data has several interesting applications.

Now what is the characteristic of streaming data, what you understand by streaming data? You have let us say streaming audio, streaming video, network traffic and sever several other such data sets which are essentially large data sequences possibly infinite data sequences. in practice of course there are finite but possibly infinite data sequences and there is no or very little storage that is it is not practical to say that I am going to store the entire streaming data into a file and then start mining the file.

 (Refer Slide Time: 46:53)

 

Because this if it is infinite or if it is extremely large, it will be impractical, it could be tera bytes or even more bytes of data that could eventually accumulate into the file. So some examples are stock market quotes or streaming audio or video or network traffic and so on.

(Refer Slide Time: 47:12)

 

So in order to mine streaming data or rather even in order to let us say query streaming data, there is a notion of what is called as running queries or also what are called as standing queries. That means in a traditional database the data is standing, the data is there and the query actually slides through the data set in order to return you the answer. But in a streaming data set it is the query that is standing and the data streams through the query and then the query keeps returning you answers as and when the data streams through it.

So how do we write some standing queries or how do we find some aggregate behaviors based on some standing queries? Let us look at some simple standing queries, computing the running mean of a data stream. That is suppose I am getting a stream of different numbers and I have to calculate the average of these numbers as and when I read a new numbers, so it’s a running mean. So a simple way to calculate this running mean is like this, let us say I just need to maintain two variables here. One is the number of items that I have read so far or the number of numbers that I have read so far and the running average that I have calculated so far. 

So whenever I read the next number, all I need to do is first compute n times average that is average times the number of numbers that I have read so far, add number to it and divide it by n plus 1 and then increment the number of numbers that you have read or the number of items that you have read that is n equal to n plus 1, so as simple as that. That is as soon as a new number comes, you generate the sum, see n times average is basically the sum of all the numbers that have come so far. So generate the sum here, add the new number and divide it by the new that is number plus 1, n plus 1 as the new set of numbers that have come and then increment your set of numbers. 
Similarly this slide shows how to write a running query that computes the running variance. Variance as you know is the square of the standard deviation of a given data set. How do you compute standard deviation? That is it is for every element x, compute x minus x bar that is number minus average whole square and compute the sigma or compute the sum over all of them, all of these differences, so mean square distances essentially. 

(Refer Slide Time: 49:34)

 

So in order to compute the running variance, we look at this formula little more carefully. Variance equal to sigma of number minus average whole square where number ranges from i equal to 1 to n or whatever. Now, when you expand this, you can expand this into number square minus 2 times number times average plus average square. So essentially what this means is we have to maintain certain variables, one is sigma of number square. So, every time you read a number, square the number and add it to the previous sum that you have maintained. Of course you also have to maintain the number of numbers that have been read so far. Then you also have to maintain two times number star average of all numbers that have been read so far. 

So you know how to compute the running average, so every time you get new number compute the running average that is we saw how to compute the running average in the previous slide and then compute two times number times average and add it to this. So essentially you can take out average out of this and sigma of number or two times average out of this and you just basically have to maintain sigma of numbers. That is the sum of all the numbers that we have calculated until now and multiplied to the new average that we have found. 

And then we have to maintain, there is no sigma that is necessary here because average is a single number and we have to just maintain the square of the average of all the numbers that we have read so far and we know how to maintain the average. Now by maintaining all this, we can easily calculate the running variance that is you just compute each of them, put each of them in their corresponding places and compute the running variance. Therefore even if I have a long, let us say stock quotes from the stock market giving me   how the quotes of, how the stock price of a particular stock is changing I can maintain what is the mean stock price that it has recorded so far and what has been the variance and I can easily calculate standard deviation at any point in time by computing the square root of the variance. So I know how much it has varied over time and what has been the mean behavior of this stock over the entire time that I have read so far.

(Refer Slide Time: 53:10)
 
 

So this slide essentially shows how you can calculate the running variance that is whenever you read the next number first compute the average, we know how to compute the average then each of these is computed like this. That is A equal to A plus n square B equal to B plus two times average star n and C equal to C plus average square and variance is A plus B plus C. We shall also look at one more algorithm for streaming data essentially what is called as a gamma consistency or looking for events that have what are called as gamma consistency.












(Refer Slide Time: 53:49)

 

What is meant by this gamma consistency? Essentially the idea behind this is as follows. Suppose an event happens at some point in time. The interestingness of that event will be high in the vicinity of the event that is right after the event happens, let us say stock market crashes. The interest in that event will be high in the next few days but over a period of time, the interest that event starts going down unless of course the stock market crashes again. So that is the essential idea behind gamma consistency. That is first consider this streaming data to be in the form of frames where each frame comprises of one or more data elements. 

Then we look for some interesting events within a frame essentially let us say support based interestingness. So by let us say number of occurrences of k divided by number of elements in frame and then we see which of these events have sustained support over all frames rate so far with a leakage of 1 minus gamma. That means in every frame let us say every day or every week or whatever, we look at events that are interesting with a support of k.













(Refer Slide Time:  55:02)

 

And if this event keeps on occurring with at least this much support then you can consider this to be some kind of beaker where you are pouring in the events which are coming in with some kind of support and this beaker has a small hole underneath where in it leaks at a rate of 1 minus gamma. So over a period of time if you take it over a period of time, if and only if this event has a sustained support over time this beaker is going to be full or this beaker is going to have a particular level. And if the event does not sustain over time eventually, the beaker is going to empty itself.

So the level in this beaker is an indication of two things. One is how sustained is the support for this event and second could also be how recent was this event. So the more recent the event is the higher the level is going to be, similarly the more sustained the support for an event is again the higher the level is going to be. So you can calculate the level like this and then you can again put a threshold for this level and look at all events which have a particular level or so or level are higher at any given point in time.














(Refer Slide Time: 57:04)

 

So we now come to the end of this second session on data mining. We have just crashed the surface of what is a vast area of knowledge discovery from databases and we have kind of scratched it in a breadth first fashion that is we looked at several representative algorithms for different kinds of data mining problems whether it was a apriori or whether it was classification or clustering or sequence data or something like language inference and streaming data and so on. But this is just still the tip of the iceberg. So   anyway that brings us to the end of this session.

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 36

Object Oriented Databases

Hello and welcome. We were looking at the ongoing saga of, trying to understand data management in terms of database management systems. We have looked at several different topics in database management. However there is some kind of a common theme or an implicit theme in a database management in whatever topic that we are looking at namely that database or databases are primarily or essentially represented using the relational data model. So what is a relational data model? That is the data pertaining to the UOD or the universal discourse is maintained as a set of tuples or as a set of rows in a table. And the main assumption here is that every possible kind of data can be reduced to a set of tuples.

(Refer Slide Time: 02:34)

 

In this lecture and in the following lecture, we will kind of generalize on this assumption or we won’t accept this assumption and look at other kinds of database management requirements where data cannot be in a sense easily mapped on to the relational model. In this context specifically we shall be looking at the object oriented databases that is shown in the slide here. So let us look at what kind of data that we are talking about when we are looking into object oriented databases but before we begin, in fact object oriented databases have been very popular in the last decade of the twentieth century in the sense that in the 1990’s. 


But however they were not as widely successful as say the relational model database. Mainly because the object oriented databases do not have a sound theory that is they do not have a nice little mathematical model that describes the complete data model. As opposed to the relational data model where you have the relational algebra or the tuple relational calculus and so on where the entire data model is amenable to a nice theoretical framework. So it’s because of one of these reasons which is probably cited as the reason why object oriented databases did not sustain lot of interest. But however the contraries also true to some extent that is object oriented databases have been in use or have been put to use in several different applications mainly CAD application, computer aided design of say electrical circuits or mechanical circuit, mechanical design and so on. And they continue to be used and there are quite a few commercial implementations of object oriented databases. 

So in this lecture and the next when we are talking about object oriented databases, we kind of implicitly assume an example application of a CAD that is computer aided design scenario where users would be using computers to perform electronic design or electrical design where an electronic design comprises of several different components, I might have an IC, I might have a capacitor, a resistor, a transistor and so on. Each component having its own characteristics and having its own behavior and so on.

(Refer Slide Time: 05:07)

 

But that’s the implicit assumption that we are going to make but that doesn’t really necessarily mean that object oriented databases are suitable only for CAD applications, of course there could be several other applications as well. So let us come back and look into what kind of complex data objects that we are talking about when we say that we are going to generalize or we are going to move away from the relational model and look into other kinds of models.

Have a look at the first example here say it’s a multimedia databases. What do you understand by the term multimedia databases? Databases that store multimedia objects. What do we mean by multimedia objects? You might have, you would have encountered several kinds of multimedia objects, if you have let us say work with any GUI based operating systems like say windows, windows XP and so on where you encounter objects like menus, scroll bars, drawing areas then something like, when you click something that there is a sound that appears and there is a for specific kinds of events, specific kinds of sound and light so to say, a messages are thrown to the user and so on and there is flash animation and so on. 

And look at the second kind of application that we have talked about that is namely the CAD. What would the typical CAD application for electronic design comprise of? A typical CAD application, in a typical CAD application the user should be able to let us say select a PCB a printed circuit board or select a resistor or a capacitor or a transistor or a particular kind of IC and voltage source and control grounds and current sources and so on and so forth. 

Now the main theme or the common theme between these two applications is that both of these applications are made up of fundamental objects which form the building blocks of these applications. So multimedia applications are built from several of these different objects, they could be menus or scroll bars and so on.

(Refer Slide Time: 07:31)

 

Similarly CAD applications or CAD projects in a sense or multimedia projects and CAD projects are in a sense built using this fundamental objects. CAD projects would have a PCB or IC and so on and so forth. So what are the characteristics of these complex data objects? It is quite apparent that these data objects are not easily amenable to reduction to a tuple. That is we can’t really reduce all of this to one set of, one tuple having a set of characteristics because there is much more to an object than a set of different attributes. 
So what is this, what comprises this much more essentially the idea of behavior of an object? A transistor for example behaves as is represented not only by a set of attributes saying what kind of a transistor is that or is it PNP or NPN or so on and so forth whatever else goes into describing the attributes of the transistor, in addition the transistor also has a particular kind of behavior. You can apply voltage at one of the pins and measure the voltage at one of the other pins and so on and so forth. 

So an object does not simply represent a set of attributes but it also abstract, rather an object is not merely a structural abstraction but it is also a behavioral abstraction. That is when I say transistor, the kind of behavior that the transistor emulates is also abstracted by the object. And in an object there are several instances of an object that can belong to the same class. So I could have several different instances of the same transistor and each of this different instances may have different set of attributes at any given point in time. That is the instance variables of each of these different transistors could be different that means each of the transistors belonging to this particular class has different states at any given point in time.

(Refer Slide Time: 09:56)

 

So let us briefly take an overview of object orientation concepts. Object orientation concepts, here I am essentially looking at from object oriented programming point of view. The idea of object orientation came from programming and there say several kinds of OOPLs or object oriented programming languages which were started right from the early 70’s and so on.

So let us look at object orientation concepts from a programming point of view and then we shall look into how each of these changes when we consider object oriented database systems. Now the fundamental building block in an object oriented system is of course the object but then an object represents an instance and an object belongs to a particular type and here this is called a class.
So an object can belong to a particular class or rather we define specific classes of objects and then we instantiate different objects of specific classes. For example we could define a class of objects called cars and we can instantiate an object of type car which specifically points to one specific car rather than the type of all possible cars. Then the idea of an object is to provide an abstraction to the user. So depending on what the application is when an object of type car is created, it represents an abstraction called car. That is each car is supposed to have a certain properties not just structural properties in terms of what attributes they have but also behavioral properties, what can you do with the car and so on.

For example take something like menu in multimedia database. So menu is an abstraction that is it not only says what are the attributes that make up a menu but what is the behavior of the menu as well. That is the menu should provide a list of items and there is a default selection by the menu and the user should be able to scroll up or scroll down the menu and so on and so forth. So an object provides an abstraction or an object emulates an abstraction of not just structure but also of behavior. And how is this abstraction provided? Through the notion of encapsulation that is an object encapsulates structure and behavior within its fold. That is an object is defined by a set of attributes which define the structure that is described by the object and a set of methods or function calls that operate on these variables which define the behavior that is abstracted by this object. And of course there is the notion of an interface. That is interface is also called the signature of an object that is an object only exposes or the only thing that theoretically at least that is exposed to the outside world is the interface of an object. 

And all external world entities should interact with the object through the interface by calling particular methods and changing attributes and so on. So again just to give an example or just to give an analogy, a car for example gives an interface in the form of a steering wheel and a gear. So you can interact with the car only through the steering wheel and the gear and of course the pedals that is the brake pedal and the clutch pedal and so on. You cannot directly go, when you are driving a car you cannot directly go and manipulate how the engine behaves for example or how the wheels behave and so on. You have to deal with the car through its interface

So interface is the, as far as the user is concerned the interface is the signature of the object. If you want to learn to drive a car, you should know how to handle the steering, how to handle the break and clutch and gear and so on and so forth. So you should be able to know how to handle the interface rather than what lies within the interface. And of course interface, in software interface is made up of methods which are function calls which change the state of the object and methods themselves have particular signatures that is each method requires zero or more input parameters and has 0 or 1 output parameter as well so that forms the signature of a method. And attributes of a class is the set of variables that define the state of the class for example again in a car the attribute would be something like which gear the car is in or what is the speed of the car or what is the acceleration of the car and so on and so forth which basically describe what is the current state of the car.
So any method call would change the or would influence the attributes of this object. And object state of course is a function of what are the values of different, each of these attribute. So the state of an object is something like let us say the state of the car can be defined as say cruising when speed is so and so and the gear is in over drive and so on and so forth. So basically you define set of values and say now if these are the set of values then this is said to be the state of an object. And there are again some more concepts pertaining to object orientation which may also be important. That is some notions of say message passing.

So when an external world entity invokes the method of an object, it is said to have pass the message to the object and the message in turn invoke the object that is invoke method of this object. And some more concepts of object orientation which are particularly useful are the notion of inheritance, polymorphism and over loading. That is when we define a class, we can define a generalization specialization relationship which we also saw in let us say the enhanced ER model where a generalized class represents a general, more general entity than its specialized classes.

(Refer Slide Time: 15:21)

 

That is wherever an object of a generalized class is required, it should be safe to substitute it within an object of a specialized class. Therefore suppose I have a generalized class called say SUVs or whatever sports utility vehicles and so on. So and there could be different classes of SUVs whatever Qualis and Scorpio and so on and so forth, so several different kinds of SUVs. And what constitutes a correct generalization specialization relationship? Wherever I need an object of the generalized class it should be safe to substitute it with an object of the specialized class. So the specialized class is said to have inherited properties from the generalized class and of course extended on the properties or over ridden on the properties and so on. So a specific property or a specific method for example again coming back to CAD databases or which we said we are going to have a running example.
So suppose I have a CAD database and I have a generalized class called say a transistor and I have a specialized class called a specific kind of transistor of some particular number. So whatever behavior is specified by the generalized class can be actually overridden by the specialized class and in a sense the same method signature seems to be giving different kinds of behaviors depending on which object of the specialized class is substituted. So that brings in the notion of polymorphism that is the same signature, method signature giving rise to different kinds of behaviors that emanate from the system. And there is also the notion of over loading which is a feature that present in many object oriented languages where slight changes in method signatures can be used to perform different classes of the same activity. 

For example we can say something like add. Now when I say add to, I can say add int, int where it takes in two integers and gives out an integer. Now the same add could be defined as int, float or float, int or float, float where you can add different combinations of integer and floating point numbers and return back. So at run time the message passing framework is going to determine which kind of add is being called depending on what is the type of the parameters that is passed. And then there are pure object oriented languages where everything is an object there are no what are called as native types. So every single entity like an integer, every integer or every character is an object and there are no native or fundamental data types other than objects. And then there are hybrid object oriented programming languages where which do allow native types and of course what may be termed as semi object oriented programming languages where you can perform both object orientation and procedural programming in the same language.

(Refer Slide Time: 20:05)

 

Now let us come to object orientation as pertaining to databases. Now what extra features to be required in the concept of object orientation, when we talk about databases? The main concept that is required for databases is the notion of persistence of an object or persistent object. What is the persistent object? A persistent object is something that can exist persistently or permanently that is the objects can exist even after the program using the object has finished. That means the object exist on some persistence storage like disk and can be recreated or can be reread back from disk whenever required. 

Now for storing persistent objects in object oriented databases, another important requirement is the notion of an object identifier. That is it is important to uniquely identify each persistent object that is stored in the database. You might think of this as a as a primary key as in that we discussed in relational database systems but there are some slight differences between an object identifier or an OID versus a primary key.

An object identifier or an OID is automatically created by the system whenever a new object is added to the system, whether the user specifies it or not. On the other hand it is the user who specifies what forms the primary key in any database relation. And of course in pure relational algebra, each tuple is unique that is a table is a set of tuples not a bag of tuples. Therefore in the worst case the entire tuple form a primary key for the table. However object identifiers are separate attributes that is the entire object cannot form the or cannot uniquely identify given object. This is because two or more objects belonging to the same type can have the same state and hence be indistinguishable as far as their other attributes are concerned. But they still would represent two different objects for example I can always have two different transistors in any given circuit board which have the same input voltage at the same time or same input or output voltage at each of its pins at the same time. However they are still different transistors.

So by default an object database is a bag of objects that is all attributes of an object need not necessarily uniquely identify an object. So we necessarily require an OID or an object identifier. And the way objects are stored in databases are as far as possible should be in direct correspondence to real world objects. So I can store an object like transistor or capacitor or PCB or whatever, so where you have direct correspondence to what one can see tangibly in the real world. And of course there are several different attributes that define an object which alter the state of an object and of course these variables for attributes are defined at a class level whereas when an object is instantiated, these become instance variables. And the instance variables of different objects could be different even though they belong they represent the same attributes. And just like an object oriented programming languages, objects are defined by signatures which are the interfaces of objects and even methods have signatures that are defined for the objects.

And every other notion in an OOPL are also reused here something like the inheritance and reuse of objects that is when you inherit the base, the derived class or the specialized class reuses certain properties of the base class that is because it inherits certain properties of the base class, we can think of it as some kind of reuse.






(Refer Slide Time: 24:05)

 

And there is also a notion of referential integrity in object oriented databases by the use of OIDs. That is every OID, suppose an object a refers to another object b, this reference is captured by putting the OID of object b as an attribute of object a. And referential integrity is enforced by ensuring that at every point in time, the OID that is represented as an attribute in any given object is always a valid OID. And of course there is operator polymorphism and overloading and other concepts that are common in OOPLs. 

So let us come back to the object identity aspect like I said before the OID is mandatory in any object oriented database system. And this is usually a system generated unique identifier that is the user need not even be aware that there is an OID that is created for each object. However the system by itself creates unique object identifiers and of course the OIDs have no relationship to the values of attributes. That is the set of all values of attributes of an object need not necessarily, uniquely identify a given object.















(Refer Slide Time: 25:53)

 

And usually OID is a logical number and it is not advisable to base OID on the physical address of an object. Suppose I have stored object in a particular directory tree, we should not keep the directory tree as the OID of an object because of several reasons like if the database is migrated to a different system or if the directory tree is changed then the OID changes and the object becomes in accessible.

Now like I said before every instance of an object is characterized by a state of an object. Now how do we define the state of an object in terms of the database or in the database parlance? This slide shows (Refer Slide Time: 27:00) a formal model of how the state of an object is represented or in a sense the structure of an object. The structure defines the state space in a sense so the different kinds of state that an object can be. So an object structure is defined by a triple comprising of three values i, c and v where i is the object identifier and c is what is called as the type constructor and v is the object state. So i is the well-known OID that we have been talking about and c can be, c is what is called as the type constructor that says what type of type in a sense or what type of value is this going to be. And usually object databases define different kinds of type constructors like atom and tuple, set, lists, bags, arrays and so on.











(Refer Slide Time: 27:52)

 

An atom type for example defines a specific atomic value. So I can say atom and then give a value of 5 for the value. So this object represents an atomic entity whose value is 5. On the other hand I can represent a tuple also as an object. So instead of one single value, a tuple represents a list of values, an ordered list of values and list of atomic values essentially. And a set is an unordered set of values, unordered collection of distinct values that we can take up. And list is similar to a tuple except that in a tuple, the size is fixed, the size of a tuple is fixed but in a list different instances may have different sizes for the sequence of values that we can take and bag of course is a multi-set that is a set with a set with repetitions and so on. 

(Refer Slide Time: 29:27)

 
So this slide shows some examples here where we defined all these things already that is when type c is atom, object state v would be one particular value from a domain of basic values and when it is a set, it is the set of values and so on. 

Now this slide shows some examples here. Let us say I define an object O1 as a triple where i1 is the OID of the object and the object is of type atom and the value of this object is Chennai. That means this object essentially stores an atomic value or an atomic entity called Chennai as part of this object. 

(Refer Slide Time: 29:36)

 

Similarly O2 has as OID of i2 and it stores an atomic value called 35 and O3 is a tuple wherein each element of this tuple is an OID that is i1 is a OID belonging to the class called PLACE. So O1 that is i1, look here that i1 refers to this i1 here. So this i1 is an object, i1 basically represents an object called O1 and O1 belongs to a class called PLACE. And similarly i2 represents an object called O2 which belong to a class called num. So this O3 is a tuple of different OIDs where different, in a sense it’s a composition of different objects of different types in the form of a tuple.

Similarly O4 is a set comprising of 3 different OIDs O1, i1, i2 and i3. So as you can see here, it is possible not only to represent specific atomic entities it’s also possible or rather or even collection of atomic entities, it is also possible to start composing objects, one object inside another. For example O3 in a sense is a composition that is made up of O1 and O2. And similarly O4 is a set or a collection that contains all three elements that is O1, O2 and O3. So depending on these kinds of associations between objects whether it is a composition association or some kind of a whatever other kind of association that we can define, an object database can actually be represented as a graph structure, so where each object in turn has some kind of an association whether it’s a containment or inheritance or some other kind of an association with other objects in the database.

And when we are talking about the states of objects, remember I had mention that two or more objects may have the same state but that doesn’t necessarily mean that they are the same object because as long as their OIDs are different, they essentially refer to different objects. So this slide shows such an example (Refer Slide Time: 32:41). So at any instance of time I may have two different objects O1 and O2 whose states are the same. That is they represent one atomic value whose value is 35 and there is one more object of the same type called num that we defined in the previous slide which represents a value called 20. However even though both of these have the same state and this has the different state, all three are different objects namely because the OIDs are different i1 i2 and i3.

(Refer Slide Time: 33:17)

 

Similarly, here these objects i1 i2 and i3 or i2 i3 and i2 or whatever, so these two O4 and O4 here have the same state that is a1 i1 a2 i2 a3 i3 and in this case it does represent the same object. Why because i4 is the same as i4 here so at the end of it is just this OID which determines whether two objects are the same even when this regardless of what is the state of this objects. And different object oriented database systems provide different mechanisms for defining custom types or custom classes.











(Refer Slide Time: 34:09)

 

So here this is some kind of pseudo code for particular kinds of object database systems and later on we will be looking at one particular standard for representing types, namely the OMDG standard. But the idea here is that the user can define his own types. For example the user defines an object of type employee comprising of different attributes that is there is first name, last name, salary, supervisor and so on. And so this forms the tuple of different attributes that represents an object of type employee. 

Similarly, there is nested declaration here that is department is a tuple comprising of department name and department number and manager which itself is a tuple comprising of an OID of an object of type employee and start date and so on. So that was, you might have got a question now that what is the difference, what really is the difference between declaring objects or types using what we saw here and with the relational data model itself. That is both seem to be different ways of doing the same thing that is defining a set of attributes.














(Refer Slide Time:  35:37)

 

However object databases differ from, in one important factor from relational databases namely that of object behavior. So you need not, when you defining a type of an object like say employee and department, it is not just the attributes that you define but also the set of behaviors. So let us look at what is the importance of behavior when it comes to object oriented database systems. 

Now object behavior is abstracted by a set of methods and which is visible as the object interface to the external world. Now the interface as I said before is also called the signature of an object that is each object should have a unique interface, each class which uniquely identifies what are the kinds of behavioral abstractions that it provides. 

So for example if I have a object of type IC of a particular IC type, let us say some kind of let us say logic gate IC 7404. So this object has particular kinds of behaviors that is you can provide input voltage to a particular pin and you can provide ground to a particular pin. You can provide inputs, logic inputs to particular sets of pins 7404 basically implements AND gates and so you can basically provide logical inputs to certain pins and get logical outputs from certain pins and so on. So probe an input pin or input voltage to a particular pin, all of these are methods that are abstracted by the object. 

And of course when we are talking about attributes itself, by default or in pure object orientation, every attribute of an object is actually hidden from the external world. That is the external world can access an object only thorough its interface or only through its method declarations. But in reality though some attributes are visible to the external world, while some attributes are hidden from the external world that is which can be accessed only through method interfaces. 

In most object oriented databases, the database management system allows the user to specify the interface of an object along with the attributes like the attributes here (Refer Slide Time: 38:17) first name, last name, salary and so on. The user can also specify a set of interfaces and the implementation of these methods that is method declarations are provided here and the definition of these methods or the implementation of these methods can actually be returned or can be returned elsewhere using a programming language or method definitions can be done using any standard programming language like C plus plus or java or so on. 

So the object database itself does not provide primitives or need not provide primitives to define methods but rather you can actually use an existing object oriented programming language in order to define a method interface. So defining a method interface, now I mean embedding methods in addition to attributes will now enable us to define a class rather than a particular type.

(Refer Slide Time: 39:21)

 

So one can define a class here for example this slide shows the definition of a class called department where the classes certain attributes like tuple which is a tuple of attributes which basically contains department name, department number, manager which is another tuple in projects and so on. In addition there are certain attributes, certain operations that are also defined like number of employees is, number employees is the name of the method which returns an integer. So when the external world calls this method, an integer is returned which essentially says what is the number of employees in this department. 

Similarly create department which is what is also called a constructor method that creates and instantiates an object of type department and put some default values in several, one or more of these attributes and assign employee that is add an employee to the department and so on. So when operations are defined in addition to attributes, we get the definition of a class in contrast to a type. And object persistence, so how are objects themselves persistently stored and referenced uniquely? Of course at the implementation level, the object database system uses the notion of OIDs that is when we refer to an object and if it is a valid object in the database it is given a unique OID, object id identifier.

(Refer Slide Time: 41:13)
	
 

But what is the abstraction? That is the OIDs completely hidden from the user that is the user or application program that is using this let us say the CAD tool does not or need not have to know the OIDs of each object that have been instantiated and stored in the database. Instead the application program refers to each object by different kinds of mechanisms. One well known kind of mechanism is by the use of a naming mechanism that is I can refer to a particular object like say IC 7401 or 7404 number 2 whatever. So each specific object of 7404 that I have created can be given a specific number or this is the first IC or second IC and so on. And like that, using that as a mechanism, the application program can uniquely refer to each object and the each unique name in turn translates internally to each unique OID.

On the other hand there is another kind of mechanism by which objects are referenced in in an object database system by the notion of reachability. That is it may be difficult to give a unique name for every object that is stored in the database system. For example if my let us say I am storing the circuit of a big computer like this, now it has several hundreds of components and this particular circuit is part of larger database of circuits and each different each of this different units or each of the object that are stored in this database has to be given a unique name and which might be impossible I mean it may not be a practical thing to do. 

So another way of representing or referencing objects is through the notion of reachability. That is let us say that one particular element in a circuit can be reached only through another particular element. Let us say transistor x can be reached only through the pin number 5 of this IC or whatever. So we don’t, in such cases we don’t give a unique name to this transistor and instead we contain with just the name of the IC and any other object that can be uniquely reachable through the IC can be uniquely identified by naming the IC and then following the links. So reachability essentially defines a sequences of references in the object graph that would leave from a well-known or named object A to an unnamed or reachable object B. So this slide shows an example (Refer Slide Time: 44:16) where in some object oriented database systems where some objects can be declared to be persistent that is when we are working with an object oriented database system lets say CAD application, a CAD application is built around an ODBMS. And the way of working with an ODBMS is seamless that is the user would be writing the application program and as part of the application program itself, the user would be interacting with the object database system. So for example there are several, let us say there are several class definitions that make up this application program.

(Refer Slide Time: 44:59)

 

And let us say one of these objects defined by this class should be persistent that is should be persistently stored in the database. So you basically define this objects say all departments which is a persistent named object of type department set which is defined in the class here. So as part of your application definition itself, you define which object should be persistent and which objects can be transient that is they lose their identity or they lose their state when the program finishes execution. And of course this is using the Pascal exsyntax where you can say d equal to create department where create a new department object in this variable called d and then make d persistent by adding it to a persistent set called all department so and then save it to the database system.

So in addition to these different features that are provided by an ODBMS like say type definition, class definition, method definitions and naming conventions and reachability and persistence and so on. There are other kinds of features that are available in an ODBMS of what are called as type hierarchies and inheritances and so on. So the concepts here are more or less analogous to the concepts in OOPLs itself that is whenever I use a type hierarchy, I am essentially referring to a generalization and specialization relationship.

(Refer Slide Time: 46:40)

 

So a type hierarchy is defined by a subtype and a supertype that is I can define something like a student is a subtype of person and so where I could have defined person as a tuple comprising of name, address, age, social security number and so on. And I can define student as a subtype of person where it contains all attributes that make up a person. In addition there are attributes called branch and GPA which are important for defining a student as well. And similarly I can one can define inheritances that is an object of type rectangle as a subtype of a GEOMETRIC_OBJECT which is not just tuple here but tuple comprising of tuple in addition to certain behaviors.

And then you say a rectangle is defined by width and height in addition to whatever makes up GEOMETRIC_OBJECTS. Then there is the notion of an extent in object oriented database systems. Extents is in some way to give an analogy to ER modeling, in entity relationship modeling we had the idea of entity types and entity sets. An entity type defined a type or a class of entities while an entity set is an entity type coupled with a collection of different instances of this entity type. So the concept that is used here for an entity set is an extent. That is extent is a collection of objects of the same type that is a type definition plus a collection of instances forms an extent.








(Refer Slide Time: 48:47)

 

So the object database system is organized in the form of extents, different extents that is different typed objects are stored in their own extents and then usually because in most object oriented languages there is always a type hierarchy and there’s usually a root class or like in java there is what is called as the objects class. There is a default extent that every object belongs to which is the object extent or the root extent. And depending on the class hierarchy or the type hierarchy there can be different sub extents that can be defined on each of these, depending on the class definitions of each of the object that are stored in the database. 

And of course the way in which objects are stored can either be structured complex objects, I mean now we are explicitly calling it complex objects that is objects which are not necessarily amenable or data that is not necessarily amenable to storage in a relational database form. So, one can think of structured storage of a complex object or an unstructured storage. Structure storage essentially is some kind of a nested structure, a tuple comprising of other tuples or sets and so on. So, some kind of structuring that is made out of the types that we define or the constructors, type constructors that we define something like atoms and sets and tuples and lists and so on.











(Refer Slide Time: 49:59)

 

On the other hand there could be unstructured complex objects where especially multimedia objects where I could have a video sequence or an audio sequence and so on where there is no specific structure as such but it’s just one heap of data or binary data that makes up this object. And there also called as BLOBS or what expanse to binary large objects. So they are just binary data which are just stored and stored in the database and defined as part of this object. 

Now there are several different object database standards that exist and there was several different commercial implementations of object oriented database systems. But many of them have in a sense gone out of business but quite a few of them have still survived and like I mentioned earlier, the main at least as of today the main application area in object oriented database systems is in CAD applications where we need to store objects of a particular, having not only particular properties, a particular structural properties but also behavioral properties. And this behavioral properties or the abstraction of this behavioral properties are extremely important when trying to build let us say an electronic circuit or a mechanical design and so on as part of a CAD application.

Until now we have been mainly talking about object database systems from a pseudo code perspective. That is these are the features that several of these object oriented database systems have or had in a sense but more concretely there have been few standards that define what an object oriented database system should look like.







(Refer Slide Time: 51:29)

 

And among them a well-known standard is the O ODMG standard that is the object data management group standard. And the idea of the standard is to enable certain kinds of features or certain kinds of properties that make up an object oriented database system. So that they can be seamlessly ported across different object database management systems or ODBMS. And the main idea behind the standard is the interoperability between different ODBMS. And ODMG 2.0 defines several different concepts, it defines a basic object model and an object definition and a query language and it also defines different kinds of bindings to programming languages.

So let us briefly look at what are the main or salient features of the ODMG 2.0 standard and in the interest of time and brevity, we shall not be looking into great, we shall not be looking in great details into the standard but rather look at what are the main features that are provided by the standard.















(Refer Slide Time:  54:18)

 

And ODMG standard provides this, basically the idea here is the standardization of terminology. 

(Refer Slide Time: 54:26)

 

And as shown in this slide, objects in the ODMG standard are defined by these different entities that is name, identifier, life time and so on. And it also defines several types of attributes atomic attribute, collections, structures and so on. 



(Refer Slide Time: 54:44)

 

And interface definition is more or less the same that we saw as in the pseudo code and there are several default objects or default classes that are defined by ODMGs standard.

(Refer Slide Time: 55:06)

 

And of these a very interesting default object is the or default class is the collection class which defines a collection of different objects. And this collection class has several methods that are defined like, you can query the cardinality of a collection, you can query whether the collection is empty or you can insert an element into a collection or remove an element from a collection and so on and so forth.

(Refer Slide Time:  55:26)

 

And there is specialized built in, collection objects can further be specialized into different kinds of these, you can specialize a collection as a set or as a list or a bag and so on so. All of these inherit the collection interface.

(Refer Slide Time: 55:41)

 

So let us not go into each of these in detail.




(Refer Slide Time: 55:43)

 

(Refer Slide Time: 55:46)

 










(Refer Slide Time: 55:47) 

 

(Refer Slide Time: 55:48)

 










(Refer Slide Time:  55:55)

 

Let us on the other hand look at some of the type hierarchies, look at the main type hierarchy that is defined by the ODMG standard itself like in java there is a root object in the ODMG standard which is object that is every object belongs to this class called object, root class which is called object. And then collection is a special class of objects which represents a collection of objects and then there are several other kinds of objects like date, timestamp, interval, set, bag, dictionary and so on and so forth. So this is a partial type hierarchy that is defined by the ODMG 2.0 standard. 

So let us summarize what we have learnt in this session today. We talked about complex data objects and how they need not be amenable to a relational, reduction to a relational storage. So we looked at object orientation concepts and wherein the notion of a state of an object and the OID of an object become important in order to be able to store objects. Then storage of objects are called persistent objects and how they can be accessed through naming and reachability and so on.














(Refer Slide Time: 56:36)

 

And then we also looked at the ODMG standard ODMG 2.0 standard which defines its own class hierarchy of different classes. So that brings us to the end of this session. Thank you. 

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 37

Object Oriented Databases II

Hello and welcome. In the previous lecture we were looking into new kinds of data management problems where the kind of data that we have may not be easily amenable to reduction to a relational model. We were specifically looking at CAD databases that is databases in computer aided design where the UOD or the universe of discourse is better described by a collection of objects and some kind of relationships between objects rather than a pure relational schema.

(Refer Slide Time: 01:39)

 

And how is an object different from a tuple in a relational schema. Well, a tuple is part of an object but an object basically a tuple is an abstraction for particular structure of attributes, the way in which attributes are put together to form a tuple. But then an object is an abstraction for not only structure but also behavior. That is an object represents some kind of a complex data entity like say an electronic component like an IC or a transistor or something like that where it’s not just the attribute or it’s not just the structure that’s important but also what kinds of behavior that the object performs.

I mean you can’t expect a transistor to work as let us say something else I mean, I don’t know, may be some kind of a current source in some other form. That is there is a specific set of behaviors that are associated with an object. And in conjunction with the attributes, it’s also the behaviors that the objects provide an abstraction forum. And we also saw different notions that define object oriented database systems for example in an ODBMS it is mandatory or it is essential to have a unique identifier for each object, for each persistent object in the database and this is exemplified by the OID relationship. And OID is contrasted from a primary key in a RDBMS by the fact that an OID need not be explicitly specified. That is the user need not even be aware that the database system is tracking each object using an OID. And just because the state of two objects are the same that is they have the same set of attributes and the same set of values for each attributes doesn’t necessarily mean that they are the same object which is very much unlike the case in relational algebra were two tuples that are the same, represent the same data object essentially or the same data element. But here even if two or more objects have the same state, as long as their OIDs are different they represent different objects.

And then there are other issues like say inheritance and polymorphism and so on where or type hierarchies were an object can actually derive or specialize from a more general object and essentially as correct specialization is one where, wherever in the system you require an object of the general class it should be correct to substitute an object of the specialized class as well. 

And there are other issues were the database system can be represented as a graph by associating different objects based on different relationships like containment and inheritance and other kinds of association. And associations are made using OID references that is object A associates with object B if the OID of B is an attribute of object A. And an ODBMS essentially is tightly integrated with OPL or an object oriented programming language where there are some kind of programmatic constructs that are provided to the language where the programmer who is writing the application program can identify certain objects to belong to the database in the sense that these objects can be declared persistent as soon as they are defined that is as soon as they are instantiated.

So as and when they are instantiated, they are also associated with an object in the database rather than just in the programming system, other objects which are not persistent are called transient objects. Now having looked into all those, we started looking into some of, we started getting little bit more concrete and started looking into the object database management group standard, there is the ODMG 2.0 standard for defining object databases.













(Refer Slide Time: 07:01)

 

Let us look at this standard in a little bit more detail today or in this lecture and look at what other options does, does the ODMG standard provide. First of all let us look back at what is the ODMG standard which is slow shown in the slide here. The ODMG standard or the object data management group standard 2.0 model is a model essentially meant to standardize the notion of object databases. And the main idea or the main reasons for this include portability and interoperability between different object databases. And ODMG 2.0 standard defines an object model and a language for defining objects and querying objects. 

So object definition language and object query language plus it also defines a number of bindings to existing programming languages like C plus plus and Smalltalk and so on where application programmers programming in any of these languages can directly interface with ODMG objects or in ODMG databases system. So essentially like we said before, the object model is meant for meant to be a standard so that the terminology is standardized and then which in turn help aids in interoperability and portability of database models.












(Refer Slide Time: 07:56)

 

So in the ODMG 2.0 standard, objects are described by a set of different values of course an object has the well-known OID or the identifier and an object has a specific name that is associated with it plus also a life time. Life time in the sense you can either call an object as transient or persistent. So a persistent object is said to exist even after the software or the application program has finished its execution in a sense permanently and of course the structure of the object. And like we saw in the previous session, there are different kinds of data types that are usually supported by an ODBMS.

(Refer Slide Time: 08:34)

 

And the ODMG 2.0 standard supports different kinds of object types like, literal types like atomic attributes or atomic objects, collections and structures. This slide shows a typical interface definition of an object in an ODMG.

(Refer Slide Time: 09:22)

 

Remember that the interface is the signature for a persistent object. That is the interface tells the external world how to interact with the object and the interface for example shows here, the interface does not show type declarations. However it is showing the operation declarations like there is an operation called equals which takes in an object of type object and returns true or false and there is a method called copy and a method called delete and so on. And the ODMG 2.0 standard defines a set of or also proposes a predefined set of classes along with the class hierarchy. 

















(Refer Slide Time: 10:19)

 

And one of the classes in the class hierarchy is the class called collection. And as we saw again in the previous session at the top of the class hierarchy is a class called object. So every object in this, in an ODMG database by default belongs to the object class. That is even if you can’t resolve in any other way which a class that a particular object belongs to, you can always say that an object of the ODMG database belongs to the root class which is the object class.

A collection is another derived class or a sub class of the object class which actually defines a collection of different objects that is a collection class can have a set of different objects or objects of the object class. And then the collection class also defines a number of methods like say cardinality of a collection, is_empty which checks for whether the collection is null or is empty then it supports insert_element where you can add an object into a collection, remove _element where you can remove an element from a collection, contains_element the searching element for a particular object and creating_iteratives. We will come to iterative soon, what exactly is meant by an iterator and what are its use.

If you have program with this standard template library in, say in C plus plus for example you probably have guessed what an iterator is where it’s a kind of a template using which you can iterate over several different objects of any kind of a collection. There are other kinds of built in interfaces that are also specified by the ODMG standard and collection objects can also be further specialized into different kinds of these classes like say a set of collectors I mean set of collection or list or bag or array or dictionary and so on.






(Refer Slide Time: 12:25)

 

So, all these are derived from the collection interface collector collection interface that is the collection class interface. We now come to some iterator mechanism that is or rather the requirement for iterators. If you are familiar with C plus plus programming, you would have probably come across the notion of a template wherein a template is in some sense a definition with a hole or a method or a operator definition containing a hole where different other objects can come and fit into the hole or different other objects of other kinds of classes can come and fit into the hole.

(Refer Slide Time:  12:59)

 

So for example if I define a template called list, I can have a template I can have a list of different objects of class a or objects of class b or any of any kinds of objects. So similarly the ODMG standard defines different template, the first of which that we are going to see is the set template. So set as shown in the slide here (Refer Slide Time:  13:55), set followed by a template mechanism here will create a set of classes of this particular type. 

Note the subtle difference between a template like this and a collection. A collection by default is a collection of objects that is it just reads every entity in the every entity in the in the collection to be an instance of type object. However every class or every object in an ODBMS is definitely an instance of type object because object is the root or is the top most class definition in an ODMG database. 

Now therefore a collection can be a collection of potentially objects of different classes at lower level. So you can have a collection comprising of one car plus one transistor plus one IC plus one truck which is very well valid. On the other hand a set here is a set of only a specific kind of objects, so you can say that it’s a set of cars so you can’t really you can’t really include an object of class transistor in a car unless of course for some strange reason, transistor is a derived class of car. And the set template also defines different kinds of operations for over which are basically set operations like create_union or create_intersection, create_difference or subset_of or proper_subset_of, superset and proper_superset and so on. Similarly there is just like the set template, we have the bag template where bag is like a set as we have seen earlier that bag or a multi set is a set which can allow duplicates in the collection.

(Refer Slide Time: 15:56)

 



However duplicate should also be objects of the same type or of derived types which is specified in the template, it can’t be arbitrary objects. And of course bag also specifies different kinds of methods like create_union and create_intersection, difference and so on. Again like set and bag, we have the list object type where unlike set and bag in a list, list is not just a collection of different objects, it’s an ordered collection of different objects that means the ordering between objects is also important. 

So you can insert an object at the head of a list or you can insert at the tail of a list, you can insert after an element, after a particular element in a list or you can remove the first element and you can remove a particular element at some particular level in the list or some particular location in the list, position in the list and so on and so forth. 

(Refer Slide Time: 16:31)

 

















(Refer Slide Time: 17:24)

 

And of course the array object type which is again array is like a list where the order is important, it’s a collection of or it’s a set of different objects or of the same type. And where the order is important and usually the number of elements are fixed unlike in a list where the number of elements can vary. There is also a dictionary data type which is derived from the collection data type which is analogous to a hash table implementation. So it basically stores a collection of key value pairs and given a key, the dictionary returns a value and the dictionary object also defines a number of different methods like bind, unbind.

(Refer Slide Time: 17:42)

 

Bind essentially means that you are associating, you are inserting a value, you are inserting a new key value or rather you are binding a value v with key value k. And you can unbind in a sense delete the value associated with key value k and you can contain, I mean you can look up the value of a key k or you can check to see whether the key, the given key is actually present in the dictionary or not. 

And this slide which we also saw in the previous session shows a typical, I mean shows the class hierarchy that is or rather it’s more like the interface hierarchy that is the ODMG does not define a complete classes rather it just defines interfaces. And the definition of each methods is the responsibility of the programmer that is it can be the, definition can be written in any high level object oriented language like C plus plus or java or whatever depending on what support is available of course. But this defines an interface hierarchy that is any sub interface in a sense that is any interface somewhere in the hierarchy will inherit all the interface elements like the method interfaces of all interface above in its hierarchy.
 
(Refer Slide Time: 18:44)

 

So as seen in this figure here (Refer Slide Time: 19:44), the object interface is the top most interface. So every object here is a kind of a, can cause a little bit of confusion in the sense that there is a class called object. So every object in a database belongs to a class called object by default and a collection is a collection of different objects and sets, lists, bags, arrays and dictionaries are different specific kinds of or kind of templates where they define specific sets or lists over specific types of objects. And then there are several other objects like, classes like timestamp and time and interval and iterator and date and so on.

The ODMG 2.0 standard also defines or also proposes an object definition language plus an object query language as we had mentioned earlier in this session. So the ODL or the object definition language that is defined by the ODMG 2.0 standard is a programming language independent mechanism of defining the structure and behavior of objects. What is meant by programming language independent mechanism? That is you can have the same ODL definition interface with let’s say Smalltalk or java or C plus plus or any other programming language. And it supports different kinds of schematic constructs that define structural elements of objects and it can specify the database schema. 

(Refer Slide Time: 20:57)

 

Note here that the database schema is actually a graph of different objects belonging to different classes and their relationships. So it can specify a database schema that is also independent of the programming language. And it has interfaces with specific languages like C plus plus and Smalltalk where you can use language bindings for mapping the ODL constructs.  

















(Refer Slide Time: 22:25)

 

The ODMG standard in addition to the ODL also defines the OQL or the object query language. Object query language is a mechanism by which you can query on either attributes or behaviors of particular objects, usually for attributes of course. And it’s a query language that’s specified by the ODMG data model and it can be integrated with existing programming languages like C plus plus and Smalltalk or java and so on. 

This slide here shows an example OQL query where note that for the programmer, the programmer is not necessarily aware of the OID mapping or the OID management that is performed by the object database system.

(Refer Slide Time: 22:33)

 
Instead a programmer refers to an object or a unique object in the database using things like an entry point or a reachability condition and so on. So an entry point in an OQL is a named persistent object that is that defines the entry to which the specific object with the particular OID can be accessed. So here in this case entry point is this object called d. So d is the name that uniquely defines an object that is stored, uniquely defines a persistent object that is stored in the database. And there are iterator variables that can iterate over or that can range over each object in the collection. 

As you can see here (Refer Slide Time:  23:43) the structure of OQL is very similar to SQL itself where you just say select d.dname from d in departments where d.college equal to engineering except that here, the way it interprets this query is different from the way queries are interpreted in SQL or in the relational model. Here d in departments essentially means that d is the name for all objects that are in the extent called departments. Remember, what is an extent. Extent is analogous to an entity set that is a collection of different objects of the same type or the same class as is, so when I say d in departments, d is a iterator variable that is it iterates over every OID of objects that belong to class departments and then it selects a particular attribute from that. 

Note here again that dname should be a visible attribute. If it is an invisible attribute you can access attribute names only through method invocation. So you should be saying something like d.get name, select d.get name from d in departments where d.get college equal to engineering and so on. So however we are assuming that d name and college are visible attributes rather than hidden attributes in the object definition.

Just as in SQL, one can define views over the over the ODBMS using the OQL and this is quite similar to SQL query except that rather than say in create view, you are saying define.

(Refer Slide Time: 25:58)

 

And look at how views are essentially created. A view is created in the form of a method that is you are defining a method called has_minors department name as this OQL query. That is has_minors department name is select s where s is an iterator variable from s in students were s.minors in d dname equal to department name and so on. So you are basically defining some kind of a method when you are saying that you are defining a view. So you just invoke this method, you just invoke this view as though you are invoking a particular method. So you just call a function called has_minors like this and the output of this is the set of all students where those students are minors under working in a department that is what is specified by the query here. 

(Refer Slide Time:  27:23)

 

Similarly when a query returns a set of different objects or a collection of different objects, you can specify one single element of this collection and then use it as a separate declaration. This slide shows an example of such a situation where there is a nested query, nested OQL query here which says select d from d in department where d.name equal to computer science d.dname equal to computer science. 

Again, so as usual d is a iterator variable that iterates over all objects or all OIDs of objects in the extent called departments. Now suppose there is only one department called computer science, this query returns exactly one object. And you can define this object as a specific element and use it as an attribute of some other object if required and that is the reason or that is the use for such a declaration. And of course such a declaration will raise an exception if this query actually returns more than one elements or on the contrary, it does not return any elements at all. That is if there is no department called computer science or two or more departments have a name called computer science. 

Again some more operators that are specified by OQL are the aggregation operator or rather what are called as the collection operators where you can have aggregate operators just like in SQL like min, max, count, sum, average and so on.
(Refer Slide Time: 28:40)

 

So you can get the minimum of all values of min over certain attributes and max over certain attributes and so on. And similarly count for example, count of something like this where the in command we have not actually see here but in is this operator that test for set membership whether s is a member of has_minors that is the set called has_minors and note that we had defined has_minors as another query and you can as a view defined by a OQL query. 

And you can just have one more element like this which just checks whether any given element s is in has_minors and average over this thing. And so you are selecting s.gpa where gpa is a numeric attribute and of course again just like we mentioned earlier gpa is a visible numeric attribute. If it’s an invisible numeric attribute then there has to be a associated method that has to exist in order to return the value of gpa. So you should in effect say s.get gpa or something like that and once you have got the set of all gpa values, you can take the average of this. 

And of course all this aggregation operators have to be applied to operators of the or collections of the appropriate type. So even though collection can be collection of arbitrary objects, I really can’t apply average over a collection of objects comprising of 3 numbers and 4 animals and whatever. So it has to be of the particular type. And so averages are generally are average or min, max and sum and so on are best defined over the set t where t would point to a numerical attribute or numerical object like number and so on.






(Refer Slide Time: 31:34)

 

Similarly you can define memberships and quantifications operators in OQL and for example, you can define something like e in c where c is a collection object and e is an element. And this we saw it in the previous slide where you can actually, this expression actually returns true if e is an element of c. And similarly you can say, for all v in c:b that means in a sense its checking whether for all v in c does the predicate b or does the condition b hold true. So it is true if all elements in collection c satisfy this condition called b.

Now what is this condition? this can be again another expression like this, that is another conditional expression were e in c or e in v and so on where v is used as part of this expression in b. And similarly there exists or exists v in c such that b is true that is there exists at least one element in c satisfying b. Therefore as you can see these are quantification operators that are for example typically used in predicate logic were by you can specify different kinds of queries there exist and for all and that’s existential and universal operators that are defined in predicate logic.













(Refer Slide Time: 33:02)

 

Then you can also define ordered collection expressions. For example here you are selecting a particular struct that is a particular attribute from an object which is a struct where f is the set of all or f is an iterator over all OIDs in the extent called faculty and you are ordering by faculty.salary and designation or description or whatever. So desc can stand for. And then from out of these you are taking the first element. So essentially out of a sorted list, you can take, you can look at any specific elements like first and second elements and then get or in effect you are seeing which is the faculty member who has the highest salary and so on. 

And there are of course, we saw that an OQL is conceptually different from an SQL query. That is an OQL even though it looks very similar to an SQL query, the way it interprets the way an OQL query is interpreted is quite different from the way an SQL query is interpreted. And in OQL we define extents and attributes and iterators and so on whereas in SQL you have a tables and attributes and tuples and so on. And there are other differences between ODBs and RDBMS’s especially in how relationship properties are handled, especially foreign key relationships and other kinds of relationships and in a sense an OBD database or ODBMS is an un normalized database in the sense that it is not even in first normal form.
 









(Refer Slide Time: 34:31)

 

Remember that a relation is in first normal from if it is not a nested relation. That is a tuple does not contain nested tuples. However an object can contain tuples which can contain other tuples and so on to any levels of nesting. So it’s not in first normal form and the way relations are handled in ODBMS’s are mainly through OID references and OID references have little, if any to do with normalization in an ODBMS. 

And then the handling of inheritance; inheritance is an integral part of ODBMS’s whereas inheritance is an alien concept in relational database. So you don’t really inherit, you don’t really see types and classes inheriting or tables and tuples inheriting from one another and of course the specification of database operations. And there are also certain techniques which talk about how you can map a conceptual schema written in the ER model or the extended entity, EER model extended entity relationship model to an ODB schema rather than a relational schema. 















(Refer Slide Time: 36:18)

 

We shall not be going through this slide in great detail though except to note that just like each entity look at the first step here where each entity in an ER schema corresponded to one table in a sense in the relational schema. Here it roughly corresponds to one class for each EER entity in a sense and relationships are referenced, relationships are essentially shown using associations rather than say foreign keys or having the same values and so on. And several different steps which are or several different thumb rules which specify, which tell a designer how to map from an extended entity relationship model to a ODB schema. And we shall not be going, looking at this in great detail. 

(Refer Slide Time: 37:10)

 

The next topic that we are going to see which is again of quite a bit of importance is the object relational databases or also what are called as ORDBMS’s. And ORDBMS’s are in some sense a middle path between RDBMS’s and ODBMS’s and both of them have their own pros and cons. ODBMS’s are better suited for handling different kinds of complex objects like BLOBS and say and also in providing say behavioral abstraction where transistor is defined by a particular kind of methods and so on which are not present in the RDBMS space. However an RDBMS by itself has its own advantages which cannot be matched by a specific, by a pure ODBMS database. For example concepts like normalization and query handling and indexing, storage structure and so on where lot of work has gone into specifying not just the relational data model but also building an implementational scheme around the relational data model. 

And all of these would have to be reinvented for at least a large part in the object database realm, if pure object orientation is being considered. Now the middle path is to use object relational database that is can we use both objects within relations and vice versa. So, an object relational database again are meant for applications dealing with complex data like satellite imaging and weather forecasting and so on and where the complex data is in turn associated with number of traditional data like which can be mapped on to tuples.

(Refer Slide Time: 38:57)

 

However it’s not just the traditional data that is of interest here, it’s the complex data that is how these complex data can be manipulated. That is what kinds of methods are there and how do we check the state of this data and so on. So there are several extension that have been proposed for RDBMS systems to make them compatible, to make them also to also support objects. For example there are support for extensible data types, remember that inheritance is not an integral part of the relational model and there have been some efforts to introduce inheritances or data type extensions into the relational model. And then support for user defined routines where the user can define certain methods by which data can be manipulated.
 
(Refer Slide Time: 39:50)

 

And implicit notion of inheritance that is you can, inheritance as an integral part of the DBMS system and some other extensions to indexing the traditional indexing of RDBMS systems. 

(Refer Slide Time: 41:01)

 

So we shall be looking at one specific kind of ORDBMS as kind of a mini case study in a in a sense namely the informix universal server. That’s from informix which provides extensions to the traditional relational data model by what are called as data blade modules. The idea behind data blade is as though that blade actually means a racer in this context, that is the actual meaning of blade. 

The concept is as though a new data types or new extensions can be cut through existing definitions as though like a blade cutting through a fabric. So you can actually cut through something and then introduce new data types. And there are several kinds of additional data types that are proposed for example the opaque type or rather the opaque table in a sense an opaque table is one where the contents of the table are hidden just like in an object, if variables are hidden or invisible then it becomes an opaque table.

Now you can actually in a sense cut through an existing database system and introduce a new type called opaque tables and so that’s the idea of a data blade module. And then there are distinct types and you have the notion of a row type that is you can define a tuple as a type and a collection type which is specified in the ODMG standard as well. Then user defined functions are provided in the form of stored procedures. 

(Refer Slide Time: 42:11)

 

We already saw stored procedures when we are looking into case studies in RDBMS’s. So in fact many of today’s RDBMS actually supports stored procedures which are in some sense, which in some sense can be argued to be some kind of support for object databases. That is stored procedures in effect define some kind of a method that works on or that is applicable to tuples of specific kinds. 

So here there is an example of creation of a stored procedure something like create function equal where you provide different arguments arg 1 and arg 2 which returns Boolean and then you define your function and you also define the language in which the function is written where you say it is written in language C and then end function. 

So note here that the stored procedure in informix universal server doesn’t really require the definition of the procedure to be specified here. The definition can be specified in a third party language like language C, like C language and then it’s only the interface or the stub for the stored procedure which is available as part of the database itself. 

(Refer Slide Time: 43:58)

 

And the informix server also supports data inheritance as an integral part of its ORDBMS data model. So for example here you are saying, you first create a row type. Row type is again a tuple type which is created like this. So you create a row type as employee type which contains employee name and social security number and salary and so on. And you are creating another row type, engineer type which says which has two other variables degree and license and then there is this key word called UNDER employee_type. 

That means, what this means is that the row type called engineer type is a sub type of employee type or it is a specialization over employee type. That is in addition to the fields that exist in the employee type, engineer type has two extra fields like degree and license. Similarly you can also define function over loading as well. 

Then there are several kinds of indexing extensions that are defined and where you can create index not just on data values but also on user defined routines or user defined methods. For example this slide here shows a declaration, shows an invocation where it says a create index emp_city on this function called employee city address.
 






(Refer Slide Time: 45:07)

 

So this is actually a function over which the emp_city is being defined. So it creates an index on the employee table based on the information given by the city method. That is it first executes the city method on all tuples of this employee table and based on this information, you create an index that is some kind of a b plus tree index or whatever. And there are several kinds of APIs that the data blades provide and where you can something like two dimensional data types and image data types and so on. There is also construct for abstract data types where you can create, where you can define an abstract data type along with specific methods to manipulate the variables or manipulate the elements of this abstract data type.

(Refer Slide Time: 46:02)

 
The next kind of ORDBMS support going from informix universal server, let us move on to another kind of ORDBMS support namely in SQL 3 or SQL 3 standard. SQL 3 standard has a number of extensions from the earlier SQL standard which provide some kind of constructs which are in some sense in line with ORDBMS requirements. And for example SQL 3 is divided into different parts like SQL framework, SQL foundation, SQL bindings and SQL object and it’s in the SQL object part where number of ORDBMS support is provided as part of SQL 3.

(Refer Slide Time: 47:12)

 

And of course there are several other extensions that provides like temporal and transaction aspects of SQL and persistent stored modules and call level interfacing and so on. So as far as object relational support goes in SQL 3, SQL 3 supports a new data type called LOB or binary large objects or a large object and so on where you can define some kind of a binary dump or binary data object to be a large object or a binary large object.















(Refer Slide Time: 47:48)

 

So for example you can have a media file like a video or a music file and as part of an attribute type and reference it and call it a binary large object. And there is also support for objects in the following ways where the first thing is the creation of row or tuple types that is just like in informix server, you can create a type out of a tuple. So once you create a type out of a tuple, you can create abstract data types like a struct which contains different elements of type tuples. So you can have different tuples that make up an ADT and then in addition certain kinds of methods that manipulate this tuples.

So here are certain examples that are shown in this slide. For example you can say this is an SQL 3 by the way. So you can say something like create row type where basically this is the tuple that is name is a varchar of 30 characters and age is an integer. Now this tuple is called emp_row_type that is this is a new type which creates emp_row_type and then you can simply say CREATE TABLE employee of TYPE emp_row _type.

So it will just create a table of different tuples of this type. And you can extend existing rows, again the notion of type hierarchies comes into play here where you say CREATE ROW TYPE comp_row_type and then you say employee REF employee row type. That means you basically create another element and then just refer to the previous row type that you have defined. And all the attributes that have been defined there automatically come into this definition here and again you can say create table employment of type comp_row_type or so with the new type that you have created. 







(Refer Slide Time: 49:34)

 

And in addition to row types, you can also define the notion of an ADT or an abstract data type in SQL 2. So ADT essentially is a user defined type for a particular variable, for a particular attribute. that is you can actually define a struct comprising of different tuples and different ADTs, nested ADTs in itself and then and then use that as a composite type, composite data object representing a type comprising of different attribute and so on. And then an ADT comes with built in functions like the constructor function wherein you can initialize and instantiate the ADT then there is an observer function which can, which is basically like the get and put operation.

(Refer Slide Time: 51:20)

 

So you can get the values of different hidden variables that the ADT defines and mutator function which is the put function where you can actually change this thing. And ADTs can also have, may have user functions associated within.

(Refer Slide Time:  51:38)

 

And there are several kinds of implementation issues in the SQL 3 specifications like suppose a user specifies a function or user defines a function then the implementation system, the DBMS implementing SQL 3 should dynamically link to a user defined function whenever required at run time. 

And there are other kinds of client server issues as to where the function is defined, if the function is defined on the client and it has to be executed on the server or whether the function should be defined and stored on the server beforehand and so on. And it’s not clear whether one can run queries within functions and of course efficient storage and retrieval of data is also important.














(Refer Slide Time: 52:31)

 

So let us in a sense summarize the ORDBMS’s issues, one is of course that object relational database design is much more complicated than relational database design. You might have already guessed why? That is creating indexes over functions or defining inheritances and so on can hinder or hamper with the traditional relational database storage and access functions. And query processing and optimization becomes more complicated when it comes to object databases. especially because when user defined functions have to be run, stored procedures have to be run there is no way to further optimizer to know how much time or how efficient is a, what is the behavioral characteristics of the user defined function. 

(Refer Slide Time: 52:49)

 
So that’s the large unknown which the optimizer has to take care of when optimizing a query. And of course there are the notions of triggers and transactions and integrity constraints and so on which can further complicate the matter of maintaining an object relational database system. 

Similarly because ADTs can define user defined types and an ADT might may contain a tuple which in turn can contain another tuples and so on. It basically an ORDBMS can become un normalized or de normalized because it’s not even in first normal form. A first normal form each tuple has to contain atomic elements. So it is necessary to remove the restriction of first normal form on object relational databases. But internally even though that’s the abstraction that is provided to the user, internally perhaps it is still more efficient to map the user abstraction that is provided to an internal pure relational database schema.

So let us summarize what we studied in this lecture or in this class. Starting from the previous lecture on object oriented database systems, we looked into the ODMG 2.0 standard and what kinds of object definition language and object query language constructs that it provides. And we saw how OQL queries although they look very similar to SQL queries are interpreted in a very different fashion in an ODBMS.	

(Refer Slide Time: 54:53)

 

And the relative short comings and advantages of ODBMS, we saw RDBMS prompted the emerges of ORDBMS or object relational database systems. And we essentially saw two kinds of specifications, the informix universal server and SQL 3 support for ORDBMS. And of course ORDBMS themselves are by no means as elegant as the pure relational database. That means they don’t have a nice specification and there are several issues like query optimization which is a major issue plus storage and retrieval and so on which still they have to contend with. So with that we come to the end of this session.

Database Management System
Prof. Dr. S. Srinath
Department of Computer Science Engineering
Indian Institute of Technology, Madras

Lecture No. # 38
XML – Introductory Concepts

(Refer Slide Time: 01:20)
 
Hello and welcome, in the previous two lectures, we have seen at different kinds of databases, or try to see what happens when we try to manage data, that not easily amenable to storage in a relational database. And in this context we looked at object oriented databases where data that has stored or complex data object in the sense that, not only abstract the structure of some data item but also its behavior, what kind of behavior. In this lecture and next three lectures, we are going to look at another important kind of data management issues that that occur in practice day in and day out. I have we are which can be in a natural kind of captured as combined data management issues related to unstructured, semi-structured data and heterogeneous data sets and what are called as self describing data sets. See, what happens in the real world is the set there are rather than having one specific database, even in specific, even in a given U R D or even in a given system context like let us say we have a huge company. Let us say huge multinational company which spans several countries or even not even not so huge company which let us say spans several cities. They do not use one single database for the entire company; it is quite impractical for several reasons, for historical and for several other kinds of for practical reasons, they use many different kinds of databases and many different kinds of data stores.
As it is in what happens is over a period of time, trying to reconcile these different data stores becomes more and more difficult. And whereas it could well be the case set one part of the company is using let say windows based system, another part is after companies using is Linux based system, another part is using Mac system so on and you would not have, it would not be possible to create a single database system, which spans across all these different operating platforms in the works coherently. And in a addition, in addition to the above huge amount of unstructured or in some sense semi-structured is cre[ate] is been created day in and day out in the form of letters, faxes, memos, webpage’s, emails, documents and so on and so forth. So much of data is being generated, all of which cannot be easily accessed or we it is not possible to simply define a database so that you put everything into this into this database. What is required, in some kind of simple mechanism that can that can help describe a data element in a fashion that is independent of any operating platform or any encoding, I mean there could be different encoding as well. I mean one could, one could be using ASCI encoding, one could be using unique code encoding, one could be using some other kind of encoding and so what are encoding, what you are be ah way data is being stored. It should be able to, one should be able to reconcile between all of them in uniform fashion.
Now what is how do we do that? And how do we reconcile all of these different data elements under one, under one simple in a simple fashion. For this the emerging answer over the last few years is XML or the extensible (( )) language. So we should be looking into XML and XML as applied to semi-structure data in the next in the following two lectures. And XML is although it is so simple in practice where it would seem that there is nothing to it has become a very important source of data interchange and data representation and data description, in the post internet world (( )). 
(Refer Slide Time: 06:03)
 
Why XML and what are the, what are the reasons when or what are situation when when we can say that XML make sense, when we can use XML. Managing heterogeneous data source is very valid example, when there are several different data sources, let us say you are you are designing a data warehouse and you are taking transactional data from different (( )) data bases or let us say you are trying to reconcile data bit from different different files. Where let say you have different spread sheet or word document or some other kinds of files and you are able, you are planning to reconcile all these data that is stored each of these different data bases and integrate them into one common data source, that is when XML comes really comes into (( )).
And similarly data self-description, what if there is no, what if you do not know or what if whoever is using your data does not know how your data is organized. Let us say your your employee record, what if whoever is requesting for your employee record does not know what are all the fields that is your employee record contains and what are the constraints on those fields and so on. What if there is no single way by which employee record can be organized at all. May be everybody has a different way of organizing employee information, one might say we require social security number as the primary key and in cases where in places where there is no such concept as social security number you could say something like PAN number and or ration card number whatever our just name and so on. So what all should go into some places have first name, last name, middle name, initial and so on and in some other places we just say name. So, there may not be a single way by which we describe data about any specific entity. So the based way to manage such situations is data self-description. What your data that you are sending across between application in different context, let the data itself describe, in what way it is organize. Let the data in the meta data, that is data about data be sent in one packet that is be integrated in a way that just by looking at this packet of information we can not only get the data but also the way in which data is organized.
And the third major use of XML is semi-structured data management. I mean several different kinds of data sources are not really structured documents for example documents, letters or faxes and so on. Where there is some resemblance there some resemblance of structures for example, letter should have from and to and some kind of addressing like dear sir and whatever and subject and complementary closing and so on, but beyond that text itself or what goes into each of them, there is no specific rules that says this is way the data has to be organized. So such kinds of data bases or data sources are semi-structured data sources. And huge supply of semi-structured data is of course the World Wide Web, I mean any HTML data that is that is written that is present over the or that is served over the World Wide Web is semi-structured data, that is there is no specific structure to HTML document, however part of the HTML document is some sense describes let say suppose you say h one and slash h one, it means there this is the first header, at the first level header semi-structured in a HTML document.
Similarly, you say p and slash denote that the paragraph is started here and it is ends here and so on. So there some sentence of structuring, but there is no rigid structure in that exist for the for any HTML document. And to manage such data sources or manage such data XML becomes very important. 
(Refer Slide Time: 10:42)
 
So, what is XML? What its properties? XML are the extensible markup language. It is actually a subject of an earlier markup language called SGML – standard generalized markup language. What do you understand by the term markup language? What is it mean by the term markup language? I am sure; you would have probably used the well-known HTML or the hypertext markup language in creating web pages. Even though there are number of web creation tools that are available today, most of us would have tried our handset working straight with plain vanilla html; that is go straight to HTML and start changing the HTML document directly open it up in notepad or (( )) or some text editor like that and open change it directly.
HTML is the markup language. What is the markup language? Markup language is essentially, the name used for languages where the Meta data that goes into describing the data is embedded within the data itself. That is a HTML document, the Meta data that is says which is the first level header, which is the second level header, which is the paragraph, which is the hyper link and where does the hyper link connect it and so on and so forth. All these Meta data is embedded as part of the data itself and it is embedded in this same form, that is using the same encoding and the same even the same font I mean if it is rendered by any text editors, editors are its rendered using the same font as them data itself is rendered. So therefore if you are using unique code as your base then the entire set of data and meta data would follow unique code. 
And let us said earlier XML is actually a derivative of the lesser known SGML. The idea behind here XML although, XML is has become so famous and in the post internet era. The idea XML for creates the internet itself and it tends from SGML which was the standard that was used for document management, how to how to manage documents within how to manage data and Meta data within documents. So there is several SGML based tools and ensure and SGML and related to for example, if you use Linux I am sure, you might have used this software called info which is, which is based markup kind of language where which in some sense predates the World Wide Web in terms of hyper text usage within a single machine. So, it was originally designed to be a flexible text formatting, text format for electronic publishing. However, today XML plays a major role in data exchange seamless data exchange over through web and several different other application areas. In fact, the de facto standard of exchanging application data over the web is fast becoming XML. Of course there are other standards, but XML because of its simplicity is faster (( )) or taking over all other different standards of data exchange over the web.
(Refer Slide Time: 14:30)
 
So what are, what are some other features of XML? XML is essentially a cross-platform, entity which is independent of any software and hardware means for transmitting information. That is it does not the way in which an XML document is stored is not dependent upon what software you are using, for example, whether you are working in windows or Linux, and even hardware, what hardware you are using – that you are using an Intel based pc, or Macintosh having Motorola based, Macintosh powered by Motorola processor and so on. So, it does not really change, I mean across all of these platforms XML the structure and description of an XML document would remain the same. An XML basically describe data in the form of one or more XML documents, where each XML documents is in the form of some kind of tree containing both data and Meta data. And XML is a markup language, but it is not a computer language; that is not a procedure language. 
However, there are it also, there are some kinds of computer program namely especially say XSLT which read XML documents and behave in different ways accordingly. For example, if we have C program or Perl program and you run it through the Perl interpreter, the Perl interpreter behaves in different ways based on ah what is written in the Perl program. So, in a sense, so in that sense one can call XML as a programming language; however it is not a complete programming language. In that sense, it does not have all construct the makeup the programming language and hence it is just a markup language. And XML can, can also use what is called as DTD or document type definition which is again being first replaced by what is called as an XML schema to actually describe the structure in which data has to be organized or to describe the data itself.
(Refer Slide Time: 16:57)
 
So, what is XML contain? If you worked in HTML, where HTML is a markup language for creating hypertext documents. You would probably come across several different deserved tags that HTML itself provides, for example we talked about h one. So suppose you (( )) h one tags within (( )). It means that whatever lies within this tags is the, is the first level header in the HTML document. H one is for example, it is an example of reserve tag in HTML. Similarly, the tag called p defines the paragraph, p and slash p defines the paragraph, so again p is the reserved tag in HTML. Like that HTML has nearly hundred different pre-defined elements only using which you can define a HTML document.
On the other hand, XML has no pre-defined elements, that is there is no h there is no h one, h two, h three there is no P, there is no PR, there is no UL, there is no LI any such element that is defined by, that is defined by XML. In fact, it is the ownership or it is the responsibility of the creation a document to creative to actually invent tags or invent Meta data that describes the data. And it is completely up to the user or up to the creator of the XML document to describe or to find have to find what may be termed as the appropriate Meta data terms in order to define the data that is available. And that that is what makes an XML document self-descriptive.
We will see how that happens in a short while. But before that let us look at the, look at what are the, what are the motivation behind XML. What is the meant to do? Let us take a contrasting feature to begin with, HTML for example. HTML was basically meant to meant to describe how data have to be displayed by a browser. It is a logical data description language but nevertheless it is meant for display; HTML is meant for display; that is it tells the browser how to render the data. It tells the browser, for example, this is the first level header so do whatever you are doing in order to describe first level header. This is the paragraph break, so do whatever you are doing to display a paragraph break ok or this is bold so this is do whatever you are doing to display bold characters and so on. So primarily HTML is design for display or logically describes describing how data are to be rendered on a browser.
(Refer Slide Time: 20:12)
 
In contrast XML is designed to describe data and focus on what the data really is, so essentially XML is make to was oriented towards data exchange rather the data rendering. And therefore XML isn’t really a replacement for a HTML. In fact XML is complement of HTML. And there are several tools that given an XML document will parse an XML document and convert it to appropriate HTML document, so that it can be rendered by a browser. So XML is basically meant for describing the data, while HTML is meant for describing how the data are to be rendered. 
(Refer Slide Time: 21:07)
 
So what is an XML do? Have look at the slide that that is shown here. This slide shows us small XML fragment. So, look at how the fragment looks like. There is a startup of tag here, which looks like there is startup of tag here, which looks like any HTML tag in the sense that it is it is embedded within angular braces. So and just like HTML tag tags end with the slash tag. So notice tag ends with the slash notice. The to tag ends with slash to and from tag ends with slash from. However as you see here, each of these or arbitrary, I mean just it is notice and it is to and from and so on. So essentially what is fragments is describing is that this is the notice where the notice begins here and ends here. The notice is send to the students of first year from hostel warden with the heading air conditioner, with the body of the notice as whatever that is written here right. So body is denoted between body and slash body and heading is denoted between heading and slash heading. Now you might ask a how to any browser note how to interpret to and from and heading and body and so on. The simple answer is it does not. And it is the responsibility of the parser or whoever writes the parser to also write what have to be done when the parser come, comes across and notice or comes across the two field in a notice and so on ok.
Otherwise if an XML document is rendered in a generic browser like say internet explorer as so. If this particular fragment to be rendered in a internet explorer, you would just get a tree like structure as you can see this is tree like structure, this is a hierarchy, that is at the first level there is there is notice comma slash notice ok which is the first level element and there are one, two, three and four second level element. So it is a tree comprising the two level of hierarchy one node at the top denoting notice, and four nodes below it or four children denoting to, from, heading and body. 
(Refer Slide Time: 23:35)
 
So as we saw that the XML fragment denoted something called notice and which has from, and to and message body and so on. But, as such, the XML document itself does not do anything; it is just describes that. So it just say notice, slash notice and from and to, it beyond that nothing else. It does not do anything else. It is just some kind of what may be term this wrapped information, that is information in this sense I’m using the term information to mean Meta data. So Meta data is wrapped around data elements in the XML document. And how to display this XML documents, how to send, how to receive, how to what actions to perform in response to each of these elements all of them have to be handled separately. XML by itself does not describe any (( )). 
(Refer Slide Time: 24:37)
 
Now what is the use of creating such a one might ask two generic a module for creating data, creating describing data. What is the use of such an element or such a mechanism. Firstly, as you can see regardless of what kind of data that you are describing XML is pure text; it is pure everything is in textual form, only that the encoding may differ obviously, but it is possible to find out what is the encoding and detect what is encoding and change it accordingly. But whatever it is, it is given an encoding it is just plain text; there has no preparatory forms by which data within the data attributes or values within the XML within the XML document is described. So, it is just plain (( )) data. So, because it is just textual data, it can be exchange seamlessly between different systems. So, whether it is different operating platforms like Linux, or windows, or Unix or Solaris whatever or different processors itself different hardware platforms, it does not matter.
So, over the web or over any web an XML document can be exchange seamlessly without any need for any kind of interfacing for XML document. But one which still ask that huge amount of an XML document is a huge waste of space, in the sense that just to describe a small notice and and to say this the notice was send from someone and send to someone and with the heading so and so, unit two at lot of tagging information like to and slash, to and from, slash from and so on. However, that is not too much of a problem, because today storage is for cheaper than what it was earlier. And storage is much more cheaper than what it was earlier and disk can disk can store large and larger amounts of data. And processing is also faster, so it is quite and then many numbers of many very efficient XML parses that is freely available; therefore parsing an XML document and creating a structure is quite simple. Also, there, there are different kinds of , there are different terms kinds of software that are available for compressing data you have, may have WinZip or G-zip or (( )) and compress and so on and so forth. So which, which can be used to compress textual data very efficiently; and especially with textual data contains repetition which an XML document describing some class of elements is apt to contain different kinds of repetition, one can one can expect huge percentage of compression to be performed. So even if a XML document is takes a lot of spaces for describing a set of data, it can still be stored in a efficient fashion by compressing the document and decompressing it on the fly and so on.
And converting any data set like preparatory block level data set to XML can greatly reduce the complexity of data exchange between different incompatible systems and again over the internet. And huge application area of XML naturally is in B2B or business to business interchange. So each business house or even within one single business house, there could be different standards that are used. And trying to interface between these standards become huge challenge and that is where the utility of XML comes becomes significant. 
(Refer Slide Time: 28:52)
 
So, XML see unlike for example Ms-Word data which can be opened only by Ms-Word or postscript data which can be only opened only by postscript viewers or PDF data which can be opened only by PDF viewers. Unlike those XML is just text data, so it is not bound to a specific application or specific browsers. You can open XML using any application that can deal with textual data, you can open XML in notepad; you can open it in VI, you can open it in normal internet explorer, which will show, which can support XML directly and which just show as a tree structured. If a browser does not support XML directly, just shows you textual form of the XML data. And there are several different that are written, wherein you can access XML data sources as though your accessing your databases.
(Refer Slide Time: 30:01)

 
So, let us come back to XML syntax and look at some more aspects of what makes up an XML document. Now here again this is the (( )) notice notice comma slash notice 
however, with a few more things that are added. Now, you might have noticed already that any symbol here, any tag that is opened here has to be ended. So, notice is ended by slash notice, body is ended by slash body, to is ended by slash to. So, in XML, it is mandatory for, for properly closing every tag, which is not show in html; in html certain kinds of tags like say p and font and so on and so forth, need not be closed and whenever a second p tag comes in the the browser automatically closes the first p tag, but here an XML parser would flag syntax error if a given tag is not closed that is it is open and it is not closed. However, there are exceptions have a look at the memo tag , memo type equal to d slash like this; so that is an other way of expressing a tag or a or a meta data that does not contain a large amount of data. 
In fact, the meta data contains what is called as an attribute of name type with value D, and that is it and there is no other no other data that is associated with this tag or tag is a meta data meta data item actually. So, by default an XML document should be properly nested and properly closed that is every open tag should be properly closed, and it goes without saying that the nesting of XML element has also to be (( )) exception cases where that is closing of for the case of closing of tags. There are cases, where you can close a tag within a single line I have shown in this memo example here in addition look at the first line here, where the first line starts with one angular braces; question mark and xml, the key word called xml, if the document begins with this, with this pattern here then the XML pattern, then and if it (( )) XML parser, the XML parser knows that this is the valid XML document. So, this one defines that this is the XML document, and this one defines the version of this document, and this one defines the encoding that is used for describing this document. 
(Refer Slide Time: 32:46)
 
So, just to summarize what we have seen, so the first line basically is a declaration that says that this document is an XML document with the given version, and with the given version and encoding and following the XML declaration is the root element of the XML document. So, so every x m l document should be a rooted tree, that is a there should be one root element that begins here and ends at the end of the document right. So, in this case the root of the XML document is the notice tag or the notice element.
(Refer Slide Time: 33:36)
 
A tag is again some more x m l based definitions, this is what is called as a tag notice or from or body and so on, they are called tags. A tag is a simple meta data, that is embedded within angular braces on the other hand the entire set of XML data from notice to slash notice inclusive is called an x m l element, that is it denotes an XML elements called notice, which in turn contain several different x m l elements like to element, and from element and so on right. 
(Refer Slide Time: 34:15)
 
So, the root element is the one which is the biggest element, which contains the entire XML document and every XML document must have a root element, and of course the next four lines describe the child element from the root, and to from heading body and so on and the last line describes the end of the document. So, as you said before in XML, it is illegal to omit closing tags. So, every tag that is open has to be closed properly. So, notice has to be closed by slash notice and from has to be closed by slash from and so on, this is again contrary to XML. 
So, all elements must have a closing tag with exception of unary tags, like the memo tag that is shown in this slide here. So, where there can be a single end tag which can be closed directly. 
(Refer Slide Time: 35:09)
 
And the XML declaration, that is the first line here in this document here, this is not an element that is why to distinguish between a declaration, and meta data a declarations starts with less than and question mark. Therefore, that that gives an implicit constraint that that any name of an XML tag cannot begin with a punctuation mark, like question mark. So, because it will be treated differently, it will be parsed differently and that is not an element; therefore, it would not have a closing tag and XML tags are case sensitive; therefore, notice with all with all small letters and notice with capital n or different tags, and it goes without saying that the tags must be properly nested. 
(Refer Slide Time: 36:03)
 
(Refer Slide Time: 36:13)
 

And as we saw in the previous slide tag or meta data can have different attributes. So, let us look at it again that for example here memo, memo is a Meta data which says that this is the memo, and here is an attribute which (( )) is to memo, and the attribute name is type and the value is D so… So, therefore it says that this is the memo whose which contains an attribute called the type, and who is the value is D. So, it is type d king of memo. So, and attributes values must be coated. So, even when I say the type equal to D and D is a single letter it has to be coated it it should it should lying within double quotes. So, so that is double quotes or single quotes.
So, that is in contrast HTML, where it is optional to coat an argument attribute value unless the values contains a space with in it, and in XML white spaces are preserved and not truncated as in HTML. So, what this mean is that for example, if I put a white space here air conditioner, and three spaces and then slash heading then the data for this heading is actually air space conditioner followed by three spaces in html (( )) all the training. And leading white spaces are ignored when HTML is being ranted on a browser this is not. So, an XML and all spaces are considered, and of course the syntax for writing comments in HTML in XML with slash exclamation mark, and two dash is and end with two dashes and open close ah angular brace right. So, this is same syntax as that is used in html.
(Refer Slide Time: 38:05)
 
 So, XML elements are given are XML document forms as specific tree structure, and the relationship between elements primarily there are exception to it which will which will look in to more detail in the next session an xml, but primarily the relationship between in between two or more tags is either parent child or siblings; that is that is one is (( )) of the other or there it, the there it the same level in some sense that is or one is at a at a higher level; and one is at a lower level. 
So, so it can associated level value for each tag in an XML document, and XML element as said that what is mean by an XML element a XML element is everything from starting tag to the ending tag include the tag (( )). So, so notice to slash notice including the tags forms the root element in the XML document, and an element what (( )) element contain; obviously, we have seen that an element can contain other elements for example, here the element call notice contains other element call to from heading and so on. 
(Refer Slide Time: 39:45)
 
So, so an element can contain other elements or can have a element element or mixed content; that is it can contain both other elements. And some textual data as part of this or could contains simple content, where it is just simple textual data or an element could be empty as well which is an empty content an example of empty element, we saw was the memo element. So, which where you need not separately specify a closing tag for empty empty element, you can just close up the elements within a single line and an element also have attributes. 
(Refer Slide Time: 40:23)
 
So, how can you define a XML tag or an XML, XML element what tell what are the rules for describing XML element as we saw earlier; obviously, you cannot begin an XML element with the punctuation mark like exclamation mark or question mark and so on and so forth. 
The element names, names of an element can contain letters numbers, and other kinds of characters like dot colon and so on; however, they cannot start with number are a punctuation character, and also they cannot start with letters XML in any form that is case (( )) form a xml, because they a are reserved for a possible future that use and names cannot contain spaces. So, you cannot quote name within double quotes in say this, this is the XML element. So, an XML element tag should be describe by a single word and any name can be used, because there is no such thing as a reserved word except for XML of course, right. So, and you should try to make the names as descriptive as possible in order to describe the meta data.
 (Refer Slide Time: 41:40)
 
And this slide show an example of how attribute can be used as children element and vice versa. So, the in the first example here, there is an element call person and person which slash person.
And the person element has an attribute (( )) gender, whose value is male and there is the first name, and last name elements which which are all as shown. And in the second example this general equal to male attribute becomes another element here, that is it becomes an element for general and the value is male. Now, which is preferable and when this is similar to the problem that we faced in e r modeling are entity relationship modeling in one of our best classes. For example, if we were asking question whether an employee an employee working in a department should it be shown as department id attribute for the employee entity or an employee working with department is a relationship, now which you are going to show depends upon of course, depends upon this specific situation, but rulers (( )) one needs to describe an XML element just like one, it is to describe an entity only for those concepts that have independent existence; that is ah a person has an independent existence in a UOD regardless of whatever else there is…
 However, general is closely associated with a person. So, it is does not have a separate independent entities. So, so that can help you decided, when the user particular data element as as in attribute versus as another child element of an existing XML element, 
(Refer Slide Time: 43:47)
 
When we talked about XML document, there is often a distinction between what is called as a valid XML document, and the well form XML document the distinction between the two is is very important. Mainly when when we note that well formed XML document (( )) invalid where, whereas all valid documents are well formed. So, well formed XML document simply is ah a XML document that that conforms to all the XML syntax specification, that is what are the XML syntax specifications namely that a XML document should be a rooted tree that was a it has it has a root element, and elements do not have spaces in them. And every starting element has as a ending element and there elements are properly nested so on. 
So, all these if an XML document conforms to all of these syntax rule, then it is called a well form XML document, but a well formed XML document need not always be valid various notion of validity come from validity. We say that an XML document is valid if it conforms to a schema speciation. So, so that… So, so it means that according to an XML schema for example, person person element has a attribute called gender, and no child element called gender in that case even though both of these are valid XML fragments; the first one both both of these are well formed XML fragment, the first one is a valid XML fragment where as the second one is not a valid XML fragment. So, varies the schema described in in an XML this schema described in what is called as DTD or a document type definition.
(Refer Slide Time: 46:00)
 
 So, I am a look at this fragment again here, where this is well formed XML document, and then here there is an extra declaration; that is say stand alone equal to yes in the XML declaration, if this stand alone is equal to yes declaration is there. Then it is when it explicit states that the XML document here is only well formed, that is it does not have a schema associated with this is stand alone XML document. So, in a sense the stand alone equal to yes indicates that the document is well formed provided, of course that that it is well formed that is parser gives no error when parsing, 
(Refer Slide Time: 46:49)
 
And on other hand you can say stand alone equal to no which is optional, and give up link here with with less than exclamation mark declaration doc type declaration.
 So, with the doc type declaration you can specify the DTD, which describes this XML document. So, note that it says doc type notice system notice dot DTD. So, it means that in the file call notice dot DTD the DTD for this XML fragment for this XML fragment document is available, 
(Refer Slide Time: 47:26)
 
And the XML document are the XML fragment is valid if and only if of course first if it is well formed and it conforms to the DTD specification. 
(Refer Slide Time: 47:45)
 
So, what is a DTD, and and how do you describe a DTD, and how do you describe an XML document using a DTD a DTD simply says that ah DTD; simply defines, what all should an XML document contain that is ah which element should be a child of which other element and which element contains what kind of data and so on and so forth. 
So, DTD stands for document type definition and DTD, DTD defines all the legal elements of a XML document, and it also defines the documents structure within a list of legal elements and DTD can be either declared inline, that is as part of an XML document or it can also be a recall from a external reference, and this external reference can be anywhere on the internet; that is it does not have to be on a on a different file or a it can be anywhere on the internet as long as you can you can dereference the DTD by a URI or a or a uniform resource indicator, where where which is which is the http equivalent slash whatever. So, so which how you specify you are (( )) document as long as you are able to dereference DTD document with url it is it is possible to place a reference in your XML document. 
(Refer Slide Time: 49:17)
 
So, if I XML if I DTD included within the XML source then it should be wrapped with in a doc type declaration like this. So, here it is says doc type root and this is the this is the name of the root tag, and these are the element declaration what are the element that should going to this, 
(Refer Slide Time: 49:35)
 
And if if the DTC DTD is external to the XML source file, then it is should be a wrapped with a doc type definition with the following syntax like this; that is doc type and root here specifies that specifies that root is the specifies the name of the root element in the in the XML document
(Refer Slide Time: 50:08)
 
And this is the file name of the XML document, and this says that this is the external reference, here is a example of a XML DTD as shown in this slide as you can see this is this is an inline d t d; that means, it starts the doctype here and notice says that notice is the root element of this x m l document, and this one says that notice contains from here to here there (( )) to be a box brace here. So, or rather from here to here that is this is the definition of notice that this is the definition of the document, and the first one says element notice contains to from heading and body; that is four different elements in this particular order, that is to from heading, and body and everything else that is element to element from element heading, and element body contains what is called as pc data or hash p c data. 
(Refer Slide Time: 51:07)
 
So, what is this hash p c data. So, let us go to this slide which talks about what each of this elements mean. So, elements to from heading etcetera are all this element names, and tags are used to markup elements and then an element can contain other elements like to from heading and. So, on or it can contain c data or p c data. So, what is c data or p c data. So, what is c data what is p c data. So, a p c data essentially means that parsed character data and c data just means character data. 
So, so ah a parsed character data can contain other x m l tags as part of the data which which will actually be parsed and opened as and when required, but character data is not parsed it is just dumped in in water form it is present. 
(Refer Slide Time: 52:03)
 
And of course, you can use this predefined entity type like ampersand l t ampersand g t like using normal h t m l to to denote each of these characters, and we talked about p c data and c data where which stands for parsed character data, and character data respectively and in in addition to each of this; there are also other kinds of option that that are provided when defining an x m l element. For example, if let us say the the option called from is if the element called from is optional in a notice a notice should just have a two address, and the from address can be optional then you can a suffix from with a question mark that is from question mark says that the the element called from may may exist either zero or one times.
Similarly, if I put a star here, let us say heading star it means that I can have zero or more heading elements in a notice element, similarly something like body plus means that a notice can have one or more body elements and so on. So, so these are what are called as wilde card declaration (( )) elements that says how elements can be structured, and in fact one can even says something like to from, and then heading pipe body pipe is the or symbol, so which says that notice can contain to element from element and either heading or body, so one of the two because there is only one of them and so on. 
(Refer Slide Time: 53:57)
 
So, that is the that is the simple way in which a DTD is written, and whenever an XML document fails to conforms with a DTD a validating XML parser, that is a parser which validates XML documents against a DTD will flag an error, and stop the parsing at that point now in recent times the world wide web consortium or what is called as the w three c has come up with alternative to XML DTD’s, which are called as x m l schemas or sometimes also termed as x schemas XML schema is a much more detailed description of of an XML document. And it is it it is used for schema definition 
(Refer Slide Time: 54:49)
 
We shall not be looking into XML schema in more detail in this session due to time constraints, but but we can look at them in one of the subsequent sessions on XML documents in one of the advanced classes on managing x m l data.
So, XML schema basically defines elements that can appear in ah in a in a document that is defines elements in attributes, which are ah which appear in a document, and like a DTD it can also it also define defines parent child relationship, and defines order in which the child elements are two appear. And it defines the number of child elements one can have and several others things that that are typically described by a DTD, 
(Refer Slide Time: 55:31)
 
And there there for more richer and suppose it will more useful then then DTD is in described in schematic structure for XML documents. And XML schemas good thing about XML schema is that they do not have a different syntax from the XML syntax itself, that is unlike DTD’s which have a different syntax in the XML syntaxes, XML schemas are written in XML itself. And they also support data types and name spaces and so on, which which we look at in more detail in one of the subsequencetion. 
(Refer Slide Time: 56:12)
 
So, let us summaries what we have a learnt, today we have started this series of ah a few session, and XML data bases which is a world wide web constructions standard; that is used for data exchange and and managing semi structure data, and and self describing data and… So, it is a it is a self expressive, it is it is a self describing way of specifying set of data elements, and then when we talk about an XML document, it is important to distinguish between what is a valid XML document versus what it is a well formed XML document, and well formednesss all valid documents have (( )) well formed, but validity itself has to be checked again DTD’s specification, and now which is being replace by the XML schema specification. So, that brings as to the end of this session.

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 39

XML-Advanced Concepts

Hello and welcome. In the previous session in the DBMS course, we have been looking into managing XML data. As we had mentioned earlier, XML is kind of a markup language which is in some way the de facto standard for information interchange over the internet. And the power of XML comes from its simplicity in the sense that it’s a markup language which can be read and understood by both humans as well as machines. And it’s independent of any operating platform or independent of any standards like how schema is describes and so on. And essentially it’s a self-describing, it describes a self-describing data set in a sense. 

Now in this session let us continue with XML by looking into some advanced aspects of how XML data are stored or how they are queried and so on. And XML as I had mentioned in the previous session has elicited wide area of interest and there are several different not just in computer science in the sense that XML is elicited interest across several different disciplines because information interchange or managing different facets of information, different points of information and integrating them, interchanging information is a common problem in several different domains. And there are different kinds of standards in specific domains like banking or finance or bio informatics or transportation or whatever and several different domains which have defined XML DTDs pertaining to their specific area of concern. So let us look into XML little more deeply in this session and see what we can do with XML. 


















(Refer Slide Time: 03:36)

 

First of all let me start by acknowledging that some of the material in the slides have been derived from an invited talk by Jayant Haritsa in the VLDB summer school held in Bangalore in June 2004. So let us have a brief recap of what we studied about XML and what are its main features like I had mentioned before XML is a platform independent and standardized extensible markup language. Now each of these different terms means a very specific thing. Platform independent means that you just store XML in plain vanilla text, character data essentially. And every platform, no matter what you use, what platform you use would support textual data regardless of whatever underlying encoding that you are using.

(Refer Slide Time: 05:04)

 
And it’s a standardized markup language in the sense that there are specific rules that specify how an XML data set should look like. For example it should be a rooted tree and a begin tag should end with an end tag and there is this notion of well formedness and validness and so on. And it’s an extensible markup language. The markup language does not define, what are the kinds of tags that you need to have in your XML data. You can define your own tags and we took up some examples where a notice was a kind of tag notice and slash notice or from and to and so on and so forth. 

It’s a self-describing dataset, in the sense that the structure of the data is implicit in the data set itself or meta data is embedded within the dataset itself. So you don’t need a separate descriptor for how the data are to be described. Of course you may have a separate set of what might called as restrictions or which specify whether that description is valid or not. And it’s a very simple standard for data interchange made up of simple building blocks like elements and attributes and nesting of different elements and so on. And we also saw the difference between what is a well formed XML fragment and what is a valid XML fragment. 

A well formed XMLs fragment essentially conforms to the XML structural requirements, I mean structural requirements of an XML document in the sense that it’s a rooted tree and the nesting is proper, every begin tag is closed by an end tag and tag names do not have spaces in them and they do not begin with a special character and so on and so forth. But a valid XML data has to be well formed of course that is unless data set is well formed it can't be valid. However valid is most stringent in the sense that it’s not sufficient for an XML data set to be well formed but also it should conform to a given document type definition, a DTD or a XML schema that is a schema that is specified as part of this document. 

(Refer Slide Time:  07:12)

 

Why XML or what is the significance of XML? It’s a very convenient way, probably the most convenient way of exchanging data over the web. And like I had mentioned before it’s easier for both machine understanding and human understanding. So in the worst case you can actually open an XML document in a text editor like notepad or Emax or whatever and understand what the XML data is all about and make changes if required. And it has a simple tree structure like a simple hierarchical structure which is easier to understand, easier to enforce because the tree has very specific constraints and well known constraints as to what makes up a tree structure. 

And enforcement is also easier that is it’s easy to build parsers which using stacks or whatever. There are several kinds of tree algorithms that can validate a given tree and it’s also easier to navigate, nice directory structure is a tree structure and hence XML data sets are usually rendered in the form of a directory structure on browsers. And like I had mentioned before, XML is eliciting interest not just in the computing community but perhaps more so in several other communities. And information interchange is not just a problem with database researchers or computer science researchers but it’s a problem in almost any domain of concern.

(Refer Slide Time: 08:42)

 

What is the information interchange problem? Simply stated that simply stated it means it is a problem of how to integrate the varying or desperate sources of information that exists in any given system. FpML for example as shown in the slide is an XML standard for the finance industry. And FpML essentially standardizes the different kinds of contracts or transaction data sets that are routinely specified, generated and specified in the finance domain pertaining to banking or stock trading or loaning and several different allied activities where contracts would have more or less some common features in the sense that or contract would have to specify what are the parties to a given contract, what is the validity period of the contract, when does the contract expire, what are the components of a contract and what’s the amount involved in this contractual interchange and so on.

And there are several standards that are used independently by different sources and FpML in effect tries to standardize this contractual specifications so that it becomes easier to integrate different kinds of contracts that are generated in different places. 

(Refer Slide Time: 10:27)

 

Similarly BioML as shown in the slide is an XML standard primarily for use in bio informatics applications and that too in annotating gene sequence data. And gene sequences as you know are very long sequences comprising of one of four different characters 80 g and c sometimes u and sub sequences of this long sequence specify, this long sequence is what is called as the genome sequence and sub sequences of this genome sequences specify genes or sometimes what are called as some a set of codon which code for a particular kind of behavior in the organism or however when the gene folds. So such kinds of codon, such kinds of code on strings or genes are annotated. 

And this annotation is done in several different ways by several different researchers and  because there several researchers trying to sequence genomes of different kinds and including of course the human genome and or annotating different parts of a genome string. 

Now BioML is a standard which specifies how this annotations had to be performed and of course it allows for free text for annotations. That is it does not really place any constraints on what should go into the annotations themselves but it specifies how and where annotations should have. And BioML in addition to its, for example FpML or BioML actually specify the schema, they basically specify the XML schema of this. So the XML schema also supports full text queries for different kinds of sequence matching problems. 
(Refer Slide Time: 12:37)

 

And then a very commonly used standard that uses XML is the SOAP protocol. You might have heard of the SOAP protocol which stands for simple object access protocol which forms in some sense the building blocks of web services. SOAP is a mechanism by which a software can access a remote object or invoke, send methods to remote object via the internet.

So what is the SOAP protocol do? It is simply a message passing or a message sent to a remote object that is wrapped in an XML document that is the message is sent as an XML document and at the other end where the services provided, the XML is parsed and the actual message is taken from the XML document and then sent to the object and question. So in this for example you might have some kind of an object that performs a given kind of calculation let us say currency conversion. So you might want to perform some kind of currency conversion and that is a service that you provide over the web. 

Now if this currency conversion object is SOAP compliant then the clients speaking with your object or speaking with your component sends messages in an XML format. And the object contains an XML parser embedded into it which parses the XML document and divides the XML input into two different parts a header and a body. So the header contains meta data about the message saying what kind of message, where is it coming from and any other restrictions and the body contains the actual message itself. 








(Refer Slide Time: 14:24)

 

So this slide shows a schematic diagram of how an XML, how XML processing would probably look like. Given XML document, given an XML document we have what is called as an XML parser. When an XML parser can check for the well formedness of an XML document so that it can parse an XML tree and create some kind of a data structure here, also called as an info set which in turn goes into a document validator. Document validator essentially validates a given XML document against an input DTD or an XML schema. 

So once the document is validated then the XML is ready for the application of the storage system where whatever it is being used for. And of course the application itself accepts different kinds of queries using XQuery or XPath and so on and can answer queries on the info set that it received from the document validator. 

So what are the key concepts in XML? You have the data model which comprises of elements attributes and data and rooted tree structure plus of course comments and processing instructions and namespace declarations and so on and so forth. 












(Refer Slide Time: 15:57)

 

In addition you have the DTD or the XML schema which specifies the meta data structure. And there are several different features that are available in a DTD in the sense that it can define regular expressions over XML tags. So you can say that, notice can have zero or more headers or one or more two elements and so on and so forth. 

So here is an example XML fragment were this is a rooted tree again. It’s an XML element but every element, every well-formed element is a rooted tree in itself.

(Refer Slide Time: 16:40)

 

So this is a rooted tree which begins at imdb and slash imdb and of course you might know that imdb stands for the internet movie database which is a huge source of XML information that is it stores a lot of movie related information in XML. And then imdb itself contains an element called show and show year equal to 1993 and comments are given like this. That is angular braces with an exclamation mark followed by two hyphens and then again two hyphens with the angular braces here. 

So show is the element from here to here, so the show element contains one or more reviews. So review one starts from here to here and review two starts from here to here and then there is a set of box office numbers. And each review itself in turn says where the review is from, so this one says the review is from sun times. And the review itself is a mixed XML plus free text that is there is free text going around here within which there are some XML tags. That is Roger Ebert is surrounded by the reviewer tag and two thumbs up is given, is surrounded by the rating tag and so on. 

So that’s how a simple XML fragment would look like and a typical DTD for this would look like this where you start your DTD by a DOCTYPE declaration and then specify what is the root element of your XML fragment. Then within this specify each element and the set of attributes.
 
(Refer Slide Time:  18:22)

 

So imdb is the root element can contain show star, show star essentially means that zero or more occurrences of the show element. Similarly the show element contains title, review star that is it has to contain exactly one title followed by zero or more reviews, review elements. And show can also contain an attribute called year which is character data and title is character data and so on. So I have not completed the DTD as yet but this is what the typical DTD would look like for this XML fragment. In contrast we have what is called as XML schema which is an emerging standard for which is fast replacing DTDs (Refer Slide Time: 20:00).
XML schema is far more expressive than a DTD in the sense and it supports, it’s a strongly typed language in the sense that it supports a type of a particular kind. And most importantly an XML schema document is an XML document itself. That is you can use the same XML parser to parse an XML schema as you would use for parsing an XML document. 

So how would an XML schema for this XML fragment look like? You see that it starts with an element declaration and the element declaration has an attribute called name equal to show. So I am not starting from the imdb declaration itself, I am just declaring the schema here for the show element. So this one says the show element is declared like this. That is the show element is a complex type having a sequence of a two or more elements where the first element in the sequence is the title and the type of this element is string.

(Refer Slide Time: 22:58)

 

And this is a single element, so the element is closed right here and then the second element is and then there is a sub sequence. So the first element of this sequence is an element of name title and the second element of the sequence is another sequence which can have repetitions by itself. 

So min occurs zero and max occurs unbounded is another name for saying the star declaration in DTD that is something can occur zero or more times. So what is this sequence contain? This sequence contains an element called review and which is of mixed data that is a review can contain both free text and XML tags in itself.

And of course this sequence finishes now and then after this there is a choice element that is the third element of this sequence where I can have a choice between box office or seasons or so on. So one of these two, I can have either box office or seasons as the third element in the sequence. so note that show itself comprises of three different elements title, review star in a sense and box office or seasons which was not specified completely in the DTD in the previous slide. And of course this sequence finishes and the next element, the next attribute of this element is year which is optional and slash element. 

So that is typically what an XML schema would look like. As you can see here this is a simple XML document in itself and then you can parse this XML document using any XML parser. And then based on the outputs of this parser, you can enforce the constraints here on the given XML fragment. 

(Refer Slide Time: 23:20)

 

Let us now turn to a specific problem that occurs when parsing XML data namely the notion of naming conflicts. Now consider these two XML fragments here that are shown, one XML fragment is shown here and the second XML fragment is shown here. The first XML fragment, if you have worked with HTML you might have recognized that this is a HTML fragment and HTML fragment can also be treated as an XML fragment there is no harm in that. 

So of course well-formed HTML fragments that is in the sense that every start tag is paired with an end tag and there is a nice hierarchical structure for this fragment. So this is a table fragment which begins at table and ends at slash table. And there is a table row which begins at tr and slash tr and table descriptor that is some table cell which begins at td and slash td. 

And then somewhere down the line there is another fragment here which says table, slash table and name and width and length and so on and so forth, so African coffee table 80 and so on. Now these two could occur within a single XML document that is this could actually be some kind of PCDATA that is parsed character data for given XML element. And this could be another XML element by itself but these two tables are different. Now this is required because the data contains HTML and the HTML has to be rendered and this is required because the semantics require that we need to declare something called table and these two tables are different. Now such a conflict is called a naming conflict that is when two or more tags mean different things but have the same name in them.

(Refer Slide Time: 25:55)

 

So in order to resolve conflicts, how would we resolve conflicts. One simple way to resolve them is to prefix each of these, each occurrence of this meta data with a particular string. So for example this one shows (Refer Slide Time: 25:28) a prefix called h where it says, where the HTML table is prefixed by a character called h. So h table and slash h table and note the use of this colon character here. So the HTML table is discriminated or distinguished from the XML table by using a different prefix called h. 


















(Refer Slide Time: 25:60)

 

And similarly the XML table itself is prefixed by f. Now that is one simple way of doing that but again the question still remains as to what if the prefix also is the same, I mean if the name of the tag can be identical, the prefixes can also be identical when two or more XML documents are brought together and meant to be integrated into one schema. 

So for this the notion of names spaces becomes valid or namespaces become important. A namespace essentially defines a unique space globally, worldwide across the web. So how does it define a unique name space across the web? Remember the notion of a URL or a URI uniform resource identifier. 

A URL or the more general form which is the URI is a unique name for a resource across the web. Therefore whenever you see a address like this (Refer Slide Time: 27:09) http://osl.iiitb.ac.in whatever. it is unique that is this stands for, it has to map on to a single ip address or a single address across the world wherever in the world that it is referred to. 













(Refer Slide Time: 28:15)

 

Now using the concept of URIs, we can ensure that naming conflicts in XML documents can be resolved. For example we can, when we say that when we put a prefix called h for example for this table meta data, for this table tag then we describe that h means a specific URI. Now this could be, this need not actually contain anything. The URI need not contain anything with respect to this table but let us say this is the URI in which we work in, let us say where we created the XML fragment.

Now once we alias a given prefix to a URI it becomes unique. So in a sense whenever the XML parser looks at looks at a prefix called h followed by colon, it replaces that with this URI. So if a different h comes from a different source and it maps to a different URI, it means that it is a different prefix. Therefore it basically means some other tag and not the same h as this prefix that is specified here.















 
(Refer Slide Time: 29:30)

 

So similarly we can use a different that is the same URI having a different prefixes so that different prefixes from the same URI can again be distinguished. Therefore what the XML parser actually does when it encounters name called f colon is that it replaces f colon by this whole thing slash f (Refer Slide Time: 29:20) internally. That is http: //osl.iiitb.ac.in/oslwiki/f. So that several different prefixes that have been defined at this URI remains distinguishable.
 
(Refer Slide Time: 30:00)

 

So the namespace attribute as we saw in the previous slides is placed in the start tag of an element.  
So for a given element, for every element you can define a separate name spaces where names for that element are uniquely describe globally across the world. And the syntax for name space begins with an xmlns, xml namespace declaration followed by a namespace prefix equal to a coated string following the namespace or coated string following the URI. And it basically gives a qualified name associated with a namespace.  

(Refer Slide Time: 30:25)

 

So, all child elements if I define a namespace for a given element like in the example table there, all children elements of this element are associated with the same namespace. So their names are term to mean unique meta data that have been defined in this URI. The address is just simply the URI address that is specified is used to simply identify a namespace but the parser itself does not try to connect to that URI or look up that information or whatever. It is simply used for resolving naming conflicts and it doesn’t necessarily have to check or validate whether the URI exists and whether the URI defines this names or anything of that sort.














(Refer Slide Time: 31:19)

 

Similarly let us look at some more issues related to CDATA. CDATA if you remember is character data. Character data, when I define an element as character data then it means that everything until the slash of that element is taken in without parsing that is the data itself might contain other tags which are not parsed and everything is taken until the end of that element, so everything is ignored until the end of this element. 

But then if the text or the character text contains a number of these characters, let us say less than or ampersand and so on, it is quite easy for XML parsers that especially if the y are little buggy it can be quite easy for XML parsers to get confused and look and try to parse them and especially if it is HTML data and not well formed and the parser might flag errors even though the XML itself is well formed and valid and so on. So there is another way of declaring CDATA where a CDATA section can be declared using a specific tag like this. 















(Refer Slide Time: 32:44)

 

So this slide shows an example where script and slash script is an element that defines CDATA. And this element contains a CDATA section which says CDATA and then defines a function or defines some script here with less than and greater than symbols and ampersand symbols and then ends the CDATA section like this here. Now what are some of the rules for CDATA sections. Simply that the body of a CDATA section cannot contain this string which defines the end of the CDATA sections (Refer Slide Time: 33:28).

Hence, which also implies that nesting of CDATA sections is not allowed. So you can't have nested like you can’t have nested comments in C, you cannot have a nested CDATA sections and there should be no spaces or line brakes inside this string. 

















(Refer Slide Time: 33:51)

 

Now let us look into querying XML data and what kinds of query languages are present and what paradigms of queries exist. The common querying tasks that are usually done over an XML data are something like filtering and selecting, navigation, selection, extraction and so on. In addition you could define some kind of joins or aggregation like we do in SQL and transformation that is convert one form of an SQL data to another.

(Refer Slide Time: 34:11)

 

So before we start up with querying itself, we have to address a more fundamental problem or fundamental issue of XML parsing itself and there is one, there are two specific kinds of XML parsing which have implications on how querying is performed and let us briefly look at these two paradigms of XML parsing and what kind of impacts they have on queries. 

(Refer Slide Time: 35:27)

 

The first kind of XML parser is what is called as a full navigational parser that means of which you might have heard of this name called DOM or document object model. This is an example of such a parser which requires an entire XML document to be available for it before it can start parsing. That is the XML document should be full, complete and well-formed and so on and which can then be parsed and DOM basically creates some DOM object which can be called by application programs in order to query the XML data. 

So the application requirements state that it should provide full navigational access to the document. So you cannot have partial XML string that is available and ask the DOM parser to start parsing. And kind of queries that DOM allows is something like this. that is DOM essentially creates an object which contains a XML document and then you can say something like if document element.getElementsByTagName show then, so which basically gets a given element of this thing then for s in this thing that is s is a element which is the set of all show elements. And in the show element if the title contains the fugitive then get the review and so on.

So you can address or dereference an element by the tag name and then it returns an element object to you and then you can get the data associated with that object and so on. 







(Refer Slide Time: 37:02)

 

So this slide here shows how the DOM parser works that is you give an XML document plus a DTD or XML schema to the DOM parser. The DOM parser performs both parsing and validation and creates an object or a DOM instance which is then given to the application. The application then start calling the DOM instance by using it or integrating it with the other sets of its objects. 

(Refer Slide Time: 37:47)

 

The other kind of XML parsing is what is called as stream based parsing. And example for such a stream based parser is SAX which is also widely available on the Microsoft windows platform. 
Again this is a language independent and programmable, programmatic API and SAX in contrast to DOM does not require full navigational access to the given XML document. Instead, you can stream the XML data and it parses as the XML data parses through it. That means you can put an X, you can put a SAX parser on a network stream which is sending you XML data and as and when the XML data streams through, SAX creates a XML object, i mean it creates an object on the fly for the XML data that is parsing. 

In other words it also means that SAX performs just one parse over the entire XML stream. And there are several applications where SAX is more important something like stock quotes which are sent in a streaming data over the web and as and when codes change, data streamed and an XML parser, a SAX like parser can parse them and what SAX does is it also has a feature of call backs. That is SAX not only parses the XML data but it also creates events that can call back into the application and interrupt the application and tell that something is happened, so take appropriate action and so on.

On the other hand in DOM it is the application who is in control and the DOM just creates a, DOM parser just creates an object and the application decides when to call the object and what to do with the object and so on. And of course in SAX its read only access for un typed notes and there is no in place updates that’s possible. 

(Refer Slide Time: 39:56)

 

So, this slide shows how a schematic diagram of how the SAX parser works that is you have a XML document plus DTD which is streaming that is which need not be the entire document as such. Now this streaming XML document parses through a SAX parser and validator which in turn sends SAX events to the application. That is the application keeps interrupting, SAX parser keeps interrupting the application by sending appropriate signals or events to the application. And the application has to perform specific tasks associated with each of this events. 

So it will say that found an element called show and which is valid and this is the element and so on. So, as and when a show element comes, the application says that a new show has arrived and this is the data that has to be rendered and so on. So the application begins the SAX parser but once the application begins, it’s the SAX parser that’s in control which calls back the application. 

(Refer Slide Time: 41:00)

 

So, let us look at XML query languages. now look at these two kinds of parsing techniques let’s see how they can affect queries as well but of course before that we should look at different kinds of XML query languages itself. 

And some examples are shown in this slide here. you have query languages like XPath, XPath 2.0 which is very commonly used language for specifying navigations or selections or extractions from an XML tree and it’s also used in, I mentioned the name called XSLT which is another kind of query language it’s more like a transformation language. That is it can convert XML to HTML or it can convert XML to text or one form of XML to another form of XML and so on.

So if you want to render an XML document, you generally use an XSLT query language which will take an XML document and give out a corresponding HTML document in return. and then there is a XQuery kind of language which is a, what may be termed as a composible language in the sense that one XQuery takes an XML document as input and gives out another XML document as output. It’s very similar to the relational algebra queries which takes a relation as input and gives a relation as output.


	

And it’s a strongly typed XML, it’s a strongly typed query language and useful for large scale database accesses itself.

(Refer Slide Time: 42:40)

 

So let us briefly look at XPath. XPath is a syntax for defining parts of an XML document and basically the way it defines parts is to use a directory like parts structures where in order to define XML stands or XML elements.
 
(Refer Slide Time: 42:56)

 



Let us quickly go to an example to illustrate this. This slide essentially says that path expressions in XPath look very similar to the directory structure in a computer file system. So I might have a directory called this slash this and slash this and so on. So if this is what to be an XPath expression, all this would be elements so some OSL.iiitb.ac.in would be an element and grace would be an element and so on.

(Refer Slide Time: 43:48)

 

So, this slide shows an example where an XPath selects a book element which lies under a catalog element where the price attribute of the book element is equal to 10.80. And the catalog element is the root element of this XML document. So this is just like the root directory in your simple file system language and so the root directory contains, defines the element called catalog, under the catalog look for an element called book and look for an attribute matching this particular criteria. 
















(Refer Slide Time: 45:14)

 

Similarly there are other examples here. If you have a double slash like this, as shown in this example it says that return all titles at any level in the imdb XML document. That is the root element is imdb and title can occur at any level, so double slash essentially means any level in your document. And similarly this one double slash at the beginning again says that return a show element at any level and doesn’t matter what the root element also is but where the show year is greater than 2000 that is all shows released after 2000. 

And you can also have full text operators like text contains Russell Crowe and so on anywhere in here… there is no element name as well that is given, so it just says star. So any element in this XML document at any level return that element where the text contains Russell Crowe. So that’s about XPath where, it’s a very brief introduction to XPath where which is a file system like language for representing different navigational aspects of an XML document. 














(Refer Slide Time: 46:04)

 

And this is where the hierarchy or tree structure of an XML document becomes significant in the sense that it is easy to express a tree structure in the form of a directory structure as done in XPath plus it is easy to enforce constraints. That is in a tree structure, a node can have at most one parent that is any non-root level node will have exactly one parent and there is exactly one path from the root to any given element in a XML tree. So you can actually specify one long file system like path which uniquely identifies each element in the XML document. 

The next kind of query that we are going to look at or query paradigm that we are going to look at is the XQuery paradigm. XQuery is a functional language in the, it’s a strongly typed query language for querying XML data. and XQuery as I said earlier is an XML to XML converter that is it can query an XML document and return an XML document just like the relational algebra queries that can query a relation and return a relation.

An XQuery in turn can use XPath expressions for its queries, so XPath can become a part of XQuery in specifying its query expressions. And many people would term XQuery as an SQL for XML databases. So it’s analogous to SQL in the sense that it takes an XML document, returns an XML document, it contains several different operators, it can contain several different sophisticated query operators something like select from where or and so on. 

So you can actually specify a query like select all papers which have been sighted more than 50 times from the collection of papers stored in the XML document called citeseer papers and so on. So you can give complex expressions based around a select from where kind of class.



(Refer Slide Time: 48:20)

 

So what are some of the XML XQuery features? They are what are called as flower queries or FLWR queries which say that which stands for let where return repeat queries. That is it’s a looping kind of queries where you can specify a for condition initial condition let where is similar to the SQL where and then repeat, so you can repeat these queries. We will see an example of this shortly.

Then there are SQL like select from where clauses were select so and so from this XML element where this condition matches. Then there are sort by operators, so you can sort elements based on certain attributes then XML construction that is transforming one XML document to another XML document. You can also have user defined functions where on this and XML XQuery basically supports strong typing. So you can say something like an integer or character or you can perform operations that are specific to integers versus character strings and so on. And it also has supports for processing recursive data sets.














(Refer Slide Time: 50:04)

 

Here is an example of an XQuery query that is a query written in the XQuery language. So the query essentially says that for each actor return box office receipts of films in which they starred in the past 2 years. And essentially let us go back to, let us go first to this last part of the XQuery document were here you say lot of let for and so on and so forth, for let where and so on and repeat and so. But this last one here is what is going to tell the Xquery engine what to return. Now here that imdb engine or the imdb XML document did not contain an element called receipt. 

However what it does contain are elements like box office or actor and so on. And what the XQuery languages returning is an XML element called receipts and slash receipts which in turn is made up of actors and totals and so on. So receipt is, receipt essentially is an XML document comprising of character data which tells what is the actor and then one more XML element called total which contains the sum of all box office I mean films per box office receipts that they have obtained over the past 2 years. 

And here an XQuery for example can first define variables like using a Pascal like syntax, so it says let dollar imdb equal to this document. So imdb basically specifies this particular document and then this is for any actor in imdb actor. That is actor is another variable. Note that all variables are prefixed by a dollar sign and where actor stands for any actor element at any level in the imdb document. So for any actor in this thing, let films equal to this one that is the show element where it contains a box office element and year greater than 2000 and actor name is the particular actor is entered as a star in this show, so star name and so on.

So essentially you let this one and for each actor this for let and the where is implicit here and return is also specified. So, this one iteratively performs for each actor at any level in the specified in the imdb document. And then it returns a set of this receipts elements which says actor and total that the actor has grossed. 
(Refer Slide Time: 52:51)

 

And here are some www links or World Wide Web where you can get more information about DOM parsers or SAX parsers or XPath queries and XQuery standards and so on. But before this let us go back to the parser problem that we are talked about. That is how does a parser affect the queries that you give on an XML document. Both DOM and SAX parsers for example create documents or create objects which the application can access that is which the application can send messages to an access. 

However the DOM parser requires a entire XML document to be present whereas the SAX parser calls back into the application as and when XML elements parse through the parser. Now what kind of implication does it have on queries? Let us take the query here. Let us say for each actor return box office receipts of films in which they starred in the past 2 years. This is when you give this, such a query to a XML document that is parsed using DOM, you essentially know that the entire XML document has been navigated and parsed and that is present in the object that is available here. So you can essentially go and look through the object and return the query results. 

On the other hand if it is a SAX parser and it’s a streaming XML data then you won't be sure whether you have encountered all possible actors in this loop. That is take a look at this (Refer Slide Time: 54:45) for actor in imdb slash actor, so you will have no idea whether all actors have been processed as part of this XML document. 

So in such a sense, in such cases this query has to be in the form of what is called as a standing query. That is in the traditional database setting, the database is static and the query parses through the database and then returns query results. But here it’s the other way around, the query is standing, the query is static and the database or data set parses through the query and the query returns backs or returns events or performs call back into the applications whenever a data set matching the query is available. 

So as and when the query finds an actor and show satisfying this criterion, it returns a receipt kind of XML fragment by calling back into the application. 

(Refer Slide Time: 56:14)

 

So let us summarize what we learnt today in this session. We started with XML namespaces and how to resolve naming conflicts by assigning a unique, globally unique URI for each name prefix in an XML document. And we also looked at some CDATA handling issues where especially if your document, if your CDATA contains angular braces and ampersand symbols and so on, you can embed it within a CDATA section.

Next we looked at two different kinds of XML parsing namely the navigational parsing of the entire document versus stream parsing and then we looked at XPath and XQueries as query languages or query paradigms over XML data sets. And how these kinds of parsing techniques can impact the kinds of queries that or how the query behaves in each of these parsing techniques. So that brings us to the end of this session. 
	
Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 40

XML Databases

Hello and welcome. In this final session on introduction to XML, we shall be delving primarily into XML databases. That is until now we have been talking about what is XML and what are the benefits of XML or data interchange using XML and XML queries or schemas and so on.

(Refer Slide Time: 01:49)

 

But today in this session, we shall be looking into XML databases and what are the issues that come up when XML data has to be stored or transmitted and exchanged over preexisting or some set of existing data stores. We shall also be looking into the larger problem of what is called as semi structured data management. Semi structured data as you might have imagined from the name is data which do not have specific rigid structures. And we shall show that more and more the data requirements of today’s world what we call the post internet world is being defined by semi structure data rather than structured organized data. And semi structured data possess some unique problems in fact some fundamental problems in database management. And what we shall do here is that in this session is we shall only motivate those problems rather than providing any solutions as such many of which are in some sense areas of active research. 

So we shall be motivating the problems with some examples or some analogies and instances and we shall conclude this session with a list of what one could expect in terms of semi structured data management. 

So let us proceed further with this session and as with earlier session some of these material have been derived from an invited talk by Jayant Haritsa in the VLDB summer school held in Bangalore in June 2004 and of course including many of the analogies and jokes that have been there in the slide and so on.

(Refer Slide Time: 03:20)

 

So here is an acknowledgement for those set of material that have been derived from those slides. So let us look back at XML again and recap what we have learnt about XML and what are its characteristics. First of all before we look into the points in the slide itself, let us remember that XML is a platform independent, standardized and extensible markup language.

So, in a sense that its platform independent because it is written in character data and every platform should at the very least support character data and it’s an extensible markup language that is it does not have a specific set of keywords as such. I mean of course it has key words where you say, where you give declarations but for describing the data itself, it doesn’t have any keywords. 

The user who is describing the XML document can come out with his or own set of tags or meta data that describe how the data is organized and what semantics to attach to each data set in an XML store. And XML syntax has a plain hierarchical structure which is easy to navigate and easy to enforce and easy to parse as well. So we have, as a result we have query languages like XPath which translate an XML query into a directory like path expression which is quite intuitive for users who have been using computers and so to traverse directories and look for something in a specific directory and so on.

And last but not the least, XML is understandable or parsable by both the machine and the computer. So if everything fails what you can do is just open  an XML document in notepad or vi or Emax or some such text editor and look through the document and see where there could be a problem especially, if may be starting tag doesn’t have an ending tag or some problem of that sort.

And of course there are issues like what about the amount of space, extra space that XML takes up and this can be answered by the fact that one need not store XML in character form on disk. You might want to actually compress XML data and then store it on disk, so that you can and decompress it when reading it, so that the actual disk space that is taken up by XML is reduced. 

So let us address the question of data interchange now using XML. XML is said to be a platform independent source of data representation and representing data that is self-describing in the sense that the meta data is embedded within the data. Now, but what is the problem, what is real and of course the advantage of this being that you don’t have to worry about whether the end user is using the same platform as yours.  

(Refer Slide Time: 03:38)

 

If you are using Linux, the end user could be using windows or Solaris or Mac or whatever and the end user could still use your services using SOAP or some other form of XML based message parsing protocol and you can still answer or you can still provide those services via an XML wrapper. But what is the interchange problem actually or in its entirety? For the interchange problem, we have to first note that most data is already stored in some existing databases. It’s quite unlikely that databases that have been existing for large periods of time now will be replaced by an XML data store or will be replaced by something else that is unifying.    

So databases are going to continue to exist and the question of data interchange will boil down to the fact that how to interface these databases using XML or using a common interchange language. And databases will not only exist, will be updated through existing interfaces. It’s again quite unlikely to expect that all updating and all interface to the databases will come through a common interchange format. So what is really required is to provide XML wrapping to existing data bases and SOAP for example in this regard that is message parsing between objects existed long before XML came into the picture. There were several different object middle wares like CORBA or DCOM and so on which supported message parsing between objects over a distributed system. 

But however one needed to be CORBA complaint in order to be able to, a platform needed to be CORBA complaint in order to be able to support CORBA that is CORBA had to be returned for that platform. On the other hand when using XML, it does not bother about what kind of platform that’s being used because every message is parsed using an XML wrapper.

So SOAP is some kind of a generalization over message parsing frame works in distributed object base systems which has been extended from simple, small distributed systems like CORBA to a larger web base services using SOAP. So a similar analogy also exists in data integration where data exists in different databases in preexisting forms and preexisting applications and they have to be integrated using, by wrapping the data around using XML wrappers. 

(Refer Slide Time: 10:18)

 

So one simple way to convert databases and now here we are assuming relational databases. And for the most part we would be correct because majority of the database implementations around the world are relational databases. So assuming that existing databases are relational databases and we need to be able to interface between these different relational databases which are across different platforms. A simple way to perform this is to convert relational tables in their canonical form to XML documents. 

And this slide shows (Refer Slide Time: 10:49) such an example where there is a slide called actors containing of two different columns last name and first name. And this table became one XML document actors and slash actors, it’s a rooted XML document. And it is some kind of a flat XML document in the sense that the number of levels in this document is fixed. That is the actors file or the actors document comprises of several children called row and slash row where each row comprises of again several children, each child corresponding to a specific attribute or attribute name and this is an attribute value within the attribute name. 

So it’s quite simple to map a given relational table on to a canonical flat XML file and given a flat XML file, it’s again straight forward to map it back into a relational table.

(Refer Slide Time: 12:07)

 

Now let us come to the question of storing XML documents itself that is having XML databases itself. In many cases for example in several commercial implementations, oracle 9i and DB2 and so on, the database gives an impression that it supports XML storage and XML based updates. However what it actually underneath, it’s still a relational database and then there is an XML wrapper around the database. 

However that is there are several other approaches to storing XML documents and in many cases it becomes necessary to treat XML not just as an interchange language but also as the language in which the data is stored. And some of the issues that occur in this regard would be issues like data layout. How would you organize the XML data there are again many alternatives to this question. 

One can map XML data on to a relational database or one can map an XML dataset as a special kind of object in an object relational database or one can store XML data in native XML form or in text form and so on.

And what about updates? I mean XML databases are prone to updates and here in XML unlike in say relational database it’s not just the values of attributes that can change, the actual structure of the database can also change very frequently. In a relational database we basically assume that once we fix the schema, it is intact and it’s only the data sets that are updated. That is new attributes are added or existing attributes might be deleted or modified and so on. But the evolution of the schema itself that is change in a structure of the database itself is considered to be far less frequent if at all it happens. So the database is modified, database is oriented, the relational database is oriented towards fast updates and retrieval of data and not the structure as such. 

But in an XML database that may not be the case that is there could be updates of not just the values but also of the structure of in which data is stored in the database. And in many case there may be no explicit schema that’s available, so well formedness itself is the schema. so we don’t we don’t really know which is a valid structural update and which is an invalid structural update and what kind of constraints exists between different elements and so on, unless there is a XML schema or a DTD that’s available for us.

Similarly there are different kinds of query supports that need to be supported by an XML database. this standard flower queries that we saw earlier where for let where return kind of queries and also select, project and join based queries where which is also supported in XQuery in addition to recursion or document construction that’s transformation from one XML document to the input XML document to the output XML document. And a far bigger problem is of indexing. How do we maintain meta data or how do we maintain indexes into an XML store? Such that we can retrieve elements as quickly and efficiently as possible whenever required. And it’s not just the normal attribute and value index that we need to store but we also need to store full text indexing, what are called as inverted indexes, so to be able to search a full text elements or data sets quite efficiently. 

In addition to these kinds of indexes we also need what are called as structural indexes, I mean structural indexes essentially talk about what are the relationships between, structural relationships between different elements is one a child of the other is one reachable from the other or what are the siblings of a given element and so on. And in addition there are general requirements like scalability and recovery in case of failures, concurrency control, updates and during updates and so on. 













(Refer Slide Time: 17:00)

 

So let us look at XML storage structures itself and what are the different kinds of choices we have for XML storage. Basically we can divide the different kinds of XML storage that storage structures that are, storage paradigms or storage mechanisms that are available into three different classes, what might be termed as flat stream based storage or native XML storage and colonial storage. So what do each of this terms mean? A flat stream based XML storage essentially stores XML data as some kind of objects in an object relational database. They are stored as CLOBs or remember what is a BLOB. A BLOB is a binary large object and a CLOB is a character large object. 

So you just store an object comprising mostly of characters, so character large object which contains its own mechanisms of access and retrieval as one of the attributes in a relational database. So just like storing any multimedia object or some such object, you can store an XML dataset in a relational object relational database itself. But of course the advantage of this is that, you don’t need to reinvent anything in the sense that there are several object relational databases that are already available. And it’s just a question of using one of the object relational databases to store XML documents but however of course the flip side of this argument is that the database itself does not support any XML centric queries. So you can query the database based on object relational constructs but not an XML constructs itself. And all those XML specific query constructs like flower queries or XPath queries and so on cannot be directly supportive. 

The next kind of database storage strategy for XML is the native XML storage. Native storage essentially means that you design a new database from scratch for storing XML data, optimized for storing XML data. So here you have to worry about everything that you thought about let’s say for designing a relational database, one is to think about the storage structure, block accesses that is the physical storage structure, indexing structures then some kind of query mechanisms and recovery mechanisms, concurrency control, transaction I mean throughputs and I mean how to maintain efficient transactions and so on. 

So everything that goes into designing any conventional DBMS should go into designing this native XML storage as well. And there are quite a few examples of databases supporting native XML storage. The third strategy is what might be termed as a colonial strategy. A colonial strategy essentially means that colonize an existing paradigm using XML. That is use some kind of, use an existing paradigm like say relational paradigm and then map every XML construct to a relational construct rather than note the difference between a colonial strategy and the first strategy which you just put XML storage as character large objects or one of an attributes of a relational table. 

However a colonial storage essentially decomposes or deconstructs an XML document into relational constructs and reconstructs them back. So whatever XML related query that you provide like XPath expressions or flower queries and so on, they have to be rewritten in SQL and you have to have a mapping, one each to have a mapping between XQuery constructs and SQL constructs and vice versa. And of course this can be both an advantage and a limitation. The advantage primarily being that you don’t have to worry about number of techniques that the native XML storage has to worry about storage structures, block storage structures, concurrency control and recovery and so on. 

However there is a significant if not huge performance overhead in terms of mapping an XQuery construct into an SQL construct. Especially when there is a recursive kind of a query that’s given, mapping it into an SQL construct can take quite avail and running the SQL query on the database can also be quite inefficient.   

So let us look into the second strategy in a little bit more detail namely the strategy of native XML storage. That is redefining or redesigning an XML database by looking at all the different aspects that needs to go into an XML store. So what are the issues, what are the typical kinds of issues that one needs to worry about when designing a native XML storage. One need to think about data layout, how is data organized on the disk and physical data layout is essentially that is if my disk is organized in terms of pages, disk pages or disk blocks what should each block contain and how should the blocks be organized and so on. 

Indexing is again a major requirement like we said before it’s not just attribute and value indexes that are important, we need to also look into full text indexing or indexing of phrases within text or structural indexes that talks about how elements are related to one another and so on. 







(Refer Slide Time: 22:25)

 

And it needs to also address query support, what kinds of queries are supported in this store and how do we optimize queries and how do we preprocess, do we need to preprocess or and if so or how do we preprocess for managing efficient query retrieval and so on. Then access control, concurrency control, updates and so on transactions, recovery and so many other issues. 
 
(Refer Slide Time: 24:00)

 

Now let’s look at the question of data storage in a little bit more detail. And what are the different approaches that are used for managing or storing physical disk blocks or managing the physical data storage that is how is an XML tree mapped on to physical disk blocks. Essentially one can think of, essentially an XML data set is a tree. So this slide shows a particular tree like this where there is an imdb at the root node and there are different show elements in the second node and each show element has different sub trees like title, year, box office and so on and so forth. And finally the leaf node contains the dataset that is available in this XML tree. 

Now how do we divide this tree into physical data blocks and which kind of division makes sense? One way to think of, one way to look at dividing a tree into disk blocks is to cluster trees based on sub trees. That is this show sub tree would go into one disk block and this show sub tree would go into another disk block and the imdb would just be in an index which in turn just stores pointers to each of this disk blocks.  

Now what is the advantage of such a storage? The simple advantages that the entire sub tree is in one disk block, hence if I am looking at navigational queries where the user just navigates from like opens an element and looks at the sub tree and opens another element underneath and looks at that sub tree and so on like in a explorer kind of file system navigation. For such kinds of navigational queries, this storage structure is very efficient because when the user is navigating, the navigation path is quite close to the actual way in which elements are related in the tree itself. It is quite unlikely that the user would open this sub tree and start navigating here. So in one disk access, one can read an entire sub tree.

However there is a flip side to this kind of data access itself. Suppose the user gives a XQuery kind of, a user gives a query saying show me all elements, show me all show elements where the title contains whatever the fugitive or something like that. Now if the query has to be searched on a particular element like say title and even though we know which element has to be searched for, we still have to access every data block in the disk because every data block which contains the show element, which stores the show element would contain a title element. So we still have to access every data block containing the show element in order to access such a query. 

So to answer attribute kind of queries, such a kind of organization is a file organization is actually counterproductive or is not very efficient. However for answering navigational kind of queries, such a kind of access is quite useful. 












(Refer Slide Time: 27:23)

 

The second kind of database storage structure is to cluster similar elements within a database, within a data page that is within a disk block. So, this slide shows such an example where the same tree is taken imdb and show and title, year, seasons and so on. However rather than storing an entire sub tree within a data block here, each element at a particular level are clustered together. So, all title elements are clustered together under stored in one data blocks, so in the red data block let us say. And all year elements are clustered together and stored in one single data block that is a blue data block and so on. And then there are pointers that point to each of this data blocks and so any of these elements that don’t contain CDATA or PCDATA, you can cluster all of these into one data block and then maintain pointers from them to each of these data blocks. 

Now, again this is in some sense the dual of the earlier mechanism where such a kind of organization is very useful or very efficient for answering attribute queries. So if I want to say show me all show elements or return all show elements where the title contains the term the fugitive then all that you need to do is to first access this block which contains the show element and find a pointer for the data block containing all the title elements. And with just one data block access, for one will and of course one or more I mean depending on how many title elements are there but generally with far lesser data blocks than that are necessary in the previous case, we can access all title elements that are there in this XML store. 

So answering an attribute query is far simpler, however answering a navigational query becomes difficult in this case. And there are other techniques for this thing, it depends on how something is clustered. If clustering is performed based on what may be termed as the lowest level elements where the elements contain just CDATA or PCDATA, it becomes difficult to navigate. That is from show, you need to open a title and year and from title to year it requires a different block access and so on. However it might be possible to cluster based on paths rather than based on single elements, so rather than clustering similar elements, some other techniques cluster similar paths. 

Therefore one would say that cluster all paths of the form imdb show and title in one block and all parts of the form imdb show and year in another block and so on. So there are different variants, however if one were to ask the question which is the best way of storing, which is the best storage structure for XML databases, the answer would be depends. Essentially there is no single, there is no single technique that is universally most efficient way of accessing or storing XML databases. And to a large extent it depends on what kind of queries that you expect from the users. So if it is navigational queries, it might be better off to cluster the tree based on sub trees, so store sub trees within data blocks. On the other hand if it is more of attribute searches or even say full text searches, it might make sense to cluster similar elements in a page rather than sub trees. 

(Refer Slide Time: 31:54)

 

Let us look into the problem of indexing XML documents and what kind of indexing requirements arise for XML document. Firstly, we said that in addition to attribute value indexing, for which we can use traditional RDBMS indexing like say B plus tree or a B tree, here we need two other kinds of indexes. one is what may be termed as full text indexing were we should be able to efficiently search on free text data that are written within an XML document. 

So one might just write a paragraph within an XML document and be able to search for some key word in that paragraph. And very common mechanism of indexing full text is what is called as an inverted index. An inverted index is very similar to what you would find at the end of a book like say text book were you have an index and there are certain keywords and if page numbers or section numbers in which the keywords are, in which those keywords are either defined or used or whatever.
So an inverted index on an XML document would index different keywords that appear in the document and then maintain links in to the XML document saying where each of these keywords can be found. And keyword based indexing can be of two kinds, it may either be an XML aware keyword indexing or an XML unaware keyword indexing. So what is the difference between the two? XML unaware keyword indexing just looks for keyword searches. So you just give a keyword search called the fugitive or jerry sign field or whatever that appears in the XML document. And keywords are searched, this table here shows XML unaware keyword searches that is for every term that appears, it just stores the document reference which document contains this or probably the element   reference or whatever. So it doesn’t bother about where keyword appears and it only bothers about the keyword and the value in the keyword. 

On the other hand an XML aware keyword or an XML aware index not only contains the keywords but also another element or another column which says in which element does the keyword appear from. So there is one more index that this ampersand t1 here is an element index or is a key into an element index where it identifies each element uniquely that is present in the XML store. So what this says is that the term fugitive can be accessed or the term fugitive appearing in the element whose key is t1 can be accessed using this pointer from wherever. So if you are looking at attribute based searches and we want to say return all show elements where the title sub element of show contains the term the fugitive. 

Then an XML aware indexing makes much more sense than XML unaware indexing. and of course the flip side of XML aware indexing is that as the number of elements increase and keywords are repeated across different elements, there is a huge amount of combinational choices that appear between a given term and an element pair. So the term 1993 for example may appear in different kinds of elements, it might appear in births date, it might appear in release date, it might appear in show date or whatever and so on. so different kinds of elements, so each of these have to be indexed separately and which leads in increase in the size of the index structure. So this table shows different XML, native XML databases that are available and quite a few of them are already available like Xyleme or Natix or GoXML and so on. 

And most of them have been built from scratch and some of them like eXelon or Tamino have been built over existing databases. So it’s not in a pure sense, a native XML storage but they do they are called native mainly because they do address many of the questions that are typically addressed in native XML storage. And there is a wide diversity or wide ray, wide diversity in the kind of features that are supported. 








(Refer Slide Time: 36:43)

 

So Xyleme for example supports full text searchers and XPath searchers and XQuery searches but it doesn’t have any what kind of APIs that it provides or unknown and Natix doesn’t support any of these but just supports some low level primitives and so on. and there is partial support for XQuery and so on.

(Refer Slide Time: 37:10)

 

Let us now move into the second part of the stock where we look into managing semi structured data. 

So until now we have been looking primarily into XML and XML is a very comprehensive tool to manage semi structured data. And so what is semi structure data and what is the significance and why is it important to study about semi structured data? 

(Refer Slide Time: 37:53)

 

So, let us first define what is semi structured data. There are sever several different definitions and of course semi structured data what we understand often is data that whose structuring is not rigid and data which doesn’t conform to a very rigid structuring mechanism. There are other kinds of definitions as well like data that is inherently self-describing and self-describing data, so with no rigid schema which basically implies there is no rigid schema, the data itself defines its schema and that is known as semi structured data. And again data which are generated half hand and without planning and so on, there also called semi structured data. 
















(Refer Slide Time: 38:29)

 

Now in today’s world semi structured data is getting more and more prevalent and data structuredness is becoming much harder to impose and define an impose. For example what is the structure of the world wide web, I mean the world wide web is the huge data store but without any structure, in fact but one cannot even say that it is an unstructured data store because there is some semblance of structure that is present like one can think of some meta data tags that are available for each HTML text, some HREF hyper link references and so on and directories and so on and so forth. But on the whole it is not possible to define specific structure and then impose the structure on the world wide web.

So the world wide web is the best example for huge semi structured data store and most semi structured data stores are characterized by rapid or rapid changes or very frequent changes to the data set. So not only is the data not defined apriori or the structure of the data not defined apriori, it is also changing dynamically or it’s also changing rapidly. And it is not able to, it’s not possible to formalize semi structured data using a nice formal model like the relational model and the best data structure that’s used to formalize a semi structured data are usually graph structures. 

And there are several different examples for semi structured data like web information systems and digital libraries or even data integration from heterogeneous data sources can be considered to be a semi structured data problem where there are several different databases that are already defined. And there are so many such databases that it becomes impossible or impractical to be able to impose a common structure over all of them and it is easier to treat them as a large semi structured data store. So the very common example of semi structured database is of course the internet movie database which we have been seeing continuous examples of when we are talking about XML as well.



(Refer Slide Time: 40:54)

 

So imdb is a classic example of a collection of semi structured data and even though I mean what makes a semi structured? The fact that even though it just stores information about movies, each movie is different from the other. Each movie may belong to a different journal and it may belong to different country, some may have language fields, some may have some star cast fields, some may have some other kinds of fields which may not be available in the other records and so on. So let’s consider an example from a movie database. Let’s say imdb is the database and of course imdb not just stores movies, it also stores information about tv serials and documentaries and other such movie related or movie like data that have been released. 

(Refer Slide Time:  41:49)

 
So even within a movie, let us say even within the movie category different movies could be different, movie one may have information about the cast and the director in the movie and who could be the cast in the director but movie three may have something called actors and actresses, it may make a distinction between who is the actor and who is the actresses and the direction that is rather than just the director, it can talk about a direction team or who directed it and so on.

So the structure of each record that makes up a movie element in imdb may be different from one another. And some data elements may annotate more information than others and some may have missing fields and the kind of relationships that exist between each of these different elements may also change, may also vary between different records.

(Refer Slide Time: 43:02)

 

And in addition to that, in addition to changes in structure the way in which data is organized itself could change. For example one might represent an actress name as first name, last name and one might represent it as last name, first name or one might, some other record might have its something like just a name and so on. 












(Refer Slide Time: 43:20)

 

And data gets added to this database dynamically and as a result and dynamically and from different sources from different independent sources. So it becomes difficult to enforce a particular kind of schema restriction on this database. So what is the problem here or what is the main problem in managing semi structured data? The main problem is trying to ascertain what structure or what is the common structure to use for different data sets that are being added to the database and to be able to formulate queries and formulate query languages and so on. 

(Refer Slide Time: 44:15)

 

And in addition we should also note that the structure of data element is implicit. So it’s not that the user providing the dataset first defines the structure and then provides data according to it but structure is embedded within the data itself. 

(Refer Slide Time: 44:33)

 

Now the structure has to be first discovered and then a common structure has to be evolved over the entire data set. And this is what is called as the problem of discovery of structure that is the structure should be discovered such that the structure is indicative in nature rather than constraining in nature.

(Refer Slide Time: 45:09)

 

That is the common structure that evolves out of a database should not constrain the database to adhere to a specific structure but rather should be indicative of what kind of data is available in the database and how they are interrelated to one another.  

(Refer Slide Time: 45:45)

 

So here is an example. This slide shows an example of what is the main problem in or what is the main challenge in semi structured data. 

(Refer Slide Time: 45:58)

 
 
In relational or what may be termed as traditional data management what really happens is you have a UOD or a universe of discourse like company or an academic institute or university or whatever. And then you have a model of the UOD that is there is the schema that defines how data in the UOD should be organized. It’s not ease organized but how the data should be organized and data that is collected from the UOD is first taken through this model and populated into the database. 

So when we say that an employee should have a pan number as the primary key and name and dependents and salary and so on, it’s only those sets of data that are extracted from the UOD and then sent into the database. And especially for example, if an employee doesn’t have a pan number and the pan number is the primary key then it is not possible to add that employee record into the database because the primary key has to be not null, it has a not null constraint and so on.

So constraints are enforced when the database is being populated and the query also is formulated within the model of the UOD. So the query just takes the model of the UOD and queries the database accordingly. 

(Refer Slide Time: 47:35)
	
 

However in what might be termed as the post internet data management which is the main problem with semi structure databases, the universe of discourse whatever is the universe the world wide web or the internet movie database where users can independently add movie data into the database, the universe directly populates the database. 

It doesn’t go through any common mental model by which the database is populated, in fact there might may or may not be any mental model here as to how the data is organized but the data is directly populated by the UOD.



(Refer Slide Time: 48:29)

 

Now the query, when the query is searching the data it should not only know what data to search, it should first try to find out what is the mental model or how is the data organized, what is the schema for this by which the data can be searched. So that is basically the schema discovery problem or the implicit schema discovery problem.  

And in addition to that, the schema discovery problem often encounters the problem of what is called as the large schematic structure. That is even when we discover a schema, this is again called the maximalist world notion, in contrast to the minimalist world model of a traditional database system. 

In a traditional database system whatever is not allowed by, whatever is not explicitly permitted by the schema is forbidden. So everything is forbidden unless explicitly allowed by the schema. So it’s a kind of exclusivist data model where things are thrown away unless they are permitted.
 













(Refer Slide Time: 49:05)

 

However in a schema discovery process, it’s an inclusive model where everything should be permitted unless it is sure that it is forbidden, that is unless it is sure that some kind of a relationship cannot exist, all kinds of relationships between data elements are permitted. So it is not, one cannot apriori define what kinds of relationships exist between data elements unless of course we know that some kind of relationships do not exist in the database or cannot exist in this UOD. 

So the associated problem from this is that the discourse schema can be quite large rather than in contrast to a relational schema where the schema is much smaller than the data set itself. so in the internet movie database for example, we might discover that lot of different, we might discover lot of different things that go into a movie based on what people add into the database. And we should allow for all such relationships, unless of course we know explicitly or unless of course we know specifically that some kind of a relationship cannot exist. 

For example we might know as a rule, I don’t if this is a true but we might know as a rule that in the universe of discourse that in the world of movies, it is not possible for a director to be the boss of the producer or something like that. So the producer reporting to the director or whatever. So unless of course we know that some kind of relationship does not exist, we have to accept all kinds of relationships that are, we might have to accept a dataset about a movie which does not contain any movie star. 

We might have to accept a data set where a movie contain 10 different movie stars and so on. So all such relationships should be accepted unless explicitly forbidden. And as a result, the actual schema that is generated is far bigger than a typical relational schema. 



(Refer Slide Time: 52:26)

 

There are several different application areas where this is useful and of course where these have been tried out and these include data integration where you design an interface to integrate different desperate data sources coming from different locations, each having their own schematic structures. And the second major area of application is in digital libraries which consists of again different kinds of semi structured data coming from different sources. 

Several more application areas like genome databases or scientific databases that talks about scientific documents, similar documents, citations, references and abstracts and where it was published and ratings and so on and so forth. And of course in E-Commerce applications where the discovery of structure problem becomes very important in business to business systems where each business if it is quite big, it might be difficult or impractical to impose a very specific schematic structure over the entire business house. 
So one needs to be able to resort to semi structure data management, when managing B two B business systems.













(Refer Slide Time:  52:44)

 

So let us skip through these slides where the need for discovery of structures are motivated even more or which talks about how discovery of structures can go about and addressing the discovery of structures problem in itself would take a complete, would involve a separate session and it is clearly beyond the scope of this particular lecture. 

(Refer Slide Time: 53:38)

 

However we can give some kind of thumb rules which talk about how implicit structure can be discovered from a set of desperate data sources. And most of these revolved around looking at some kind of regularity in the data set and then generalizing based on this regularities. 
And so several different kinds of data mining and machine learning and artificial intelligence kind techniques are explored for trying to fit a structure on to a data set. And there is also a notions of what is a best fit. A best fit data structure or a schematic structure should not be too general and should neither be too specific and so it has to be, the structure discovery process should be able to generalize based on whatever examples were encountered while passing through the data. 

However it shouldn’t be too general in the sense that it can accept anything or any structure in the data set. That is it should also identify what are the forbidden relationships among data elements in addition to what are all the possible relationships among the data elements. 

(Refer Slide Time: 55:41)

 

And several different kinds of query languages are also supported for semi structured data in addition to XQuery which is primarily meant for XML databases and of course keyword based searches which are useful for, which are useful for full text searching. There are other kinds of primitives like navigation based queries or searching for patterns or temporal queries based on how particular data element evolves over time and so on.











(Refer Slide Time: 56:16)

 

So XML is an embodiment of semi structure data in the sense that XML is the natural choice in which semi structure data can be organized.

(Refer Slide Time: 56:27)

 

And the problem of discovery of structure over XML, over semi structured data can reduced to discovery of a XML schema given a desperate set of XML documents.




(Refer Slide Time: 56:27)

 

And queries can be revisited using XPath and XQuery expressions based on whatever structure that have been discovered. 

(Refer Slide Time: 56:43)	

 







(Refer Slide Time: 56:51)

 

So let us summarize whatever we learnt in this session and of course the idea of semi structure data itself is a vast ocean and it is beyond the scope of this lecture to explore all of them. So therefore we looked at native XML storage and XML publishing and different kinds of storage structures that have been proposed for XML and mainly we touched upon the problem of the semi structured data and the larger problem of discovery of structure which is very important for semi structured data management. So with that we shall end this session. 

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 41

Case Study - Part One Database Design

Hello every one. Welcome to another session in database management systems. Until now we have seen different aspects of DBMS design. We have seen what a typical life cycle of a database management system looks like. Essentially we saw that a DBMS is something like or could be treated analogous to the engine of an information system. And what is an information system? Anything, a part of a larger system that deals with information flow, management, storage, retrieval, handling and so on.

So everything to do with information is usually driven by a database management system at the core. So what I assume that you should know by today’s class is that you should be familiar with what are the roles of different, what are the roles of a typical DBMS system, what are the different kinds of actors that exist in a typical DBMS and conceptual modeling using the ER model, we saw little bit about ER or entity relationship based design for conceptual modeling. And also the relational model which is the physical model or rather it’s not exactly the physical model as in terms of the disk storage that’s used but it’s still called the physical schema because that is the way in which the database schema appears to all the programs that utilize this DBMS system. So the relational model and different terms as to what is meant by a table relation or normalization, functional dependencies and so on.
 
(Refer Slide Time: 02:15)

 

And also a little bit of set of rules as to how to convert a given conceptual model in ER diagram to a given to its correspondent relational schema. So today what will do is let us look at a typical case study database design case study. So how do we go about designing an application around a DBMS system. Note that we are not here talking about the design of a DBMS itself but we are talking about design of an application on top of a DBMS. So when I was talking about UODs in one of my earlier class, this is what we are going to look at that is we are going to consider particular universe of discourse and then take it up. So rather than taking toy examples and rather than taking an example comprising of just a little bit of database or data management requirements.

I have taken up fairly comprehensive example. At the same time one should be aware of the fact that real life databases for example the moment when we talk about databases, we are first reminded about banks and railway reservations and so on. I have not taken either of them because they are massive database systems, Indian railways for example huge in terms of the amount of transactions that happen and amount of data that is generated and stored every day. So it would be a disservice, in fact it would be plain wrong to take up such a massive database as a case study and in fact we would be simplifying it so much that you will not appreciate the actual complexity that lies in managing such a huge system. 

So what I have done here is to take up an actual system that you might actually want to implement as part of a class project or something which and several of such database management systems exists in practice. 

(Refer Slide Time:  5:04)

 

So the case study that we are taking is shown here it is a conference management system. As you know several conferences today are managed by a web based interface where you can manage all the activities and data that are related to the conference. So what is the conference management system contain? So, here is a brief description of the UOD that the universe of discourse and the different kinds of requirements that make up this UOD. 

So let me read it out from the requirements itself. Of course this is a simplified conference management system, it does not make sense to take up a real life database in its complete gory details but at least what I hope is that the gist of a particular requirements of a given UOD should be captured by these requirements. So let us look at the requirements once again. The technical program of a large conference is decided by a program committee. So there is a committee of people who decide which paper should be published or which paper should be presented and which paper should not be presented in a given conference. And the program committee is headed by a PC chair or a program committee chair. All other members of program committee will act as reviewers. So people who would submit papers to the conference and they would be reviewed by different members of the program committee. 

Now that’s about the program committee. So let us look at the next set of requirements. What about a paper? A paper is authored by one or more authors of course and it should have a unique contact author. So there should be one author in the paper who should take responsibility of the paper. So it is to this author, its to him or her that all further correspondence will be addressed to. So correspondence regarding whether the paper is accepted finally or is it rejected or it should be accepted after another process of review and what kinds of changes to be made in the paper and so on and so forth. 

So look at the other set of requirements (Refer Slide Time:  07:10). So any person who is a member of the program committee cannot be an author of any paper that is published in the conference. Of course in real life conferences, it’s a bit more relaxed than this that is you can actually submit papers to a conference even though you are a program committee member. But for our purposes let us keep it kind of stringent, stringent meaning it’s just going to make things simpler. So as long as you are on the program committee, you cannot publish any papers in this conference. 
















(Refer Slide Time: 08:27)

 

What is the reason for that? Because a program committee member should not push his or her own papers into the conference, so they should act only as reviewers. What about authors? An author may submit one or more paper, there is no limitation on that. But each paper has a unique identity. So we are selecting papers and not authors, so that’s an important thing here. And a paper cannot be submitted to more than one conference or a journey. So if I submit a paper somewhere, I cannot submit the same paper to somewhere else and I cannot obviously also summit a published paper somewhere else.  

Now the last set of requirements is, the last block of requirements is that a paper is reviewed by at least 3 reviewers. So when I send a paper to a conference, it goes to at least 3 other reviewers and a reviewer will give suggestion as to whether to accept or reject the paper. So that is shown here (Refer Slide Time: 08:44), a reviewer may either accept or reject a paper or be neutral towards a paper, if the reviewer cannot take decision the reviewer just says that I am neutral to this paper. So the actual decision should be taken by the other two reviewers and in very rare cases all three reviewers would be neutral and well the program committee chair or the PC chair should take a call on such papers. So based on reviewer comments PC chair prepares a set of paper for acceptance and then those set of papers are accepted into the conference.

Now let me pause for a little while here and go through the requirements once again. So carefully look at the requirements of your end user, there is a program committee, there is a PC chair. Ultimately what is that we have to do? We have to take care of the conference activities.





(Refer Slide Time: 09:47)

 

Now if you look at any set of requirements carefully, you’ll see that there are two things that a requirement says. A set of requirements will indicate an explicit set of required behavior. That is these things have to be there, the paper says that every paper has to be reviewed by 3 reviewers and every paper has to have a contact author and so on. 
There are some things which are explicitly required by our specifications. Similarly there are some things which are explicitly forbidden by this specification. If I have sent a paper to a conference, I cannot or I may not send the same paper to some other conference. So this is a specific forbidden condition, you shall not do this and so on. But if you see again carefully, there are number of requirements or number of things here which one might talk about which or neither required nor forbidden by the requirements. 

Can you think of some requirements for the conference management system itself that is neither required nor forbidden? Let us take something like how many paper should a reviewer review? Can I say that a reviewer can review 5 papers, 10 papers or exactly 1 paper and so on. There is nothing that is said in the requirements here. If you look at this carefully the requirements says neither yes or no, so there is nothing said about this requirement itself. So that is an important thing to note in most application design. When we capture requirements, the requirements tells us something that needs to be there and tells us something that should not be there but is silent on a large number of things as well. 

So that greatly affects how we design our application and whether our application, suppose you design a DBMS system and you say that because of some constraint, somewhere you say that a reviewer cannot review more than two papers. Is it correct or is it wrong? So there is no specific answer to this because the requirements neither requires this nor forbids such a thing. So usually this is how a systems development life cycle, some of the top or the early stages of a system development life cycle would look like. So if you look at the slide here, you have the systems requirements specifications were there are set of explicitly required conditions and there are set of explicitly forbidden conditions and this is the entire UOD where the number of things which are not addressed by the requirements.
 
(Refer Slide Time: 12:35)

 

Now based on these you get a high level design of your system, usually this in the form of a ER model or whatever when it comes to DBMS design. So you end up with a ER diagram here, in turn you reduce the ER diagram to a relational schema or a low level design and then you get a system model, relational schema plus transactions and so on small set of application logic and some set of constraints, triggers, stored procedures or so on and then you get a system model. 

Usually what happens is this process, how do you get design from requirements or how do you move from high level design to low level design. These sets of processes involve human activity or human creativity to be more specific. And as is so common with human actions, the system model may not exactly reflect the systems requirements specs. Ideally what should the system model do? The system model should exactly reflect the requirement specification here.  

So as shown here, the red spot is slightly bigger here. What is that mean? That the system model has more forbidden conditions than what is explicitly forbidden by the requirements specification itself. So it brings us to some two important concepts, when we are designing real life system, the concepts are what are called as liveness and safety. 






(Refer Slide Time: 14:14)

 

So look at the English definitions of liveness and safety. Liveness means what, that something is alive or something is existent and so on and safety of course is obvious. Now if you look at the systems requirements specifications, why would a set of requirements. So let us go back here (Refer Slide Time: 14:31). Why would a set of requirements say that this is forbidden, why would a set of requirement say that a member of the PC committee shall not be an author of a paper. 

Why would a set of requirements say? Because it would compromise the integrity of the system if that were to be alone because if I allowed a PC committee member to be an author of a paper, there is quite a likelihood or there is quite a chance that the PC committee member may push his or her own paper and have an unfair advantage over others. So it is the safety of the system is getting compromised, so that is why I forbid the behavior, forbid this activities. So essentially whatever is forbidden usually constitutes of safety requirement, in order to safe guard the system against integrity violations I say that this shall not be there.

However what is the simplest form to build a safe system? How do we build a system that is absolutely safe and from any kind of integrity violations? Simple, don’t start the system at all. If a system that doesn’t work, it is absolutely safe. If your database system doesn’t work at all, it is absolutely safe because it does not violate any integrity constraints at all. So that is why a trivial way of ensuring safety is to make a system that doesn’t work. But that is not what we want. In addition to safety we need, we require certain behavior to happen. So those are what are called as liveness requirement that is the system should perform certain activities and should not perform certain activities. 

So let us use some notations when just to talk about mismatches. Now suppose I say that R of SRS here (Refer Slide Time: 16:17) is the set of required behavior by the SRS or the systems requirements spec. Similarly F of SRS is the set of forbidden behavior or safety constraints specified by the SRS. Similarly let us say R of M or where M is the model that we build, the final system model that we build. So let R of M denote the set of all liveness criteria in the system model that is the system model will do this. And F of M denotes the set of safety criteria in the system model that is the system model will not do this and so on. 

Now when we talk about a system model that is when we talk about building a system model from a set of requirement spec, we can think of various kinds of mismatches that can occur. So, various things can go wrong when we are talking about capturing user requirements into a system model. What are the things that can go wrong? A tentative list of things I mean these are not the only thing that can go wrong, in fact in addition to this a huge number of things can go wrong but anyway.

(Refer Slide Time: 17:20)

 

Now let us say what if R of M, remember what is R of M. R of M is the set of required behavior of the model. What if R of M is a proper subset of R of SRS. What is this mean? The set of required behavior by the model is a proper set of the required behavior by the SRS that means that the model is incomplete. The SRS require certain behavior to be done but you don’t implement all the behavior, you don’t factor all those behaviors, you factor a subset of those behaviors. 

Now what if in addition to this R of M being a proper subset of R of SRS, in addition to this let us say the R of SRS minus R of M that is the set of requirements specified by the systems requirements spec which are not factored into the model is actually a part of F of M, is actually a part of the set of forbidden behaviors by the model. What is that mean? It means that not only does the model address all the requirements in the requirement spec, in fact there are certain requirements of the requirement spec that the model actually forbids that is that the model will not do. So it means that the model is not only incomplete, it is incorrect it forbids certain required behavior.
Similarly what if R of SRS is a proper subset of R of M that means that the model is performing more activities than what is required by the SRS itself. That is the model has extraneous behavior and having extraneous behavior is not that is having an added feature for example, suppose the model as for the birth date of the author when were you born and so on does not always be a desirable feature, it can actually be potentially unsafe. When is it potentially unsafe, when R of SRS is a proper subset of R of M that is what I saw here and the difference between R of M and R of SRS is actually a part of F of SRS that is the extra behavior that or the extra so called value addition that your model is doing is actually part of the set of safety conditions that is actually forbidden by your requirements. So the model has extraneous and unsafe behaviors. 

So when you build a system model, you should be careful to or this is one set of guidelines by which you can measure whether your system model is good enough against the requirements. That is just try separating the requirements into set of required behavior and set of forbidden behavior and your model also into set of required behavior and a set of forbidden behavior. So let us see some more mismatches here. Now what if F of M is a proper subset of F of SRS. That is the set of all forbidden things of the model is a proper subset of the set of all forbidden things of the SRS that means that the model is unsafe. That is the requirements require you to forbid certain things which you are not forbidding.  

Similarly, what if it is the other way around. That is the model forbids more than what is required by the SRS then you say that the model is conservative. Now conservative again doesn’t mean that you are safe. Usually in English, we say that oh let us be conservative and go about like this and take this action and so on. But just be saying let us be conservative doesn’t necessarily mean that you are safe. Why? Because you could actually be violating a liveness criteria.  

(Refer Slide Time: 20:58)

 

So this is the case here that is F of SRS is a proper subset of F of M that is the model forbids more than what is required to be forbidden by the SRS and the difference that is what the model forbids which is not forbidden by the requirements is actually part of the required behavior of the SRS. So if forbids something which actually needs to be there, so in being more conservative you are actually hampering the liveness of the system. So just being conservative doesn’t always mean you are building a safe model.  

(Refer Slide Time: 22:56)

 

So let us see, let us take a step by step approach to see to let us try to divide our requirements coming back to the conference example to see what kinds of required behavior are there by the model and so, by the requirements spec and so on. So of course the kind of the example that we are seeing here is a simplified example and real life examples are far more difficult than these but anyway this gives the gist of how to factor a requirements into set of required behavior or liveness behavior or set of safety conditions and so on. 

So what could be the step by step approach? Let us, the first thing is we have to find the set of required and forbidden conditions. Then once you start that then start identifying the various entities, their attributes, the relationships between entities and so on. Then build a complete ER model for the problem statement and then convert the ER model into a relational model and perform normalizations if they are not already normalized. 








(Refer Slide Time: 23:46)

 

So let us look at some of the required conditions. A paper is reviewed by at least 3 reviewers that actually means that a paper should be reviewed by at least 3 reviewers. So if I try to review or if I try to accept a paper that is being reviewed by only 2 reviewers then your conference management system should flag an error, it should not allow it to do that. So this is a required condition that is a paper is reviewed by at least 3 reviewers. Each program committee has a PC chair, so this is another required condition, you cannot have a program committee without a chair.

Each paper has a contact author. If you go back through the requirements that we saw, all these have been picked from the set of requirement itself. So each paper has a contact author that means that each paper should have a contact author and so on. A paper is authored by one or more authors, so obviously this means that you should not accept a paper without any authors in it. And reviewer must comment or must give comments which can be one of the following accepted, rejected or neutral. You cannot, the reviewer cannot give any other comment other than this three and the reviewer should give one of these three. 

The reviewer cannot remain silent saying that I am neither accepted nor rejected nor I am neutral about the paper and so on. And the reviewer should give only one of this. So this is a set of required condition. What are some of the forbidden conditions? We saw some examples already. A paper cannot be submitted to more than one conference or a journal. So you may not submit a paper to more than one conference and so on. An author of a paper may not be a member of a program of the program committee. So that’s another forbidden, explicitly forbidden conditions so that have been explicitly stated in the requirements that these are forbidden and a paper may not have more than one contact author. So there has to be one and only one contact author, so that it may not have more than one. So these are some kinds of required conditions and forbidden conditions and so on. 
So when you build a system model, what you should be able to do is take up each set of required conditions and see whether your model also has that required behavior. Take each set of forbidden conditions and see that whether your model also forbids those conditions and the other way around, take each set of required behaviors by your model and see that whether they are actually required by the set of requirements so on.

(Refer Slide Time: 24:53)

 

So let us now go to the next step and start identifying entities. So how do we identify entities and what is an entity. An entity is some logical item one could say or logical something of which has an independent existence of its own. So I was about to say logical entity which kind of becomes a circular definition in this case. So any way let us look at the problem statement once again.

















(Refer Slide Time: 25:57)

 

The technical program of a large conference is decided by a program committee headed by a PC chair. If you just look at that statement, you can already find several entities here. So the conference is an entity of a large conference, conference is something that has a logical existence. Program committee is an entity here essentially the nouns in this sentence and is headed by a PC chair is an another entity. And one could even say the technical program could also be an entity and so on. So just reading through each sentences, you can identify what could be potential entities in your system. 

(Refer Slide Time: 27:25)

 

Similarly the next statement, all other members are some statement down here. All other members of program of the program committee will act as reviewers. So reviewers is another entity as soon as we found. So similarly a paper is authored by one or more authors, so paper is an entity. Now author is a, now here there is a question this is not as simple as that. So is author is it an entity or is it an attribute. Is an author an attribute of a paper that is a paper has an entity and this paper is authored by so and so authors and so on. 

Now some cases we can make author as an attribute of a paper but here we see that author also has an independent existence. Why because we have something called a contact author, we have something called author may not be a member of the program committee and so on. So the author may actually participate in other relations as well and an author may write more than one paper. If I make author as an attribute of the paper entity, there is no way to relate or there is no relate to equate that paper one and paper two have been published by the same author and so on. So there is no way to equate those two papers. So, in our case it’s better to take author or its better to design author as an entity itself. 

Similarly again some more, a paper cannot be submitted to more than one conference or a journal. Again there is an entity called journal and so on. Conference we already saw is an entity. Now what about relationships? Now that we have identified entities, of course we are no way near to finishing the identification of entities but let us look at relationships. I mean you should have got the ideas as to how to go about identifying entities and its attributes and so on.
 
(Refer Slide Time: 28:31)

 

Now again take a look at the statement. Now what are the entities that you can see in a given statement? The entities would generally be the nouns of a particular statement. Now what could be the relationship here. 
(Refer Slide Time: 29:05)

 

Now look at the verb something is headed by something else and so on, so or handled by and so on. So the verb statements that connect two or more nouns would actually be prime candidates for relationships. So if you look at this statement here, the technical program of a large conference is decided by a program committee headed by a PC chair. So as you can see here, this part already forms a relationship that is or rather the first part is a relationship here that is conference is handled by program committee that is handled by or technical program decided by if I have to make it very explicit. 

So conference is handled by a program committee and as you can see the technical program of a large conference is decided by a program committee. So basically it is a one to one relationship that is one conference, one program committee. But then look at this here program committee, we have made program committee into a week entity here. Why is it a week entity? First of all what is the week entity? If you notice carefully, a week entity is an entity or if you remember your ER modeling classes, a week entity is an entity which does not have an independent existence of its own, its existence is defined by a relationship.

And the relationship that defines a week entity is also called a defining relationship. So this is a defining relationship, so shown by double arrows like this (Refer Slide Time: 30:53) and this is what is called as the total participation if you remember ER classes again. So a program committee totally participates in this conference that is the same program committee may not participate in more than one conference entities and this is the defining relationship. So if there is no program committee then there is no conference.

Similarly let us look at another statement. The technical program committee of large conference is decided by a program committee headed by the PC chair, same statement. So again program committee here and PC chair heads program committee that is a PC chair is an entity which we have found and a PC chair heads a program committee. 
(Refer Slide Time: 31:53)

 

Now here if you can see again, we see that this cardinality that is a program committee is headed by a PC chair is clear that is one program committee should have exactly 1 PC chair. But 1 PC chair can head how many committees? It is neither specified or rather it is neither required nor forbidden, it’s not specified in the requirements. So here we have made it into a N cardinality 1 to N or whatever. That is a program, a PC chair can head any number of program committees and so on. So because there is no explicit specification as such in terms of how many program committees can a PC chair head. 

(Refer Slide Time: 32:26)

 


Again some more relationships, so take a look at this statement. The technical program of a large committee whatever headed by a PC chair and so on. All other members of the program committee will act as reviewers. So let us say we already had this one. That is program committee is here and that is headed by a PC chair and 1 PC chair can head 1 to N program committee, program committees and so on.

Now a program committee consists of reviewers which is apparent by the second sentence. That is all other members of program committee will act as reviewers. However it is not exactly correct, it’s not exactly what is specified by the requirement. That is what is the requirement say all other members of the program committee. So basically what does this means? That is all members who are not PC chair that is who are not acting as PC chairs can be reviewers of this. So if I take two separate relationships like this in isolation, they don’t form a consistent set here because it is violating a forbidden condition, it is violating a condition that the PC chair may not be a reviewer.

So how would you rewrite this condition here? So basically we will introduce a new entity called members and basically form, what might be termed as a generalization specialization relationship. So remember the extended ER model allows for a specialization relationship were given a member or given a entity, you can inherit one or more entities from it that is it actually shows the is a relationship. 

And in addition to the is a relationship, here we have this circle called D. What is the D specified? D basically specifies that these are disjoint entities that is no entity instance that is part of reviewer can also be PC chair and vice versa.

(Refer Slide Time: 34:07)

 

So our committee would now look like this. Committee would consist of members where members would in turn consist of reviewer and PC chair which are disjoint. So committee can consist of 1 to N member, N number of members but there has to be exactly one PC chair. So basically in addition to this, we have to give a cardinality of 1 here and N here for N reviewers and 1 PC chairs. 	

Again let us look at some more statement when to identify relationships. A paper is authored by one or more authors and has a unique contact author. So again we can see that we can identify relationships straightaway here, paper is authored by authors, one or more authors. And an author can author how many papers. There is no specification, so we just introduced 1 is to N. So we are kind of being liberal model, we are not being very conservative model that is we are allowing for more behaviors than has been required that is an author can submit any number of papers unless it is explicitly forbidden of course.

(Refer Slide Time: 35:31)

 

And a paper should have a contact author that is a paper here any number of papers should have exactly one contact author. So, one author who acts as the contact author. Again in isolation these two relationships are not sufficient because why do you think they are not sufficient, let me pause for a little while here. Why do you think going back to these two set of entities, why do you think they are not sufficient in themselves. I am sure you would have got the answer. 

The thing is while a paper can be authored by one or more authors and a paper can have contact authors, there is no relationship that states that the contact author should be one of the authors here. So one of the authors from here should be taken and be formed as the contact author for a given paper that is so you have to, in the earlier case there was a disjoint relationship, PC chair cannot be or may not be a reviewer. And here there is a membership requirement that is a contact author ought to be one of the authors of the paper. 



(Refer Slide Time: 37:29)

 

So how would you go about, let us come back to this again later. So again any person who is a member of the program committee cannot be the author of any paper. So we will come back to that earlier thing after taking this other constraint also into perspective and then draw the entire set of relationships at one goal. So what is this say here? Any person, note that now again person is a noun here so we need another new entity called person. 

So any person who is a member of the program committee cannot be the author of any paper. One way to show this is have a person called, have an entity called person and make a disjoint specialization between member. Remember we had an entity called member here, members or whatever. So we had an entity called member and an entity called author. So an author may not be a member or a PC committee member and a member may not be an author and both are persons and so on. So that way you can identify that any person cannot be both an author and a member of the PC committee or the program committee. And a paper may not be submitted even though sometimes when talking in English, we say a paper cannot be submitted to more than one conference or journal. To be more precise, it actually should be a paper may not be submitted to more than one conference or a journal. 











(Refer Slide Time: 38:35)


 

So again here what, first of all what can we imply from that? A paper can be submitted to a journal and a paper can be submitted to a conference. So these two can be implied but what is that we actually need. What we actually need is that while a paper can be submitted to a journal as well as submitted to a conference, it may not be submitted to more than one conference or a journal, the same paper may not be submitted to more than one conference or a journal.

(Refer Slide Time: 39:21)

 

So here in order to identify that we have used the union or the concept of a union were it is, one might call it the opposite of the specialization condition were you take two or more entities and form a union out of them and form a single entity. So take a journal or a conference and form a union out of them and make an entity called event and the paper is submitted to an event. And how do you say that it has to be submitted to only one conference or a journal at any point in time, only thing is make this, the cardinality of event as one here. 

So given paper can be submitted to one paper here or 1 to N papers that is a given event may have N papers and a given paper may be submitted to exactly one event. And what is that event? An event could be a conference or a journal. Calling a journal as an event is not exactly correct sounding in terms of the English definition but anyway we have used this term but you might think of a better term than event to specify or to take the union of journal and a conference. 	

(Refer Slide Time: 40:50)
	
 

Similarly a paper is reviewed by at least 3 reviewers and a reviewer may either accept or reject or be neutral towards the paper. So what is this statement say? A paper is reviewed by reviewers, so N number of papers is reviewed by 3 to N that is at least 3 or anything more than 3. 

Now take a look at the second half of the statement. A reviewer may either accept reject or be neutral towards a paper that means the reviewer is going to give a result. The result is either accept, reject or neutral or be neutral. Now, if you see carefully the attribute called result does not belong either to the reviewer nor to the paper because a paper, the result of a paper is actually a combination of the results of three or more reviewers. 

And a reviewer may be reviewing more than one papers, so you can't assign result to a reviewer as well. So the attribute called result is actually an attribute of the relationship itself. So remember that we had talked about attributes which belong to relationships. So as long as there is an instance of this relationship existing in the system, an instance of this attribute may also exist in the system. Whenever the relationship instance does not exist, when can a relationship instance not exist? For example when there is a paper which is not assigned to any reviewers for example then there is no instance of the relationship at all that is reviewed by and so on. 

So there is no question of a result existing in this or even when a paper is assigned to just 2 reviewers. So if you look at the relationship here, the relationship requires that a paper be submitted to at least 3 reviewers. So therefore there is no instance of such a relationship existing and therefore there is no instance of the result in the databases.

Similarly coming back to attributes. Now let us say conference. Now the requirements doesn’t say anything as such but as application designers, it is our responsibility to identify some of the major attributes of a particular entity and also identify key attributes. So here in this case a conference name and the date and the place, topic and all of those things would be attributes of the conference and usually something like the conference name would be the primary key or would be the key attribute of the conference.

(Refer Slide Time: 43:21)

 










(Refer Slide Time: 43:37)

 

Similarly program committee does not have any primary key because it’s a week entity type which we actually saw earlier. So a program committee does not have a key attribute but it may have other attributes like what is the strength of the program committee, how many people are there in the program committee as of now and so on. So a program committee has a is a week entity type having no key attribute but it has its own other attributes. 

And have a look at the entity called person. For person again you can think of lot of different attributes, what is the name of the person. Now when you say name, usually in several cultures, you actually divide a actually name into first name, last name, middle name and so and so on, the initials and title and so on and so forth. So name could actually be a composite attribute here which in turn has multiple other attributes, many other attributes say first name, middle name, last name, title, initials and so on and so forth.














(Refer Slide Time: 44:11)

 

Then there could be age or date of birth address and usually you need to have a unique identity to unique way of identifying a person. This was actually created by some of my students whose pan number has a key attribute for a person but usually in a, it is quite unlikely that in a conference setting, you would ask for a person’s pan number. Usually it would be the email address of this person which or the contact email address of this person which would be the key attribute. 

Similarly they could have something like phone numbers and phone number here is treated as a multi valued attribute which means that this attribute can have multiple values. So what is that mean? That a person can have multiple phone numbers. And I hope you know the difference between a multi valued attribute and a composite attribute. A composite attribute is also made of multiple attributes but each of these different attributes may belong to different domains. 

So name can have first name, middle name, last name were a middle name can be constraint to be a single letter, if the middle name is an initial whereas first name and last name can be varchars or strings and so on. But when I say phone number, when there are multiple values for that phone number, all of these values belong to the same domain or of the same type. So that’s the difference between a multi valued attribute and composite attribute.
 







(Refer Slide Time: 46:25)

 

So similarly other these thing when I say that, when I say author you can give an Author_id for each author, a login id or whatever and every other attributes of a person would be inherited by author because author is a person and so an author is supposed to have a pan number and date of birth and phone number and so on and so forth.

(Refer Slide Time:  46:45)

 

Similarly for paper, its already specified there that each paper would have a unique identification or a unique key. So for paper, paper_id would be the key and several other things what is the title, what is the category, the classification of those attributes, the paper content itself, the keywords that are given for the paper and so on, all of them could be attributes of a paper. 

(Refer Slide Time: 47:16)

 

And when I say PC member, you can again give member_id for each members, again some kind of a login id or something which would form the primary key for each member. 

(Refer Slide Time: 47:32)

 

And again several other this thing reviewers and PC chair. So reviewer would have something called subject of expertise and PC chair would have something called conference headed and so on which can be attributes of those respective entities. And journal again, so journal _id year of publication topic were I can always have a journal_id as a primary key.

(Refer Slide Time: 47:49)

 

So finally we come to an overall schema for the entire system where we put all of these together to form one big ER schema. So let us spend a little bit time in turn by reviewing this schema. So what all did we go about looking at. We started out by saying where is the conference. Yeah, the conference is here.

(Refer Slide Time: 48:06)

 

So conference is handled by a program committee and program committee is a week entity type, so it has no existence without a conference. And program committee consists of different members, so among the members there are reviewers and a PC chair and there is one PC chair, there is exactly one here. And this is a disjoint relationship that is a PC chair may not be a reviewer and a reviewer may not be a PC chair and a PC chair heads program committee. So PC chair may head one or more program committees like this.

And similarly you have a conference and a journal forming an event. That is a given conference or a journal may be forming an event to which a paper is submitted. So a paper may be submitted to 0 or 1 event. So you may not submit a paper at all or you may submit it to at most one event. And an event should have at least one paper or it may have any number of papers and so on. And a reviewer reviews a paper or paper is reviewed by reviewer and there is a constraint here that is a paper is reviewed by at least 3 reviewers.

Now the reviewer, the review of a paper, the process of review of a paper will result in a result being assigned or will create a new attribute called result which the reviewer assigns for this paper. So this result is actually a attribute of the relationship itself. Now again a paper is authored by an author and there is a contact author. So there is exactly one contact author and it is authored by one or more authors and so on. And both author and members or persons, why do we need this persons? Because we are having attributes of a person separately that is a person should have a pan number and address and telephone number, email and so on.

So, all of those attributes of a person are inherited by both members and authors. Similarly all of the attributes of or a combination of the attributes of conference and journals is inherited by comes to event and all attributes of members are inherited by reviewers and PC chairs. So a member should have certain privileges or benefits or whatever, all of those are inherited by both reviewers and the PC chair. And because PC chair is a separate member, a PC chair may also have some attributes which are not shared by reviewer or which may not exist for a particular reviewer and so on. 

So what we saw today is we have taken a fairly complicated example, I mean it’s not a and this is a realistic example a conference management system in fact you can search the internet for something called confman which is a freely available I guess open source conference management system which uses a back end database management system in order to manage activities like this or you might going to MSRCMT which is the Microsoft research conference management tool which is actually used by major conferences around the world and which also has something like this. That is there are reviewers, there are authors, there are papers, there are PC chairs, there are committees and so on and so forth.  

And there are little bit or rather significantly more complicated than this but the level of complication to which or the level of detail to which we have seen in this is fairly representative enough because we have seen some of the major kinds of conceptual requirements that arrive. For example a PC chair is a member of the program committee but may not be a reviewer, but may not be any other reviewer and contact author should be part of the author list and so on and so forth. 

So, all of this form tricky details which manifest themselves during your conceptual design. So what will do in the next class is to take this idea forward and take up individual chunks or pieces of this ER diagram and try to convert them to the lower relational schema and see what kinds of tricky situations arise when we convert them to a relational schema. So let us finish this class here and see you all in the next class. So this brings us to the end of this session. 

Database Management System
Dr. S. Srinath
Department of Computer Science & Engineering
Indian Institute of Technology, Madras
Lecture No. # 42

Case Study - Part 2 Database Design

Hello everyone. Welcome to this session in DBMS. So in this session today what we would be doing is we would be continuing from the previous session were we talked about a case study of a database design. So again let me put forth a kind of assumptions or the prerequisites that I am expecting for this session. I am assuming that you know or you have gone through the first few sets of lectures on the life cycle of a DBMS and you also know what is conceptual modeling, what are the building blocks of conceptual modeling, what are the different kinds of nuances in conceptual modeling like a week entity type, like a multi valued attribute, like a composite attribute and so on.

You also know the relational schema or the relational model and the characteristics of relational model like, the models like the functional dependencies and normalization and so on. And you also have an idea of how to convert what kinds of rules that you can use to convert a given ER schema to a relational schema. So I am assuming that so will not be going into the rules of how to convert a ER schema into a relational schema except that we are going to take a running example of a conference management system that we started in the previous class. And start by explaining which kinds of, we just take up specific aspects of the ER diagram of the conference management system and just convert them to the corresponding relational model. 

And of course we will also look at few kinds of transactions on top of this relational model. I know that we have not really looked into transactions as part of this series of lectures as such but I will also define transactions as we go along. And in an informal sense of course there we shall be looking at transactions and transaction processing in much more detail in later sets of classes. But at least I will be defining transactions in an informal sense so that we can see what kind of transactions at least we can think of on top of these sets of requirements. 

So the main idea behind this being that to try to cover the large or the first few aspects of a systems development life cycle that the high level design, the architecting, the conceptual modeling, the high level transaction design and just have a look at how does the entire system look like. And then you can over a period of time, when we address several different questions throughout this course, you can try to target each one of these different architectural aspects of the system like the relational model, the conceptual schema, the business logic and so on and transactions, transaction manager and so on and then try to see how you can make a detail design into each of these different aspects of, each of these different building blocks of the overall system.
 

(Refer Slide Time: 04:28)

 

So let us come back to the case study for a database design that we are looking at. So today we look at the relational schema design and the transaction design for the case study that we are going to look in. 

(Refer Slide Time: 04:52)

 

So what was the case study that we have been looking at? Let us briefly review the case study once more and before going into today’s work. So the case study was about a conference management system and we went through the set of requirements in the previous class. So let me briefly go through them once again to try to see what exactly is required by this conference management system. Like I said in the last class, this is a fairly comprehensive example in the sense that it’s fairly representative of a real life conference management system although not as detailed as one. A real life conference management system would be far more detail than this but at least it kind of captures the main essence of a conference management system. So what are the requirements? The technical program of a large conference is headed by a program committee. So a conference is, a large conference the adjective called large actually we haven’t really made use of here because there is no other further set of requirements as to how to handle small conference. So we are just talking about a conference. So as far as we could understand as a systems designers or systems analyst, the technical program of a conference is headed by a program committee and is determined by a program committee. 

And the program committee in turn is headed by a PC chair, a program committee chair where we saw a program committee chair is a one person who heads the program committee and all other members of the program committee will actually act as reviewers. And again we saw in the previous class that the reviewer and the program committee chair should not be the same person. So a person who is the PC chair will not be a reviewer and vice versa. And so that’s about the program committee. The next chunk of requirements for about the paper itself, the papers which are going to be presented with the conference.

So a paper is an entity again which is authored by one or more authors and as a unique contact author. It basically says that it should have a contact author and it’s a unique contact, there is only one contact author for a paper. And any person who is a member of a program committee cannot be the author of any paper, like I said in the previous class also this is not a realistic requirement were because in most real life conferences this is not exactly true. But just to make things simpler, we are imposing this constraint in order to see the repercussions on our database design. 

(Refer Slide Time: 07:56)

 
And an author may submit one or more papers each of which has a unique identity. So a paper has to have a unique identity which in turn means that a paper ought to be treated as a separate entity in its own. So an author may submit one or more papers, there is no constraint on that. However a paper cannot be submitted to more than one conference or a journal at the same time. So, a given paper cannot not be submitted to more than one conference, so given a particular paper it has to go to a particular conference and so on.  

And a paper is reviewed by at least 3 reviewers, so is reviewed. So it basically says that a paper shall be reviewed or should be reviewed by at least 3 reviewers. So even if a paper is reviewed by 2 reviewers, it’s not sufficient it has to be reviewed by 3 reviewers and a reviewer should give a decision. And the decision should be either accept or reject or be neutral towards the paper. So a reviewer has to give one of the 3 decisions and the reviewer can't say, I am neither going to say accept nor am I going to say reject nor am I going to say that I am neutral about this paper. I have an opinion but I am not going to state it and so on. 	

(Refer Slide Time: 09:09)

 

So anyway based on the reviewer comments, the PC chair either accepts or rejects papers and prepares a set of papers for acceptance. Now again we also saw this in the previous class were the characteristics of a given requirement specifications. So it’s again good to review them once more before we go in to relational model design as well. So that will help us understand the suitability of our model much better. 

So, any given requirement specification has a set of explicitly required behavior. So the requirements or the specification say that this and this has to be there, this functionality has to be there and then there are a set of explicitly forbidden behavior which says this and this functionality should not be there or shall not be there and so on. So we also noted that the set of forbidden behavior constitutes what are called as safety constraints which usually you forbid something in the interest of safety. 
So if you violate that constraint, it means that you are compromising the integrity or the safety of the system. Similarly the set of required behavior constitute the liveness of the system. So a completely safe system is one which does not work at all. So if you don’t go out into the battle field then there is no danger of you being hit by a bullet. But the thing is that does not make a soldier, you have to go into the battle field. So there is a set of required conditions were which form the liveness properties of the system, so that are specified by the explicitly required behavior. 	 

And in addition to liveness and safety usually in almost all cases, the UOD will contain a number of behaviors that are neither required nor forbidden. So the requirement specifications neither says yes, it is required nor does it say no it’s forbidden. So one might call that, call them as permitted behavior or whatever. So depending on how you treat this kind of mid way area, your conference management tool can offer an edge over the others in the sense that my conference management tools manages birth dates of and sends greeting cards and so on. I mean they are not, they are neither required nor forbidden and so on. 

And coming to the first few steps of a systems development life cycle or an SDLC, we see that what we want to do or what we are going to do here is given a set of requirements specs, requirement specifications we come out with the high level design. We kind of address this in the previous class in the sense that given SRS, we created an ER diagram for the given SRS.

(Refer Slide Time: 11:35)

 

So today we are going to be doing this one and of course with a little bit of transaction design will also be doing this one. So from the high level design, we go to the low level design and from the low level design add dynamics to that is high level design low level design were purely in the static. So add dynamics to that by adding the transactions and so on, so you get into a system model. 
So the system model should ideally reflect the systems requirement spec or the SRS document and of course there are, usually there are mismatches between the systems model and the requirements spec and rather than going about wildly looking for features or looking for characteristic features of the systems model, it is good to classify this deviations between the systems model and the requirement spec into different kinds which helps us understand in what way is the systems model deviating from the set of requirements. 

(Refer Slide Time: 12:48)

 

So we also saw this in the last class (Refer Slide Time: 12:47) were we saw how to quantify in some sense or the kind of deviations that system model has from the set of required requirement specifications. So let us briefly again revise through all these so that it helps understand how to evaluate a given system model, two or two or more systems model against one another. So one can say that a given systems model is incomplete, if the set of required behaviors that the model performs is a proper subset of the set of behaviors of the requirements itself, so it is incomplete. And being incomplete itself doesn’t, may not be all that serious but sometimes it may be quite serious, it could be incomplete and incorrect.











(Refer Slide Time: 13:07)

 

So when is the systems model incomplete and incorrect? When not only does it not, only does it not satisfy all requirements but it forbids certain requirement that is it explicitly does not satisfy certain required behavior. So it forbids certain required behavior that is that’s when you say that the model is actually incorrect and it’s possibly unusable. And if the set of requirements, if the set of required behavior by the model is a super set, proper super set of requirements spec then the model has extraneous behavior, it does something extra. And again providing an extra feature doesn’t always necessarily mean a good thing, it could potentially be unsafe as well. When will it be potentially unsafe? When this extraneous behavior actually trespasses on the forbidden conditions of the requirement spec. So R M minus R of SRS actually belongs to F of SRS that is where you trespass on the forbidden requirements.

















(Refer Slide Time: 14:57)

 

Similarly if I have a set of forbidden conditions by the model being a subset of the forbidden conditions by the set of SRS then the model is unsafe that is the model does not forbid everything that the SRS is asking you to forbid. And if it is the other way around that is if the model forbids more than what the SRS asks you to forbid then you say that the model is conservative. But again conservative doesn’t necessarily mean that it’s a safe system. Why? Because in trying to be conservative you might be blocking certain liveness behaviors, you might be actually stopping certain behaviors that are required functionality for the model. 

So if F M minus F SRS is actually a part of R of SRS that is part of the required behavior then the model is not only conservative, its actually incorrect. So it forbids certain required behavior. And what are the required conditions, what are some of the required conditions in this case study? We saw certain required conditions in the case study that is a paper should be reviewed by at least 3 reviewers for example and each program committee should have a PC chair.













(Refer Slide Time: 16:10)

 

And each paper should have a contact author and a paper is authored by one or more authors, I mean we cannot accept papers with no authors in them and so on. And a reviewer must give a comment or must give a decision either accepted or rejected or neutral. So reviewer cannot afford to keep silent on a paper. And paper and what are some of the forbidden conditions in the model? A paper cannot be submitted or may not be submitted to more than one conference or a journal which is shown in the slide here. 

And an author of a paper cannot be a member of the program committee like we said even though this is not realistic, we have chosen that for the sake of simplicity here. That is I cannot submit a paper as well be in the program committee to review the paper and so on. And a paper may not have more than one contact author, each paper should have one and only one contact author.
















(Refer Slide Time: 17:11)

 

So there is only may email sent out for each decision and it has to go to the contact author and so on. So, having said that in the previous class, we went into each requirement sentence by sentence and identified several kinds of entities and relationships among them and also attributes for these entities. 

(Refer Slide Time: 17:40)

 

And finally put all of these entities and relationships together to form one big ER diagram for the problem. So let us spend a few minutes to revise the ER schema for this problem. This is important because now we are going to cut this ER schema bit by bit and then convert it into a relational schema. Now what are the first thing that we did? We started by a conference. A conference is handled by a program committee and so on. So a conference is an entity and it is handled by a program committee. And we also saw that the program committee is a week entity type it’s not, if you can see, if you can notice carefully it is not really specified by the requirements and that the program committee are to be a week entity type. But we have to infer it, I mean that is why the high level schema design that the conceptual schema design is a very human centric process, you have to use your common sense in order to identify or in order to say which kinds of, what should be the characteristics of each of this entity type and so on. 

So program committee is a week entity type and this is a identifying relationship which says that a program committee that handles a conference defines the existence of the program committee in the first place. And a program committee consists of different members and the set of members are divided into a disjoint set of specialized members which are called reviewers and PC chair. So given a member a member could either be a reviewer or a PC chair and there are to be only one PC chairs, so there is a cardinality constraint here. And these are disjoints, so therefore a reviewer may not be a PC chair and vice versa. 

So, a program committee consists of members and the PC chair heads the program committee. So one PC chair heads any number of program committees or given a program committee there is just one PC chair for that program committee. And similarly a reviewer reviews a paper or a paper is reviewed by a reviewer. So a given paper, a given reviewer may review 0 to N paper that is a reviewer may not be assigned any papers at all. There is no, there is neither a requirement nor a forbidden condition as to how many paper should be assign to a reviewer. However there is a condition on how many reviewers should be assigned to a paper. That is a paper should be assigned at least to 3 reviewers, so 3 to N. so there can be any number of reviewers but there should be at least 3 reviewers.

Now once a reviewer reviews a paper, the reviewer gives a result. Now the reviewer has to give a result, it’s a required condition again that is a reviewer has to say either accept, reject or neutral. So now this result is an attribute of neither the paper nor the reviewer itself. Why because a paper is reviewed by more than one reviewer and a reviewer may review more than one paper. 

So you cannot assign result to any one of them. In fact the only way place you can assign result is to the relationship itself. That is that the process of being the process of reviewing a paper by a reviewer gives out an attribute called result. So in addition the paper itself is related to authors and a paper is authored by one or more authors. Note that here it is one or more, it can't be zero or more. So we also had a condition, forbidden condition that authorless papers are not accepted, are not acceptable. And a paper has to have exactly one contact author. So one of them is a contact author and it is authored by many of these other papers and so on. 

And in addition authors and members form a disjoint specialization for person that is we store certain personal details of people in the system their address, name, date of birth, phone numbers and so on and all of them have to be stored for both members and authors. And similarly a paper can be submitted to exactly one event or zero or one event, maybe submitted to exactly one event that is it need not be submitted. So but if I choose to submit, I can submit it to exactly one event. And what is an event? Event is a conference or a journal. So if I am submitting a paper to a conference then I should see that it’s neither submitted to any other conference or nor to any journal as such. So this forms the overall ER schema as part of the problem that we saw in the previous class.	

now let us go to the next step now that is from the ER schema we should reduce it down to a relational schema and which is actually managed by the DBMS. Now the parlance of a relational schema is quite different I mean you have, you don’t have entities and relationships and attributes and so on. Although, you do have something called attributes but in a different sense. You have tables and columns and rows and attributes which is a column and there are key relationships then foreign keys and decomposition of tables and functional dependencies and so on and so forth. So let us take things step by step and take up ER to relational mapping. 

So the first set of ER to relational mapping that we are doing is shown in the slide here. So we started out with conference. Now conference is handled by a program committee and we also saw that one conference, one program committee. And program committee is a week entity type that is its existence is defined by a conference. And the conference is defined by several attributes like conference name, place, date, topic and so on. 

(Refer Slide Time: 23:34)

 

And the conference name is used as the conference key or the primary key for the conference. So how do we reduce this to an ER model? First thing we notice that the entity conference can become a table called conference. And all the attributes that the conference has will form the attributes of table of which conference name is the primary key. 
Now here if you look at program committee, program committee may have a key but that is not sufficient. Why because, the program committee has no existence without a conference. So essentially while we can create a table called program committee which corresponds to the entity called program committee, it is not sufficient to just add the attributes of program committee here and leave it at that. Why because we need the conference name as well so, because it is defined by the conference name, program committee for which conference. So take the primary key of conference and make it into the primary key of the program committee itself. So in fact this would be a foreign key relationship into the conference table. 

(Refer Slide Time: 25:17)

 

So the next set of conversions. Now a PC chair heads a program committee and a PC chair and every other member. Note that again for the sake of clarity, I have not shown the entire ER diagram here but if you remember how it was a PC chair here (Refer Slide Time:  25:41) is a member. This kind of relationship here defines a is a relationship. 

So a PC chair here is a member and every member is defined by a member id. The every member is uniquely identified by a key attribute called member_id and whenever a general entity class or an entity set has a key attribute every subclass of it or every specialization of that entity type also inherits the key attribute. Therefore the PC chair also inherits the member_id as its key attribute. So now the member_id is the key attribute of this PC chair and what is the key attribute of program committee. It is the conference name, so that is a key attribute of the program committee. 

Now a PC chair heads program committee and look at this relationship. It’s a 1 to N relationship. So 1 PC chair heads, may head several program committees and so on. So how do we convert this? First of all converting a PC chair to a table is trivial. so make a table called PC chair and put all the attributes of PC chair in addition to all the attributes of member into this table and make the key of the super of the general entity type that is a super class if one might call it that, so of the general entity type called member plus any key attributes in PC chair make into composite key in the PC chair table. 

Now for this relationship itself, how do we go about doing that? Take program committee as the key, now as the table and its key attribute was conference name. Now in addition to the key attribute called conference name just put the member_id of the PC chair here as one more attribute. So there are two foreign keys here, so one forming the conference name attribute and that is one is the conference name and the other is the member_id.

However it is sufficient if we just keep the conference name as the primary key even though this is a foreign key here. Why is it sufficient to keep just the conference name as the primary key? Because let us look back here (Refer Slide Time: 28:02). Program committee is a week entity type and its existence is defined by the conference. And what is the primary key? 

A primary key usually of course is something which a key attribute which defines the existence of a particular tuple in a table. So it’s the conference name which defines the existence of the program committee. Therefore while we add member_id to the program committee table, it is sufficient if we just keep conference name as the key for this program committee table. 

Now the next set of attributes ER to relational mapping. So have a look at this thing, so we already saw PC chair here. PC chair is a member and a member could be a either a reviewer or a PC chair. And a program committee can consist of several members and each member is uniquely defined by a member _id. Now because this is a super class or a generalized entity type, this member_id is inherited by both reviewer and PC chair. So it forms a part of the primary key for both reviewer and PC chair and they may have other key attributes as well.

(Refer Slide Time: 29:20)

 
So one way to reduce this is we already made a table for PC chair with member_id as the,   with member_id as the key, we can just make one more table for reviewer and again say member_id as the key and all other attributes. Now you might ask a question, why not make table for members itself. Members is an entity and why not make a table for members itself. Note that here it is not really necessary. Why because, a member is either a reviewer or a PC chair but not both because it’s a mutually exclusive relationship between reviewer and PC chair. 

So there is no special identity assigned to a member other than a reviewer or a PC chair. And we have seen that we need this PC chair table in some other context, in the previous context here. So we need PC chair as a separate table in some other context and therefore we have also made one more table for reviewer and that’s it and both of them are collectively exhaustive in terms of this thing. So not only are they mutually exclusive, there are also collectively exhaustive and they collectively define the complete set of members. 

Now the bigger problem here is the reduction of this larger relationship. What is that relationship? A program committee may comprise of several members. However a member can also be a member of several program committees, so it’s a N: N relationship not a 1:1 relationship. So how do you reduce a N to N relationship into a relational schema? The best way to reduce that is make a separate table. So we are calling this table as membership here where just take the primary keys of both of these. 

What is the primary key here? The conference name. So just take the conference name and member_id, so of a given member. So given a member I know of which conference is that member of, PC member of and so on. So, N to N relationships can be reduced by creating a separate table and combining both of the primary keys into that table. 

(Refer Slide Time: 32:21)

 

The next set of reduction here. So we have a paper that is authored by an author. However a paper can be authored by more than one authors and an author can submit any number of papers. And of course a paper has a key attribute called paper_id whereas author has a key attribute called author_id. so again because this is an N to N relationship, you need one entity called paper with paper _id as a primary key rather one table called paper with paper_id as the primary key, one table called author with author_id as the primary key and a separate table called authored by which relates this and this. So it’s an N to N relationship, so which contains both paper_id and author_id as part of its tables. So, two foreign keys into both these earlier tables. 

(Refer Slide Time: 33:01)

 

Now on the other hand for the contact author each paper ought to have exactly one contact author and it ought to have a contact author. So as you can see here this forms a 1 to N relationship. So in such case in a 1 to N relationship, there is no need to create a separate table here except that for a given paper, you just add the author_id here which is the id of the author who is the contact author. So the primary key of the single entity type here can just go as an attribute to the table of the multiple entity type in a 1 to N relationship. 

Now again another part of the schema. As you can see here we are taking parts of the schema, breaking it down and bringing them down to the actual relational schema. Now here we saw that both members and authors are persons obviously of course. And why do we need that person relationship here? Because there were certain other requirements that we needed to capture. That is for every person in the system, we need to have some personal details like the pan number or the email address or whatever that we use to uniquely identify a person, the phone numbers, the address name, last name, first name and so on and so forth and whatever else that is required for maintaining records about a person.

(Refer Slide Time: 34:10)

 

Now we said that members and authors are persons, that is member is a person and author is a person. In addition they are disjoint that is the members and authors are disjoint. Why? There is again a specific safety condition that is explicitly specified in the requirements that no PC member ought to be or is allowed to be an author of any paper. 

So you only submit papers, you only consider papers by authors who are not members of your program committee. And members have member_ids and so on and authors have author_ids as their words and a member can be a reviewer and a PC chair. So among here we already have tables for reviewer and PC chair and we saw that we don’t need a table for member and we already have a table for author. So, all that we need is a table for person that is a person and pan number and so on and so forth. And when we say person, we also have to add all of these details but we see here that phone number is a multi valued attribute that is a person can have more than one phone numbers. 

So when you have a multi valued attribute, you cannot, you don’t know how many number of phone numbers a person is going to have. One person may have just one phone number, another person may have 5 phone numbers, another person may have 3 phone numbers and so on. So in order to deal with multi valued attributes, you have to create a separate table here called phone details where you put the primary key for person plus the phone number of the person. 

So the primary key may repeat while the phone number may change, so you can have all possible phone numbers associated with a person. And of course you might have got a doubt that why are we creating a separate table called person here and not club them into members and authors. Because members and authors are mutually exclusive as well as they are collectively exhaustive as far as this system is concerned. That is in this system we are only talking about either a PC member or an author and nobody else as far as the requirements is concerned.
The answer is yes, you can do that is you can put, if you want to do that then what you have to do is take this pan number and bring it down to the member table or an author table so that it forms part of the primary key of each of them and then take all other details of person and bring it down to either member or author. And then for phone number, you need to take just this one the pan number of this one and then create the different phone numbers here and so that would form a good, I mean that would form a valid derivation schema. 

But as you can see the problem there is that when you bring pan number down to member, you suddenly have two key attributes there is a pan number and member_id. So which attribute do you associate with phone number? Now it is not incorrect, it is not in correct if I associated member_id with phone number as well in phone details it is not in correct if I say member_id plus phone number or it is not in correct if I say author_id and phone number but it is bad design. Why is it bad design? Because when you perform any kind of systems design, we should always be concerned about what kinds of changes will the system expect in the future. 

So in the future we may want to add more categories of people into this system for example. So right now the only two categories of people we have in the system are either member or author. and they are mutually exclusive and collectively exhaustive but then they need not be collectively exhaustive in the future, we might add one more kind of a person maybe say delegate or a conference registrant or conference volunteer or something like that who is neither a member nor an author. 

And in that case the conference delegate or a volunteer is also a person who has a phone number and pan number and so on and so forth. So the safest bet to associate phone number is with the pan number. So that is the primary key of the person attribute is the safest bet for associating the phone number attribute. So as far as this particular schema is concerned here, it is not in correct if I say that oh I won’t have separate table called person but in the interest of or in the interest of anticipating what might be required in the future, this might not to be a good idea in future. 

And the software design or any kind of information system design is full of such implicit requirements in the sense that if you write a software that works here and now, it is not usually sufficient. Your software should be extensible and extensible with minimal changes and with minimal impact and that is what makes a good design. 










(Refer Slide Time: 40:11) 

 

So basically this is what I said earlier that is one way to take this up is either bring all of them down that is from person bring it down to member. but then we see that member, there is no table called member as at all so bring it in turn down to reviewer or PC chair. So you can either bring down everything to either reviewer or PC chair and author who are all persons and then put pan number and every other attribute there. But in the interest of robustness of your system, it might probably be a good idea not do that and create a separate table called person itself. 

(Refer Slide Time: 41:23)

 

Now coming to some more parts of the ER schema. So a paper maybe submitted, a paper may not be submitted to more than one conference or journal at the same time. That is if I am submitting a paper to this conference then one of the requirements is that, it is not submitted to any other conference neither is it submitted to any other journal. Note that even though we have not using journal at all in this system that is we are not, the journal is not playing an active role in the system but it is still required because we need to check whether a paper is submitted to a journal or not. And here we have, how did we model that using a conceptual model? We used a separate entity called event were an event could be either a conference or a journal and conference has its conference name has its key and journal has journal_id has its primary key and a paper can be submitted to 0 or 1 event that was what we had made here. 

So how do you reduce this to a relational model? Conference, we already have a table called conference. So just create a table called journal as well because journal is an entity and it has its own key and attributes and so on. Now because a paper is submitted to one of these two, journal or a conference just take these two primary keys and add them as foreign keys here. So put both journal id and conference id or conference name rather as part of the paper. So given a paper, you know exactly to which journal or conference was it submitted. 

Now note that these are added as foreign keys and not primary keys of course. that is they don’t define the existence of the paper and a characteristic of foreign keys is that foreign keys can be null that is because a paper can be submitted at most to one of these two, that is one of either journal or a conference one of these is guaranteed to be null in any given tuple. In fact if they are, if both of them are non null then we have an integrity violation, we have a constraint violation in the system already. 

And the last one is the attribute which is assigned to a relation itself that is a paper should be reviewed by at least three reviewers so 0 to N papers reviewed by 3 to N reviewers and the reviewer should assign some result accept, reject or neutral. And the result as we see here belongs neither to the paper nor to the reviewer. In fact the result belongs to the process of reviewing itself. 

So the result belongs to the relationship called paper is reviewed by a reviewer. So how do we manage this? In order to capture that the result belongs to a relationship make a separate table in the name of the relationship. So reviewed by becomes a table and how do you recognize or how do you define tuples in this table. Take the paper id or take the primary key here, make part of the primary key here, take the primary key here member_id that becomes a primary key here and the result.







(Refer Slide Time: 43:38)

 

So for this paper_id, this member_id gave this result and so on. So whenever we have a relationship that is part of the, that is part of the rather whenever we have an attribute that is part of the relationship then we have to create a separate table in order to maintain this, in order to reduce this to the relational model. 

(Refer Slide Time: 46:01)

 

So that brings us to the relational schema itself, so the final relational schema. So how does the relational schema look like? There are so many tables that that we have already found. There is the conference table which we created to begin with where conference name is the primary key and then there is the date of the conference place, topic and so on and so forth. 

And then there is the pro program committee table which actually represents the week entity type called program committee. Therefore it takes the primary key of the conference table namely conference name as its own primary key. And then there are other attributes like strength which is the attribute of the program committee and then there is member_id which is the foreign key into the PC chair table which says who is the PC chair of this program committee and so on. 

And then we have the reviewer table, after the program committee table we have the reviewer table where member_id is the is the primary key and of course pan number, we inherited from the person attribute because we may require it for other purposes and then some other attributes like subject_of_expertise and so on. And then there is the PC chair table where member_id is again the primary key and maybe some other attributes like number of conferences that is headed and so on and whatever. And in addition we have inherited any other attributes from members or persons and so on, because PC chair is a member which is in turn a person and so on.

And then there is the membership table, remember were the membership table came from. The membership table came because a member or PC member can be a member of more than one conference committee and a conference committee would have more than one PC members and so on. So in order to maintain this N to N relationships, we created a separate table called membership where both conference name and member_id form the primary key and which are both foreign keys into their respective tables that is the conference and either program committee or either reviewer or PC chair and so on. 

And it’s actually the reviewer not the PC chair. And then we have the author table where author_id is the primary key then pan number is inherited from the person entity and everything else I have not specified any other attributes that an author might have. Then authored by is another table in order to denote the N to N relationships between an author and a paper. That is an author may submit any number of papers and a paper may have any number of authors, more than one authors in there. 

Similarly person is a separate entity. Now we created the separate entity called person like I said previously in the interest of extensibility of the design rather than even though it is not in correct to just take all this person details like first name, middle name, last name, date of birth and put them straight to reviewers or authors and so on. But in the interest of extensibility and the interest of the robustness of the system, we have made person into a separate table. And of course as you can see, there is no mathematical rule that says that you have to do that and its again a common sense decision that we expect that more kind of persons may be added to the program, may be added to the system at a later point in time and so on.

Then a phone details is another table where which is required because of the multi valued attribute in the person table were a person can have any number of phone numbers. So you have a pan number and phone number combination which details any number of phone numbers that a person may have. And then there is the journal which is required just to keep track of the fact that a paper may not be submitted to more than one conference or journals at the same time. And then the journal_id, in fact for our purposes here the only requirement of journal is the journal_id or the primary key of the journal in this thing.

So the journal_id comes into the paper as well as conference name or not the conference_id and we can add an integrity constraint that no tuple shall have both journal_id and conference_id as non-null attributes or not null attributes and so on. And reviewed by is another table which comes out because of the attribute called result which is a member of the relationship reviewed by rather than a member of either paper or reviewer, so we get a separate table. So once I create table tables like this then you can essentially for each table, you can write the corresponding SQL statements.

(Refer Slide Time:  50:35)

 

So create table conference conf_name place and so on and say primary key conf conf_name and so on or something like create table program committee and conference name and strength and so on.










(Refer Slide Time: 50:57)

 

And say primary key is conference name and but conference name is the same conference name as the conference name of the other table. Therefore you have a foreign key relationship saying that it refers to that attribute in that other table and so on. 

So, basically that I won't go in to the creation of those SQL statements again I am assuming that you know SQL and given a set of relational, I mean given a set of table specifications I am sure that you know how to write corresponding SQL statements and give it to a DBMS client which in turn can create a tables in a DBMS at the back end. 

Now let us briefly come to the last aspect of today’s lecture as well namely that of designing transactions or designing the dynamics of the system. We have not really talked about transactions as yet in the course of this series of lectures but nevertheless for the sake of completeness, let us just look at one or two transactions and we leave it at that and we won’t go in to lot of details into transactions itself. So what’s a transaction? A transaction is a set of activities or a set of tasks which represents one semantic activity or a set of database tasks like read, write, update and so on which represents zero or more database updates which represent one semantic activity. This is a very simplistic definition of transactions but we will just have it at that.










(Refer Slide Time:  52:29)

 

And transactions have to follow, have to possess what is called as the ACID property. Again I shall not be going into more details into this ACID property but you will be looking into ACID properties in a later lecture in much more detail. ACID basically means stands for atomicity consistency isolation and durability. So essentially what it means is that the entire set of activities or tasks that form a transaction should be performed as one atomic whole either you do the entire thing or none of them. 

And they should be performed in isolation of other transactions. So no two activities of, no activities of two or more transactions should interfere with one another logically and so on. And in order to manage a design transaction, we should also know what kind of system architecture we use in this. So let us make some assumptions about the system architecture. So we are using a client server architecture where the databases serve or is powered by a DBMS or a database management systems and the client sessions are stateless. Meaning what? The server does not remember previous client interaction. So every time you need the server to do something, you need to give every possible requirement or every possible data to it in order to perform this activities. 












(Refer Slide Time: 53:42)

 

So let us look at some transactions some simple transactions that give us an idea as to what kind of activities would go on. So let us say registration, let us say I want to register a new author or user or whatever reviewer or PC chair into the system. 

(Refer Slide Time: 54:37)

 
 
So how would you register? Typically you would have a form in front of you where you would have all details. What are the details? look at the person or author or reviewer tables and just make all those details there, the name, first name, last name, pan number and so on and so forth and phone numbers and so on. 

So verify those details for any syntactic errors then read the corresponding tables, verify that there are no duplicate registration that is one person is not trying to register multiple times. Once this is not there, once this is not the case, once we verify that this is not the case then you can update the table. 

Now what this means is that all this four activities have to be performed as one semantic whole. As you can see how many different activities are happening here, there are N number of reads happening here and one updates happening here either all of them should happen or none of them should happen. So, all of these form one semantic activity called the transaction. 

Similarly login. So let us say that I have created user id for myself, I have to login. So verify user name and password field for syntactic correctness that is I provide my user name and password for login. Then read the corresponding table author, PC chair, reviewer table, match specified user name and password, we have not really taken care of password then. 

(Refer Slide Time:  55:35) 

 

So another idea here is that once you start designing transactions, you will see that you need these two attributes also that is user name and password which we had not factored in at all when we are looking at the ER or the relational model. So you have to go back and make those changes and this thing. And then you have to maintain sessions and so on and let me not go into sessions, session management and so on. And one more transaction which I am not going to go into detail here, upload paper transaction or whatever. 





(Refer Slide Time: 56:19)

 

(Refer Slide Time: 56:27)

 

And you can think of several more transactions like this assign reviewers, submit review, arbitrate so all of these are semantic activities which perform zero or more updates of the entire system. So how would the entire system look like? See why did I just take up transactions for a brief while because just to show you how does the entire system look like.




(Refer Slide Time: 56:49)

 

So here we had the ER model and from the ER model we develop the relational schema and this is the DBMS here, actually this whole thing would be the DBMS, so this is the database here. And on top of the relational schema there would be a transaction manager which would manage transactions, on top of this would be what is called as a business logic manager which handles multiple transactions and so on which in turn would be served by some kind of a DBMS server through which the client connections are handled. 

So this would, basically this would conclude the overall system design of one particular case study of the entire system. Of course firstly, we are not comprehensive I mean this not comprehensive enough to take up a real life system. But mainly the whole idea here was to give an idea, the whole idea behind this is to give you a perspective into what it takes into designing a particular DBMS. So with that let us come to the end of the session. 	































 
















 














