Artificial Intelligence Artificial Intelligence (Lecture – 1)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

Introduction

[00:50] Ttoday we start the lecture on the course artificial intelligence Artificial Intelligence. 
thisThis course will be delivered by me sudeshna Sudeshna sarkar Sarkarthat’s myself and professorProfessor anupam Anupam basu Basu both from the Ccomputer Sscience and engineering department IITi i t Kkharagpur. 
(refer slide time [01:10])
  

theThe goals for this course isgoals for this course are to introduce you to the field of artificial intelligence Artificial Intelligence. 

[01:16] we We want to explain to you the challenges that are inherent in building a system that can be considered to be intelligent. 
inIn this course we will be explaining the key paradigms of artificial intelligence Artificial Intelligence, the core techniques and technologies that are used and algorithms for some of these techniques. 	 
(refer slide time [01:43])
 

theThe instructional objectives of this course:
on On taking this course you should b able to understand the role of basic knowledge representation, how to represent our knowledge about the world and knowledge of problem solving techniques and a knowledge of some of the learning methods in a iAI. andAnd we will see how these are used to solve different problems and to build a complete intelligent system system.

[02:21] on On taking this course you should be able to assess the applicability the strengths and weaknesses of these methods of the different techniques that that we will discuss. 
weWe will discuss the strengths of this method and situations where these methods can be applied to solve different types of problems that require intelligence. 
youYou will learn how to develop intelligent systems by assembling solutions to concrete computational problems. 
(refer slide time [02:56])

 

[03:01] tThe way we will do it is, we will look at the different components of intelligence and for each of these we will discuss ways of solving these problems. and And then, depending on the full system the functionality of the system that you wish to construct or engineer you can put together some of these solutions to get the full system. 
afterAfter you have taken the scores you should be able to appreciate the role of problem solving, the role of natural language processing, the role of computer vision etcetera in understanding human intelligence from a computational point of view. 
someSome more to the points on objectives of the course: 
Oon taking this courser you should be able to formulate certain types of problems as state space search problems and you should learn what arethe efficient methods to solve them 

[04:13] depending upon the characteristics of the problem space, you should be able to write programs that play games particularly towo player games, 
you should be able to use learning to find patterns in data to finsd rules from data, 
you should be able to build expert systems for different diagnostic and other purposes. 
(refer slide time [04:38])

 

someSome of the text books that we will follow for this course are the two main the two books which iI will be referring to and professor bBasu will also talk about the other books that he will refer to. 
theThe books are artificial intelligence Artificial Intelligence a modern approach second edition by stuart Stuart Russell and Ppeter noNorwich.rvig 

[05:04] Tthis book is published by prentice Prentice Hhall and also by pPearson. 
theThe second book is artificial intelligence Artificial Intelligence a new synthesis by nils nilssonNilsson published by morganMorgan kaufmannKaufmann publishers. 
today’sToday’s lecture will be the first lecture  inlecture in athe series. 
theThe first module for this course is the introduction and today’s lecture is the first lecture this module which is introduction to a iAI. 
nowNow let u’s come to the objectives of today’s lecture. 
theThe instructional objectives of today’s lecture isobjectives of today’s lecture are: 
(refer slide time [05:50])

 

number one toTo understand the definition of artificial intelligence Artificial Intelligence, 
what artificial intelligence Artificial Intelligence is? 
whatWhat is it aboutabout? 
secondlySecondly we will be discussing the different faculties that are involved with intelligent behavior, 

[06:08] 
the different components that define intelligence. 
we We will also be examining the different ways of approaching a iAI and finally we will also  look at some example systems that have been constructed, that are well known popularly known which use a iAI and lastly we will also take a brief look at the history of a iAI. 
onOn taking this lesson you should become familiar with the different ways of defining artificial intelligence Artificial Intelligence. as As we will see different people may define a iAI differently and we will familiarize ourselves with these definitions. 

[07:01] secondly Secondly as iI mentioned we will try to understand the different components of intelligent behavior. 
(refer slide time [07:06])

  

anotherAnother objective of today’s lecture is also to let you develop an appreciation of the vast scope of artificial intelligence Artificial Intelligence and the intellectual challenges that are there in the field. 
on On talking today’s course you should be able to have a fair idea of the types of problems that can be currently formed solved by the computers and today’s techniques that we know. today  and wWe will also have an idea of those problems that i’s still difficult or we cannot yet solve it by the techniques that we know today. 
(refer slide time [07:54])

 

so theseThese are four main components of today’s lecture. 

[08:00] definition Definition of a iAI, example systems, approaches to a AIi and the brief history. 
firstFirst we will take up the definition of a iAI. 
nowNow, what is artificial intelligence Artificial Intelligence? 
thereThere are too many definitions of this term floating around. 
(refer slide time [08:17])

 

asAs you see or what is clear from the two words artificial intelligence Artificial Intelligence 	
it i’s clear to see that a iAI is concerned with the design of intelligence. and And in the first term artificial a iAI is actually concerned with design of intelligence in artificial artifacts and artificial devices. 
S,o systems artificial systems are or man made systems and are building intelligence into them. 
thisThis term was coined by mccarthyMcCarthy in nineteen fifty six1956 in a famous conference the dartmouthDartmouth conference. which we will have occasion to talk about later 

[09:14] Nnow, the term artificial is easy to understand but what is intelligence? 
(refer slide time [09:25])

 

It i’s very difficult to define intelligence. 
oftenOften we look at some people look at intelligence as something that characterizes humans. 
ifIf you take human beings to be intelligent you can say artificial intelligence Artificial Intelligence means having behavior which is like a human. 
inIn fact there are two schools of thought here. 
so some people orHere an idea is to have a machine or have a system that behaves like a human. or the other school of thought is to you see humansHumans are not always completely intelligent right even though humans are very good. actuallyActually pretty intelligent but all the time humans do not behave intelligently. 

[10:21] so So the other school of thought is that artificial intelligence Artificial Intelligence concerns with intelligence which is the ideal or the best behavior 
 or the most rational behavior. 
it’ is the machine that should behave in the best possible manner. 
thereThere is another dichotomy in the definition.
whenWhen we talk about behavior what sort of behavior are we talking aboutabout? 
thereThere are two main types of behavior that people will like to talk about. 
numberNumber one is thinking,  
thinkingthinking intelligently, reasoning properly and intelligently in order to come up with a solution. and And the second approach is to talk about not thinking but acting how the system actually acts or behaves. 

[11:15] so we see there are wWe can talk about intelligence as something which characterizes humans or something that means behavior in the best possible manner or behaving rationally. 
againAgain we can talk about intelligence in thought or intelligence in action. 
soSo based on this criteriathis criterion we can look at the different ways of defining a AIi. 
soSo we may look at thought processing or reasoning versus behavior, we may look at human like performance versus ideal rational performance. and And this diagram shows the four different definitions that emerge from these two dichotomous.“   “ terms
(refer slide time [12:10])

 

(refer slide time [12:12])

 

[12:13] so Oon the one hand we have thought or reasoning versus behavior and
on the other hand we have human like performance versus ideal performance. 
soSo there are systems that think like humans humans. 
forFor example we will discuss the famous turingTuring test which was devised by alanAlan turingTuring where the system which passed the turingTuring test would be a system that behaves like a human or thinks like a human. 
theThe second definition is systems that think rationally. 

[13:00] tThe school of thought where different philosophers, mathematicians and computer scientists who have worked on logic and laws of thought believe in this approach. 
thirdlyThirdly there are systems that act like humans. 
cognitiveCognitive scientists look at the properties of systems that act like humans and finally we have the definition systems that act rationally or systems that act in the best possible manner. and And for this we have the approach of constructing a rational agent an agent which acts rationally. 
alanAlan turingTuring considered by many to be the father of a iAI devised the turingTuring test. 

[14:05] Iin the turingTuring test this is the experimental set up that its devised.  
thereThere will be a closed room and
i in this closed room there will be a being which may be a computer and it may be a human. 
There i’s an interrogator outside the room. 
theThe interrogator does not know whether the being inside the room is a computer or a human. 
so So what the interrogator does is that the interrogator asks questions and the human the being inside the room processes these questions and returns some answer and the interrogator on the left room he receives the answers on the screen. 

[15:09] Nnow the interrogator has to make out from the answers whether the being inside the room whether it’sis  a computer or human. 
Now, if there i’s a computer inside the room the computer tries to convince the interrogator that it i’s actually a human being 
(refer slide time [15:30])

 

in the way it answers to the questions and it i’s the task of the interrogator to decide who is human. 
so Tthis is a schematic diagram of the turingTuring test, 
 this is the interrogator sitting in front of the terminal, 
this is a wall room. 
theThe wall room may contain either a human or a computer and the interrogator has to decide whether what is inside is a computer or a human being.  
(refer slide time [15:51])
 

(refer slide time [15:59])

 

Now, if the interrogator cannot reliably distinguish between a human answerer and a computer answerer then we can say that the computer system possesses artificial intelligence Artificial Intelligence. 

[16:19] so tThis is the test devised by turingTuring to find out whether the machine has been able to come up with a right amount of intelligence to match human intelligence in answering questions. 
Nnow let us look at typical AIa i problems. 
intelligentIntelligent entities arte agents the need to be able to do different types of tasks. 
such tasks thereThere are some tasks which are mundane tasks that we do as a matter of fact in out daily life and there are some tasks that we consider intelligent like solving difficult mathematical problems, playing games of chess in an expert fashion and so onother activities which intelligent people can do well. 

[17:20] Nnow examples of mundane tasks are planning route. 
supposeSuppose you want to go to here from the market and you plan a path along which you will go. Oor you want to go from here to let us a say a particular place in delhiDelhi and you have to plan your journey and plan your path. 
then you have to suppose somethingSomething that we do all the time is try to recognize objects or recognize faces of people, 
 this requires vision. 

[18:00] Tthirdly we communicate with each other through natural language. 
fourthlyFourthly we navigate around obstacles on the street street.
so So these are the tasks that we do routinely. 
inIn fact most animals do this task routinely. and And then there are expert tasks like medical diagnosis which are only the doctor or the expert in the field does. 
And mathematicalmathematical problem solving which can be done effectively only by good mathematicians. 
(refer slide time [18:31])
  

nowNow which of these problems are easy for the computer to do and which of these problems are hard. 
surprisinglySurprisingly it has been much easier to mechanize many of the high level tasks which are so called expert tasks 
it which has been easier in the history of a iAI and the history of computers. 

[19:02] iIt has been easier to solve problems which are really the domain of experts but a iAI has not had the same amount of success in dealing with mundane tasks. 
forFor example, a iAI systems can easily do symbolic integration. 
someSome of the systems can do can prove some theorems. 
a iAI systems can play chess quite well. 
thereThere are systems [“  “] diagnosis in particular domains. 
howeverHowever there are certain things that humans and animals do quite effortlessly. 
forFor example, walking around without running into things catching prey and avoiding predators, interpreting complex sensory information, modeling the internal states of other animals trying to understand what they are thinking about us and how to plan what to say and so on and also working as a team or collaborating. 
(refer slide time [20:03])

 

[20:17] Tthen these tasks unfortunately have not all been easy to do by machines. 
Let u’s look at some of the basic intelligent behavior in human beings. 
perceptionPerception that i’s the ability to see, hear sensory information. 
reasoningReasoning: reasoning Reasoning with the information that we have. 
Learning: Llearning for new situations, 
 understanding natural language, communicating in natural language, 
 solving problems. 
(refer slide time [20:50])

   

nowHence we have looked at so these things are what
 these things namely perception, reasoning, learning, language, understanding and solving problems are examples of some of the things that we want our a iAI systems to solve. 

[21:13] now that we haveHaving looked at the definition of a AIi let u’s have a look at some examples of AI a i systems that have been around. 
(refer slide time [21:26])

 

theseThese are some of the applications of a iAI: 
computerComputer vision, image recognition including face recognition, robotics, natural language processing and natural language understanding, speech processing, etc.etera 
thenThen if you look the practical impact of a iAI the AI a i components are embedded in numerous devices. 
evenEven in some copy machines there are a iAI components embedded. 

[2:04] a iAI systems are in everyday use in detecting credit card fraud, in configuring products, in complex planning tasks, in advising physicians. 
theThen intelligent tutoring systems provide students with personalized attention. 
theseThese systems are there being used and they have a tremendous impact because they are so useful. 
(refer slide time [22:22])

 

thisThis is a system alvinn ALVINN which stands for autonomous Autonomous land Land Vvehicle in a Nneural Nnetwork. 
itIt was conceived it was designed in nineteen eighty nine1989 by dean Dean Ppomerleau at carnegieCarnegie Mmellon Uuniversity.  
thisThis systems drove a car from the east coast to the west coast across united states of americaUnited States of America using computer control. and And it drove completely autonomously for most of the two thousand eight hundred and fifty2850 miles. 
onlyOnly for fifty 50 miles especially at exits to freeways etcetera the human driver took charge. 
forFor two thousand eight hundred2800 miles the car drove itselfit. and And the idea behind the car is quite simple. 
inIn front of the car is a camera which takes a picture of the road in front. 
(refer slide time [23:55])

 

andAnd this picture or this image is used in a neural network. 

[24:04] Tthis picture is captured into an image having thirty by thirty two30/32 pixels. 
theseThese pixels are fed into a neural network four hidden units and the output comes to the output tells the computer the processor which way to turn the wheel and decide the speed and so on. 
inIn nineteen ninety seven1997 the deep blue chess program developed at i b mIBM be itat the current world chess champion garyGary Kkasparov. 
thisThis is the computer deep blue and this is garyGary Kkasparov acridly after he lost the match accidentally. 
(refer slide time [25:00])

 

[25:06] Iin a machine translation if we could have immediate translations between people speaking different languages that would be a remarkable feat and it has very wide ranging economic and cultural implications. 
inIn the world today there are people speaking so many different languages and we do not understand the languages of many other people. 
evenEven in indiaIndia as you know there are so many languages, 
 there are more than twenty 20 official languages and iI cannot understand the language of each in hereIndian.
soSo would it no’t it be nice if we had a system which would do simultaneous machine translations so that we can effortlessly understand each other. 

[26:06] Ffull machine translation is not yet there but there are there has been quite some progress in the field of machine translation in a small way. 
forFor examplexample,e the u sUS military is giving a simpler one way translation device 
they are using this in iraqIraq. 
u sUS forces are using the Pphraselator to communicate with injured iraqisIraqi prisoners of war travelers at checkpoints and for other peace keeping duties. 
carnegieCarnegie mellonMellon universityUniversity is working on a system called the speechlator for use in doctor patient interviews. 
(refer slide time [26:29])

 

imagineImagine how difficult it i’s when a doctor does no’t understand the language of the patient. 
And when the patient does n’ot understand the language of the doctor 
the patient will not be able to communicate his symptoms to the doctor. 

[27:03] so So speechlator is used in order to help doctors do so. 
inIn space exploration robotic space probes autonomously monitor their surroundings, make decisions and act to achieve their goals. 
thisThis is the homepage of mars Mars exploration Exploration rover Rover mission Mission. 
(refer slide time [27:35])

    

so nasa’s mars rovers wait
so ifIf you have a look at this page hosted by jet propulsion laboratory this page brings us live the explorations that are being carried on by the mars Mars Rrover. 

[28:07] Tthere are two mars Mars Rrovers spirit and opportunity that have been sent to mars Mars. 
theyThey have already finished their primary assignments and are continuing with exploratory duties. 
so theseThese two pages contain updates of spirit and this page contains the update of opportunity. 
(refer slide time [28:32])

 

forFor example, let me read for you an excerpt from the spirit update. 
justJust a little rat RAT,
 spirit Spirit spent the last few salts investigating pot of gold including a successful grind with the rock Rock Aabrasion Ttool, 
 that i’s what RATrat is, 
 a rat RAT is a rock Rock abrasion Abrasion tool Tool.
soSo, what spirit is doing is using a rock abrasion tool and getting samples of rocks from the surface of mars Mars and it’ is trying to find out what chemicals are present in the rock. 

[29:10] so So one of the objectives of this mission is to find out whether there is water in mars Mars. 
inIn fact these mars Mars Rrovers have been able to trace the presence of water from the rock samples in mars Mars. 
thenThen opportunity is going from Virginia to London. 
theseThese are different locations defined on the mars’ surface and opportunity is currently in a crater called the endurance crater and it i’s abrading and examining rocks. 
thisThis image shows the area inside endurance crater that opportunity has been examining. 
theThe rover Rover is investigating the distinct layers of rock that make up this region. and And this image taken by rover Rover highlights the nodulare and nuggets that cover the rock that has been named the Ppot of Ggold. 

[30:18] Tthese nuggets appear to stand on the end of the of stalk like features. 
theThe surface of the rock is dotted with fine scale pits. and And there are so many other news about these two Sspace rovers rovers. 
theThe Sspirit rover rover is currently exploring a range of marsh and hills that took two months to reach. 
It i’s finding curiously eroding rocks that may be new pieces to the puzzle of the  region’sthe region’s past. 
spirit’sSpirit’s twin opportunity is also negotiating sloped ground examining exposed rock layers inside a crater informally named endurance. 
(refer slide time [31:06])

 

[31:08] so you see that wWe have intelligent agents that are going to unknown territory where no human has been before and they are carrying on explorations and making new inferences. 
thenThen there are internet agents. 
(refer slide time [31:29])

 

allAll of you are familiar with the explosive growth of the internet in recent years and there is a growing interest in internet agents that can monitor users’ tasks, seek information that is needed from the web and learn which information is most useful for a particular user. 
Nnow that we have looked at different examples of systems that use artificial intelligence Artificial Intelligence we will briefly look at some approaches to a iAI and some approaches to solving a iAI tasks. 

[32:26] Oone way of looking at a iAI is strong a AIi or weak AIa i. 
strongStrong a iAI aims to build machines that can truly reason and solve problems. 
machinesStrong AI are machines that are self aware and whose overall intellectual ability is indistinguishable from that of a human being. 
(refer slide time [32:54])

 
soSo strong a iAI proponents want to develop systems that are completely intelligent and that can do things fully using their own intelligence. 

[33:07] such Such systems can be human like can be non- human -like but rational. 
when a iAI was conceived in the nineteen fifties1950s and nineteen sixtie1960s there was a huge optimism about a iAI and there was a prediction that very soon a iAI systems will be able to overtake humans and able to everything that a human can do and can do them much better and do tasks that humans cannot do within a short time. 
Hhowever such optimism havehas been ill founded and this was partly the reason why some people lost faith in the techniques of a iAI. Bbut now after research into a iAI has taken place for over fifty 50 years now we are in a position to understand and appreciate the true difficulty of the different problems that a iAI face. and And we known what we can aim to solve now and what is more difficult and you we will need much need different techniques, different hardware and different paradigms to be able to be able solveto solve. 
weakWeak a iAI unlike strong a AIi weak a i deals with the creation of some artificial intelligence Artificial Intelligence that cannot truly reason and solve problems but act as if it were intelligent. 

[35:06] so So the proponents of weak a iAI claim that machines which have been suitably programmed can simulate human cognition, appear to behave intelligently, appear to do tasks well and intelligently without really having the same intelligence or understanding as humans possess. 
so Therefore strong a iAI really deals with machines that really have mental states that think, reason, understand their behavior whereas weak a iAI is involved in simulating human behavior or simulating intelligent behavior without really claiming that the reasoning process behind it is intelligent. 
(refer slide time [35:45])

  

[36:00] applied a i the goal of applied a iAI is to produce viable smart systems. 
forFor example, it will be nice to have a security system that i’s able to recognize the faces of people who are permitted to enter a particular building. 
so thereThere are certain applications which are useful to us and applied a iAI aims to solve theise applications intelligently, not necessarily to construct a complete intelligent agent but an agent which is intelligent in doing a specific task task.
forFor example, recognize people, detect credit card fraud, drive a vehicle autonomously. 
 soSo they take up specific tasks and develop systems that solve those tasks. 
fourthlyFourthly cognitive a iAI deals with the studies where computers are used to test theories about how the human mind works. 

[37:14] cognitive Cognitive scientists want to understand how humans act, how humans behave, how humans think and these theories can be tested by building these theories into machines and watching and testing how well the machines function suing using those theories.
(refer slide time [37:49])

  
forFor example, one may have a theory about how humans recognize faces. 
weWe do not know how we recognize faces, how our brain recognizes faces, how we store all the different faces or some of the many different faces that we have seen in our life time and how we look at a person and recognize them. 

[38:05] so So cognitive scientists have come up with different theories about how people recognize faces or how people solve different types of problems. aAnd some of these theories can be tested by building similar mechanisms which are machine and testing how well the machines perform. 
so hereHere iI have outlined some of the topics of a iAI. 
(refer slide time [38:37])

 

inIn the core areas we talked about knowledge representation, reasoning, machine learning. 
 generalGeneral algorithms: search, planning, constraint, satisfaction. 
Perception: vision, natural language processing, robotics. 
applicationsApplications: game playing, a iAI and education, distributed agents. 
Uncertainty: probabilistic approaches, decision theory, reasoning with symbolic data. 

[39:18] Tthese are some of the topic that people study in a iAI and in this course also we are going to study some so these topics or most of these topics. 
todayToday successful a iAI systems operate in well defined specific domains employing narrow or specialized knowledge. 
(refer slide time [39:55])

 

howeverHowever if you want to artifact a system that has general intelligence that can work intelligently in any domain we need to have a lot of things. 

[40:07] Ffor example, such a system must have common sense knowledge which is needed to function in open ended worlds. 
weWe sue use such a huge amount of common sense knowledge or background knowledge to do our tasks well. 
ifIf we really start thinking andt trying to note them down then it is’s a huge effort. 
There i’s an effort at Stanford university University by Ttog lLenneat gGuha and others called the psyche project whose objective is to document all common sense knowledge so that one can have a system that can use all theise common sense knowledge for their reasoning. 

[41:00] secondly Secondly a general unconstrained a iAI system must be able to understand natural language, 
in fact unconstrained natural language. 
thoughThough there has been a lot of strides in natural language understanding, then understanding unconstrained natural language in general is a very difficult problem which will require a lot of expertise to solve completely. 
whatWhat can today’s a iAI systems do? 
weWe have systems that can recognize faces, 
 we have almost autonomous vehicles, 
 our natural language processing systems can do simple machine translation. 
ourOur expert systems can do medical diagnosis in a narrow domain. 
(refer slide time [42:01])

 

[42:06] our spoken language systems are capable of thousand word continuous speech.
planningPlanning and scheduling systems are used in hubbleHubble telescope experiments. 
you may know that hubbleHubble telescope is one of the most well known telescopes which have been around for several years. 
nowNow there i’s a talk of dismantling it the Hubble telescope because it has become quite old and the cost of maintaining it has become huge. butBut it has is for a long time the hubbleHubble telescope has been the most important telescope for gathering a lot of data and there are so many people who want to use the hubbleHubble telescope. 
There i’s a complex planning and scheduling problem to schedule theise tasks on the telescope 
itwhich  has been done by a iAI systems. 

[43:03] Iin learning our text categorization systems can work and categorize the text at about thousand topics. 
inIn games a iAI system hasve achieved grand master level in chess where the “[noise   ]…“  world champions we have good programs playing checkers. 
(refer slide time [43:22])

 

butBut there are many limitations to what a iAI cannot do yet. 
a AIi systems currently cannot understand natural language robustly. 
a iAI systems cannot surf the web yet or interpret an arbitrary visual scene. 
weWe have seen that they can recognize facial images or work in a narrow domain of recognition. 
the a iAI systems cannot fully learn a natural language. 

[44:03] Tthey cannot construct plans in all sorts of dynamic real time domains in general. and And aAI i systems do not yet exhibit true autonomy and intelligence. 
(refer slide time [44:05])
 
  

nowNow that we have looked at some of the approaches of a iAI and what a iAI can do and not do at present 
 let us have a look at the brief history of artificial intelligence Artificial Intelligence. 
theThe dream of making a computer imitate us began many centuries ago. 
intellectualIntellectual roots of a iAI stretch back thousands of years into the earliest studiesy of nature of knowledge and the nature of reasoning. 
(refer slide time [44:58])
 

[45:03] Tthe concept of intelligent machines is found in greek Greek mythology. 
inIn eighth 8th century Pygmalion is credited to have asked the goddess and obtained a ivory statue of a woman built after the fashion that whe liked. 
(refer slide time [45:22])

  

Hephaestus created a huge robot Ttalos to guard Ccrete. 
soSo this robot used to go around the island of Ccrete hurling stones at invaders and to detract invaders and if found an opponent it would squeeze him to death. 
artificial intelligence Artificial Intelligence draws from many areas from philosophy, from mathematics, from economics, biology, psychologyand psychology and from computer engineering and also from linguistics. 
(refer slide time [46:14])

 

[46:19] Pphilosophers have analyzed the nature of knowledge and have explored formal frameworks for developing conclusions. 
thereThere have been mathematical formalizations in logic, in computation and probability. 
economistsEconomists have developed decision theory and reasoned about not economists biologists have reasoned about how the brain processes information. 
psychologistsPsychologists have long studied human cognition and they require a knowledgeknowledge about the nature of human intelligence. 
(refer slide time [47:02])

 

andAnd finally we want to know how to build an efficient computer. 

[47:12] so So, in the earliest ancient days Aristotle in the fourth 4th century b cB.C. developed an informal system of logic which was the first formal deductive reasoning system.
(refer slide time [47:26])

 

in the thirteenth 13th century we have ramonRamon Llull a Spanish theologian who invented the idea of a machine that would produce all knowledge by putting together words at random. 
heHe even tried to build such a machine as concept wheel. 
(refer slide time [47:43])

 

theThen early in the seventeenth 17th century Descartes proposed that bodies of animals are nothing more than complex machines. 
(refer slide time [47:54])

 

Bblaise Ppascal in sixteen forty two1642 built the first mechanical digital calculating machine. 
Leibniz in sixteen seventy three1673 improved pascal’sPascal’s machine.
(refer slide time [48:05])
 

[48:10] so So that was the first step in building a mechanical computing device. 
inIn nineteenth 19th century George Boole developed a binary algebra representation which laid the foundation of Boolean algebra. 
charlesCharles babbageBabbage and Llady Aada Bbyron worked on programmable mechanical calculating machines. 
(refer slide time [48:37])

  

Iin the late nineteenth 19th century and the early twentieth 20th century mathematical philosophers like Ggottlob fFrege, Bertram Russell, Alfred Wwhitehead and kurt Kurt godel Gödel built on Bboole’s initial logic concepts to develop mathematical representations of logic problems. 
(refer slide time [49:00])
 

[49:04] Tthe advent of electronic computers provided really provided a revolutionary advance in our ability to study intelligence. 
(refer slide time [49:12])


 

inIn nineteen forty three1943 mMccCulloch and pPitts built a Boolean circuit model of the brain. 
aA Llogical Ccalculus of Iideas Iimmanent in Nnervous activity Activity was published and it explained for the first time how it i’s possible for neural networks to compute. 
(refer slide time [49:32])

 

marvinMarvin Mminsky and dean Dean edmonds Edmonds built the snarc SNARC in nineteen fifty one 1951 which is a neural network computer. 	
(refer slide time [49:44])


 

weWe have already seen alanAlan turingTuring. 
inIn nineteen fifty1950  turingTuring published his computing machinery and intelligence and this article articulated a complete vision of a iAI of solving problems, how a iAI systems can solve problems by searching through a space of possible solutions guided by heuristics. 
(refer slide time [50:09])

 

[50:09] Hhe illustrated his ideas on machine intelligence by reference to chess. 
heHe propounded the possibility of letting the machine alter its own instructions so that machines can learn from experience. 
inIn nineteen fifty two1952 to fifty six56 Samuel designed a checkers playing program. 
inIn nineteen fifty six1956 alanAllen newellNewell and albertAlbert simonSimon designed the logic theorist. 
thenThen the general problem solver was built by the same people. 
inIn nineteen fifty nine1959 Gelernter developed the geometry engine for solving plane geometry problems. 
(refer slide time [50:48])

 

inIn nineteen fifty six1956 a meeting was held in dartmouthDartmouth where the first researchers in a i AI met. andAnd in this month long meeting the term artificial intelligence Artificial Intelligence was adopted. 

[51:11] Tthis conference brought together the founding fathers of a iAI for the first time. 
inIn nineteen sixty one1961 jamesJames slagle Slagle wrote the first symbolic integration program.
thisThis program saint could solve calculus problems at the college freshman level. 
inIn nineteen sixty three1963 Thomas evan’s Evan’s program analogy was designed, 
 it could solve i qIQ test problems. 
inIn nineteen sixty three1963 Ffeigenbaum and Feldman wrote a collection of important articles about a AI.i 
(refer slide time [51:45])

  

thenThen we have dannyDanny Bbobrow in sixty four64 who worked with algebra word problems and 
in nineteen sixty five1965 allenAllen robinsonRobinson developed a resolution method. 

[52:07] in nineteen sixty six1966 to 74seventy four there was a lot of work on computational complexity by not really a AIi researchers but by computer scientists computer theorists which had a tremendous impact on the field of a iAI. 
(refer slide time [52:05])

 

beforeBefore that people felt that a lot of things were possible by a AIi and we will soon have an extremely intelligent computer. butBut the limitations to the computational power was discovered when computational complexity was understood. 
inIn nineteen sixty seven1967 Ffeigenbaum and others developed a general program which was demonstrated used to demonstrate and interpret mass spectrum on organic chemical compounds. 
inIn nineteen sixty eight1968 there was a very significant paper by minsky Minsky and Ppapert which demonstrated the limits of simple neural net.s 
thisThis paper had a tremendous negative effect in discouraging the field of neural network for the time being. and And later of course people realized that there are ways of coming out of this problem. 
inIn nineteen sixty nine1969 the SRI robot, shakey Shakey developed at s r i in Stanford demonstrated locomotion perception and problem solving. 
(refer slide time [53:40])

 

In 1969nineteen sixty nine to 79seventy nine knowledge based systems were developed. i will skip theseIn 
in nineteen seventy six1976 dougDoug lLenat handled the program called a mAM and heurisko Heurisko which demonstrated the discovery model. 

[54:05] in nineteen seventy eight1978 herbHerbert simonSimon from c m uCMU won the nobelNobel prizePrize in economics Economics for his theory for his theory of bounded rationality. 
(refer slide time [54:12])

 

inIn nineteen eighty1980 lisp machines were developed and marketed. 
inIn nineteen eighty five1985 to 95ninety five neural networks returned to popularity. during the period from eighty five to ninety five 
inIn nineteen eighty eight1988 there was a resurgence of probabilistic and decision theoretic methods. 
(refer slide time [54:32])

 

earlEarliery a iAI systems used very general systems of little knowledge but recent a iAI systems use specialized knowledge to perform specific tasks. 
(refer slide time [54:40])

 

inIn nineteen ninetie1990s there have been major advances in all areas of a iAI including machine learning, intelligent tutoring, multi agent planning, uncertain reasoning, natural language understanding, translation, vision and other topics. 

[55:05] Rodney bBrooks worked on the cog project at m i MITt which made significant progress in building a humanoid robot robot.
(refer slide time [55:10])

 

weWe have already looked at the deep blue chess playing program and we have interactive robot pets which have become commercially available realizing the vision of the eighteenth 18th century toy makers. 
inIn two thousand2000 the nomad robot explored remote regions of antarticaAntarctica and a iAI is a popular topic which is constantly in the news. 
(refer slide time [55:38])

  

so So this is the triple AI site which publishes news in the triple a i site which publishes news about a iAI and if you visit the site you will find that at any time there areis a lot of interesting news on a iAI. 
(refer slide time [56:00])

 

[56:05] so wWith this we will end today’s lecture and before we end we have a few questions. 
(refer slide time [56:16])

 

questionQuestion one1 is, define intelligence. 
questionQuestion two2, what What are the different approaches in defining artificial intelligence Artificial Intelligence. 
questionQuestion three3, suppose Suppose you design a machine to pass the turingTuring test
what are the capabilities such a machine must have? 
questionQuestion four 4 is, design ten questions to pose to a man or a machine that i’s taking a turingTuring test. 
questionQuestion five 5 is, will building an  a iAI computer automatically shed light on the nature of natural intelligence, 
 do you think so? 
(refer slide time [57:01])

 

questionQuestion six 6 is, list five tasks that you will like a computer to be able to do within the next five years.
theThe last question, question seven 7, list five tasks that computers are unlikely to be able to do in the next ten years. 
withWith this we come to they end of today’s lecture, 
 thank you.  
Artificial Intelligence (Lecture – 2)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

Intelligent agents

[00:49] Today we will start our second lecture of artificial intelligence. 
In the first lecture we introduced certain things namely what artificial intelligence is all about, the definition of intelligence and we looked at several examples of AI systems and we also traced the history of AI. 

[01:16] Today we will go to the second part of this introduction where we will talk about intelligent agents. 
In a major part of this course we will talk about the various aspects of an intelligent agent. Today we will introduce what an intelligent is and how we look at intelligent agents. 
Now to describe the instructional objective of today’s lecture we would like to define an intelligent agent as to what we mean by intelligence as an agent, 
define a rational agent. 

[02:00] In the last class we talked about rationality. 
We will talk about what we mean by a rational agent. 
Then we will explain the concept of rationality as we see or the concept of bounded rationality which we will deal with. 
We will discuss the different characteristics of the environment in which the agent operates and we will explain different agent architectures. 
(refer slide time [02:31])
   

On completing this lesson you will be able to understand what an agent is and how an agent interacts with the environment.
As we will see in the course of this lecture the environment is an important component of the agent design and the agent should be designed so that it can work properly in its environment. 

[03:09] After you have taken today’s lecture you should be able to do the following: 
When you are given a problem situation, you should be able to identify which are the percepts to the available to the agent or what the agent can sense and how the agent should act to optimize its performance. 
We should also look at what are the performance measures by which we should evaluate an agent to try to see if the agent design has been successful and we hope to understand what we mean by the definition of what a rational agent is.  
(refer slide time [03:48])

  

[04:01] We will also look at the concept of bounded rationality which is the rationality that we will deal with. 
In summary we will be familiar with different agent architectures including stimulus response agents, state based agents, deliberative or goal directed agents, utility based agents as well as learning agents. 
(refer slide time [04:28])

 

And we should also we able to identify given a problem situation the characteristics of the environment and recommend what the architecture of the desired agent should be in this environment. 
(refer slide time [04:48])
 

So this is our agent. 
The agent operates in an environment. 

[05:03] 
The agent receives percepts from the environment and the agent acts and its actions can change the environment. 
The agent uses its various sensory organs so depending upon the sensors that the agent has for example the agent may be able to see if the agent has a camera, the agent may be able to hear if it has a sonar sensor and so agent can see or hear or accept different inputs from the environment. 
Inside the agent there is an agent program which decides on the basis of the current percept or the percept sequence it has received till date to decide what should be the good action to take in the current situation. 

[06:12] So the agent has actuators or effectors to take actions. 
These actions can potentially change the environment and the agent can use its sensors to sense the changed environment. 
(refer slide time [06:39])
  

The agent does the following things: 
The agent operates in an environment. 
The agent perceives its environment through its sensors. 

[07:03] The agent acts upon the environment through actuators or effectors 	and also the agent has goals. 
(refer slide time [07:10])

  

Goals are the objectives which the agent has to satisfy and the actions that the agent will take will depend upon the goal it wants to achieve. 
What is a percept? 
The complete set of inputs at a given time the agent gets is called its percept. 
The input can be from the keyboard or through its various sensors. 
The sequence of percepts may be the current percept or may be all the percepts that the agent has perceived so far can influence the actions of an agent. 
The agent can change the environment through effectors or actuators. 

[08:14] An operation which involves an actuator is called an action. 
So, agent can take action in an environment through the output device or through the different actuators that it might be having. 
These actions can be grouped into action sequences.
(refer slide time [08:35])

 

We have already seen that the agent has sensors and actuators and has goals. And the agent program implements a mapping from percept sequences to actions. 
 (refer slide time [09:06])
  


[09:13] We also have to talk about a performance measure by which we will evaluate an agent, evaluate how successful the agent design has been. And finally we will like to talk about autonomous agents. 
(refer slide time [09:25])

 

In artificial intelligence artificial intelligent agent’s autonomy is extremely important.
The agent should be able to decide autonomously which action it should take in the current situation. 
So an autonomous agent decides autonomously what action to take in its current situation in order to maximize its progress towards its goals. 

[10:03] So, if the agent has a goal the agent should try to maximize its goal too so that its performance measure is at maximum. 
The behavior and performance of intelligent agents we will look at in terms of the agent function. 
An agent function as we have already talked about is a mapping from perception history to action. 
Now what is the mapping that the agent should implement? 
 Obviously the mapping the agent should implement is the one which maximizes its performance measure. 
The ideal mapping of an agent specifies which action an agent should take at any point of time. 

[11:02] We will talk presently about how the agent should achieve this maximization. 
The performance measure is in fact a subjective measure which characterizes how successful an agent is. And how the performance measure is characterized can vary.  
(refer slide time [11:15])

  

The performance measure could be the amount of how rich the agent will become if the agent behaves in a particular way or how quickly the problem can be solved or how precise or how good the solution is, what is the quality of the solution that the agent has been able to achieve, the amount of power the agent’s objective may be to minimize the amount of power consumed or the performance measure could be a combinations of several of these factors. 

[12:05] Actually an agent could be anything. 
We can look upon a human being as an agent, we can look upon a calculator as an agent. 
So, in order to characterize something as an agent we have to look at the different characteristics in terms of the different mappings the agent performs such as its percepts, its actions, the environment it operates, the entire characteristics of the environment and certain other things. 
So let us look at some common things that we are familiar with and look at what are the characteristics associated with such agents. 
We are all familiar agents, so what are our sensory organs? 
We can see with our eyes, we can hear with our ears, we can smell, we can touch, we can taste so these are the five primary input mechanisms we possess. 

[13:06] What are the actuators we possess? 
We have our hands with which we can take some action, 
we have our fingers, we have our legs and several other things including the mouth with which we can take our actions. 
Now, if we design a robot what are the percepts, what are the different sensory organs we can give to the robot that we can build into a robot? 
We can have a camera with a robot, a camera with which it can take pictures of what is in front of the robot and the robot function or the robot program can analyze that image and find out salient characteristics of the environment which will decide the way it should act. 
Then the robot can use other types of sensors like sonar sensors, infra red sensors etc.
And the type of actuators we can have or we can build into a robot are wheels, speakers, lights, grippers and any other output device that we may want to. And then we can look at a software agent.

[14:26] The software agent is becoming increasingly common now-a-days. 
So many people call them softbots. 
These softbots have some functions as sensors. 
They compute a function of the input. 
So they have some functions as sensors and some functions as actuators. 
We will later look at some specific software agents to look at the sort of function which they take as the input and the sort of function they compute to give the output. 
(refer slide time [14:58])

    

[15:00] 
This is a picture of a robot. This is the Xavier robot developed at CMU it was one of the earlier robots that was developed at CMU. 
(refer slide time [15:02])

 

Then this is a robot called the Cog robot which was developed by Rodney Brooks group at MIT. 
(refer slide time [15:15])

 

We will have a occasion to talk more about Rodney Brooks’ activity in the course of today’s lecture. 
So the basic motivation behind creating Cog is the hypothesis that they wanted to build a humanoid robot a robot having intelligence like a human being. 
So this picture shows 

[16:04] professor Rodney Brooks with the robot Cog. 
 (refer slide time [16:14])

  

And thirdly we have this entertainment robot. 
This Aibo robot is sold by Sony. 
This robot is claimed to be autonomous, sensitive to its environment, 
it can learn, it has different stages of its life and it can be personalized according to the environment in which it grows up in.
(refer slide time [16:43])

 

Then there are other types of agents like softbots or software agents. 
For example, there are sites like askjeeves dot com which you can consider as an example of a software agent. 

[17:02] There are various expert systems including the medical expert systems for example, the cardiologist which are also software agents. 
Other than software agents we have other types of robots for example, the autonomous spacecrafts like the Mars Rover and then there are other types of robots like intelligent buildings which have intelligence built into them to decide the lighting condition, air conditioning etc. 
(refer slide time [17:33])

 

So an intelligent agent must have certain fundamental faculties.
It should be able to act. 
An intelligent agent must act.
The intelligent agent must sense its environment. An agent must sense its environment. 

[18:02] If the agent acts without sensing that is a blind action and blind action cannot be intelligent. 
So there are certain types of architectures where the agents can sense and act and the agents do not do, do not perform any deeper deliberative action. 
In fact robotics is about sensing and acting only. 
At the outside we only require the agent to act appropriately. 
Understanding is not necessary. 
Understanding may be important for choosing proper actions but understanding by itself may not be necessary. 
However we must realize that sensing really needs understanding to be useful. 
So an intelligent agent who possesses complete intelligence must be able to do the following:  

[19:05] It must be able to understand or interpret what it senses, it should be able to reason and finally the agent should also be able to learn so that the agent can operate in an unknown environment. 
In fact learning is a prerequisite for the agent to be autonomous. 
An autonomous agent which can adjust to a changing environment must have some learning component. 
(refer slide time [19:36])

 

Also, intelligent agents must be rational. 
(refer slide time [19:45])

 

In artificial intelligence we will talk about building rational agents and we will talk about different aspects the rational agent has and look at the [ ….  ]
Now, what is a rational agent? 

[20:13] A rational agent is an agent which always does the right thing. 
Now what is the right thing? 
So in order to understand that we must understand what are the functionalities or the goals of the agent. 
What are the components of the agent and we must look at how we should build these agents. 
(refer slide time [20:35])

 

Perfect rationality assumes that the rational agent knows everything and will take that action which maximizes her utility.
So, perfect rationality is prevalent to demanding that the agent is omniscient or all knowing. 

[21:02] If the agent knows everything and the agent can reason extremely fast then the agent is a perfect rational agent. 
However, a perfect rationality is something which is not within the scope of even human beings. 
We do not satisfy the definition of perfect rationality. 
We are not omniscient; there are many things which we do not know, 
there are many things which we cannot reason in a reasonable time frame given what we know. 
The concept of bounded rationality was introduced by Herbert Simon of CMU in 1972 in his theory of Economics.  
So, bounded rationality says that because of the limitations of the human mind humans must use approximate methods to handle many tasks.
(refer slide time [22:08])

 

So bounded rationality does not aim at maximizing the absolute utility of the agent because that is something which may not be achievable given a realistic agent given realistic resource path.
So, instead rationality as we will look at we will concentrate on using approximate methods where appropriate so that the agent can take the best action given its resource limitation and given what it already knows.

[23:00] Rational action is the action that maximizes the expected value of the performance given the percept sequence to date. 
When an agent takes an action the action may not always have the same outcome. 
Sometimes the outcome of the action may be good, sometimes it may be bad. 
So when we talk about rationality we must look at the expected value of the performance measure and not always the absolute value of the performance measure. 
Also we must evaluate an agent not on the basis of how it should behave in a perfect case but how it can behave based on what it can sense. 
So something that the agent cannot sense or foresee we cannot hold the agent responsible for those things. 

[24:00] Therefore rational action in the light of bounded rationality talks about maximizing the expected value of the performance measure given the percept sequence which is available to the agent. 
Therefore does rational action means the best action? 
The answer is yes but to the best of the agent’s knowledge based on what percepts the agent already has access to. 
Does rational mean optimum? 
Yes, but to the best of the abilities of the agent and subject to the resource constraints. The agent may have limitations in the way it can act. 
The agent may have limitations in terms of the resources that it has access to. 
There could be time limitations, there could be space limitations. 

[25:01] 
So, given these constraints given its abilities the agent should take the best action that is expected to maximize its utility. 
That is the rationality we will talk about. 
(refer slide time [25:15])

 

We must understand that a rational agent need not be omniscient because it does not know the actual outcome of its actions. And certain aspects of the environment may be unknown to the agent. 
So rationality takes into account the limitations of the agent, the percept sequence that it has access to, the background knowledge the agent has and the actions that the agent can take. And we only deal with the expected outcome of actions. 
(refer slide time [25:50])

 

As we have already talked about, in 1957 Herbert Simon proposed the notion of bounded rationality. 

[26:02] Formally if we define bounded rationality, It is that property of an agent that behaves in a manner that is nearly optimal with respect to its goal as its resources will allow as nearly optimal as its resources will allow. 
(refer slide time [26:22])

 

Now we come to another very important component of an agent. 
Actually the design of the agent depends a lot on the environment in which the agent operates. And the task environment of the agent can be defined in different ways. 
So we will look at several ways of characterizing the environment. There are two ways of characterizing an environment. 
One way of characterizing the environment is in the absolute sense and another is from the point of view of the agent. 

[27:01] For example, when we want to talk about whether an environment is deterministic or stochastic we must not look at whether the environment is deterministic from an absolute point of view but rather whether the agent appears, whether the environment appears deterministic to the agent based on what it can perceive. 
(refer slide time [27:34])

 

Environments can be divided into two types based on the observability of the environment. 
An environment may be fully observable or it may be partially observable. 
If the environment is fully observable the entire environment which is relevant to the action being considered is observable. 
All the relevant portions of the environment are observable.

[28:14] On the other hand, in the case of partially observable environments not all the relevant portions may be observed. 
The relevant features of the environment are only partially observable. 
For example, consider an agent playing chess like the deep blue chess playing program.
The agent has complete knowledge of the board. 
So everything about the environment is accessible to the agent. 
So the chess environment is a fully observable environment. 
Consider again the environment where the agent is playing pokers where the agent cannot see the hand of the opponent. 

[29:02] So the environment in this case is only partially observable to the agent. 
So, if the environment is fully observable the agent need not keep track of how the environment is changing and deliberating about the environment. 
So, if the environment is partially observable the agent in order to behave successfully in such an environment has an added task of keeping track of the environment and reasoning about the properties of the environment. 
Then, if you look at the aspect of determinism again environments can be divided into two or three types. 
(refer slide time [29:53])

 

Deterministic environments: In deterministic environments the next state of the environment is completely described by the current state and the agent’s action. 
When we looked at diagram of agent environment we have already seen that the agent’s action changes the environment. 
Now in a deterministic environment given the agent’s action how the environment change is deterministic. 
There is no other unknown thing which comes into the picture to decide how the environment changes. 
In stochastic environments on the other hand there is some element of uncertainty. 
Therefore how the environment changes depends not just on the action that the agent takes. 
So, whether an environment is deterministic or stochastic depends on how you look at the environment. 

[31:11] If you cannot observe the environment fully the environment may appear stochastic to you whereas it is actually deterministic if you have access to the entire environment.
For example, take the example of pokers again. 
If the agent did have access to both the players’ hands and all the cards which are there then the environment actually behaves in a deterministic way. 
However when it is partially observable it appears to be a stochastic environment. 
So this is an environment which appears to be stochastic because of partial observability. 
If you look at the game of Ludo it is a stochastic environment because how the environment evolves depend on the value which the die has rolled. 

[32:13] So whether the die has rolled 1 or 2 or 6 depending on that the agent can decide its actions. Therefore this is a truly stochastic environment. 
A strategic environment is an environment state which is wholly determined by the preceding state and the actions of multiple agents. 
In a strategic environment apart from the current agent itself there are other agents around and the actions of all these agents influence the environment. 
A strategic environment is one where the environment is only changed by the actions of the agent itself and other agents. 

[33:00] For example, a chess playing agent operates in a strategic environment.
(refer slide time [33:18])

    

Then if we look at the episodicity of the environment an environment may be episodic or sequential. 
An episodic environment is one where subsequent episodes do not depend on what actions occurred in previous episodes. 
First of all sometimes certain tasks can be divided into different phases or different episodes. 
In such episodic environments the one episode may or may not influence the subsequent episode. 

[34:00] So, if one episode does not influence the subsequent episode such an environment is called an episodic environment. 
Therefore in such an environment an agent did not plan beyond episodes. 
In a sequential environment on the other hand the agent engages in a series of connected episodes. 
So subsequent episodes are dependent on what happened in the previous episodes. 
So in this case the agent may need to plan ahead or base its action upon what happened in the previous episodes. 
(refer slide time [34:46])

  

Then if you look at another characteristic of the environment is its dynamism. 
Environments may be static or dynamic. 
A static environment does not change by itself.

[35:04] So an advantage is that a static environment does not change when the agent is deliberating. 
The agent does not need to observe the world during its thinking process. 
A dynamic environment changes by itself that is apart from the action that the agent takes. 
(refer slide time [35:29])
 

Also, another dimension by which we can characterize environments is by its continuity. 
An environment is discrete or continuous. 
An environment is discrete if the number of distinct percepts and actions are limited and the number of states is limited. 
If the number of states is discrete then that environment is a discrete environment. And the environment is a continuous environment if the range of percepts is continuous or the range of actions that it can take is many or the number of states is either continuous or too many so that we treat them as continuous. 
(refer slide time [36:21])

 

(refer slide time [36:24])

 
And finally we can also characterize the environment according to whether there is one agent whom we are talking about in the environment or whether there are other multiple regions in the environment. 
That is, if the environment contains other agents then it is a multi agent environment. 
In games when we look at two person games we usually talk about one opponent agent. 

[37:00] There are environments where there is a single agent but for many tasks that the agent does usually we talk about a single agent environment but there are many situations which require distributed agents or with social and economic systems we deal with multiple agent systems. 
(refer slide time [37:39])
	
 

Therefore the complexity of the environment includes knowledge rich environments and input rich environments.
In knowledge rich environments there is enormous amount of information the environment contains. 
Knowledge rich environments contain lots of information. And input rich environment is one where there is enormous amount of input, enormous amount of percept that the agent can get. And in such environments which are complex which are either knowledge rich or which are percept rich the agent must have a way of managing this complexity. 
So, such considerations in the environment are complex. 
The agent must develop it strategies for sensing or its strategies of attentional mechanisms.  
The agent must decide selectively what to sense, what is more relevant and what it should give its attention to rather than deal with the entire complexity of the full environment. 
So an agent needs to focus its effort in such rich environments. 
(refer slide time [38:57])

  

Now we will look at the different types of agent architectures. 

[39:05] Table based agents, stimulus response agents, goal based agents, and utility based agents and learning agents. 
We will briefly talk about these agent architectures. 
A table base agent is a very simple agent. An agent has to take actions given its sense given what it senses, given its percepts. 
Now, in a table based agent the mapping from percepts to actions is stored in the form of a table. 
So on the left side we have the percepts and on the right side we have actions. 

[40:00] And the agent program is extremely simple. 
So given the percept the agent looks at the table and decides what action to take. 
Therefore based on this mechanism we can develop what we call reactive agents which can take the action depending on the percept. 
Unfortunately the best action for the agent it depends not only on the current percept but on the percept sequence. 
Now the number of possible percept sequence can be very large. 
In fact it can be infinite till the agent acts over many time steps, many unbounded time steps then it can become infinite. 
So this table can become very large if you have a mapping from percept sequence to action. 

[41:00] In simple tasks where the action only depends on the current percept it may be easy to develop a table based agent but it is infeasible when the correct action or the best action depends on the past and not only what the agent sees or perceives at the current time step. 
(refer slide time [41:24])

   

(refer slide time [41:34])

  

So, a table is a very simple way to specify a mapping from percepts to actions but tables may become very large. And in a table based agent there is no intelligence in the agent itself and all the work is done by the designer in designing the table. 
So the designer makes the table and the agent just looks it up to decide how to act. 

[42:03] So such agents have no autonomy. 
All actions, all behaviors are predetermined and there is really no concept of learning. 
Now the mapping from percept to action may be done in different ways. 
It may be in terms of a natural table, it may be in terms of a production system or rules. 
This mapping can also be implemented by neural networks or one can implement the mapping by algorithm. 
So, this mapping can be implemented in a rule based manner using a neural network or using some algorithms. 
(refer slide time [42:48])

 

So such agents where the action depends only on the percepts and there is no deliberation involved are called reactive agents. 

[43:08] The term reactive agents means those agents whose information come from sensors and they can take actions through their actuators or effectors. And we can additionally assume that the actions change the current state of the world and the agent can only take some action without any deliberation. 
Such agents are also called stimulus response agents. 	
They have no notion of history but all their history is encoded in the current state as the sensors see it right now. 
(refer slide time [43:56])

  

Now let us talk about professor Rodney Brooks of MIT and the subsumption architecture he proposed based on stimulus response agents. 

[44:11] In 1986 professor Rodney Brooks gave this notion of the subsumption architecture. 
His argument was that lower animals behave in a largely reactive manner. 
They have a very little sense of deliberation. 
Therefore most of their actions are reactive actions. And his argument was that in the time scale of evolution reactive behavior came much earlier than deliberative behavior. And he has been able to show that a number of such components having simple reactive behavior can achieve a surprising degree of intelligence. 

[45:09] So Brooks’ idea has been to follow the evolutionary path how life has evolved and build simple agents for complex worlds. It is a combination of simple agents working well in the complex world. 
The features of the subsumption architecture are that there is no explicit knowledge representation. 
Behavior is not centrally controlled but behavior is distributed among different components. 
The response to stimulus is reflexive and more importantly the design is bottom up. 
The complex behaviors are fashioned from the combinations of simple behaviors. 
Simple behaviors are put together to achieve complex behaviors. And each individual agent is simple and inexpensive. 

[46:13] Therefore the subsumption architecture is basically a layered architecture. 
(refer slide time [46:18])
   

According to professor Brooks the time scale for evolution has been 5bn years. 
It has take 5bn years to evolve from single cells since cells evolved to the present day.
The first humans appeared two and a half million years ago only whereas the first cells appeared 5bn years ago and symbols did not appear until only 5000 years ago. Therefore his proposition is that we should look at simpler behavior as the first step towards building intelligent agents. 

[47:11] So, in subsumption architecture there are different layers of behavior and higher layers override lower layers and each activity in each layer consists of a simple finite step machine. 
Now, this is an example of a simple architecture proposed by Rodney Brooks. 
In this system there are several layers. 
This is a very simple robot. 
In the layer zero there is the avoid obstacles layer. 
In this layer there are several sensory organs. 
The sonar component generates sonar scan, 

[48:01] the collide component sends halt message to forward and the feel force component is the signal sent to runaway or turn around. 
So layer zero of the agent is to avoid obstacles. 
This is the first layer of the agent which enables the agent to navigate in an environment without colliding with other agents. 
So it is a very simple behavior of a navigating robot without using a lot of deliberation. 
In the second layer, layer 1 there is the wander behavior. 
This wander behavior generates a random heading. 
It allows the robot to occasionally wander. 
So in the wander layer there are two components. 

[49:00] One generates a random heading and secondly the avoid component reads repulsive force and generates new heading and feeds that to turn and to forward. 
(refer slide time [49:17])

 

Layer 2 that is the third layer has the exploration behavior. 
So this has several components. 
The whenlook component notices whenever there is idle time it looks for an interesting place. 
The path plan component sends new direction to avoid. 
The integrate component monitors path and sends them to the path plan. 
(refer slide time [49:53])

 

So as we have seen this percept based agents are efficient because they do not require internal representation for reasoning or inference. 

[50:06] However there is no strategic planning or learning and they are not good for multiple opposing goals. 
(refer slide time [50:13])

 

In the second type of agent the more knowledge rich agents we come to state based agents. 
In state based agents information comes from sensors and it changes internally the agent’s current view of the world based on the state of the world and the knowledge the agent has. 
It triggers action through the effectors. And in order to do this the agent does some deliberation. 
(refer slide time [50:51])

  

So based on the state of the world and knowledge the agent chooses the action and carries out the action through the effectors. 
(refer slide time [51:00])

 

[51:00] Now, in goal based agent the agent’s action depends upon its goals and goal formulation is based on the current situation. 
So based on the goal that the agent has to achieve and the current state of the agent as it perceives the agent goes through some deliberations to decide what the next action should be. 
We will look at search and planning. 
These are the two fields of AI in which we will look at different ways of deciding what action to take in order to achieve the agent’s goals. 
So in goal based agents the sequence of steps required to solve a problem is not known a priori. 
They are not available in a table and they must be determined by a systematic exploration of the alternative actions by deliberations. 
(refer slide time [52:05])

 

[52:05] Thirdly we have utility based agents. 
Utility based agents are akin to goal based agent but they are a more general frame work. 
If an agent has several goals and it can achieve only some of them these goals may have preferences associated with them as to which goals are more preferable.
In this case we can talk about utility based agents where we have different preferences for different goals. 
A utility function is a very general function that maps a state or a sequence of states to a real valued utility. 
So a state is mapped to a utility value and we can say that the goal of the agent is to maximize its utility or the goal of the agent is to maximize its expected utility. 

[53:00] When we look at the decision theory we can consider agents which are based on utility. And finally and very importantly we have learning agents.
(refer slide time [53:08])

 

We have said that learning is an extremely important component of autonomy. 
An autonomous agent should have some learning built into it. 
Learning allows an agent to operate in unknown environments. And the learning element modifies the performance element and learning is required for true autonomy. 
We will take up learning later in this course when we look at several types of learning that can be carried on by the agents. 
(refer slide time [53:51])

 

In summary of today’s lecture an agent perceives and acts in an environment, 
it has an architecture and an agent is implemented by an agent program. 

[54:02] An ideal agent chooses the action which maximizes its expected performance. 
An autonomous agent uses its own experience rather than built-in knowledge of the environment by the designer. 
(refer slide time [54:16])

  

An agent program maps from percept to action and updates its internal state. 	
Reflex agents respond immediately to percepts. Goal based agents act in order to achieve their goals. 
Utility based agents maximize their own utility function. And then in order to have goal based agents the agent must represent its knowledge, must represent its history, must have some background knowledge etc.
So representing knowledge is important for successful agent design. 
(refer slide time [54:59])

   

[55:00] So now we will come to a set of questions based on today’s lecture. 
Question 1, define an agent. 
Question 2, what is a rational agent 
Question 3, what is bounded rationality 
Question 4, what is an autonomous agent 
Question 5, describe the salient features of an agent 
Question 6, 
(refer slide time [55:31])

 

find out about the mars rover. 
What are the percepts for this agent? 
Characterize the operating environment for this agent. 
What are the actions that this agent can take? 
How can one evaluate the performance of this agent? 
What sort of agent architecture do you think is most suitable for this agent? 
Question 7, answer the same questions above for an internet shopping agent. 

Artificial Intelligence (Lecture – 3)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

State Space Search

[00:45] Ttoday we will start theour third lecture of our course on artificial intelligence. 
inIn the last two days we talked of thediscussed about the first module which was an introduction. 
todayToday we will start on the second module and the first lecture is on state State space Space searchSearch. 
(refer slide time [01:04])
 

[01:03] Bbefore iI continue on to this module iI would like to discuss the questions from the previous module. 
soSo let us go back to the questions of lecture one 1. 
theThe first question was, define intelligence. 
the answer intelligenceIntelligence is actually a rather hard to define term. 
It i’s often defined in terms of what we understand as intelligence in humans. 
allenAllen newellNewell has defined intelligence as the ability to bring all the knowledge of a system to bear on the solution of the a problem. 
aA more practical definition has been given in the context of building artificial systems. 
intelligenceIntelligence is the ability to perform better on tasks that humans currently do better. 
soSo that was question number one 1. 
(refer slide time [01:55])

 

questionQuestion number two 2 of the first lecture was, what are the different approaches in defining a iAI? and thereThere are four approaches. 
thinkingThinking rationally approach 
the actingActing rationally approach 
the tThinking like a human approach and the 
actingActing like a human approach 

[02:15] Tthe third question was, suppose you design a machine to pass the turingTuring test 
 what are the capabilities such a machine must have? 
(refer slide time [02:27])

 

theThe answer is, the machine must be capable of natural language processing in order to understand the questions that the interrogator is posing to it. 
secondlySecondly the machine must have capability for knowledge representation to represent not only the prior knowledge about the world but also the facts given to the machine by the interrogator. 
thirdlyThirdly the machine should be should have the capability for automated reasoning so that it can do inferences based on the facts that it knows. 

[03:01] Ffinally the machine must have capabilities of machine learning so that it can learn and adapt to new situations. 
alsoAlso, in order to pass the so called complete turingTuring test or to really act like a human the machine would also have to have expertise in computer vision as well as robotics. 
theThe other four questions from the first lecture are: 
(refer slide time [03:36])

 

designDesign ten questions to pose to a machine or a man taking the turingTuring test. 
It is the i will leave it to you to design appropriate questions 
same for the next three questions. 
willWill building an artificially intelligent computer automatically shed light on the nature of natural intelligence? 
well iIn a sense yes, because people who proposed the thinking humanly approach to intelligence believe that if we can have a deep understanding of how humans’ reason we can build machines like that and the vice versa we can gain some insights. but i leave it to you to give your answers to the question 

[04:22] same It is the same for the next two questions. 
 now let’s go to lecture two 
(refer slide time [04:27])

 


Let us go to lecture 2. theThe first questions was, define an agent. 
theThe answer is straight forward. 
anAn agent is anything that can be viewed as perceiving its environment through sensors and executing actions using actuators actuator. 
theThe second question waswas what is a rational agent? 
aA rational agent is one who selects an action based on the percept sequence that it has received so far so as to maximize the expected performance measure given the percepts it has received and the knowledge possessed by it. 

[05:12] so So within the bounds of what the agent knows and what are the percepts that it can receive the agent should behave so as to maximize its expected performance measure. 
thirdThird question is what is bounded rationality? 
(refer slide time [05:36])
  

aA rational agent that can use only bounded resources cannot exhibit the optimum behavior. 
aA bounded rational agent does the best possible job of selecting good actions given its goal and its bounded resources. 
soSo if the agent has only limited time in its disposal and limited memory or other resources the agent should do the best that it can under the circumstances. 

[06:10] T that is the definition of bounded rationality. 
fourthFourth question is what is an autonomous agent? 
so autonomousAutonomous agents are those agents that are capable of independent action in environments which are dynamic and unpredictable. 
anAn autonomous agent needs to learn and adapt to a new environment. 
theThe fifth question is, describe the salient features of an agent. 
(refer slide time [06:43])

 

anAn agent perceives its environment using sensors. 
anAn agent takes actions using actuators. 
a A rational agent acts so as to reach its goal or to maximize its utility. 

[07:03] reactive Reactive agents decide their action on the basis of their current state and percepts. 
deliberativeDeliberative agents reason about their goals to decide their action. 
questionQuestion number six, 
(refer slide time [07:15])

  

i asked you to find out about the mars Mars rrover. 
so theseThese are the questions that we want to answer about the rover. 
a a)was Wwhat are the percepts of this agent agent?
bB) characterizeCharacterize the operating environment. 
c) whatWhat are the actions the agent can take? 
d) howHow one can evaluate the performance of the agent? and
 e) what What sort of agent do you think is most suitable for this agentagent? 
nowNow let’ us look at the answers to these questions one by one. 
(refer slide time [07:49])

 

Firstly, about the percepts that the agent receives we looked at the website of the agent spirit. 

[08:00] spirit Spirit is one of the mars rovers that we talked about. 
spirit’sSpirit’s sensor include: panoramic and microscopic cameras, a radio receiver and various spectrometers for studying rock samples including an alpha particle x ray spectrometer, mossbauermoss Bauer spectrometer and miniature thermal emission spectrometer. 
(refer slide time [08:26])

 

theThe operating environment of this rover it was operatingis supposed to operate inbe the martianMartian surface. 
thisThis environment is partially observable. 
thisThis is in fact a very realistic environment. 
thisThis is a partially observable environment. 
theThe environment is non- deterministic. 
It i’s sequential, that i’s, it is not episodic episodic.
theThe environment is dynamic. 
It i’s continuous and not discrete and we can look upon it as a single agent environment. 

[09:00] Hhowever, if we are modeling other agents or the interaction of this rover with the mother ship then we can look upon this as a multi agent environment but otherwise this can be modeled as a single agent environment. 
(refer slide time [09:27])

 

whatWhat are the actions that this agent can taketake? 
Now, this rover spirit has the following actuators. 
itIt has motor driven wheels for locomotion. 
itIt has a robotic arm with which it can bring sensors close to interesting rocks. 
itIt has a rock Rock abrasion Abrasion tTool or RAT which is capable of efficient drilling holes in hard volcanic rocks. 
spiritSpirit also has a radio transmitter for communication. 

[10:01] tThe final question was or rather question number d) was, how can one evaluate the performance of his agent? 
nowNow let u’s look at the possible performance measures that we can design for this agent agent.
(refer slide time [10:20])

 

aA mars Mars rover may have the task of maximizing the distance that it has traversed or the exploration the diversity of the exploration that it has carried out or the variety of terrain that it has traversed. 
itsIts performance could be measured by seeing how much variety of samples it has been able to collect or whether it has been successful in finding water or finding life. 
weWe already know that the mars Mars rovers’ wereas successful in detecting traces of water in mars. 	
thereThere could be other criteria such as maximizing the lifetime or minimizing the power consumption. and And there are other goals like, if it crashes or runs out of power it cannot explore. 
so it should-, so 
Therefore there are various ways in which we can define the performance of this agent agent.
theThe final question on the mars Mars rover was what sort of agent architecture do youyou think is most suitable for this agent? 
(refer slide time [11:30])

 

inIn my mind a hybrid architecture which has the components of both   stimulus response andor reactive type of architecture as well as deliberative architecture would be ideal. 
Ffor example, iI would visualize a model based reflex agent for low level navigation and a goal based or utility based agent for other tasks like route planning, experimentation etc.etera andAnd the next question was the same set of questions about an internet shopping agent. and letLet u’s look at some of the answers.
(refer slide time [12:17])

  

[12:19] Ffor the internet book shopping agent the sensors would be the ability to parse web pages or interface for user requests. 
gettingGetting requests from users and being able to parse web pages 
those would be looked upon as sensors for internet shopping agent. 
(refer slide time [12:43])

 

theThe environment it acts in is the internet.  
thisThis environment is partially observable. 
It i’s partly deterministic, sequential sequential,
 it i’s static but not quite because the web is changing. 
theThe environment is discrete and it’ is usually a single agent unless we are trying to model auctions and other such things in which case we go for a multi agent environment. 

[13:12] The tthird question was what are the actuators of this agent? 
theThe actuators ithat’s the action that this agent can take is the agent can follow links, 
it can fill in forms, 
 it can give information to the user. These 
so these are the actions that the agent can take. 
d) we We wanted to look at the performance measure of the agent. 
thisThis agent’s its  performance is whether it has been able to obtain the requested items or books, whether it can minimize the cost of the books or time to fetch the books and so on. Aand the agent architecture that we visualize is we have a goal based agent with utilities for open ended situations. 

[14:01] so it canTherefore  in situations which are not very well specified we can have a utility based agent but mainly we can have a goal based agent which has a particular goal in mind for which it has to minimize the cost and the time. 
nowNow we come to our second module. 
thisThis module is called problem solving using search. 
the first lecture today weWe will introduce the concept of state State space Space search Search in the first lecture today. 
(refer slide time [14:39])
    

instructionalInstructional objectives for this module are as follows: 
in search ourOur objective is that the student should be familiar with various search paradigms which we will deal with these various search paradigms likelike depth first search, breadth first search, uniform cost search, iterative deepening search and bidirectional search etc which we will deal with in the subsequent lectures. in subsequent lectures 

[15:00] Iin today’s lecture we will look at the basic formulation of search problems as state State space Space searchSearch. 
(refer slide time [15:15])

 

atAt the end of today’s lecture the student should understand the state space representation and gain familiarity with some common problems which can be formulated as state State space Space search Search problems. 
theThe second objective is, after the student has taken this course taken this has learnt this material he should have this following capability. 
givenGiven a problem description he should be able to formulate it in terms of a state State space Space search Search problem. 
He should be able to do the state space decomposition of the problem, decide how a state should be represented, how state change takes place model actions and formulate the state State space Space search Search problem. 

[16:08] tThirdly the student should understand how implicit state spaces are described and how implicit state spaces are unfolded during search by making the generated state explicit. 
fourthlyFourthly the student should understand how states can be represented by some features of a state. 
Now, before we come to the state space search problem let us review the definition of an intelligent agent and let us see how search is important or why search is important to an intelligent agent agent.
nowNow to recapitulate we have this agent which is working in this environment. 

[17:02] tThe agent receives the percepts from the environment which the agent visualizes and the agent takes actions using its actuators which change the state of the environment. 
(refer slide time [17:18])

  

aA goal directed agent as we have seen needs to achieve certain goals. 
many problems todayToday we will see that many problems can be represented as a set of states and and a set of rules of how one state is transformed to another. and And in these problems the agent must choose a sequence of actions in order to achieve its goal. and And we will see that such goal based agents can be modeled using state space search. 
(refer slide time [17:56])

 

eachEach state is an abstract representation of the agent’s environment. 

[18:03]
 itEach state i’s an abstraction that denotes a configuration of the agent. 
aA state denotes the current configuration of the agent. 
ItEach state i’s an abstract representation of the current configuration of the agent which is sufficient for solving the problem that the agent needs to solve. 
theThe initial state is a description of the starting configuration of the agent. 
anAn action or an operator takes an agent from one state to another state. 
so byBy taking an action the agent moves from a current state to its successor state. 
aA plan is a sequence of actions that the agent can take. 
soSo a plan is a sequence of actions and the agent starts from the initial state executes the plan that i’s takes the a sequence of actions so that it reaches a goal. 
(refer slide time [20:24])

 

aA goal is a description of a set of desirable states of the world. 
goalGoal states are often specified by a goal test. 
so weWe can say that the following are the possible goal states. oOr we can say that any state which satisfies the goal test is a goal state. 
weWe will take up examples to illustrate this. 
soSo a plan or a path as we said is a sequence of actions. and And the cost of a path is a positive number which is usually the sum of the costs of the different steps or different actions which are part of the path. 

[20:03] Nnow, in this class we are going to talk about problem formulation. 
whatWhat is problem formulation? 
problemProblem formulation means choosing a relevant set of states to consider and a feasible set of operators for moving from one state to another. 
(refer slide time [20:16])



  

soSo given a problem we want to formulate the problem in terms of a set of states and a set of operators or actions. 
searchSearch as we will see is the process of imagining sequences of operators applied to the initial state and checking with sequence reaches a goal state. 
 we will elaborate on this further 
(refer slide time [20:50])

 

nowNow let u’s look at what constitutes a search problem. 
s S is the set of states 
s is thethe complete set of states. 
ss0 zero is the initial state. 

[21:07] So ss 0zero is the member of small s zero is member of the set of states capital sS. 
so supposeSuppose this is the representation of a state space and these circles here are the different states. 
so thisThis is the set of states or capital sS. 
And sS0 zero is the initial state. 
suppose thisThis green circle here 
this is the initial state s0 zero. 
a A is the set of actions. 
a A is a mapping from s S to s S. 
anAn action takes the agent from one state to another state. 
And g is the set of final states. 
theseThese red circles are the goalld states in this case. 

[22:00] a A search problem consists of finding a sequence of actions which transforms the agent from the initial state to a goal state. 
forFor example, from this initial state the agent might have available three actions. 
ifIf heit takes action a he goes to this state, if heit takes action b he reaches this state 
and if ithe takes action c it reaches this state. 
So, from this again the agent can take another action, another action, another action and then the agent can take a sequence of actions to reach a goal state. 
so this is a state 
theseThese are the possible actions that are available from this state. 
theThe agent can take one of the available options actions from a state. 
theThe search problem is, given an initial state 
find a sequence of actions which transforms the agent to a goal state. 
(refer slide time [23:11])

    

[23:11] so So, a search problem as we saw before was represented as a directed graph. 
inIn this graph the states are represented as nodes.
theThe states are the nodes and the operators are the edges between the nodes. 
theThe operators are the edges between the nodes and the states are the nodes. 
theThe searching process is grossly the following. 
(refer slide time [23:41])

 

theThe agent checks the current state and it can execute allowable actions to move to the next state. 
supposeSuppose this is the current state and suppose from this state there are two possible operators, 

[24:00] this is a state a, 
 operation a takes the agent to the state x and operation b takes the agent to the state z, 
 so a and b are the executable operators in state a. 
nowNow the agent is at a. 
itIt takes one of the executable operators.
supposeSuppose the agent selects the action b and moves to the state z 
 the agent checks if this state is a goalld state. 
ifIf it i’s a goalld state the agent has reached a solution and 
 if it i’s not a goal state 
the new state may become the current state of the agent and this process is repeated. 
That i’s the agent finds out the possible operations that it can execute in this current state and it continues this until the goal state is reached. 
(refer slide time [24:58])
 

nowNow this is an example of a search space. 

[25:00] as you can see Tthis is the initial state here and these three are the goal states. 
theseThese other circles are the other states in the system. 
Now, the objective of the agent is to start from this initial state and reach one of these goal states. 
nowNow from this state the agent can take two possible actions to reach either this state or this state. 
fromFrom this state again the agent can take two possible actions and then these are the other nodes that the agent can reach and so on until the agent reaches a goal state. 
soSo this is the sort of process that the agent carries out. 
fromFrom here the agent has a choice of going either in this direction or this direction, 
from here you can go here or here and so ond 

[26:01] from here it can go here and here and so on.
soSo the agent can start from the initial state and explore the state space and its objective is to reach one of the goal states states.
nowNow let u’s look at another problem. 
thisThis is a pegs and disks problem which is a variation of the block problem or these various towers Hhanoi problem. 
In this particular problem we have three pegs and we have three disks red, blue and green. 	 
(refer slide time [26:40])

 

thisThis picture shows the initial configuration of the problem. 
weWe have three pegs and we have red, blue and green on peg a.
theThe following operators are allowed. 
oneOne may move the top most disk or any peg to the top most position of any other peg. 
theseThese are the only allowable operations. and And our objective is to reach this configuration where the red, blue and green or disks are in this position same position on peg b and these are the allowable operators. 
weWe will see how we can find a solution to this problem. 

[27:27] so tThis is the starting configuration. 
nowNow what iI will do is iI will outline a sequence of steps by which we can move from the starting configuration to the goal configuration and later we will see how we model the different states. 
theThe actions that we will take here is move from a to c,
 that’ is, move the top most disk of a to c. 
theThe next action is move from a to b, 
 next action is move from a to c, 
 next action is move from b to a, 

[28:00] then move from c to b, 
 then move from a to b, 
 then move from c to b and finally we have this desired goal configuration. 
nowNow the next problem that we will look at is the well known eight8 queens queens problem. 
Tthe problem constitutes the following: 
weWe have a chess board and we have eight 8 queens. andAnd according to the rules of chess a queen can attack any other piece which is situated either in the same row or the same column or on one of the diagonals. 
(refer slide time [28:36])

 

these are the positions in which ifIf we have any piece in these positions they can be attacked by the queen.
theThe eight 8 queens problem is the following: 
placePlace eight 8 queens on the chess board such that no queen is attacking any other queen. 

[29:10] Nnow how can we formulate the state space for this problem and then we will look at how to solve this problem as a state State space Space search Search problem. 
Nnow there are different ways in which we can formulate the eight 8 queens problem. 
(refer slide time [29:29])

 

So, before we look at this let us look at a typical solution of the eight 8 queens problem.s 
this is a solution thisThis is a valid solution of the eight 8 queens problem and you can verify that none of the queens are attacking each other. 
thisThis is an example of a non solution to the eight 8 queens problem. 
(refer slide time [29:47])

 

whyWhy is it not a solution because these two queens are attacking each other and may be yea these two queens are attacking each other and so on. 
soSo this is not a good solution not a correct solution to the eight 8 queens problem. 

[30:03] Nnow let’ us look at some possible formulations of the eight 8 queens problem. 
(refer slide time [30:09])

 

inIn the first formulation we can say that a state is an arrangement of zero 0 to eight 8 queens on the board. 
soSo a state is any arrangement of less than or equal to eight 8 queens on the board. 
soSo a state is specified by the position of the queens which are currently in the board. 
theThe initial state is no queens on the board.
theThe possible successor functions isfunction is add a queen in any square. 
supposeSuppose in this configuration we have five queens 
 in the next configuration we can add any queen to the boards that it does clash with the same position. 

[31:02] Tthat is, from this board we can have sixty four minus five64 – 5 = 59 fifty nine  possible successor states. 
theThe goals state the goal test is that eight 8 queens are on the board and none are attacked. 
soSo in this formulation this is the initial state. 
thisThis state will have sixty four64 successors depending upon the placement of the first queen that we introduced. 
soSo this is one successor where we have placed the first queen in this square, 
 this is the second successor ,
 this is the third successor, 
 this is the fourth successor and so on we have sixty four64 successors. andAnd each of these states will again each will have six three63 successors. 

[32:00] 
so So if you we start from this state there will be sixty three63 successors. 
(refer slide time [32:02])

 

So, that was one possible way of formulating eight 8 queens as a Sstate Sspace sSearch problem. 
weWe can look at other alternative formulations of the eight 8 queens problem. 
(refer slide time [32:23])

 

inIn the second formulation a state is any arrangement of eight 8 queens on the board. 
soSo we always have queens and the
so state is any arrangement of eight 8 queens. 
inIn the initial state all queens are at column one. 
soSo this is the board board,
 in the initial state all the queens are at the column one and the successor function hais changed the position of one queen. 
theThe goal test is the same where
 8eight queens are on the board and none are attacked. 

[33:02] so So this is the initial state. 
nowNow the possible operators are, we can take this queen and move it to another column. 
soSo this particular queen we can move to any of these seven columns or we can take this queen and move to one of theses seven columns. 
so theseThese are the possible successor states. 
nowNow this is an intermediate state for this formulation. At and this intermediate state also we can take one of the queens and move it around. 
soSo this is one formulation of the eight 8 queens problem. 
aA third formulation is, we describe a state by any arrangement of k queens and the first k rows. 
soSo at an intermediate point we would have a current value of k. 
supposeSuppose k is three= 3 
this means that one queen is placed somewhere in row one, 
 another queen is placed somewhere in row two and 
 the third queen is placed somewhere in row three. 
soSo for k equal to three= 3 this is a possible state of the system. 
theThe successor function is, add a queen to the k plus one+  1th row so that none are attacked. 
thatThat is, the partial solution does not violate any constraints. 
soSo we take the fourth queen and put it in one of the rows such that the queens are not attacked by each other. 
(refer slide time [34:55])
  

[35:00] so this is sSuppose for k equal to three= 3 this is a possible starting configuration and this is a possible then these are the there are two let us see in this configuration what are the what are the positions in which we can keep the queen. 
weWe cannot keep the queen here because it will be attacked, 
 we cannot “  “buy  this queen we cannot keep the queen here, 
 we cannot keep the queen here because it will be attacked by this one, 
 we can keep the queen in this position, 
 we cannot keep the queen here, 
 we cannot keep the queen here because it will be attacked by this queen, 
 we cannot put the queen here because it will be attacked by this queen, 
 we can put the queen here. 
soSo this state has two successor states. 
(refer slide time [35:46])

 

soSo what we have seen is that, given the same problem there are different ways of abstractly representing the state space of the problem. 

[36:04] when When we look at the different search algorithms we will try to see whether a particular formulation is suitable or good for obtaining a solution of the problem in an efficient manner and in a correct manner. 
when we nowNow we come to the concept of implicit state space. 
(refer slide time [36:28])

 

theThe state space may be explicit or it may be implicit implicit.
inIn an explicit state space we model explicitly all the states of the system and we model all the operators possible operations on the system. 
soSo this is an example of an explicit state space where every stat6e is explicitly modeled. 
However, in this coursease  we were mainly dealing with very huge state spaces. and And it may not be possible or easy to model explicitly all the states of the system explicitly neither is it necessary. 

[37:16] Iit may not be necessary to spell out all the states of the system in order to find a solution. 
soSo we will look at mainly problems where the state space is implicit implicit.
thatThat is, we need not explicitly in the beginning keep all the states 
rather we give a way of representing the states and we give methods so that given a state and given an operator we can generate the next state. 
weWe give tests for testing whether the agent has reached a goal state. 
soSo our state space will mostly be implicitly represented. and And in this implicit state space representation the agent will know what is the initial state that the agent is in b but ut he will not initially need to keep all the states that the system has. 
heHe will know the initial state and he knows the operators and the effects of this operatorsthese operators so that the agent can unfold the state space. 

[38:26] so So he knows that if these operations are applied at this state these are the states that he can reach. 
thenThen if he selects the state for expansion we can obtain the different other states that he can reach from this state. 
anAn operator is a functions which expands a node. 
thatThat is, it computes the successors of the node. 
givenGiven a state given an operator the agent will be able to generate the successor node. 

[39:00] so So let us take as an example the eight 8 puzzle problem as an example. 
so may ofMany of you may be familiar with either the eight 8 puzzle or its cousin the fifteen 15 puzzle. 
soSo in this eight 8 puzzle problem 
there there is a boat consisting of nine 9 squares and there are eight 8 blocks 
one numbered one two three four five six seven eight1 2 3 4 5 6 7 8. 
so theseThese blocks are in a particular configuration in this boat and one position is empty. 
nowNow in this sliding tile puzzle we can slide these blocks which are adjacent to blank twothe blank position. 
Tthat’ is we can slide this tile four to this position or slide tile eight to this position. Aand the objective of this eight 8 puzzle problem is to reach the goal state. 

[40:08] Sso the goal state is another configuration of this system. 
Tthere could be various different configurations that we decide for the goal state. given Given a particular goal set configuration the objective of the puzzle is to start from the initial state and slide the blocks alonground so that the agent reaches the goal state. 
(refer slide time [40:25])

  

so whatWhat is the state space representation of eight 8 puzzle? 
(refer slide time [40:36])

 

aA state is a description of each of the eight tiles in each location that it can occupy. 
theThe operators are; the blank can move left right up or down. 
goalGoal test: iIf the current state matches the goal configuration then we say that the system has reached thea goal state. 

[41:00] Tthe path cost is the number of steps or the number of times one ahs has to slide a block in order to go from the start state to the goal state. 
soSo the objective of the eight puzzle problem is to start forrom the start state and reach the goal state taking the minimum number of steps. 
soSo let u’s look at how the eight puzzle we represent eight puzzle and how we can expand the state space of the eight puzzle. 
so thisThis is our initial state. 
fromFrom this initial state two possible actions we can take.
Wwe can either move four to the right or eight up or rather we can say we can move blank to the left or blank downwards. 
ifIf you move blank to the left we come to this configuration and 
i if you move blank downwards we come to this configuration. 
nowNow from this state again blank can move left or down. 
[42:04] we We do n’tot move the blank right because then we will go back to this state. 
soSo we can get these two are as the successor states of this state. 
similarlySimilarly we can expand this state and we can move the blank either to the left or downwards to get either this configuration or this configuration and so on. Likewise we can unfold the state space of eight puzzle. 
so thisThis is an example a very simple example of an implicit state space where we describe the operators the result of applying the operator. 
theThe operators are moved blank up, down, left or right.
thereThere are four possible operators and we also give the pre conditions under which an operator can be applied. 
weWe can move blank up only if the blank is not already at the top most row rows.
(refer slide time [42:57])

 

[43:00] similarly Similarly for each of the four operators we can lay down the pre condition and we can give the effect or describe the effect of these operators so that we can unfold the state space.  
asAs another example let us take the game Ttic Ttac Ttoe or Knots and crosses which many of you are familiar with. 
thisThis by the way is a two person game which cannot be modeled as a search for a single agent but as that of two competing agents. 
(refer slide time [43:30])

 

soSo, in a Ttic Ttac Ttoe we have a board configuration like this and in the first instance the cross the first player plays a cross  
suppose the first player puts a cross at the center location. 
soSo in the search tree the different successors will correspond to cross at the central location and not placed at any of the other eight locations. 

[44:10] so So knot can be placed either here or here or here or here or here or here or here or here. 
so theseThese are the eight possible places in which the player can place a knot. 
supposeSuppose a knot was placed in this location 
 what are the possible successors? 
nowNow at in the next move the cross can be placed at any of the other seven locations. 
forFor example, the cross can be put either here or here and so on may be here. 
(refer slide time [44:52])

 
so thisThis way we can take the Ttic Ttac Ttoe problem  to 
represent it as a state space representrepresent each state by the position of the knots and the crosses. 

[45:06] we We can represent each state and we can say the operator the current operator is either a cross or knot and depending on whose turn it is to move whether it i’s a odd level move or even level move and the cross or the knot can be placed in a blank location. 
nowNow in a state space search problem so the input is a set of states, 
(refer slide time [45:34])

 

a set of operators and may be costs associated with the operators. 
thereThere is a distinguished state called the start state and there is a goal test which tells us whether we have reached a goal state small g which is one of the goal states. 
a A path starts from the initial state and it i’s a sequence of actions or sequence of states and the last state is the goal test. and And the objective of the search problem is either to find a solution that is find a sequence of moves to take the agent from the start state to the goal state or the objective would be to find the shortest possible such path. 
(refer slide time [46:28])

 

nowNow let us look at the description of the basic search algorithm. 
in the next lecture we will go into more details of about the search algorithm 
today weLet us will look at the basic idea of a search algorithm. 
so letLet l be a list containing the initial state. 
soSo l is called the fringe. 

[47:00] So l is a list containing the initial state and 
 l will containin the intermediate state 
it will contain the set of states that we have generated so far and that we need to expand. 
thenThen the search algorithm executes in a loop. 
ifIf l is empty that is if the fringe is empty return failure. 
 otherwiseOtherwise we select the first node from the fringe. 
soSo select is a function which selects a node from l l.
itIt may be the node with the minimum cost. 
ifIf this node happens to be a goal then we return the node or return the path from the initial state to this node. 
soSo if this node is a goal then we return the node that is return the path from initial state to node. 
otherwiseOtherwise we apply all applicable operators to node to generate all the successors of node and these successors we merge into l. 

[48:09] so So l is l union the successors of node. 
nowNow, as we will see, the various search algorithms differ as to how we positions the different successors in to the fringe and consequently how which node we select for expansion from the nodes in the fringe. 
(refer slide time [48:47])

  

nowNow let us discuss we will run the details of the search algorithm in the next class but today we will talk about the key issues that we will discuss about search. 

[49:04] 
so inIn state space search we start from an initial state and we expand this state to get other possible states. 
nowNow these states that we get constitute go into the current fringe. 
weWe select one of the nodes of the current fringe for expansion.
soSo this node goes out of the fringe and its successors are placed on the fringe. 
thatThat is, if we look at this search tree the fringe constitutes those nodes which are at the leaf of the search tree. 
soSo out of so here there are five nodes in the fringe. 
theThe search strategy will determine which of these five nodes iI will expand next. 

[50:00] suppose Suppose we decide to expand this node then we generate the successors and place this in the fringe. 
nowNow we decide which of these sixc nodes we will expand. 
nowNow one thing that we will notice is that, if we model out search problem like this the search tree may become unbounded because of loops in the state space representation. 
supposeSuppose there is an edge, 
 there is an operator which takes you from a to b, b to c and c to a and
n now if we say that this is a, this is b, and this is c then the successors of c will contain a and then again under a we will have this entire tree. 
So, in effect even if the number of states is finite the search tree may become unbounded if we unfold it like this tree. 

[51:00] so So a finite state space may become unbounded because of presence of loops and in some cases we will see the state space can itself be infinite. 
soSo when we talk about a search algorithm we have to handle these issues. 
anotherAnother issue that we willwe need to handle is, whether depending upon the problem whether the search problem returns the path or whether it returns a node. 
forFor example, in the eight 8 puzzle problem we the solution to the search problem is a set of operations whether it is up, down, up, left, right, left, left, left, down.
so It is sequencea sequence of operations which takes us from the initial state to the goal state. 
howeverHowever for problems like the eight 8 puzzle we really do not need to spell out the operators. 

[52:05] Iif we just give the goal configuration the operators are clear operators are evident. 
ifIf you say that this is an initial state and this is the goal state then such a thing that is sufficient. 
givenGiven the goal state we can find the sequence of operators very easily which takes us to this goal state. 
so Therefore depending on the problem definition we will either decide to return a path or a node. 
then A very important aspect for the design of a search problem is to discuss how merge and select are to be done. 
thatThat is, when we put nodes in the fringe, when you put the successors of the current generated node in the fringe where should we put the successors? Is it in the beginning or in the endend? 
Hence  
so how we should know how to put the nodes on the fringe and also how the select program is done carried on. 

[53:01] do We should know whether towe select the first node, do we  select the cheapest node and so on. and And to consider these things we need to consider certain factors. 
isIs the graph weighted or un  weighted? 
ifIf all steps have same cost we can model the state space as a un weighted graph. Bbut if different operators have different costs we can associate weights with the edges corresponding to the cost of that operator and we will deal with weighted graphsgraphs. 
then we will need to knowwe must also find out how much is known about the quality of the intermediate states. 
Do w e have any information or heuristics to guide the search process process?
So, if we know that out of the nodes available in the fringe, 
so there are one two three four five six seven eight1 2 3 4 5 6 7 8 so 8 eight nodes available in the fringe ,

[54:08] if we know that this node has very high promise and it i’s likely to reach take us to a goal solution fast we might use that knowledge to expand this node before expanding the other nodes. 
soSo if we have some information about the quality of these intermediate states we can have better search strategies so that we can find a solution fast. and And also we need to consider whether our objective is to find a minimal cost path or any path as soon as possible. 
we will in the class we will discuss several problem solving strategies and whenWhen we discuss problem solving strategies we will evaluate each problem solving strategy by the following parameters whether it’ is complete, whether it i’s optimal and what is the cost associated. 
(refer slide time [55:08])

 

soSo a strategy is complete if the strategy is guaranteed to find a solution to the search problem if a solution exists. 
ifIf a solution exists the strategy must guarantee that one solution will be found. 
a A strategy is optimum if it has the least cost. 
theThe search cost associated with the strategy is the time and memory resources required to find a solution. 
theThe time and space complexity of the algorithm is a very important thing that we need to keep in mind and that we need to measure. 
theThe various search strategies that we will study are blind search without heuristics. 
inIn those we have depth first search, breadth first search, iterative deepening search, iterative broadening search etcetera. 
(refer slide time [56:04])

  

[56:04] we We have informed search that makes use of heuristic information. 
weWe will also study constraint satisfaction problem and we will also look at adversary search involving two person games. 	
 so we will come back to breadth first search and breadth first search in the next lecture and i will end this lecture by letting you think of a problem 
(refer slide time [56:33])

 

Here is a problem to think of: you You have three jugs measuring twelve 12 gallons, eight 8 gallons and three 3 gallons and you have a water faucet or a pipe from which you can get water. andAnd your objective is to check out exactly one gallon of water. 
inIn the beginning you have three empty jugs and you objective is to get one gallon of water in one of the jugs any one of the jugs. 

[57:00] now i want you to tThink of how you can formulate this as a state space problem, how you can represent a state and how you can get a solution to this problem. 
so So think about this problem and you will see that representation is extremely important for search problems. 
now we come to the end of this lecture
 before i end i will give you a few more problems that you must work out before you come to the next lecture 
(refer slide time [57:34])

  

so iI will describe a few problems. 
forFor each of these problems you have to represent them as a state State space Space search Search problem and specify the initial state, the goal test, the successor function and the cost function. 
youYou must choose a formulation.
so youYou must also of course give the state space formulation and discuss how a state is represented. 
so youYou must choose a formulation that is precise enough to be implemented. 

[58:10] Ffirst problem is, you have to color a planar map using only four colors in such a way that no two adjacent regions have the same color. 
secondSecond problem, the traveling salesperson problem, 	
 there is a map involving n cities some of which are connected by roads. 
theThe aim is to find the shortest tour that starts from a city visits all the cities exactly once and comes back to the starting city. 
(refer slide time [58:35])

 

problemProblem three is the missionaries and cannibals problem. 
thereThere are three missionaries and three cannibals on one side of the river. 
thereThere is one boat which carries two at a time. 
theThe missionaries must never be outnumbered by cannibals. 
giveGive a plan for all to cross the river. 
so that is the end of this lecture 
in the next class we will continue on search strategies 
thank you

Artificial Intelligence (Lecture – 4)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

Uninformed Search

[00:36] Today we will start the fourth lecture of this course Artificial Intelligence. 
Today’s topic is uninformed search. 
The last class we saw how the state space can be represented. 
Today we will look at some of the strategies and algorithms that are used for search. 
Before we start today’s lecture we will look at the questions of the previous class and the answers to those questions. 
(refer slide time [01:07])
 

[01:05] The first question of last class was this;
Consider the following as a state space search problem. Choose a formulation that is precise enough to be implemented. 
For each of the problems that we have described you would have to show the representation of the state, describe the initial state and the goal test and describe the successor function. 
(refer slide time [01:30])

 

The problems we looked at in the last class were, number one was the graph coloring problem. 
In the graph coloring problem you have to color a planar map. 
You have a map which has a number of regions. 
As you all know, in the map there are several regions. 
Different regions may denote different countries or different states and you have to use some number of colors to color these regions such that no two adjacent regions has the same color. 

[02:18] Now we will look at our representation of this map coloring problem. 
Rather than graph coloring this is the map coloring problem. 
What we will do is we will first represent the map by a graph. 
For each region we will have the vertex of a map. 
Suppose this is region a, this is b, this is c this is d, this is e and this is f 
we will say that the region a corresponds to the vertex of a graph. 
And b and f are adjacent to a. 
For b a, f and then d and e these are adjacent to b and so on. 

[03:10] So we will represent this map by a graph where for every region we will have a vertex of a graph and if two regions are adjacent we will have an edge connecting those two vertices. 
So we will call these vertices as v1 v2 vn and we will represent the colors by c1 c2 c3 c4. 
In the map coloring problem we will use four colors to color the map. 
In fact it has been shown that any planar map can be colored by four colors. 
What we will do is a state will be represented by an n tuple. 

[04:01] So for every region we will have one entry in the state representation. 
If a region is colored by a particular we will use that color as the attribute for that region. 
If a particular region is not colored at a particular time we will represent that region by x. 
So let us look at this example.
(refer slide time [04:33])

 

This is an example of a map. 	
In this map there are five regions a, b, c, d and e. 
Corresponding to this we are constructing a graph which has five vertices. 
Now a and b are connected, a and d are connected, b and c, b and d, c and d, c and e and d and e. 
So this graph represents this map. 

[05:07] Now this is a particular state of the graph where a and e are colored blue, 
b is colored green, c and d have not been assigned colors as yet. 
(refer slide time [05:19])

 

We will represent this state as blue green x x blue. 
Therefore for this map coloring problem in the initial state we will start with all the states unassigned, all the regions unassigned and none of them has been assigned a color. And we will have reached a goal test if when all the five regions are colored such that if two vertices si and sj are adjacent their colors are not equal. 
So goal state is a state where all regions have been assigned colors and the coloring does not violate the constraints. 

[06:09] 
Now, what are the different operations we can apply to the state space? 
For example, this is a particular map. 
We can apply the following operator: 
Change to yellow, that is, change region 2 that is b to the color yellow. 
(refer slide time [06:28])

 

And as a result we get this state. 
Now on this state if I again apply the operator change three green we get this state. 
So this is a state space representation of the graph coloring problem. 
The second problem we assigned in the last class was the traveling salesperson problem. 
In the traveling salesperson problem we have a map involving n cities some of which are connected by roads. 

[07:01] The objective is to find the shortest tour that starts from a city, visits all the cities exactly once and comes back to the starting city. 
(refer slide time [07:10])

 

Now this is how we will represent the traveling salesperson problem. 
(refer slide time [07:19])

 

This is the map, this is the graph corresponding to the traveling salesperson problem. 
And a, b, c, d, e are the different cities and there are roads connecting some of these cities.
For example, the distance between city a and b by the direct road is 94 units. 
Distance between a and d is 384 units and the distance between a and e is 190 units and so on. 
So we represent by y the set of cities. 
 In this case y is equal to the five cities a, b, c, d and e. 	

[08:03] We represent by this distance function dxy the distance between cities x and y. 
Now corresponding to this traveling salesperson problem we will design the state space. 
In the state space a state would be represented as a path which a path connecting a number of cities such that in this path no city is visited more than once. 
So such a path is called a Hamiltonian path.
In graph theory such a path is called a Hamiltonian path. 
So a state for our state space representation will be a Hamiltonian path. In this graph a Hamiltonian path is a path which does not visit any city twice. 

[09:01] Now for our representation x will be the set of states in our state space representation. 
Each state is a Hamiltonian path of this TSP graph. 
(refer slide time [09:19])

  

So each state is represented by x1 x2 xn. 
This is a path such that each of these xi corresponds to a city and no two cities are same. 
That is, xi not equal to xj unless we get the full traveling salesperson tour 
that is i = 1 and j = N + 1 where there are n cities in this TSP problem so we have n cities. 

[10:00] A TSP tour is actually a tour of N + 1 cities where the first city is the same as the N + 1th city. 
So that salesperson completes a tour of all the cities and comes back to the starting city and every other city in the path is distinct. 
So this is the representation of the state. 
The successor of a state x1 x2 xn is represented like this. 
So delta x1 x2 xn is the successors of this state or all states which are extensions of this Hamiltonian path. 
So at those states which are x1 x2 xn + 1 where xn + 1 is a city which is not equal to any other city which already existed in this path. 
Now this is the state space representation of the TSP problem. 	

[11:00] What is the goal? 
The goal state is a state where we have a Hamiltonian tour of length n + 1.
So the set of goal states include all states of length n + 1. 
(refer slide time [11:19])

 

The third problem was the missionaries and cannibals problems. 
Three missionaries and three cannibals are on one side of the river, there is one boat which can carry only two persons at a time. 
Missionaries must never be out numbered by cannibals, you need to give a plan to cross the river and for this problem I want you to come up with a state space representation. 
Now the answer to this is, one way of representing a state is by a three tuple where we represent M C B. 
M is the number of missionaries on the left bank so this is a river which the missionary is trying to cross. 

[12:02] Initially we have 3 missionaries and 3 cannibals on the left side of the river. 
So this is the left side and this is the right side and we have one boat and this boat is initially on the left side and it can carry 2 people. 
Now the state will be represented by three numbers. 
Here M is the number of missionaries on the left bank so initially M is 3, 
in the goal state M will be 0 because all the missionaries need to be on the right bank, C is the number of cannibals on the left bank. Initially C is 3 and in the goal state c will be 0 and B is the position of the boat whether the boat is the left bank or the right bank. 
In the beginning the boat will be on the left bank.
(refer slide time [12:53])

  
So the initial state will be presented by 3, 3, L. 
That is 3 missionaries, 3 cannibals on the left bank and the boat also on the left bank. And the goal state will be represented by 0, 0, R that is no missionary on the left bank, no cannibal on the left bank, 3 missionaries on the right bank, 3 cannibals on the right bank and the boat is on the right bank. 

[13:20] The operators for this problem are those are two tuple M and C. 
So M is the number of missionaries on the boat and C is the number of cannibals on the boat. 
Since only at most 2 persons can be on the boat at the same time the possible operators are 1 0 2 0 1 1 0 1 and 0 2 and 0 0 is not a valid operator because the boat needs at least 1 person to steer the boat. 
(refer slide time [13:55])

 

So what we find is that, each state is represented by a three tuple M C B. 

[14:05] So m can take four values 0 1 2 3, 
c can also take four values 0 1 2 3 and b can take two values L or R. 
Therefore the possible number of states is 4 × 4 × 2 that is 16 × 2 = 32. This 16 contains the boat on the left side and this 16 contains the boat on the right side. But some of these states violate the constraints that missionaries must never be out numbered by cannibals on any side because otherwise the cannibals will devour the missionaries. And if you apply that constraint these green states become invalid states. 
So finally these are the only valid states. 
So there are 20 valid states in this state space. And if we apply the operators to each of this state we get the state space graph. 
(refer slide time [15:00])

 

[15:08] this is the initial state: 
3, 3, L 0, 0, R is the goal state and these arcs denote the successors of each state. That is, the operators that can be applied on each state to go to the other state. 
(refer slide time [15:24])
 

So this is a state space representation of the missionaries and cannibals problem. 
Now we are ready to start today’s lecture which is module2 which I started in the last class, the problem solving using search. 
In today’s lecture we will talk about uninformed search. 
The instructional objectives of today’s lecture: 
In this lecture the student will learn the following strategies of uninformed search, 
breadth first search, depth first search, iterative deepening search, bidirectional search. 

[16:05] 
So we will look at these four strategies for uninformed search. 
We will know what uninformed search is. 
Uninformed search means searching through the state space without using any extra information, without using any domain specific information. 
For each of these search strategies the student will learn the algorithm for this strategy and we will analyze each of these algorithms to find their time and space complexities and the student will also learn when to select a particular strategy for a given problem, on what basis given a problem which strategy is the best strategy to select. 
(refer slide time [16:50])

  

At the end of this lesson the student should be able to do the following. 

[17:01] 
You should be able to analyze a given problem and identify the most suitable search strategy for the problem. 
(refer slide time [17:15])

 
Given a problem she should be able to apply one of these strategies and find the solution to the problem, find the sequence of nodes that will be expanded and what would be the final solution that will be obtained.
(refer slide time [17:26])

 

Here is a quick recapitulation of the representation of the search problem. 
We have seen in the search problem we represent s to be the set of states, 
the initial state is s0 which is a member of S, we have a set of operators or actions a and we have a set of goals g represented by an explicit set of goal states or represented by a goal test or by explicit goal states. 

[18:12] A solution to a search problem is a plan which is a sequence of actions 
A0 a1 an which leads to traversing a number of states starting from state s0 by applying action a sub 0 the agent goes to state s sub 1 and from s sub 1 by applying action a sub 1 the agent goes to s sub 2 and so on. 
Finally the agent arrives at state sN + 1.

And if this plan is a solution is a solution to the search problem then sN + 1 must be a goal state. We also mentioned that for every path we will associate a cost. 

[19:02] 
Therefore path cost is a positive number and usually path cost is the sum of the operator costs. 
Now, this is the state space of missionaries and cannibals problem which we just discussed. 
Now let us see what is a plan for this missionary and cannibal problem? 
We have already seen the state space for this problem and we have identified that 3, 3, L is the initial state and 0, 0, R is the goal state. 
So 3, 3, L is the initial state. 
We will trace a solution so one of the operators brings which operator by taking two cannibals by a boat one comes to the 3, 1, R state and from there the agent can come to the 3, 2, L state and then to the 3, 0, R, 3, 1, L 1, 1, R, 2, 2, L, 0, 2, R, 0, 3, L, 0, 1, R, 0, 2, L and then 0, 0, R. 

[20:13] So this is a solution to the missionaries and cannibal problem. By applying a sequence of operators the agent can traverse the state space and reach the solution. 
Now, in today’s lecture we will look at various strategies to systematically explore the state space by using a search graph or a search tree. 
Corresponding to this state space we will see how to construct the search tree or a search graph. And we will also see how to systematically explore this search tree or search graph in order to find a plan or a solution to this problem. 

[21:12] Now this is an abstract example of a very simple state space. 
We have these eight states A B C D E F G H and these arcs show the connectivity information between the states. 
Here A is the initial state and G is the goal state. 
Now, when we look at a search algorithm we would be interested in finding a path from the start state to one of the goal states. 
Now there are several alternatives for which our algorithm will strive for. 
The objective of the algorithm may be to find the shortest path from the start to a goal state or it could be to find any path from start to the goal. 

[22:05]
 Today we will look at blind search or uninformed search which means that the algorithm does not use any information about the domain. And the most important blind search strategies are Breadth First Search and Depth First Search. 
In short it is BFS or Breadth First Search and DFS Depth First Search. 
(refer slide time [22:34])

 

Now we will see what is a search tree corresponding to a problem. How do we get the search tree? 
This is the state space graph. 
From this state space graph we will start from the initial node and we will list all possible paths. 

[ 23:02] 
So we start from a, from A one can go to B or C, from B one can go to D or E, from D one can go to C or F, from C one can go to G, from F one can go to G or H but again from G one can go to C or F etc.
So actually if we take these states, and unfold this graph into a search tree we may get an infinite search tree. 
In order to make this search tree finite we will need to eliminate cycles from every path. 
That is, if in this path we have already visited C then from G we will not come back to C. Anyway G is a goal state. 

[24:00] 
So in order to construct a search tree we start from the initial state we unfold the graph and we list all possible paths until one reaches to a goal state or to a dead end and the result is a search tree. 
(refer slide time [24:15])

 

Now let us look at some terminologies corresponding to a search tree. 
(refer slide time [24:33])

 
Now the root node is the node from where we initiate the search tree. 
So in this example A is the root node. 
The leaf nodes are the leaves of this tree. 
This particular tree has the following leaves: 
G G H E E G H G out of which G is a leaf node because G is a goal node and E and H are leaf nodes because we have reached dead ends. 

[25:06] For the search tree E’s parent is B and ancestors are B and A. 
For this node F the ancestors are D, B and A. 
The descendents of F are G and H, the descendents of C are D, G, B, F, E, G and H. 
The branching factor means the maximum number of children of a node. 
So, in this particular search tree we see that every node has less than equal to two children.
Therefore in this case branching factor equal to two. 
We represent the branching factor usually by this notation b, b = 2. 
A B D C G is a complete path. 
A B D is a partial path. 

[26:01] 
(refer slide time [26:07])

  

Now let us look at a basic search algorithm, a systematic way of searching the entire search space. We will put certain restrictions on this general search algorithm to come up with special algorithms. 
So, in the basic search algorithm we maintain a data structure called the fringe. 
A fringe is referred by many people as open or queue or frontier. 
So fringe will contain those nodes that our algorithm can expand at a particular time. 

[27:01] Initially fringe will contain only the initial state of the state space. 
Fringe is a list which contains the initial state in the beginning. 
The search algorithm will execute this loop. 
First step is, if fringe is empty, if there is no more state to expand and the algorithm has not found a solution n yet the algorithm returns failure, if fringe is empty return failure which means no goal found and no node remains to be expanded.
Otherwise the algorithm removes the first node from the fringe. Let us call this a node.

[28:07] 
If node is a goal here the goal test is applied. 
If node is a goal then the algorithm returns the path from initial state to node. 
Starting from initial state when the algorithm expands node finds that node is a goal and the algorithm needs to return the path to come from the initial state to the goal. 
Otherwise it will generate all successors of node and incorporate those successors of node into fringe. 
The algorithm will put the newly generated successors into fringe. 
How these new nodes will be placed in fringe will depend on a particular algorithm. 

[29:01] 
So this is the basic search algorithm. 
You start from the initial state and then you expand its successors so that you can get new nodes. And when you select a node for expansion you must check whether it is a goal. 
If it is not a goal you expand it and get its successors. 
The successors are put into fringe which is potential nodes. 
Now we will discuss several search algorithms today. 
For each of these algorithms we will look at the following: 
We like to measure how effective these algorithms are in terms of, number one, completeness, is this algorithm guaranteed to find a solution if a solution exists, 

[30:02] given a problem given the state space representation of the problem if a solution exists to this problem can the algorithm find the solution or is the algorithm guaranteed to find the solution. 
Secondly we will evaluate whether the algorithm always results in an optimum solution. 
If a solution is found by this algorithm is the solution guaranteed to be the one that has minimum cost? 
Thirdly we will look at the efficiency of the algorithm. Namely we will look at the time complexity of the algorithm which the time taken is measured by the number of nodes expanded either in the worst case or average case to find the solution. 
So we will look at the time complexity of the algorithm. 
We will also look at the space complexity of the algorithm. 
That is the memory required by the algorithm which we will measure in terms of the maximum size of fringe. 
(refer slide time [31:04])

 

[31:06] 
Fringe is the list the algorithm must keep track of. 
How big can be the size of fringe in the worst case or average case, could be a measure of the efficiency of this algorithm or whether the algorithm is practical to all. 
Today we will look at several blind search strategies. 
Depth first search, breadth first search, iterative deepening search and bidirectional search. 
In the subsequent classes we will look at informed search: Search using heuristic function and we will also look at constraint satisfaction problems which can be modeled as search problems and two person games or adversarial search. 
Now the first algorithm we will discuss is breadth first search. 
We will see how the basic search algorithm can be modified to give rise to the breadth first search algorithm. 

[32:09] 
This is the search algorithm we discussed. And we now mentioned that, in order to have different strategies we need to change the way this step works. 
Merge the newly generated nodes into fringe. 
(refer slide time [32:27])

 

How to merge the newly generated successors into fringe will determine what sort of search strategies we get. 
So the strategy in breadth first search is to expand the shallowest node first. 
In order to achieve this, the merging of successors into fringe will take place like this. 
We will add the generated nodes to the back of fringe. 
Fringe is a list of nodes which are waiting to be expanded. 

[33:01] 
The new nodes which are generated will be put at the back of fringe. And we will see that this strategy results in the expansion of the shallowest nodes earlier than the deeper nodes. 
Suppose this is a very simple search space we will see how we will apply breadth first search on this search space. 
Earlier we saw that this search space gives rise to this search tree and breadth first search will actually search on this tree in order to find a goal. 
Now this is the potential search tree that can be generated by search. 
Initially we have in fringe a single node A. 

[34:00] 
This node is the only node in fringe. 
Now we will execute the loop of our algorithm which says that remove 
the first node from fringe. The first node is A so we will remove this node from fringe and get its successors. 
The successors happen to be B and C. 
So A will be removed from fringe and B and C will be incorporated at the back of fringe. There is nothing in fringe so B and C will be put at the fringe. 
Therefore what should be the next step? 
B has to be removed from fringe as B is the first node in fringe and its successors D and E will be generated. 
So B has to be removed from fringe and its successors D and E are generated. 
At the next state C is removed from fringe and its successors D and G are generated and put at the back of fringe. 

[35:10] 
 In next step D will be selected for expansion so D will be removed from fringe and its successor C and F will be generated and put at the back of fringe. 
The next step would be, E has to be removed from fringe and its successors have to be put at the back of fringe, E has no successors so E is simply removed from fringe. 
So you notice here that this is the order in which nodes are expanded.  
First A is expanded, then B then C then D then D then E then F then G etc. 

[36:00] 
In breadth first search nodes are expanded in this order according to their level.
First it will be A then B then C then D E D G. 
So nodes will be expanded in the order of their depths. 
E has been removed from fringe, then D will be removed from fringe and its successors B F will be added to the back of fringe and then G is the next node to be expanded and G happens to be a goal node so in this case the search will stop. 
We notice that first all nodes at level zero from depth zero from the start state get expanded that is A itself then all nodes which are at distance one from the start state then all nodes at distance two then all nodes at distance three and so on. 
(refer slide time [37:03])

  

Now we will analyze this algorithm and look at its properties whether a breadth first search is optimal whether it is complete and will also look at the time and space complexities so that we can discuss the time and space complexities of this algorithm. 
We must assume a particular structure of the state space. 
(refer slide time [37:34])

 

Therefore what we will do is we will assume that in our state space we have a very uniform type of search tree. 
Having a uniform search tree will enable us to analyze the complexity. 
So let us assume that every node has utmost b children or exactly b children 
and let us assume that d is the depth of the shallowest goal node. 

[38:08]
There can be many goal nodes but let us say that the shallowest goal node is at depth d and let m be the depth of this entire search tree. 
So m is the maximum depth of the search tree, d is the depth of the shallowest goal node and we have a uniform search tree where each node has exactly b children except the leaf nodes. 
So under this assumption let us look at the properties of breadth first node. 
(refer slide time [38:56])

 

We have seen that in the breadth first node the generated nodes are put at the back of fringe. 

[39:03] 
That is, new nodes are put on the fringe in first in first out order or FIFO order like a queue. 
So fringe is actually implemented as a queue. 
New nodes are put at the back of the queue and then node to be expanded is the first node in front of the queue. 
Firstly, breadth first search is complete. 
That is, if a solution exists breadth first search systemically expands zero level nodes first, then all one level node, then all two level nodes, all three level nodes and will eventually expand all d level nodes and will come to the goal node which is at level d. 
So breadth first search is a complete search algorithm. 

[40:01] 
Secondly, breadth first search is optimum. 
If all operators have same cost, you notice that breadth first search expands node according to their level. 
So a node at level d will be expanded before any node at a larger level is expanded. 
So breadth first search will expand that goal node first which is at the lowest level. 
So if our search cost is equal to the number of steps to the solution then the breadth first search is optimum. 
However, if operators have different costs then a goal node at depth 5 may have a cost of 10 whereas a goal node at depth 3 may have a cost of 23.

[41:08] 
So, in this case breadth first case may not be optimum. 
Therefore breadth first search is optimum if all operators have same cost. 
Otherwise breadth first search finds that solution whose path length is shortest but path cost is not necessarily the cheapest. 
And finally it is the time and space complexity. 
If we go back to the previous slide, suppose this is the search tree and this is depth D and there is a goal here at depth D and G1 is at depth D then breadth first search will expand all these nodes up to depth D – 1 and some of the nodes at depth D in order to get this solution. 
Therefore the number of nodes expanded by breadth first search is equal to order of bd. And the number of nodes which are at fringe is also of the order of maximum number of nodes at a depth. 
We can see that the time and space complexities of breadth first search are both of the order of bd where d is the depth of the solution and b is the branching factor at each node.

[43:04] 
Now, in breadth first search if we have a complete search tree of depth d where each non leaf node has b children then breadth first search will have a total of 1 + b + b2 + bd. 
(refer slide time [43:28])

 

So this is bd + 1 – 1/b – 1 which is the order where bd nodes will be expanded by breadth first search. 
For example, if we have a complete search tree whose depth is 12 and every node up to depth 11 has 10 children then the number of nodes expanded by breadth first search will be order of 1012 that is 10 followed by twelve 0s. 
To give you an idea of how big this number is, if we have a computer where 1000 nodes can be expanded per second and each node uses 100 bytes of storage then breadth first search will take 35 years to run in the worst case and it will use 111 terabytes of memory so this is a truly frightening figure. 
So the time and space complexities of breadth first search are exponential in terms of d which is the depth of the solution. 
(refer slide time [44:49])

 

The advantage of breadth first search is that it finds a path of minimal length to the goal. But one of the major disadvantages of breadth first search when we compare it to the next strategy depth first search is that, breadth first search requires the generation and storage of a tree whose size is exponential. 

[45:12] 
So it requires exponential space to store the fringe and it also requires exponential time. 
Later we will look at uniform cost search which is optimal in terms of giving the lowest cost solution. But today let us look at the other search strategies. 
Now we come to the next search strategy which is depth first search. 
This algorithm will be again a variation of the basic search strategy we outlined in the beginning where instead of merging the newly generated nodes into fringe we will give a particular strategy for merging. 
In breadth first search we put the generated nodes at the back of fringe. 
(refer slide time [46:04])

 

[46:04] 
In depth first search our objective is to expand the deepest node first and to achieve this we add the generated nodes to the front of fringe. 
That is, fringe is maintained as a last in first out structure or a stack structure. 
The newly generated nodes are put at the front of fringe. 
Now let us use the same search tree we analyzed earlier and trace depth first search through this search tree. 
Initially the fringe will contain the node A. 
In the next step A will be removed from fringe and its successors B and C will be generated and put in the beginning of fringe. 
Now B will be removed from fringe and its successors D and E will be put in the beginning of fringe. 

[47:06] 
Next D will be removed from fringe and its successors C and F are generated and put in the front of fringe. 
Next C is removed from fringe and its successor G is put at the front of fringe. 
At the next level G will be expanded found to be a goal node so we terminate our search after having found this goal. 
So we notice that A B D C and G are the nodes expanded. 
A goal node at depth four has been found by depth first search. 
On the same search tree we saw that this goal was found as a result of breadth first search. 
This goal is at depth two and in that case we had expanded A B C D E and D. 
(refer slide time [47:55])

  

[48:03] 
So let us analyze the characteristics of depth first search. 
In depth first search we en queue the nodes on fringe in a last in first out order by using a stack. 
Depth first search also requires exponential time ordered bd to explore the entire search space. 
However, if you notice the size of the fringe is utmost the total and the maximum depth of the tree. 
(refer slide time [48:40])

 

So the space required is actually of the order bm where m is the depth of the entire search tree. And the time required is order bd.
Depth first search may not terminate if the search tree is infinite. 

[49:01] 
If the search tree is infinite depth first search may not terminate. 
However, if it does terminate the time taken is equal to the size of the search tree if the search tree is finite at the worst case and the space taken is order of depth of the search tree. 
One advantage of depth first search is that it only takes linear space. 
However the other characteristics are not that good in the sense that it is not complete. 
It may not be able to find the solution if the search tree is infinite and when it does find a solution the solution may not be an optimum solution.

[50:00] 
Now to recapitulate let us look at the State Space Search problems. 
A state space is a graph bd where V is a set of nodes E is the set of arcs. 
Each arc has a cost. And in order to run these algorithms we will use the data structure for every node. 
This is very important. 
(refer slide time [50:29])

 

Every node data structure will contain a description of the state the parent of the node. Keeping information about the parent is very important if we have to retrace the solution. 
So when we get a goal node in many cases we need to output the solution by which this state was obtained. 
So, to find the solution we need to keep track of the parent of each node. 

[51:01] 
For some algorithms we may need to keep track of the depth of the node. 
We also need to keep track of the operator that generated this node. 
If you want to find the final plan we need to find the operator that generated this node. And we need in some case to keep track of the cost of the path. 
Especially when we look at uniform cost search and best first search in subsequent lectures cost is important. 
But at least we need to keep track of the state description, the parent and the operator. And in some algorithms we need to keep track of the depth and sometimes the cost. 
(refer slide time [51:55])

  

A solution we have seen is a sequence of operators associated with the path from a start node to a goal node. 

[52:02]
 Now, for large state space it is not practical to represent the entire space.
So the state space like the 15 puzzle problem we may not generate the entire state space and represent it as an explicit graph. 
Rather as we expand and generate nodes we will make explicit only a portion of the implicit state space graph. 
That is required for us to obtain the solution. 
We need not unfold the state space all at a time. 
So we can work with implicit state spaces and only make those portions explicit by our successor generator operations. 
Now each node, because at every node we keep a link to the parent of the node and the operator that generated the node every node represents a partial solution path from the start to the given node. And from this node we can expand this partial solution to get may be one or more actual solutions. 
(refer slide time [53:22])

 

[53:22] 
Our search process will construct a search tree where the root is the initial state and the leaf nodes are those nodes for this search tree that we currently have. The leaf nodes are the nodes which we keep at fringe. 
They are those nodes which are not yet expanded. 
That is which have been generated and are yet to expanded or those nodes that have no successors that is they are dead ends. 
The search tree may become infinite even when the search space is finite because there could be loops in the state space. And in some cases our search algorithm needs to return the entire path and in some cases only the final state.

[54:12]
For example, in the 8 queens problem the final state is just the placement of the queens on the board. 
We need not give the solution path because given the state the solution path is evident. 
For the 8 puzzle problem the final state description does not carry any information as to how that state was reached. 
So we have to output the entire solution plan. 
We mentioned that depth first search may not work in infinite state spaces where it may not reach a solution and go into an infinite loop. 
So we have a variation of depth first search which we call depth limited search where we cut off search at a particular depth. 

[55:00] 
So depth limited search works like this: 
At every node we keep track of the depth or level of that node and we modify depth first search so that if depth of node which we try to expand is equal to limit 
then we terminate the search. 
(refer slide time [55:24])
 

So if we modify depth first search by cutting off search at a limit we get depth limited search which takes as parameter a depth limit and does depth first search up to that limit. 
Now, if we choose a limit before hand a solution may not be found at that depth. Therefore we have a variation of depth first search which is called depth first iterative deepening search. 
(refer slide time [55:58])

 

The idea is that we do DFS up to a limit and if we do not find a solution we increase the limit by 1 and continue. 
So until solution found do DFS with depth cutoff c then put c = c + 1 and so on. 
So depth first iterative deepening is a complete search strategy where initially we set limit to 1 then to 2 then to 3 then to 4 and so on. 
(refer slide time [56:26])

 

The advantage of iterative deepening search is that it has linear memory requirement because at every step we do a depth first search. And we start with limit equal to 1 then limit equal to 2 then limit equal to 3. Therefore it is guaranteed that when the algorithm finds a goal node it is a node of minimum depth. 
(refer slide time [56:52])

 

So let us illustrate this. 
This is a search tree. 
If we do iterative deepening search this is the first node to be expanded. 

[57:01] 
So initially if limit is equal to 1 so we get this node then this node then this node. 
For the second depth limit with limit equal to 2 this node is expanded then this then this then this then this then this then this. 
Then we set limit equal to 3 for which we get this this this this this and so on. And then we set limit equal to 4 and we have a depth first search of the search tree with depth bound = 4. 
(refer slide time [57:44])

 

So depth first iterative deepening is complete. 
It is optimum if all operators are of the same cost and its time complexity is a little worse than breadth first search because you notice that some nodes are expanded more than once they are repeated. 
However the order of node expansion is almost similar to breadth first search. 
(refer slide time [58:08])

 

[58:08] 
So, if the branching factor is b and the solution is at depth d the nodes at depth d are generated once, nodes at depth d – 1 are generated twice and so on. Therefore if this is the number of total nodes expanded by depth first iterative deepening then it is actually of the order of bd. 
However depth first iterative deepening search has linear space complexity unlike BFS and it is complete and it is usually the preferred algorithm for large state spaces. 
I come to the questions for today’s lecture. 
1) You are given a 5 gallon jug and a 2 gallon jug. 
Initially the 5 gallon jug is full and the 2 gallon jug is empty. 

[59:02] 
Your goal is to fill the 2 gallon jug with exactly 1 gallon of water. 
(refer slide time [59:12])

 

So, given this problem description your problem is to create the search tree and discuss which search strategy is appropriate for this problem. 
(refer slide time [59:26])

 

2) Consider the following graph. 
Starting from state A you execute GFS. 
G is the goal node, you have to show the order in which the nodes are expanded assuming that the alphabetically smaller nodes are expanded earlier in case of ties. 
On the same graph you also run iterative deepening search. That is your problem 3.  

Artificial Intelligence (Lecture – 5)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

Informed Search

[01:00] 
Now we will start the fifth lecture for this course Artificial Intelligence. 
Let us discuss about the module on search methods.
Today we will primarily focus on informed search methods. 
In the last class we talked about several blind search strategies that do not use any problem specific information. 
We discussed depth first search, breadth first search as well as iterative deepening search. 
Today we are going to finish that discussion after talking about by bidirectional search and then we will move on to informed search that use heuristics information.  
In bidirectional search we will discuss the algorithm, the time and space complexities and then we will move onto informed search. 

[02:02] 
We will talk about the algorithm A* and before that we will talk about greedy search algorithm as well as uniform cost search which are special cases of A*. 
(refer slide time [02:10])
 

At the end of this lesson the student should be able to do the following:
(refer slide time [02:21])
  

He should be able to understand what a heuristic function is? 
They should be able to design heuristic functions for a given problem. 
They should be able to prove that if A* uses an admissible heuristic function it will terminate and produce an optimum solution. 
They should learn how to compare two heuristic functions as well as how to combine multiple heuristics. 
So, in the last class we talked about blind search methods namely depth first search, breadth first search, iterative deepening search. 
Today we will talk about bidirectional search and then we will move on to informed search. 

[03:07] 
Subsequently in the other lectures we will talk about constant satisfaction which can be modeled as search problems and we will also look at adversary search which is used for dealing with two person games. 
(refer slide time [03:23])

 

Bidirectional search: In the other search methods we discussed one starts from the start node and then the search process explores the different nodes in search of a goal node. 
So the search branches out from the start state. 
In bidirectional search, in addition one will also start from a goal node and search backwards from the goal node trying to reach either the start state or one of the states which is reachable from the start state. 

[04:19] 
So, if one can reach from a goal to a state which is also reachable from the start state then we have found a path from the start state to a goal state. 
The strategy which employs this is called bidirectional search. 
(refer slide time [04:40])

  

So we can look at this diagram which illustrates bidirectional search. 
This is the start state 
(refer slide time [04:53])

 

and this is the tree which would be expanded if one starts from the start state and moves forward and one will be able to find the goal after examining these light grey states. 
In bidirectional search however, one would start both from the start state and examine certain portion of the start state. 

[05:32] 
At the same time he will also start from the goal state and move backwards until the search frontiers of the forward search as well as the backward search meet. And if they meet at a particular node we would be able to find a path from the path to the goal state. 
So we see that it is possible that in bidirectional search one many to expand fewer nodes than one would if one carried on forward search.

[06:11] 
However this may not always be the case. 
This is an example where the forward search is illustrated by this light grey envelope. 
So this is the envelope of forward search and this is the envelope of backward search. And we see that these two envelopes do not really meet so that the these paths are disjoined so we do not save on expanding any nodes if we do bidirectional search. 
(refer slide time [06:48])

 

In bidirectional search what we do is that we carry on the search process forwards from the start state as well as backwards from the goal and we alternate these two phases. 

[07:06] 
For example, we first expand some nodes in the forward direction and then expand some nodes in the backward direction then again in the forward direction again in the backward direction and so on. 
Every time we expand a node we need to check whether that node has been expanded in the other search tree. 
If it has been expanded then we would have found a path from the start to the goal. 
That is, we stop when the frontiers of the forward and backward tree intersect. 
(refer slide time [07:41])

 

However, in order to do the bidirectional search we have to start from a goal state. 
There are many problems where there are many goal states. 
It is difficult to decide which goal state we should start from. 

[08:01] 
Therefore bidirectional search works best if there is one goal state for the problem which is easy to get to. 
The second problem one might encounter in bidirectional search is, how do we search backwards? 
For all problems it may not be possible to search backwards. 
So being able to search backwards means we have these reversible operators.
That is, we can generate the predecessors of a state as well as the successors. 
In such problems bidirectional search is helpful.
Also we alternate from searching in the forwards direction and searching in the backwards direction and every time we expand a node we have to check whether that node occurs in the frontier of the other search tree. 

[09:06] 
Therefore for bidirectional search to work well we would need an efficient way to check whether a given node has already been expanded. 
If we take every node in the frontier and check whether it is the same as the correct node then any advantage we get by bidirectional search will easily be lost. And then for each of the forward search as well as the backward search we have to select a given search algorithm. 
However bidirectional search can sometimes lead to finding a solution more quickly. 
(refer slide time [09:50])

   

So let us see what we would gain if we use bidirectional search. 

[10:00] 
Suppose this is our start state and this is our goal state and let us say the branching factor is b and the distance from start to goal is d then breadth first search for example would expand order BFS will expand order bd nodes and
that would be the time and space complexity. 
In bidirectional search if we are lucky the forward tree and the backward tree will meet exactly half way. 
So this is the forward tree and this is the backward tree and if we are lucky we have found an intersection of the forward tree and the backward tree. Therefore we would get a path from the goal to this node as well as from the start to this node that is we will have a path from start to goal. 
Now in the best case this distance will be d/2 and also this distance will be d/2. 
So the number of nodes expanded in bidirectional search would be two times bd/2 which is better than o b to the power d nodes expanded by BFS. 
So, in bidirectional search in the best case we will get two times bd/2 nodes.
[12:06] 
However the space complexity is also bd/2 because we would have to store the frontier of at least one of the search trees. 
Like often what is done is we do breadth first search in one direction and DFS in the other direction. 
So at least for one of the search trees we must have a breadth first search type of procedure where we have a frontier which can be ordered bd/2.  
(refer slide time [12:35])

   

Let us just go back and compare the different line search methods we have considered so far. 
Breadth first search requires time of bd space of bd it is optimum and it is complete. 
Depth first search requires time of order bd space of bm where m is the depth of the search tree, it is not optimum and it is not complete. 
Iterative deepening search has a time complexity of bd and space complexity of only b × d where d is the length of the cheapest solution, it is optimum, it is complete. 
Bidirectional search when it is applicable might have a time complexity of bd/2, space complexity of bd/2 it is optimum you can show that and it is also complete. 
However in some cases unless you have a very efficient way of checking with the frontier of the other search tree they could be more overhead involved in trying to check if a node is there in the search tree. 
(refer slide time [14:14])

  

Now, we have so far considered search trees where we start from the search start state and unfold the tree we get. Now it is quite possible that the search space is more like a graph but not a tree.
That is, from the start state to a node n there could be multiple paths. 
If we look at the search space as a tree we might be getting to that node many times over and we are expanding a node more than once. 
So if you want to deal with this we must consider that the search space may be graph and that in this case the search tree may contain different nodes corresponding to the same state. 
(refer slide time [15:12])

 

[15:18] 
These are examples of some search spaces which contain nodes more than once. 
Look at this search graph: 
This is node a, this is node b, this is node c, this is node d and so on. 
there are two paths, two arcs from a to b, two arcs from b to c, two arcs from c to d and so on. 
Now if we unfold this as a tree this is a, this is b, this is c, c c c. 
So there would be 8 nodes corresponding to d, 16 corresponding to e and so on. 

[16:00] 
So if you have n nodes this tree will have size of the order of 2n so the search tree can be exponentially larger than the search space. 
(refer slide time [16:21])

 

Tree search methods are not very good for such types of search spaces. 
So, in order to have an algorithm which can work efficiently when the search space is a graph we need to avoid repeated state. 
There are different mechanisms we could use. 
For example, we can choose not to return to the state we just came from. 
This is a very simple trick that we often use in 8 puzzle. 
When we expand a node expand a node and generate its successors we do not generate its immediate parent. 

[17:04] 
By this we mean we can avoid some sort of duplication. 
A more sophisticated method is not to create paths with cycles in them. 
That is when we expand a node to generate its successor we check that the successor does not occur in the path from the node to the root. 
That is we do not generate in cycles. And the best way of doing this is not to ever generate a node more than once by checking whenever we generate a state whether it was ever created before. 
(refer slide time [17:51])

  

Unfortunately for doing this we must keep track of all the nodes that were expanded ever and we must check that this node was not expanded before. 

[18:02] 
We need to maintain a list other than the fringe in the search algorithms we have.
We usually call this particular list closed. 
So closed is a list which keeps track of the entire expanded node and whenever we generate a node we check whether or not it is already enclosed. 
This is the basic graph search algorithm which is a variation of the basic tree search algorithm that we looked at earlier. 
(refer slide time [18:43])

 

So fringe is the list containing the initial state. 
A fringe is often referred to as open. 
So these are the nodes which are in the frontier of the search tree and which are candidates for expansion. It is called the open list or the fringe. 

[19:01] 
Closed is another list which is initially empty and it will keep all the expanded nodes. 	
So the algorithm proceeds like this: We have a loop. 
If fringe is empty that is no more nods to expand we return failure.  
Otherwise we remove the first node from fringe and check that node is a goal or not.
If node is a goal we have found a goal state and we return by tracing the pointers and finding the path from the root to this node. 
Otherwise if node is not a goal node we put node in closed.
Generate all successors of node. 
Let us call them S. 
S is the set of successors of node. 
For all nodes m in S if m is going to be generated if m is not in closed then you add m to fringe but if m is in closed you ignore it. 

[20:06] 
Therefore we have modified the tree search algorithm for graph search. 
The basic difference is, we have included this closed list which was initially empty. 
Whenever we expand a node we put it closed. When we generate a new node we check whether it is already in closed. 
If it is closed then we do not generate it else we generate it. 
Now let us look at a variation of breadth first search which we call uniform cost search. 
As we discussed breadth first search expands node according to its level, level by level. 
So it expands nodes which are smaller number of steps away from the start earlier. 

[21:03] 
However sometimes arc costs are not uniform. 
So, instead of generating nodes level by level we would like to generate those nodes that have smaller cost from the parent. 
So we might like to generate nodes in the order of their distance from the parent and this is done by Uniform Cost Search UCS. 
(refer slide time [21:31])

   

In Uniform Cost Search when we put the nodes in the queue the order of the nodes by the path cost from the root to that node. But instead in breadth first search what we do is we maintain a queue for storing the fringe. 
That is the nodes expanded are added at the back of the queue. 
In Uniform Cost Search we maintain the fringe as a priority queue. 
The priority of a node is its distance from the root. 

[22:01] 
So, nodes are enqueued by the path cost for which a priority queue can be used. 
The heap data structure is ideal for storing a priority queue. 
Now we denote by g(n) the cost of the path from the start node to the current node n. 
So g(n) is the cost of the path from the start node to the current node. 
We store nodes in the priority queue according to the value of g. And the algorithm expands the lowest cost node of the fringe. 
The algorithm can be shown to be complete. 
It is optimal or admissible and it has exponential time and space complexity in the worst case. 
 As an example let us look at this search graph consisting of these five nodes. 

[23:04] 
This is the start node and this is the goal node.
In Uniform Cost Search we will start will the start state s. 
First generate A which has a g value of 1 then generate B which has a g value of 5. 
Then the candidates for expansion are g along this path with a g value of 13, 
g along this path with g value of 10, c along this path with a g value of 18. 
This is smaller so we will expand this g which has a g value of 10. And then we will try to find out which is the shortest path and then we will expand this node goal and we would have found the shortest cost path from the start state to the goal state. 
(refer slide time [23:58])

 
[24:01] 
As a result of breadth first search we can find the smallest length path from the start to the goal. 
If you do Uniform Cost Search you can find the minimum cost path from the start to the goal. 
Next we will come to Informed Search.  
(refer slide time [24:27])

 

In Informed Search we use heuristics about the problem domain. 
Uninformed search methods we looked at earlier systematically explore the state space to find the goal. 
They are not very efficient in most cases. 
We saw that most of the time the complexity of the algorithm was order bd that is exponential in the length of the search path. 
Informed Search method tries to improve problem solving efficiency by using problem specific knowledge. 

[25:07] 
Let us first try to see what we mean by heuristics.
Heuristics literally means rule of thumb. 
This is a definition of heuristics by Judea pearl. 
Heuristics are criteria, methods or principles for deciding which among several alternative courses of action promises to be the most effective in order to achieve some goal. In Informed Search we use heuristics to identify the most promising search path. 
(refer slide time [25:48])

 

Let us look at some examples of heuristic function.
A heuristic function at a node n which we will use for the purpose of the search algorithms is an estimate. 

[26:00] 
The heuristic function is an estimate of the optimum cost from the current node to a goal. 
We usually denote a heuristic function at a node n/h(n) which is the estimated cost of the cheapest path from node n to the goal node. 
For example, let us say we want to get a path from Kolkata to Guwahati, so here we have a map. 
This is Kolkata on the map and this is Guwahati on the map. And the actual path from Kolkata to Guwahati might be this. 
If you do not know what the actual path is, an estimate of the distance of this path is the Euclidean distance or the shortest distance between Kolkata and Guwahati. 
So this distance, the straight line distance between k and g is an underestimate of the actual distance of the path from k to g. 
(refer slide time [27:11])

 

[27:11]
Let us look at the game of 15 puzzle we discussed earlier. 	 
This is one configuration for the 8 puzzle. 
So this is a given configuration of 8 puzzle and this is the goal we are trying to achieve. 
One heuristic for 8 puzzle is the number of tiles out of place. 
What is the actual cost to get from n to goal? 
We have to take a number of move to get from n to goal and that is what we would like to find out. 
However it is not easy to look at this and say what would be minimum cost path. But what we can easily do is find out the number of tiles which are not in their correct location. 

[28:09] 

For example, look at 2 here, 2 is not in its correct location in state n. 
In order to move to its correct location we have to move 2 at least once.
Similarly 8 is not in its correct location, 3 is in its correct location, 1 is not in its correct location, 6 is not in its correct location, 4 is in the correct location, 5 is in the correct location, 7 is not in the correct location. 
So the heuristic at node n = 5 because five tiles are not in their correct location. And we must make at least five moves to move them to their correct location.  
So h(n) that is 5 is an underestimate of the actual number of steps required to move to the goal state. 
(refer slide time [29:10])

  

[29:10] 
Another heuristics for 8 puzzle is the Manhattan distance heuristic. 
For example, let us say 2 this node 2 is not in its correct position. 
In order to move to its correct position we have to move to one position to the right. 
However look at node 8. Node 8 is not in its correct position.
It has to move to this position. 
To move node 8 to this position we have to move one step down and one step to the left. 
That is, we need at least two moves. 
To move 6 to its correct position we need at least one move. 
To move 7 to its correct position we need at least one move. 

[30:04] 
To move 1 to its correct position we need at least one move. 
So h(n) in this case is, one move for 2, two moves for 8, one move for 6, one move for 7 and one move for 1 that is in this case it is equal to 6. 
Therefore this is an underestimate of the actual number of moves required to move from this state to this state. 
Now we will look at another search algorithm which uses this heuristic information. 
Best first search is a generalization of breadth first search where the fringe or the open list is maintained as a priority queue and a cost function f(n) is used which denotes the priority of a node. 

[31:14] 
So f(n) is the cost function of the node which denotes the priority of the node. 
(refer slide time [31:20])
 

Nodes are put in fringe sorted according to the value of f(n). 
So this is the modification of the basic search algorithm using the function f(n). 
Fringe is maintained as a priority queue containing the initial state. 
(refer slide time [31:41])

 

If fringe is empty return failure otherwise remove the element with the highest priority from fringe let that be node, if node is goal we return the path from initial state to the node otherwise we generate all successors of node. 

[32:00] 
Put the newly generated nodes into fringe according to their f values, 
end loop. 
So best first search is a variation of the search algorithm where fringe is maintained as a priority queue and nodes are put in the priority queue ordered by their priorities which is denoted by the f value of a node. 
So let us try to now look at different variations of best first search according to the function f that they use. 
The simplest algorithm we will look at is the Greedy Search algorithm. 
In Greedy Search we always expand the node with the smallest estimated cost to reach the goal. 
That is, the f value of a node is h value of a node. 
So h is a heuristic function. 

[33:01]
The h value of a node is an estimate of the actual path cost from the nod to reach the goal. 
So we use f(n) = h(n). 
However, we can show that this search algorithm is not optimum and is not complete. 
(refer slide time [33:20])

 

This is an example of greedy method. 
We are starting here from the start state which is Seattle and our objective is to reach Boston. 
So these are the arcs, these rectangles are the cities and the r x denote the cost between cities. 
Now, if we do greedy search we will evaluate a heuristic function at each of these nodes and from Seattle we will move to that city which has the smallest h value to Boston. and in this case suppose if it turns out to be Reno then from Reno we go to Memphis, Memphis to Atlanta to New York to Detroit and to Boston and we get a path like this. 
However greedy search may not always give us the optimum solution which we can show. 
 (refer slide time [34:27])

 

Next we will come to A* search which uses a slightly more sophisticated heuristic function. 
So in best first search f(n) = g(n) + h(n). 
(refer slide time [34:55])

 

We have seen that g(n) is the cost to get to the node from the start state and h(n) is an estimate to get from the node n to the goal state. 

[35:09] 
That is, g(n) is the sum of the edge costs from start to end and h(n) is an estimate of lowest cost path from n to goal.
Now it is a theorem and we will show that if h(n) is admissible then A* search which uses f(n) = g(n) + h(n) as the priority function which will find a optimum solution. And h(n) is admissible if it underestimates the cost of any solution which can be reached from node. 
So an underestimating heuristic function is an admissible heuristic function which gives an estimate which is a lower bound of the actual cost. 

[36:07] 
Now this is the algorithm A* for graphs. 
(refer slide time [36:16])

 

This is a generalized A* called graph search. 

Here we maintain two lists open and closed. 
Open maintains the nodes on the frontier of the search tree and closed maintains the expanded nodes. 
Initially we put the start set s on open. 
We also keep at every node a pointer to its path from the root. 
That is, we keep a pointer to its parent. 
So open contains the tuple s nil. 
Then we have a loop, while open is not empty we remove from open the node n, p which has the minimum value of f(n). 

[37:05] 
We put n, p on closed. 
If n is a goal node we return success and we return the path p for each edge connecting n and m with cost c. 
So n is the node that we are expanding and we find its successors. 
For each successor m of node n we check if m is in closed with a path q with the parent point q. 
So, if m q is on closed already and the current path the path from n is p.
So the current path for m is p concatenated with e. 
Therefore if the cost of the path p, e is cheaper than the path q then the current path to m is cheaper than the path we have already got so we will consider this path. 
However otherwise the path we obtained earlier for m was cheaper and we will throw away the current node m. 
So if m, q is already on closed and p concat e is cheaper than q we remove n from closed and put m, p concat e on open. 
Otherwise if m, q is not on closed but m, q is on open and p concat e is cheaper than q we replace q with p concat e. 
Otherwise if m is not on open we put m and the path p concat e on open and when open is empty we return failure. 

[39:05] 
So this is the generalized algorithm for A* search. 
(refer slide time [39:13])

 

We will show that A*is an optimum algorithm. 
That is it is also optimally efficient, it gives the optimum solution and it is also the optimally efficient algorithm. 
A* is complete. However the number of nodes searched is still exponential in the worst case unless the heuristic is extremely or logarithmically accurate. 
(refer slide time [39:46])

   

Now we will try to find out the condition on h(n) for which A* gives the optimum solution. 
So the property of admissibility is that, provided a solution exists the first solution found by the algorithm is an optimum solution. 

[40:07] 
If A* can guarantee this we will say that A* is an admissible algorithm. 
So in order to show that A* is admissible we will try to find out the conditions under which A* is admissible. 
Firstly, the state space graph should have this characteristic. 
Every node must have a finite number of successors. 
In the search tree or search graph every node must have a finite number of successors. 
Every arc must have bounded cost. 
That is, there exists an epsilon such that every arc cost is greater than epsilon. And thirdly we have a heuristic function h(n) which is always an underestimate of the actual optimum cost from n to goal. 

[41:06] 
We denote that by h*n. And h*n is the cheapest cost of a path from n to a goal node. And our heuristic function h(n) must be an underestimate to h*n. 
(refer slide time [41:26])

  

Now it can be shown that A* is optimally efficient. 
What do you mean by optimally efficient? 
For a given heuristic, given a particular heuristic function h of any optimal search algorithm that you could have that expand search path from the root node it can be shown that no other optimal algorithm will expand fewer nodes than A* on the average and still always guarantee in finding a solution. 

[42:09] 
Hence A* is an optimally efficient algorithm. 
Secondly, we will consider a heuristic function which has the monotonicity property. 
That is, along any path the f cost never decreases. 
The f cost of different nodes along a path can only increase but it cannot decrease. 
So many heuristic functions h n satisfy this monotonicity property. 
However even if the heuristic function does not satisfy this property we can easily enforce this property if we have an underestimating heuristic function by doing the following trick. 

[43:01] 
Suppose we have a node n and m is a child of n.  
So f(m) is normally g(m) + h(m) and at node n we have f(n). 
Now we can say that we will use as f(m) the maximum value of f(n) and g(m) + h(m). 
That is, if g(m) + h(m) is smaller than f(n) we will use f(m) = f(n) to ensure that the monotonicity condition holds. 
We can do this because the f value of a node is an underestimate of the cost from start to goal through this node. 
The f value of this node m is an underestimate of the cost from start to goal through n and m. 

[44:08] 
So, if f(n) is an underestimate of this cost we can also use f(m) here without sacrificing the admissibility condition. 
Now let us look at the proof that A* is admissible. 
That is, A* always finds an optimum path to the goal. 
(refer slide time [44:40])

 

Suppose g is a goal state and f* is the optimal path cost for the algorithm A* so we have this start state s, 
g is the nearest goal for which there is a path whose cost is f*. And suppose we have this other goal g2 which is sub optimum and there is a path from s to g2 and the cost of this path is greater than f*. 
Now we will try to show that it is not possible that A* algorithm will find g2 first before it finds g. 
In order to prove this let us start with the opposite contradiction that, suppose A* has selected g2 from open for expansion. 
(refer slide time [45:51])

 

Suppose A* has selected g2 from open for expansion, now we have our node, this is my start state, this is g and this is g2. 

[46:08] 
Suppose if g is not on open there must be at least another node on this optimum path which is on open. 
So the node n of the optimum path must be on open. 
Now, suppose our algorithm A* has selected g2 for expansion and not selected n. 
 If n is on the optimum path from s to g then f(n) must be less than equal to f*. 
that is, f* is the cost of the optimum path from s to g and f(n) which is an underestimate must be less than equal to f*.

[47:01] 
Now, if g2 is chosen for expansion and n is not chosen for expansion it must be the case that f(G2) is less than equal to f(n). 
That is, f(n) is greater than equal to fg2. 
Now, because g2 is a goal state f(G2) is simply g(G2) because h value of a goal state is 0. 
Therefore it follows from these three conditions that g(G2) = f(G2) less than equal to f(n) less than equal to f* that is g(G2) is less than equal to f*.. 
So, if A* has to select a goal node for expansion while the actual optimum goal has not yet been selected it must be the case that g(G2) is less than equal to f*. But this is a contradiction because we just said that g2 is a sub optimal goal so this cannot happen. 
That is, when A* selects a node for expansion and if that node happens to be a goal it must an optimum goal state.  
(refer slide time [48:28])

 

So this is a sketch of the drawing. 
This is our s, this is g and this is the node on this optimum path which is on open when g2 is selected. And we just showed that f(G2) = g(G2) which is greater than g (G) by resumption which has to be greater or equal to f(n) so this cannot happen. 
Therefore A* does find the optimum solution. 

[49:00] 
Now we have seen that A*is complete is Optimum.
We have to show that A*is complete. 
Now suppose g is an optimum goal state 
(refer slide time [49:16])

   

what are the conditions under which A* is not complete? 
A* will not be able to read g only if there are infinitely many nodes for which f(n) is less than equal to f*. 
For expansion A* only selects those nodes which have f value is less than equal to f*. 
So A* will not be able to get to the goal node if there are infinitely many such nodes. 
This can only happen if either we have a node with infinite branching factor or we have a path with finite cost but infinitely many nodes. 

[50:05] 
The first condition a node with infinite branching factor cannot happen because we assumed that the branching factor is finite. 
(refer slide time [50:16])

 

The second condition cannot happen because we assumed that all our costs are bounded and they are always greater than epsilon. 
So there cannot be infinitely many nodes on the path. 
Thus A* is complete. 
(refer slide time [50:38])

 

This lemma states that A*expands only those nodes whose f value is less than equal to f*. 
In fact A*expands nodes in order of increasing f values. 
So we start from this node this start state and A* expands node with increasing value of f values. 

[51:09] 
 Properties of heuristic functions: 
Suppose we have two heuristic functions h2 and h1 we say that h2 dominates h1 if the value of h2 at any node n is grater than the value of h1 at that node n and we can show that A* will expand fewer nodes on average using h two than when using h1. 
(refer slide time [51:40])

  

Now let us show the proof sketch, the proof of this: 
A* with h1 is the heuristic function expands every node for which f(n) less than f*. 
So every node for which f(n) less than f* will be expanded. 
(refer slide time [52:03])

 

[52:03] 
Some nodes where f(n) = f* some of these nodes will be expanded. 
A* with h2 as the heuristic will expand all nodes with f2 and f*. 
So, if h2(n) is greater than equal to h1(n) the nodes which A* with h2 will expand must be a subset of the nodes which A* with h1 can expand. 
However depending on the execution those we might have some leeway for those nodes where f(n) = f*. But on an average because A* expands those nodes whose h value is less than f* – g(n). 

[53:00] 
So, if h2 is greater than h1 at all nodes then A* with h2 will expand fewer nodes. So A* with h1 is preferred to A* with h2. 
So long as the heuristic is an underestimating heuristic we prefer that heuristic function which gives a higher estimate. 
(refer slide time [53:30])

  

Suppose we have identified a number of underestimating heuristics for a problem where h1(n) h2(n) hk(n) and so on so we can combine this multiple heuristic functions by taking the maximum of h1 h2 and hk. 
So we can find the values of h1 h2 hk at that node and since all of them are underestimates we can take that value which is maximum. And this happens to be a more powerful non overestimating heuristics. 

[54:10] 
Now we will quickly look at a variation of A* which is similar like iterative deepening search. But instead of using a depth bound we use an f limit. 
Initially we start with limit, the f limit = h value of the start node and then we do a depth first search. And we prune any node for which f value of the node is greater than the f limit. 
(refer slide time [54:40])

 

We set the next f limit to be the minimum cost of any pruned node. 
So we start from node a here we have to reach node d which is a goal node. 
We first set the f limit as 15 and expand this node in a depth first manner. 

[55:00] 
Then we set the f limit as 21 and we expand these nodes and then we change the f limit and then we expand all the other nodes. 
So, iterative deepening A* can be shown to be complete and optimal. 
However, because we use depth first search the space usage is proportional to the depth of the solution and it saves some space. 
The number of nodes expanded, it expands some nodes more than A* but usually it is of the same order as nodes expanded by A*.
So in general if there are lot of possible f values in the search tree IDA* can generate square of the number of nodes that A* generates. But where many several nodes share the same f value IDA* will typically expand a constant order of nodes more than A*.
(refer slide time [56:14])

 

[56:14] 
Therefore in 8 puzzle we have few values of f because they are all integers, 
all the f values are integers so there are a few values so IDA* is quite efficient.
(refer slide time [56:30])

 

In traveling salesman problem each f value is unique if the path costs are real numbers. And the number of nodes expanded will be order of n2 because it would be the case that at every iteration one more node is expanded. 
So the total number of nodes expanded is 1 + 2 + 3 + up to n which is of the order of n2. And it is very difficult, if we are using depth first search we really cannot detect nodes which have been expanded before. 

[57:01] 
Therefore it is not a very good choice if we have a search graph. 
Otherwise depth first search is very attractive because the space requirement is small. 
(refer slide time [57:22])

  

We will stop this lecture by quickly running greedy search as well as A* on this graph. 
We start from start state from a.
If we do greedy search we will try to go to that node for which h value is smallest. 
Suppose we choose as h value the straight line distance from a node to the goal node. 
This is my start node, this is the goal node 
in greedy search from a we will go to b, from b we will go to e, from e we will do g, from g we will go to the goal node h. and we get this path whose cost is 70 + 15 + 6 + 8 = 85, 91, 99.. 

[58:02] 
This path is not optimal. 
Now let us take the same graph and run A*.
We first expand node a and then we add node b to the fringe whose g is 8, h is 18.6 and f is 26.6. 
(refer slide time [58:15])

  

Then we expand b and then we add c, d and e to the fringe. 
For c the f value is 35.1 
We put it in the fringe 
We evaluate d, f value is 35.2. 
Then we evaluate e with f value 27.5 then we expand e add successors g and c evaluate g and g(n) = 84, h(n) = 8.5, f(n) = 92.5 
We add it to the fringe then we can take c whose f(n) = 41.3 which is more than the f value of c which you already have on the fringe we discarded. And then we expand c add node f whose f value is 37. 

[59:11] 
We put it in the fringe, then we expand d which has no children then we expand f 
add nodes g and h to the fringe. 
We evaluate g whose f value is 42.5 we replace the f value. 
Then we evaluate h whose f value is 39 we put it in the fringe we expand h which is a goal node and we have found a goal whose cost is 39 which is better than the cost of 99 which we found in the greedy search and this is the optimum path to this goal node. 
(refer slide time [59:50])

Artificial Intelligence (Lecture – 6)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

Informed Search -2

[01:07] 
Today we start our 6th lecture. 
This is the 2nd lecture on Informed Search. 
In the last class we looked at the different properties of A*
Today we will continue with A* and talk about another algorithm called IDA*.
(refer slide time [01:35])
 

The instructional objectives of today’s lecture are the following: 
In this lecture the student will learn the algorithm IDA* which is an algorithm for Informed Search. 
They will be able to analyze the space and time complexity of IDA* and they should be able to understand the difference between IDA* and the algorithm A*.
The student will also get an introduction to some of the restricted memory search algorithms like RBFS that is recursive best first search as well as MA*.[02:15] 
We will also discuss briefly some local search algorithms like hill climbing as also simulated annealing. 
The students will get a brief idea about these algorithms. 
They will also have an understanding about the properties of search algorithms. 
(refer slide time [02:35])

   	

Let us just quickly go over A* search. 
So A* is a best first search where we use the function f(n) = g(n) + h(n) to order the nodes for expansion. 
The nodes are expanded according to their increasing f values. 

[03:04] 
We have also seen that in case h(n) is admissible then search will find the optimal solution. 
A heuristic function h(n) is said to be admissible if it underestimates the cost of any solution which can be reached from the node. 
(refer slide time [03:30])

 

So, to briefly recapitulate algorithm A* we have two lists open and closed. 
If we want to deal with a state space containing repeated states and we want to ensure that a particular state does not get expanded multiple times we need to keep along with the open list all other lists as closed which consists of nodes which have already been expanded. 
Open is the list of nodes on the frontier of the search tree. 

[04:02]
 Initially closed is empty and open has a single node the start node and the path associated with the node is an empty path. 
The algorithm A* executes this loop. 
While open is not empty certain things are done. 
So in this loop that we will see in the next slide we try to see if we can find the path from the start node to a goal node. If we do not find such a path in this loop and we have no more lists no more nodes to expand. That is, open becomes empty, the algorithm should return failure. 
Now this is what we do in the body of this loop. 
(refer slide time [04:54])

  

We remove from open the node n, p whose f(n) value is minimum. 

[05:01] 
So n is the node, p is the path associated with the node, n is that node whose f(n) value is minimum, we put n, p on closed because it has been expanded. 
if(n) is a goal node we return success and we return the path b which is the path associated with this loop. 
Otherwise we find all edges coming out of the node n that is edges connecting node and with some other node m. 
Suppose there is edge e connecting n with m whose cost is c. 
Now we consider this state m. 
If m is already on closed with the path q and our current path p concat e happens to be cheaper than the cost of the path q then we will like to consider this current path as the best path here. 

[06:02] 
If q is cheaper we throw away our recorder path. But if m, q is on closed and the current path is cheaper than q we remove the node n from closed and put the new node the current node whose state is m whose path is p concat e we put this node at open. 
Otherwise if it is the case that a node m is on open already with a different path q and the current path cost is cheaper than the cost of q then we replace q with the current path p concat e. 
Otherwise if m is not on open and not on closed we put m, p, e on open. 

[07:00] 
So this algorithm is expanded in this section. 
(refer slide time [07:05])

 

In the last class we have done an analysis of the algorithm A* and we have seen that A*is an optimum algorithm. 
That is, if there is a path from the start state to the goal state then A* is guaranteed to find the optimum path and optimum path to the goal node. 
We have also seen that all algorithms that use the same heuristic function h(n) A* is an optimally efficient algorithm. 
A* is also complete. 
That is, it finds a path if one exists. And the number of nodes searched by A* is exponential in the worst case. 
(refer slide time [08:06])

 

(refer slide time [08:10])

 

So, if we model the search space and assume that the search space is a uniform b r e tree and there is a unique start state s and one goal state g whose distance from the start state s is N and every arc has a cost of 1 then we can show that the number of nodes expanded A* is exponential in n unless the heuristic estimate is logarithmically accurate. 
That is, suppose A*n is the actual cost from n to the goal, the optimum cost of n to the goal and suppose h(n) is our estimate. If h(n) – h*n is less than or of the order of log of h*n then the heuristic is said to be logarithmically accurate. And in this case A* has a polynomial complexity on such a search tree. 
However, in most cases we can say utmost that practically the heuristic functions we can get are utmost proportional to A*m. The error is proportional. Error is not logarithmic and in those cases A* has exponential complexity. 

[10:00] 
So A* has exponential complexity. 
As we have seen A* tries to improve upon general breadth first search by using this heuristic function to restrict the number of nodes being expanded. 
(refer slide time [10:21])

 

However this restriction is not good enough because the complexity of A* is still exponential.
If we get a better heuristic function then the number of nodes in expansion can become less. 
So A* still can take a lot of time on a reasonably large problem. But even worse than the time complexity of A* is the fact that it becomes difficult to use A* even before the time runs out because the size of the open queue becomes too large because the space complexity of A* is the space required for storing the open queue. And in those cases where we use the closed queue we also have to store the closed. 

[11:13] 
Now the solution so that A* does not have this bottleneck or such algorithm does not have this bottleneck is to have a search algorithm which uses less space.
Now we will discuss an algorithm IDA* which is Iterative Deepening algorithm. And IDA* requires less space. 
We have already talked about Iterative-Deepening search which is basically search which does a level by level expansion of the search tree. And we have seen that Iterative-Deepening depth first search has a space complexity of order b, d where b is the branching factor and d is the depth of the tree. 

[12:04] 
Here d is utmost the N which is the cost of the solution. 
So, if we use Iterative Deepening search we can have an algorithm which works in linear space. 
However Iterative Deepening A* is actually slightly different from Iterative-Deepening search. It is actually very similar where the nodes to be expanded in a particular iteration depend of the f value of the nodes and not on the level of the nodes. 
So, in Iterative-Deepening A* the cutoff for nodes expanded in an iteration is decided by the f value of the nodes. 
Therefore we fix a limit of the f value and in an iteration we only expand those nodes in a depth first manner whose f value is less than or equal to this f limit. 

[13:02] 
In the next iteration we increase the value of this limit. 
Now let us look at Iterative-Deepening A*.
(refer slide time [13:22])

  

In Iterative-Deepening A* is a depth first Iterative-Deepening search but the depth bound used is a f limit. 
So we start with the initial limit as the f value of start. 
We use a f value of start as the initial cutoff or initial limit. 
Then with that limit we do a depth first search. And in doing this depth first search we prune any node if f of that node is greater than the value of f limit.  

[14:11] 
So, after the current iteration the next f limit is chosen to be the minimum cost of any node that was proved. 
Now let us try to illustrate Iterative-Deepening A* on the following graph. 
We have six nodes a b c d e f. 
Initially we fix a f value equal to the f value of the start node. 
Suppose the start node is a and suppose the f value of the start node is 15 
so using the f limit as 15 we do a depth first search in which a, b and e are expanded. 
Now c and f were the nodes which were pruned in this first iteration and suppose out of f value of f and f value of c the f value of c was minimum and it was 21 
And at the next iteration we will use the f value of 21 and we will do a depth first search with this f limit equal to 21. And in the next iteration c was the node pruned with a particular f value. 
We will set that equal to the f limit and do the next iteration and so on. 
So this is the schemata of Iterative-Deepening A*.
 Basically it is Iterative Deepening search where search is pruned when the f value of a node exceeds the current f limit. And the minimum f value of a node pruned is used as the next value of f limit. 

[16:07] 
Now let us look at the properties of the algorithm IDA*.
(refer slide time [16:13])

 

IDA* is complete and it is optimal. 
It is complete because it will find a solution if one exists. 
It is optimal because the first time it expands a goal node it would have found an optimum solution. 
The space usage is proportional to the depth of the solution. 
Each iteration of IDA* is a depth first search so it requires only space equal to b times d, the branching factor times the depth of the tree so space requirement is linear. 
Now you note that what are the extra nodes which are expanded in every iteration? 
They are those nodes whose f value is immediately just greater than the f value of the nodes in the previous iteration. 

[17:09] 
So the number of nodes expanded in subsequent iterations is the nodes with increasing f values. Hence IDA* is an optimal algorithm. It is obvious that IDA* expands more nodes than A* because certain nodes are expanded more than once. 
How many more nodes IDA* expand compared to A*?
That depends on the number of unique values of f(n).
So, if it is the case that between two iterations the extra number of nodes expanded is large then IDA* is fairly efficient in the number of nodes expanded. But consider the case when every node has a unique f value. 

[18:07]
 In that case at every iteration only one new node will be expanded and in this case IDA* will expand the square of the number of nodes in A*. 
(refer slide time [18:23])

 

So space required by IDA* is O(bd), number of nodes expanded relative to A*.
In 8 puzzle there are few values of f and the number of nodes expanded by IDA* is very close to the number of nodes A* expands. 
Suppose in 8 puzzle we use a Manhattan distance or the number of misplaced tiles as the heuristic function the number of such values such a heuristic can take is very small, the number of misplaced tiles can be utmost 8. 

[19:00] 
So the total f value is actually also not very large. 
As a result the number of unique m values is small. 
So IDA* tends to expand a large number of extra nodes in every iteration. And the number of iterations is equal to the number of distinct f values. 
However consider the traveling salesperson problem where we have a graph of n cities and the objective of the graph in salesperson is to start from a city traverse all the cities exactly once and come back to the starting city. 
Imagine that every distance between a pair of cities is a different real value.
In this case it is possible that each f value is unique. 
So IDA* will expand one new node in every iteration and the number of nodes expanded would be the first iteration 1 second iteration 2 third iteration 3 and in the last iteration the number of nodes expanded will be equal to n. 

[20:14] 
So the total number of nodes expanded equal to O(n2) where n is the number of nodes A* expands. 
So we see that IDA* can expands utmost a quadratic number of nodes expanded corresponding to A*.
However in other situations like 8 puzzle IDA* would expand only a constant factor more nodes than A* would expand. 
Also we see that because in IDA* basically we are using a depth first search we cannot avoid repeated node expansion so IDA* does expand duplicate nodes in cyclic graphs. 

[21:07] 
If a graph contains cycles IDA* will not be able to detect the cycles and it will expand those nodes repeatedly. But the main advantage of ID* is its linear memory. 
For example, 15 puzzle is a problem similar to eight puzzle except that the problem is on a 4 × 4 grid, there is one empty square. 
So if you run A* on 15 puzzle it will take a huge amount of time to solve and a huge amount of memory. 
And 24 puzzle possibly you cannot solve in A* using normal resources because of the huge space requirement and time. 
However, you can use IDA* which does not need to keep this open queue which will be able to solve 8 puzzle and 15 puzzle. 

[22:11] 

 IDA* uses very little memory, it only uses linear memory and it may expand up to n2 nodes if A* expands n nodes. 
(refer slide time [22:18])

       
The idea of memory limited heuristic search algorithms is to use the available memory optimally so as to have a more efficient algorithm which reduces node expansion. 
Several such algorithms have been proposed and we will briefly mention these algorithms. 
Recursive Breadth First Search or RBFS is an algorithm actually uses only linear space but it is often better than IDA* because it mimics best first search. 

[23:14] 
RBFS keeps track of the f value of the best alternative path available from any ancestor of the current node. 
So whenever RBFS is a linear memory algorithm it does not keep track of all the nodes but those nodes that it has expanded and the algorithm chooses to forget. The algorithm backs up at a node the f value of the best successor of that node. 	 
So, at a particular node we have current f value and we also know what is the second best f value based on what we had expanded earlier. 

[24:13] 
When the current f value this alternate f value then RBFS will explore this alternate path. 
Let us now see another algorithm MA* and its related algorithm SMA*.
(refer slide time [24:39])

 

These are restricted memory best first search algorithms that utilize all the available memory. 
The algorithms execute best first search when memory is available. It does normal best first search. 
When the memory becomes full the worst node is dropped and the value of the forgotten node is backed up to the parent. 

[25:06] 
When the node is dropped the value of this backed up node is backed up to the parent. 
We have seen systematic search algorithms: depth first search, breadth first search, A*, IDA*, bidirectional search and so on. And we have seen that A* and IDA* and such algorithms or even RBFS, MS* use some heuristic function to restrict the number of nodes that are expanded. And the most natural types of heuristic functions we can obtain is that these search algorithms have exponential time complexity. 

[26:05] 
There are situations where it is not possible to use such algorithms to get the best solution to a problem. 
(refer slide time [26:17])

 

Local search methods are used in a class of search algorithms where we need not have an optimum solution. 
There are problems where we need a solution and we can get a solution which may not be the optimum solution but we can get the solution in limited time. 
Local search methods work on formulations where in every configuration we have a complete state. 
Therefore local search does a search on complete state formulations.  

[27:02] 
In local search typically constant number of nodes is kept in memory and these nodes are all configurations of the entire state. 
These states are perturbed to get the next state. 
Local search is useful for two types of problems. 
One it is useful for solving optimization problems in those optimization problems where it is often easy to find a solution but hard to find the best solution. 
Consider the traveling salesperson problem in a fully connected graph. 
It is easy to get one solution to the problem because any permutation of the city is a solution but it is difficult to get the best solution because there are n factorial such permutations and we do not know one efficient way of finding the best permutation in polynomial time. 

[28:08] 
So in TSP the objective is to find the optimum configuration which is difficult to find. 
We do not know to find a good algorithm but if we just want to find a solution then it is easy. 
(refer slide time [28:26])

 

Many local search algorithms use iterative improvement and we can try to get a solution and get improvements to that solution. 
Secondly, for these optimization problems like TSP we do not have to keep track of the path to the solution. 
When we have the solution we immediately know the path. 
in TSP we do not have to keep track of the path. 
Local search is ideal for such cases. 

[29:01] 
In problems like 8 puzzle the final state does not give us any information about the path. 
The solution path has to be obtained. Local search is not very good for such problems. But for problems like TSP where the path is not important local search is very useful. 
Another example of a problem is the n queens problem where we have a n/n chess board and we have to put n queens on the board so that none of the queens are attacking each other. 
For this problem once we have the final configuration we know the solution. 
So n queens problem is not obvious how we can get a solution. So we can define n queens as an optimization problem.

[30:02] 
So the basic idea of the iterative methods is to start with the solution and improve it so that we can get a better solution. 
(refer slide time [])
 

Example is the n queens problem. 
We have to put n queens on the chess board. 
So in this case we have to put 4 queens. 
What we can do is that we can start with any placement of the 4 queens on the board. But if you place the queens like this obviously there are many conflicts. 
(refer slide time [30:32])

 

We can reformulate our objectives as saying we want to have a board with 0 conflicts. Or we can say we want to minimize the number of conflicts. 
That is we want to minimize the number of pairs of queens which can attack each other. 
So we start with this state where this pair is attacking each other, this pair is attacking, this pair attacking, this pair is attacking, this pair is attacking. And then we move to this configuration where this is attacking, this is attacking, this is attacking. And then we move to this configuration where this pair is attacking each other and none of the other pairs are attacking each other. 

[31:16] 
So we have reduced the number of conflicts between queens. 
So hill climbing is one local search method. 
It is also called gradient ascend or gradient descent. 
In hill climbing the idea is that, we start with a current configuration and we move to a new configuration to maximize the value. 
For example, in n queens our objective is to minimize the number of conflicts. 
So we start with an arbitrary configuration and we move to a new configuration where the number of conflicts is reduced. 

[32:05] 
So we find from the current configuration different configurations of different neighbors and we move to that neighbor which has the minimum number of conflicts. 
So we move to the neighbor which is best. 
That is why it is called hill climbing.
We go to the best neighbor and we continue until we get to a state where all the neighbors are worse than this state. 
We can talk about hill climbing when we are either minimizing a value function like minimizing the number of conflicts or maximizing the value function, they are symmetric. 
For example, in 8 queens we can start with an initial state and we can define our successor function as moving a queen to another square in the same column. And the cost associated with the configuration is the number of pairs of queens that are attacking each other. 
(refer slide time [33:08])

  

[33:08] 
We want to minimize these pairs. 
Now, this is a schematic diagram of the state space of hill climbing. 
We are starting at a particular position and we want to move to a neighbor whose value is large. 
(refer slide time [33:30])

 

Now depending on which initial state we start from we can get stuck in local extremum.
Suppose we start from this position and then we find that this is the best neighbor and then we find this is the best neighbor so we can move until we get here whose neighbors are all worse than this. 
So this is the solution that we have obtained and this also happens to be an optimum solution. 

But suppose you start from here and then if you move to the best neighbor and then its best neighbor and then here you will get to this position where all its neighbors are worse but nevertheless it is not the optimum solution to this problem. 
So, if we use hill climbing because it is a local search method we can get stuck at a local extremum instead of all this landing up at the global optimum. 
(refer slide time [34:41])

 

So we can compare our state space to that of a physical system that is subject to natural interaction and we can look at the analogy of our value function to the overall potential energy of the system. 
On every updating we have delta e less than equal to 0. 

[35:00] 
So this is the change in potential energy. 
(refer slide time [35:08])

 

Now in this schema of the value of the state space in this state this is our global optimum. 
This diagram is for a minimization problem where c is the global minimum and what we see is that, if we start at any of these positions we can get to the global minimum. 
So this region is the region of attraction the basin of attraction for c. 
So, if our initial starting is between these two lines then we can reach this global minimum. 

[36:05] 
However if our initial starting position is somewhere here we cannot reach this global minimum but we will instead settle to a local minimum. 
Therefore in local search we cannot guarantee global minimization because of existence of many local minima in general. 
Now the question is, so if you use hill climbing you cannot avoid this problem 
how can you at all avoid this problem. 
Let us look at an example: 
Suppose this is our value function at different states and this is the starting point. 
(refer slide time [36:57])

 

[37:00] 
Now if you start at this starting point and we use hill climbing or hill descending this is the direction of steepest descent and this is where our ball will roll until it settles at the local minimum. 
So this hump is acting as a barrier to the local search and our ball is settling at the local minima instead of going into the global minima. 
(refer slide time [37:30])

 

Now, if you want to avoid this problem we must let our ball also go in the sub optimum direction in order to get to the local minimum. To the global minimum it may be necessary to climb the hill at certain points even though you want to reach the deepest valley. 
So, occasional ascents are required so that the ball can get over this hump and settle at a minimum which may be global or local. 

[38:07] 

So, ascent will help escape the local optima. But if we allow ascent may also help the ball get past the global optima after reaching it. 
We can avoid this by keeping track of the best state. 
if you start from here and you also allow ascents and then we keep track of the best state reached so far we may be able to identify the best state which this ball has reached. 
(refer slide time [38:45])

 

Simulated annealing is an algorithm which avoids some pitfalls of hill climbing and the basic idea is this. 
From the current state we pick our random successor state. 

[39:00]
 If that successor state has a better value than the current state then we accept the transition. 
If it has a worse value we do not give up but with some probability we accept a worse transition. 	
(refer slide time [39:21])

 

If we get a successor whose value is worse we flip a coin and accept the transition with some probability. 
Sometimes we accept non optimal solutions. 
Initially with a higher probability we accept non optimal solutions. 
As time passes with smaller probability we accept non optimum solutions so that we allow our ball to settle at a minimum. 
There are other local search algorithms like genetic algorithms. 
[40:05] 
 What we will do now is discuss the solutions to the questions of lecture 4. 
This was the question. 
You are given a full 5 gallon jug and an empty 2 gallon jug. 
Your objective is to fill the 2 gallon jug with exactly 1 gallon of water. 
(refer slide time [40:28])

 

(refer slide time [40:33])

 

You use the state space formulation like this. 
A state is represented by a paid x, y where x is the number of gallons of water in the 5 gallon jug, y is number of gallons of water in the 2 gallon jug. 
Initial state is first the jug contains 5 gallons and secondly the jug is empty so 5, 0. 
Goal state is *,1 that is the second jug must have 1 gallon. 

[41:01]
 The first jug can have any value we do not care. 
For this problem you have to create the search tree and discuss which search strategy is appropriate for this problem. 
Let us look at the solution to this problem. 
(refer slide time [41:15])

  

Now this table illustrates the different operators and the effect they have. 
These are the operators. 
Empty 5 gallon jug, empty 2 gallon jug, transfer from 2 to 5, transfer from 5 to 2,
transfer from 5 to 2 partially, now empty 5 so these are the pre conditions. 
Empty 5 can be always used and as a result you move from the states x, y to the state 0 y because this jug becomes empty. 
Empty 2 that is the empty 2 gallon jug if you apply it on state x, y you get this state x0. 

[42:05] 
And 2 to 5 is the operator to pour 2 gallon into 5 gallon. 
If you start with x2 you get to (x + 2, 0) 
So pour 2 gallon into 5 gallon so this 2 gallon comes here. 
If you pour from 5 to 2 you can apply this only if x is greater than 2. That is, the 5 gallon jug initially has more than 2 gallons of water. 
In that case x0 will give you (x – 2, 2). 
And 5 to 2 partially you can apply when y has less than 2 gallons of water. 
In that case 1y will become (0 y + 1) so you pour the partial 5 gallon into 2 gallon. 
This table shows the effect of all the transitions. 
(refer slide time [43:11])

 

Now these are the different pairs of states 0, 0 0, 1 0, 2 1,0 1,1 1, 2 2, 0 2, 1 2, 2
all these are the states of our jug problem. 
(refer slide time [43:25])

 

Now the empty 5 takes you from 5, 2 to 0, 2 from 4, 2 to 0, 2 2, 2 to 0, 2 and so on. 
It also takes you from 5, 1 to 0, 1 3, 1 to 0, 1 2, 1 to 0, 1 from 0 to 0, 0 and so on. 
The next operator is empty 2 which takes you from 5, 2 to 5, 0 4, 2 to 4, 0 5, 1 to 5, 0 3, 1 to 3, 0 and so on
And 2 to 5 takes you from 3, 2 to 5, 0 4, 1 to 5, 0 2, 2 to 4, 0 3, 1 to 4, 0 and so on. 

[44:00] 
And 5 to 2 takes you from 5, 1 to 4, 2 5, 0 to 3, 2 4, 1 to 3, 2 and so on. 
So 5 to 2 part takes you from 2, 2 to 2, 0 2, 1 to 2, 0 and so on. 
Now let us see how from this state space we have to find the solution from the starting state to a goal state? 
(refer slide time [44:36])

 

This is the state space we start with and this yellow box is our starting state. And now if we do a search we find that the optimum solution will go from 5, 0 to 3, 2 then from 3, 2 to 3, 0 from 3, 0 to 1, 2 and from 1, 2 to 1, 0 from 1, 0 to 0, 1. 

[45:17] 
So, we have a solution which requires five steps. 
(refer slide time [45:20])

 

Now let us see which search algorithm is appropriate for this problem. 
Depth first search is not appropriate because as you can see the state space is a graph and there are many repeated states so depth first search is not appropriate. 
Breadth first search is appropriate. 
(refer slide time [45:43])

 

Now let us look at the second question from lecture 4. 
You are given the following graph: 
Starting from state A you have to execute depth first search and reach the goal node G and you have to show the order in which the nodes are expanded. And the third question was you run Iterative-Deepening search on the same graph. 
(refer slide time [45:49])

 

[46:05] 
We will see the solution to this problem. 
This is our graph and this is the search tree we obtained when we looked at these states. 
(refer slide time [46:21])

 

After we unfold this graph this is the tree we get. 
So, if you depth first search, initially our list open, that is fringe contains the node A.
Then A is removed from the fringe, B and C are expanded and put in open. 
Open is a stack when we do breadth first search. 
Then B is removed from the front of the stack and D and E are added to the front of fringe. 
When the first node from fringe D is removed and a successor F is added to the front. 

[47:00] 
Then F is removed from fringe its successor H is added, then H is removed and then E is removed. 
Then C is removed its successors D and G are added in the beginning 
then D is removed from the front its successor F is added, F is removed H is added H is removed G is removed and we have found a goal state. 
By depth first search this is the order in which the nodes are expanded 
A B D F H E C D F H and G. 
Now let us run Iterative-Deepening search on the same state space. 
Initially the limit is 0 so we can expand A only. 
Then we set limit equal to 1 and we get this tree which is expanded in a depth first manner that is A then B then C. 

[48:02] 
So this illustrates the second iteration of IDS. 
In the third iteration limit is set to 2 and in this iteration this tree in this iteration this tree will be searched in depth first manner. And when we do search this tree we get, first A is removed from fringe and then B and C are added. 
B is removed D and E then D is expanded E is expanded C is expanded 
D and G are added to fringe D is expanded and finally G is expanded and then we have found a goal node in this iteration where limit is equal to 2. 
(refer slide time [48:49])

 

For lecture 5 these are the questions: 
(refer slide time [49:15])

 

[49:16] 
You will be given a following search space. 
This search space is specified by this state. 
(refer slide time [49:25])	
 
In this state space there are seven states 
A B C D E F G.
For state A there is a edge to state B with cost 4, there is an edge from A to C with cost 1, there is an edge from B to D with cost 3, B to E with cost 8, 
C to C cost 0, C to D cost 2, C to F cost 6, D to C cost 2, D to E cost 4, E to G cost 2 and F to G cost 8. 

[50:14] 
So, given this state space you are required to draw the state space of this problem. 
We assume that the initial state is A and the goal state is G. 
For this state space you have to trace each of the following search strategies to create a search tree and find a path from initial state to the goal state. 
(refer slide time [50:44])

 

So you are required to use this state space and run uniform cost search greedy search as well as A* search. 
For each of these three algorithms we will have their execution and find the order in which the nodes are expanded. 

[51:02] 
At each step of the search algorithm you have to show which node is being expanded and the content of the open list of fringe. 
You also have to report the eventual solution found by each of the algorithms and the solution cost that is obtained. 
(refer slide time [51:24])

 

 Questions from lecture 5: 
You are required to write the algorithm for bidirectional search using pseudo code. 
We discussed bidirectional search in the last lecture but you will have to provide the pseudo code for this algorithm. 
You assume that each search is a breadth first search and the forward and backward searches alternate and they expand one node at a time. 
 (refer slide time [51:59])

 

Third question: 

[52:10] 
You have to find the time complexity of bidirectional search assuming that the test for connecting the two searches is done by comparing a newly generated state in the forward direction against all the states generated in the backward direction one at a time. 
So you remember in bidirectional search you have to check whether the two frontiers of the forward tree and backward tree are met. 
Suppose you do this check by checking the node expanded in the forward tree with all the nodes in the backward tree what would be time complexity of bidirectional search? 
That is question number 3. 
Now we come to the questions from the current lecture that is lecture 6.
(refer slide time [52:54])

  

The problems for this lecture are of this type. 
You will be given a search space. 

[53:02] 
You can use actually the same state space that we gave in question number 1 of lecture 5. 
In question number 1 of lecture 5 we specified the search space by this table. 
For the same search space you have to apply IDA* and for IDA* you will also require the value of the heuristic function. 
Assume some values of h(n) and apply IDA* on this state space. 
You have to compare IDA* with A* in terms of time and space complexity in general. 
So you compare the correct heuristics of A* with IDA* in terms of time complexity and in terms of space complexity, that is the second question. 
Third question, suppose you are using hill climbing to find a solution to the n queens problem the question is, is hill climbing guaranteed to give you a solution? 
Question number 4: 
If you use simulated annealing are you guaranteed to find the optimum solution of the traveling salesperson problem? 
These are the questions of lecture 6. 

Artificial Intelligence (Lecture – 7)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

Two Player Games – 1

[00:57] 
Today we have the seventh lecture of the course Artificial Intelligence. 

[01:03] 
In today’s lecture we will talk about two player search. 
In the last few lectures we have looked at different state space formulations of some types of problems which were formulated as search problems. but in all these cases we assumed that there is one agent or one machine who is trying to reach the goal from the start position. 
In today’s class we will consider situations where there is more than one agent. 
In particular we will talk about two agent situations. 
In fact a specialized two agent situations where there are two competing players so that the gain of one player is the loss of the other player. 

[02:01] 
So we will see that such problems can be formulated as different kinds of search problems which have different characteristics than the single agent search problems we considered earlier. But before we go on to today’s lecture I would like to discuss the questions of the previous two lectures. 
(refer slide time [02:29])

 

First let me come to the questions from lecture 5. 
Question number one: 
Suppose you have the following search space this search space will be given to you in the form of a table you were supposed to draw the state space of this problem, assume that the initial state is A and the goal state is G and you have to work out a few search strategies and show the search tree and what sort of path the search generates from the initial path to the goal. 
(refer slide time [03:08])

 

[03:08] 
So this table gives you a representation of the state transition of the system. 
There are states A B C D E F G and these are the edges AB, AC, BD, BE, CC, CD, CF, DC, DE, EG and FG and in this column we have the costs of the different edges. 
In addition, for heuristic search we will need to have an estimate of the cost at a particular state. 
So the estimate at A = 8 at B = 8 at C = 6, at D = 5 at E = 1 at F = 4 and at G which is goal state the estimate is 0. 
(refer slide time [04:07])

 

[04:01] 
Now given this state transition diagram we have to work out three search strategies. 
1) Uniform cost search 
2) Greedy search and 
 3) A* search. 
(refer slide time [04:28])

 

And you have to show at each step of the search algorithm the node which is being expanded and the content of fringe. 
Also you have to find out the eventual solution discovered by the specific search algorithm and the solution cost that is achieved. 
Now let us go back to this table representing the transition. 
So A to B we have an edge with a cost of 4. 
This is a graphical representation of the same table. 

[05:01] 
A to B there is an edge with cost 4 
A to C there is an edge with cost 1, B to D has cost 3, B to E has cost 8, 
C to C has cost 0, C to D has cost 2, then C to F has cost 6, 
D to C has cost 2, D to E has cost 4, E to G has 2 and F to G has cost 8.. 
So this is a representation of the state space for this problem. 
Now let us quickly look at how uniform cost search works on this problem. 
In uniform cost search we start from the initial state and we expand nodes in the order of their distance from the initial state. 

[06:02] 
So, from the state A we can reach the nodes C and B. 
(refer slide time [06:09])

 

 Initially we will put the node A in the fringe or in the open list.
This will be the initial content of fringe. 
At the next instance we will remove this node from the fringe and expand it.
So A will be the first node that gets expanded and as a result we will get its two children B and C which we will put in fringe. 
B has a cost of 4 and c has a cost of 1 so C and B will be put in fringe, C with a cost of 1 and B with a cost of 4. 

[07:00] 
Now at the next step we remove the lowest cost node from fringe which happens to be C. 
So we remove C from fringe expand it and then find the children of C which happened to be D, F and C itself. 
Now C has already been expanded so if we keep track of it then we will not put C in the list instead we will generate D and F. 
D will have a cost of 1 + 2 = 3 and F will have a cost of 7. So we will have D with a cost of 3, B with a cost of 4 and F with a cost of 7. 
Next we will again remove the lowest cost node and expand it which happens to be D.
After that we will find the children of D which happen to be C and E out of which C is in the closed list if we do maintain a closed list but if we do not we will put it in there.
So let me not put in C right now. 
let me put in E so what we get is B, E and F.
D has a cost of 3 + 4 = 7, so E will have a cost of 7. Then B is already there with a cost of 4 and F is there with a cost of 7. 
Now out of these three nodes which are in the fringe at this instant we will remove B which is the smallest cost. and then we will find the children which happen to be E and D out of which E is already there in the fringe and the current cost of E through B would be 4 + 8 = 12 and E is already there in fringe with a cost of 7. 
So E will not go into fringe but D will and D will have a cost of 4 + 3 = 7.

[09:17] 
So, in the fringe we will have D with a cost of 7, E with a cost of 7 and F with a cost of 7. 
Now we have to remove the lowest cost node in fact all these three nodes have the same cost and we can remove any one of them. 
Let us say that we remove D but D has already been expanded so we will not put D and then we will remove E. 
If we remove E we will have F left in fringe and E has the child G. 
Now G will have a cost of 9 and F is there in fringe with a cost of 7. 

[10:01] 
 Next we will expand F and F has no new child. 
F has another child G which will have a cost of 15 so we will not put it in here 
instead we will keep G with original cost of nine and the next step G is the only node in fringe so we will expand it and G happens to be the goal so we are done. 
Therefore in this search we have expanded the following nodes A C D B E F G. Then if we broke this tie in other way rather if we had expanded F first then we might have got some other sequence of nodes. But once we get the goal we can retrace and find the path. 
So it happens that in this case G will have the parent as E, E will have its parent as D, D will have its parent as C and C will have its parent as A. 
So the path found would be A C D E and this path has a cost of 9 
So uniform cost search has found an optimum solution and this optimum solution has a cost of 9. 
Let us now go and see what happens if we use greedy search for this problem? 

[12:00] 
In greedy search we start from A and find the node which is closest to A or rather we find the node which is reachable from A and whose H value is at minimum. 
(refer slide time [12:06])

 

Now from A there are two nodes which are reachable B and C. 
B has A edge value of 8 and C has a edge value of 6 so what we will do is that we will take C because C has a lower H value. 
Now from C the two successors are D and F. 
D has a H value of 5 and F has a H value of 4 so we expand F so we go to F. 
Now F has one successor which is G which has H value of 0 so G is expanding 
So we get a path A C F G which is the solution found by greedy search in this case. 

[13:04] 
What is the cost of the solution? 
It is 1 + 6 = 7 + 8 = 15 so cost is 15. 
Obviously in this case the greedy algorithm does not give you the optimum cost path.
Next we will look at A* search as to how A* works on the same graph. 
Now in A* first the fringe contains the single node A 
(refer slide time [13:41])

  

then A is expanded and its children C and D are generated. 
If you remember in A* the F value of a node N is equal to its G value that is its cost from the initial state plus the estimate of its cost to the goal. 

[14:02] 
So, for the node C the G value is 1 here and h value is 6 
So C has a cost of 7, B has a G value of 4 and H value of 8 so B’s cost is 12. 
Now C is expanded removed from open and its children are C, D and F 
and C has already been closed so D and F are put in open whose cost F value happens to be 8 and 11. 
Next D is removed from open and its children are inserted. 
Then E is removed then finally we get G and as a result we get the solution path A C D E G whose cost is 9 and the nodes expanded are only A C D E and G which happens to be lesser than the number of nodes we expanded in the case of uniform cost search on the same graph. 

[15:00] 
Now let us look at the second question. 
(refer slide time [15:10])
 

In the second question you were asked to write the algorithm for bidirectional search using pseudo code.
Assume that each search is a breadth first search and that the forward and backward searches alternate expanding one node at a time. 
(refer slide time [15:29])

  

This is a pseudo code. 
Fringe: We maintain two open lists fringe1 and fringe2. 
Fringe1 contains initially the start state. 
It represents the search tree as we move forward from the start state and fringe2 is the open list of the search tree which we get while going backward from the goal. 
So while fringe1 and fringe2 are not empty both are non empty we first remove the first node from fringe1 let it be node. 
So if node is in fringe2 then we return the path that we get from start to node concatenated with the path we get from goal to node reversed. 
Otherwise if node is not in fringe2 we generate all successors of node and add the generated nodes to the back of fringe1. 
If we are done we go to fringe2, we remove the first node of fringe2 generate all successors of m and add the generated nodes to the back of fringe2. 
So this is a pseudo code for bidirectional search. 
(refer slide time [16:49])

 

Third question, give the time complexity of bidirectional search when the test for connecting the two searches is done by comparing a newly generated state in the forward direction against all the states generated in the backward direction one at a time. 

[17:06] 
So, in bidirectional search we have the start state here, we have the goal state here, we have a forward search from the start state, we have a backward search from the goal state until they meet. And when they meet we get a path from start to this node N and from goal to this node N and we concatenate them to get the path. 
Now if the tree has a branching factor of b and if these two search tree generations alternate these trees will meet about half way. 
So the depth of each of these trees would be d/2. 
So the number of nodes in this tree is bd/2 and the number of nodes in this tree is bd/2.
So total two times bd/2 are expanded. But every time we expand a node in the forward tree we check it against all the nodes in the backward tree and the number of nodes in the fringe of the backward tree is of the order of bd/2.
[18:19] 
So for every node expanded in the forward direction we spend order of bd/2 time checking it against the fringe of the backward tree.
So the total time taken is order of bd. But bidirectional search can be made faster if we do not check every node in this fringe but use some hashing scheme so that this test can be performed quickly.
In that case we may be able to get a performance which is nearer to order bd/2 which is square root of order bd/2 for plain breadth first search. 

[19:08] 
Now we move onto questions from lecture 6. 
(refer slide time [19:12])

   

1) Compare IDA* with A* in terms of time and space complexity. 
(refer slide time [19:20])

  

So let us assume a search tree with a uniform branching factor of b. 
So in this case the time complexity of A* is bn of IDA* is b × bn. 
The space complexity of A* is bn of IDA* is only b × n.
This is linear. However if the tree does not have a uniform branching factor this may not be true but we can see that if A* expands m nodes IDA* expands utmost m2 nodes. 
This happens if at every new iteration exactly one new node is expanded. 

[20:01] question number two 
(refer slide time [20:03])

 

Is hill climbing guaranteed to find a solution to the n queens problem? 
No, hill climbing can be applied to different formulations of the n queens problem but there is no guarantee that for all value of n an optimum solution can be found. 

[20:17] 
(refer slide time [20:19])

 

3) Is simulated annealing guaranteed to find the optimum solution of an optimization problem like traveling salesperson problem? 
No, simulated annealing reduces the tendency of the search being stuck at a neighborhood local minima. 
However it does not guarantee that a global optimum is achieved. 
Now we will move onto today’s lecture and today’s lecture as I have said earlier is on two player search. 
(refer slide time [20:46])

  

This is module 2 problem solving using search and lecture 7 is two player search part 1. 
(refer slide time [21:00])

 

[21:00] 
The objectives of today’s lecture: 
The student will learn how to formulate two agents zero sum games as a search problem. 
They will learn how to apply the mini max procedure to decide a move. 
They will learn how estimated pay offs are used when search to terminal nodes is not possible. 
(refer slide time [21:25])

 

Humans have always been fascinated with games. 
Games challenge our ability to think. 
So people have been engaged in many different types of games. 
There are simple games that we play like Tic Tac Toe and Knots and Crosses. 
Then there are other simple games like Nim, Kalah and 8 puzzle and there are more difficult or more complex games like Chess, Othello, Bridge, Go, Checkers etc but Othello is actually not that complex. 
(refer slide time [22:00])

 

[22:00] 
Now computers play games. 
Since the beginning of the history of computers or Artificial Intelligence people have been fascinated by the idea of making computers play games. 
So if you recollect we discussed that Alan Turing conceived that computers would play chess. 
In fact the first machine to play chess or first machine that claims to play chess go back many years. 
The machine was called the Turk. 
Actually it was a contraption which claimed to play chess. Even though actually there was no machine like intelligence involved in it because this contraption had a human hidden inside the interior who would play the moves. 
Since then we have come a long way and today the chess program in 1997 the deep blue chess program developed by IBM succeeded in defeating the world champion Gary Kasparov in a 6 game match. 

[23:16] 
There are other games for which computer programs have been written like checkers. 
Checkers programs have been able to beat the human world champion. 
Backgammon: Backgammon like TD Gammon by Tesauro has been written. And backgammon is actually a game of chance involves the throw of dice but the TD Gammon competes with the top few human players. 
It is very good but there are human players who are also as good as it.
In Othello the computer program plays much better than the best human players. 
Then there is a game of GO where unfortunately computer programs do not do so well so GO has a very high branching factor and computer programs have not been successful in GO so human players are much better. 
So we will consider a particular type of game involving two players where the two players alternate 
(refer slide time [24:22])

 

they compete and both the players have perfect information of each other, 
there is no dice throw or any chance factor involved. And these are fully competitive games called zero sum games where the pay off of one player is exactly the negative of the pay off of the other player. 
 They have exactly opposing interests and that is why they are called zero sum games. 
For example, this is like the game of chess where if there are two players a and b 
if a wins b loses and if b wins a loses or a and b both have a draw.

[25:06] 
Now let us see that game playing can be formulated as a search problem. 
(refer slide time [25:11])

 

We can take the initial position of the board. 
For example, in chess the initial board can be taken as the start position and the goal position is the position where one of the kings has been captured. 
If you take the game of Tic Tac Toe the initial position is a 3/3 board with no cross or not in it. And the final position is a position where either there are three crosses or three Os in a row in a column or the diagonal or all the positions are occupied. 
The successor function in a game tree defines the set of legal moves from any position. Like in chess board if white is playing white has a number of possible moves. 

[26:11] 
So these are the possible successor positions for white. 
The terminal test determines when the game is over. 
In chess the terminal test means the king of one of the sides is captured. 
 When a game is over the utility function gives an outcome of the game. 
For example, in chess if white and black are playing if white wins we give white a utility of + 1 and if white loses we give white a utility of – 1 and if white draws with black we give white a utility of 0. 
(refer slide time [26:48])

  

So the basic strategy in game playing is to grow a search tree. 
In search trees for games only one player can move at each door. 

So we have a tree starting from the initial position to the successor positions and so on until we reach the goal position. 
at this level it is player one‘s turn to move. 
At the next instance it will be player b’s turn to move then again player one’s turn to move and so on. 
At the leaf position when the game is finished we can assign utility to the player. 
For example, suppose it is a simple game involving just win, draw and loss we can give a utility of 1 to the winner, – 1 to the loser and 0 to both the players if there is a draw. 
In games like backgammon there is concept of gammon and the backgammon. 

[28:01] 
So a normal win gets a value of + 1, a gammon may get a value of + 2 and 
a backgammon may get a value of + 4.
So there are games where the utilities are more. 
So in general we can have a numerical value of the utility. 
Now, once we have the search tree we know the utility of the leaf positions 
and we can assign utility to the leaf positions of the game tree. And we can propagate these values backwards to compute the utility of other nodes which are in the intermediate nodes of the tree or at the root of the tree. The difference between single agent search and two agent search is that in single agent search there is only one player who moves at every step. 

[29:08] 

So we can plan the entire path for the player, We can find the entire path to the goal state. 
However, in two agent games if I am one player I can only make one move and then I have to wait for the move of the other player. 
Only after the other player moves then only I can plan my next move. 
So I cannot plan the entire sequence of moves at a time. 
I can only plan one move at a time. And then depending on the move that the opponent makes we make our next move. 
Now we do not have any control on the move that the opponent makes. But for the sake of analyzing the worst case performance of these algorithms we will assume that the opponent makes the move which is the best for the opponent. And because it is a zero sum game it is that move which puts us in the worst position. We will pick the best moves on our own turn and assume that in the opponent turn the opponent picks the worst move for us. 

[30:26] 
This is a very simple example of a game tree. 
A is the initial position, it has two possible moves to B and E, from B the opponent can make a move to C or D, from E the opponent can make a move from F to G. And from C we can make a move to this position or this position, 
from D we can move here or here, F we can move from here or here, G we can go from here or here. These are the terminal positions. 

[31:00] 
In the terminal positions we have the following utility.
Here we have a utility of – 1, here we have a utility of 5, – 2, – 4, 1 – 1, 2, 4.
(refer slide time [31:16])

 

Since this is our turn to move and at C we have two choices – 1 or 5 we seek to maximize our utility. 
So we will choose the move to this node whose utility is 5. 
So the backed up utility value at C is 5. 
Similarly at d we have two choices to move to – 2 and – 4 of which – 2 is better for us so the backed up utility of t will be taken to be – 2. 
At F we have two choices and one is better a choice than – 1 so the backed up utility at F will be + 1 and we will move in this direction. 

[32:06] 
At G we have a choice of 2 and 4, 4 is better for us so G will have a backed up utility of + 4.
Now here at B and E it is the opponents chance to move. 
So the opponent has a choice of going to 5 or – 2.
The opponent will choose something which is worst for us. 
 If the utility is 5 for us it means that from the opponents’ point of view it is – 5. And at D from opponents’ point of view it is + 2 so opponent will choose this and the backed up utility at this node will be – 2. 
At E the opponent has a choice of going to + 1 or 4. 
It will choose this which is the worst for us and the backed up utility is + 1. 
At A which is our turn to move we have a choice of getting utility of – 2 or + 1 and we will choose this because we get a better value up here.

[33:07] 
Therefore the backed up utility will be + 1. 
So we are assuming while backing up the utility values that at every step we make the best moves and the opponent makes the worst move for us. And we guarantee that if we go along this path the minimum pay off t we can get is + 1.
To compute these utility values we are alternating minimization and maximization. 
So here we are doing minimization at this layer where we play and we are doing minimization at this layer where the opponent plays again maximization at this layer where we play. 
(refer slide time [34:07])

 

[34:05] 
This sort of backing up is called a mini max algorithm.  
This is the algorithm in action. 
These are the backed up values and A has a backed up value of + 1. 
(refer slide time [34:25])

 

In two player games we have two players alternating, this is a zero sum game. 
It has perfect information where we know exactly that all the moves are deterministic and there is no chance involved. 
The two players take turns and try. The max players are the players who try to maximize utility and the other player the min player tries to minimize the utility function. 
We assume that the max player makes the first move. 
The leaves represent the terminal positions. 

[35:03] 
Now, given the description of the game we will draw a game tree. 
In the game tree the successive nodes represent the positions where the different players make moves. 
A game tree could be infinite in size. 
So there could be a path which never leads to a goal state.
Here are some quick to finish definitions: 
(refer slide time [35:35])

 

The ply of the node is the number of moves needed to reach that node. 
Suppose this is the root of the game tree and then to reach a particular state we go through different levels. 

[36:00] 
To reach this node from the root we need to move along three arcs. And given a complete tree the ply of a tree is defined as the maximum of the ply of its node. 
That is, the depth of the deepest leaf is called the ply of the game tree. 
First we are going to look at a brute force search algorithm to back off the utility function. 
(refer slide time [36:46])

 

And this brute force approach wants us to start from the initial node and generate the entire search tree up to the leaf positions assuming that the game tree is finite. 

[37:02] 
If the game tree is finite we can reach the leaf positions and infinite time if we have sufficient space. And we will see how to back up the utility values there by the mini max method. 
However for all but the most trivial games the game tree can be extremely large. And it will not be feasible for us to generate the entire game tree. Then we will see how to decide the best move using some heuristic. Nevertheless we will initially look at the brute force search and then will improve upon that. 
Let us take an example of a simple game 5-stone Nim. 
So this game is played with two players and a pile of stones. 
Suppose we have 15 stones to start with. Each player removes either one or two stones from the pile and the player who removes the last stone wins the game. 
This is the description of the game of the 5-stone Nim where each player can remove one or two stones at a time. 
Let us draw the game tree of this game. 
(refer slide time [38:35])

 

Initially let us assume that we start with 5 stones. 
Now, at this position the max player can move either one stone or two stones giving rise to either 4 or 3 stones left over. 
The min player can remove either 1 or 2 stones from 4 or from 3.
Again the max player can remove 1 or 2 stones etc and we stop expanding the tree until all the leaves have 0 that is no stone left. The nodes are colored alternately by red and green. 

[39:16] 
Red or pink represents the max player and green represents the min player. 
This is the max player and this is the min player. 
Let us look at the leaves. 
The green leaves correspond to win for min so the pay off for max here is – 1. 
The pink 0s represent pink for max and the pay off is + 1. 

[40:00] 
Now let us apply mini max to this tree.
Let us look at this node.
It has only one child so its pay off will be same as this child which is – 1. 
This node has two children whose utilities are – 1 and 1. 
This is a minimizing node so the backed up utility value will be – 1. 
At this node the backed up utility value is 1, this is a max node, 
The backed up utility value is the max of 1 and – 1 which is 1. 
At this node the utility is 1, at this max node the utility is max of 1 or – 1 which is 1 at this node the utility is min of 1 and 1 which is 1, at this node the utility is 1 and this node the utility is again 1.
Here the utility is – 1, here this is a min node the utility is – 1 and max node utility is 1. 

[41:01] 
So, for the max player the backed up utility at this node is + 1 and the best move for the max player is along this direction that is removed once. 
At the max nodes the max player needs to consider only the best move. 
At the min node it must consider all possibilities and it must make sure that for all ways in which min could move max still has the best value. 
So these min nodes can also be looked upon as AND nodes and the max nodes as OR nodes. 
Minimax theorem:
(refer slide time [41:56])

  

The Minimax theorem states that every two person zero sum game is a forced win for one player or a forced draw for either player, 

[42:10] 
in principle these optimal MINIMAX strategies can be computed. 
So, if we have the pay offs at the leaves and we back this up finally at the root we will get a value.
That value can be either 0 or positive or negative. 
So if the value is positive, if max plays judiciously max can force a win from this position.
If the backed up pay off value at the root is negative no matter how well max plays if min plays well and extremely intelligently then min can force Max’s loss. 
If the pay off is zero if both the players play at their best this game would end in a draw 

[43:03] 
Now, in the previous slide we looked at the game tree of 5-stone Nim and we saw that it was a win for max. 
Similarly if we draw the game tree of Tic Tac Toe, Tic Tac Toe is a game which all of you are familiar with you have nine squares and you play cross the other player plays zero, you play cross, the other player plays zero, you play cross the other player plays zero and this is a win for min or win for O. 
Tic Tac Toe is actually a very simple game and you can see that the pay off of the root is 0 that is you can force a draw. 
If both players are intelligent you can force a draw. 
(refer slide time [43:58])

 

So in MINIMAX search the max player selects the max player selects the move that leads to the successor node with the highest score. 
Conversely the min player selects the move that leads to the successor node with the lowest score. 
The scores are computed starting from the leaves of the tree and backing up these scores to their predecessors in accordance with the MINIMAX strategy. 
In the MINIMAX strategy every node in the tree is exhibited. 
So you start from the root you back off at every intermediate node until you get the backed up value at the root. 
(refer slide time [44:52])

 

This is the pseudo code for the function MINIMAX. 
So we are starting at the node N in the function MINIMAX. 

[45:01] 
if N is a leaf node then we return the pay off value of this leaf, the estimated score of this leaf. 
Otherwise if N is not a leaf suppose N has M successors N sub 1, N sub 2, N sub 3 up to N sub m if n happens to be a min node then we will return min of MINIMAX of N1 MINIMAX of N2 … MINIMAX of N sub m. 
Otherwise if n is a max node then we will return max of MINIMAX of N1 MINIMAX of N2 up to MINIMAX of Nm. 
So this is the MINIMAX algorithm. 
(refer slide time [45:58])

 

This algorithm was invented by Von Neumann and Morgenstern on 1944.

[46:07] 
However the game tree for all but the most trivial games become extremely large
so we cannot do exhaustive search, we cannot generate the entire game tree. 
The MINIMAX strategy as we described earlier works when we can draw the entire search tree. But such trees can get big very fast, can get big exponentially 
so you know that if the tree has a branching factor of B then if we look at a search tree of depth D the number of nodes in the search tree is of order Bd.
So in an exponential number of nodes the search trees grow very fast. 
So, if D is not extremely small but D is reasonably large then if B is more than 1 this becomes large very fast.
(refer slide time [47:08])
 

[47:08] 
For example, if you consider the game tree for chess, chess has an average branching factor which is estimated to be about 35. 
Chess games are often about fifty moves for each player which means that the size of the game tree is about 35100 which happens to be approximately 10154 which is an extremely large number, inconceivably large number. 
However if you consider not a search tree of chess but the search graph of chess in fact in chess there are about 1040 distinct positions but even 1040 is an extremely large number. 
So we cannot enumerate the entire search tree for chess in anyway. 
(refer slide time [48:03])

 

[48:03] 
So what we do instead is to use heuristic MINIMAX search where we search to some fixed depth fixed cutoff which is called the search horizon. And if you only expand the tree up to a limited depth what will happen is that, we will not reach the leaf nodes. And if we do not reach the leaf node, that is, such an intermediate position of chess we will not know whether it is a win for white or win for black or draw for both of them. 
So we have to estimate at the intermediate position whether it is good for white or good for black. 
So we estimate the merit of positions at the frontier of the search tree by some heuristic static evaluation function because we do not yet know the final outcome. 

[49:06] 
So after we make some moves we have to know whether that move is a good move and we can even apply the static evaluation function at this level. But if you assume that when we grow deeper in the tree we can have a better evaluation function. 
We can expand the tree up to a cutoff, we can estimate the value of the boards at this step and on the basis of this estimate we can apply MINIMAX to find the backed off utility value at the root and also which is the best move for the root to take? 
(refer slide time [49:52])

  

So the modified heuristic MINIMAX function will look like this MINIMAX at the node n. 

[50:00] 
If node n has reached the cutoff has reached our terminal test of where we want to terminate search then we return the estimate of n we call this the pay off. 
so we this will an estimated pay off of n. Else if n is a max node then we set v equal to + infinity. 
If n is a min node we set v equal to – infinity. 
Then we find out all successors of n. 
For each successor m(n), if n is max node v equal to max of earlier value of v and MINIMAX of n. 
Otherwise if n is a min node then v is min of earlier value of v and MINIMAX of m. 
So, basically this is MINIMAX of the partial search tree where the search tree is only evaluated up to a certain cutoff. And the estimated value at this frontier is computed by some heuristics and then MINIMAX is applied on those estimated values. 
(refer slide time [51:22])

  

[51:21] 
Here are some examples of heuristic function: 
In chess a lot of people have written lots of programs on chess. 
We will just mention one very simple heuristic that is used for chess. 
It is, how good a board is for a player depends on how many pieces the player has. But just the number of pieces is not enough. 
The types of pieces which are there are also important. 
So, in chess the utility function could be the number of pieces on board of each type multiplied by relative value summed up for each color. 

[52:04] 
So we call this heuristic the material gain. 
We find out the material gain for white minus the material gain of black to compute the utility of white at a node. And the material gain of white is the weighted sum of the pieces that white has. And each piece has a weight associated with it. Like a pawn can have some small weight may be the knight has a larger weight and so on. 
So this gives us an estimate of the board. 
Therefore heuristic static evaluation function for a two player game is a function from a state to a number, to a number which gives its estimated pay off. 
(refer slide time [53:03])

   

[53:04] 
The goal of a two player game is to reach a winning state but the number of moves to reach that winning state is unimportant. 
So the length of the path is not important but what is important is whether we can get to a winning state or the state with a higher pay off or not. 
This is one of the differences with one agent search. 
(refer slide time [53:32])





 

So, if you design a heuristic static evaluation function for a two player zero sum perfect information game then we can use this MINIMAX strategy to play the game. 
From any given position we simply generate all the legal moves, apply our static evaluator to the position and then move to the position with the largest or smallest evaluation. 
Suppose we want to find out the best move to take from this position we find out all the successors, find out the evaluation of all the successors and take the max of them and move towards that. 

[54:12] 
However static evaluation functions are not very accurate. 
So instead of going on one level we might go for several levels and then back up the values there. 
(refer slide time [54:27])

  

Let us look at Tic Tac Toe.
In Tic Tac Toe we must have a function to detect if the game is over. 
If X is maximizer then the function returns infinity if there are three X’s in a row which is win for X or + 1 – 1 if there are three O’s in a row. And our static evaluation function is the count of the difference in the number of different rows columns and diagonals occupied by X and O. 
So we count the number of rows columns and diagonal occupied by X minus the number of rows columns diagonals occupied by O.

[55:06] 
So this is a Tic Tac Toe search tree.
This is the initial position, this is Max’s turn to move. 
There are 9 possible successors but since this is symmetric there are three different actual successors. 
(Refer slide time [55:22])
 

For this one the evaluation is 3 – 0 = 3 so X occupies one row, one column, one diagonal and O occupies 0. 
Here X occupies one column one row and two diagonals, here X occupies one row and one column. 
So the evaluation is 3 here 4 here 2 here so this is the best move to take for X based on this one level of look ahead.
(Refer slide time [55:59])

 

This algorithm is very efficient but it only considers immediate consequences of move.
That is it does not look over the horizon. 
(Refer slide time [56:06])

 

[56:06] 
So in MINIMAX search we expand the search tree further, look at further depth and then find out, suppose this looks at one more depth and finds out the evaluation at this depth and uses MINIMAX to backup these values. 
For example, this board has a pay off of 1, this has a pay off of 2 so this pay off will be min of 1 or 2 that is 1. Similarly we will just find out, here the min is – 2, 
here the min is 1 and here the min is – 1 so max will still move here because it has a pay off of 1. 
(Refer slide time [56:59])

 

So we search as deeply as possible and then use MINIMAX. 
(Refer slide time [57:04])

 

[57:03] 
So we will just stop with this last example of the MINIMAX tree. 
This is a MINIMAX tree with showing us the terminal evaluation functions. 
In MINIMAX we will evaluate this node which is 4, this is 2, this is minimizer nodes. 
The blue ones are minimizing nodes, the green ones are maximizing nodes and then the final evaluation function is 4. 
 (Refer slide time [57:49])

 

Questions for lecture 7:

Suppose the game tree for particular problem has a branching factor of b. 
If you do a p-ply look ahead and then apply MINIMAX on this game tree how many nodes will you expand per move? 

[58:01] Question number 2: 
Consider MINIMAX search on a game tree with p-ply search and another MNIMAX search with a q-ply search. 
If q is greater than p which one is better? 
 
Artificial Intelligence (Lecture – 8)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

TwoTwo Players Games – II

[01:02] 
todayToday is our second lecture on game tree search. 
inIn the last class we talked about twotwo player games and how we can formulate it as a search problem. 
todayToday we will have the second part of this lecture. 
(Refer slide time [01:24])
  

inIn this lecture the student will learn how the search tree for twotwo player games can be pruned using alpha beta pruning. 
inIn the previous class we looked at minimaxMINIMAX search for computing the best move for a player. Bbut minimaxMINIMAX is quite inefficient and we will see how minimaxMINIMAX search can be pruned using alpha beta cut off cutoff values. 
givenGiven a search tree the student is expected to able to apply alpha beta pruning on the search tree and show which are the nodes which will be expanded and the nodes which will be pruned. 

[02:03] 
theThe student will understand what is the optimal saving by using alpha beta pruning for a game tree and what is the minimum amount of pruningminimum amount of pruning  
that can be obtained? 
theyThey will also be familiar with the certain strategies used in game tree search like the quiescent effect, the horizon effect, transposition table and end game database etc.etera 
soSo, to recollect we have considered two2 playerplayers perfect information zero0 sum games where we have two2 players min and max. 	
(Refer slide time [02:45])
 

andAnd the full minmax tree can be drawn so that the terminal positions are either a win draw or a loss and we can assign a value a pay off value to the terminal position. 

[03:00] 
whenWhen generating the entire tree is not possible we saw that can cut off we can decide to cut off cutoff search at a particular depth at a particular level and use a static evaluation function at the terminal nodes. 
thusThus in the tree the game tree that we generate the leaves are given definite score. 
inIn the last class we saw how the minimaxMINIMAX algorithm defines how the scores are backed up from the leaf towards the root. 
so that’sThat is what we studied in the previous class. 
now toTo briefly recollect the game tree is like a search tree.
theThe nodes are the search states. 
theThe edges between the nodes correspond to moves. 
leafLeaf nodes correspond to determined positions and at each node it is one or the other player’s turn to move. 

[04:04] 
nowNow the problem with minimaxMINIMAX is that minimaxMINIMAX is extremely inefficient. 
(Refer slide time [04:10])

 

thatThat is, minimaxMINIMAX expands all the nodes in the given search tree. 
Tthat is, if we start from the root and search pb- ply or d -ply up to a depth of d and assuming that this game tree has a uniform branching factor of b then the number of nodes expanded or explored by minimaxMINIMAX is bd to the power d .
soSo minimaxMINIMAX will expand bd b to the power d nodes in order to compute the minimaxMINIMAX value of at the root if it decides to cut off cutoff search at depth d. 
howeverHowever note that while we are doing this minimaxMINIMAX search we are only concerned with the best move for max at the root level. the best move at the root level 

[05:12]
 weWe really do not need to have the correct minimaxMINIMAX value at all the intermediate nodes. 
the exact minimax value weWe don’t need notto have the exact MINIMAX value for all the nodes. 
so weWe really just need top know the minimaxMINIMAX value at the root node or rather we want to know which is the best move at the root node. 
So, if we consider this we can we show that we can device an algorithm which expands less nodes than minimaxMINIMAX does. 
now let us beforeBefore we go to the formal properties of the algorithm we will take up an example and show why such pruning is necessary or such pruning can be done. 
(Refer slide time [06:00])

   

[06:00] 
soSo let us consider this game tree. 
theThe square nodes correspond to the max player and the round nodes correspond to the min player. 
hereHere it i’s the max player’s turn’s to move. 
atAt this level it is the min player’s turn to move and at the lowest level here again we have the evaluation for the max player. 
now you see suppose so Therefore the minimaxMINIMAX values can be computed by depth first search. 
soSo, suppose this leaf gets evaluated first and it has a value of three3 
now when this leaf is evaluated the value of the min node must be three3 or less than three3 because min will compute the min of all the children values. 
soSo the value of the min node can be three3 or less than three3. 

[07:03] 
whenWhen the second node is evaluated we know the exact minimaxMINIMAX exact minimax value of this min node which is equal to three3. 
Now, when the minimaxMINIMAX value of this node is equal to three3, since this is a maximizing node the value of this node must be greater than three3. 
theThe min of this node is exactly equal to three3. 
soSo the value of this max node must be greater than three3 because max is a maximizing node. 
itIt has already found a path through which the minimaxMINIMAX value is three3. 
itIt can try another path. 
itIt should go to another path if its minimaxMINIMAX value is greater than three3. 
soSo when we come here we know that the max value must be greater than three3. 

[08:02] 
nowNow we come and explore this node. 
nowNow at this node max has a value of two 2. 
now ifIf max has a value of two2 the value of min at this node must be less than two2. butBut if the value of two min is less than two 2 then max has already a path where the value of minimaxMINIMAX is three3. 
alongAlong this path the value of minimaxMINIMAX is less than two2. 
soSo max whatever be the actual pay off values at these nodes max will not be interested in this path. 
Ffor example, even if the value at this node is equal to let us say hundred100 suppose the value is equal to hundred even then max will not be interested in this path because in that case min value will be still less than two2. 
ifIf the value here is let us say minus– one 1 the min value will be equal to minus– one 1. 

[09:02] 
soSo the min value at this node can be less than two2 and cannot be greater than two2. 
thereforeTherefore the values of these nodes are not important for max because max knows that this is a path with a better guarantee. 
soSo max need not compute these nodes. 
soSo search can be cut off cutoff at these twotwo paths. 
so thisThis is the idea of the pruning that we will talk about today. 
soSo what we do is that we start propagating the costs as soon as the leaf nodes are generated. 
(Refer slide time [09:45])

  

weWe do not explore nodes which are guaranteed not to affect the choice of the move using minimaxMINIMAX search. 
soSo we want to prune the search tree so that we still get the same strategy as we would get by full minimaxMINIMAX search but we avoid expanding those nodes which do not have any effect on deciding the correct move for max. 	

[10:15] 
soSo this strategy of our pruning is called alpha beta pruning. 
soSo alpha beta pruning is some type of branch and bound pruning. and And the idea is that,e we cut off cutoff search when we cannot do better than the best so far. 
(Refer slide time [10:34])

 

toTo implement alpha beta pruning we associate with the nodes, the alpha and beta values are associated with the nodes.
usuallyUsually alpha is meaningful for max nodes and beta for min nodes. 
soSo we can keep our beta values at all the nodes and alpha value of a min node could be the alpha value of its max ancestor and beta value of the max node is the beta value of its min ancestor. 

[11:09] 
soSo at every node we store this alpha and beta values. 
(Refer slide time [11:18])

 

Aat a max node we will store an alpha value. 
theThe alpha value at the max node is a lower bound on the exact minimaxMINIMAX score of that node. 
soSo initially we can initialize the alpha values of all max nodes to be minus– infinity. 
atAt any intermediate point the alpha value of the node is a lower bound on its actual minimaxMINIMAX value. 
soSo alpha value of a node can only increase. right 
soSo, if a max node has an alpha value equal to alpha its true value might be greater than equal to alpha. 

[12:07] 
Now, if we know that min can choose moves with scores less than alpha then min will never choose to let max go to a node where the score will be alpha or more. 
soLet us let’s just look at a diagram to explain what this is aboutabout. 
supposeSuppose this is a min node. 
now  supposeSuppose min has a path whose minimaxMINIMAX value is known to be let us say equal to seven7. 
supposeSuppose min has another path to a max node whose alpha value is equal to ten10 which means that its minimaxMINIMAX value is greater than or equal to ten10. 
Now, since min is a minimizing node min knows that this path is better than min knows that this path is better than this path because in this path the actual value is going to be greater than equal to ten10. 
so So search can be pruned at this path. 
(Refer slide time [13:37])

 

similarlySimilarly the beta values can also be used for another type of cut off cutoff. 
atAt a min node the beat value happens to be a lower bound on the exact minimax sorry at the min node the beta value is actually an upper bound to the exact minimaxMINIMAX score. 
(Refer slide time [14:09])

 

[14:09] 
atAt a max node the alpha value is a lower bound and 
 at a min node the beta value is a upper bound. 
soSo the true value might be less than equal to beta because beta value is aan upper bound. 
nowNow if we know that max can choose moves with score greater than beta then max will never choose to let min go to a node for the score will be beta or less. 
Let u’s again draw a diagram. 
supposeSuppose this is a max node. 
 now supposeSuppose at this min node max has a path whose minimaxMINIMAX value is known to be equal to so let us say ten10 and there is another path 

[15:03] so there is a min successor whose beta value is equal to seven7. 
soSince the this beta value is an upper bound 
 the actual value of this node is less than equal to seven7 because max is a maximizing node and max will always prefer this path to his path 
 so this path need not be explored any further. 
soSo this is the sense of beta cut off cutoff. 
(Refer slide time [15:34])

 

nowNow why can we cut off cutoff search?
h 
let us consider this example wait 
letLet us consider this example: 
inIn this example at this node the value the terminal value is equal to three3, 
 at this node the value is equal to five5. 
So w hen this three3 is discovered min gets a beta value of three3. 

[16:05] 
thatThat is, the value of min will be greater than equal to three3. 
when When this node is found then beta value remains three3 and the value of min is now exactly equal to three3, because the value of min is exactly equal to three3 we can set the alpha value of this node equal to three3. 
thereforeTherefore max value will be less than equal to three3. 
nowNow, search comes to this node node.
thisThis node has a value equal to two2. 
soSo because this node has a value equal to two2 the beta value of this min node is set to be equal to two2 and the actual value of min is less than equal to two2 
now sinceSince beta value is two2 here and the actual value is three3 here 
max will never consider going along this path. 

[17:00] 
soSo this path can bethe rest of this path can be pruned off which means that these nodes need not be explored. andAnd the optimum path for max is along this path and for min here it i’s along this path. 
Now, if the alpha value at a max node is equal to alpha means that max can guarantee a score of at least alpha if heit plays judiciously at that point. 
ifIf the beta value of a min node is equal to beta it means min will guarantee a score no more than beta if it plays judiciously. 
soSo, this is the essence of alpha beta cut off cutoff. 
moreMore generally speaking, at max nodes if an ancestor min at suppose we have a max node 	
(Refer slide time [18:15])

 

if there is an ancestor min node whose beta is less than alpha then this node is irrelevant. 
soSo the ancestor need not be the exact predecessor 
 it could be some other ancestor. 
soSo just to draw a diagram, 
 supposeif this this is an ancestor 
this is a max ancestor and there is some parts of the tree and there is a min predecessor. 
now ifIf this max ancestor has has an a alpha value of of equal to alpha and if this node has a beta value of of equal to beta and if beta happens to be less than alpha then max will never consider a move through this point. 
soSo below this node the rest of the node expansions can be cut off cutoff. 
soSo, if a min node has a beta value which is smaller than alpha value of one of its max node ancestors then  search can be cut off cutoff at this node because a path through this node would be irrelevant for max because he has already found a path with a guarantee of at least alpha. 
(Refer slide time [19:56])

 

[20:05] 
so there are twoTwo types of pruning that can be done. 
oneOne is from the point of view of max and
 the another from the point of view of min. 
soSo search can be discontinued at a node under circumstances. 
numberNumber one, if the node is a max node and the alpha value of this node is greater than equal to the beta value of any min node ancestor. thisThis is called beta cut off cutoff. 
(Refer slide time [20:35])

 	

theThe other case which we just discussed is, if we have a min node and its beta value is smaller than the alpha value of any of its max ancestor then also search can be cut off cutoff. 
thisThis is called alpha cut off cutoff. 
Now, how to compute these alpha beta values? 

[21:02] 
weWe will outline an algorithm for computing the alpha beta values. 
so The computations of these alpha beta values will be very similar to minimaxMINIMAX computations and they can be done in a depth first manner. 
so howeverHowever while computing the alpha beta values we need not explore all the nodes of the search tree. andAnd as a result of this computation we get a final backed up value of the nodes. 
someFor some nodes that we do not explore we do not care about the value. for them we do not care about the value 
forFor those nodes where we do find a backed up value that backed up value may be the exact minimaxMINIMAX value or it may not be the exact minimaxMINIMAX value. 
(Refer slide time [21:52])

 

soSo, the backed up value is either the exact minimaxMINIMAX value of the node or it i’s an approximate value. andAnd the alpha value of the max node is a lower bound on the actual minimaxMINIMAX value. 

[22:08] 
theThe beta value of a min node is an upper bound on the actual minimaxMINIMAX value. andAnd as i mentioned we do not really need to know the exact minimaxMINIMAX values to compute the best strategy for the max. 
nowNow let us see how we can calculate the alpha values at a max node. 
(Refer slide time [22:36])

 

whatWhat we do is, we first when we are at a max node we first evaluate the value of one of its successors. 
if the exact value of one of its successors is known then let me just draw a diagram 

[23:00] 
supposeSuppose this is a max node and suppose this max node has these threethree min children. 
so So, when the exact value of the first child is computed. supposeSuppose the value of this first child is minus– one 1then so initially the alpha value of this max node is let us say minus– infinity. 
whenWhen this node is fully evaluated and its exact value is equal to minus– one 1 we set the alpha value to be minus– one 1. 
thenThen we evaluate the value the beta value of this min node.  
now orOr rather when we find the value of this min node 

[24:09] if its value is let us say three3 then the alpha value is updated to three3. 
ifIf the value of this node is minus– two2 because minus– two2 is less than three3 the alpha value stays as it is. 
whenWhen all the children of the max node have been evaluated its value is the alpha value is the exact value. butBut before that we can say that the alpha value is a lower bound on the exact minimaxMINIMAX value. 
(Refer slide time [24:42])

 

so instead of so thisThis happens when all the nodes have been fully evaluated. but Eeven if one of its successors have not been fully evaluated but a beta cut off cutoff has occurred we can terminate search at that point and find out the final backed up value at the max node as its current alpha value when we finish dealing with all its children or when a beta cut off cutoff occurs at this node. 

[25:20] 
similarlySimilarly and symmetrically we can compute the beta value at min node. 
afterAfter we obtain the final backed up value of the first child of a min node we set beta of the node to the value of this first child. 
(Refer slide time [25:38])

 

whenWhen we get the final backed up value of the second child we decrease beta update beta if the new value is smaller. 
whenWhen we have the value of the final child or if alpha cut off cutoff occurs somewhere then the beta value becomes so the alpha cut off occurs at this node then the beta value becomes the final backed up value of this node. 

[26:08] 
nowNow you see the question that we have to ask is; how effective is alpha beta pruning? 
howHow many nodes does it prune? 
so beforeBefore we answer this question let us trace alpha beta pruning over a slightly larger tree and see how it works and then we will discuss how many nodes are pruned under different circumstances. 
so thisThis happens to be the schematic diagram of a game tree where the leaf nodes have been assigned certain values. 
(Refer slide time [26:57])

  

now you see whenWhen depth first reversal occurs first this left most leaf will be evaluated. 

[27:08]
 itsIts value is equal to four4. 
thereforeTherefore the beta value of this min node is equal to four4.
thatThat is, the value of this min node is less thaen four4. 
Now, after five5 this node is evaluated, 
 the value of this min node will be equal to four4. 
soSo initially the value at this min node is less than four4, when this is evaluated the value at this mind node is equal to four4. 	
Now, if the value at this node is equal to four4 the alpha value at this max node will be greater than four4. 
soSo we will put here greater than four4. 
Nnow this child will be evaluated. andAnd to evaluate this this, this child will be evaluated. whenWhen this value is evaluated the value of this min node will be greater than three 
sorry less than three it’s not greater than greater than equal to 

[28:19] so this is less than equal to three3 and
 when this node is evaluated this value will become equal to two2 
the final value will be equal to two2.
 aAnd if if the final value is equal to two2 the final value up here will be equal to four4. 
so ifIf this value is equal to four4 the beta value of the min node will be equal to four4. 
thatThat is, the value of this min node is less than four4. 
nowNow this child is going to be evaluated. 
toTo evaluate this this child to evaluate this this child 

[29:02] 
so this node gets a beta value of six6. 
afterAfter this node is evaluated the final value of this node is equal to six6. 
nowNow, the alpha value of this max node will be equal to six6. 
thatThat is, the value of this max node is greater than six6.
nowNow you see let us look at this min ancestor. 
theThe value of this node is equal to four4. 
theThe value of this node is greater than six6. 
soSo this node does not need to look at this path. 
thisThis path is clearly better than this path. 
soSo search can be pruned at this point and the backed up value here will be equal to four4. 
nowNow since the backed up value of here this is equal to four4 the alpha value of max will be equal to four4  

[30:02] that is the value of max is greater than equal to four4. 
nowNow this child, therefore this child, therefore this child, 
therefore this node will be evaluated. 
theThe beta value here will be one 1 and after this is evaluated the beta value will be the actual value will be equal to one 1. andAnd then the alpha value of this node will be equal to one 1. 
nowNow let u’s come to this child. 
thisFor this child we evaluate this 
so the beta value of this node is equal to two2 
so this node can improve further, 
 so this is better than this 
so we evaluate this node and the actual value is equal to two2. 
thereforeTherefore the max will actually get a value of two2. 
nowNow let us just look at this this.
thisThis min node now will have a beta value of equal to two2 
 so the actual value is less than equal to two2. 

[31:08] 
nowNow look at this max node. 
itIt ahas one child whose value is equal to four4 and this child has a beta value of two2 
that is its actual value is less than equal to two2 
 so max need not consider this path and this will the best strategy for max at this point. and thereforeTherefore search can be cut off cutoff at these points. 
soSo these nodes need not be evaluated. 
(Refer slide time [31:34])

 

so thisThis is the actual the minimaxMINIMAX values of the nodes. butBut if we do alpha beta pruning this is how the nodes are evaluated. andAnd as we can see that these nodes were cut off cutoff by alpha beta pruning they and were not evaluated. and And also this portion of the tree 

[32:11] they need not be evaluated when we use alpha beta pruning. 
(Refer slide time [32:22])

 

thisThis shows the same diagram. 
now soSo here we have some deep pruning at the right half of the tree. 
nowNow let us try to outline the entire minimaxMINIMAX search. 
soSo we assume that minimax is done at the max node and maximin sorry maximinMAXIMIN is done at aa max node and minimaxMINIMAX is done at a min node and let us assume that v n is the static evaluation of node n. 

[33:00] 
nowNow we outline the algorithm for maximinMAXIMIN and minimaxMINIMAX. 
so maximinMAXIMIN is done at a max node. 
(Refer slide time [33:13])

 

so maximinMAXIMIN takes as argument n the node and the alpha value and the beta value the current alpha and beta values. 
so ifIf n is at the search depth we have reached a leaf 
then we return the v( of n). 
otherwiseOtherwise for each child m of n so each child of max will be a min node. 
itsIts value will be computed by recursively computing minimaxMINIMAX m alpha beta. 
now ifIf this value returned is greater than alpha then we set alpha equal to value. 
soSo we look at one child, 
 it returns a value and 
 if the value is greater than the current alpha value of this max node we update the value of alpha. 

[34:06] 
Now ifIf the current alpha value happens to be greater than equal to beta where beta is the value of its min ancestor which has been propagated to this node 
in that case we will return alpha because we need not explore this max node any further. 
soSo this is the computation done at the max node. 
weWe start at a max node, 
 we find all its min successors, 
 we expand the min successors one by one, 
 we take the first successor find its minimaxMINIMAX value and if that value is greater than the current value of alpha then alpha is updated. and And as a result if alpha becomes greater than equal to beta then alpha value is returned 
. otherwiseOtherwise we continue until all the children have been solved and the alpha value has been obtained. and And this alpha value is returned by the procedure maximinMAXIMIN. 

[35:12] 
atAt a min node we do minimaxMINIMAX which is just a symmetrical function. 
minimaxMINIMAX of a min node again takes alpha and beta. 
(Refer slide time [35:24])

 

soSo beta is the beata value of this node and
 alpha is the alpha value of its max ancestor or max parent. 
soSo if n is a leaf node we return v( of n). 
thenThen otherwise for each max child of n we find value of that child by calling recursively maximinMAXIMIN. 
ifIf the value returned is less than beta we set update the beta value of this node. 
now ifIf this beta value happens to be less than equal to the alpha value of its max ancestor then we return the beta values 

[36:09] there is a mistake here
so we return the current beta value and then we cut off cutoff search at this point. 
otherwiseOtherwise at the end of this loop we return the beta value. 
so thisThis is minimaxMINIMAX. 
nowNow let us look at the performance of alpha beta. 
so if you see whenWhen we have a tree whose branching factor is b and its depth is d we have seen that the number of nodes in this tree is of order bd to the power d 
.
[37:05] 
itIt has been shown by nNoth that in the best case the minimum number of nodes expanded or explored by alpha beta pruning will be equal to so best or minimum number of nodes expanded or explored by alpha beta is order bd/2 to the power d by two which is square root of order bdb to the power d . andAnd the best case occurs when the best child is always the first one which is explored forexplored for at every node. 
(Refer slide time [37:45])

 
inIn this case when the best child is the left most child then that is explored first. 

[38:00] 
weWe get the correct value in the beginning and in that case the number of nodes expanded is smallest and it happens to be the square root of what would be expanded by minimaxMINIMAX search. 
theThe worst case not surprisingly occurs when the worst child is always the first one to be expanded and the best child is the last one to be expanded at every nodenode. 
inIn that case alpha beat pruning would not have any extra savings over minimaxMINIMAX and all the nodes expanded by minimaxMINIMAX would be explored. 
soSo the number of nodes expanded is between order bd/2b to the power d by two and order bd b to the power d if we use alpha beta pruning. 
inIn practice for the random ordering of the nodes you do, the average number of nodes pruned is quite significant to make alpha beta pruning very attractive. 

[39:05] 
however wait 	okay i am not getting the slide 
so howeverHowever we can improve our chance of achieving better pruning by doing move ordering. 
thatThat is, you see we do not know when we start with the search we do not know which child is better. butBut if we use the static evaluation function of the children to order the decide the order in which the children are expanded. For a good evaluation function we can expect for a good evaluation function that a minimaxMINIMAX or with alpha beta pruning it will expand many more nodes sorry will expand less nodes and prune more nodes. 

[40:08] 
soSo the efficiency of alpha beta depends on the order in which the nodes are encountered at the search frontier.
(Refer slide time [40:15])

 

theThe optimal number of nodes expanded is order bd/ to the power d by two2. 
so thisThis happens if the largest child of a max node is generated first and the smallest child of a mind node is generated first. 
Iin the worst case the complexity is order bd. b to the power d aAnd if you assume a random ordering then the number of nodes expanded is order b3/4thd b to the power three fourth d wwhich actually means that if due to our resource limitations we can if we use minimaxMINIMAX search we can go up -, go down to a depth of d. 
Tthat is, if we can do a deep ply p ply search with minimaxMINIMAX we would be able to do a four4th b by d/3 search fourth b by d ply search using minimaxMINIMAX assuming random node ordering. and And if assuming very good node ordering we can search to twice the depth and that would oftenten  lead us to have a much better strategy. 
soSo minimaxMINIMAX does improve the depth to which search can proceed due to pruning the number of nodes that need to be expanded. 
(Refer slide time [41:58])

 

additionallyAdditionally a number of improvements have been developed to improve performance with limited computation. 
so weWe will briefly discuss some of the other issues that people have come up with for game trees. 

[42:15] 
so as i already mentioned byBy using node ordering, using the static evaluation function or the current value etcetera we can get close to order bd/2. b to the power d by two 
(Refer slide time [42:32])

 

soOr the effective branching factor would become root b. 
soSo node ordering instead of generating the tree left to right we reorder the tree based on the static evaluations at the interior nodes. 
soSo at every node we look at the children. 
weWe apply static evaluation on the children and according to the value of the static evaluation we order the nodes. 

[43:05] 
another idea whenWhen we have limited resources available so another idea is to do iterative deepening search. 
so initiallyInitially we do a search up to one 1 ply and we get a possible value a particular best suggested best move and
t then if we have time we do a two2 ply search and update our best move and then we do a three3 ply search then four4 ply search then five5 ply search then six6 ply search and so on. 
when When the time is over, when there is no more time left we use the best move as given by the previous iteration. 
soSo we use iterative deepening and we use time 
, when time runs out the move recommended by the last completed iteration is made. 

[44:05] 
iterativeIterative deepening can be combined with node ordering to improve pruning efficiency. 
insteadInstead of using the heuristic value we can also use the backed up value from the previous iteration at the nodes.
so that’sThat is another idea that one could do which can improve the pruning in search go tree.
 thereThere are twotwo other points points.
inIn a game tree with fixed cut off cutoff depth there are oftenten some problems which people have noticed. 
oneOne of these problems deals with the concept of quiescence.
quiescenceQuiescence happens when the play has settled down. 
(Refer slide time [44:49])

   

andAnd if you go down the search tree further it’ is unlikely that the evaluation function would change much. 

[45:01] so
 ifIf we can recognize in the game tree that at a node it has reached a quiescent position we need not waste our effort in expanding that search frontier fFurther. andAnd we can concentrate our efforts in other parts of the search tree which are more likely to change the evaluation function as we go down. 
so thisThis effect is called the quiescence. 
Secondly, oftenten in game tree search we notice the horizon effect. 
so horizonHorizon effect happens when there is a particular set of moves so that there will be a dramatic change when a particular move is taken but you can use stalling operations to stall the inevitable bad or good position.

[46:05] 
so inIn the game playing situation one can stall for time. 
so theyThey can postpone the inevitable when win or inevitable damaging move by the opponent over the horizon so that if we stop search at a particular point we might get an evaluation function which is dramatically different from what would happen if a move is taken to take care of that stalling. 
soSo what is the solutionsolution? 
so game we given peoplePeople even write game programs. 
(Refer slide time [46:42])

 

for qQuiescence search: so instead of cutting of the search at a fixed depth the good game playing programs they evaluate those positions which need to be explored further because you expect a change in the backed up values if you explore the search tree. and And there are some positions whose value is sort of closed unand likely to change where you explore the tree further. 

[47:09] 
soSo if you can detect the quiescent positions you can terminate search at those points.
soSo you have a variable depth search tree. 
theThe horizon problem is not so easy to handle and there is no general solution so 
so some heuristics could be used for some specific games. 
then thereThere are certain other strategies that people have employed. 
forFor example,  sometimes transposition tables are used. 
(Refer slide time [47:45])

 

so someSome game trees or game graphs are more graphical isn nature that is. thereThere are many repeated positions. 
so forFor efficiency, it i’s important to detect when when a state has already been searched. 

[48:04] 
inIn order to detect a search state previously generated game states with their minmax values are stored. 
you see thisThis sort of transposition tables will be important if you look at game of Nnim. 
so inIn Nnim there are many repeated states 
 so it makes sense to store
if ia node has been solved it makes sense to store the state along with the backed up value obtained there to avoid repeated exploring of the same node. 
thenThen in many games the opening and the closing situation need very careful attentention. whetherWhether it be a game of chess or the game of backad gammon which is of course a game of chance. 
so you see at mostMost node games they start at the same initial position and there are certain best moves that you can take from the initial position. 

[49:04] 
soSo instead of using minimaxMINIMAX or alpha beta with minimaxMINIMAX or alpha beta to find the best move it makes sense to generate a table of good initial moves in consultation with the game expert human expert and this is known as the opening book, 
(Refer slide time [49:23])

 
so opening book can be used. 
thenThen the end game is another interesting place where there is a lot of expertise required. 
soSo database of end game moves will be very helpful. 
(Refer slide time [49:42])

 

soSo a technique for calculating endgame databases is also helpful. 
thenThen for playing more sophisticated games one needs special purpose hardware. 	
(Refer slide time [49:51])

 

forFor example, in the chess games game you can generate more number of moves per second if you have specialspecial purpose hardware. 

[50:03] 
soSo the best machines today are based on special purpose hardware design and built to play specific games. 
(Refer slide time [50:13])

 

and And then as i already mentioned do selective search. 
theThe motivation is that humans are competitive because they know humans do not when humans play games they do not look at all possibilities because they are not expected to do that. 
Tso they are very good at selecting those promising possibilities. 
soSo selective search will identify the promising possibilities and search only on the interesting portion and we get best first minimaxMINIMAX. which i will not discuss now 
so to end today’s lecture i will end with a set of questions and before i come to the question number one i willLet us learn about just define t the game of m p nimm, p Nim. 
(Refer slide time [51:06])

 

[51:06] 
The m, p nNim is a stones game where we have a matches game, there are p piles of m matches m matches in each pile. 
toTo move a player will remove any number of matches from exactly one pile and the winning player is the one who removes the final match. 
thisThis is the game of m, p Nnim. 
so the first question for this lecture asks you to consider the game of two2 two2 nim Nim. 
thatThat is, you start with twotwo piles each containing twotwo matches. 
soSo you have to draw the game tree part a. 
(Refer slide time [51:41])

   

part b) you mark on this tree the values at the terminal nodes. 
theThe terminal nodes will have a value of plus+ none1 if it i’s a win for max and  minus– one 1 if it i’s a win for min. 

[52:01] 
question c) aApply minimaxMINIMAX on this game tree and mark on the tree the backed up value on the root node and use these values to find the best starting move for max. 
part d) canCan max force a win in this game? 

part e) findFind out if alpha beta pruning on this tree gives rise to pruning some parts of this particular search tree. 
(Refer slide time [52:36])

 

then question number two2) 
 youYou are given this game tree. 
youYou are going to apply minimaxMINIMAX with alpha beta pruning on this game tree and identify the best possible move for max. very simple game tree 
(Refer slide time [52:55])

 

now if you have time we will discuss briefly the two questions that we had on lecture seven 

[53:01] 
questionQuestion number one 1 for lecture seven7 was: 
supposeSuppose the game tree for a particular problem has a branching factor of b. 
ifIf you do a p ply p-ply look ahead and then apply minimaxMINIMAX on this game tree how many nodes will you expand per move. 
so thisThis is actually quite easy. 
youYou have a tree whose branching factor is b and the tree is to the depth of p ply p-ply.
soSo this is p ply p-ply.
So, for every move you need to do a p ply p-ply full minimaxMINIMAX search where b dto the power d nodes are expanded. 
forFor each move you need to expand bdb to the power d nodes. 
(Refer slide time [53:58])

 

[54:02] 
question number two2) actually it should be two not one 
considerConsider minimaxMINIMAX search on a game tree with p ply p-ply search and another minimaxMINIMAX search on the same game tree with a q ply q-ply search. 
ifIf q is larger than p which of these searches is guaranteed to find a better strategy for the max player? 
soSo the question is that, if we do a p ply p-ply search and find the best move in those situations and we do a q ply q-ply search and then again find the best moves of find the minimaxMINIMAX value which of these searches gives rise to a better strategy? 
(Refer slide time [55:00])

  

[55:00] 
theThe answer is, we normally think that a q ply q-ply search is better than a p ply p-ply search if q is larger than p. Aand the intuition is that, we think that if we go deeper down the tree we will have more accurate evaluation functions as we are nearer to the terminal positions. 
soSo if we are at a terminal position then we know the correct value of a node. and And we assume that if we are nearer to the terminal position we have a better estimate of the correct value. 
Sso we expect that the leaves evaluated for a q ply q-ply search the leaves evaluated for these leaves will have a better estimate and based on these evaluations when you do minimaxMINIMAX we may find a better strategy than what we would find in at search having p- plies. 

[56:06] 
howeverHowever there is no guarantee here. 
soSo we cannot guarantee that a q ply q-ply search will be larger than a p ply p-ply search even though we expect a q ply q-ply search to be better. 
soSo the answer is, none of them it’s not guaranteed t it is not guaranteed that one of them will be better than the other even though the q ply q-ply search is expected to be better than a p ply p-ply search. 
with this we end today’s lecture 
thank you  

Artificial Intelligence (Lecture – 9)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

Constraint Satisfaction Problems – 1

[00:48] 
So today we start with lecture nine. 
This is the first lecture on constraint satisfaction problems. 
The instructional objectives of today’s lecture are as follows: 
(Refer slide time [01:02])

 

[01:00] 
Students will be introduced to the class of problems which we call constraint satisfaction problems. 
Students will learn how different types of constraints can be expressed in the formal manner. 
Students will also learn how constraint satisfaction problems can be modeled as search problems and they will also see how depth first search can be used with backtracking to solve these problem. 
We will also discuss how different heuristics can be used to make this search process more efficient. 
Students should be able to cast different types of constraint satisfaction problems as search problems in this framework. 

[02:02] 
Many problems that occur in Artificial Intelligence as well as in many other areas of computer science are different types of constraint satisfaction problems. 
Many of you must be familiar with the satisfyability problem the 3-sat.
You are given a propositional formula and you want to know whether this formula is satisfiable. 
That is, does there an assignment of values to the different propositions so that the formula evaluates to true? 
So satisfyability problem is a type of constraint satisfaction problem. 
Every variable can take exactly one value true or false and the formula must evaluate to true. 

[03:00] 

So, if you have a formula which is a conjunction of different clauses then each of these different clauses must be individually true for the entire formula to be true. 
(Refer slide time [03:23])

 

Different types of scheduling problem like the time tabling problem, different types of job shop scheduling can also we looked upon as constraint satisfaction problems. 
There can be different constraints among the different jobs. 
For example, job one may need to precede job five. 
So the precedence relationships among the jobs imposed are constraints among these jobs. 
Other types of constraints can also be imposed. 
For example, in the time tabling problem our objective is, we are given a list of class rooms, a list of courses, a list of teachers and students taking the courses. 
Our objective is to schedule courses to time slots and to class rooms so that at any time slot not more than one class can be scheduled at a class room at a given time slot. 

[04:33] 
A teacher at the same time slot cannot be teaching two courses. 
A student at a same time slot cannot be taking two courses. 
So these are the different types of constraints in the time tabling problem. 
There are other problems like supply chain management, the graph coloring problem, etc. 

[05:01] 
Then there is constraint satisfaction arising in machine vision in the age detection walls filtering and then different types of puzzles can also be looked upon as constraint satisfaction problems. 
Many of you have worked on crossword puzzles. 
You are given a rectangular grid and you have to fill up words row wise and column wise. And there are constraints because several words may share a common letter. 
So formally a constraint satisfaction problem consists of a set of variables x.
(Refer slide time [05:42])

  

So these are the variables x. 
For each variable xi belonging to x the variable xi can take its values from a domain di. 

[06:04] 
And di is a finite set of possible values. 
So we assume today that the domain of every variable is discrete and finite. 
We are also given a set of constraints restricting tuples of values. 
For example, we can have binary constraints. 
We may say that x5 and x7 cannot take the same value. 
So this is an example of a binary constraint or we can say that the value of xi must be numerically less than the value of the variable xj. 
So these are binary constraints. 
Unary constraint means constraints involving a single variable. 
We may say that xi must take values which are only odd integers. 

[07:00] 
So we can have unary constraints, we can have binary constraints and
we may even have constraints involving more than two variables. 
However, we will discuss constraints mainly involving two variables or binary constraints and we will later discuss how other types of constraints can be formulated in this term.
So, if the constraints concern only pairs of values we have a binary constraint satisfaction problem. 
A solution to a constraint satisfaction problem is an assignment of a value to each of the variables xi. 
So each variable xi can take values from its domain di and assignment of values to each of these variables which does not violate any of the constraints is a solution of the constraint satisfaction problem. 

[08:06] 
Let us look at an example of a graph coloring problem or a map coloring problem. 
(Refer slide time [08:13])
   

But I think we had occasion to discuss this problem when we discussed search. 
Here we have a map which consists of four regions: V1 V2 V3 and V4 and we are given three colors.
Let us say the colors are red, green and blue. And we want to know, can you assign regions to colors such that two adjacent regions cannot have the same color.
This problem is also called the map coloring because suppose you have a map we have different countries and you want to color the countries using colors such that two adjacent countries always have different colors. 

[09:15] 
A related problem is the graph coloring problem. 
In fact a map coloring problem can be transformed to an instance of a graph coloring problem involving a planar graph. 
(Refer slide time [09:37])

 

If we look back at the previous slide we see that V1 is adjacent to V2, V3 and V4. 
V2 is adjacent to V1 and V3, V3 is adjacent to V2, V1 and V4, V4 is adjacent to V1 and V3. 
We can model V1 V2 V3 V4 as the vertices of the graph and we have an edge between two nodes if V1 is adjacent to V2. 

[10:10] 
So we have an instance of a graph coloring problem where we want to assign colors to the vertices of the graph such that two adjacent vertices to not have the same color. 
So, we have a variable for each node and the domain for each of the variables is the colors red, green and blue. 
V1 can be red, green or blue, V2 can be red, green or blue, V3 can be red, green or blue, V4 can be red, green or blue and so on. 
So there is a constraint on each edge. 
All constraints are of the form that the color on one end point of this edge should be different from the color of the other end point of this edge. 
So these two nodes must be V1 and V4 and they must have different colors. 

[11:02] V1 and V2 must have different colors, V2 and V4 must have different colors and so on. 
The solution to this problem gives the coloring of the vertices. 
This is an example of a binary CSP. 
Similarly, the satisfyability problem of propositional formula can be also looked upon as a constraint satisfaction problem. 
(Refer slide time [11:33])

 

So, in the satisfyability problem in the formula we have different variables corresponding to each of these variables we have a variable in the CSP problem. 
The domain of each of the variables is either true or false. 
A proposition can be either true or false. 
The constraint corresponds to each clause. 
We disallow tuples which falsifies the clause. 
So, if we have a clause x1 or x2 bar we cannot have the situation where x1 is 0 and x2 is 1.. 
If x1 is 0 and x2 is 1 then this clause will not be satisfyable. Therefore if you have a conjunction of clauses the entire conjunction cannot be satisfied. 
So, here the constraints are, for each clause we must disallow those tuples such that the clause is falsified. 
We have a clause x1 bar or x4 or x5. 
We cannot have a situation that x1 is true, x4 is false and x5 is false. 

[13:03] 
So Sat is an example of a constraint satisfaction problem. 
However the general satisfyability problem is not a binary CSP.
A clause may have k variables so the constraints involved are a set of k variables. 
Thirdly let us look at the n queens problem. 
(Refer slide time [13:34])

 

We are given an 8/8 chess board and we have to place 8 queens in the chess board so that no 2 queens attack each other. 
So we can formulate this problem in this way. 
We have eight variables representing the positions of the 8 queens. 
Here 2 queens cannot be in the same row so let us say that x1 is the queen in the first row, x2 is the queen in the second row and x3 is the queen in the third row. 

[14:02] 
There has to be exactly 1 queen per row. 
So let x1 demote the position of the queen that is the column position of the queen in the first row.
So x2 denotes the column position of the second queen in the second row. 
Now, because no 2 queens attack each other the domain of xi is 1 2 3 4 up to 8 for each position in the row. 
(Refer slide time [14:32])

 

The constraints are, for any different i and j xi should not be equal to xj because we cannot have 2 queens in the same column so each of these variables must have a different value. 
Also 2 queens cannot be in the same column. 
So xi – xj cannot be equal to i – j or and xj – xi cannot be equal to i – j 

[15:06] 
That is, the queens cannot be either in the right diagonal or in the left diagonal. 
So these are the constraints for the n queens problem or its variation the 8 queens problem. And this is the same statement we have seen. 	
(Refer slide time [15:24])

 

So a CSP to summarize consists of a set of variables x consisting of x1 x2 up to xn. Each variable xi has a domain di from which it takes values. 
And d is a finite set of possible values. 
We have a set of constraints restricting the tuples of values. And a solution is an assignment of a value in di to each variable xi such that every constraint is satisfied. 
So let us formally define what we mean by constraints. 

[16:01] 
A constraint c i j k . . . involving the variables xi xj xk etc. 
So constraint can involve a single variable, it is called the unary constraint and 
unary constraint basically restricts the domain. 
We can have binary constraints which involve two variables, ternary constraints involving three variables and k ary constraints in general involving k variables. 
(Refer slide time [16:31])

 

So a constraint c i j k . . . involves the variables xi, xj, xk etc.. 
It is any subset of combinations of values from the domains of these variables xi xj xk which are allowed.. 	
That is, this constraint specifies that a subset of the Cartesian product of di dj and dk are allowed sets. 

[17:01] 
There are different ways in which we can express such constraints. 
We can specify as to which are the valid tuples. 
For example, we can say that suppose d1 and d2 are same and they are 1, 2 and 3 then let us say the valid tuples are (1,2) (1,3) (2,1) (2,3) (3,1) (3,2) and
the rest of the tuples are invalid.
Or we can specify constraints like x1 is not equal to x2 or constraints like x1 less than x2 or x1 equal to x2 + 1 and so on. 
So there are different ways in which constraints can be expressed. 
Another example is crypt arithmetic which is a type of puzzle. 
We have every letter standing for a digit and every letter stands for a different digit.
(Refer slide time [18:00])

 

[18:00] 
We have to find an assignment of letters to digits such that a given arithmetic formula is correct. 
For example, we have this formula: 
Send + more = money and the variables are D, E, M, N, O, R, S, Y and the domains of these variables are, or rather domains for D, E, M, N, O, R, S, Y are the digits from 0 to 9. 
And s and m cannot be 0 so s and m must have values from 1 to 9 and each of these variables must have different values. 
We want to know if there is a solution to this problem. 
That is, is there an assignment of values to each of these variables such that this arithmetic expression is correct? 
(Refer slide time [19:01])

 

[19:01] 
So how do we specify the constraint for this crypt arithmetic problem? 
We can write one long constraint for the sum. 
It is 1000*S + 100*E + 10*N + D + 1000*M + 100*O + 10*R + E = 10000*M + 1000*O + 100*N + 10* + Y. 
So this is the sum constraint but there are also other constraints. 
(Refer slide time [19:32])

 

We have to specify that S is not equal to E, S is not equal to O, S is not equal to R, S is not equal N, M is not equal to 1 and so on.
 Or we can express it as a single constraint on all the variables. 
We can say that the values of each of these variables must be different by saying all different D, E, M, N, O, R, S, Y.
These two constraints the sum constraint and this constraint together precisely characterize this problem. 

[20:06] 
Now let us see that constraint satisfaction problems can be looked upon as search problems. 
(Refer slide time [20:16])

 

It is a kind of search in which a state is not indivisible. 
 We will look at the formulation. 
A state consists of assignments of values to the different variables 
So state is factorized into the states of the different variables inside the state. 
The search involves finding an assignment of values to these variables, finding that state which corresponds to a particular assignment of values to these variables. 
So these constraints provide the structure to the state space. And we will discuss about backtracking algorithms which can be very well done with depth first search and that can work well for these problems. And we can use other methods along with backtracking including constraint propagation, variable ordering and different preprocessing steps to make this search more efficient. 
(Refer slide time [21:24])

  

Now let us see how CSP can be looked upon as a search problem. 
The states are the nodes in the graph, the operators are the arcs between the nodes and then we have to know what the initial state is and what are the goal states. 	
For example, suppose we take this problem the n queens problem for n = 4 and for this problem the initial state is the state where none of the variables are assigned values.
(Refer slide time [21:55])

 

That is, we have not placed any queens on the board. 

[22:00] 
The neighbors are the cases where the first queen is assigned. 
The first queen can be assigned either here or here or here or here or here. 
So let us say the first queen corresponds to the first column. 
The first queen can be assigned 1 or 2 or 3 or 4. 
If the first queen is assigned 1 the second queen cannot be assigned 1 because the constraint is violated, 2 queens cannot be in the same row. 
The first queen cannot be assigned 2 because the diagonal constraints will be violated. 
If the first queen is in 1 the second queen can be in 3 or 4 
If the second queen is in 3 the third queen cannot be in 1 or 3. 

[23:02]
 It cannot be in 2 or 4 because diagonals constraints will be violated. 
So, after we have placed the first queen here and the second queen here we cannot place the third queen anywhere. 
If we cannot place the third queen anywhere wherever we place the fourth queen the problem cannot be satisfied because the constraint of third queen is violated. 
So this path is fruitless so we can terminate search below this. 
Then we can look at this sibling that the first queen is in first row and the second queen is in fourth row. 
Now the third queen can be placed neither in 1 nor in 4 nor in 3 but only in 2. 
So you place the third queen in 3 and then we have to see whether we can place the fourth queen anywhere.
If we place the third queen in 2 then the fourth queen cannot be placed in 1 or 2 or 3 or 4. 

[24:11] 
So we have to prune search below this node. 
Now, when you prune search below this node we have to backtrack and find out the next place where we should explore the state space. 
So, as you can see the nature of this search space is such that we can start from the root node and we can do a depth first search. And whenever we generate successors we only generate those successors that do not violate any constraints. 
If a constraint is violated at a node then further assignment of values to the other variables cannot help resolve that constraint. 

[25:00] 
So, exploring that region of the search tree is fruitless. 
So if a constraint is violated we can abandon that path. 
If we reach a node so that we cannot place some variable then we cannot assign any value to the one of the variables, then we can prune that search portion of the search tree and we backtrack to our next choice point. 
So depth first search with backtracking seems to be a good solution to such search problems. 
Binary CSPs are the special types of constraint satisfaction problem which involve constraints between two variables. 

[26:02] 
Suppose this is an assignment of values to variables 
(Refer slide time [26:07])

 

suppose xi is assigned aI, xj is assigned aj and they are consistent with a set of variables xm to xn if and only if there exists a tuple of values am up to an such that xi aI, xi assigned to aI, xj assigned to aj, xm assigned to am, xn assigned to an this whole thing is consistent. 
In a search tree a particular node in the middle of the search tree denotes a partial assignment of values to the variables. 
This partial assignment can be explored further only if there is a full assignment of values to all the variables which includes this partial assignment and which satisfies all the constraints. 

[27:05] 
We will use this property later to see where we can prune the search space even further. 
So, when we have a constraint satisfaction problem we can look upon this as a search problem and depth first search seems to be a very good technique to solve such search problems.
 But if you are able to propagate constraints then we can improve the efficiency of the search. 
Let us discuss different ways of constraint propagation including forward checking and maintenance of arc consistency. 
(Refer slide time [27:48])

  

So the backtracking framework we will use for constraint satisfaction problem basically involves that consistency check is performed in the order in which the variables are instantiated. 
(Refer slide time [28:09])

 

[28:09] 
Whenever we instantiate a variable we check its consistency with respect to the variables we have already assigned. 
If the consistency check fails at a particular point we look at the next possible value of the current variable. 
If there are no more values for the variables which are consistent with the previous assignments then we backtrack to the most recent choice point. 
So this is the sense of chronological backtracking which is the basic framework of search for CSPs. 
Let us look at the depth first search algorithm for solving CSPs. 
(Refer slide time [28:55])

 

The initial state is the empty assignment. 
That is, none of the variables are assigned. 

[29:01] 
All variables are unassigned. 
In the goal state all the variables will be assigned values from their domain and all the constraints must be satisfied. 
The successor function assigns a value to any variable which is as yet unassigned such that this assignment does not violate any constraints with respect to what has already been assigned. 
All CSP search algorithms generate successors by considering possible assignments for only a single variable at each node in the search tree. 
When we start from the initial node where all the variables are unassigned there are n candidate variables. 
Each of them can take different values. 
So there are potentially many successors of the first node. 

[30:00] 
So what we do is, instead of trying all these values at once we pick one of the variables and for that variable we try to assign the different possible values and that is how we model the search tree for state space. 
In the goal test when the assignment is complete and by the way in which we have assigned we have to make sure that no constraints are violated. 
So if we have able to get a complete assignment to all the variables so that no constraints are violated we have reached a goal state. 
If there are n variables typically if we get to a node which is at depth n then we have found a solution to the CSP. 
Now in constraint satisfaction problem there may be different objectives. 
Your objective may be to find one solution any one solution or all solutions. 

[31:00] 
If you want to find all solutions you must explore the entire tree until except those where you have pruned the search. 
If you want to find only one solution you stop as soon as you get a solution. 
If there is not solution you have to again explore the entire tree to rule out all possibilities. 
What is the path cost? 
Actually path cost is not very important. 
So we can say that the cost is 1 for every step so assigning n variables will take a cost of one. 
(Refer slide time [31:38])

 

So this algorithm can be recursively formed because we are basically using depth first search we can have a recursive formulation of this algorithm. 
So this is a simple recursive function which captures this basic algorithm. 
Recurse (assignment, csp). 

[32:01] 
If the assignment is complete, that is all variables are assigned values then we return the current assignment as the solution. 
Otherwise we select an unassigned variable. 
How do we select? 
We can simply select the next one in the order or we can select smart key. But we select one unassigned variable and for each value in the domain of the variable. Assuming that we order the possible values of the variable can be taken in a particular order. 
One by one we consider the different values or we can be smarter and we can decide in what order we should test the values so that our search effort is minimized. 
In any case for a given order we take up values one by one and if that assignment of that value to this variable is consistent we add this variable value tuple to the assignment and we call recurse again with this new assignment. 

[33:17] 
If recurse succeeds then we return the assignment. 
Otherwise if this recurse does not succeed we remove this assignment and try the next choice. 
If we have not been able to succeed then we return failure. 
If none of these assignments are successful then we return failure. 
So this is a simple recursive structure of the basic dfs problem for backtracking which solves the csp. 

[34:03] 
If we want to consider the efficiency of the csp problem there are various things that we can consider. 
Among the unassigned variables so far which variable should we pick next?
And then, for a given choice of variable, in what order the values of the variables should be tried? 
(Refer slide time [34:30])

  

Secondly how does the assignment to the current variable influence the assignment for other unassigned variables. 
So, if the current variable is given a certain assignment that will affect the assignment of values to the other variables because the constraints that are imposed by this current assignment is carried over to the other variables which converts the constraints other variables including the currently unassigned variables. 

[35:05] 
So we may have to consider this effect in deciding which variable or which variable value pair we will try next. 
Thirdly when a path fails, suppose at a particular point there is a failure there’s an inconsistency can the search avoid repeating this failure in subsequent paths? 
These issues need to be discussed to see how we can try to make this search more efficient. 
We have discussed a heuristic search, different types of heuristics in csp, we do not normally use heuristics.
Instead we try to see how these issues can be tackled and these issues have the effect of reducing of improving search efficiency. 

[36:02] 
 First let us take up the issue of variable ordering. 
When we want to select the next unassigned variable we can use the following heuristic which is called the minimum remaining value heuristic or which is also called the most constrained variable heuristic which states choose the variable with the fewest legal values. 
(Refer slide time [36:31])
	
 

Every variable has a domain to start with. 
 When we assign values to some of the variables the domain of the other variables get restricted. 
Suppose V1 and V2 are adjacent in the graph coloring problem, if I assign V1 to red V2 could initially be either red or green or blue but if V1 is assigned red and V2 is adjacent to V1 then V2 cannot be red so V2 must be either green or blue. 
If V2 is also adjacent to V3 and V3 is green then V2 also cannot be green 
so V2 cannot be red or green it has to be only blue if at all.
Therefore the domain of V2 has to be restricted. 
After we have made partial assignment to some of the variables we look at the remaining variables and their valid domain with respect to the current assignment. 
We choose that variable whose domain is smallest. 
Suppose V2 has only one value in its domain blue so we can just assign V2 to blue and that is the only choice. 
So we choose the variable which has the least number of legal values. 
In the beginning suppose when we have not assigned any variable to any value we can use another heuristic which is called the degree heuristic. 

[38:04] 
In the degree heuristic we select the variable which is involved in the largest number of constraints with other unassigned variables. 
We select variables one by one but finally we have to select all the variables. 
If we choose to select a variable so that it constrains severely the domains of the other unassigned variables it can help to reduce future search. 
The degree heuristic is useful especially in the beginning or in the case where the minimum remaining value heuristic is a very good heuristic and we usually try it first. But we can use the degree heuristic to resolve ties between two variables which have the same importance according to the mrv heuristic. 

[39:05] 
So minimum remaining value heuristic or the most constrained variable heuristic is a heuristic which is used for choosing the next variable to a sign and it is very effective. 
If you run different instances of constraint satisfaction problem you can test the effectiveness of this heuristic. 
The degree heuristic is also useful and is often used to break ties. 
Once we have picked which variable to try let us consider the order in which we will consider the values we assign to the heuristics. 
Here the heuristic used is called the least constraining value heuristic. 

[40:04] 
We prefer to first try that value that rules out the fewest choices for the neighboring variables in the constraint graph. 
So we will choose the values one by one. 
First time when we choose a value and suppose our objective is to find one solution to the csp problem then we will be very happy if we quickly reach a solution because after that we can stop our search. 
Of course in those cases where we want all the solutions or in those cases where a solution does not exist we have to look through the entire search space. 
The order in which we choose the values do not matter because we have to go through the entire search space. But if we interested in only one or a few satisfying solutions then we try to choose that value for which there is a greater hope that a solution will be found. 

[41:04] 
So we choose that value which is least constraint. 
That is which rules out the fewest choices for the remaining variables or at least the neighboring variables in the constraint graph. 
Secondly, for csp problems we do depth first search and whenever we find an inconsistency we backtrack. 
 Inconsistencies arise because the current assignment of variable value does not agree with the previous assignments. 
In those cases we backtrack. 
It is possible to take this effort one step further. 

[42:01] 
Whenever we assign a value to a variable we propagate constraints to the future variables to the unassigned variables. And then if we notice the other variables as a result of current assignment there is a variable whose domain becomes null. 
That means there is another variable which is not consistent with the current partial assignment. 
If we can discover that we can terminate our search. 
This can be achieved by various types of constraint propagation.
There are many algorithms which do constraint propagation to different degrees. 
We will discuss briefly two types of algorithms forward checking and arc consistency. 
(Refer slide time [42:51])

 

 (Refer slide time [43:03])

 

[43:03] 
Forward checking is the simplest type of constraint propagation. 
The idea is simple. 
When I assign a value to a variable and if we find that it is inconsistent with some other variable, another variable that cannot take any valid value for the current assignment then we can terminate the search at this point. 
This is the idea of the algorithm. 
So whenever we assign a value to a variable we use that assignment to restrict the domains of the unassigned variables. And if we find that the domain of some other future variable becomes null we terminate the search at this point. 
This is the idea of forward checking. And how do we carry out forward checking? 
With every variable we keep its current value domain. 

[44:00] 
So whenever we assign a variable to a value we update the domains of the other variables. 
So we use a data structure. 
The data structure is, for every variable xi we maintain its current domain cdi. 
Initially cdi is equal to the whole domain of the variable, cdi = di to start with. 
When we set variable xj equal to particular value v we remove xi = u from the domain of xi if some constraint is not consistent with both xj = v and xi = u. 
My current assignment is xj = v. 
Therefore xi is an unassigned variable and u is one of the values in its domain. 
But if xi = u and xj = v violates some constraints then we remove u from the domain of xi. And in this way if we find that a particular variable xi has its current domain cdi to be null then we can stop search beyond the current point. 

[45:29] 
Now let us briefly discuss how constraint propagation is used in the graph coloring problem. 
(Refer slide time [45:36])

 

So in a graph coloring problem we start with all the nodes unassigned then we pick a node and assign it a color. 
Assign it possible colors from what its current domain is. 
After we instantiate a node with a color we propagate the color. And how do we propagate the color? 

[46:02] 
We remove the color. 
Suppose we have colored the current node red we remove red from the current domain of all its uninstantiated neighbors. 
By this process if any of these neighbor domains become empty then we backtrack.
Now for each n in these neighbors if n previously had two or more available colors but now has only one color. 
Suppose there is a neighbor, earlier its domain cdi had two or more colors but as a result of this constraint propagation xi has only one valid color left on the domain, now I can use this color to propagate constraints further.
So I can take n assign n that color because that is the only color n can be assigned and propagate this constraint so that its neighbors get affected. 

[47:08] 
So we call propagate color nc. 
So this is the code for propagating constraints for the graph coloring problem. 
Whenever we assign a node the color we call propagate color node color. and propagate color node color has three steps: 
1) Remove color from all of available lists of the uninstantiated neighbors of node 
2) If any of these neighbors gets the empty set then this is an inconsistent situation so we backtrack. 
Now we look at these neighbors. 
Earlier its current domain had two or more colors but as a result of this propagation currently that node n has only one color c in its current domain then we call propagate color nc.
So this is the algorithm for propagating constraints in the graph coloring problem. 
Let us look at one more example of forward checking. 
(Refer slide time [48:23])
 

We have variables x and y. 
The domains of x and y are 1, 2, 3, 4, 5. 
The constraint is that x is less than y – 1. 
So initially the current domain of both x and y are 1, 2, 3, 4, 5. 
(Refer slide time [48:37])

 	

Suppose we set x = 2 then y has to be greater than equal to x + 1 so y can only be either 4 or 5 so cdy becomes 4 or 5. 
Now, if we set x = 4 then cdy becomes null because there is no value in its domain. 

[49:01] 
So, if we set x = 4 we have to retract this choice and then backtrack. 
(Refer slide time [49:08])

 


I will end this lecture by setting two questions for you to consider answering. 
1) Give precise formulations for the following problem as a constraint satisfaction problem. 
This is the class time tabling problem. 
You are given a situation where there are a number of teachers and given number of classrooms. 
You have a list of courses that have to be offered and you are also given a list of time slots for these courses. 
Each teacher has a set of classes which the teacher can teach. 

[50:05] 
Now your objective is to schedule the classes, schedule the courses to time slots, to teachers, to class rooms and so on. 
So you have to consider the different constraints in this problem and pose it as a constraint satisfaction problem and indicate how you will go about solving such problems. 
(Refer slide time [50:40])

   

2) You are given the following crypt arithmetic problem: 
Send + more = money, and your job is to assign digits to s e n d to each of these characters so that each of these characters correspond to unique digits 
m and s cannot be 0 and this arithmetic relation is satisfied. 

[51:18] 
We have already seen how we can pose this as a constraint satisfaction problem. 
Your job is, firstly to solve this problem on paper using backtracking and for a particular ordering of the variables and values. 
You choose a particular ordering of the variables. And given with that particular ordering you draw the dfs tree and indicate where backtracking will occur. 
The second part to this question, you have to do this problem again but this time you have to use the minimum remaining variable heuristic to choose which variable you should consider next. 
So use the minimum remaining variable heuristic and check if it has any effect on pruning of the search tree for this particular problem. 
The third part to this question, you use forward checking of the type that we discussed today. We use constraint propagation or forward checking on this problem and see how you can work on it. 
 
Artificial Intelligence (Lecture – 10)
Prof. Sudeshna Sarkar
I.I.T Kharagpur

Constraint Satisfaction Problems 2

[00:53] 
todayToday we start with the lecture ten of the course artificial Artificial Iintelligence. 
inIn the last class we introduced what you mean by constraint satisfaction  problems and we looked at how we cast such problems as search problems and solve them by depth first search with backtracking. 

[01:13] 
todayToday we will explore some more efficient techniques for solving constraint satisfaction problems. 
(Refer slide time [01:25])
 

theThe instructional objectives of today’s class isobjectives of today’s class are as follows: 
inIn this class the student will be introduced to more efficient search for constraint satisfaction problem. 
weWe will talk about the following strategies: 
forwardForward checking and 
 then we will look at constraint propagation algorithms like the a c threeAC-3 algorithm. 
thenThen we will briefly talk about intelligent backtracking or back jumping. 
onOn taking these topics the student should be able to apply these techniques to constraint satisfaction problems. 

[02:11] 
so beforeBefore we start let me try to brieflyus recapitulate the formulation that we did for constraint satisfaction problems. in the previous class 
so ifIf you remember, the sort of constraint satisfaction problems we are looking at we had a number of variables and each variable has a domain. 
soSo v onev1 is a variable and v onev1 has a domain 
it can take a set of values. 
v twov2 is another variable 
 it can take some set of variables some set of values. 
soSo we have different variables along with their domains. 

[03:02] 
ourOur objective is to assign variables to values and this thethe value for a pa particular variable must come from its domain and the constraints must be satisfied. 
so constraints weWe especially looked at a most common type of constraint called binary constraints. 
binaryBinary constraints involve a pa pair of variables. 
supposeSuppose we can have a constraint saying that v onev1 and v twov2 cannot have the same value 
 this is one type of constraint. 
weWe can say that the constraint between the variables v twov2 and v threev3 is that the value of v threev3 must be greater than the value of v twov2. 
soSo we have a set of variables, 

[04:01]  each variable has a domain. 
theThe variables must be assigned values from the domains such that the constraints are satisfied.
(Refer slide time [04:09])

 
thisThis is a general class of constraint satisfaction problems. 
now theseThese constraint satisfaction problems can be cast as state space search problems and we can do the search in the following mannermanner: 
weWe pick up a variable, 
 so this is the start, when none of the variables are assigned any values
 so v onev1 is not assigned a value,  
v twov2 is not assigned a value, 
v threev3 is not assigned a value and so on. 
soSo this is the empty state where none of the variables are assigned any values. 
inIn the next step we look at the assignment of different values to variable v onev1. 
supposeSuppose variable v onev1 can take the values one two four seven1, 2, 4, 7 
so  we have four4 branches corresponding to the all the possible values that v onev1 can take. 

[05:09] 
inIn the next state we have to pick up another variable. 
letLet us say we pick up v twov2 and we have to assign we have to look at all possible assignments of values to v twov2 that are consistent with the assignment that we have so far. 
forFor example, if v onev1 has the domain one two five seven sorry one two four seven1, 2, 4, 7 and v twov2 hashas the domain two three four2, 3, 4 t 
thenhen if v onev1 is already one v twov2 can be either two2 or three3 or four4. 
ifIf v onev1 is two2 then v twov2 can be either three 3 or four 4. 
weWe cannot have v twov2 equal to= two2 because any further assignment after we have v onev1 equal to= two2, v twov2 equal to= two2 will not undo this constraint. 

[06:07] 
So so if we have an assignment which is inconsistent we should not proceed further in that direction. 
Next, suppose we have another variable v threev3 which has a domain 
let us say one1 and five5 and we have the constraint that the value of v threev3 must be greater than the value of v twov2 
then in this case v threev3 can only take the value of five 5, 
inin this case v threev3 again can only take the value of five 5 and so on 
soSo, in this way we can construct the search tree corresponding to the constraint satisfaction problem and we note that a solution to the constraint satisfaction problem is a leaf in the search tree which corresponds to all variables being assigned to specific values that do not violate any constraints. 

[07:18] 
Therefore so all the leaves at level n if there are n variables all the leaves at level n are the different solutions to the search problem. and And the strategy which is appropriate for solving such search problems is depth first search. 
wheneverWhenever we find that a constraint is violated we do back tracking.
thisThis back tracking is called chronological back tracking because we backtrack to the previous choice point. 
soSo, depth first search with chronological back tracking is a very appropriate method for solving constraint satisfaction search problems. 

[08:03] 
in the last class weWe also discussed that we can try to make this search efficient by looking at a pa proper ordering proper order in which we choose which is the next variable that we select nt next for assignment assignment.
soSo this is a heuristic which might so if you take a pa proper order of the variables we might end up in reaching the first solution much faster. and And then once we pick a variable to assign values tootoo we will be considering in what order should we assign values to these variables. 
(Refer slide time [08:53])

 

thenThen we will look at other questions like, can we detect inevitable failure earlier? 

[09:00] 
So, in the general search problem we said that whenever we see that a constraint is violated we terminate the search at that point and then backtrack to the previous choice point.
 butBut it may be the case that future problems can be detected even earlier on. and today we willLet us see some techniques to handle that and then there’ is a question whether we can take advantage of the problem structure to further prune the search specific to a pa particular problem. 
now as we already discussed in the last class oneOne heuristic to choose the next variable is the minimum remaining value heuristic. 
(Refer slide time [09:51])

 

inIn the minimum remaining value heuristic the variable with the fewest remaining values is chosen next. 
whatWhat do you mean by variable with the fewest remaining values? 

[10:01] you see we said that
 everyEvery variable has a domain. 
now as a result soSo when we come when we have a pa partial search tree some of the variables have been already assigned values and as a result of these assignmentassigning  of values let us say to v twov2 and v threev3 the domain of v onev1 has become restricted, 
 the domain of v fourv4 has become restricted, 
 domain of v fivev5 has become restricted. 
soSo we find out the size of the domains of the remaining variables and we pick the variable for whomwhich the remaining domain is smallest in size. 
theThe intuitive argument the intuition behind this is, that the variable which has most constraints since in the constraint satisfaction solutions or constraint satisfaction problems all variables have to be assigned values, 

[11:03] the variable which is most constrained will have the fewest remaining values. 
soSo we need less of backtracking in order to consider the effect of that on the search. 
next onceOnce you have chosen a variable we have to choose the order of value assignment. 
(Refer slide time [11:30])

 

andAnd here the heuristic is, choose the least constraining value. 
you see allAll variables have to be assigned value and the solution but each variable has to be assigned only one value. 
soSo our intuition is, choose a value which is most likely to take you to a solution. andAnd so take consider that value which constraints the domains of the remaining variables the least. 
thatThat is the value that rules out the fewest values in the remaining variables. 

[12:08] 
now we also discussed briefly that apartApart from the depth first search formulation that we are considering we can propagate constraints earlier on. 
(Refer slide time [12:25])

 

so insteadInstead of considering variables one at a time we look at the constraints early on to reduce the search space. andAnd there are several ideas which people have formulated which gaveave rise to different algorithms for constraint satisfaction search and 
 the simplest of these ideas is forward checking. 
so whatWhat is forward checkingchecking? 
(Refer slide time [12:50])

 

weWe keep track of the remaining legal values for the unassigned variables. 
So, when we assign values to a variable we suitably constrain the domains of the remaining variables. andAnd then if we detect that a variable there is a variable which does not have any legal values left 

[13:14] then we can immediately terminate that path because along a path finally we have to assign values to every variable along a correct solution path. 
soSo, if you find that there is a variable which does not have any legal values left we will not be able to assign values to the variables 
so that search path is bound to fail. 
so ifIf we can detect this early on we can terminate search at that point of the search tree. 
so for example Tthis is an example of a graph coloring problem where we have fivefive nodes and there are threethree colors red green and blue and these are the links in the graph.
(Refer slide time [13:57])

 	 

[14:01] 
so correspondingCorresponding to this problem suppose we show how a constraint satisfaction search proceeds. 
(Refer slide time [14:11])

 

so initiallyInitially suppose we pick the variable m h node MHm h we can paint is red green or blue 
weWe paint m hMH = red 
then if we pick k KNn next KNk n is a neighbor of MHm h 
 so KNk n can be either green or blue. 
ifIf we pick KNk n equal to= green and then we pick KEk e next 
KEk e is a neighbor of both MHm h and kKN n 
 so KEk e has to be red sorry k e is a neighbor of KNk n 
 so it has to have a different color than KNk n 
so it can be either red or blue. 
soSo this way we can construct the search tree and we can do depthdo a depth first search. 
(Refer slide time [14:56])

 

Now, if we do forward checking let us see how we will proceed. 

[15:00]
 initiallyInitially we have fivefive variables MH KN KE AP m h k n k e a p and TN.t n .
eachEach of these variables can take each of the threethree values red green and blue. 
whenWhen we set m hMH equal to= red k nKN and a pAP are neighbors of m hMH 
so they can be either g G or B,b 
 the others can be r g bRGB. 
ifIf we pick nk now k nKN equal to= green then a pAP which is a neighbor cannot be green 
so it can only be blue,  
k eKE and t nTN can be either r bRB or RBr b 
 they are also neighbors of k nKN. 
thenThen we pick k eKE equal to= red 
as a result a pAP and t nTN can be only blue. 
nowNow if you take a pAP blue we find that t n the domain of t nTN is empty because t nTN is a neighbor of a pAP. 
soSo we detect here that there is a pa problem with the assignment and we backtrack. 

[16:03] so
 thisThis is what forward checking does. 
now you see that letLet u’s look back at the figure. 
you see atAt the four4th step we found that k eKE and t nTN have only blue left in their domains. 
nowNow k eKE has a legal value left,  
t nTN has a legal value left but we also know that there is a constraint between k eKE and t nTN. 
k eKE and t nTN cannot be of the same color. 
so weWe notice here sorry we are talking about a pAP and t nTN. 
theyThey cannot be of the same color because they are neighbors. 
soSo in this step you see that a pAP and t nTN have only one legal value left. 
so evenEven though they are individually legal together a pAP equal to= blue and t nTN equal to= blue cannot happen. 
soSo a smart algorithm might detect this early on and terminate search. 
(Refer slide time [17:04])

 

[17:04] 
so thisThis is a limitation of forward checking. 
forwardForward checking checks that every remaining variable has at least one legal value but it does not explore further whether these values can be assigned to this variable subject to the constraints between these variables. 
so forwardForward checking detects some of the inconsistencies but it does not detect all inconsistencies. 
forFor example, in the previous example it does not detect that a pAP and t nTN cannot be blue simultaneously. 
(Refer slide time [17:45])

 

nowNow let u’s look at another constrained graph on which we will run the next algorithm.
so in this graph againAgain this is a graph coloring problem. 
weWe have six6 nodes which we have numbered one two three four five1, 2, 3, 4, 5 and 6six  and this is the connectivity information between the nodes. 

[18:09] 
eachEach of the nodes can be colored red, green or blue but twotwo neighboring nodes cannot be assigned the same color. 
nowNow, if we do forward checking let’ us see what happens. 
(Refer slide time [18:24])

  

supposeSuppose we first pick up one 1 and we assign one 1 to red. 
nowNow, if iI assign one 1 to red two 2 and three 3 cannot be red 
 so we reduce the domains of two 2 and three 3 w
wee have blue or green.
 
soSo, for each variable which is in constraint with one 1 we eliminate the conflicting values from the domain.
now nextNext is, suppose we assign green to 4four 
(Refer slide time [19:03])

 

[19:03] 
now if we assign green to four then 2, 3two three and 6six  cannot be green 
 so we remove green from the domains of two 2, 3three and 6six. 
nowNow two 2 can be only blue,
 3three can be blue, 
 6six can be red or blue. 
nextNext we assign blue to five 5 and as a result it affects the domains of three 3 and six 6. 
soSo we remove blue from the domains of three 3 and the domains of six 6. 
(Refer slide time [19:44])

 

nowNow as a result we see that the domain of three3 becomes empty. 
soSo this is impossible 
 so we have to back track at this point. 
so letLet u’s now look back and seefind out what is the weakness of forward checking. 

[20:04] you see 
whenWhen we assigned for green we saw that two2 and three3 have only one legal value left which is blue but these twotwo legal values cannot be assigned together 
 so we should have detected this problem early on, 
 forward checking does not detect this. 
(Refer slide time [20:29])

 

soSo this is not detected by forward checking. 
weWe should have backtracked at this step. 
(Refer slide time [20:37])

 

nowNow let us see what further improvements we can do. and so thereThere are several things that we could do.
oneOne is we could try to propagate the implications of the constraint on one variable on to the other variables. 
weWe propagate not just values but constraints between these variables. 

[21:04] 
so thisThis is the idea of constraint propagation that we will explore next. 
(Refer slide time [21:12])

 

secondlySecondly what we will see after that is, that we can backtrack intelligently which we call conflict directed backtracking. 
soSo we will save possible conflicts for a variable value when we assign that variable value and we will back jump to that assignment which gave rise to this conflict. 
inIn chronological backtracking on normal depth first search we backtrack to just the previous choice of point. 
inIn back jumping when we detect a constraint violation we backtrack to that assignment which led to the conflict which could have taken several decisions earlier. 
soSo first, let u’s explore constraint propagation. 
(Refer slide time [22:06])

 

[22:06] 
now arcArc consistency is a fast method for constraint propagation. 
so thisThis is how arc consistency works: 
givenGiven the current domains of v onev1 and v twov2 the arc from v onev1 to v twov2 is consistent if, for every value x of v onev1 there is some value y of v twov2 that is consistent with x. 
so inIn forward checking which is said that check if the domain of in the remaining legal the legal domain of v onev1 is empty or not. 
hereHere we are not only checking whether the domain has a value but 
 we are just checking whether for every value that’s left in the domain there is some value y in the domain of another variable so that these twotwo assignments of values are together consistent. 

[23:01] 
thThisat’s is what constraint propagation does. 
thereThere are several constraint propagation algorithms. 
weWe will look at a cAC- three3 and some algorithms like that. 
letLet u’s see an example of the same graph that we discussed earlier. 
(Refer slide time [23:24])

 

 weWe assign green to four4.
afterAfter we assigned green to four4 we check all the arcs from four4, two2, three3 and six6. 
first we check two2. 
soSo when we check four4 equal to= two2 we eliminate green from the domain of two2. 
then butBut we have to check back to later with the variables with which two2 has some constraint constraint.
(Refer slide time [23:54])

 

nextNext we check the constraint four4 to three3 and eliminate green from the domain of three3. 
afterAfter that we check the constraint four4 to six6 and eliminate green from the domain of six6. 

[24:09] 
now afterAfter we have finished checking the neighbors of four4 we go back to those variables whose domains have been changed to check if their current remaining values are consistent with the values of their neighbors. 
(Refer slide time [24:27])

 

so weWe now go back to two2 and check the neighbors of two2. 
firstFirst we check two2 with one 1
 there is no violation,
 there is no problem. 
thenThen we check two2 with three3 and then we notice that if two2 is blue three3 cannot be blue 
(Refer slide time [24:45])

 

so we eliminate blue from three3. 
ifIf we do that the domain of three3 becomes empty which cannot happen 
so we backtrack at this point. 
thisThis is the arc consistency algorithm and we can backtrack when we detect that there are variables which do not have domains which are consistent together because of the constraints between those variables. 

[25:13] so
 thisThis is the idea of arc consistency. 
(Refer slide time [25:18])

 

wheneverWhenever the domain of a variable is revised the other arcs may need further revisions until no more inconsistencies remain. 
now oneOne implementation of the arc consistency algorithm is the a cAC- three3 algorithm. 
tThe a cAC- three3 algorithm uses a queue to keep track of the arcs that nt need to be checked for consistency because as we said that when we assign four to blue four4 to green it i’s not enough that we change we check the domains of two2,  three3 and six6 which is neighbors of four4 but if these domains did change we have to check two2 with its neighbors and do this recursively until there is no further constraint propagation. 

[26:14] 
soSo a cAC- three3 does this constraint propagation and in order to keep track of so whenever the domain of a variable is reduced according to this algorithm you propagate the constraints to the neighbors of that variable. 
so thisThis is done recursively in the AC-a c three3 algorithm. 
nowNow let’ us look at an implementation of the a cAC- three3 algorithm. 
(Refer slide time [26:42])
 

so weWe have a data structure of a queue. 
now initiallyInitially the queue contains all the arcs of the graph. 
soSo we have all the v iVI Vv j that are in the graph in the arcs that are the edges of the graph such that i0 not equal to= j. 

[27:08] 
thenThen while q is not empty we select one of the arcs from this queue that is v kVk, v mVm and we delete v kVk, v mVm from queue. 
thenThen we check if revise v kVk,  v mVm is true. 
thatThat is, when we delete this arc we see if the domains get reduced. 
whenWhen we consider this constraint between v kVk, v mVm we check if the domains of these variables get reduced.
ifSo that is true then so as a result of assignment of v mVm if the domain of v kVk gets changed then we check we find all the arcs in the graph of the form v iVi, v kVk. 

[28:02] 
thatThat is, if v iVi is a neighbor of v kVk then we have to check those constraints. 
soSo we append to the queue all the arcs vV ii, v kVk. 
ifIf as a result of an assignment to v mVm the domain of v kVk gets changed. 
nowNow let us see what is the complexity of this algorithm AC-a c three3? 
Now, if we have a binary constraint satisfaction problems then the constraints are only between pa pair of variables. 
soSo there are utmost n2t n square arcs in the constrainedt graph. 
(Refer slide time [28:58])

 

you see inIn the algorithm every arc can be inserted utmost d times if the domain of v iVi has d values. 

[29:12] 
So, when we are revising when we are putting an arc involving a variable into the queue we are only doing it when the domain of the variable is reduced. 
so supposeSuppose initially the domain of a variable let us say v kVk has b values 	 
 we consider this arc for revision only when this domain is getting reduced. 
so every that is correspondingCorresponding to every v kVk the arc can be put into the queue utmost d times. 
Now, checking the consistency of an arc can be done in order d2 square times because we check it against all the arcs in the graph. 

[30:11] 
Sso the worst case time complexity of this algorithm is n2 square times d times d2 square which is O(“  “ of n2 square d3 cube ).	
soSo O(n2 d3)o n square d cube is the worst case complexity of the a c threeAC-3 algorithm. 
now that’sThis is what constraint propagation is about. 
inIn constraint propagation as a result of reduction in the domain of a variable we look at the implications in terms of the constraints of the variables with other variables. andAnd there are a set of algorithms which do constraint propagation propagation.

[31:08] 
one of the well known algorithms is a cAC- three3 which we discussed. 
nextNext we will consider another technique which we mentioned as intelligent backtracking. 
(Refer slide time [31:22])
  

weWe have already seen backtracking which ahs has a very important role in solving the constraint satisfaction problems. 
soSo what you have in plain depth first search is chronological backtracking. 
whenWhen the branch of a search fails we need to backtrack to the most recent choice point. 
thisThis is called depth first search. 
inIn depth first search we backtrack to a conflict point 
sorry in depth first search we backtrack to the previous choice point but intelligent backtracking involves backtracking to the point where because of which the conflict has arisen. 

[32:07] 
so if supposeSuppose we have a search tree like this and we have a constraint violation here, in chronological backtracking what we would do is, if there is a violation here we will go back here and then go back here which has a choice point. 
soSo we will go back to this node which is the next choice point. butBut in chronological backtracking if we find that this violation is because of an assignment here we will backtrack directly to this place. 
thisThis is called back jumping 
(Refer slide time [32:58])

 

or conflict directed back jumping. 
(Refer slide time [33:07])

 

[33:07] 
so in consistent inIn chronological backtracking consistency check is performed in the order in which the variables were instantiated. 
ifIf consistency check fails we try the next value of the current variable. 
ifIf the current variable has no more values we backtrack to the most recent variable which that has a choice left. 
thisThis is what is done is normal depth first search.
backBack jumping is also a type of backtracking but it’ is more efficient when there is no consistent instantiation to be found for the current variable. 
(Refer slide time [33:51])

  

andAnd in this case we go to the deepest variable which was constraint checked against the current variable. 

[34:00] 
soSo when we find that the current variable is inconsistent and has no possible instantiation left instead of backtracking only to the previous choice point we find out what is the last variable which constrain the current variable. 
you see theThe current variable has no legal values left. 
inIn order that the current variable does get some legal value some constraint which participated with the current variable has to be changed. 
soSo we find thee deepest nt node in the path from the current variable to the root where a variable was assigned a value and that constrain the domain of the current variable, 
weWe have to find that and 
so that is what intelligent backtracking does. 
backBack jumping reverts to the deepest variable which was checked against the current variable in which introduce the constraints. 

[35:08] 
nowNow let u’s look at this example. 
soSo here we have here the six6 queens problem and we have put queen one 1 in row two2, 
queen two2 in row five5, 
 queen three3 in the row three3 and queen four4 in row six6. 
now afterAfter that we have put queen five5 in row four4. 
nowNow we see that, after we have placed these four4 queens none of the other the last queen cannot be placed in any of the rows. 

[36:06] 
so we do not really need so whenWhen we cannot put queen five5 we really do not nt need to look at t nanay further assignment to the value of queen five5. 
(Refer slide time [36:22])

   

so with this let us wrap up today’s lecture 
so in today’s lecture weWe have looked at some efficient constraint satisfaction problems. 
weWe have looked at we have revised forward checking 
tthen we have looked at constraint propagation for which we looked at arc consistency algorithms and then we looked at back jumping.
 aAnd what is what is the idea behind these algorithms? 

[37:17] 
inIn forward checking when we assign values to some of the variables we lookm forward and then we update the domains of the remaining variables. and And in forward checking we just checked whether the domain of any of the remaining variables become empty. 
(Refer slide time [37:41])

 

ifIf they do become empty then we terminate search at that point.   
inIn constraint propagation, in the general constraint satisfaction class of problems what we do is that, we not only propagate the constraints to restrict the domains of the unassigned variables but we also do a consistency check on those variables which have constraints between them. 

[38:28] 
thatThat is we check whether every variable whose domain gets affected, whether it has a legal value for a legal value of its neighbor with which it has some constraints. 
a cAC- three3 is a constraint propagation algorithm which does this recursively. 
wheneverWhenever it reduces a domain of a variable it takes up those variables and looks at its constraints with its neighbors. 

[39:05] 
ifIf there are constraints with its neighbors which reducesreduce the domains of the neighbors then again it checks those with their neighbors and so on. 
soSo a c AC-3three is actually quite a c three is quite an efficient algorithm and is able to detect constraints earlier. 
now here you must note that you see asAs you make your constraint checking algorithms very intelligent you must you are reducing your reducing the number of nodes in the constraint satisfaction tree that you are going to search but you are spending more time at every step. 
soSo you have to select a right trade off about whether these intelligent processing does give you benefit. 

[40:05] 
doesDoes this take up time? 
 whetherWhether the benefit that you get in terms of reduced search space if they warrant that sort of benefit? 
finallyFinally we looked at back jumping. 
whatWhat back jumping does is that, when in your search tree some constraint is discoveredor some conflict is discovered that is the domain of this variable has become empty. 
inIn back jumping we find what is the earliest variable which had been checked with this variable in which we introduced “[      ]“  
nowNow we must undo those effects in order to get out of this situation. 
soSo instead of going to the next the most recent choice point we jump to a conflict point. 
(Refer slide time [41:05])

  

[41:09] 
nowNow we look at the questions for this lecture: 
(Refer slide time [41:15])

 

theThe first question: 
applyApply the different constraint satisfaction algorithms that we discussed in today’s class on the five5 queens problem. 
inIn each case you should find out the number of nodes that are expanded expanded.
soSo first you will try ordinary depth first search. 
secondSecond you will try variable ordering heuristic along with ordinary depth first search. 
In the th third instance you try forward checking. 
The four4th instance you work out a cAC- three3 and finally you work with back jumping. 

[42:14] 
so we have come to an end of today’s lecture 
letLet us look back briefly at the questions that wewe have set for lecture nine. 
so ifIf you remember the first question asked you to do the class room scheduling, to look at the class room scheduling problem and formulate it as a constraint satisfaction problem. 
soSo let us briefly look at the class room scheduling problem. 
so weWe have a number of class rooms. 
supposeSuppose c l1, one c l2, two  c l three3 are the class rooms and we have some times slots 

[43:02] 

letLet t1 one, t two2, t three3,  t four4 be the time slots and we have some teachers. 
soSo let p1 one, p two2, p three3, p four4 etcetera be the teachers. 
nowNow this teacher for each teacher we have a set of classes which the teacher should be able to teach. 
soSo let us say p1 one should be able to teach c l1 one and c l two2, 
 p two2 should be able to teach let us say c l two2 and c l three3, 
 p three3 should be able to teach c l four4, 
 p four4 should be able to teach c l five5 and c l1. one 
 soSo we have a set of teachers and each teacher should be able to teach a set of classes. 
now we have to assign classes to class rooms and time slots 

[44:09] 
so weWe have also a set of sorry we have to assign courses  
so we have a set of courses. 
(Refer slide time [44:16])

 	

nowNow this is a constraint this is a pa problem which involves a large number of variables of different types. 
nowNow let us look at the basic types of constraints which are involved. 
Now, if a teacher has to teach both c l one and c l two which means that sorry not c l the courses namely course 1 one and course two2 then it means that course one 1 and course two2 must be in different time slots. 
ifIf a teacher is teaching c1 one and c five5 it means c1 one and c five5 must be in different time slots. 

[45:07] 
nowNow each class room at a slot can have only one class.
this is a thisThis is something that you have to follow. andAnd if a teacher is teaching twotwo classes those twotwo classes cannot be of the same time slot. 
nowNow what we have to do is, we have to assign classes to class rooms and time slots. 
soSo we can start with picking let us say class c1 one and assigning it to class room one 1 time slot t1. one. 
Nnow, if c two2 and c five5 cannot have the same time slot c two2 domain will not have t two will not have this time slot t1 one and c three’s domain will also not sorry c five5’s domain will also not have t1.
 one and then sSo when we consider the next instantiation, when we look at c two2 we have to look at the choices for c two2 in time slots other than t1. one 
(Refer slide time [46:34])

 

[46:27] 
whenWhen we look at c three3, if after c1 one we look at c three3, c three3 can be in time slot t1 one but in time slot t1 one it cannot be in a class room c l 1.one 
soSo any twotwo courses any two courses cannot share the same room and slot. 
soSo both rooms and slots cannot be the same for twotwo courses. aAnd twotwo courses that can be taught by the same professor cannot share a time slot. 

[47:31] 
soSo based on these constraints we have to do the assignment of values. 
so this youYou can work out these for a small set of values. 
(Refer slide time [47:38])

  

theThe second problem that we said was a crypt arithmetic problem. 
specificallySpecifically we are were asked to work out the values of the different letters for this crypt arithmetic problem. 

[48:05] 
so iI will discuss briefly how you go about the solution to this problem. and you can sometimesSometimes for these problems if we are working out manually you can use certain heuristics from your knowledge. 
so you see theThe value of m M in this case cannot be more than one 1. 
the value of m soSo even if s S and m hM have the highest values this can be utmost one 1. 
so thisThis is not zero0 because if this value is 0zero  we would not be putting this here 
 so this is very easy to see that mM is equal to= 1.one 
soSo what we can do is, we can simplify this problem by putting one 1 in this position. 

[49:11] 
nowNow we have some other variables left. 
whatWhat are the variables that we have? 
theThe variables are D, E, Y, N, R, O and Sd e y n r o and s 
s so we have seven variables in this problem. 
so letLet us say we first pick up d D.
nowNow what are the values that d D can ttake? 
d D can be one 1 or two2 or three3 or four4 or five5 or six6 or seven 7 or eight 8 or nine 9 or zero 0 
so we try the different values of D.d 
so thereThere are many possible values of d D
 so we try all possible values of D.d 

[50:00] 
nowNow let us say first we try Dd equal to= one 1. 
nowNow if dD equal to= one 1 we have to try the different values of e E. 	
now suppose weWe first try the value eE equal to= one 1. 
nowNow if eE equal to= one 1 then yY ahas to be equal to= two2 
ifif d D is one 1 and e E is one 1 and y Y is two2.
 
sSo y Y is fixed and if y is fixed we have to look at so here we have e E is one 1,
 this e E is one 1,
 these are the values that we have assigned and y Y has become two2. 
nowNow we have to choose n N and r R so that we have a1 one here. 
so supposeSuppose so we choose the different possible values of nN, 

[51:02] 
supposeSuppose you first choose nN equal to= one 1,
 if nN is equal to= 1one  then r R has to be zero0 to satisfy this constraint 
so we have this assignment.

 andAnd then e E is one 1 along this path and we do not know the value of o O and n N is one 1 
so e E is one 1, o O is not known, 
 nN is one 1 
so it must be equal to= zero0. 
soSo here we have o equal to= zero0 and we have this is one 1
 so s S must be equal to= nine 9.
so Therefore a solution to this problem is sS is equal to= nine 9, eE is equal to= one 1, nN is equal to= one 1 and dD is equal to= one 1, 
mM is 1one, oO is zero0, r R is zero0, e E is one 1 and m M is 1one, o O is zero0, n N is one 1, e E is one 1 and y Y is two2 and you can verify whether this is correct. 

[52:17] 

It is 1 + 1 = 2one plus one is two, 1 + 
one plus zero0 = 1is one, 
1 +one plus zero0 = 1is one , 9
nine plus + 1 = 10. one is ten 
 soSo this is a solution t 
too the crypt arithmetic problem send plus + more equal to= money. andAnd we have solved this problem by simple depth first search and in this case this is a simple problem with many solutions 
so so we would could get this problem by only exploring one path. 
(Refer slide time [52:40])

  

so we will stop here today 
in the class we will first briefly discuss the answers to today’s problem and we will start on the next topic which is on machine learning 
thank you very much 

Artificial Intelligence
Prof. Anupam Basu

Lecture 11
Knowledge Representation 
And Logic
(56:58)


In the earlier lectures you have learnt about different search techniques, game theory and other aspects of artificial intelligence. nowNow one of the most important aspects of any artificial intelligence activity isis how to represent knowledge that is knowledge representation. andAnd one of the major techniques of knowledge representation is logic. 
so So in this lecture we will be discussing about the different knowledge representation schemes. inIn fact this will be this discussion will be divided into number of lectures i would say and that will together constitute a particular module which you are naming as module four of this lecture of this lecture series. 
theThe first technique that we will be looking into is propositional logic. inIn the course of this lecture we will gradually come to know about what is propositional logic and how it can help us in representing knowledge. butBut even before coming to the aspect of knowledge representation let me first enumerate the objective of this lecture. Tthis lecture will enable you to represent simple facts (Refer Slide Time 02:33)  

that we use in our day to day life in the language of propositional logic. inIn order to express anything we need to take the help of some language and propositional logic in general logic is one such language. weWe will see how we can represent simple facts using propositional logic and we will see how we can interpret a propositional logic statement. 
suppose thatThat means, that when ever a propositional logic statement is made we have to understand what it means. andAnd there is a mechanism that helps us in understanding the meaning of a propositional logic statement. unlessUnless we understand what it means we will not be able to work with it. forFor example, if somebody tells you something in a language that you do not know you will not be able to understand it and unless you understand it you will not be able to act according to it. henceHence it’ is essential to interpret a propositional logic statement that is the second objective of the lecture. andAnd thirdly the we will be able to compute the meaning of a compound proposition. a compound proposition okay 
so aA compound proposition is essentially a collection, a collection particular way of collecting a number of single propositions and joining them together and thatwhich  forms a compound proposition. So, in order to understand meaning of a compound proposition first of all we have to understand the meaning of the individual propositions that is we will be we should be able to interpret a proposition and we should also learn as to how we can integrate the individual meanings of the individual propositions to understand the whole. 
(Refer Slide Time 05:22)  

now bBefore delving into the details of knowledge representation first let us look at what is meant by intelligence and how is knowledge related to intelligence. thereThere are different ways of interpreting knowledge philosophers, scientists, maloggicians have all interpreted psychologists have all interpreted knowledge have all interpretedetc have give some meaning to intelligence. 
Now, what is the role that knowledge plays in an intelligent behavior? letLet u’s try to first look at it. letLet u’s first try to address the question does knowledge have any role at all in demonstrating intelligent behavior? i know that Tthis is a debatable issue. forFor example, you may ask that, well this boy is very intelligent but he has never gone to school and he does n’ot have enough knowledge but still all of us accept that he is intelligent. 
butBut the debate remains that what is really is meant by the knowledge? isIs it the knowledge that we derived from formal education only? yesYes that’ is certainly a part. but besides that there are different other aspects that come from day to day experience as we evolve as any human being or animal evolves. theyThey acquire from nature some intelligence, some knowledge and that is somehow incorporated inside the being and that plays a major role in displaying intelligent behavior. sometimesSometimes that is too succinct too intuitive that we do not understand the fact at that knowledge exists that knowledge exists but still for that is evident from the behavior. commonCommon sense knowledge for example, even without any formal education people can act according to their common sense. instead of going to the details or debate on what is knowledge and how far we will say that okay this is knowledge, this is intuition etcetera (Refer Slide Time 07:17)  

let u’s first look at how this knowledge in what ever form it might be can help in displaying intelligent behavior in whatever form it might be. hereHere you can see that we are trying to sense something from the world, we are sensing. all right andAnd we have to sense the environment. asAs soon as we sense we accept that and work accordingly to it. weWe make some decisions and those decisions force us to take some actions so that is the action part. soSo this is the loop that is executed all the time in our day to day life. nowNow when this decision maker senses from the environment some event, may be the temperature has gone up pretty very high and we have to act accordingly. theThe decision maker takes the help of knowledge to decide on what to do given the environment that it has sensed, given the fact that it is derived that it has observed. Bbased on that it takes makes the decision and while making the decision the background knowledge that is either explicitly there or it is in the background okay either we can recognize that, okay yes this he is a knowledgeable man so he can do that or even for a common person that in -built knowledge that has come with him with evolution in what ever form might be that plays a role and that tells the decision maker what to do that may be in the biological system as such and now  based on that the action is taken. 
soSo we can see from here that knowledge certainly have a role in demonstrating intelligent behavior because if you we cut this part off then we will be able to sense but we will not be able to say what to do, we will not be able to act properly. theThe fact that we act properly tells us that we have got the  knowledge in some form which tells us how to act given a particular scenario in the environment. 
okay soSo that is the basic background of knowledge and intelligence. now thereforeTherefore if we just go up one slide here you see that this knowledge certainly plays a very important part. butBut the question is our decision maker when we are planning we when we are starting artificial Artificial intelligence Intelligence our objective is to build machine that will be able to behave like a human being or at least demonstrate some intelligent behavior. Ttherefore it is necessary 
for human beings we may or may not know exactly how the knowledge is has been incorporated in the brain. okay  butBut for in order to build a machine that will act intelligently it is mandatory that we must have a means of representing knowledge. howHow to represent knowledge? howHow to represent knowledge in a way that a machine can understand that is all that we are going to learn in this lecture and the subsequent lectures in this module. 
knowledgeKnowledge representation: Hhow can we represent knowledge in a machine? okay nowNow, in order to represent express anything that we want to talk about we need a language. 
So, just for expressing the knowledge we also need to have a language that will be able to represent the domain knowledge. i have underlined thisThe word domain is underlined because knowledge is not a small thing; it i’s very difficult to have a complete knowledge of everything. Eeven for a small problem solving activity, the amount of knowledge say for example geometry, the amount of knowledge besides the theorems the intuition that okay this theorem can be applied at this point of time that’s is also a part of knowledge. nowNow all these isthese are really vast. and henceHence we should try to first capture or first focus on a particular domain and try to represent the knowledge about that domain. we We can only expect a machine to demonstrate an intelligent behavior when that machine is left to work in a particular environment in a particular domain provided we empower the machine with the relevant knowledge of that domain. So in order to just as Llet’ us take the example of the theorems in the case of geometry, we need a theorem is just an expression and that takes help of the language English as well as the basic definitions, names, vocabulary of the domain geometry. all right  soSo we always need a languaglanguage.e algebra Algebra for example is a language of expression. weWe can very succinctly express many things in algebraic equation. 
(Refer Slide Time 13:38)  

Similarly, for representing knowledge we need to represent we need to have take the help of some language. now one thing is that supposeSuppose there is some language that has represented some knowledge, now what is the use of that knowledge? Uunless there is somebody who can take the help of this knowledge who can understand the language in which the knowledge has been expressed and therefore can use it the knowledge is of no significance. Hhence we must first have a language no doubt about it but also there must be a method that will be able to use this knowledge. okay so if weyou think of knowledge representation we must think about the things, one is the language another is a viable method that can be executed in a machine so that it can read the language, understand the language and act according to the knowledge that is expressed in that language. 
so this thisThis method that is used to interpret understand interpret knowledge in response to an environmental fact that has been sensed. Rremember the earlier slide that we have talked about thatwe discussed earlier, we sense from the environment, first you have to sense and then you have to use the knowledge to act accordingly. thatThat entire thing is clubbed in the name inference machine. soSo an inference machine or inference mechanism what it does is, it reads the environment, can interpret the knowledge that has been represented in a suitable language and use that knowledge to act according to the need. and thereforeTherefore it can generate proper actions which we often say see that the machine or object which is behaving intelligently. 
soSo in order to demonstrate intelligent behavior specifically two things are needed. oneOne is the language the knowledge represented in some language and the method to use the language in all we must have a powerful inference machine that can understand the language in which knowledge has been represented. so So a knowledge representation should include both; the language to represent knowledge and an inference mechanism that can use this knowledge. theseThese two together when defined gives us a knowledge representation scheme. now wWhen ever we talk of any language then we talk about the gramrammar, some rules that the language must follow because unless the language is gramrammatically well formed, it is gramrammatically correct it will be difficult for others to understand it. weWe can take the example of our normal day to day English, if we say a wrong English statement might bethen many  of the people will not be able to understand it, some may be able to apply their own knowledge and still understand it. 
(Refer Slide Time 17:15)  

butBut for a computer or for a machine the scenario is little different because here a machine can play with the things that it knows it cannot as yeit interprets in any particular way. soSo we must stick to the gramrammar the syntax of the language so that the inference machine which is nothing but a machine which is a programram running in a computer will be able to understand what it says. soSo the syntax is the gramrammar of any language and semantics of a language is athe meaning. 
okay forFor example, iI say pen cuot road okay cuot is a verbverb; penane is a noun and road is also a noun, but all these together doesdo not really convey anything meaningful to you okay although everything was there. i can say hereHere iI have made a syntactic mistake, i should have said even if it is syntactically correct I should have said that as the pen couts the road. nowNow does it really mean anything? canCan you really map this to a real world scenario? haveHave you ever seen a pen couttingt in the road? soSo the semantics is an issue, does it carry the meaning? 
sSo the two things are important; one is the syntax, one is the semantics. bothBoth of them must be well defined in order to form a knowledge representation language. forFor example, if you write laughs tom. nowNow laughs then parenthesis tom. whatWhat does it mean? doesDoes it mean tom laughs? doesDoes it mean tom is laughed at? whatWhat does it mean? there must be assumingAssuming that this is the correct syntax in some language what is the meaning of that, what is the semantics of this? weWe must understand this. forFor example, likes sunita Sunita aditi Aditi now what does it mean? sunita Sunita likes aditi Aditi or aditi Aditi likes sunitaSunita? thereThere must be a specific interpretation that should be given in order that this statement like sunita Sunita aditi Aditi can qualify as a value knowledge representation language. okay 
now whenWhen we are talking of language we must search for a language and logic is one such formal language. okay propositional logic letLet u’s start with propositional logic because propositional logic is which say let it be seen through for in a a relatively simpler form of logic. anythingAnything that we use in our day to day world is a proposition. say Now i make a statement anil Anil is intelligent. It i’s a statement, it’ is a proposition. anilAnil is hard working, it’ is another proposition. iI am making a statement about anilAnil, okay  anilAnil is hard working. soSo both of these are propositions. okay now soSo anil Anil is intelligent is a proposition, anil Anil is hardworking is a proposition. againAgain if iI make a statement like, if anil Anil is intelligent and anil Anil is hard working then anil Anil scores high marks. thisThis is also proposition because iI am making another statement. (Refer Slide Time 20:21)          

butBut you see that this statement is a combination of some other propositions. see anilAnil is intelligent is a proposition and this one says, if anil Anil is intelligent and anil Anil is hardworking this anil Anil is hardworking is itself a proposition here, all right  then anil Anil scores high marks, so these all these white ones are also individual propositions. butBut when iI am connecting all this together iI am making another proposition. we will say later that thisThis one is called compound proposition. okay 
so So briefly speaking when ever we speak in our day to day life, we always what ever statements we make consists of propositions. nowNow these propositions see anilAnil is intelligent, anil Anil is hardworking these are constituted of objects and relations objects and relations or functions. nowNow, the statement, anil Anil is intelligent i will make a statement this statement can be true this statement can beor false. anil Anil is hardworking can be true or can be false 
(Refer Slide Time 21:59)  

so Therefore any proposition in propositional logic can have either of two values true or false. okay now so andAnd it is constituted of objects and relations or functions. soSo anil Anil is an object, hardworking, intelligent these areare  some functions or relations depending on their usage. and  soSo, when we write intelligent on anilAnil so Ii am putting in some function on anil Anil and that means anil Anil is intelligent and this entire thing can be true if the anil Anil that iI am taking about if the anil that i am talking about is true, is intelligent if that anil Anil is intelligent then this statement is true. hardworkingHardworking anil Anil again similarly it means anil Anil is hardworking. 
so So again these are propositions. nowNow, propositions can be written either in this way or intelligent anil Anil I could have also written in that way i could have also written provided my language the syntax of the language are allowed. aA proposition can be true or false. as just now told 
nowNow let u’s look at it a little more formally. So, can iI write it in any particular way can i write it in any wayor the way  iI like? forFor example, anil intelligent anil  Anil  
(Refer Slide Time 23:04)  

or intelligent anil Anil within parenthesis or what should iI write? in order to know that weWe must come to the syntax in order to know that. okay aAnd as i have said the syntax basically talks about the gramrammar of the sentence. Nnow, let P stand for this proposition that iI have beenwas using intelligent anil Anil. okay letLet Q stand for hardworking Anil. nowNow what does P look at this symbol this is a logical n. and what does P AND Q mean? P AND Q means 
so P AND Q is a new proposition. nowNow P AND Q is true when P is true and Q is true and that is when both of them are true. ifIf either of them is false then P AND Q is not true. 
so butBut you see P AND Q is another proposition, although P is true Q is false, P AND Q both will be false. orOr if P is true when P is true and Q is true iI will have to separately evaluate P AND Q and see that P AND Q is true. okay similarly what does P Tthis symbol means P OR Q, what does P OR Q mean? thisThis is again another proposition P OR Q. this means that when either P is true or Q is true any one of them is true then this proposition P OR Q will be true. soSo P AND Q P OR Q are examples of compound propositions okay because P is a proposition statement, Q is a statement. iI have formed a new statement by applying some operators some connection operators like AND OR etc.etera 
we will see that thereThere are other operators also which you can use to make more compound statements. nowNow let’ us look at it in a more formal manner.some formally a little more formally 
(Refer Slide Time 25:00)  

the sSyntactic elements Elements of propositional Propositional Llogic: any Any languagee we will have some vocabulary. forFor example, the language English for that we have got the vowels, the consonants, some numbers etc.etera similarlySimilarly, for propositional logic also the vocabulary is a set of propositional symbols. Now  whether Aanil is intelligent or anil Anil is hardworking, gGeetha is beautiful, sSita is smart etc all these propositions we can symbolically represent each of these propositions as say for example as P, Q, R so etc.etera soSo let us symbolically represent P, Q, R which is a set of propositional symbols. now thereThere should be a set of propositional symbols and as we have seen that any proposition can be either true or false. 
so eachEach of theise P, Q, R can be true or false. besidesBesides there will be a set of logical operators logical operations. now here i like to highlight this logic logical whyWhy do you say it is logical operator? logical operatorIt is because this operator can have either true value or false value. i mean afterAfter iI apply a logical operator on two variables say P and Q okay so iI will get say P AND Q that itself will be will return a value true or false. 
so thatThat is the significance of logical operators. nowNow we can have say AND which is look at the symbol OR NOT and implies implies these are the basic four operators logical operators that are used in propositional logic. oftenOften parenthesis is also used for grouping. 
thereThere are two special symbols true and false. basicallyBasically these are logical constants. time and again i was saying that time and again we are saying that anyAny proposition can have the value true or false. and thereforeTherefore both these true or false these two are very important logical constants that you will be using all through. 
so basicallyBasically we have seen that there are four basic operators  
(Refer Slide Time 28:02)  

besides parenthesis AND OR NOT and implies. nowNow let’s see we will see the meaning of those and but before that let’s we will see how to form propositional sentences. weWe have got the vocabulary which is set of propositional symbols, we have got the logical operators and we have got the logical constants like true or false. right  soSo let u’s see how we can form logical sentences in thisusing these. each Each symbol whether it is each symbol what ever that isa proposition or a constant is a sentence okay it, it i’s always the sentence. ifIf P is a sentence and Q is a sentence then if we block them in parenthesis, parenthesis P is a symbol. okay sinceSince P is a sentence and Q is a sentence P AND Q P AND Q is a sentence. all right P AND Q is a sentence similarlySimilarly P OR Q is a sentence. 
ifIf P is a sentence NOT P is also a sentence. forFor example, if we say anilAnil is good then which is that is P all right then NOT P is NOT anilAnil is good. anilAnil is good was true and NOT of anilAnil is good will be false. butBut NOT of anilAnil is good is also a valid sentence. itIt may evaluate to true or may evaluate to false. P implies Q is also a sentence, nothing else is a sentence. 
now Tthis is a very important that, in order to qualify as a valid propositional sentence these each of these say P what ever the statements that have are formed, that where P is a sentence and Q is a sentence now iI can comply with these only if these all areallowed are ways. either Either iI block them bracket them in the parenthesis then it i’s a valid sentence, P AND Q will be a sentence, P OR Q will also be a sentence, NOT P is a sentence, P implies Q is a sentence and nothing else is a sentence. (Refer Slide Time 30:31)  

now this is oftenOften you will see that sentences are also called well formed formulae written in short wff. okay well formed formulae 
now this is so weWe will use the term sentence or well formed formulae interchangeably.ing  Here are some now let’s see now therefore some examples of wff’s well formed formulae: P S is a well formed formulae, true is a logical constant so it i’s an well formed formulae, 
 P AND Q it uses allowed operator and so it’ is a well formed formulae. Now let us look at P OR Q implies R let’s look at this so P OR Q is a valid proposition and any valid proposition P implies R that is a valid proposition so this whole thing implies R is a valid proposition so this is a valid sentence. P AND Q OR R implies S is a valid sentence. okay Nnow, NOT of P OR Q let’s see we can look at we can just observe here iI see that P is a proposition or a sentence, Q is a proposition or sentence, i write say for example sayif P is a valid sentence, Q is a valid sentence and therefore P OR Q is also valid sentence. and  sinceSince this is a valid sentence if iI apply the NOT operator over there that i’s also a valid sentence. okay similarlySimilarly here you see that NOT of P OR Q implies R AND S is also valid sentence. okay 
 (Refer Slide Time 32:22)  

now thereThere is a special meaning of this implication statement that we are talking of. whatWhat does implication mean? implication say wWhen we say X implies Y or P implies Q that means if P is true then Q is true. all right P implies Q when P is true then Q is true. but But not the other way round. itIt tells of a sufficientnecessary condition. ifIf P is true then Q will certainly be true. but it tells for a sufficient condition okay say forFor example, if it rains then the roads are wet. if it rains then the roads are wet soSo, it rains, that is my P and the roads are wet that is Q. so if it rains, that is if it rains is true then the roads are wet. butBut if you say if the roads are wet then it rains now is it an implication because if the roads are wet can iI always infer that it rains because the roads can be wet because of some other reason also. thereThere might be a sprinkler that has been used to clean the road or there has been the road has been washed it did not rain. so So roads are wet is not a sufficient condition to say that it rains 
soSo, in the case of implication when we write P implies Q that means if P is true then Q is true but Q can be true even if P is false. nowNow this is sufficient condition but not a necessary condition. equivalenceEquivalence is a bi junction that means for example it i’s a bidirectional bidirectional statement. P is true if P is true then Q is true and again if Q is true then also you can infer that P is true. 
now hear let’sNow here let us try with an example: Iif two sides of a triangle are equal then two base angles of the triangle are equal. nowNow we know from our knowledge of geometry two sides of a triangle are equal then that i’s an isosceles triangle and the base angles of an isosceles triangle are equal. 
soSo if iI had stated it in a different way that if two base angles of a triangle are equal then the two sides of the triangle are equal that will be true also. so iTherefore nin that case if iI say two sides of a triangle are equal that is P and two base angles of a triangle are equal that i’s Q then iI could have said P implies Q all right and Q implies P. 
so therefore (Refer Slide Time 36:01)  

since the eEquivalence can be expressed using two sentences P implies Q, Q implies P. Iin propositional logic equivalence is not a necessary operator. weWe have seen that along with AND OR NOT. weWe had the implies operator and with implies operator with two sentences giving it the both directions of implication we can capture equivalence. all right sSo we can have P implies Q and Q implies P. 
nextNext comes is a very important issue. so withWith this brief discussion on syntax let u’s come to the semantics the meaning of a propositional sentence. now whenWhen we try to understand a sentence then essentially what we do is we interpret that sentence. weWe try to understand each of the propositions and try to see whether that is true or false and then we make a decision on the whole. now soSo, if there be a sentence P we must first interpret what that does that P mean, anilAnil is intelligent okay what is the meaning of intelligent? whatWhat is anilAnil? allAll theise is are nothing but symbolic labels. thatThat is, we interpret in a particular world. we will discuss what this is wWhen we interpret a sentence in a world in a particular world the world can be the world of football match, world of cricket match, the world of picnic, okay the world of class room all these things are different worlds and each of these worlds have got some known relations some known functions and there are some interpretations that we give in a particular world. whatWhat is true in this world may not be true in another world. okay theThe rule of a baseball match is different from the rule of a cricket match. 


 







(Refer Slide Time 38:07)
  

soSo the cricket match along with all it’s rules all it’s definitions all these things form a world. similarlySimilarly a soccer match or a baseball match forms a different world. soSo when we interpret a sentence in a world we assign meaning to it and that evaluates to true or false. 
okay forFor example, let us start with a proposition P and suppose that proposition means a child can write that is a proposition. soSo the child this proposition P is a level for the child can write. andAnd suppose here we have got a world nursery and there is another world class two, okay now when i what is the truth value of this statement the child can write? whenWhen iI interpret it in this world nursery probably it evaluates as false because suppose the children in the nursery cannot write. butBut again when iI interpret it in the world of class two then itthat evaluates to true. all right you see theThe same proposition P can be interpreted in two different worlds in two different ways. again Again suppose this P is now interpreted as another statement the child can speak now the child can speak is an interpretation of P but again that one when iI interpret to the world nursery that should evaluate to true, again that thing when interpreted in the world of class two that is true. 
so you seeTherefore this statement P can have value true or false depending on it’s interpretation. whenWhen iI interpret P as the child can write okay and that is interpreted in the world nursery then it evaluates to false. butBut when this is interpreted in the world class two that then it evaluates to true. nowNow when P is interpreted to the child can speak that can evaluate to true when interpreted in this world. please note theThe statement, when interpreted in a particular world, that’s is very important to note. andAnd this interpretation itself attributes the meaning or the semantics to the proposition. weWe will see a couple of more examples. see again hereHere there is another 
example (Refer Slide Time 40:19)  

example P. Nnow P is ramRam is intelligent, P is leveled as ramRam is intelligent and Q is another statement which is say gitaGita is diligent. all right nowNow you can say that P there is some hidden interpretation here P ramRam is intelligent which ramRam iI am talking about? iI must be meaning some ramRam which is constant that is subject to the way i have interpreted it. 
now see whenWhen iI will again look at class two 2 and there is class three 3 and there is a ramRam in class three who is intelligent? thenThen this ramRam is intelligent evaluates to true and might be gitaGita is diligent, that is talking about new proposition, gitaGita is diligent is false. when i wWhen iI talk about the gitaGita who studies in class two all right2 b but ramRam is intelligent there is a ramRam also in class two 2 and that ramRam is not that intelligent so that evaluates to false. 
(Refer Slide Time 42:05) 
 
similarlySimilarly gitaGita is diligent and that evaluated to true what the for gitaGita that who was in class two 2 but if iI interpret for the gitaGita whoich  is in class three 3 she is also intelligent okay so both of them are intelligent. okay fine all right so
 you can see here soHere you can see here that it is very much dependent say we start with we will be dealing with these two symbols P and Q. Nnow whether P is true or P is false Q is true or Q is false that is totally dependent on the way we interpret it in a particular world. These are the  two examples of worlds wethat you  are talking of. okay soSo how do you get the meaning of it? Remember that now please remember that ssentences can be compound propositions. weWe have seen that P is a sentencesentence; Q is a sentence now we can connect them in different ways and thereby we can get different compound statements. right 
soSo sentences can be compound propositions. forFor that, in order to understand the meaning of the sentence we have to interpret each individual atomic proposition in the same world and that is important. And we must interpret it in the same world unless we interpret it in the same world the meaning it in the same world it will not be really meaningful. okay now soSo the first step is, we interpret each atomic proposition in the same world and assign truth values to each interpretation to each of the interpretations each of the atomic interpretations. weWe take atomic propositions we assign truth values to the interpretation of each of the atomic propositions and compute the truth value of the compound proposition.  
(Refer Slide Time 44:01)
  
    
okay sorry i moved a little fast soTherefore there are three steps: we interpret each atomic proposition, assign truth values to each interpretation and then compute the truth value of the compound proposition. all right now eExamples: so  P is a proposition that is actually meaning likes sumitSumit sunilSunil. that means sumit Mmy interpretation is that sumitSumit likes sunilSunil and Q is knows jyotiJyoti sudhirSudhir that means jyotiJyoti knows sudhirSudhir. sumitSumit likes sunilSunil okay suppose that i’s my syntactic aspect now the world that we are talking of is that sumitSumit and sunilSunil are friends and jyotiJyoti and sudhirSudhir are known to each other. thenThen what we will happen? sumitSumit and sunilSunil are friends so obviously friends like each other so this one will evaluate to true and jyotiJyoti and sudhirSudhir are also known to each other so this is also true in that world. 


butBut might be gitaGita and sudhirSudhir do not know each other so the proposition knows gitaGita sudhirSudhir will evaluate to false. butBut knows jyotiJyoti sudhirSudhir evaluates to true. 
so hereHere we see that P evaluates to true and Q also evaluates to true. thereforeTherefore P AND Q in this case P AND Q is true P AND Q is true because both of them are correct. butBut what happens to P AND NOT Q? Hhere you see Q was true and here NOT of Q that means let’s see what does NOT Q mean here  knows jyotiJyoti sudhirSudhir. iI negate that which means does not know jyotiJyoti sudhirSudhir. butBut jyotiJyoti and sudhirSudhir know each other so knows jyotiJyoti sudhirSudhir is true and the statement does not know jyotiJyoti sudhirSudhir is false. you see that Here the world that we are talking about that plays a very important role because (Refer Slide Time 46:24)  

the validity the truth value of this proposition would have varied depending on in the world in which you are interpreting. Here you find you can see the P AND Q is true but now NOT Q means that this part which was true in this world NOT Q is false in that world in the same world. soSo now this is false P is true so P AND NOT Q is false. 
now let’s talk about now weWe have seen the world part. nowNow there is a very important concept validity of a sentence. oftenOften you say this is valid this is invalid but in the case of logic this validity is a very important notion and we must try to understand that. aA propositional sentence is said to be valid if under all possible interpretations it is true. okay ifIf a proposition sentence is true under all possible interpretations then it is called valid. So, irrespective of irrespective of the world in which we interpret it right if the propositional sentence always evaluatesentence always evaluates to true then it is a valid proposition. will see later thereThere are other definitions like satisfiability which we will look into later. so Therefore when ever a proposition is evaluated to true in a particular world then that is satisfied by that world but it is not a valid statement. whenWhen we say a particular statement is valid then it is it will be true irrespective of the world in which iI interpret. 
okay Nnow a typical example is, P OR NOT P. P OR NOT P Nnow P can be true, if P is true then NOT P will be false. butBut we know that in OR connectiveor P OR NOT Q in that case if any one of them is true it will be true. 
so nowNow what will happen if P is false? ifIf P is false then NOT P will be true so any one of them will be true any one of them will ultimately be true always irrespective of what ever P means. ifIf P means gitaGita is intelligent, sita Sita is diligent etcokay what ever interpretation you give it does not really matter P OR NOT P will always be true for any particular interpretation you give in any particular world. If P beis some statement from the world of cricket and NOT P is also computed in that particular domain then of course P orOR NOT P either of them will always be true and hence this proposition will always be true. 
so tautologyTherefore this is known as tautology and tautology is a classical example of a valid sentence. (Refer Slide Time 49:45)  

soSo once again iI repeat, what is a valid sentence? a A sentence that is true irrespective of interpretation. now beforeBefore going to the quiz let me once again summarize what ever we have looked at. weWe said at the very beginning that, in order to represent knowledge let me just in order to represent knowledge we have to take recourse to a particular language. that’s number one andAnd any language if it has to be very easily and mechanically understood, if it has to be understood by machine then it must adhere to a strict syntax aandnd that syntax will be interpreted by the machine which will understand it’s meaning okay now wemeaning. We also said that, knowledge is important in order to exhibit intelligent behavior because when ever we sense something from the world when ever we sense something from the world we have to act according to it. nowNow, in order to act according to it we will have to sense it but just merely sensing will not do but we will also have to act according to it and how do you know how to act? forFor that we will have to have some knowledge that will be accumulated and kept. nowNow we may keep the knowledge in some way. butBut how do you know it will be not it will not at all be useful unless we can use that knowledge. inIn order to use that knowledge we need aan inference mechanism which must understand the knowledge and in order that an inference mechanism can understand it and can interpret it then it must have a strict syntax language structure. andAnd we started with logic and specifically propositional logic which is a part of logic. all right 
now thisThis propositional logic has got a set of vocabulary. each of those vocabularies can be the v A vocabulary consists of some atomic statements like P, Q, Rand R each of which we have given examples of. right and we can form sentence eEach of them are sentences and there are some rules using which we can form compound sentences. we We can use AND OR NOT implication and using this we will make compound sentences. and And the semantics of a particular sentence may be compound or simple depends on it’s interpretation in a particular world. okay weWe have shown how to interpret it. nowNow after interpretation we will find out the truth value of thatthat. okay nNow we will see in the next lecture we will see how the actual inferencing is done in case of propositional logic. inIn this lecture we have just introduced you so that you can take it up from here. inIn the meanwhile it will be good to have a couple of these exercises done. so here you see  
(Refer Slide Time 53:40)  

Quiz: express Express the following English statements in the language of propositional logic. Iit rains in July. okay theThe book is not costly. ifIf it rains today and one does not carry umbrella he will be drenched. now someSome of these will be very easy and some of them will be little difficult. weWe will look at in the next lecture how to solve them. nowNow you can see, here this one is a compound proposition. itIt will require some compound proposition and these are simple propositions. butBut here again you will need the some logical operators. you just find out what it is andAnd the second part is, if P is true and Q is true then are the following true or false? please note down P implies Q, so if P is true and Q is true then iI am asking whether thise entire proposition is true or falsefalse. NOT P OR Q implies Q, NOT P OR Q implies Q, NOT P, the third one is NOT P OR Q implies P, P OR NOT P implies T where T is a logical constant true. so please note down we will discuss about this in the next lecture let’s i do not know whether i am able to take up take it down so in the first one Iin the first one what we are showing here is, we want you to first translate these in the language of propositional logic. all right so justJust follow the syntax and transform them in the language of propositional logic. theThe second one, the book is not costly, if it rains today and one does not carry umbrella he will be drenched etc. so and the second one  
 





(Refer Slide Time 56:05) 
          

In the second assignment here there is a little bit of semantics. When you look at it  that you have to look at you have to know whether they are evaluating true or false. You have to twe have not i know that we have not formally discussed how to evaluate this but i am just leaving it as an exercise you try you try whether you canto compute the truth value of for these. in the next class next lecture we will see how it can be formally done so much for today and 


inIn the next lecture we will be talking about the inferencing and how we will automatically find the truth value of the propositional sentences.    
      
Artificial Intelligence
Prof. Anupam Basu

Lecture 12
Interface in Propositional 
Logic 
(52:45)


inIn the last lecture we had introduced the basic idea of propositional logic and we talked about representation of different sentences in propositional logic. 
inIn this lecture we will look atin a formal way as to formally how do we make inferences when some statements are represented in the form of propositional logic. asAs it has been proved in the last lecture that the objective of a knowledge representation scheme is not only representing the facts or some rules but also we have to be able to make the inferences. unlessUnless we can make inferences, that knowledge that we have in what ever form we represent the knowledge that we have that will be of no use. soSo we will be talking about inferencing in propositional logic representation today. butBut before we proceed with that let u’s have a quick look at some of the answers to the quiz’s Quizzes that were given in the last lecture. 
now insteadInstead of going into all the problems that were given iI will rather give you some examples. likeLike the first problem was it rains in July. nowNow how do we represent this in propositional logic? itIt’ is really a simple casecase; we can say that rains July. thatThat is sufficient to say that it rains in July. Similarly, if the statement was that it does not rain in November then in that case we could have written it as not rains November etc.etera 
(Refer Slide Time 03:06)  

okay so thisThis is rather simple but the next example that we have this was also one of the problems given. ifIf it rains today and one does not carry umbrella he will be drenched. 
now this one howHow do you we represent this? um as you can see thereThere is some implication here if and then. okay Iif it rains today and one does not carry umbrella then he will be drenched, then is implicit here. 
soSo, we can say rains today for this part of the clause we can say rains today and does not carry and not carry umbrella. ifIf these two things happen then get drenched. okay  thisThis is one way. but here i have done some mischief i will say um here you see thatHere there is a word one and one iI have some how escaped usingthat the statement one does not carry umbrella, iI have simplified that because in order to capture this sort of thing one does not carry umbrella, no one carries umbrella, somebody does not carry umbrella etc this sort of constructs rrequire some more power than what is provided in the case of propositional logic. 
so let u’s neglect or ignore this one part here for the time being. So, if we neglect that we will get rains today and carry umbrella and does not carry umbrella implies that somebody gets drenched. nowNow if we make it if we deal do away with the problem of one here then let u’s take another example rains today and tomTom does not carry umbrella that implies that tomTom will get drenched. soSo, rains today, AND NOT carry umbrella tomTom, get drenched tomTom.. in this way you can try the rest of the examples i think two more are there which you can try and get them across i think the only tricky thing was here 
(Refer Slide Time 04:27)  

so nowNow let’ us come back to today’s lecture. inIn today’s lecture our objective is to put you to the position where you will be able to infer the truth values of a proposition. youYou know that any proposition can be true or false. theThe truth value of the proposition will be evaluated. howHow to evaluate that? andAnd reason you will be able to do reasoning towards arriving at new facts when you are given a new set of propositions. alsoAlso this will help you this lecture will help you to prove even a set of propositions. This means given a set of propositional facts that means when a set of propositions are given and another third another proposition is given then you have to prove this later proposition using the existing propositions which are known to be true. 
In today’s lecture we will also see the now what is the mechanism of doing that. we will see that too in the lecture today 
so coming to the next one i have iI had given two (Refer Slide Time 05:20)          

parts of the quiz,’s one iI have answered but the other one iI have not. butBut iI assume that you have been able to do that some way or the other. butBut since we did not introduce any formal method for doing it iI will try to give the answers of those a little later. butBut before that let u’s look at the procedure that is formally adopted to arrive at the truth value of compound propositions. so supposeSuppose we have got a proposition P and proposition Q the objective is to arrive at the truth value of the composite propositions like P AND Q P OR Q etc.etera nowNow the standard method for doing that is using the truth table. 
nowNow here you see that we are deriving a truth table for the AND logical operation. soSo P AND Q each of them can have each of them P OR Q can have can have a value true or false.  right now soSo we can have four combinations like both of them are false, one is false one is true, one is true one is false and both of them are true. soSo we make a truth table like this. there is a slight small thing to be noted here 
we usually try to follow a system while you enumerate them false false then we make one of them true that is false true then we make the other one true the other one false true false and then true true. This is something to be noted here. so thisThis is a typical order in which we usually write.  so let us seeNow, when we talk of P AND Q we know that AND is true if both sorry if if both of them are true if both of the propositions are true. so Therefore when this is false false P AND Q will be false, if P is false and Q is true then P AND Q will be false because it requires both of them to be true, true and false will become false and only whenif both of them are true it will become true. 
similarlySimilarly if you go about over here now we can see follow the mouse pointerit is again the same combination is there but OR operation P OR Q will be true when any one of them either P or Q any one of them is true. 



so you see false false (Refer Slide Time 08:22)  

Therefore if it is false false it will be false but in false true case when since one of them is true the result will be true, true OR false one of them is true so it will be true, if both of them are true then also it will be true. thatThat is how we can find out the truth value of theise composite symbol propositions P OR Q. 
let’s haveA a few more examples: Hhere we are looking at the implication logical operation. nowNow what does implication mean? to as told in the last lecture ifIf P is true then Q will certainly be true. soSo P is a sufficient condition for Q being true. butBut if P is false then also Q can be true. all right andAnd if P being false still Q is true then also P implies Q this statement is not valid. all right (Refer Slide Time 10:24)  
so So the truth table will be, both of them are false then obviously this is true because it is telling the truth that if it is false it is false remainsmay be  false that is true this entire thing is true. ifIf this is false if P is false here and still Q is true yes that can happen because P is a sufficient condition P is truth ite is athe sufficient condition but not a necessary condition for Qs is being true.  so you see ifIf it is false and Q is true P implies Q is statement is this statement true yesthen this statement is still true. now Iif P is true and Q is false now we have a problem because P is a sufficient condition. Therefore so if P is true Q has to be true if this is true. butBut P is true but still Q is false then obviously P implies Q this statement does not hold. thereforeTherefore P implies Q becomes false. butBut if both of them are true then also is true. 
so So, when we try to derive the formula for one such composite statement in proposition calcululuss  we can use this method called […..].10:29
now hHere we have taken the example of a little more complicated thing that we want to find out the truth value of the statement NOT P OR Q implies P AND Q. Is this statement true or when now when wwill it be true?this statement true is it true let’s see 
now again we take toAgain we have to start with the standard method of enumerating the truth value of P AND Q. Nnow again we will have theise four combinations false false, true true, true false, true true. nowNow, again since we have got NOT P here so if P is false NOT P is true we make a column for NOT P. P is false NOT P is truetrue; P is true NOT P is falsefalse; P is true NOT P is false. andAnd now we make another column for this entire NOT P OR Q and that column is here P OR Q. and soSo NOT P OR Q how do you compute? itIt’ is an OR so NOT P is true Q is false NOT P OR Q is true, NOT P is true look now i am looking atwe have to  just look at these two columns because i we need only these two. bothBoth of them are false so this is false, this is false this one is true so this will be true so iI have got the truth value of this.  
(Refer Slide Time 11:48)  

andAnd on this side we have got P AND Q. all right Nnow we have got P here Q here so P true P false Q false makes it false, P false Q true makes it false and since it is only when both of them   
is AND only when both of them are true it becomes true. 

nowNow iI have got these two. nowNow iI want to see whether this statement which is an implication within between this implies this, this implies this whether that is true? now whatWhat is the truth scenario for that?  ifIf the antecedent is true if this part is true then this has to be true because this is a sufficient condition. nowNow we see here this is true but this is false so this combination makes theis entire statement false. 
similarly thereThere is a mistake here, this should be false, this should be false and this should be true there is a mistake here. i am sorry thisThis should be false and this should be true then this should be true. okay this soSo, if this is false and this is true then this should be true. ifIf both of them are false then also this is true, if both of them are true then this is true. 
okay so nowNow we come to another very important theorem that is called the De Morgan’s theorem. De Morgan’s theorem states that, (Refer Slide Time 13:16)  

if iI make a negation, say for example P AND Q is negated P AND Q this entire thing is negated then each of these literals each of these propositions like P AND Q will be individually negated and this operation would be complimented, this dual one will be taken so the dual of AND is OR and the dual of OR is AND. so Therefore NOT of whole thing P AND Q will be NOT P and this one with thewill be complementary and will become OR NOT Q. 
similarly Similarly NOT of P OR Q will be NOT P and this will be complimented AND NOT Q. now Now this is De Morgan’s theorem so if we just write down that to prove the truth table then we can see that P AND Q will have these combinations againwithin so P OR Q will have this value NOT of P OR Q will be compliment right true, false or true becomes false true becomes false. nNow we look at the right hand side of this, this side NOT P here P was false and NOT P is true, Q wais false so NOT Q is true, false so we get these things NOT P AND NOT Q these things and so NOT P AND NOT Q true true will become true so iI am trying to compute this part, iI am trying to prove De Morgan’s theorem. soSo iI have found the column for this value and n 
nowow iI am trying for this value. soSo, as soon as this becomes true false false false iI can compare this column and this column and Ii can see they are identical. right thereforeTherefore these two are equal that provesproving the De Morgan’s theorem. 
theThe next thing that we can now do is retrieve the earlier quiz. whatWhat was given here is, if P is true and Q is true then areis the following are true or false? firstFirst thing that was given P implies Q is it true or false if P is true and Q is true. nowNow we have got a formal method in order to expressat our disposal so let u’s make a truth table for that. we We see that P and Q and ifand P implies Q if we make this picture we will find if P is true and Q is true P implies Q is true. right nowNow let u’s take the second example. the second  anotherAnother example was therefore i am just showing the results of the two (Refer Slide Time 15:40)   

is NOT P OR Q implies P, how do weto deal with that ? againAgain (Refer Slide Time 15:57)  
 if P AND Q is true is it true let’s seelet us see if P AND Q are true. so Hhere iI have used another notation NOT P can be written in this or P prime can also be used so here you seeto denote NOT P. Here you see P is true because it has been told in the problem if P is true then P prime is false okay negation of P is false and Q is true then what happens is S is this entire statement, is it true? let’s see what happens NOT P is false so this is false Q is true and therefore this part is true. And all right this part is true implies that P and P is also true. 
(Refer Slide Time 17:40)  

soSo this is true implies that this is true so this entire statement is true. okay so you can thereforeTherefore you can use the truth table method to derive the validity of correctness of any propositional statement. nowNow let us come to reasoning. howHow do we carry out reasoning in the case of propositional logiclogic? soSo, what do we mean by reasoning that isyou are given some propositions you are given using the given propositions which are assumed to be true you are trying to derive new facts which will also be true. okay so supposeSuppose there is a proposition it is the month of July okay and there is another proposition it rains okay now and there is a rule that is told that P implies Q that means if it is the month of July then it rains is the statement. nowNow another statement is given, it is the month of July that means P is true then what can you infer? usingUsing your common sense you can say that you can conclude that it rains. right nowNow how do we dowe did  it? whatWhat was the mechanism that went behind this? weWe can write this in this form: say  P implies Q this is a true proposition, if it is the month of July then it rains. iI was also told it is the month of July that’s all and so if P implies Q AND P then we can infer Q. 
soSo P implies Q is given AND P is given so we can infer Q. tThis is a very common deduction technique that we often use in our day to day life and it has got one name and that name is Mmodus ponens. okay  thisThis is one typical inference rule which is very popularly used for carrying out inferences. all right nowNow was it true? you see P implies Q can be isIs modusModus ponens a valid? isIs it a correct inference rule? Let u’s look at that, P implies Q can certainly be written as NOT P OR Q that you have seen in the last class, NOT P OR Q they result in the same truth table inference rule so we have got NOT P OR Q and P is true. and s So, if we conjoin it with P then we can write P AND NOT P OR Q and therefore P AND NOT P by associativity OR Q so P AND NOT P is false OR Q anything false OR Q will give yoube [….] Q so that is the proof of the modusModus ponens. thatThat means if P is given and NOT P AND P plusP implies  Q is given then you can certainly infer Q and that i’s a very common and usual inference rule. andAnd when we talk of a valid inference rule it is irrespective of the interpretation. (Refer Slide Time 19:41)      

weWe are talking of inference rules which can be mechanically applied and these are irrespective of the meaning which that i’s the interpretation that (Refer Slide Time 20:16)  
we give. hear sSo, irrespective of whatever the interpretation is modusModus ponens allows us irrespective of what ever the interpretation is to infer the truth of Q. so So modusModus ponens is an inference rule that allows us to deduce the truth of a consequent depending on the truth of the antecedents, it is which is a just a rule. okay 
now whyWhy are we so much bothered about inference rules? we We are so much bothered about inference rules because we want to develop some mechanical procedures using which we can make the machine infer for new facts.  
so we must onlyOnly for those inference rules are of interest was to us which can be mechanically applied. Bbesides modusModus ponens there are couple of other inference rules like which you can see through your common sense like if P AND Q. Iif P is true and Q is true then obviously P is true. okay ifIf P is true then P or Q is true. all right Iif NOT of NOT of P is true then P is true it is a double negation and by De Morgan’s law or any other law what ever it’s a double negation of so these two cancel out each other and then we will get P. then there is a veryanother interesting and useful rule that’s called the chain rule. Ffor example, if we have one rule like if P is true then Q is true and iI have got another rule if Q is true then R is true then we can certainly infer that, if P is true then R is true because Ps is being true makes Q true and Qs being true makes R true so so there is a chain. rule P is true makes Q true and Q true makes R true. so thereforeTherefore by transitivity we can say if P is true then R is true. thisThis is called the a chain rule in which we will find a lot of applications when you we look at rule based systems.this 
(Refer Slide Time 21:55)  

so now we come to a very besidesBesides these inference rules contemporary importantnow we come to a very important concept called satisfiability. satisfiability whatWhat does satisfiabilityit meanmean? 
Let u’s remember that when ever we try to prove any proposition we are proving it with respect to a particular interpretation. and And that interpretation is nothing but mapping to a world. and And a sentence is satisfiable by an interpretation if that interpretation makes the sentence true. thatThat means, if under that interpretation that sentence evaluates to true only then that interpretation satisfies the sentence. okay nowNow, if no interpretation   

(Refer Slide Time 22:48)  

makes a sentence true then only then ththat sentence is called at interpretation satisfies the sentence okay now if no interpretation makes a sentence true there is no interpretation that makes the sentence true then that sentence is called unsatisfiable or inconsistent. unsatisfiable ifIf there are no interpretations that makes all the sentences true that you have in a set then that set of sentences is inconsistent okay or unsatisfiable. so thisThis is a very important notion. satisfiability so brieflyBriefly speaking satisfiability what does it mean it means that, if given an interpretation a sentence evaluates to true then that interpretation satisfies the sentence. theThe sentence is satisfied under that interpretation. 
now thisThis leads to us leads to a very important concept called logical entailment. we often in our day to day useIn our day to day way of speaking we often say this is a logical consequencecept of that, this logically follows from that. now whatWhat does the term logically follows really meanit mean really it logically follows? whatWhat does it mean by the term following logically? thatThat is essentially captureding in the idea of entailment. ifIf something logically entails something else then we can say it’ is a logical consequence. butBut what iI say saidit now is equally vague so let’ us try to make it concrete. suppose Suppose there is a set of sentences S suppose and let’ us suppose there is another sentence small sentence here shown in small green bubble that’ another sentence and we are thinking of one particular interpretation I one particular interpretation that has been given to theise set of sentences and this interpretation obviously makes these set of sentences true. thatThat means this interpretation I satisfies theise set of sentences. okay satisfies this set of sentences Snow suppose we apply the same interpretation to this new candidate sentence and if the same interpretation also makes this sentence true that means the interpretation that made these set of sentences true also makes this candidate sentence true then we can say that these set of sentences logically entail this sentence.  (Refer Slide Time 25:24)   

okay theseThese set of sentences went throughrue will also make this one true. okay so thatThat is called logical entailment. now  ifIf put in words, there is a set of sentences and if a sentence s oneS1 that was a the small bubble here that was small bubble here this is s one and there is a set of sentences okay capital S 
if a sentence s one there is a small bubble value has a value true for all interpretations that make all sentences in a set of sentences S true then we can say that this set of sentences S logically entails s oneS1. thatThat means S the set of sentences logically entails this new sentence or in other words s oneS1 logically follows from S or s oneS1 is a logical consequence of S etc.
etera 
so nowNow we no longer remain to be plaguevague about when we say that this thing logically follows from this thing. thatThat means a particular interpretation makes a set of sentences s tworue will also make the new candidate sentences small s or S1s one  to be true. all right nowNow this is a very important consequence of our understanding of logical inference because when we try to infer we are actually trying to derive new sentences which are correct which are valid. now just looking at again inNow by looking from  another direction or from thethe angle of inference suppose we have got s a set of sentences here and iI have got a new proposition or a new sentence and iI have got a decision machine, the decision machine takes this set of sentences which are known to be true. supposeSuppose iI have told some geometric theorems to the machine, now theise geometric theorems are already proved that and they are known to be true. 
so andAnd iI have got a new theorem that iI want to prove or provide a new problem that iI want to prove. soSo iI have got a set of sentences which are known to be true. all right Nnow, given that the decision machine takes theis new proposition and asks this question is it true and the decision making machine will come up with the answer yes or no. tTherefore it will be yes the answer will be yes if this new proposition is logically entailed by this set of sentences which are known to be true under a particular interpretation. thatThat’ is what our proof mechanism is and nothing other than that. (Refer Slide Time 28:12)  

soSo if we have got a set of sentences which are known to be true and if a new proposition logically follows from that then in the decision making machine will be able to say that yes this is true. thatThat is how our basic inference mechanism goes on. 
nowNow we have seen modusModus ponens to be a very interesting and powerful way of inference that’ is an inference rule. nowNow we will look at another interesting inference rule inference mechanism that is known as resolution. butBut before going to the resolution quickly we need to have a look at a special form of representation of propositions and that is called the clause. (Refer Slide Time 29:00)  

aA clause is a special form. now before that beforeBefore defining a clause we must know what is a literala literal is. aA single proposition which we often call it an atom,tom  a single proposition is always called an atomtom but a literal can be a single proposition or its negation. forFor example, P or NOT P, P is a literal NOT P is a literal. okay nowNow what is a clause? aA clause is a disjunction is a disjunction of all theise literals. nowNow what does disjunction mean? disjunctionDisjunction means connection through OR not AND. when When we connect it with AND we call it conjunction. butBut disjunction means that all these literals are connected with R. so So you say P OR Q OR NOT R this is a valid clause. nowNow this is a particular form that is very useful for inferencing. now givenGiven any particular proposition can we convert it to the clausal form? the form theThe representation form in the form of clause is called a clausal form. 
so howHow can we convert a compound proposition to a clausal form? thereThere are some steps that we can follow and do that. considerConsider the sentence or a well formed formulae A implies B OR C implies A. Tthis is not in clausal form because we have got literals no doubt but we have got we have got disjunction but we have got operators which are not disjunction. inIn a clause there will be only disjunction operators. thisThis is a particular form of clause that iI am talking of that’s called disjunctive Disjunctive normal Normal fForm DNFd n f okay that i’s of interest to us right now. okay so i howHow can iI convert this to a disjunctive normal form? Ffirst of all i will have to iI can eliminate theise implication signs by the simple rule that we know that if P implies Q if P implies Q if we write P P implies Q then that thing can be written as NOT P OR Q that we have already discussed that it can be written as NOT P OR Q. 
so inIn the same way we can we can convert it in the clausal form. so you see anyAny implication can be converted to this. so it was sorry not very visible NOT of myMy clause was NOT of A implies B OR C implies A so first iI am trying to eliminate the implication sign. so thereThere was a NOT here find it it will beand let it be there. S theroe A implies B will become NOT A OR B and C implies A will become NOT C OR A in which is the first step. nowNow we have got this, still it is not a clausal form now how can we make it a clausal form? 
(Refer Slide Time 32:15)
          

we We will have to eliminate the double negation sign and reduce the scope of the NOT signs using De Morgan’s law. so i had NOT of NOT A OR B so i will remove this indication double negation so NOT if i apply De Morgan’s law NOT applied here will be negate this so this will become A and this will become NOT B but this operator will become AND which is the dual of that and similarly for this it will be NOT C OR A. but But unfortunately as we have done this iI have got an AND operator which is not allowed here in a clausal form so iI have to do something more. the third step is soSo what we got here is you see it i’s not a disjunctive normal form there are some AND’s also here. Therefore  as we go down we will have to convert it to conjunct this conjunctive normal form to disjunctive. iI will first convert that to conjunctive normal form using theise rules that I have got. so i have got these rules soTherefore by using the distributive and associative rules A AND B OR NOT C OR A they can be converted to A OR NOT C OR A AND NOT B OR NOT C OR A. Tthis can be further deduced be reduced to A OR NOT C this is one, AND NOT B OR NOT C OR A. nNow each of them you see is a disjunction but they are connected with an AND operation. so this is also called as sinceSince they are connected with AND this is called conjunctive normal form. butBut our objective was to derive clauses and in the clauses there are were only disjunctions allowed so what do you we do? weWe have got here individual clauses which are individual clauses they are satisfying the definition of clause but they are connected with an AND. tThe answer is very simplesimple. eachEach of these clauses can be looked upon a separate clausesupon separate clauses which are true. soSo we have to get the set of clauses A OR NOT C is one clause NOT B OR NOT C OR A is another clause. so  therebyThereby we derive the conjunctive normal form but the conjunctive normal form consists of a conjunction of individual parts which are themselves in disjunctive normal form, so theyre are clauses 
(Refer Slide Time 34:54)  

so we have got two clauses over here. Therefore so in this way we can convert any particular proposition into the clausal form. nowNow why do we get look at all these? whyWhy are we so much interested into getting the clausal formform? It is  because the clausal form will allow us to apply a very interesting inference mechanism that is called resolution. a resolutionResolution is another technique of inference that we learn. 
atAt the very beginning in the objective we have said that wesaid we learnted to prove new facts given a set of facts. proving Given a set of facts proving a fact given the set of factsmeans logical entailment proving the logical entailment. which weWe have also defined this. modusModus ponens was one way but resolution is another way. resolutionResolution is a sound inferencinge mechanism. all right now  supposeSuppose x is ala literal and S1 one and S2 two are two sets of propositional sentences represented in represented in clausal form S one and S two are two sets of propositional sentences that are represented in clausal form then if we have x OR S1 one AND this AND is a logical AND NOT xand  this AND is a logical AND NOT x OR S2 two then we can get S1 one OR S2 two. thisThis is the basic principle. that  you can easily prove that x OR S1 one and NOT x OR S2 two if we apply the associativity rule over here then again we will have false OR S1 one OR S 2two okay so then we get S1 one OR S2 two so that therefore we can eliminate a literal with it’s compliment and therefore we can here S1 one OR S2 two is the result of the resolution. (Refer Slide Time 36:54)       

so this is thisThis process is called the resolution. so So we take two clauses and resolve one literal with it’s negation and the remaining part when combined gives us the resolvent.ultant S1 one OR S2 two is the resolvent and x is a literal element  that will be is resolved upon and the entire process is resolutioresolution.n part of the sentence 
letLet u’s take an example. ifIf a triangle is equilateral then it is isosceles. okay if a triangle is equilateral then it is isosceles that is told itIt i’s a rule in a geometric theorem which somebody have already proved by someone. andAnd again if a triangle is isosceles then two sides are two sides A B and A C are equal and that i’s the definition. Iif A B and A C are equal then the base angles angles B and C are equal. something you know okay A B C is an equilateral triangle and suppose you are asked to prove angle B is equal to angle C can you prove it? okay or weWe can try to do these things in this way. or  supposeSuppose iI change this a little bit  
(Refer Slide Time 38:20)  

i want to ifIf now it is told that A B C is aan equilateral triangle and iI have been told to prove that these two angles are equal B and C are equal how can iI prove it? myMy task is to prove angle B is equal to angle C, how do you prove it? thereThere are different ways of proving, you can try modusModus ponens or let us try the with resolution.  because we are learning it for the first time now so theThe first thing that we will be doing is, convert each of these English like sentences into a propositional form. So, if a triangle is equilateral then it is isosceles. 
iI can write this equilateral A B C implies isosceles A B C. Iif a triangle is isosceles then two sides A B and A C are equal. iI can write that isosceles A B C implies equal A B and A C. Iif A B and A C are equal then angle B and angle C are equal. I can also write this this statement ias can also say equal A B A C implies equal B C. and the last one was the statement 
so thTheise were some if then type of implications or rules and there is a statement A B C is aan equilateral triangle. that statementThe proposition corresponding to that is simple equilateral A B C. now sThereforeo from these statements we have got a set of propositions. nowNow we have to convert each of these propositions into the clausal form. so So equilateral A B C implies isosceles A B C that is turning out to bewith NOT equilateral A BC OR isosceles A B C. isosceles A B C implies equal A B C that will be equivalently be NOT Not isosceles A B C OR equal A B A C. equal A B A C implies equal B C that will be converted to NOT equal A B A C OR equal BC.    you see P implies Q is NOT P OR Q i am sorry i have made a mistake here again it should be NOT equal A B A C OR NOT equal A B A C OR equal B C okay  
(Refer Slide Time 40:32)  

so hereHere you see here this implication sign is changed to OR. andAnd the fourth was automatically was originally in the clausal form so iI need not so i did not do anything. Therefore so  iI have got four clauses that were given but iI had another thing to prove, angle B is equal to angle C and w so hen that when youwe represent it that in a proposition is equal B C and so equaltherefore equal B C is a proposition AND it is in the clausal form itself so iI need not convert it to clausal form any further.
nowNow, one approach that all of you must have done in the course of mathematics sometime that’s calledis called proof by refutation sometimes it i’s also called reductio ad absurdum.refutation 
soSo in order to prove equal B C what we can do is, we can also disprove not equal B C. that ifIf we can prove that not equal B C is false that implies equal B C is true. 
soSo resolution actually tries to prove in that way. so weWe were supposed to prove equal B C and what iI will be doing here is iI will take the negation of this so NOT equal B C and iI will try to disprove it. thatThat means iI will try to prove that NOT equal B C is not a logical entailment of the set of propositions that has been given. okay thatThat is the method of resolution.
 
so let us try in this way hHere iI have enumerated all the clauses that iI had NOT equilateral A B C etcetera all the clauses including that A B C is an equilateral triangle except the clause that iI want to prove or actually the clause that iI want to refute. soSo iI take it on this side NOT equal B C NOT equal B C that is the thingwhich iI want to disprove by this resolution. 
nowNow we will start the method of resolution, how can iI do it? iI will select one of the clauses from here. where iI needwill  to be a llittle intelligent and find out that here is a clause which has got the compliment of this literal NOT equal B C and here is equal B C.  




soSo iI will select this clause, so iI will take this clause and i will try to resolve between these two clauses. whyWhy did iI select this clause? It is because these are the negation of some part of the clause that iI am going to disprove. so So when iI select these two clauses iI resolve them and when iI resolve them what iI get is these two will resolve out and the resolvent will be NOT equal A B A C. right Nnow when iI get NOT equal A B A C again iI look over here and iI will try to find out another clause which has got the compliment of that. so equalEqual A B A C is here so iI take that this clause and try to resolve between these two. asAs iI resolve what do i get as i resolve these two will resolve out so iI will get NOT isosceles A B C. 
soSo, if you just have a clear lookcareful look you will see NOT isosceles this clause was actually meaning isosceles A B C implies equal A B AC. 
(Refer Slide Time 44:19)     

soSo from that what have iI have  got is if it is equal A B C then iI get isosceles A B C. all right sSo you just form it in the clausal form. so iI have got the NOT isosceles ABC. all right 
now againAgain next step is, I start with this NOT isosceles A B C and iI will try to resolve, so which clause have got isosceles A B C? thisThis clause has got so iI select this clause so this clause comes in iI resolve between this resolvent and this what shall iI get? i will get NOT equilateral A B C. Nnow Ii get Not equilateral A B C so iI resolve further, here there is an equilateral A B C so what will happen if iI resolve between these two? oneOne is stating it’s equilateral A B C, another is saying NOT equilateral A B C 
so when i resolve them i will get a null clause. (Refer Slide Time 45:05)  

and let us stop for a second and think of whatWhat is the significance of this null clause? theThe significance of this null clause is that iI have arrived at a contradictory situation that is a a situation that is not supported by the given set of facts. thereThere were a given set of facts like this and iI started with NOT equal B C and from there by resolving iI will have come to a situation where I have got a null clause and the null clause and the null clause is essentially meansing that here is a contradiction. 
soTherefore the premise with which iI started is false. andAnd what was my premise? myMy premise was negation of the goal to be proved and that is disproved that means the goal is proved. thisThis is the method of resolution. okay soSo the basic procedure for resolution if you enumerate now will be like this: convert Convert a given proposition into a clausal form. convertConvert the negation of the sentence to be proved into clausal form. soSo first take the sentence to be true, take it’s negation then take it to the clausal form and combine all these clauses together into a set and iteratively apply resolution to the set as we are were doing, and adding the resolvent to the set. So, if we go up a couple of slides earlier say here 
so we tried to resolve this and once we resolved this we got this. nowNow my resolution is taking place between sorry between this clause and all the set clauses this entire set of clauses and we are resolving between this element and this element and getting a null clause. okay soSo the fourth step is, iteratively apply resolution to the set and add the resolvent to the set and we will continue until no further resolvents can be obtained or a null clause is obtained. okay so thatThat is the complete steps of resolution. Here is a quiz. now with that let us quickly
(Refer Slide Time 47:29)  

come to some practice quiz which you have to try these please take down considerConsider the following sentences: mammals drink milk. assumeAssume whether in the real world this is true or not, let’ us not bother about that. weWe are thinking of logical world so when we are making a statement iI am assuming that this statement is true. mammalsMammals drink milk, man is mortal, man is a mammal, tomTom is a man so these are four statements that we have said. all right proveProve that tomTom drinks milk, prove that tomTom is mortal it is very simple. okay actuallyActually you can also imply you can also can have put in some implication okay fine you can add in more implications but what you can try to do is, here  represent all these sentences in the clausal form. okay represent all the sentences in the clausal form nowNow prove five and sixthese two statements 5 and 6 
so these are the two to prove is to five and six  using modusModus ponens sorry and again try to prove five 5 and six 6 using resolution. Thereforeokay try to prove them using modusModus ponens and resolution. so that This is the assignment. 

Now let us look into the objective of today’s lecture that is the that you deprive so quickly summarizing what we wanted to do today was 
let’s go back to the objective we have the objective of today’s lecture was to show you how what is the formal way of inferencing using propositional logic. and weWe have discussed about a couple of things. oneOne is the truth table method. usingUsing the truth table method we can find out the truth correctness the truthness or the truth of any compound proposition when you know the truth values of the individual propositions. truth table is one method weWe have also seen that there are some inference rules which are not dependent on any interpretation. theThe clauses or the propositions will evaluate to true or false based on some interpretation. so we have got a set of propositions we will give you the interpretation we will have some to be true some to be false 
but the inference rule is independent, it i’s true for all interpretations. modusModus ponens is one such inference rule. okay weWe have also seen a couple of inference rules like P OR Q implies P that i’s also another inference rule. butBut modusModus ponens that if P is true then Q is true and we also know that P is true then we can derive that Q is true is a very powerful method of inference. thisThis is typically the deductive method of inference. theThe other rule and the a very powerful technique that we havewe learnted today is that of resolution. the resolutionResolution requires that propositions can be converted into clausal form.
(Refer Slide Time 48:22)  

now we have also seen the technique of how we can convert a proposition into its clausal form. Tthe steps are, that first we will have to do away with the implication sign which you can easily do by converting P implies Q to the technique NOT P OR Q and after that we will eliminate the double negation. thenThen we will apply the distributive and associative properties and after that we can get we will get a conjunctive normal form. And now we will decompose this conjunctive normal form into each of those components which are disjunct’s so each of them will be clauses. Hence we start with a set of clauses.
next Next when we want to do resolution we take the negation of the goal and convert into clausal form and then we take all these known things as well as the goal to be proved and resolve it in the manner that has been described, we iteratively apply that until we come to a null clause. (Refer Slide Time 49:11)  

ifIf we get a null clause then certainly the negation of the goal has been disproved and therefore the goal has been proved. so that was for today you work with the quiz’s that i have given in the next lecture we will see some of the limitations of propositional logic and we will see how new languages have been developed to overcome those limitations thank you                        

Artificial Intelligence
Prof. Anupam Basu

Lecture 13

First Order Logic
 (55:18)

inIn the last lecture we had discussed about one form of logic that is called propositional logic. wWe have also seenalso saw how reasoning is carried out in propositional logic. todayToday we will start discussions ondiscuss about a little more powerful form of logic called first order logic which is also called as predicate logic. thisThis form of logic can capture more variety of sentences than can be captured with propositional logic. butBut before moving into the tales of first order logic let us start with quiz’s that we had given in the last class. You can have a look here is you can have a loop at the statements that we had already given: mMammals drink milk, man is mortal, man is a mammal, tomTom is a man. theseThese were the four sentences which were given and you were asked to try to prove these two statements. tomTom drink or tomTom drinks for grammatical part of English in this tomTom drinks milk and tomTom is mortal. now you must have seen that i am not sure how many of you succeeded in proving this given the sentence these four sentences but intuitivelyIntuitively all of you can understand that mammals drink milk and man is a mammal therefore and tomTom is a man soand tomTom drinks milk that i’s obvious to common sense. butBut if you had tried the techniques that we had discussed like modusModus ponens resolution probably you will not be able to prove this mechanism with the information that was provided. you see thereThere is a subtle difference between these two (Refer Slide Time 03:05)  

statements these two statementsI am making.  oneOne is that, intuitively you are being able to understand and derive tomTom drinks milk. butBut what is the mechanism that is going behind this reasoning? unlessUnless we can understand that mechanism we cannot emulate that in the a computer and because a computer will mechanically do this. soSo the mechanical methods that we had discussed like modusModus ponens, resolution will not be able to prove it that. 
soSo what is missing? whyWhy cannot’t we do it mechanically although you can understand that? weWe had these statements: mammals drink milk, man is mortal, man is a mammal, tomTom is a man and we are supposed to do prove these. nowNow mammals drink milk 
 if we write that in this new form that there were some intuitive assumptions that we are making. tomTom is a mammal so mammal tomTom if tomTom is a mammal then drink tomTom milk is true. soSo you see the reasoning is possible and in order to make this reasoning possible we had to bring in this implication sign here and we have to write this it as an implication proposition.
similarlySimilarly man is mortal that can be written as, if tomTom is a man then tomTom is mortal this is my second implication. manMan is a mammal that can be written as, if tomTom is a man then tomTom is mammal because if tomTom is a man then tomTom is mammal which my third statement and tomTom is a man. nowNow this is no implication but it i’s just an assertion tomTom is a man. 
soSo what iI want to really say is that, in my statement mammals drink milk that’s a statement iI can say drink mammal milk that will be fine. butBut that would not help me in this particular type of reasoning and there is also ana hidden implication within this mammals drink milk. thatThat means say a particular person say tomTom in this case is a mammal then it is also true that it is that particular person that is tomTom will drink milk. 
 (Refer Slide Time 05:45)       

all right so inIn this way if we write then let us see how we can prove our desired statements. iI am now naming these as propositions P, Q, R and S. Yyou can see that mammal tomTom implies drink tomTom milk is a proposition. 
similarlySimilarly man tomTom implies mortal tomTom is another proposition. soSo all these are propositions these are implications and this S is just an assertion. iI have also numbered them one two three four1, 2, 3, 4. okay nowNow, given these propositions and if iI select my favorite tool like modusModus ponens if iI apply modusModus ponens in R and S that is man tomTom implies mammal tomTom is true and man tomTom is true then applying the knowledge of modusModus ponens you can certainly arrive at the fact that mammal tomTom. soSo iI can prove that tomTom is a mammal. okay again if iI apply modusModus ponens iI have derived mammal tomTom and if iI apply modusModus ponens on this number five 5 and number one 1 that is if tomTom is a mammal then tomTom drinks milk then if iI apply modusModus ponens on P and this new one which is mammal tomTom then iI can again derive drink tomTom milk. thusThus the desired statement tomTom drinks milk can be proved using modusModus ponens. untilUntil iI wrote them down as implication you shouldI could not apply modusModus ponens, iI could not apply the inference rule of modusModus ponens. nowNow let us try to do the same thing using the other proof technique that we have learnedwe learnt. theThe other proof technique was resolution. andAnd again in resolution there is a subtle difference between modusModus ponens and resolution in it’sin the application of resolution. inIn order to apply resolution iI have to convert each of this propositionsthese propositions into their clausal form.  
(Refer Slide Time 08:09)  

A and i hope you remember that clausal form is nothing but a disjunction of literals. So, mammal tomTom prime implies drink tomTom milk can be converted to again mistake here it should be not mammal tom sorry iI have written it in different way mammal tomTom prime that is the same is not NOT mammal so not NOT mammal tomTom OR drink tomTom milk iI can write it in that way. and And Q man tomTom implies mortal tomTom. whenWhen iI convert that into clausal form it becomes man tom notNOT man tomTom OR mortal tomTom. similarly Similarly the third one, man tomTom implies mammal tomTom says that man tomTom prime of that not NOT man tomTom OR mammal tomTom in this way iI convert each of them in clausal form note that is last one tomTom is a man, man tomTom is itself in clausal form so iI need not do anything in with it. nextNext i have what do iI have to prove? suppose Suppose i am taking there is only one of the statements that you were given is proven say iI take a statement tomTom drinks milk. 
so tThe form clausal form of that is drink tomTom milk and that the propositional form of that is drink tomTom milk but in resolution what we do? inIn resolution we try to disprove we proved by refutation so my starting point would be the negation of the goal which iI should try to disprove. thatThat was the basic approach of resolution. 
soSo here the goal is drink tomTom milk i so iI convert my goal to is to disprove that is proved that tomTom does not drink milk. tom The compliment of drink tomTom milk compliment of thatis not drink tomTom milk means tomTom does not drink milk iI have to disproved it, if iI can have to disprove it given these statements then iI can certainly prove that tomTom drinks milk. 
so nowNow let us try to do with resolution. iI start with the negation of the goal, iI select that one clause which has got the compliment of this and if iI resolve these two iI will get a new statement mammal tomTom prime that means not mammal tomTom. nowNow iI will have to again look into the set of statements that iI have to find out which clause has got the negation of not mammal tomTom or which clause has got the literal mammal tomTom. andAnd here is one not man tomTom OR mammal tomTom. soSo let me resolve these two. asAs iI resolve what would iI get mammal tomTom and not mammal tomTom will resolve out and my resolvent will be not man tomTom. soSo not man tomTom iI get. nowNow iI have to select the other clause that iI have man tomTom. okay nowNow as iI resolve these two what would i get iI get the contradiction with the null clause. thatThat means this goal has been disproved. nowNow let me just go through this and state it in a different way. myMy job was to disprove that tomTom drinks milk and in order to disprove tom drinks milk that means in order to prove that sorry my job was to disprove that tom does not drink milk, iI had to disprove that tomTom is not a mammal and in order to disprove that in order to disprove (Refer Slide Time 12:16)  

that iI had to prove show that man tomTom man tomTom is not a man but tomTom is a man so there is a contradiction. andAnd since there is a contradiction iI am arriving at a null clause thereby refuting my basic assumption drink tomTom milk. soSo this is proven that tomTom drinks milk is tom does not drink milk is refuted and tomTom drinks milk is proven. 
so we have proved similarly for the other statement that was given in the quiz you can try yourself and see how you can do this nowNow that is was for all about propositional calculus propositional logic. 
I also made a subtle adjustment in the formulation. we have i will tell you later that also the formulation i had made subtle adjust that say that wWhen it was written that mammals drink milk so iI have just said if tomTom is a mammal then tomTom drinks milk. well  iI will show you later that was not very fairir. we will come to that a little later now i am coming to this specific objective of the things that we learnt or that we are able to do after this specific lecture 
theThe objective of this specific lecture is to enable you to formulate different more variety of sentences using logic. andAnd it will enable you to write correct predicate logic formulae. weWe will discuss what is predicate logicpredicate logic is. so forFor that first let us look at the limitations of propositional logic. whyWhy do we need another form of logic when you have propositional logic? so theThe limitations of propositional logic are will be evident from thise statement. considerConsider the following argument: that i make all dogs are faithful. tomTommy is a dog therefore tomTommy is faithful. okay thatThis argument is’s a quite understandable argument allunderstandable. All dogs are faithful. tomTommy is a dog therefore tomTommy is faithful faithful. nowNow, how to represent and infer this in propositional logic? let’s try supposeSuppose P is the proposition all dogs are faithful that stands for all dogs are faithful. andAnd q is a proposition that says tomTommy is a dog. butBut can iI combine all dogs are faithful all dogs and how do iI capture how do i know that tomTommy belongs to this all dogs. soSo if all dogs are faithful is true then tomTommy is also covered over there if i say i mean all dogs is covering all dogs other than tomTommy so p AND q can iI say impliesusing tomTommy is faithful? No, we cannot infer this in propositional logic because propositional logic here there are two different constants two different things tomTommy and all dogs they are different. okay iI do not know what all dogs mean. may be you and me will understand but remember that whenWhen we are talking of logic 
 (Refer Slide Time 15:55)  

we are talking of mechanical methodology that will work irrespective of my subject of inferenceterpretation sokay so we have to prove it mechanically. Let u’s see a couple of more examples. tomTom is a hardworking student. iI can certainly write that as hardworking tomTom that’s as the a proposition. tomTom is an intelligent student, iI can write down intelligent tomTom. 
Iif tomTom is hardworking and tomTom is intelligent then tomTom scores high marks. soSo this is also an implication proposition and iI can write it as hardworking tomTom AND intelligent tomTom implies that scores high marks tomTom. (Refer Slide Time 16:45)  

There is fine no problem afterup to this. but what about jJoohn and jillJill. see here what we have said is, iI have made hardworking tomTom, intelligent tomTom and iI have written one rule that, if tomTom is hardworking and tomTom is intelligent then tomTom scores high marks. 
suppose Suppose Jjohn is another student so iI will havehave to write, if Jjohn is hardworking and Jjohn is intelligent then Jjohn scores high marks. Iif jillJill is hardworking and jillJill is intelligent then jillJill scores high marks. doDo iI have to enumerate what each of the student’s the rules for each of the students separately? isIs it a feasible way of doing things? andAnd a new student worksalks in the class you have to know his name his or her name and you have to write down another rule? that’s now we wantThat is not the way to do it. youYou should have a general statement to make which should apply to all these. butBut unfortunately if we are restricted to propositional logic we are restricted to this sort of statements that we can make. 
I
if we could write instead instead of this if iI could write, all students who are hardworking and intelligent scores high marks that define we have to find that it covers tomTom that it covers Jjohn that it covers jillJill or any student who will come in. uUnfortunately theise sort of statements all students whoa are hardworking and intelligent, scores high marks cannot be written in propositional logic because in propositional logic the structure of the proposition do not allow us to do that. 
so Therefore essentially when iI want to write some statement like this; all students who are intelligent (Refer Slide Time 18:33)           

and hardworking iI can write that in this way. Ffor all x for all x where x is a variable you see this is white this is variable, so for all x such that x is a student and x is intelligent and x is hardworking, first of all, all x, x can be a student, x can be a teacher, can be a visitor but iI am restricting myself to only the students. soSo for all the x such that x is a student and x is intelligent and x is hardworking then x scores high marks. nowNow x being a variable iI can put in a value for this variable and can make it tomTom, john jillJill or what ever I likei like. 
soSo what iI need is the ability is the power to write such sentences which unfortunately was not possible in the case of propositional logic. thatThat’ is why we need the predicate logic. 
soSo essentially what was the problem? theThe problem is therefore the problem of infinite model. inIn propositional logic we have to restrict ourselves to constants. butBut in general, propositional logic can deal with only a finite number of propositions. Iif there are only three dogs for example tomTommy, jimmyJimmy, laikaLaika for example then i could have written T issuch that tomTommy is faithful, J jimmyJimmy is faithful, L laikaLaika is faithful then all dogs are faithful will be T AND J AND L. Iif there are be a new dogs then it how long should we gwill go on that i’s not possible. okay sSo that is the finiteness of the statements and also even part of tomTommy, jimmyJimmy and laikaLaika iI have to write separate propositions and iI have to do this conjunction and that i’s where the problem of propositional logic lies. 
soSo what if there were infinite number of dogs? iI would have faced a problem. 
(Refer Slide Time 20:44)  

so thereforeTherefore we move to the a new thing called the first order logic. firstFirst order logic or predicate logic can be considered to be a generalization of whatever your propositional logic we you have understood and it allows us to express and infer facts over a large or infinite models. forFor example, all men are mortal. 
(Refer Slide Time 21:14)  

some birds cannot fly, at least one planet has life on it. goingGoing back to the previous statement all mammals drink milk, all mammals are mortal. all those statements i can doFor these kinds this sort of statements iI can do predicate logic first order logic or predicate logic but not the propositional logic. 
soSo what is added in first order logic? inIn first order logic in addition to propositional logic we have got the concept of variables and we have got concepts of quantifiers. which we will explain now  theThe syntax of first order logic syntax of first order logic can be defined in terms of, terms, predicates and quantifiers. (Refer Slide Time 22:11)  

okay weWe can express them in terms of three things namely terms, predicates and quantifiers. iI will explain each of them one by one now. whatWhat is a term? aA term denotes some object other than the constant that we have already encountered true or false. okay tomTommy is a dog, tommy is a dog all men are mortal, men is a term very similar to what we talked in propositional logic lectures (Refer Slide Time 22:56)  

wewe mentioned them them as objects so each of them is a term. okay nowNow, terms in this case can be constants as well as variables. inIn the case of propositional logic it was only itit werewas only constants. nowNow here we can see a constant of type W any type is a name that denotes a particular object in a set W. just try to understand whenWhen we are talking of about types of objects then that is a set of objects of a particular type type. Nnow when iI am saying constant then iI am particularly pointing at an individual object of that set. okay iIf iI say any particular variable that belongs to the that set belongs to thate type then it can connect itself to any particular object of that set. butBut when ever we are saying constant then that is mapping to the particular object of that set. forFor example, if iI say five 5, 5five is an object is a particular object in the set on natural numbers so it is’s a constant. tomTommy, among all the names all the objects called dogs given the their names tomTommy of is athe particular dog.
Oon the other hand a variable of type W is a name that can denote any element in the set W. any element in the set Ffor example, x belongs to N is a set of natural numbers numbers. soSo x is a particular natural number, okay x is a variable that can connect to any particular natural number whereaswhereas if iI say five 5 then that is in a particular natural number, it i’s not meaning any element of that set of numbers. similarlySimilarly d denotes the name of a dog any dog so that is the basic difference between constants and variables. 
(Refer Slide Time 25:00)  

nextNext let u’s understand what is a function? aA functional term of arity n, okay how many parameters the function will have, takes n objects of type W1 one, W2 two like that up to W n of different types maymay be of different types as inputs and returns an object of the type W some other type W okay can be anything all right soso it takes parameters of objects which can be variables or constants and returns an object of type W. 
Nnow, for example, we are familiar with theise statements, function f is a function of W1w one, W2 w two, W w n plus is a function which has taken two constants, two objects of type constant okay from the set of natural numbers, so this is a function. 
now soSo here this is the functional term plus is the functional term, three 3 and four 4 are constant terms and seven 7 is also a constant term. 
soSo seven 7 is a value that is an object that is returned by this function. that is returned by this function okay (Refer Slide Time 26:30)  

nowNow let plus be a function that takes two arguments of type natural number and returns a natural number. nowNow if that is be my definition then let’slet us see,  plus two three2, 3 this one plus two three that’s a valid that’sis a valid function. plusPlus five 5 plus seven three7, 3 is also a valid function because plus seven three7, 3 will return an object of type natural number that i’s 10. ten and tTherefore plus five 5 and ten 10 both belonging to type natural numbers we will add up and return a value of type W that is ten nthe natural number 15.
similarly Similarly plus plus hundred 100 in that way iI can go on writeing  in this order it is also valid although they are number of nested functions. here And here you see based on my the definition that iI gave that plus is a function that takes two arguments of type natural numbers and returns a natural number based on that definition this is an invalid function because minus – 1 one is not a natural number and iI have defined plus to be a function that takes two arguments of that both the arguments should be of type natural numbers and returns a natural number neither of these two satisfy. 
similarlySimilarly plus “one point two”1.2, 3.1 “three point one” this is not a valid function based on the definition of plus because “one point two1.2” and 3.1“three point one” are not natural numbers they are belonging to the real numbers. 
similarly (Refer Slide Time 28:12)               

Similarly we can have functions with variable arguments like plus x, y product prod x, y, z x y z any of these things we can have. It i’s not necessary although in the examples we have shown in the earlier example we have shown it with constants it can also have variables. nowNow that is what is constant function is.
(Refer Slide Time 28:36)  

where some of you have learned that in the discussion of the course you can consider this to be studied then whatWhat are predicates? Letlet us very easily understand that predicates are like functions except that they can only return, the return type can be only true or false. theseThese two constants can be true. otherOther things are same as functions. but return aA function can be any type of values but here it i’s a little restricted that a predicate will either be true or false. okay 
so So predicate is a function except that return type is only true or false. nowNow let u’s look at these examples. Ggreater than g t stands for greater than, greater than x, y variables is true if x is greater than y. now these are variables you see Iin propositional logic we could not deal with variables but here in predicate logic in functions we can have variables and predicates being nothing but functions and we can have variables in predicates as welll right and that is giving us some a lot more expressive power. 
so So, greater than x y is true if x is greater than y. Iif x is instantiated with a value five 5 and y is instantiated with a value ten 10 then greater than this entire predicate value will be false because five 5 is not greater than ten10. soTherefore  here greater than is a predicate symbol that is taking two arguments of type natural numbers but it is returning a value true or false. So or greater than three four3 4 is a valid predicate is a valid predicate but greater than three minus four3 – 4 is not a valid predicate why because greater than is a predicate that has been defined. suppose Suppose you take two arguments of type natural number four is not a natural number– 4 minus four is not a natural number therefore this one is not a valid predicate. 
theThe validity of the predicate now has got one relationship with it being evaluated true or false. theThe valid predicate g t x, y can sometimes evaluate to true sometimes evaluate to false depending on what values (Refer Slide Time 30:56)  

are being given to theise variables x and y. Nnow let u’s see the different types of predicates. theThe simplest predicate is a predicate with no variable. ifIf we have no variable then iI will get what we had done earlier that i’s a proposition. soSo tomTommy is a dog is a proposition we know that and this thing is also predicate sopredicate. So predicate is more general. soSo predicates can capture everything that proposition can but not the other way round. 
soSo predicate with one variable is called a property. dogDog x that is x is a dog. if x is if an thisThis is true if and only if x is a dog. mortalMortal y is true if and only if y is mortal. 
(Refer Slide Time 32:01)                

okay soSo a predicate with one variable is called a property. nowNow how do we formulate itit? letLet P having arguments x, y etcetera and Q having arguments x, y etcetera are two predicates. rememberRemember how we defined well formed formulae or valid sentences in case of propositional logic. weWe have got exactly similar type of techniques definitions in the case of predicates also. So, in predicate logic if P and Q are two predicates then P OR Q is a valid predicate well formed formulae, P AND Q is a valid predicate, NOT P is a valid predicate, P implies Q is a valid predicate. exactlyExactly these similar things we had in the case of propositional logic. okay soSo let’ us have some more examples. ifIf x is a man then x is mortal, this is a statement that iI want to make. soSo iI can write, man x implies mortal x, this is a valid predicate it i’s no longer a proposition because iI have got variables here. anybody ifIf anybody is a man that is any man is mortal. soSo in the clausal form it will be NOT man x OR mortal x. iIf n is a natural number then n is either even or odd. just try to do it 
 
iIf n is a natural number that means n is a variable that must be a natural number then n is either even or odd. howHow do iI express it? iI can express it natural n if n is a natural number that then even n OR odd n. all right Nnow even n OR odd n any one of them will be true. 
so in that way you can see just letLet u’s once again look at the last example here that looks like a complicated sentence but one as soon as we write in the form of predicate logic and if you understand that then it becomes very really subtle. okay nowNow we have said that predicate logic requires variables, i mean it allows you to have variables and in addition to variables there is another thing that predicate logic is introduced and that is called quantifiers. Nnow there are two basic quantifiers (Refer Slide Time 34:37)  

in first order logic or predicate logic for all which is denoted as this v with a horizontal line across it. nowNow for all this is also known as universal quantifier. 
forFor example, if we say for all x then that is iI am talking of all for any value that x can take the proposition to be true. anotherAnother one is; there exists which is meaning which is also called existential quantifier that something is existingexists. okay thereThere is a subtle difference again between these two. soSo, for all x suppose think of a set let u’s just think of a set, all right let’s try to do this suppose iI have got a set and in that set iI have got a number of elements. all right nowNow i when iI write for all x something is iI write P( x) that is the P x  iI just say that P( x) is true for all x. nNow these are not thesthise need not be new predicatesdenoted as x so iI can just put thick dots here, theise areis just elements of the set. whenWhen iI write this then what iI mean is that this P (x) will be true for any element that iI choose from this set. all right nowNow, suppose let me choose another caller here and i am marking some might be this is not very feasible let me put this color and mark it they are some special elements, and i write suppose iI write there exists x such that Q( x) all right. thatThat means this Q( x) should be true there should be at least one element in this set for which Q x is true 
now here i have shown that for these two elements in this set for which Q(x) is true. 
Now here I have shown the elements Q( x) is true. all right letLet me just write down an example, there exists x such that holiday( x). okay nNow if it is 1, 2, 3, 4, 5, 6one two three four five six and let me put in another one, suppose these are the seven days in a week now is this statement true? thereThere must be an element in theis set suppose this is the days of the week. Now, obviously here lies Saturday and Sunday two days for which holiday( x) is true. thereforeTherefore there exists an x there exist a day for which holiday( x) is true. atAt least one would have 38:2sufficed. S8 suppose Saturday is not a holiday, okay only Sunday is a holiday so that this one is not a holiday this is just an ordinary dot but at least iI have got one then this statement will be true. thisThis entire predicate will be true only if this entire thing i mean if there is at least one x. 

onOn the other hand in order that this statement is true all these elements in this set must satisfy P( x) that is the basic difference. all right (Refer Slide Time 39:00)         

soSo, that is the essential difference between existential quantifier and universal quantifier. nowNow let u’s look at some more universal quantifiers. allAll dogs are faithful. nowNow iI write, x is faithful is being denoted as faithful x, x is a dog is denoted by dog x so iI can write for all x for all dogs in that set for all dogs dog x implies faithful x and that i’s true for any x which will make dog x true. thatThat meanis for any dog any x that for any x that is a dog thiis faithful x will be true. Now, how can we do this? (Refer Slide Time 39:56)  

theThe next statement is all birds cannot fly. suppose Suppose we decide that with fly x we will denote x can fly and x is a bird denoted by bird x. all All birds cannot fly, therefore it is not the case that for all x bird x implies fly x. Fforget about this NOT part right now. forget about this inIn that case if iI hadve written this for all x if x is a bird then x can fly. nowNow that is not true. iI am saying all birds cannot fly so this statement is not this entire statement for all x bird x implies fly x is not true. soSo this is the implicate this is the predicate logic expressionexpression for this statement predicate logic expression with quantifier and variable for this statement all birds cannot fly.
(Refer Slide Time 41:06)  

nowNow let us quickly revisit (Refer Slide Time 03:05) the quiz that we have done and iI said that iI did played a little trick iI did a little bit of adjustment. theThe statements which are given were; mammals drink milk, man is mortal, man is a mammal, tomTom is a man these ware ere given and iI had written them as mammal tomTom implies drink tomTom milk. like that butBut this statement is unfortunately not a statement made for tomTom alone. soSo iI should write it in a little different way. howHow should iI write it? iI can write it in this way now. nowNow iI know predicate calculus so why should iI do that sort of adjustment that iI did. 
soSo iI can write; for all x mammal x implies drink x milk. noteNote here, milk is a constant object, among all the food milk is one, x is a variable so for all x mammal x, if x is a mammal iI take one animal and see whether that i’s an mammal and if that is the case then x will drink milk. manMan is mortal, for all x if x is a man then mortal x. man is a mammal, here iI should write i have i should write for all x here here i should have written for all x 
so (Refer Slide Time 43:00)      

here i should have written for all x with some color. all right this was the mistake here soSo it should be for all x man x mammal x and tomTom is a man man i tom is a man was as before man tomTom there wasis no problem with that at all. 
soSo now see we can do it in a much easier way much moreand natural way instead of making adjustments and that is the power that predicate logic has given us. okay now let’s look a little bit those examples i could have done this example right nowIn the examples given to you  the universal quantifier was sufficient to give youdeal with it. nowNow let u’s look at existential quantifiers. if iI make a statement at least one planet has life on it that means here iI say planet x will denote x ais a planet has life x will denote x has life on it. Tthen my statement would be, there exists an x such that planet x AND has life x that means there exists there must be one planet there must be one element of that set of planets at least one such that it is a planet and it has got life. all right

 soSo this is where iI mean universal quantifier would not do. ifIf iI had written here for all x planet x AND has life x then this would be true only if all planets have life. But no that is not what iI want to do, what i want to do is to show to find whether there is at least one planet which has got life. okay iI am not going into starts iI am just going into planets that i’s why iI put in this predicate planet x. Nnow another one saylet us look into another  statement; is like this all birds cannot fly all birds cannot fly tthis statement is just equivalent to the statement there exists at least a bird which cannot fly. Iif you say well all birds cannot fly i saythen okay go ahead and prove it and you can prove it just by selecting at least one bird might be ostrich and you can show that you see here isthis a bird but it cannot fly. soSo what you need to really say is; there exists a bird there exists a bird which cannot fly or iI can say there exists an x such that x is a bird and x cannot fly. soSo we can straight away say all birds cannot fly that is equivalent to and by bidirectional implication that is equivalent to there exists a bird that cannot fly there exists an x such that x is a bird and NOT fly x. bBoth these things are true for this for a particular x at least. inIn that case i can prove that well all birds cannot fly. right 
(Refer Slide Time 46:30)  

all birds cannot fly right now let’s look at another interesting aspect that’s called the duality of quantifiers. thereThere is a duality between this existential quantifier and universal quantifier. youYou must have observed that duality if you have noted that thisin the earlier example that all birds cannot fly, it plays a duality, plays it well i am doing I am saying all but iI am translating considering that into some sort of existential statement. nowNow say if we have, all men are mortal that is equivalent to no man is immortal. similarSimilar to De Morgan’s okay double negation double negation no man is immortal. Nnow formally we can say for all x men x this is the notation for a particular type, for all x men x implies mortal x. Tthat means it is equivalent to there does not exist an x such that men x and NOT mortal x so iI can always do that. similarlySimilarly there exists birds that can fly. iI can always make it make the statement like this it is not the case that all birds cannot fly. nowNow try to read it a little carefully. thereThere exists birds that can fly. there exists birds that can fly aAgain going back to the earlier type, of thing if iI have a set here if i have a set here like this in which there are elements and my statement was, is not the case that all birds cannot fly all birds cannot fly then it i’s again sufficient for me to show one particular element one particular element and show that it can fly so there exists an x that can fly. okay soSo there exists birds that can fly, one is sufficient. thatThat means it is not the case that all birds cannot fly. nowNow how to express that? formallyFormally there exist a bird which can fly, if iI can prove that then iI can certainly prove that for all x birds cannot fly it is not true. 
soSo that shows theat duality (Refer Slide Time 49:18)  

of such statements. universalUniversal quantifiers can again be expressed in a different way in a different way with existential quantifiers. we can do that now we can go ahead with sentences now let’s just do this a predicate is a sentence anyAny predicate is a sentence if s e n is a sentence then s e n compliment is a sentence. okay andAnd x is a variable then within parenthesis this sentence is a sentence, complement of a sentence is a sentence, there exists x sentence is a sentence, for all x sentence is a sentence, sentence AND all sentences are connected with AND OR implication are sentences. thisThis varies and nothing else is a sentence. Nnow if you recall if you recall what we discussed in the case of propositional calculus we had also given sort of such strict definitions for what is a well formed formulae we had given that and that is we had given a couple of examples and so in some sort of hard definitions. 

 we will come to this points later now before that let us look at couple of exercises now predicate logic will require a couple of more lectures to cover all the points but what let’s quickly first see what we have learned today we have seen that in the case of predicate logic predicatePredicate logic is an extension of propositional logic. It is a more powerful version of propositional logic with incorporation of variables and with the incorporation of quantifiers. thereforeTherefore with these incorporations we can capture sentences much more naturally which we do not either we could not capture it propositional logic or we had to play tricks and do some artificial maneuvering 51:54 toto capture those as iI did in the first solution of the quiz. butBut as soon as we have got the power of predicate logic we can do that. andAnd what are the two types of quantifiers? theThe two types of quantifiers that we have are universal quantifiers and existential quantifiers. fine now againAgain there is a duality between these quantifiers. now closing the whole thing with see recall that inIn the case of propositions we said if P is a proposition and Q is a proposition then P if P is a proposition then what was the compound proposition P OR Q? NOT P is a proposition, P within parenthesis is a proposition, P OR Q is a proposition, P AND Q is a proposition, P implies Q is a proposition and nothing else is a proposition. that’s what we did inIn the case of predicates we also said that if P is a predicate so and Q is a predicate then so is not P that i’s also a predicate. thenThen for all x P is a predicate, there exists x P is a predicate. okay anyAny predicate implying another predicate is a predicate. predicatePredicate within parenthesis is a predicate and nothing else is a predicate. so  nowNow propositional logic and predicate logic we can put them side by side and see the similarity and also you have to observe the extra power that we have got in the case of predicate logic because of incorporation of variables and quantifiers. now quickly let us have a couple of examples or exercise that you should try let’s take Here are some sentences which you should convert in the form of propositional logic sorry which you should convert in the form of predicate logic. someSome dogs bark, (Refer Slide Time 54:01)  

all dogs have four legs, some dogs bark all dogs have four legs all barking dogs are irritating, no dogs purr, fathers are male parents with children, fathers are male parents with children students are people who are enrolled in courses. students are people who are enrolled in courses okay so please take them down and theseThese are some of the exercises to work out.
which you can try to do and in the next class we will come back to this point we will assume that the basic definition and writing predicates will be clear in the examples that you do and again in the next class we will look more into predicates and specifically we will see how we can perform inferencing with predicates thank you            

Artificial Intelligence
Prof. Anupam Basu

Lecture 14
Reasoning Using 
First -Order Logic 
(59:19)

In the last lecture we introduced the first order logic also known as predicate logic to express the different domain specific knowledge we require in order to build an integrated system. In today’s lecture we will be talking about different ways and means for reasoning using such a language that is the first order logic. How we can use different sentences of first order logic to reason out a particular problem is what we will start looking at from today. Before we go into the inferencing process we would like to first show or rather revisit some of the notions we presented in the last lecture. To start with we introduced sentences in first order logic. What a sentence is? A predicate is a sentence and we had already seen what a predicate is. And if sen is a short form of any sentence and say sen prime is a negation of sen, so suppose there is a sentence or the negation of a sentence these are sentences on the whole. (Refer Slide Time 03:27)  

If these are sentences and x is a variable then sen is a sentence, NOT of sen is a sentence, there exists x such that the sentence is true is a sentence in first order logic, for all x sen is a sentence, now these two are equivalent. If you look I have sometimes written the NOT symbol as this and sometimes as this prime. But essentially these two are meaning the same thing. So sentence and the negation of the sentence is a sentence, sentence OR the negation of the sentence is also a sentence. Suppose we are considering two sentences this is not the proper way, say sen and sen prime are two sentences suppose there are two sentences  and NOT sen is the compliment of sentence, if sen is a sentence and sen prime is another sentence and x is a variable then sen is a sentence, NOT of sen is a sentence, there exists x sen is a sentence, for all x sen is a sentence, sentence, sen and sen prime the conjunction of these two sentences is a sentence, sen OR sen prime is a disjunction of these two sentences is a sentence, sen implies sen prime that is one sentence is implying the other is also a sentence. And in proper predicate logic nothing else is considered as a sentence. 
So what we really mean is, we start with a predicate say P(x) tall(x) that is a sentence and say strong(y) is another sentence. So tall(x) tall(x) is a predicate and hence it is a sentence. there exists some x such that tall(x) is a sentence, for all x tall(x) may not be true, that is for all people they are tall, for all x tall(x) is a sentence may not be true, may not be satisfied in a particular domain. So tall(x) and strong(y) is a sentence, tall(x) strong(y) is a sentence, tall(x) implies strong(y) is another sentence. So these are sentences. That means if we take a predicate and the negation of the predicate is a sentence since the predicate itself is a sentence existential quantifier which we had introduced in the last lecture existentially quantified sentence is a sentence, universally quantified sentence is a sentence, the conjunction of two sentences are sentence, the disjunction of two sentences is a sentence, implication one sentence implying the other is also a sentence and nothing else is a sentence. Now let us look at some example sentences. If we write the predicate birthday (x, y) so this is a binary predicate and why is it a binary predicate? It is because it has got two variables as parameters, what is the meaning of this in a particular domain? It means that x celebrates birthday on the date y. (Refer Slide Time 09:15)  

So for example, Tom celebrates birthday on 1st January or Julie celebrates birthday on 1st March etc. So in general I can say x celebrates birthday on date y and can be written as this predicate, so this is a sentence. Now If we write for all y there exists x such that birthday(x, y) what does it mean? Looking at this predicate we can see x celebrates birthday on the date y. And what does this sentence mean? This is a sentence according to our last definition. This is also a sentence because it is existentially quantified. You said there exists x sen is a sentence but again you are saying for all y. Let us go back a little bit and let us have a look at that again. Here it is said that an existentially quantified sentence is a sentence so this whole thing is a sentence. Therefore if I apply a universal quantifier over a sentence then that also remains a sentence. So here we can see that x celebrates birthday on date y and what does it mean? That is, for all y that means for all dates, for all possible dates there exists some person who has got a birthday on that date. 
(Refer Slide Time 11:11)  

Now if I express this in English then it would mean, that for all dates there exists a person who celebrates his or her birthday on that date. That is, everyday someone celebrates his or her birthday. Now this relatively complicated notion can be very succinctly, in a very expressive manner, in a very brief and packed manner can be expressed in the language of first order predicate logic as is shown here. So this is a sentence and when I quantify this with an existential quantifier that is a sentence and this whole sentence when quantified with a universal quantifier is also a sentence. 
Next let us go to another example; when I write brother (x, y) that means that x is the brother of y. Now, when I write loves (x, y) that means x loves y. These two are predicates, both binary predicates and these two are also sentences. Now I want to make a more expressive statement; brother (x, y) implies loves (x, y) and that is universally quantified over all x and all y. That means for all x and for all y if x is the brother of y then x will love y whether in real world that is true or not that is a different issue but this is a proper sentence in terms of language of first order predicate logic. So what is the meaning of this? The meaning of this sentence is; everyone loves, all of his brothers or her brothers, now this is a correct sentence. Now you might have noted that often I am saying valid and then I am just changing that word valid because I am using this word as a valid sentence in normal English but in terms of logic since we have already discussed the interpretation of sentence in a particular domain this word valid has got a special meaning. When a sentence is evaluated to be true in a particular domain then the sentence is said to be satisfied. But when we say a sentence is valid strictly in terms of first order logic then actually we mean that for all possible domains this sentence will evaluate to be true. So it has got a special logical connotation although in our normal day to day English we often use the word valid in a much looser sense. That is why whenever I am talking of logic and normally the word valid is being used but I am trying to retract from it. So it is a correct sentence in terms of the grammar of first order logic. So, everyone loves, all of his or her brothers is expressed in this sentence. 
Now let m (x) represent mother of x then I want to express this English sentence everyone loves his or her mother. How would I express it? Suppose by m (x) I denote m is a predicate this predicate will denote mother of x and I want to express everyone loves his or her mother what would be the correct first order predicate logic sentence in order to express this? It will be for all x loves x m (x) why is it for all x? Everyone so I can take any x and whatever x it be Tom, Dick, Harry, Ram, Shyam, Jagjeet then x will love mother of x because m (x) denotes this. So these are some examples of first order predicate logic sentences. 
(Refer Slide Time 14:01)           

Till now we were giving examples from our day to day usage but when we come to a little more mathematical domain then also we will find that first order logic can express a lot more things in a powerful and succinct manner. Here is an example; 



Suppose any number is the successor of its predecessor, I take any number 7  what is the predecessor of 7? It is 6 and what is the successor of 6? It is 7 so these are very true sentences that any number is the successor of its predecessor. so in order to express this I choose two predicates successor x and predecessor x that is succ x pred x. Now with these two I also Introduce another predicate equal (x, y) which means x and y are equal. Now using these three predicates I would like to express this English sentence that any number is the successor of its predecessor how can I do it? For all x equal x successor predecessor x, now let us try to see what it means? For any number if x be the number then predecessor of x will return me the predecessor of that number and the successor of that number will return me the successor of predecessor of x. And this number, and this number, and this number should be equal. Say x is 20 then predecessor x will return me 19 then I am computing successor of 19 what is that? It is 20. So 20 is my y and the x was my 20 so these two are equal. So these two are equal and the same thing can be expressed in this way. It is for all x so you can take any number and this will hold true. Now you can think that I could have represented that in a better manner why I did not use the equality sign? I tried an alternative way of representing it. So the previous example can be represented very succinctly as, for all x successor of predecessor x equals x. 
Now if you are careful you can note that I have put in this equality in a different color. Unfortunately the definition of sentences and the valid operators we mentioned in the case of first order logic did not have any equality operator. There were predicates, there were quantifiers but this equality operator was not there. Therefore we had to take recourse to this sort of (Refer Slide Time 18:00)  

predicate equal (x, y) which was essentially denoting x equals y. In the standard predicate logic as defined this equality operator does not exist therefore this is not a grammatically correct predicate sentence. However it was found that this limitation was really becoming a little difficult to maintain because often if we had the equality operator allowed in predicate logic we could have expressed many sentences in a better way. Therefore first order logic was extended with first order logic with equality; it was extended to first order logic with equality. In first order logic with equality we are allowed to use the equality sign between two functions. This is for representational ease and so we can now modify the definition of sentences we have provided where we had written sen, sen prime as two sentences we can call them as terms. 
 Each of these sen can be called terms and there are different ways of defining this. We said sen, sen prime are sentences therefore for all x sen is a sentence. Now whenever it becomes a sentence we can call that small sentence as a term. So now we extend this definition as term equals term is also a valid sentence.  (Refer Slide Time 20:00)  

So we can now use this equality operator as a part of the first order predicate logic [r….] 20:20 valid operator. In the last quiz that was given to you now with this revision of predicate logic let us work that out a little bit. The first sentence we wanted to represent in predicate logic was; some dogs bark, you must have tried this and this one we can represent as a first order predicate sentence as there exists x because it is some such that dog x AND bark x, why do I need this existential operator? It is because some dogs bark and I did not say that all dogs bark. But if all dogs bark then also this will be true because in order to prove its truth I just need to find one dog one x which is a dog and which barks then this sentence is true. The next example is, all dogs have 4 legs. Now this can be represented as for all x dog x implies have 4 legs x here I should have put another parenthesis, here there should be another bracket because the scope of this for all x is over this entire sentence so there should be a corresponding matching parenthesis with this parenthesis. Actually there should be another parenthesis here. 
So, for all x dog x implies have 4 legs x. now there could be another alternative representation to this, I could have said here in the first one I have used this both these are unary predicates because have 4 legs is a predicate which is having only one variable parameter whereas here for all x dog x implies legs x 4 so legs is a binary predicate which has got one which is the variable x and here is a constant term. 
Here again I have missed one parenthesis, so for all x dog x implies legs x 4. So this is another way we can do it. Now just a question here, could I write this as, for all x dog x and have 4 legs x? Would it be correct or is this a better representation? Here I have said for all x dog x implies have 4 legs x. If I had written it as for all x dog x and have 4 legs x would there be any problem? That is another issue. 
Now another example that was given in the last quiz was, all barking dogs are irritating. Now with this discussion we had in revision of first order logic you should be able to work on it. The next (Refer Slide Time 24:34)  

sentence that was asked to converted was, no dogs purr, one possible representation of that is, there does not exist an x such that dog x AND purr x. now this is one way of representing it. Could there be another alternative way? If I had started this as, suppose for all x then how would I have written this? For all x dog x implies not purr x or could I write it for all x dog x AND not purr x, it is also possible. Here we bring in the duality nature that any existential quantification can also be converted to universal quantification. Therefore no dogs bark can be represented in this way. 


Now here was the next example; fathers are male parents with children. Now this is the solution to that; for all x if x is a father implies that x is a male and x has children. So I can represent this as father x implies male x AND has children x.  
Now the next example was; students are people who are enrolled in courses. May be you should be able to do that now. Try something like this; student x implies people x etc and put in the proper quantifier to that. Although it was given in the last lecture (Refer Slide Time 28:02)  

try out whether you can write them down in the first order logic. Also try another thing whether you can represent some of these in the form of propositional logic and when you cannot think why you cannot do it in propositional logic then try to do it in predicate logic. May be there will be some sentences which you cannot really capture in predicate logic also but those are different cases. But you will find many sentences that we use in our day to day conversation that you can represent it in predicate logic. 
This is all about the revision we intended to do about the first order logic. The next thing that we will be doing is inference rules because given the sentences which are represented in first order logic our ultimate objective is to reason using them. And in the inference rules there are some basic techniques which we must learn. One is universal elimination. That means, if we have a sentence like this for all x likes x flower then likes x flower will hold for all x. Therefore I can substitute this x by some constant like a particular name Shirin, Shirin likes flower, there will be nothing wrong that I am doing because you have said for all x. So whenever there is a universal quantifier I can drop it and represent this with a constant term. So eliminate this for all x part and the substitution of that x in this part of the parameter should be done with a constant term and that is technique one and that is universal elimination, the elimination of universal quantifiers. (Refer Slide Time 30:20)          

The next thing is which is little more complex which is elimination of the existential quantifiers now why are we doing this? We are trying to come to a state where we can take these predicate logic sentences and use them in our reasoning in a mechanical way and that is the purpose. So we are trying to give them a different form where we are removing the universal quantifier but what will happen in the case of existential quantifier. For universal quantifiers it was so simple. For all x P(x) so I can always drop that x and put in any constant because it will hold for all x. But what happens in the case of existential quantifier there exists some x for which P(x) is true. Now with which x can I replace that x part of P(x)? 
This method is also known as skolemization. Here for all x there exists x such that likes x flower. 
Now this one means likes person flower. Now this person is some particular person which is not there in the knowledge base and if I can find out that person I can put that in and that particular person say there is a sentence there exists x such that likes John x and food x say x is a food and there exists x such that John likes x. now then there is some x which John likes. Suppose I find out apple then I can represent that particular x parameter with apple and remove that existential quantifier. This method of finding out a particular constant that satisfies the predicate there exists x such that john likes x. Finding out that particular constant that satisfies this is known as skolemization and when we can find that out we can represent it. Thus by eliminating this existential quantifier we can get such a sentence where the existential quantifier is not there. Just the opposite, in some cases we may like to introduce existential quantifiers. That is existential introduction. Suppose I am given a sentence likes Shahid flower now I know that Shahid likes flower then I can certainly write it as there exists x such that likes x flower because I know that it will always be true because I can replace this x with Shahid and there will be no problem. This is just the opposite of existential elimination and this is existential introduction. Now with this
(Refer Slide Time 34:30)  

let us go into the reasoning and start with a particular problem. And the problem is defined here as we are making a statement; a perfect square is nothing but 16, 20, 5 etc, so, if a perfect square is divisible by a prime number p say 25 is a perfect square and is divisible by a prime p 5 then it is also divisible by the square of p this is one sentence I state and in mathematics that is right. The next statement I make is; every perfect square is divisible by some prime number. Think about a square 16 it is divisible by 2 it is a prime number. I also say 36 is a perfect square. So what are the three statements that have been made? If a number is a perfect square then it is divisible by a prime number and if a perfect square is divisible by prime number then it is also divisible by the square of that prime number. And every perfect square is divisible by some prime, 36 is a perfect square. Now with these three statements now I want to reason to find out does there exist a particular prime number q such that the square of q divides 36.  That is what we want to find out. The next step will be to convert these into predicate sentences. 
(Refer Slide Time 37:05)  

The next thing we will be doing is convert them into the first order predicate logic form. So here is a sentence; the method is, you have to take the sentences one by one and convert them into predicate form. 
So if a perfect square is divisible by a prime p then it is also divisible by the square of p. So I write it as, for all x and y or for all x for all y also which means the same thing the perfect square x if x is a perfect square and prime y so y is a prime and divides (x, y) that is this prime y divides x that takes care of this if a perfect square is divisible by a prime p. Although over here I have said p in my sentence I need not write p I can write x, y anything but I must be consistent in this sentence then it is also divisible by the square of p. This implies divides x which is a perfect square by the square of y. Now I did not use p in my logic sentence, I have used y so I have used y here also. So if a perfect square is divisible by a prime p then it is also divisible by the square of p. This is the conversion of this sentence into the predicate form. Next is, every perfect square is divisible by some prime. Since that is every perfect square so for all x perfect square x there is some prime number which divides it. So I put in for all y for all x there exists y some prime is there such that y is a prime so perfect square x (Refer Slide Time 39:45)  

AND prime y AND divides (x, y) x is divided by y. Therefore x has been universally quantified by y it is the same for every perfect square and y has been quantified with existential quantifier y because there is some prime which divides it, not that all prime numbers will divide it 
 There exists y and y is a prime number AND divides (x, y). So these are two sentences, what was the next sentence? It is very simple, 36 is a perfect square so I can write down perfect square 36 there is no problem with this. Does there exist some prime number q such that the square of q divides 36. In order to answer this question does there exist I can also pose it as a theorem which I will be proving. If I can prove it I can say I just make a statement there exists a prime q such that the square of q divides 36 I make that statement and then I try to prove it by inference. If I can prove it then there exists an x and if I cannot prove it then it does not exist. So this particular query that is shown here can be converted in the form of a predicate which I shall try to prove using my reasoning mechanism of first order logic. Therefore I can write it down as there exists y such that y is a prime AND divides 36, I can divide 36 by square of y. 
These are my sentences. With these I have got a knowledge base and what is that knowledge base? These statements; for all x, y, perfect square x AND prime y, AND divides x, y, implies, divides x square y that was the first sentence, the second sentence was this, the third sentence was this which we just now did. Now comes the reasoning (Refer Slide Time 42:40)  

the inference from two and universal elimination. What was two? Two was this for all x perfect square x AND prime y AND divides (x, y). Now when I eliminate this I want to prove it that I am dealing with a particular perfect square that is 36 that is of interest to me. Therefore I can put 36 in place of x and remove this for all x part. That was I put 36 here as well as I put 36 here that was our inference method of elimination of universal quantifier. So I eliminate that then I get a new clause, now the part for all x is gone so there exists y perfect square 36 AND prime y AND divides 36 y that is just the thing that was here and from here I have replaced these two x‘s with 36 and I have removed this universal quantifier and I have got the 4th clause. 
Now from this 4 I want to eliminate this existential quantifier. Now existential quantifier elimination is by skolemization that means I will find out some constant which is a prime. So let me put some constant P and remove this existential quantifier and get this new fifth clause, perfect square 36 AND prime p AND p divides 36. This is the fifth one and what was the one? One was this. I have eliminated from two but here for all x, y perfect square x AND prime y AND divides x, y implies divides x square y. Now 1 and 5 what is my 5? It is this one I can get. So I get perfect square 36 prime p divides 36 p. Now this was also universal quantifier so I put 36 here and in place of y I put p and I can remove all these quantifications straight away. So I get perfect square 36 and prime p and divides p 36 that was this part clause 5 implies divides 36 square of p.  
So that is the 6th one that I am getting.  (Refer Slide Time 45:54)  

 Now this is the 6th one. So I get divides 36 square p. Now from 5 and 6 prime p AND divides 36 by square of p what was 5 and what was 6? Here 5 was this perfect square 36 prime p and divides 36 p and I get p as divides 36 square of p. so in place of p I can simply put square p, here also square p so what I get is perfect square 36 and prime square p, prime p and divides 36 square p. Now you can see the use of existential introduction. I introduce this existential quantifier now all these are constants and in place of p I put a variable y, here also for p I put a variable y and introduce this existential quantifier and I get there exists y such that prime y and divides 36 square y. 
Now, if you recall, that was my thing to be proven. My objective was to prove this; does there exist a prime q such that the square of q divides 36? That was this thing; prime y divides 36 square of y and that is the thing I wanted to prove. And through the technique of universal elimination, existential elimination and again existential introduction and replacement of variables I get the same thing that means I could prove this I could infer this. However inferencing would have been easier if the knowledge base consisted of the following sentences. Suppose the knowledge base consisted of the sentences like this; perfect square 36 which was there, prime p divides 36 p and for all x, y prime perfect square x AND prime y divides x, y then the inferencing would have been easier. 
(Refer Slide Time 48:36)  

Now I would like to introduce a very important notion called a particular type of sentences which is known as horn sentences or horn clauses. This horn sentences are also known as horn clauses and this has got a tremendous application in the case of inferencing in predicate calculus. What are horn clauses? 
 (Refer Slide Time 49:08)  

What are the possible horn sentences? Any atomic sentence is a horn sentence. 
So, sentence like perfect square 36 is a horn sentence. Implications with a conjunction of atomic sentences where atomic sentences are joined together conjuncted and a single atom is on the right so a sentence like this; for all x, y perfect square x AND prime y AND divides x, y implies divides x square y that is a horn sentence and there is no existential quantifier. There should not be any existential quantifier. On the left hand side of this implication there are atomic sentences, there can be negations on that side also and on the right hand side we have got a single atom and there are no existential quantifiers. 
So if this be our horn sentence and this side the conjunction there is no ORing and here also there is only one unit now if a particular sentence given to you is not a horn sentence then we will have to convert it to the horn sentence. How can I convert it to a horn sentence? (Refer Slide Time 51:04)  

We know the fact that existential quantifiers can be removed using existential elimination. You can eliminate the existential quantifiers by the process of skolemization. If the existential quantifier is outside any universal quantifier a skolem constant is introduced. You know a skolem constant is a particular constant that I find out from the knowledge base and replace it in place of that particular variable y for example which was quantified with there exists. So, if I have something like this there exists y prime y now y being existentially quantified I can put a skolem constant p in place of this. This is a horn sentence. Otherwise the skolem function is introduced. When I do not find the constant I can still leave the responsibility as if I am leaving a responsibility to a function that function is a magic function which will some how find out a particular constant value which will satisfy the clause of the sentence. It will find out if there exists something that skolem function will find out that constant. For example here, for all x there exists y prime y and divides x, y here y is universally quantified so I replace there exists y with for all x prime say P D is a function say prime detector or whatever you can think of x. That divides x so in the given x I find out some prime P D AND divides x P D x where P D x is a skolem function, this is a skolem function. (Refer Slide Time 53:41)     

so using that I can get rid of the universal quantifiers. And the AND part can always be eliminated on the other side if a conjunction here on the right hand side I can always do say prime p AND divides x p can always be written as two clauses like prime p and divides xp it is possible. I can break them up into two different clauses and each of them remains a horn clause because any atomic sentence is a horn clause. The other thing which is often required in inferencing in first order logic is substitution. Substitution replaces variables with constants. Variables are represented with constants. If substitution is an operator x is replaced with 49 then if I do this substitution then perfect square x will become perfect square 49. substitution of x with 49, x is substituted with 49 and y is substituted with 7 then divides x, y with this particular substitution it will reduce to 49 replaces x so divides 49 and y is 7 so 7 divides 49. This is a substitution operator where I have to define a consistent substitution and then from this divides x, y I can come to this. Now this gives us 
(Refer Slide Time 55:05)  

a very nice operator often used in predicate logic that is unification. Let us just look at some examples of unification. There are algorithms given in any artificial intelligence book where you can find unification algorithms. Unification is a process of finding a particular substitution that will make two atomic sentences identical. When two atomic sentences are identical then I can use one for the other and that is our attempt. That is often our attempt that helps out in inferencing. So what is a substitution? All substitutions cannot unify. For example, there are prime x prime 7 and prime x now I apply the substitution x substituted by 7 then this substitution really makes these two identical so they can be unified. This means it is possible to unify prime 7 with prime x with the substitution of x with 7. It is possible to unify these two sentences divides 49 by x x divides 49 and divides 7 y these two can be made identical with the substitution of y by 49 and x by 7. So if I do this then what will happen is, this will become x by 7 so divides 49, 7 and divides y by 49 divides 49, 7 
This particular substitution is possible which unifies these two sentences. On the other hand you see unify prime 7 and prime 17. It is not possible because there cannot be any substitution that will change these to constants. 
(Refer Slide Time 57:43)  

On the other hand here you see, the attempt to unify divides 49, x and divides x, 7. It is not possible because if I substitute x I can substitute x here with a particular variable. In that case I will have to again replace it here these two cannot be unified these are not possible. So it is not the case that always I can find a substitution but if I can get a substitution then it will be possible to make two clauses identical and that will help us in inferencing.
 
 Let us quickly revisit what we had done in today’s lecture. A revision of first order logic, how to make the sentences and we have seen the basic steps of universal elimination, existential quantification elimination, existential quantification introduction and substitution as well as we have seen horn clauses and how non horn clauses can be converted to horn clauses. So with this background we will move to the next lecture which will be farther reasoning techniques on first order logic. 

              
Artificial Intelligence
Prof. Anupam Basu

Lecture 15
Resolution in FOPL 
(59:02)

inIn the last lecture we had discussed about inferencing mechanism in first order predicate logic. weWe ended with examples of inferencing where we had eliminated universal quantifiers, we had eliminated existential quantifiers and then we had also at times we ihad introduced existential quantifiers. weWe have also seenalso saw howtwo two very important operators as to how they aare used. oneOne is substitution and another is unification. Now, one very powerful inferencing mechanism that is used in inferencing in first order predicate logic is resolution. resolutionResolution is an inferencing method that is sound and it can be automated as well. so today before we todayTherefore in today’s lecture we will be discussing in this lecture how we infer make inferences using the resolution method. now  beforeBefore we start discussing about resolution technique as applied in first order predicate logic we will have a bABrief revisit a revision of how it was applied in propositional logic. weWe had discussed that this earlier also. soSo let us quickly have a recapitulation of how resolution was applied in propositional logic and then we will see how it can be applied in the case of predicate logic. 
soSo here the basic principle is that, suppose (Refer Slide Time 03:00)  

x is a literal and S1 one and S2 two are two sets of propositional sentences, right now we are not discussing predicate logic cases but we are restricting ourselves to propositional logic cases. andAnd what is the basic difference between propositional logic and predicate logic as we have seen? First, in predicate logic we can have variables whereas in propositional logic the parameters are all constant. and And the second difference is that, in predicate logic we can have quantifiers over these variables and may need two type of quantifiers namely the existential quantifier and universal quantifier and such quantification is not there in propositional logic. Sso here suppose S1 one and S2 two the sets of propositional sentences represented in the clausal form, we already know what is a clausal formAClausal form is, clauseand clause is a disjunction of literals. andAnd also we mentioned what is a horn clause is. In the horn clause horn clause just in the last lecture we had discussed that on the right hand side of the implication there can be only one literal that means in the clausal form there can be at most one non negative literal because say p AND q implies r in that case if iI write in the clausal form p AND q implies r if iI convert into the clausal form it will become NOT p OR NOT q implies NOT p OR. NOT q OR r right sSo if there is only one non negative literal if there can be only one literal on the right hand side and the left hand side are all conjunctions then we will have it when converted in the clausal form we will have at most one non negative literal. 
so clause isBut clause is a disjunction of literals so suppose x is a literal and S oneS1 and S twoS2 are two sets of propositional sentences which we have represented in the clausal form. ifIf we have x OR S oneS1 AND NOT x OR S twoS2 then we get S oneS1 OR S twoS2. you see lookLook at this, this is a revisit, we have already shown the same slides while discussing the propositional logic. soSo here if iI just do this AND then obviously x AND NOT x that part will vanish sorry x OR NOT x that will be always be true. so S oneS1 OR S twoS2 it will come to heretrue. Here S oneS1 OR S twoS2 this entire thing is called the resolvent. that means these two we have resolved these two we have resolved in order to get this all right and x has been resolved upon. all right (Refer Slide Time 06:15)  

so the exampleSo the same old example, if a triangle is equilateral then it is isosceles, if a triangle is isosceles then the two sides A B and A C are equal, if A B and A C are equal then angle B and angle C are equal A B C is a equilateral triangle and we are supposed to prove angle B is equal to angle C. 
nowNow given this (Refer Slide Time 06:38)  

now we next represent these in the form of propositions. ifIf a triangle is equilateral then it is isosceles. soSo we can write equilateral A B C implies isosceles A B C. 
if a triangle is isosceles then the two sides A B and A C are equal that we can represent as isosceles A B C implies equal A B A C. Iif A B and A C are equal then angle B and angle C are equal. writtenWritten in the predicate form it turns out to be equal A B A C implies equal B and C where B and C are angles. A B C is an equilateral triangle so simply we can write equilateral A B C. Nnow what is the next step? weWe have to convert them to clausal form. nowNow we had equilateral A B C implies isosceles A B C converteded to clause it becomes NOT equilateral A B C OR isosceles A B C. Isisosceles A B C implies equal A B  A C and  NOT that one translates to NOT isosceles A B C OR equal A B A C. 
similarlySimilarly, equal A B A C implies equal B C gets converted to NOT equal A B A C why is this NOT because if iI convert this in the clausal form this antecedent part becomes negated, i am sorry here  there is a mistake NOT equal A B A C this should be changed to OR right just hhas happened here NOT equal A B A C OR equal B C. okay please correct it in your notebook this part where the mouse pointer is pointing to should be converted to OR and next one is equilateral A B C. so So now we have got all thiese in the clausal form. 
nowNow resolution is the basic of resolution is proof by refutation. now thatThat means to prove that angle B is equal to angle C that i’s equal B C. Oour approach is let us disprove that B and C are not unequal. thisThis is a very well known proof method where in order to prove that B and C are equal we first try to prove B and C are not equal and fail to prove it. soSo we try to disprove NOT equal B CC. okay (Refer Slide Time 09:40)  

so ourOur goal our objective is to prove this equal B C. so So we start with NOT equal B C we start with NOT equal B C and we try to disprove it. soSo what is there?at weWe take the clausal form of the goal and negate it all right and try to disprove it that isand that is our approach. 
soSo let u’s look at this animated resolution. weWe have got in our knowledge base all these clauses equilateral A B C OR isosceles A B C etcetera etcetera all that we have discussed and this is the one that we want to prove. weWe want to disprove this NOT equal B C you have to disprove this.it so So, in order to prove equal B C we want to disprove NOT equal B C and then we have to resolve it and from here we will have to select one clause where we have got the negation of this equal B C where is that that’sthat is here equal B C. tThis is NOT equal B C and here is equal B CBC so iI select this clause and take this NOT equal A BAB A CAC OR equal B CBC. nNow this part and this part will resolve together and we get NOT equal A BAB and A C NOT equal A B and A CAC okay so this is the first level resolventing. nowNow we have to find the clause when the i mean dual of NOT equal A BAB A CAC, there is equal A BAB A CAC here so that is this oneis there so that this one so i. nowNow I try to resolve this resolvent with this clause and as iI do it what do iI get? I get equal A BAB A CAC and NOT equal A BAB A CAC resolve out and resolved upon and we will get NOT isosceles A BA CBC here. soSo this is a new resolvent and NOT isosceles A BA CBC we start with that and which one should we select? obviouslyObviously it is this ones one  because this ones one  has got the negation of this. hereHere isosceles A BA CBC is there so iI select this and as iI resolve these two what do iI get? theseThese two again resolve out and Ii get NOT equilateral A BA CBC. andAnd when iI resolve out NOT equilateral A BA CBC with this ones one equilateral A BA CBC what do iI get? iI get a null clause. thatThat means point from where iI started is disproved. Here by resolving this is a null clause that means iI am trying to resolve two contradictions. thatThat means the objection the goal with which iI started is not consistent with the knowledge base that iI have. with With the given knowledge base (Refer Slide Time 12:27)  

the goal which iI started with is contradictory. nowNow what was the goal? theThe goal was the negation of the goal that iI actually wanted to prove. soSo the negation of the goal that iI wanted to prove is disproved. thereforeTherefore the actual goal is proved. thatThat is the principle of resolution. andAnd the same principle we will hold for resolution in case of predicate logic as well. nextNext let us quickly see what is the then the basic what are the basic steps of carrying out resolution for propositional calculus. firstFirst step is we convert the given propositions into clausal form. nextNext we convert the negation of the sentence to be proved. pleasePlease note that we take a sentence to be proved and then we first negate it and after this negation we convert it to the clausal form. 
soSo convert the negation of the sentence to be proved into clausal form. combineCombine all the clauses in a set. and And then iteratively apply resolution to the set.t when we take so oneOne of the candidates that we start with is the clausal form of the negation of the goal that iI want to disprove. soSo iI take that one and take a suitable clause from the remaining clauses in the set and try to resolve it. as ifIf the resolution is possible then through this resolution iI will get a new clause. tThen iI will take the new clause and take another one and try to resolve it further until all the clauses are exhausted or iI come across a ca contradiction that i’s a null clause. asAs soon as iI get a null clause then iI immediately prove that the negation of the goal is contradictory therefore the goal is true. soSo iI iteratively apply a resolution to the set and add the resolvent of every iteration to the sentence. okay thenThen we continue until no further resolvents can be obtained or a null clause is obtained and that i’s where we stop. thatThat is the way resolution works. andAnd we have seen it through an example. Next, when we try to do it for predicate logic there will be some differences but the major principle of resolution is remaining the same, the major approach is remaining the same. hereHere also we will start with the negation of the goal converted into clausal form and the procedure of generation of resolvents are the same. (Refer Slide Time 15:31)  

howHow ever since you realize that we are now going to deal with predicate logic and not with propositional logic we will have to handle the cases of variables as well as quantifiers. andAnd in the last lecture we have already seen that how these quantifiers can be handled and how we can get the clausal form. soSo those steps are to be followed in order to apply resolution for first order predicate logic as well. soSo let’ us start with another set of examples. Let u’s consider a few statements as shown here. allAll people who are graduating are happy. allAll happy people smile. someoneSomeone is graduating. isIs someone smiling? thatThat is the conclusion okay that iI want to prove. i mean Here it has been posed as is someone smiling? iI want to have an answer yes or no but that is equivalent to proving a statement someone is smiling. someone is smiling nNow, if that is true that is if then the answer is yes, if it is disproved someone is smiling if that is disproved that means no one is smiling then for, is someone smiling the answer to that is no. now Now, if you look at these sentences we you will soon realize that all none of these sentences can be really represented in the form of propositions. now all people who are graduating letLet u’s forget about mechanical inferencing for the time being. ifIf we just apply our common sense reasoning llet u’s see what comes out. allAll people who are graduating are happy. allAll happy people smile. thereforeTherefore iI can reason and say all people who are graduating are smiling. andAnd then someone is graduating. thatThat means there is at least one person who is graduating and anybody who is graduating is smiling. soSo, is someone smiling should be yes or if iI write that someone is smiling that will be true should be true. write common sense butBut how ever a computer will not apply common sense in that way. itIt needs a mechanical procedure by which the same reasoning can be carried out. so first thing first we will (Refer Slide Time 19:07)  

Therefore we intend to code this problem in first order predicate calculus. andAnd then we will use resolution refutation, this is also known this resolution is also known as resolution refutation. nowNow why this word refutation wordis coming in? youYou can very simply guess. whatWhat are we trying to refute? inIn order to prove a particular goal g we are trying to refute NOT g and and if we can refute NOT g then we have to bringare proving g. right so thatThat is what is resolution is all about. soSo we used are using resolution refutation to solve the a problem. andAnd what is solving a problem? solvingSolving means finding out whether the conclusion can be answered from the given set of sentences. theThe conclusion is whether we can prove the statement; that we have expected is someone smiling  that thing whether that can be proved okay using what ever is there. Now, in order to capture the sentences that we had shown just a few minutes back we will have to first convert them in the form of predicates. so let us select something so inIn order to make the statement x is graduating someone is graduating we have to select the predicate and we have selected the predicate graduating x and that shows x is graduating. we use happy x actually strictly speaking dependingDepending on the language actual language programming language that we are using there will be some syntax’s, here i have taken some unnecessary may bethe conventions that mayshould have been in the order follow that all these predicates start with small letters. Here but, okay x is happy iI can write as happy x, x is smiling, smiling x. okay  nowNow we try to encode this sentencesthese sentences into predicate logic using the predicates that iI have just now stated. all All people who are graduating are happy. (Refer Slide Time 21:50)  

now that can be certainly written as; for all x graduating x implies happy x. all All people who are graduating are happy and that is universal quantification so there is no problem with that. all All happy people smile that is again universal quantification. all right someone is graduating, now what is the you should have done that here it has beenis shown but you could have also done that, someone is graduating, that means there exist an xand this parenthesis is not required like this here because there is no scope of it, there exists x such that graduating x that is someone is graduating. isIs someone smiling? now thatThat was my objective but let us consider that this statement is; someone is smiling. thatThat means there exists x such that smiling x is true. okay soSo, if there exists x smiling x is found to be true then the answer to this question is someone smiling is xyes. so these are soSo the first step was selecting the suitable predicates. andAnd once the suitable predicates are chosen then the next step that you do is write down these sentences express those sentences using predicate logic, using implication, using quantifiers. bBut the next step would be, you take these predicates here as they are but here iI have made a small change, is someone smiling or someone is smiling was taken stated as there exists x such that smiling x. so that has been converted to now thereAnd that  was a cthe conclusion and the negation of the conclusion therefore is there does not exist x such that smiling x. iIf iI can prove this that there does not exists x such that smiling x that means no one is smiling. okay butBut if iI disprove it that means someone is smiling. soSo we first negate the conclusion and then keep the others as they are. iI have not made any change in this.  (Refer Slide Time 24:58)  

butBut the next step is we have to convert them into clausal form. inIn order to convert them into clausal form the first step would be to eliminate the implication sign to eliminate the implication operation right and to eliminate this implication operation you know that we will take this clause for example. you see weWe are retaining this for all x for the time being and graduating x implies happy x will be not graduating x OR happy x.  
(Refer Slide Time 25:31)  

you know that we will take this clause for example you see we are retaining this for all x for the time being and graduating x implies happy x will be NOT graduating x OR happy x right so sSimilarly we can do it for the others. Hhappy x was implying smiling x so that gets converted to NOT happy x OR smiling x. Tokay there exists x graduating x and this did not have any implication at all and that was as it is and the second one was also not having any implication,  this ones one the fourth one was also not having any implication. Now iI have got a set of clauses which are all free of the implication sign. okay now theThe next step is to convert them toin canonical form. nowNow it may be that this canonical form often it’s called normal form may be new to you but let u’s not get too much barked bogged down with it, it is a form where the quantifiers are there and we have got the examples of inference. ultimatelyUltimately
(Refer Slide Time 27:02)  

we are trying to go to the clausal form ultimately we will get the clauses okay and it is basically the a conjunctive normal form so there will be disjunctions and there can be disjunctions in clauses. so here you can see theThe next step to do that is to reduce the scope of negation. all right soSo let u’s have a quick re look at what we had earlier. 
now you see forFor the first two the scope of the negation is over this predicate graduating x. all right andAnd for this the scope of this negation is over this entire thing even over this quantifier. soSo the scope of the negation is fine over here iI do not make any change here. However just compare the earlier clause number four and the present clause number four. hereHere the scope of negation was over this quantifier as well as this statement. nowNow iI want to push this negation as much as close to the particular predicate. soSo the negation of the existential quantifier is a universal quantifier. all right forFor example, iI say there does not exist a person, there does not exist any text x such that NOT mortal x but x is a man. So there does not exist iI can convert it to for all x mortal x. my question was let me work out here say so let’s look at it in this way i sayI say that there does not exist a person there does not exist x such that NOTot mortal x. so So the same meaning is communicated if I say for all thing people for all x mortal x so iI convert that there exists for all and NOT mortal to mortal. okay soSo iI push the negation inside so that i’s exactly what is being done over here. thisThis earlier one gets transferred to; for all x NOT smiling x. all right sSo you see iI have reduced the scope of negation here. okay 

theThe next thing is that we need to do is, we have to standardize the variables apart. it’ is possible that we can put in the variables the same variable that might have been used at different points. forFor example, here right graduating x happy x here again Not NOT happy x smiling x all those things all right so but when iI am trying to do this i will what iI will try to do is, for every clause iI will change that thethe different variablesvariables; every clause will have every sentence will have a distinct variable. nowNow if you recollect the idea of unification, now in order to we will see thisthis automatically gives rise to some problem that if we have got different types of variables then we must have some mechanical means of converting those variables in some way in order that we can do the unification. so here afterAfter we do this the next thing that we need to do is standardize the variables. soSo here you can see for that graduating x happy x (Refer Slide Time 32:22)  

happy x or smiling x so here we have put in we have taken differently we take the liberty of using different variables. Ffor all x NOT graduating x OR happy x so this ones one has got x, this has got y. Nnow if there were if there was a ca clause like say graduating x and happy y in that case put both x and y are reserved for that clause. inIn the other clauses we will have to use y z p q all those things. andAnd only later as we will see we will try to see whether these variables can be compared or unified. youYou remember what we discussed about unification. unificationUnification is the process of finding a particular substitution of it variables such that two different clauses become identical. Therefore we will have to do okay so tthat unification anyway.implication you have to do so here you see hereHere we have selected y smiling y. Nnow here again we have introduced another one that is itz and for all w smiling w so here you see youwe have made a change hereagain. all right 
You can very well realize that still it is not a clause. the next thing that we need to do but still it’s not a clause you can very well realize still it is not a clause soSo let us quickly recapitulate what ever we have done till now. whatWhat we have done is we have we are trying to convert it to clausal form. theThe first step that we did is the first step is we have selected some predicates and  predicates after selection of the predicates we encoded the given sentences in the form of first order predicate logic. andAnd then we negated the goal clause we negated that goal clause the goal and put it in the knowledge base then we will start converting then we took the canonical form or the clausal form, how do we do it? now Iin order to do that we will first eliminate the implication sign and that can be achieved by the simple rule that we have come across so many times p implies q is not p OR q after that and there also we convert the entire knowledge base in two Dincluding the negation of the goal. all right afterAfter that we used reduce the scope of implication and after the that is wwe try to bring the negation to as close as possible to the variables as possible and in that process we may require to change the quantifiers also. forFor example, NOT for all x will become there exists x  all right NOT there exists x will become for all x. okay afterAfter moving these negation reducing the scope of resolution negation then we standardize the variables. onceOnce theise variables are standardized then we try to push all the quantifiers to the left. okay inIn this example the quantifiers were not very much  (Refer Slide Time 36:08)  

within so but there can be complicated clauses. like say Ffor example let’s try to do an example there can be something like this; for all x iI am not bothering about too much of meaning here say P( x) implies there can be something else there exists y such that Q y it i’s possible like to express something like this it i’s possible. soSo in such cases there are two quantifiers but ultimately my objective will be to push all the quantifiers to the outside as much as possible. soSo the next thing that we try to do is, we move all the quantifiers to the left and using which we get in this case there was not much of change we come to these four clauses. okay thenThen we have to start working on elimination of the quantifiers. firstFirst of all let us try to eliminate the existential quantifier. andAnd we have are already introduced to the process of skolemization. in the just last lectures we had discussed that skolemization is a process of finding a ca constant or a function for the existentially quantified variable that will make this clause to be true. so forFor example, here there was in these two the earlier one here there were no existential quantification at all. theThe only existential quantification in this case we had was this. right thatThat was the only existential quantification. so what we do here itIt was, there exists z such that graduating z we remove that and in place of that we find out name one. name one (Refer Slide Time 39:05)  

is this a skolemization constant. itIt is the a name that somehow we are finding out who is graduating.e so So that particular value of x or z is the case is satisfying thise statement graduating x. Iin the last lecture we had also shown that in some cases we replace itresent NOT with a ca constant but with a skolemization functionconstant. likeLike in the prime divisibility case we had selected a function p d. recall now so So we have removed the existential quantifier that was the step five. what is the whatWhat is the sixth step? we We drop all the universal quantifiers         
(Refer Slide Time 40:01)  
                  
because of the simple reason i we do n’ot really need it. byBy now iI have removed the existential quantifiers by now iI have reduced the scope of negation to as close as possible to the predicates. andAnd by now iI have pushed all the quantifiers to the left and the existential quantifiers have been removed already. soSo what is left is just the universal quantifier and all theise clauses are therefore now implicitly universally quantified. soSo iI need not specify any further and so iI can simply drop them. andAnd as iI drop them iI get these clauses. now inIn this case again we did not have we only had disjuncts. nowNow, in a ca canonical form it can be conjunctsions of disjunctsions what is a disjunct?ion  again Again let’ us go herehere; say iI have got P( x) OR Q( x) this is a disjunction. andAnd say M( x) AND N( x) this is a ca conjunct. thisThis is a disjunct and this is a ca conjunct. nowNow, canonical form is a ca conjunction of disjuncts.ion all right soSo iI can say that P xP(x) OR Q xQ(x) if iI have a form M(x) AND M x sorry OR N( x). nowNow this is say R(y) x R y R y OR OR say G( z). nowNow this is another disjunct. soSo this is a disjunct, this is a disjunct and iI have got a disjunct AND disjunct and i have got a cconjunction of this disjuncts so it’ is a ca conjunct. Therefore right so canonical form will be a ca conjunction of disjuncts only all right disjuncts onl andy that is the canonical form. nowNow what is the implication of this? theThe implication of this is, if iI have conjunction of disjuncts,  say this one what ever we I have written here Ii just take that as an example and if you just think a little while you will realize that once iI have conjuncts of disjuncts then each of these disjuncts iI can write down separately. And say this one i can this one i can write down this ones one  Ii can write down as separately as P xP(x) OR Q xQ(x) and this ones one  iI can write down separately as R( y) OR G( z). nowNow according to our definition both these are clauses and a ca conjunction of disjuncts therefore each of these disjuncts can be written down as (Refer Slide Time 44:33)  

a separate clause and that is our objective, convert in the clausal form. okay sSo, after we have dropped all theise quantifiers we convert to conjunct of disjuncts form and make each of these as a separate clause and we standardize variables apart again. Sso that each of these clauses will now have separate variables. theseThese steps do not change the set of clauses in the present problem. (Refer Slide Time 44:51)  

so nowNow we come to the resolution problem. The but before that let’s come here that here it’s not a very those steps 7 seven and 8eight are not very so much important here because by step six 6 here we have already got them in the clausal form. thereforeTherefore we now can apply resolution and let us recall what how should we start resolution? weWe will start with the clause that is the negation of the goal to be proved. and thenThen from the remaining clauses in our knowledge base we will select the clause that can resolve this. so quickly let us i rather suggest that you take a piece of pen and paper and start working that on so you will start Now start with this NOT smiling w and iI will try to resolve with this with which one should iI resolve? iI will select from these clauses the clause which has got complementary of the negation of this so obviously this clause clause two should be selected. i assume you are just working with a piece of paper andAnd if we resolve these two then what we get? theThe resolvent is NOT happy y. Bbut there is a ba big question here. we had standardized the variables you see inIn case of propositional logic the problem was not there but here since iI have got variables and iI have standardized the variables NOT smiling w and smiling y can be resolved. they can be resolved only if there is a possibility of substitution, there is a possible correct substitution, if iI can substitute y by w then only iI can unify these two, then only thise smiling y and smiling w they will become Q y. all right soSo, merely resolution will not do. andAnd what will be the result of the resolvent? willWill it be happy y? let’s just see i Here still suggest that you do not start copying this part you first concentrate on this part only the first level so iI have NOT smiling w and here iI had the clause two NOTot happy y OR smiling y and iI can resolve this only under a substitution where y substitutes w i am sorry w is substituting y. so this ones one  becomes happy w, smiling w and this smiling w and NOT smiling W resolve out and iI get NOTot happy w. all right  (Refer Slide Time 48:46)  

again here when iI get NOT happy w the next thing iI have to select is some clause from here. forFor example, here it is happy x so iI will try to unify them. Now for, this unification is very important. so hereHere iI selected this graduating x OR happy x so iI now replace this x with w all right so iI get happy w graduating w and I combine these two and iI get NOT graduating w. all right so hereHere you see iI had replaced for the sake of unification resolving for the sake of resolution iI substituted y with w and, here iI substituted x with w and there was no contradiction. andAnd iI came to NOT graduating w and there was a ca clause graduating name that is name one. soSo now this variable here this w is substituted with the constant name one. andAnd with this iI resolve what do iI get? I get a null clause. andAnd what does this null clause imply? itIt implies that the goal with which you started is contradictory. andAnd what is the goal with which iI startedstarted; it was is the negation of the goal to be proven.d soSo the contradiction of the goal to be proven has been disproved so the goal is proved. so iIsf someone smiling? Yes, someone is smiling. so you just quickly see what isSee the major difference between these two. It is the r propositional resolution resolutionesolution in propositional logic and resolution in predicate logic. tThe problem is, that here we have got variables so we have to take some extra pain in order to convert them to clausal form. weWe had to convert them intothe  clausal form in propositional case also but here we had to take a little extra pain. andAnd after that we have resolved them and in order to resolve them we had to unify, we had to give a proper substitution. and soSo this is a very powerful inference mechanism that is applied in many mechanical proving or theorem provingtheoretical systems and other applications. 
Here is a problem to solve. This next before concluding this lecture okay in between let me give you a quiz and you quickly copy that out solve the problem this is the problem that we have solved in the last lecture using this is the problem we solved in the last lecture using other inference mechanisms. but But the same problem you try to solve using resolution. okay so this is Tthe same problem is, if a perfect square is divisible by a prime p then is also divisible by the square of p. every Every perfect square is divisible by some prime, thirty six36 is a perfect square, does there exist a prime q such that the square of q divides thirty six36. So then the same thing (Refer Slide Time 52:01)  

you convert into the clausal form which you did in the last lecture and tried to answer it and try to obtain the solution.
Next, we will quickly look at some very interesting applications of this resolution theorem proving. answerAnswer extraction: let’s have a look at gGiven a set of clauses a knowledge base we want to find the answer of to a question. ifIf you were noticing, here also ultimately when you we got the null clause we could answer the question is someone smiling? we We got a null clause because we found that someone if now if iI had really asked who is smiling you say that you have proved that someone is smiling, but well who is smiling? theThe answer lies here because here this name one which was a ca constant at least he or she is smiling so that answer is there. butBut now iI want to mechanically have that answer at the leaf of my resolution tree. so hereHere is a nice problem: T we know that all packages suppose a robot is trying this example has been taken from the book of N. J. Nilsson. allAll packages in room twenty seven27 a particular room area particular room is smaller than those in room twenty eight28. roomRoom twenty eight28 can have some smaller packages also. butBut all packages in room twenty seven27 are smaller than those in room twenty eight28.  packagePackage A is either in room twenty seven27 or in room twenty eight28 the robot does not know. packagePackage B is in room twenty seven27. okay packagePackage B is not smaller than package A. now the question that is posed is, where is package A? nowNow how can we answer this questionquestion? let’s quickly see that now hereHere what we will do is we will just forget about this part for the time being. weWe will add another literal to the to each clause that will come from the negation of the theorem. Ffor example, here our negation of the goal soit says all packages in room twenty seven27 are smaller than those in room twenty eight28 can be converted in the clausal form in this way all right right  because all packages in room twenty 27seven  (Refer Slide Time 54:57)  

are smaller than those in room twenty eight28 so x is a P xP(x) AND P( y) AND int x 27 twenty seven that means package x is in twenty seven27, package y is in room twenty eight28 and x is smaller than y. Nnow, if iI  (Refer Slide Time 55:18)  

convert them in clausal form iI get this: A is a package B is a package A is in room twenty seven27 OR A is in room twenty eight that was the problem here. packagePackage A is in either in room twenty seven27 or twenty eight28 and package B is in room twenty seven27. and And package B is not smaller than package A. Nnow my question is in which room is A. that is, iI have to prove that there exists a some room number there exists u such that in I standing for in A u, A is in u. so that is my goal query. soSo iI have to negate it for resolution. all right only thing in order to get this look at this i will just go a little slow soSo the negation of the clause is this is as straight away taken from Nilsson’s book: goal those of who got this can look into this NOT of A, u and only thing that iI have to added it is for this u iI have got an answer literal. let’s see what happens aAs iI go on resolving this obviously iI will select a ca clause which can resolve this out so iI take this and iI can resolve it with the substitution of twenty seven27 for u all right otherwise this is not substituted. soSo u is substituted with twenty seven27 and iI get the resolvent A twenty eight my resolvent becomes this. andAnd with this now iI look for a particular clause which can resolve this out. iI take the first clause and as iI resolve this iI resolve it with a particular substitution that is y is replaced with A and based on that when iI go on doing it iI am remembering the resolvent remembering the substitution and in this way as iI come here i get iI substitute this and ultimately when iI get this contradiction my this leaf node will come to twenty seven27 and that is my answeanswer.r just see that Iin the normal resolution (Refer Slide Time 57:55)  

because case this part was not there, if it was there I this part was not there i h would have started with this and ultimately iI would have come to an null clause here. butBut if iI start with this if i start with adding with the negation of the goal clause negation of the goala clausal form of that or iI put this answer of that literal u in which iI want to hold the answer. thenThen the substitution values will automatically put me in the resolve that what iI want and that will come as a leaf node. isn’t that interesting so theSo the same resolution method can be applied to carry this out.

 soSo in this lecture we have seen that we can apply the resolution refutation method for the case of predicate logic with some additional techniques that we apply that we haveas discussed namely the clausal form, the unification part and also we can apply this to find out answers to some questions by applying it in aan intelligent manner. thank you    
        
Artificial Intelligence
Prof. Anupam Basu

Lecture 16

Rule Based Systems
(57:17)

todayToday we will be discussing about specific practical approach to implementation of systems which behave intelligently. earlierEarlier we had discussed about one form of knowledge representation and that is logic. weWe have also seen how inferencing can be carried out in logic. todayToday we will be discussing our discussion will focus on rule based systems which is a very practical implementation of intelligent behavior. ofOf course it derives from logic. weWe will see how it is depended on logic and how it is also some sort of simplification to make the systems implement under engineering terms. we We have seen well formed formulae in the case of propositional logic (Refer Slide Time 02:01)  

and first order logic. nowNow these essentially represent assertional knowledge or they represent assertions. whatWhat are assertions? whenWhen ever iI make a statement; today is Sunday, yesterday was very hot, these are some assertions that iI am making and these statements as we know from our knowledge of logic can either be true or can be false. weWe can say that these can be evaluated to true or can be evaluated to false these are assertions. andAnd we have seen that propositional well formed formulae or predicate well formed formulae all evaluate to either true or false. nowNow, if we look at theise assertions in little more detail we can see that thisese can be decomposed or thisese can be categorized into two different forms. oneOne is rules another is facts. now rulesRules are assertions given in implicational form. An implication for example is, i hope all of you remember what is meant by implication p implies q okay that’sis an implication and rules are usually given in that form. we will soon see how the rules look like thereThere is another set of assertions which we call facts which represent domain specific knowledge. say For example,  if the boy the boy is intelligent just we say the boy is intelligent that i’s an assertion, it may be a true statement or it might be a false statement. butBut when we assume that it i’s a fact then we usually assume that is true the boy is intelligent. butBut if you say that the boy if the boy is intelligent if the boy is intelligent then the boy will score good marks then that is a rule we are saying and this implication is being noted by this construct if then. soSo we can a have complete knowledge system which consists of two distinct components. oneOne is rules another is facts. weWe can have a set of facts which are presented from the domain of this course. forFor example, let u’s consider the domain of geometry school geometry. weWe can say A B C is a triangle. weWe can also say A B C is aan isosceles triangle. A B and A C are two sides of the triangle. nowNow all these statements that iI have made are nothing but assertions but there is no implication in that. iI have just said A B and A C are two sides of the triangle. iI have said A B C is a triangle or A B C is an isosceles triangle. allAll these are facts. Nnow if iI say, if A B C is a isosceles triangle then the sides are equal if A B C is a isosceles triangle then the sides two sides are equal now this is a rule which iI am stating. andAnd based on this rule using this implication that is inherent in this rule iI can infer new facts. okay so lets go a little further now
 inIn logic we represent knowledge in declarative and static way. okay weWe just say we say we make statements which are declarative. andAnd rules in logic say what is true given some conditions. forFor example, when iI say if A B C is an isosceles triangle then sides A B and A C are equal. now thereforeTherefore using this rule if iI know that A B C is ind need an isosceles triangle then iI can infer the new fact that A B and A C are equal. all right soSo rules in logic are stating are or saying (Refer Slide Time 06:41)  

what is true given some conditions. whatWhat is the condition? theThe condition is, that A B C is an isosceles triangle and what am iI saying using this implication? i am saying thTheat rule is implicitly saying therefore A B and A C are equal. 
nowNow rule based systems (7:03) to (7:17) are based on rules these are just implications implicational statement and in logic. nowNow in case of rule based systems we go a little one step farther. ourOur implicationsimplication that means the right hand side of the implication, usually in logic makes another assertion. okay ifIf the boy studies hard then he will score good marks is an assertion. butBut iI am not asking anything to be done. onOn the other hand look at the statement: if the temperature of a plan of a furnace is very high then shut the heater off. look here in the implication iI am not just making an assertion iI am also asking for some actions shut the heater off so that is also possible to be stated in a rule. soSo rule based systems are based on rules that say what to do. okay now hereHere iI would like to make a distinction between two types of knowledge representation. oneOne type is declarative knowledge representation. thatThat means where as we just said what is to be done but we do not say how the thing has to be done. wWhen ever we write a program say for example sorting a number okay or to write a program to draw a diagram there in that program itself we specifically mention all the steps that we need to carry out in order to perform a particular task. thereAnd it is only there we specify how exactly the thing has to be done. butBut in a declarative system as in a logiclogic we make some assertions okay we make some assertions, we make some statements. nowNow, which statement has to be executed at which point of time is not the headache of these rules. thatThat headache will be transferred to another thing another system called the control mechanism or the inference mechanism. That i’s why often thisese sort of knowledge generation is also called declarative knowledge representation whereas the type of knowledge that we are embedding in the piece case of a program is often called procedural knowledge representation. howHow ever in this case you can see that rule based systems can be of this form if this is the case that means is the condition then do this. thisThis is one way of statement. It i’s also possible to state in this way, that do this. all right but (Refer Slide Time 10:27)  

But we can also make some assertions over here and as i said just now that these rules are just some declarations which do not say how the thing has to be done. weWe require a special interpreter a special interpreter which i just now said as a control mechanism to know the procedure.okay thereThere are different names such as control mechanism, inference engine, inference machine etc different names but the purpose is the same. That’sThis is the procedural block of knowledge that that interprets these rules that and applies these rules as the case may be. nowNow we can have very simple rules which are very similar to the rules in logic. but we can think of now whatWhat can a rule do? aA particular rule has got if you recall a special rule has got the left hand side part which we call the antecedent of the conditional part if this is the case. ifIf it is really true that this is the case that is if it is really true that this is the case then on the other side of the implication some actions may be stated like shut off the heater, open the window, close the tap okay or we can infer we can make some assertions such as the boy is intelligent, the season is monsoon etc.okay  Cconsider rules like if it is raining everyday then possibly the season is monsoon, all right  or if it is monsoon then there is high chance that there will be rain today. nowNow the condition is, if the season is monsoon and supposedly my fact is my fact is it is really the month of July which is the season of monsoon so the fact is true. so given that gGiven this fact to be proved we can infer a new fact that possibly it will rain today that i’s another fact but that we are inferring but we are not stating any action as we had done if the right hand side where shut off the heater, open the tap etc.etera soSo rule the consequent of a rule can be either an action or can be a new fact. ifIf it is be an action that action is taken by the interpreter that is actually executed by the interpreter and if it is a fact then that fact is added to the fact base of the rule based system. all right also Also another important thing is that, along with the fact degrees of certaintiesy can be associated. just now in the course of discussion i haveAs we saw the stated a  rule, if the season is monsoon then it is possible that it will rain today. iI can make a the same statement in different ways. the same statement  ifIf the season is monsoon then there is sixty percent60% chance of raining today. iI can also say, if the season is monsoon then it i’s highly probable that the that it will rain today. nowNow these are you see that it will rain today is not coming as it is but iI am also adding some possibilityies some certaintyies with this statement. soSo when iI infer a new fact what is the new fact that iI am inferring? given Given that this season is monsoon my fact that will be inferred will be possibly it will rain today, it’ is sixty percent60% probable that it will rain today etcetera etcetera. soSo it’ is possible that iI can associate some sort of uncertainties with these rules. nowNow let’ us again think back again what happened to logic. inIn simple propositional logic or predicate logic we had an assertion which would either be evaluated to  true or false. theseThese were the two possibilities there was nothing in between. butBut here along with these facts iI can add on some certainties. in a separate lecture laterLater on we will see how we can in intelligent system we will see how we can handle such concepts. 
so we just Ffor the time being remember that in the case of rules or rule based system when we infer a fact this can be a hard fact which can be either true or false or it can be a fact which is associated with some sort of certainty, some degree of belief or disbelief what ever it is. okay soSo it i’s no longer a black and white decision there can be a grey region in between. nowNow we can have also have different control strategies which are often heuristic. weWe often need to take decisions in real world where we do not have all the possible information that youwe need. oftenOften we have got very partial information about scenario. supposeSuppose you want to travel from a place A to place B you know some facts that there is a bus route from A to B and there is also a train railways connecting A to B, you do not know exactly what the faresears are. butBut if you really need to save money then probably you will select bus travel because your other domain knowledge in general tells you that bus travel is often cheaper than train travel. It may not be always true but let’ us assume that there is a scenario as such. so So you have got some partial information and based on this partial information and the other domain specific knowledge that you might have got through some other means or general knowledge that we have there are some thumb rules of making decisions. andAnd we often use those to take the decisions. andAnd we can say well probably bus will be a cheaper mode of travel so going from A to B for low cost bus would better way of going. nowNow theise sort of decisions that we often take make do not mayneed not always have very strong logical basesplaces but these are often very much practical and we often take such decisions. This particular scenario is also called as all right so these are will encounter this scenario time and again the term for this is heuristic all right and such heuristics we will be employing for rule based systems and we will also see for  as well ddifferent control schemes. we will see (Refer Slide Time 18:08)  

now let’s see here is an example of a rule based system. ifIf it rains today roads will be wet. thisThis is clearly understood when wee can write in English. We can write this in the form of logic as rains which is a predicate, rains today implies implication, wet road today or in the form of a rule we can say if rains today then wet road today. You can see then you see the correspondence between this logic statement and this rule. all right againAgain there is one rule there is another rule that i Ihave added here; if rains today and not covered roads that is if the roads are not covered then wet road today. nowNow look at the difference between these two rules. thisThis rule is obviously the mostmore specific because it says that the road will be wet today if it rains today and the roads are not covered. whereasWhereas this is a statement which is much weaker if rains today then wet road today. okay nowNow look at the antecedent part of this rule; rains today is an antecedent and wet road today is a consequent. all right inIn this case we have got a number of antecedents rains today and not covered roads these are two antecedents and these two together we call the antecedent p field or simply we call them antecedent. 
Nnow what happens to this rule if the fact is that rains today, that means if rains today evaluates to true then we will conclude that the roads will be wet today, this part okay wet road today. onOn the other hand when will we infer wet road today? weWe will infer wet road today if both these conjuncts of this antecedent evaluates to be true. inIn our discussion iI will often refer to base to be the antecedent part orf the antecedent field and each of them are individual antecedents. nowNow when does this rule will really work? thisThis rule will be active when both these antecedents since they are connected with an AND are both true. suppose the fact is, it rains today and the roads are covered today, in that case according to this rule this part will be true but this part will not be true because the roads are covered then iI cannot really infer that the road will be wet today. okay soSo in a rule we have got two parts one is the antecedent part another is the consequent part. theThe antecedent part can consist of one fact or can be a conjunct of facts or can beanti conditions. okay and lookLook at this consequent, i was just now talking of there are  two different types of consequents; some things can be actions and some thing can be assertion. wetWet road today i’s simply an assertion that is adding to my collection of facts and since iI know it rainss today and not covered today iI know one more fact that the road is wet today. okay Mmight be there is another rule, if it is wet road today then where wouldwear good shoes or something or of this sort so that i can so this fact can be used for something morefurther more. Now which are the now which are the facts here i have already said that facts here? Facts a are which are already known a priorprioryy but you can see that facts are also generated just as in this case in this example wet road today the is a new fact that has been generated. all right 
soSo now (Refer Slide Time 22:25)  

i hope all of you recall what is meant by let us get back to modus Modus ponens which was a very common deduction mechanism that we talked about when we were talking about logical inferencing. hereHere we just revisit modus Modus ponens. for a sake let’s see h Here you see P implies Q and P so P implies Q is an implication and P is a fact that is given. So, given these two using modus Modus ponens what can we infer? weWe infer Q. that means if P is true then Q is true and since P is true we infer Q is true. nowNow suppose there is another rule; Q implies R. Tthat means if Q is true then R is true. Nnow here from these rules two we have inferred Q and again from theise tworule we can infer R. soTherefore you can see that if we consider this to be a rule and this to be a fact then we can infer an fact and using this fact and another rule which rule the rule whichis the rule that has got this one you see this fact Q has it’s antecedent we can infer another fact R. so you can see therefore starting from this fact given fact P and these two rules P implies Q and Q implies R iI can first infer Q and then R. sSo this sort of P this rule is allowing to infer Q and inferring Q is allowing me to infer R so this phenomenon is also known as rule chaining where we are inferring a fact using a rule new fact using a rule and with the newly inferred rule i am sorry with the newly inferred fact iI am applying another rule and inferring a new fact and the chain can go on.. okay soSo let u’s now come to rule based systems once again. (Refer Slide Time 24:33)  

rule based systems are also known as production systems. thoseThose of you who have worked on compiler we you have seen that often grammars are specified in the form called abacus Backus normal form which is also a set of productions. you i leave it to you to findFind the similarity between those statements which are given in abacus Backus normal form and the type of rules that we will be discussing. certainlyCertainly there are quite a few similarities. now you see aA rule based system which is also known as production system is a system whose knowledge base is represented as a set of rules and facts. soSo we were talking about knowledge representation earlier. logicLogic was one form of knowledge representation. weWe will see different other types of knowledge representation as well. nowNow rule based system is therefore one type of knowledge representation where the knowledge is represented as a set of rules and a set of facts. weWe have already seen this, a rule based system consists of a collection of rules which are in if then form and a collection of facts. and Nnow with thiese rules and facts we would not be able to infer anything unless there is another interpreter or control engine what ever name you give it inference mechanism which will look at theise facts and see which rule is applicable now and apply that particular rule and generate a new fact because in my rule base iI will have thousands of rules. Now, given a particular objective a particular problem that iI want to solve all these rules may not be relevant. 
so Therefore it is essential that iI have to find out only those rules which are relevant at this particular point of application. soSo, that task is done by the interpreter. andAnd in today’s lecture we will see how the interpreter applies these rules, how the interpreter selects these rules and what does this interpreter really do to infer new facts and thereby solve some problems. okay now as i say theThe rules (Refer Slide Time 27:12)  

are represented in the following form; if antecedent then consequent, and antecedent can be a conjunction of number of antecedents. okay when When the antecedent part is NULL it i’s also possible that iI will just have then this or i just have a consequent. ifIf the antecedent part is null then obviously it becomes a fact that i’s a true fact.  fact inIn general we will assume that a fact is true. okay nowNow rule says that if these conditions are true then iI can infer a new fact. butBut if no conditions are given then that consequent stands on it’s own merit on it’s own right so it i’s true, it is fact. all right some of you might recall if you have written those of you who have written c programs Structures like while one, while two those sets which isis always true. So, similarly if there is no condition attached to a rule then it becomeis a mere fact. all right nowNow here is a statement that i am making; rules are normally represented as horn clause. thereThere is reason for this. Before being introduced to horn clause let us recall what a clause or a clausal form is. i will be putting horn clause but before that i assume that you recall what a clause is clausal form aA clausal form consists of disjunction of literals. or disjunction of literals forFor example, p OR NOT q OR r that i’s a clause or all right that’s a particular form. andAnd implications can also be converted to clausal form. hornHorn clause is a particular form of such clauses and which it is referreferred that rule based systems based on horn clauses. see what it is l Let us look at this implication; P implies Q we will recollect now the clausal form of this we will recollect iswhich is NOT P OR Q because this implication just for the sake of revision it means if P is true then Q is true. all right nowNow if P is true then Q is true otherwise P is not true so NOT P OR Q this is thein clausal form. similarlySimilarly P AND Q implies R is a rule form. iI can convert to the clausal form NOT P OR NOT Q OR R is a clausal form. nowNow look at this; P AND Q implies R OR S. if i convert to the clausal form (Refer Slide Time 30:06)  

If I convert to the clausal form it becomes NOT P OR NOT Q OR R OR S. now  thisThis is not a horn clause, what is a horn clause? If we call anything a horn clause the first condition it must satisfy is that it should be a clausefirst let’s look at that horn clauses first of all they must be clauses that’s why i have underlined , it there must be clauses. hornHorn clauses can have at most one non negative literal. at most one non negative literal look over here hereHere Q is a non negative literal, here R is a non negative literal, there is only one non negative literal but in this case we have got two non negative literals so this is not a horn clause. soSo these are acceptable rules but this is not typically in a regular systemrule based system. It i’s not the case that you cannot write rules likeat this but usually it’ is not encouraged and the reason is there are some severe consequences in to this. but i am not going into that in this lecture but still let’s try to see ifIf iI use this particular form which is not a horn clause because it has got more than one non negative literal what is the problem? supposeSuppose my fact base says that the person is rich that is P and the person is large hearted okay so P AND Q. Tthen the person will make donateion OR the person will build up a school or what ever. soSo there are two possible implications that we can take. Nnow, in the case of an implementation or execution the interpreter will really be in a problem to select which one of these consequents to take, should it infer both because iI do not know whether both of them are true. thisThis part will be true if any one is true or should it infer anyone? ifIf it has to infer anyone then which one should it inferinfer? all right soSo those problems really crop up but those problems that are absent in these cases P implies Q or P AND Q implies R so we usually stick to horn clauses. thatThat means in the consequent part we would do not [    ] 32:49. okay nowNow there are two more terms which you we should first understand. try to imagine i mean the rule forFor a rule base system we have got a rule base and we have got a set of facts which we give in a fact base. all right nowNow theise rules are evaluated to be true. now whenWhen is the a rule applicable? aA rule is applicable when all it’s conditions evaluate to true. ifIf any of the conditions evaluate to false then that is not applicable. nowNow it may be the situation that given a set of facts and the set of rules more than one rules can have their conditions satisfied. soTherefore more than one rule is are applicable.  recall that theseThese rules are triggered and out of these triggered rules that means those rules have all been enabled to be fired. Now, when we apply a particular rule we say that that rule is fired. andAnd what happens when a rule is fired? whenWhen a rule is fired then the consequent part as to whatever they is specified in the consequent is inferred or that action is taken. So, if it is inferred if that new fact is inferred then this fact is added to the fact base. if a new ifIf the rule just specifies some action and if that action is taken then that actionthat action creates some state change in state. Ffor example, if as iI was giving the example put off the heater then the heater is put off so the state changes wherein  is that earlier the heater was on and iI changed that fact iI delete that fact heater is on and added a new fact heater is off. So, in either case there is a change in the fact base when a rule is fired. butBut it’ is not the case that all the rules which are enabled are fired. iI can either fire any one of them, if iI have to fire any one of them or to make a decision as to which one should iI fire so all these things are there. butBut first of all let u’s make a decision that a rule is triggered when all it’s antecedents evaluate to true and the rule is fired when the actions stated in the consequent part or the inference related to the consequent part is inferred or is taken. 
(Refer Slide Time 35:38)       

okay Thereforeso we will make a distinction between these two. nowNow look at this architecture. thisThis is an overall architecture of a rule based system. youYou can see this big rectangle this is known asand you forget about the rules which are written over here. forget about this for the time being these rules aAll these rules that are here constitute the rule base whereas we can have a number of facts stored and that is known as the fact base and here there is a procedural component which can as i said which can have different names control scheme, interpreter, inference machine, control engine, etc different rules different names which are given. nowNow you see look at this arrow, what does this control scheme do or interpreter do? It is looking at the facts and looking at the rules. andAnd is trying to see given the particular set of facts which are the rules which are applicable which are triggered. Now, out of the rules which are triggered it will fire one or more than one (Refer Slide Time 37:07)  

and consequently when the rule is fired a new fact will be generated and that will update this fact base. soSo, at the top level we look at cthree components in a rule based system namely  thethe rule base, the fact base and an inference machine. okay theseThese are the three basic components of rule based system. nextNext let’ us look into this rulesthese rules. hereHere the rule is, if hot and smoky if hot and smoky then add fire. ruleRule two says, if alarm beeps then add smoky. ruleRule three says, if fire then add switch on sprinkler. so So here we are talking of a toy problem where we hasve got an intelligent system installed rule based intelligent system installed which has got some sensors put on in the room and its sensing temperatures it is trying to see whether the temperature becomes very hot, and there are it’s also the smoke detectors so it is trying to see whether there is any fire and if it infers that there is fire it will automatically start on sprinkler or it will set up an alarm which will make people run and start switch on the sprinklers. Therefore  on so it is that sort of a scenario we have here. soSo we have a model with toy problem modeled with these three rules. andAnd what is there in the fact base? Alarm, beeps and hot so there are two facts. oneOne is alarm beeps and hot. now let’s if sinceSince the alarm beeps what does this control scheme what will this control scheme dothis control scheme does? itIt will look at this rulesthese rules. nowNow which rule is enabled? iI can see also hot, all right so this rule have got this antecedent satisfied if hot, yes, true so this is satisfied. andAnd smoky, but well i I do not know anything about smoky in my database of facts so iI cannot say that it this smoky so this part is true. hotHot is true but this smoky is false should therefore this antecedent part of this rule is not completely satisfied. right is not completely satisfied so Therefore this rule does not apply. next so it looks at so it cannotSo this rule is not triggered and it comes to the second rule. ifIf alarm beeps, and well looks at in the fact base alarm beeps is there so this part is true so this rule is obviously enabled because it has got no other antecedent. thisThis entire antecedent field this antecedent part is satisfied. well let’s look at theThe third rule is, if fire, now iI do not know anything about fire here and nothing is here so this antecedent is also not true so this rule is also not triggered. inIn this case we have got only one rule that is triggered that i’s rule R2 two.
so So the control scheme then finds that there are onlyonly R2 two is triggered so it has got no choice and it fires that. andAnd what does this what is the consequent part of this rule R2? two theThe consequent part says, if alarm beeps then add smoky. soSo it immediately adds when it fires it immediately adds to the database the new fact smoky. andAnd you can well guess what will happen next. Next the control scheme will next look at again look at which rules which are being enabled as fresh. hotHot is true so this part is true and now smoky is also true. sinceSince smoky is true this entire antecedent part is true. it is then it will okay this rule is R2 two  is also true but R2 two is already fired so we are taking up the strategy that iI will not fire the rule one rule time and again, it tries for the third rule third fire but i cannot iI have got nothing in the database as yet which tells that there is a fire. Therefore so only this rule is enabled this rule is fired and what is the consequent of this rule? It is add fire, so a new fact is added to the fact base which is says fire. soSo what am iI inferring? iI am inferring yes that there is a fire. nowNow iI again go back to this cycle and look at this third rule, the third rule is now enabled if fire then add switch on sprinkler. so nowNow this third rule will be fired and sprinkler on will be put in the database. So so e essentially this is the way in which a rule based system will work. Now, if you have noted this example very clearly then you will realize that this can be modeled in the form of an automataof automata of inferencing. okay whatWhat is theat automata? thereThere are three faces phases, nowin the first face what are the components that iI had have? iI have got a set of rules, iI have got a set of facts now what are the other components and the inference machine is also there. soSo what is happening? firstFirst the inference machine is looking at the fact base and the rule base. iI am trying to find a match as to which of the rules are predicatestriggered? ifIf we it finds that more than one rule is triggered then it will have to select some of the rules may be one rule or may be a subset of these rules. and after that let’s Ffor the sake of simplicity for the time being let u’s assume that we are selecting only one rule and that rule is fired that i’s executed. andAnd when that rule is executed what’s happening a new fact is generated in the fact base. soSo we can essentially carry it on in the form of cycle. soSo here is what is depicted, (Refer Slide Time 43:50)                

the inference machine or the interpreter okay is a machine that implements the strategies to utilize the knowledge base and derive new conclusions from it. nowNow how it is being done? hereHere is the automataare the automata of an inference machine. thereThere are three phases. firstFirst is the match phase then there is a conflict resolution phase and then there is a execute phase. nowNow in the match phase the inference machine is comparing the fact base and the rule base and it will find out some rules which are matching. ifIf no rules are matching then we canno’t proceed any further. butBut suppose there is a set of rules which match then those set of rules are being passed on will be used by the next stage which is the conflict resolution stage. whereWhere is the conflict? theThe conflict is among the rules. thereThere are five rules which are triggered in the match phase. nowNow which of these rules iI shall be firing? forFor that iI need to have some strategy which is often called the heuristic strategy. okay weWe often use heuristics but using heuristic what we do is the conflict resolution strategy. iI will be selecting one rule or more than one rule. okay which ofOf thise subset of match rules which part of the subset iI will be executing that is decided in the conflict resolution phase. 
soSo we come to the conflict resolution phase and after the conflict resolution phase we decide on which rules to find. andAnd then that in with that in formation we will go to the next statge which is known as the execute state. Iin the execute state those rules are fired and when the rule is fired, see  here in the match phase we passed on only the triggered rules to the conflict resolution phase. nowNow in the conflict resolution statge we have resolved the conflicts and we have selected some rules to be fired. and when the rules are fired then the firing causes a change in the again the new fact in the new fact base all rightthe new fact scenario.  after that afterAfter this execution you will again come to the match phase. butBut how long will it continue? so We will come to the match phase and in the match phase again the matching will be done and we will see which rule is enabled again, we will do conflict resolution, and we will execute and this cycle will continue. nowNow how long will this cycle continue? thisThis cycle will continue until the goal or the objective with which we are trying to solve the problem. theThe problem that we have, we are trying to solve that and that we call a goal, and as long as the goal is not met we should try to do that this. butBut it may be that we have not been able to solve the goal and we do not have enough rules at our hand. see rulesRules are essentially representation of knowledge and knowledge is always limited. andAnd often there are situations when we the human beings fail to solve a problem because of lack of knowledge okaymay be at times.  sSo, in those cases we try till we exhaust all the knowledge components that we have so we will carry it on but if we solve a particular problem then we will stop otherwise we will exhaust our knowledge source and then stop. soSo this scenario will this cycle will continue and after every execution phase it will go to theis rule base and after every executione it will check for the goal whether the goal is met. ifIf the goal is met then we will stop otherwise we will carry on. so thisThis is a very fundamental cycle of rule based system. okay so hereHere is a summary; Tthe execute state fires the rules once all its antecedents match okay and essentially the function of the execute state can be thought of as searching through a path througho  the goal in a search space. hereHere iI have added a new name search space. (Refer Slide Time 48:37)  

okay so So what iI am trying to take here is that, through these rules we are essentially trying to depict all the possible ways in which a problem can be solved. ourOur objective is to solve a particular problem that is as a goal okay and in order to do that there may be different alternative paths all the paths may not lead to the goal but there are paths which may lead me to the goal. soSo, effectively our task is to search through this entire space which iI am calling the search space. nowNow here is a search space node. typicallyTypically a search space that is denoted by these states. in thisThis state that iI am showing is the start state say that this start state and iI have got a goal state also here. orangeOrange node is the goal state and this the root node of this is the start state. nowNow the start state and what do iI have at the start state? iI have got my rules 
 (Refer Slide Time 50:04)       
and iI know about the goal and right now this goal is not solved okay so iI start my rule based system here. andAnd with the facts that i have and the rules that iI have iI try to see which of the rules are applicable which are triggered. supposeSuppose at this point three rules are triggered, I have just named only two of them; Oone is R1 one and the other is  one is R5 five see but iI am denoting using this tree structure theseis set of rules using this tree structure. okay so i can take now i can seeNow what this one shows is that this state shows is that the result of my match which areas shown here that this match has resulted into three rules which hashave been passed to this conflict resolution module. andAnd in that i have suppose iI have a i have resolved in favor of R1 one so iI can come to R1 one and when i fire R one a particular rule R1 one iI will be adding new facts to the rule so iI continuously refer to this diagram. (Refer Slide Time 43:50) so iI have resolved in favor of R1 one and there has been executed and new facts have has been new facts has been added to the fact base okay and that might have enabled the me with two other rules and iI can select any one out of them. supposeSuppose iI have selected this one that and fired this rule so here again there was a conflict. atAt this state again a match has been done and two rules have succeeded and these are triggered given this new state. andAnd iI have fired this rule okay and as iI have come to this new state. And as I come to this new state and if iI fire this rule iI will come to the goal node. i will come to the goal node so this is butBut what are my possibilities? My possibilities are that iI could have followed this path, iI could have followed this path, please follow the mouse pointer this path or my Bmight be at this conflict resolution strategy iI could have followed another path. supposeSuppose at this point after firing R1 one i I come to a new state and iI fire a new rule and iI fire this rule and come to a state from that where iI do not find any other rule to fire then i cannot what do i do iI cannot do anything. let’s see butBut iI think iI can do something. supposeSuppose at this point R1 one suppose this is R2 two and R5 five all these three were enabled but iI have selected in favor of R5 five and I have come to this point. andAnd when iI come to this node i after match iI do not find any rule that is enabled so iI cannot proceed any further and my goal iI cannot reach my goal. butBut what can iI do? what can i do what What iI can do is this, from here iI can  (Refer Slide Time 53:21)         

move go back, since this is the a dead end and iI am not finding any other rules to fired I i can go back to the earlier node to see which are the other possible rules that has been left unexplored. forFor example, iI came to this node but iI found i resolved the conflict in favor of R5 five and iI have come to a dead end so iI go back to starting node and see well what were the other choices that iI could explore. ifIf iI really remember that yes there were other alternative paths like R1 one, R2 two, etcetera and explore these paths then possibly iI could find the path to the goal. actuallyActually this is what we do in our real life and this phenomenon is known as backtracking. weWe often make wrong choices but when we find that this wrong choice is leading me to a dead end we revise our earlier decision and this is known as backtracking. andAnd what iI have depicted over here is the search space and how the rules really traverse the possible search space and my success will rely onreally lie on how fast iI can arrive at thea goal from starting state. so So what we have just now (Refer Slide Time 54:46)  
learnted  is that is the phenomenon backtracking. ifIf the inference machine reaches a dead end that is no new rule is enabled and the goal is not met. Iif the goal is met then iI am successful, if the goal is not met then we will backtrack to the earlier node as we have done here we will backtrack to the earlier node and see whether there are some unexplored possibilities and we will be exploring that. okay and sSo backtracking is a very important methodology of exploring the search space because it i’s often not possible to really make the correct choice all the time it may not be always possible so we will select the rules which are not proper given a particular context. and And obviously conflict resolution strategy plays a crucial role over here if we have a good conflict resolution strategy then obviously the backtracking will be reduced. ifIf you have good conflict resolution strategy then we will be reducing the backtracking. now Now, in this lecture we conclude at this point where iI have just tried to give you an overview of what rule based systems are and how they work. in the next lecture we will look at other aspects, we will look into more examples, so we will look into more details of inference mechanism. butBut iI hope that you understood how a set of rules really help us in solving the problem. youYou should remember a couple of facts namely the distinction between rules and facts, the distinction between match phase conflict resolution strategy and execution strategy. we You must understand what is an inference mechanism? anAn inference mechanism actually selects the rules to be fired given a set of facts. soSo given all these we will move on to the next lecture where we will be going into more details of rule basesd systems.    

Artificial Intelligence
Prof. Anupam Basu

Lecture 17

Rule Based Systems
(01:01:15)

Today we will be discussing about specific practical approach to implementation of systems which behave intelligently. Earlier we discussed about one form of knowledge representation and that is logic. We have also seen how inferencing can be carried out in logic. Today our discussion will focus on Rule Based Systems which is a very practical implementation of intelligent behavior. 
Of course it derives from logic. We will see how it is depended on logic and it is also some sort of simplification to make the systems implement under engineering terms. We have seen well formed formulae in the case of propositional logic (Refer Slide Time 02:01)  

and first order logic. Now these essentially represent assertional knowledge or they represent assertions. What are assertions? Whenever I make a statement; today is Sunday, yesterday was very hot, these are some assertions I am making and these statements as we know from our knowledge of logic can either be true or can be false. We can say that these can be evaluated to true or can be evaluated to false these are assertions. And we have seen that propositional well formed formulae or predicate well formed formulae all evaluate to either true or false. Now, if we look at these assertions in little more detail we can see that this can be decomposed or this can be categorized into two different forms. One is rules another is facts. Rules are assertions given in implicational form. An implication for example is, p implies q is an implication and rules are usually given in that form. There is another set of assertions which we call facts which represent domain specific knowledge. For example, the boy is intelligent just we say the boy is intelligent that is an assertion, it may be a true statement or it might be a false statement. But when we assume that it is a fact then we usually assume that is true the boy is intelligent. But if you say if the boy is intelligent then the boy will score good marks then that is a rule we are saying and this implication is being noted by this construct if then. So we can a have complete knowledge system which consists of two distinct components. One is rules another is facts. We can have a set of facts which are presented from the domain of this course. For example, let us consider the domain of school geometry. We can say ABC is a triangle. We can also say ABC is an isosceles triangle. AB and AC are two sides of the triangle. Now all these statements I have made are nothing but assertions but there is no implication in that. I have just said A B and AC are two sides of the triangle. I have said ABC is a triangle or ABC is an isosceles triangle. All these are facts. Now if I say, if ABC is a isosceles triangle then the sides two sides are equal now this is a rule which I am stating. And based on this rule using this implication that is inherent in this rule I can infer new facts.
In logic we represent knowledge in declarative and static way. We just say we make statements which are declarative. And rules in logic say what is true given some conditions. For example, when I say if ABC is an isosceles triangle then sides A B and A C are equal. Therefore using this rule if I know that ABC is indeed an isosceles triangle then I can infer the new fact that AB and AC are equal. So rules in logic are stating or saying (Refer Slide Time 06:41)  

what is true given some conditions. What is the condition? The condition is, A B C is an isosceles triangle and what am I saying using this implication? The rule is implicitly saying therefore AB and AC are equal. 
Now Rule Based Systems (7:03) to (7:17) are based on rules these are just implicational statement in logic. Now in case of Rule Based Systems we go a little farther. Our implication that means the right hand side of the implication, usually in logic makes another assertion. If the boy studies hard then he will score good marks is an assertion. But I am not asking anything to be done. On the other hand look at the statement: if the temperature of a furnace is very high then shut the heater off. here in the implication I am not just making an assertion I am also asking for some actions shut the heater off so that is also possible to be stated in a rule. So Rule Based Systems are based on rules that say what to do. Here I would like to make a distinction between two types of knowledge representation. One type is declarative knowledge representation. That means as we just said what is to be done but we do not say how the thing has to be done. Whenever we write a program for example sorting a number or write a program to draw a diagram in that program itself we specifically mention all the steps we need to carry out in order to perform a particular task. And it is only there we specify how exactly the thing has to be done. But in a declarative system as in logic we make some assertions, we make some statements. Now, which statement has to be executed at which point of time is not the headache of these rules. That headache will be transferred to another system called the control mechanism or the inference mechanism. That is why often this sort of knowledge generation is also called declarative knowledge representation whereas the type of knowledge that we are embedding in the case of a program is often called procedural knowledge representation. However in this case you can see that Rule Based Systems can be of this form if this is the case that means the condition then do this. This is one way of statement. It is also possible to state in this way, do this.  (Refer Slide Time 10:27)  

But we can also make some assertions over here and these rules are just some declarations which do not say how the thing has to be done. We require a special interpreter as a control mechanism to know the procedure. There are different names such as control mechanism, inference engine, inference machine etc but the purpose is the same. This is the procedural block of knowledge that interprets and applies these rules as the case may be. Now we can have very simple rules which are very similar to the rules in logic. What can a rule do? A particular rule has got if you recall has got the left hand side part which we call the antecedent of the conditional part if this is the case. If it is really true that this is the case then on the other side of the implication some actions may be stated like shut off the heater, open the window, close the tap or we can make some assertions such as the boy is intelligent, the season is monsoon etc. Consider rules like if it is raining everyday then possibly the season is monsoon, or if it is monsoon then there is high chance that there will be rain today. Now the condition is, if the season is monsoon and supposedly my fact is it is really the month of July which is the season of monsoon so the fact is true. 

Given this fact to be proved we can infer a new fact that possibly it will rain today that is another fact we are inferring but we are not stating any action as we had done if the right hand side were shut off the heater, open the tap etc. So the consequent of a rule can be either an action or can be a new fact. If it be an action that action is actually executed by the interpreter and if it is a fact then that fact is added to the fact base of the rule based system. Also another important thing is that, along with the fact degrees of certainties can be associated. As we saw the rule, if the season is monsoon then it is possible that it will rain today. I can make the same statement in different ways. If the season is monsoon then there is 60% chance of raining today. I can also say, if the season is monsoon then it is highly probable that it will rain today. Now it will rain today is not coming as it is but I am also adding some possibilities some certainties with this statement. So when I infer a new fact what is the new fact that I am inferring? Given that this season is monsoon my fact inferred will be possibly it will rain today, it is 60% probable that it will rain today etc. So it is possible that I can associate some sort of uncertainties with these rules. Now let us again think back again what happened to logic. In simple propositional logic or predicate logic we had an assertion which would either be evaluated to true or false. These were the two possibilities there was nothing in between. But here along with these facts I can add on some certainties. Later on in intelligent system we will see how we can handle such concepts. 
For the time being remember that in the case of rules or rule based system when we infer a fact this can be a hard fact which can be either true or false or it can be a fact which is associated with some sort of certainty, some degree of belief or disbelief whatever it is. So it is no longer a black and white decision there can be a grey region in between. Now we can also have different control strategies which are often heuristic. We often need to take decisions in real world where we do not have all the possible information we need. Often we have got very partial information about scenario. Suppose you want to travel from a place A to place B you know some facts that there is a bus route from A to B and there is also railways connecting A to B, you do not know exactly what the fares are. But if you really need to save money then probably you will select bus travel because your other domain knowledge in general tells you that bus travel is often cheaper than train travel. It may not be always true but let us assume that there is a scenario as such. So you have got some partial information and based on this partial information and the other domain specific knowledge that you might have got through some other means or general knowledge that we have there are some thumb rules of making decisions. And we often use those to take the decisions. And we can say probably bus will be a cheaper mode of travel so going from A to B for low cost bus would better way of going. Now these sort of decisions we often take need not always have very strong logical bases but these are often very much practical and we often take such decisions. This particular scenario is also called as heuristic and such heuristics we will be employing for Rule Based Systems and we will also see for different control schemes. (Refer Slide Time 18:08)  

here is an example of a rule based system. If it rains today roads will be wet. This is clearly understood when we write in English. We can write this in the form of logic as rains is a predicate, rains today implies implication, wet road today or in the form of a rule we can say if rains today then wet road today. You can see the correspondence between this logic statement and this rule. Again there is another rule I added here; if rains today and not covered roads that is if the roads are not covered then wet road today. 
Now look at the difference between these two rules. This rule is obviously more specific because it says that the road will be wet today if it rains today and the roads are not covered. Whereas this is a statement which is much weaker if rains today then wet road today. Now look at the antecedent part of this rule; rains today is an antecedent and wet road today is a consequent. In this case we have got a number of antecedents rains today and not covered roads these are two antecedents and these two together we call the antecedent field or simply we call them antecedent. 
Now what happens to this rule if the fact is rains today, that means if rains today evaluates to true then we will conclude that the roads will be wet today, this part  wet road today. On the other hand when will we infer wet road today? We will infer wet road today if both these conjuncts of this antecedent evaluates to be true. In our discussion I will often refer base to be the antecedent part or the antecedent field and each of them are individual antecedents. Now when does this rule will really work? This rule will be active when both these antecedents since they are connected with an AND are both true. suppose the fact is, it rains today and the roads are covered today, according to this rule this part will be true but this part will not be true because the roads are covered then I cannot really infer that the road will be wet today. So in a rule we have got two parts one is the antecedent part another is the consequent part. The antecedent part can consist of one fact or can be a conjunct of facts or anti conditions. Look at this consequent, there are two different types of consequents; some things can be actions and some thing can be assertion. Wet road today is simply an assertion that is adding to my collection of facts and since I know it rains today and not covered today I know one more fact that the road is wet today. Might be there is another rule, if it is wet road today then wear good shoes or something of this sort so this fact can be used for something more. Now which are the facts here? Facts are which are already known a priory but you can see that facts are also generated just as in this case in this example wet road today is a new fact that has been generated.  
So now (Refer Slide Time 22:25)  

let us get back to Modus ponens which was a very common deduction mechanism we talked about when we were talking about logical inferencing. Here we just revisit Modus ponens. Here you see P implies Q and P so P implies Q is an implication and P is a fact that is given. So, given these two using Modus ponens what can we infer? We infer Q. that means if P is true then Q is true and since P is true we infer Q is true. Now suppose there is another rule; Q implies R. That means if Q is true then R is true. Now here from these two we have inferred Q and again from these two we can infer R. Therefore if we consider this to be a rule and this to be a fact then we can infer a fact and using this fact and another rule which is the rule that has got this fact Q as its antecedent we can infer another fact R. therefore starting from this given fact P and these two rules P implies Q and Q implies R I can first infer Q and then R. So this rule is allowing to infer Q and inferring Q is allowing me to infer R so this phenomenon is also known as rule chaining where we are inferring a new fact using a rule and with the newly inferred fact I am applying another rule and inferring a new fact and the chain can go on.. so now let us come to Rule Based Systems once again. (Refer Slide Time 24:33)  

Rule Based Systems are also known as production systems. Those of you who have worked on compiler you have seen that often grammars are specified in the form called Backus normal form which is also a set of productions. Find the similarity between those statements which are given in Backus normal form and the type of rules that we will be discussing. Certainly there are quite a few similarities. A rule based system which is also known as production system is a system whose knowledge base is represented as a set of rules and facts. So we were talking about knowledge representation earlier. Logic was one form of knowledge representation. We will see different other types of knowledge representation as well. Now rule based system is therefore one type of knowledge representation where the knowledge is represented as a set of rules and a set of facts. We have already seen this, a rule based system consists of a collection of rules which are in if then form and a collection of facts. Now with these rules and facts we would not be able to infer anything unless there is another interpreter or control engine whatever name you give it inference mechanism which will look at these facts and see which rule is applicable now and apply that particular rule and generate a new fact because in my rule base I will have thousands of rules. Now, given a particular objective a particular problem I want to solve all these rules may not be relevant. 
Therefore it is essential that I have to find out only those rules which are relevant at this particular point of application. So, that task is done by the interpreter. And in today’s lecture we will see how the interpreter applies these rules, how the interpreter selects these rules and what does this interpreter really do to infer new facts and thereby solve some problems. The rules (Refer Slide Time 27:12)  

are represented in the following form; if antecedent then consequent, and antecedent can be a conjunction of number of antecedents. When the antecedent part is NULL it is also possible that I will just have a consequent. If the antecedent part is null then obviously it becomes a fact that is a true fact. In general we will assume that a fact is true. Now rule says that if these conditions are true then I can infer a new fact. But if no conditions are given then that consequent stands on its own merit on its own right so it is true, it is fact. Structures like while one, while two is always true. So, similarly if there is no condition attached to a rule then it becomes a mere fact. Now here is a statement that i am making; rules are normally represented as horn clause. There is reason for this. Before being introduced to horn clause let us recall what a clause or a clausal form is. A clausal form consists of disjunction of literals. For example, p OR NOT q OR r that is a clause or a particular form. And implications can also be converted to clausal form. Horn clause is a particular form of such clauses and it is referred that Rule Based Systems based on horn clauses. Let us look at this implication; P implies Q we will recollect the clausal form of this which is NOT P OR Q because this implication means if P is true then Q is true. Now if P is true then Q is true otherwise P is not true so NOT P OR Q is the clausal form. Similarly P AND Q implies R is a rule form. I can convert to the clausal form NOT P OR NOT Q OR R is a clausal form. Now look at this; P AND Q implies R OR S. (Refer Slide Time 30:06)  

If I convert to the clausal form it becomes NOT P OR NOT Q OR R OR S. This is not a horn clause, what is a horn clause? If we call anything a horn clause the first condition it must satisfy is that it should be a clause, there must be clauses. Horn clauses can have at most one non negative literal. Here Q is a non negative literal, here R is a non negative literal, there is only one non negative literal but in this case we have got two non negative literals so this is not a horn clause. So these are acceptable rules but this is not typically in a rule based system. It is not the case that you cannot write rules like this but usually it is not encouraged and the reason is there are some severe consequences to this. If I use this particular form which is not a horn clause because it has got more than one non negative literal what is the problem? Suppose my fact base says that the person is rich that is P and the person is large hearted so P AND Q. Then the person will donate OR the person will build a school or whatever. So there are two possible implications we can take. Now, in the case of an implementation or execution the interpreter will really be in a problem to select which one of these consequents to take, should it infer both because I do not know whether both of them are true. This part will be true if any one is true or should it infer anyone? If it has to infer anyone then which one should it infer? So those problems really crop up but those problems are absent in these cases P implies Q or P AND Q implies R so we usually stick to horn clauses. That means in the consequent part we would not keep a disjunction. Now there are two more terms which we should first understand. For a rule base system we have got a rule base and we have got a set of facts which we give in a fact base. Now these rules are evaluated to be true. When is a rule applicable? A rule is applicable when all its conditions evaluate to true. If any of the conditions evaluate to false then that is not applicable. Now it may be the situation that given a set of facts and the set of rules more than one rule can have their conditions satisfied. Therefore more than one rule is applicable. These rules are triggered and out of these triggered rules that means those rules have all been enabled to be fired. Now, when we apply a particular rule we say that that rule is fired. And what happens when a rule is fired? When a rule is fired then the consequent part as to whatever is specified in the consequent is inferred or that action is taken. So, if that new fact is inferred then this fact is added to the fact base. If the rule just specifies some action and if that action is taken then that action creates some change in state. For example, as I was giving the example put off the heater then the heater is put off so the state changes wherein earlier the heater was on and I changed that fact I delete that fact heater is on and added a new fact heater is off. So, in either case there is a change in the fact base when a rule is fired. But it is not the case that all the rules which are enabled are fired. I can either fire any one of them, if I have to fire any one of them or to make a decision as to which one should I fire so all these things are there. But first of all let us make a decision that a rule is triggered when all its antecedents evaluate to true and the rule is fired when the actions stated in the consequent part or the inference related to the consequent part is inferred or is taken. 
(Refer Slide Time 35:38)       

 Therefore we will make a distinction between these two. Now look at this architecture. This is an overall architecture of a rule based system. You can see this big rectangle and you forget about the rules which are written over here. All these rules here constitute the rule base whereas we can have a number of facts stored and that is known as the fact base and here there is a procedural component which can have different names control scheme, interpreter, inference machine, control engine, etc different names which are given. Now look at this arrow, what does this control scheme do or interpreter do? It is looking at the facts and looking at the rules. And is trying to see given the particular set of facts which are the rules which are applicable which are triggered. Now, out of the rules which are triggered it will fire one or more than one (Refer Slide Time 37:07)  

and consequently when the rule is fired a new fact will be generated and that will update this fact base. So, at the top level we look at three components in a rule based system namely the rule base, the fact base and an inference machine. These are the three basic components of rule based system. Next let us look into these rules. Here the rule is, if hot and smoky then add fire. Rule two says, if alarm beeps then add smoky. Rule three says, if fire then add switch on sprinkler. So here we are talking of a toy problem where we has got rule based intelligent system installed which has got some sensors put on in the room and its sensing temperatures it is trying to see whether the temperature becomes very hot, and there are also smoke detectors so it is trying to see whether there is any fire and if it infers that there is fire it will automatically start on sprinkler or it will set up an alarm which will make people run and switch on the sprinklers. Therefore it is that sort of a scenario we have here. So we have modeled a toy problem with these three rules. And what is there in the fact base? Alarm, beeps and hot so there are two facts. One is alarm beeps and hot. Since the alarm beeps what will this control scheme does? It will look at these rules. Now which rule is enabled? I can see also hot, so this rule have got this antecedent satisfied if hot, yes, true so this is satisfied. And smoky, but I do not know anything about smoky in my database of facts so I cannot say that it is smoky so this part is true. Hot is true but this smoky is false therefore this antecedent part of this rule is not completely satisfied. Therefore this rule does not apply. So this rule is not triggered and it comes to the second rule. If alarm beeps, and in the fact base alarm beeps is there so this part is true so this rule is obviously enabled because it has got no other antecedent. This entire antecedent field this antecedent part is satisfied. The third rule is, if fire, now I do not know anything about fire here and nothing is here so this antecedent is also not true so this rule is also not triggered. In this case we have got only one rule that is triggered that is rule R2.
So the control scheme then finds that only R2 is triggered so it has got no choice and it fires that. And what is the consequent part of this rule R2? The consequent part says, if alarm beeps then add smoky. So when it fires it immediately adds to the database the new fact smoky. And you can well guess what will happen next. Next the control scheme will again look at rules which are being enabled as fresh. Hot is true so this part is true and now smoky is also true. Since smoky is true this entire antecedent part is true. R2 is also true but R2 is already fired so we are taking up the strategy that I will not fire one rule time and again, it tries for the third rule fire but I have got nothing in the database as yet which tells that there is a fire. Therefore only this rule is enabled this rule is fired and what is the consequent of this rule? It is add fire, so a new fact is added to the fact base which says fire. So what am I inferring? I am inferring that there is a fire. Now I again go back to this cycle and look at this third rule, the third rule is now enabled if fire then add switch on sprinkler. Now this third rule will be fired and sprinkler on will be put in the database. So essentially this is the way in which a rule based system will work. Now, if you have noted this example very clearly then you will realize that this can be modeled in the form of automata of inferencing. What is that automata? There are three phases, now what are the components that I have? I have got a set of rules, I have got a set of facts and the inference machine is also there. So what is happening? First the inference machine is looking at the fact base and the rule base. I am trying to find a match as to which of the rules are triggered? If it finds that more than one rule is triggered then it will have to select some of the rules may be one rule or may be a subset of these rules. For the sake of simplicity for the time being let us assume that we are selecting only one rule and that rule is fired that is executed. And when that rule is executed a new fact is generated in the fact base. So we can essentially carry it on in the form of cycle. So here is what is depicted, (Refer Slide Time 43:50)                

the inference machine or the interpreter is a machine that implements the strategies to utilize the knowledge base and derive new conclusions from it. Now how it is being done? Here are the automata of an inference machine. There are three phases. First is the match phase then there is a conflict resolution phase and then there is a execute phase. Now in the match phase the inference machine is comparing the fact base and the rule base and it will find out some rules which are matching. If no rules are matching then we cannot proceed any further. But suppose there is a set of rules which match then those set of rules will be used by the next stage which is the conflict resolution stage. Where is the conflict? The conflict is among the rules. There are five rules which are triggered in the match phase. Now which of these rules I shall be firing? For that I need to have some strategy which is often called the heuristic strategy. We often use heuristics but using heuristic what we do is the conflict resolution strategy. I will be selecting one rule or more than one rule. Of this subset of match rules which part of the subset I will be executing is decided in the conflict resolution phase. 
So we come to the conflict resolution phase and after the conflict resolution phase we decide on which rules to find. And then with that information we go to the next state which is known as the execute state. In the execute state those rules are fired and when the rule is fired, here in the match phase we passed on only the triggered rules to the conflict resolution phase. Now in the conflict resolution state we have resolved the conflicts and we have selected some rules to be fired. and when the rules are fired then the firing causes a change again in the new fact base the new fact scenario. After this execution you will again come to the match phase. But how long will it continue? We will come to the match phase and in the match phase again the matching will be done and we will see which rule is enabled again, we will do conflict resolution, we will execute and this cycle will continue. Now how long will this cycle continue? This cycle will continue until the goal or the objective with which we are trying to solve the problem. The problem we have, we are trying to solve that and that we call a goal, and as long as the goal is not met we should try to do this. But it may be that we have not been able to solve the goal and we do not have enough rules at our hand. Rules are essentially representation of knowledge and knowledge is always limited. And often there are situations when we the human beings fail to solve a problem because of lack of knowledge may be at times. So, in those cases we try till we exhaust all the knowledge components we have so we will carry it on but if we solve a particular problem then we will stop otherwise we will exhaust our knowledge source and then stop. So this cycle will continue and after every execution phase it will go to the rule base and after every execution it will check for the goal whether the goal is met. If the goal is met then we will stop otherwise we will carry on. This is a very fundamental cycle of rule based system. Here is a summary; The execute state fires the rules once all its antecedents match and essentially the function of the execute state can be thought of as searching through a path through the goal in a search space. Here I have added a new name search space. (Refer Slide Time 48:37)  

 So what I am trying to take here is that, through these rules we are essentially trying to depict all the possible ways in which a problem can be solved. Our objective is to solve a particular problem that is a goal and in order to do that there may be different alternative paths all the paths may not lead to the goal but there are paths which may lead me to the goal. So, effectively our task is to search through this entire space which I am calling the search space. Now here is a search space node. Typically a search space is denoted by these states. This state I am showing is the start state and I have got a goal state also here. Orange node is the goal state and the root node of this is the start state. Now what do I have at the start state? I have got my rules 
 (Refer Slide Time 50:04)       
and I know about the goal and right now this goal is not solved so I start my rule based system here. And with the facts and the rules I have I try to see which of the rules are applicable which are triggered. Suppose at this point three rules are triggered, I have just named only two of them; One is R1 and the other is R5 but I am denoting these set of rules using this tree structure. Now what this state shows is that the result of my match as shown here that has resulted into three rules which have been passed to this conflict resolution module. And in that suppose I have resolved in favor of R1 so I can come to R1 and when i fire a particular rule R1 I will be adding new facts to the rule so I continuously refer to this diagram. (Refer Slide Time 43:50) so I have resolved in favor of R1 and there has been executed and new facts have been added to the fact base and that might have enabled me with two other rules and I can select any one out of them. Suppose I have selected this one and fired this rule so here again there was a conflict. At this state again a match has been done and two rules have succeeded and these are triggered given this new state. And I have fired this rule  and I have come to this new state. And as I come to this new state and if I fire this rule I will come to the goal node. But what are my possibilities? My possibilities are that I could have followed this path, I could have followed this path, this path or might be at this conflict resolution strategy I could have followed another path. Suppose at this point after firing R1 I come to a new state and I fire a new rule and I fire this rule and come to a state from where I do not find any other rule to fire then I cannot do anything. But I think I can do something. Suppose at this point R1 suppose this is R2 and R5 all these three were enabled but I have selected in favor of R5 and I have come to this point. And when I come to this node after match i do not find any rule that is enabled so I cannot proceed any further and I cannot reach my goal. But what can I do? What I can do is this, from here I can  (Refer Slide Time 53:21)         

go back, since this is a dead end and I am not finding any other rule to fire I can go back to the earlier node to see which are the other possible rules that has been left unexplored. For example, I came to this node but I resolved the conflict in favor of R5 and I have come to a dead end so I go back to starting node and see what other choices I could explore. If I really remember that yes there were other alternative paths like R1, R2, etc and explore these paths then possibly I could find the path to the goal. Actually this is what we do in our real life and this phenomenon is known as backtracking. We often make wrong choices but when we find that this wrong choice is leading me to a dead end we revise our earlier decision and this is known as backtracking. And what I have depicted over here is the search space and how the rules really traverse the possible search space and my success will really lie on how fast I can arrive at the goal from starting state. So what we have just now (Refer Slide Time 54:46)  
learnt is the phenomenon backtracking. If the inference machine reaches a dead end that is no new rule is enabled and the goal is not met. If the goal is met then I am successful, if the goal is not met then we will backtrack to the earlier node as we have done here we will backtrack to the earlier node and see whether there are some unexplored possibilities and we will be exploring that. So backtracking is a very important methodology of exploring the search space because it is often not possible to really make the correct choice all the time it may not be always possible so we will select the rules which are not proper given a particular context. And obviously conflict resolution strategy plays a crucial role over here if we have a good conflict resolution strategy then obviously the backtracking will be reduced. If you have good conflict resolution strategy then we will be reducing the backtracking. Now, in this lecture we conclude at this point where I have just tried to give you an overview of what Rule Based Systems are and how they work. In the next lecture we will look at other aspects, we will look into more examples and into more details of inference mechanism. But I hope that you understood how a set of rules really help us in solving the problem. You should remember a couple of facts namely the distinction between rules and facts, the distinction between match phase conflict resolution strategy and execution strategy. You must understand what is an inference mechanism? An inference mechanism actually selects the rules to be fired given a set of facts. So given all these we will move on to the next lecture where we will be going into more details of Rule Based Systems.    


Preview of next Lecture

Lecture 18

Rule Based Systems 2


In the last lecture we discussed about Rule Based Systems. We introduced the concepts of Rule Based Systems. And in today’s discussion we will further touch upon the aspects which we did not cover earlier. As we will recall we talked about an automata that the inference machine of a rule based system executes. And it consists of three states or three phases. First is the match phase where the antecedents of the rules are compared. Later on we will see that we can also compare the consequents but for the sake of simplicity for the time being let us assume that the rules and conditions of the rules are compared with the existing facts in the fact base. And if there is a match the rules whose antecedents match are triggered and they are fed to the conflict resolution strategy. In the conflict resolution strategy we select a subset of those matched rules and feed it to the execute phase and the execute phase we fire those rules and as a consequence of firing these rules new facts are generated and the fact base gets updated. We go on carrying out this activity till either the goal is met or there are no more rules to fire.   

Artificial Intelligence
Prof. Anupam Basu

Lecture 18

Semantic Net
(57:04)

inIn the last lectures we had looked into different one particular knowledge representation scheme namely the rule based system. ourOur objective is for the entire course is to look into different ways in which a system can demonstrate intelligent behavior. andAnd one of the major components of demonstrating intelligent behavior is acquiring and using knowledge. inIn the last lecture we have seen saw one such way of representation that is the rules. and And we also concluded the lecture with one very important and popular application of such rules in the form of expert systems which are rule based systems some of the expert systems which are rule based systems. besidesBesides rules there are other methods and modes of representing knowledge. todayToday we will start discussing on a few of those other modes of representation of knowledge. soSo we will be looking at other knowledge representation formalisms and we will be starting with our discussion on semantic nets. Now, as you must be knowingknow the word semantic means meaning. all right so So we are familiar with two words syntax which is essentially the grammar of a language and semantics is the meaning of any sentence for example. iI may form the sentence correctly which may be grammatically correct that means it may be syntactically all right correct but it must be meaningful. inIn order to be meaningful it should be semantically correct. nowNow, when do you say a particular sentence or statement beis meaningful? aA sentence is meaningful when we can really understand it and map it to some of the known concepts of the real world that in which we live or we see we or can visualize you canor realize. soSo the essence of knowledge in what form in one form is built around the concepts the different concepts that has playedare spread around us. what everWhatever we look at for example, if iI look at this room then iI will be looking at different chairs, tables okay etc. now each of these are different concepts. andAnd the chair for example is made up of wood usually. thereThere can be chairs which are not made of wood that may be made of iron, steel or some other material. soSo always when everwhenever we talk of chair we can think of some other associations with the material by which the chair is made of. when everWhenever we talk of chair another pertinent question can be what is the size of the chair? whatWhat is the weight of the chair? all right when everWhenever iI use a word say a boy then obviously with that usage of the word boy it certainly maps into me it maps into a particular concept within me because along with the boy iI have got so many other associations. a boy will be at most if iI call him a boy will be at most of say eighteen 18 years old, when everwhenever he is more than eighteen 18 years usually iI call him a man or what everwhatever. a A boy will be having heightigh, will be having weight, usuallyand usually may be going to school so there are so many other associations that come up. Iif iI talk of tiger then immediately some concepts, the tiger is a concept and along with that iI can associate how many legs does the tiger have and the immediate answer is four, okay what is the color of the tiger? thereThere may be two possible answers white or yellow. soSo along with when everwhenever we use any particular word we are actually referring to some concept and along with that concept we immediately associate some other related concepts. andAnd this association of the different concepts buildthis association of the different concepts builds up our knowledge system, our conception, our knowledge base. soSo this is another view of looking at knowledge that knowledge can be represented as a network as a connection interconnection of different concepts. Now, in semantic nets that is what has been explored how we can represent a domain specific knowledge using the different concepts. inIn today’s lecture we will see how we can represent the concepts and how we can represent the knowledge using this conceptsthese concepts. andAnd also as it has been mentioned earlier that iI cannot call any collection of facts or information to be in knowledge unless iI can use it use it for inferring some new facts. so So, if iI have to call this semantic net as a knowledge representation method or scheme then iI must also have some particular inference mechanism by which iI can utilize this representation to infer new things to answer different questions all these things we will have to look into and that i’s what we will try to see in today’s discussion. now (Refer Slide Time 07:26)  

Nnow, the idea of semantic nets dates back i mean to nineteen sixty six1966 and in two very important papers that were written by Ross Quillian who was one of the early workers of A I and who tried to develop a representation of meaning. and so thisThis slide as you see has been titled as the knowledge representation the thoughts about knowledge representation as it were in the nineteen sixties1960s. andAnd also the subtitle is networks and meaning. thatThat means that here we try to represent the different concepts in the form of an network and also try to capture the meaning of the concepts in the network structure. we will see how Bbut what Quillian tried to do is to represent different concepts as a hierarchical network,s that is very important because if iI just say, he is a boy or he is a man then immediately iI also know that man yes man is a concept but man is a mammal and iI also know the concept mammal and there are many properties that we share in common between man and mammals. andAnd when iI say Ttom is a man and Jjohn is a man then both these other entities tom and johnJohn are also examples of man and they share some of the properties in common because of the relationship with the common concept man. they also share tomTom and Jjohn also share some properties of the concept mammal because all of them are connected to the concept mammal because Jjohn and Ttom are man and man is a mammal. so in that way we will illustrate this all through our lecture today and we can see that asAs a collection of different concepts organized in a network that this is organized in a hierarchy or layer. nowNow this model was later on amended with some psychological assumptions because the original objective of all these exercises all these research was to try to understand how we the human beings think, how we represent our concepts in our mind. so So there has to be some correlation with the structure of human semantic memory not that everything is known about it but a lot of research has gone in into this aspect by the psychologists and what everwhatever is learnt from them that can also be utilized in our computer representation of knowledge semantics and meaning. so inIn this approach of representing meaning as networks concepts can be represented as hierarchies of interconnected concept nodes.
(Refer Slide Time 11:07)  

okay ifIf iI assume that one concept will be one node, say for example, animal is a concept, bird is a concept, canary is a particular type of bird ,okay stalk is another type of bird all these are birds and they are related to the concept bird therefore. bBut canary is another class of bird,s sparrow is another category class of birds but all of them share some things in common with bird and bird again being connected to animal will have some common properties with animal. soSo any concept we can say that any concept has a number of associated attributes at a particular given level. weWe can say that animal the concept animal immediately tells us it has got skin, it eats, it runs, etc.etera etcetera etcetera allAll these properties are related to the concept animal. similarlySimilarly we will have some concepts related some attributes related to the concept of bird. nowNow some concept nodes are super ordinates of other nodes. Ffor example, animal is at a higher level than bird because a bird is an animal, a tiger can be an animal, okay a lion can be an animal, a crocodile can be an animal so animal is more of a super class or super ordinate of all these sub classes or sub ordinates like bird, tiger etc.etera againAgain if we go one level down the hierarchy from here we will see that bird is again a super ordinate of canary. canary Canary can be a bird, stalk can be a bird, peacock can be a bird, ostrich can be a bird all these are different types of birds but all of them are birds and all birds are animals. sSo we can see clearly in this example we can see clearly a three level of hierarchy at the top level as we have shown is animal. afterAfter that there is bird and okay bird is at one level lower than animal, more specific than animal. okay andAnd tiger, lion all these things may be in the same level as bird. nowNow special classes of individual classes categories of birds like canary, peacock, penguinand penguin come one level down that hierarchy. all right soSo in this example we have seen a three level of hierarchy but there can be multiple levels of hierarchy. all right soSo the essence of this idea is that we can organize the different concepts at different levels and all these concepts may have some relationship from one level to another. okay so thatThat is the basic idea and each of these concepts will have some attributes specified at that level. likeLike animal we will have attributes, bird we will some additional attributes etc.etera nowNow how ever (Refer Slide Time 14:40)  

we cannot store in a computer or might be in our human mind also we do not explicitly store everything. soSo we often talk of cognitive economy. thatThat means how much do we store. So, for reasons of cognitive economy sub ordinates inherit all the attributes of their super ordinate concepts. soSo what does it mean? Iit means that if iI had specified something about bird then those attributes will be inherited by who ever are the descendants ined at that hierarchy. Here is an example, let me give another example to make the thing clear say iI can say that mammal is a category okay all right so here iI can say human and iI can say might be some animals and they are related. thisThis is at the top level L0 zero and this is at level L1 one. nowNow human beings can have some attribute, i am just drawing one attribute let me use some other ink for that purpose let me use blue ink this is an attribute link which says has and iI write down another concept hands. all right nowNow, if iI draw another level down here can be man and here can be woman. nNow man is linked to human it is down the level this is L level L2. two 
nowNow man can have some additional properties, woman can have additional properties over human but what everwhatever is there as a property of human will also be there for man orboth man and woman both. Iif it was not the case say if iI just go on adding properties at the human level okay say i saylike color of blood the color of blood is red, now if iI had to store everything here then iI had to store has, hands, color of blood, red etc.etera butBut this part is not required to be stored explicitly. iI would have to store it here as well as for woman andfor everything so Ii do no’t need to store this here because it i’s already stored at the upper level so iI do n’ot need to store this part here and  all right and iit i’s sufficient to store  at the parent level or the higher level. thisThis is what is meant by cognitive economy. that means i do n’ot want to store all the attributes at all the levels instead what everwhatever is there at the top level say mammal iI can certainly say it drinks milk all right or born from mother what everwhatever it is. thenThen all the children will have all the concepts which are children of this level will inherit theise properties that are there at the top level. okay so So that is the basic concept (Refer Slide Time 19:08)              

of this sentence that is for reasons of cognitive economy sub ordinates that means which ever level are down the hierarchy inherit all the attributes of the super ordinate concepts. nowNow for some instances of the concepts are excepted or there are exceptions from the attributes that help us to define the super ordinates. for example let’sLet us take one example, say a typical classical example is, that when we say bird, bird is a concept then birds can fly, what is the locomotion of birds? It is flying, how do they move? They move  by flying so if it i’s a sparrow it is a type of bird then obviously it will be automatically inferred that well sparrow is a bird and iI know that birds can fly so sparrows can fly. thisThis is what we can inherit from the parent class parent category. butBut ostrich is an exception ostrich cannot fly but ostrich is a bird. iIt has got all other attributes that a bird has such as feathers, wings, two legs, has a beak all those things are fine but only the locomotion is an exception. thereforeTherefore at the sub ordinate level iI may need to modify some of the attributes in special cases which are exceptions. so So the general rule we that we can think of is, that usually we will inherit all the attributes of the parent but at the lower level we can modify them in special cases as required. variousVarious processes search these hierarchies for information. we will see how it is done now hereHere is a hierarchical network that we are talking about. (Refer Slide Time 21:24)  

now please look at this whatWhat is the super ordinate in this hierarchy? weWe have got animal, look here when the mouse is moving all right this is the part where this is the super class i mean the top level. nowNow we have got bird, we have got fish. Now and so these are related to this concept animal. eachEach of these ovals represent one concept. this oval represents bird and look at this green link the name of this green link or this green link is leveled with is- a that means bird is a animal, in proper English it should be is an animal but we are not making a distinction, so here is -a means bird is a sub ordinate of this super ordinate. similarlySimilarly fish is a sub ordinate of this animal. nowNow look at these parts; animal this concept animal is associated with some properties like can breathe, can eat, has skin etc. Nnow this can and has are different attributes of this concept animal. similarlySimilarly bird has got some additional attributes like can fly, has animals wings etc. all animals i cannot say that animals has wings because all animals will not have wings. butBut bird has can fly, has wings, has feathers. similarly Similarly if we look at this concept fish we have got some additional attributes like can swim, has fins, has gills etcetera and fish is a animal. Now, if we go one level down the hierarchy we will see that these are these colored ones are specific instances specific not instances these are even more a further specialization of birds. canaryCanary is a bird, ostrich is a bird, salmonand salmon is a fish. now note a couple of things here that if iI look at this concept bird the bird as it is written over here has got these three attributes can fly, has wings, has feathers. but just because it is a type of animal it i’s a sub ordinate of a animal the propertyies of animal are also inherited, can breathe, can eat, has skin. So, if i anybody asks the question can a bird eat the answer will be yes why because though iI do not find that the attribute can eat is here in a bird but iI know that a bird is aan animal and therefore what everwhatever property is there in the case of animal will also hold here. so thereforeTherefore iI can say bird can eat. similarlySimilarly if iI ask the question can canary fly or can does canary have feathers? you see whenWhen iI described canary iI just specified some specific attributes that are special to the category class canary can sing and is yellow, is yellow the color is yellow. butBut just looking at this iI cannot say whether canary has wings. butBut yet iI can say that canary has wings because iI also know there is a  interlink between canary the concept canary and the bird, the relationship between them is easy. soSo canary is a bird and iI know that birds has wings therefore canary has wings. similarlySimilarly look at salmon, salmon is a fish and a fish can swim, has fins, has gills. so if butBut salmon has got some additional specific attributes like lays eggs, swims upstream, is pink, is edible etc.etera etcetera etcetera nowNow if i say straight away if I say say does salmon lays eggs? theThe answer is yes because iI can see that attribute right here. butBut if iI ask the question can salmon swim? theThe answer is not here but if iI follow this link iI can see that salmon is a type of fish is a fish and all fish can swim that means all fish can swim unless specified otherwises so iI can infer that salmon can swim. ifIf iI ask can salmon breathe well then the answer is not here so iI go up this link the answer is not here as well but the answer is here you can see. that sinceSince fish is a animal it can breathe. butBut now let’ us look at ostrich. if i forget about nowNow here iI have written ostrich has got the attributes runs fast, cannot fly and is tall. supposeSuppose this attribute cannot fly is not written over here suppose that in that case if iI ask can ostrich fly? iI will first look at the answer here and since this cannot fly is erased from here iI will look over here and find well iI do not find an answer here but let me try because ostrich is a bird well therefore here is written can fly so the attribute of bird is can fly therefore ostrich can fly that would be my inference. butBut here in this case we have got specific information that ostrich cannot fly. soSo this attribute locomotion cannot fly or locomotion like itmight be walks but it cannot flydoes not fly that will have overriding precedenceference over what everwhatever iI know from the parent’s. soSo as iI go down the hierarchy in this direction iI come from the more general to the more specific. okay so tThat is’ what we just now discussed that we usually we take the properties from the parent’s but we go down as we go down there we can be more specific and we can at times override the property at a lower level. okayS so i hope this part is clear so this is an example of a hierarchical network which demonstrates the different levels of hierarchy such as animal is a level sayin the top level L0 zero, bird and fish is at lower level and canary ostrich salmon is at even lower level. so here(Refer Slide Time 28:57)  

i just show what i just discussed once again that theThe same hierarchy and here it has been shown in red that ostrich cannot fly to show that it is an exception at the lower level. nowNow there are three properties three things we have just now shown, the first one is called inheritance. inheritanceInheritance means any child concept. We will inherit all the properties of the parent concept. so So canary, ostrich salmon canary ostrich will inherit all the properties of bird, salmon will inherit all the properties of fish and in turn bird and fish will inherit all the properties of animal that i’s the first thing it is inherited with some exceptions. specificsSpecifics can be more detailed. thinkThink of a human species, i mean we inherit many of the genetic characteristics from our parents and that i’s exactly is what’ iss happening here. It is the parent child relationship so in general the child inherits all some of the properties of parents and the child can have itstheir own properties which were not exhibited in the parent.’s similarlySimilarly here specifics can be more detailed and can be something that is inherited from the parent that can be overridden.  okay so this again the same example and it has been shown throughThrough some psychological experiments (Refer Slide Time 30:48)  

that has been carried out that’s the tests have demonstrated that the subjects recognize and when the tests were tried on human subjects it was evident that the human subjects recognize propositions or statements which have lowered down the hierarchy much faster much readily than the propositions which are higher up. now what are the propositions? say  ifIf iI say canary can sing that proposition is some how stored here. howHow is the proposition stored? propositionProposition is stored in terms of the attribute and the object relationship. canaryCanary is an object having the attribute can sing so that is at this lower level. usuallyUsually human beings beings very first very quickly recognize the specifics when they look at a canary the first thing well a canary is yellow, a canary can sing, and ostrich? itIt cannot fly it runs very fast. Bbut the thing in order to state the proposition is, canary has wings, okay canary can breathe they automatically do not come in because what you are looking or what you are perceiving is at this lower level and so what everwhatever is very much evident in front of you at the lower level we will have more precedence more importance and human beings quickly recognize that. Bbut for recognizing, not that they cannot understand the other things they can also understand that canary can breathe but somehow the reason well that the canary is a bird and bird is an animal so all animals can breathe therefore the assumption is thatso canary can also breathe. so oneOne thing is ready acknowledgement another thing is acknowledgement through reasoning. itIt has been found through psychological tests that the facts which are at a lower level have got more ready acceptance to human subjects and that justifies that validates this model of knowledge representation. now we come so tillTill now what we have been discussing is, how concepts can be represented as hierarchical network. nowNow we can define semantic networks and semantic network is a structure for representing knowledge as a pattern of interconnected nodes and arcs. (Refer Slide Time 33:15)  

okay which you must have observed by now through all the pictures nodesNodes in the net in the semantic net represent concepts of entities, attributes, events, values many thingsetc. okay arcsArcs in the network represent relationships that hold between concepts. Therefore okay so arcs are denoting relationships, till now we have shown only two types of relationships only one type of relationship that is easy. butBut where priory this part requires a little bit of explanation. nodesNodes in the net represent concepts of entities, attributes, events, values etcetera all right so  because iI can also draw a semantic net in this way. say okay manMan is a concept and man is a mammal as i said and man has got different attributes iI can say number of reps legs, iI can draw another type of node two has that will be there in the mammal.  so i would put has and it is this it is this part man Man, number of legs, i can say number of hands etcetera or number of eyes can beto two but there can be other attributes also like height. nowNow this height can be i can say say height isis less than seven 7 feet which is rather common. inIn that way iI can have different types of nodes. you see thisThis is one type of node that i’s representing a concept. thereThere can be nodes which are representing values as well like these are representing values. allAll these together are giving me some statements; mMan has may be has hair and hair is a type of skin. nowNow you see all these things are concepts, has hair, we okay saw here we saw that birds has feathers but man has hair and what is hair? hairHair is a type of skin so here you see this is another concept, all right this is another concept, this is another concept and these are also theythey are different entities (Refer Slide Time 36:46)  

and there can be also values so it i’s not necessary that only concepts will be nodes. inIn a semantic network we can have different types of nodes. all right so let’s go back to the definition again let’s see now nodesNodes in the net represent concepts of entities, attributes, events, values etcetera we will see events later a. Arcs in the network represent relationships that hold between the concepts. okay now let’s proceed a little further now when i store this now hereHere iI have got so many other concepts. nowNow, when iI store that in a computer iI will store that with some particular type of coding. nowNow iI will level the concepts and might be in a table i will be restoring those each of i can create a table where iI will have C1 one, C2 two etcetera iI will have to encode each of those concepts and each of those concepts can have some attributes as well so I have to store them. nowNow here what we are showing is that C1 one is the top level concept and iI store C1s one  attributes. theThe next level is C1 1 one one and all the child concepts are leveled as C1 1 1 one one one C1 1 2 one one two so C1 one has got two children’schildren C1 1 one one and C1 2 one two a sort of subscript notation that we use. 
soSo, in that particular way we can represent all theise concepts. all right nowNow where is the meaning? weWe are calling it semantic network we have seen that it is a structure of nodes and arcs. theThe nodes are representing different concepts. Now, do the concepts only the concepts by themselves does it eexpress the meaning? No, the concepts do not express the meaning by themselves no. inIn order to understand meaning of the semantics it lies in the structure as well as the relations that are there. okay forFor example, you see the meaning will quickly change when I look at when i look at this diagram this network and let us compare  this network and compare it withwith the earlier network. (Refer Slide Time 39:12)  

this network and has got two links salmon fish and animal are connected by is- a links and iI just change at this level these two links from is -a to it. you see the structure ifIf iI just withdraw theise levels the other structures the concepts and the linkages are remaining the same but only thing that i have changed is the relation. Now, immediately the meanings are have changed. nowNow what this structure means is, salmon eats fish and fish eat animals. nowNow iI can no longer say answer questions which iI was being able to dodoing earlier. okay doesDoes salmon has fins? well iI look here and find salmon salmon lays eggs, salmon swims upstream, salmon is pink, salmon is edible and well iI do not know whether it has fins or not and because iI cannot now recognize that well salmon is a fish that link that connection is lost. ifIf iI had another link over here is- a link then it will bewould have been all right and I all right i could have traversed through that as ais-a link and could have arrived at that answer salmon has fins. butBut as the structure lies now well iI can only say salmon eats something called fish which has fins. butBut whether salmon has fins or not is what iI do not know. inIn fact it has become really a meaningless thing that fish eat animal and it is not always the case but you see how by changing the relations the total meaning changes and what everwhatever iI was being able to do earlier in this case iI cannot do any further because of the missing link here. all right sSo the important thing to state is that, whether we can represent the meanings and concepts in the form of semantic networks, yes the answer is yes we can do it but the meaning lies in the structure as well as the levels of the arcs and those are thethat is what are the relations that we are giving now. we We are talking of concepts, we have just mentioned about propositions salmon eats fish that i’s a proposition, ostrich cannot fly that i’s a proposition. ifIf you recall we had encountered proposition earlier while discussing logic. so So, is there any relationship or they are absolutely two different things? can Can we represent what everwhatever we are stating in semantic nets through logic? aAnd what everwhatever we are stating through logical propositions can they be represented in semantic nets? All these are that is a very natural questions. so letLet u’s look at this example. suppose Suppose (Refer Slide Time 42:27)  

this semantic net where we have got four concept nodes person is a mammal, person has part head, and Ttom is a person. nowNow, here you see Ttom is a specific person. all right  
so till now person is a category or class. thoseThose of you who are familiar with object oriented systems, the concepts of objects, classes, sub classes we will find a similarity here with the concept nodes that we are discussing. butBut let me state here that the concepts of objects and object based design, object oriented systems came much later and hais been systemized much later. theseThese network concept of representation meaning representation of meaning dates back to nineteen sixties1960s as we have said and so this is a predecessor. andAnd it has been certainly much systemized later. but nevertheless  hereHere if we had talked about object oriented terminology then tTom is an instance of the class person. but person could have see Iif iI have added a node here man is a person, woman is a person then man woman are still classes iI am not talking of a particular man or particular woman but Ttom is a particular person and through this is- a links in semantic nets iI am not to be able to in general through a is a link i cannot make this distinction between whether it i’s an instance or a class. Hhowever that is that we will discuss much later here you see that Ttom is a person, person is a mammal and person has part head. soSo, if we just look at this part iI can answer some questions. doesDoes tom have a head? yesYes iI can answer Ttom is a person and person has part head yes so Ttom has head. andAnd what everwhatever were the attributes of the mammal iI could have also talked about those attributes to be valid for Tom.tom nowNow the same structure can be represented in logic where iI just use theise relations as the predicates. Iis a person mammal is a person mammal so it i’s a binary predicate with two attributes. Now, in logic iI can say instance Ttom person that Ttom is a instance of a person, has part person head. 
If you take any two paso you see each of these partitions and any two if we take and take the relation connecting them iI can represent them as propositions as we do in logic. now if we now let’s pause for a second and revise what we have just now discussed weWe started with the point that we want to represent the concepts and the meaning because that is essential to represent knowledge. andAnd we have seen that concepts can be restored or represented in a form of a hierarchical network and using this idea the semantic net representation came into being where the nodes can represent it consists of nodes and arcs and the nodes represent concepts, entities, events, values and the arcs represent different attributes. okay thatThat’ is how we can represent different facts that we want to state or model about the real world. that we want to model nowNow let u’s see how we can representsee some more facts in semantic nets. suppose Suppose there is a game being played between two teams all right Norwich and spurs Spurs. nowNow one team has is a winner and another is a looser  loser. (Refer Slide Time 46:45)     

nowNow how do you represent that fact that Norwich defeated spurs Spurs?say  iI just want to say Norwich was the winner spurs is the looser. then what i need to do that Norwich is a winner of what? so So winner of a particular game which iI have just named here G5 five and what is G5? five G5 five is a game and any game will have a winner and a looser so Norwich is the winner and spurs Spurs is the looser. all right Nnow, if iI want to state it was a football match and that so if i want to state that Norwich defeated spurs Spurs with a score it was a football match then 3 for 1i then I  have to add another attribute score three 3 and 1.one so in that let’Let us forget about what i’s written below let’s forget let’s start firstand look at this white thing. nowNow what can we say about this? we We can say that well G5 five iwas a game that was played in some particular ground, iI could have even stated G5 five played at, iI could have added a particular arc over here saying that it was played at some particular ground football ground and the attribute G5 five is the a game and it has got a winner and looser so if i look at iI can say that in the game G5 five Norwich was the winner and spurs Spurs is the looser. and And if iI look at this iI can also say that the result of game G5 five was was 3 1.three one now typicallyTypically earlier what we have seen is, when we take these two nodes and look at the relation between them what we represent is a two place predicate a predicate is- a which can take two parameters. butBut often we may need to represent facts with more than two parameters. forFor example, Norwich spurs Spurs, Norwich defeated spurs Spurs with a score three one3 1 or the score of Norwich over sSpurs was 3 1three one. nowNow this is a three place predicate, now how do you do this? inIn order to do this if iI had just taken these two if i had just taken these two iI would have said spurs Spurs was the looserloser in game G5. five ifIf iI had just taken these two what would i have said iI would have said Norwich was the winner in game G5 five. ifIf iI had said what is G5 five? iI would have looked into these two nodes and would have said G5 five is a game. ifIf iI had taken these two iI would have said what is the score of G5 five? theThe score of G5 five is 3 1three one. allAll these were two place predicates. butBut when everwhenever we need to represent three place predicates iI need to create a new node okay this new node hasd to be created otherwise iI could have just said game in this game there is a winner and looser iI could have said spurs iand Norwich.n this butBut the score of different games will differ so iI may need to create a particular node which will specify the additional fact. weWe will see some more examples from here. forFor example, if we look at this point we see that it is a more complicated example which we are trying to show: Jjohn gave Mmary the bookbook; Jjohn gave the book to maryMary. nowNow how do you represent this? weWe have shown that gave is an action, there is a event this giving thethis book to maryMary is a particular event and this event of giving has got an agent as somebody who gives and somebody who receives. anyAny giving will have two requirements. somebodySomebody will have to give and somebody has to receive. who is giving theThe person who is giving is an agent and the person who is receiving is a recipient. so Therefore forthis event which is an event of giving the action is giving, (Refer Slide Time 51:21)  

it has an agent johnJohn and a recipient maryMary. nowNow what is being given? theThe object that is being given is a book. whatWhat is that? aIt is a particular book. johnI gave mary you see i did am not saying johnJohn gave maryMary a book but Ii am saying johnJohn gave maryMary the book so iI am talking of a particular book so iI can say johnJohn gave maryMary this event this giving event has got an object iI can give you the pen, iI can give you the money so giving is an action which has got a giver, which has got a recipient and also some object that is being given. okay thereThere areis more objects of it like how it is being given, all right when it is being given etc. but i am not going into those because i those are not required by this sentence johnJohn gave maryMary the book. so So the object that is being given iI could have said book but why didn’t i sI did not say it is a book because book is a general class and we are talking of a particular book that is book sixty nine69 and what is this book sixty nine69? itIt is an instance of book. soSo you see how this English statement johnJohn gave maryMary the book simply translates to this nice semantic net form. nowNow here we have got these where all these are entities, event johnJohn maryMary book sixty nine69, book is a class, giving is a class, these are concepts concept of giving. iI could have written over here given also but this particular node is gave because it’s tense is past which is a type of giving wheren the tense is past etcetera and Ii can make it more detailed therefore so  this is one example. so we will just conclude today (Refer Slide Time 53:15)  

let us by giving another example and will proceed from this point onwards in the next lecture to show how the inferences are done so first let’s look at another example; suppose Suppose we are asked to build a semantic net hat represents the following knowledge. weWe have just now shown that given a particular semantic net how that can be represented in the form of logic. nowNow what we are showing in this example is, given some logical statements can we draw a semantic net with that? soSo let u’s look at these statements; Mman Marcus that means Marcus is a man, married Marcus Madonna so Marcus is married to Madonna so these are you see propositions. gaveGave to Madonna Marcus Measles that means Marcus was infected by the disease measles because Madonna had it so Marcus got it from Madonna. all right soSo this infection we are explaining through this gave to Madonna, Madonna gave to Marcus measles. so you seeThese are  this is a three place predicates. soSo these three statements so it’sis a story. man Marcus is a man, Marcus married Madonna and you know what happened was Marcus Madonna Marcus was infected with measles by Madonna. nowNow how do iI represent this story in the form of a semantic net? you see here Marcus is an instance of man so iI take this sentence man Marcus, Marcus is an instance of man which is a concept, Madonna is another instance, Marcus is married to Madonna so you see the second statement is also covered and third thing is giving. nowNow Marcus and Madonna married and they probably gave so many gifts to each other may be so many other i mean other things were there but this giving measles is a particular instance of giving. soSo iI cannot directly relate it to the concept giving it is a particular giving so therefore i have iI can denote it with a particular giving G17 seventeen is an instance of give action. Madonna also gave a book to Marcus all right so Ii cannot just say all the give actions are connected to this. thisThis is a particular give action. Nnow this particular give action has got what was given? think given of course  mMeasles was given but of course it is a disease but here we are simplifying itthis so thing is measles. andAnd who gave? when ever there is a giving who gave Madonna gave and who got? Marcus got.  so you see nowNow iI can look at the earlier presentation here and answer iI can understand many things from these three logical statements and the same thing iI can understand (Refer Slide Time 56:29)  

from this semantic net structure that is show  about here.


 so today we stop at this point and the next lecture we will start from this point onwards and we will Later on let us see some more representation of semantic nets, how semantic nets can be used to represent more complex concepts and how we can infer using semantic nets and that will be followed by other knowledge representation schemes. so we will stop here today            

Artificial Intelligence
Prof. Anupam Basu

Lecture 19

Reasoning in 
Semantic Net
(56:40)

In the last lecture we saw a new knowledge representation scheme called semantic net. We introduced the basic concept of semantic net in that lecture and prior to that we have seen that knowledge can also be represented using rules. There are some differences between the rule based representation and the semantic net representation. In semantic nets the word concepts are represented as nodes and the relationships between the different concepts are denoted as edges between the two nodes. Now there are some varieties of such representations. Any entity or any concept we represent from the world can be represented as a node. The relationships can be of different types but most important relationship that is occurring in semantic nets is for example, cat is a mammal, tom is a boy. Now essentially it turns out to be very similar to a graph or a network of concepts and relationships. But as you will recollect that no knowledge representation scheme is worth its name unless we have a mechanism using which we can reason and infer new facts of answer questions and really do some useful work of inference. Only in that case if we can do those then we can certify that to be a valid knowledge representation scheme. In the last lecture what we discussed is just the representation the graphical structure and we also gave a couple of examples. Today (Refer Slide Time 04:00)  
we will be extending that discussion and will include in our discussion how semantic nets can be used to reason so that we can infer new things or can answer questions. Today our objective is to look into reasoning in semantic net. Here is one more example, a new example which is little different from what we did in the last class. In this case we are trying to represent the heights of two people. Now Chris is one person and John is another person so the heights might be in meters or whatever but these are the values. but these are again in semantic nets we are not making any distinction between the types of the concepts and these two concepts are linked by the relationship height therefore it is standing to represent the sentence the height of Chris is 1.9 and the height of john is 1.6. These are two independent statements, height of Chris is 1.9 and height of John is 1.6 but if we want to state that the height of Chris is greater than height of John in that case we will have to add some extra nodes to designate that concept. So you can see how semantic nets can be extended. Here we can see that we are now bringing in two other concepts, these are constants 1.9 and 1.6, now the height of Chris is H1 and the height of John is H2 and the value of H1 is 1.9 and the value of H2 is 1.6 so we have already brought in one new relationship called value over here. Now (Refer Slide Time 05:50)  

for the sake of comparison what we can do is we can establish another new relationship between H1 and H2 and state that H1 is greater than H2. So, from this augmented semantic net now we can say that height of Chris is H1 and the height of John is H2 and H1 is greater than H2 therefore I can compare the height of Chris and John. But you can se that in order to achieve this we had to incorporate extra nodes for the concept as well as its value. We had to add two new nodes H1 and H2 and also we had to incorporate another relationship called value. At this point of time let us try to see how we can utilize semantic nets to reason the basic mechanism of reasoning using semantic nets is the concept of inheritance. Before going into the reasoning mechanism let us have a look at what we mean by inheritance. (Refer Slide Time 07:38)  

Inheritance is a word that is very common in our vocabulary. People inherit property from their parents, from their grandparents etc so it is getting some things from their predecessors parents etc. In other words we can extend it for example, if we say cat then cat is a concept and we say cat is a mammal then cat must share or inherit some properties that mammal has. Now the concept mammal will have some properties so cat being a mammal cat will also inherit some properties of mammal. So, using this idea AI researchers have refined the basic notion of inheritance called a specialized inference technique for representing properties of classes. We have got the notion of classes and we have got the notion of objects. You will find very close similarity and this is no magic that there is a similarity; they are actually the same concept being refined over years of research. So, if we think of some class and in the earlier example as I said mammal is a class, mammal designates a class of animal including human beings and cat is a subclass of mammal. Since it is a subclass of mammal it inherits some basic properties of mammal. In addition it can have some specialized features which cats have but human beings also being mammals do not have. For example we have got two legs whereas cats have four legs but mammals have got some feature that mammals drink milk so that property is inherited both by cats and human beings. So there is a notion of classes and subclasses in the inheritance mechanism. The is-a relationship as we defined in semantic nets is nothing but a class subclass relationship. Cat is a mammal so cat is a subclass of mammal. We want to represent properties of classes and exceptions to inherited properties. Some times the mechanism of inheritance will also allow us to make exceptions to the properties inherited because all the properties that are there in the class may not be there in subclass. It is also possible that one particular concept or one particular class will have multiple parents so there can be multiple super classes. Each concept can have a structure within itself and the elements of that structure can have specific relationships among them. That is what is known as inheritance. Here are some examples.
(Refer Slide Time 11:52)        

as we know in semantic nets the most powerful relationships or the most common relationship we use is the is-a relationship. In order to reason in semantic nets how this is-a relationship can be used? Semantic nets allow us to define relations between objects including class relations. For example X is a Y. The first step is we can define this class subclass relationship X is a Y so X is a subclass of Y. The only restricted inference supported in semantic nets is the methods that is based on inheritance. So here is an example, Fido is a dog and dogs have four legs so we can infer Fido has four legs. Now, if you just think a while you could have also represented this using logic. So how did it look? Fido is a dog and dogs have four legs so Fido has four legs. That can very well be inferred using our well known Modus ponens method, Fido is a dog and dogs have four legs that means I am writing in propositional form. If dog Fido then has four legs Fido. Therefore if I say this is P and this is P implies Q then according to modus ponens if you recall you would have inferred Q using this and Q is nothing but has four legs Fido. So you could have inferred this using logic also or using rules. (Refer Slide Time 15:05)        

But these sorts of notations are not very common to our experience and it has been introduced by logicians as a very powerful tool to handle.. But using semantic nets we can achieve the same thing. For example, if I just try to draw the semantic nets equivalent to this we could see that, here is a concept dog and dog number of legs is equal to four and Fido is a dog. Now this picture very clearly shows the relationship Fido is a dog and the number of legs of dog is four. So now from here we can inherit, you see how inheritance can be used. Since Fido is a dog and dog has number of legs to be four therefore Fido will also have number of legs to be four. Now this property of number of legs has not been stored with Fido because I could have had for example Jim as another dog. Now I need not store the fact that Fido has four legs Jim has four legs. If I had to store that all the time if I had to store this here again Jim has four legs then that would be redundant storage which is not require. On the other hand I can very well infer from this structure in a logical way. So where is the inheritance? The inheritance is here that Fido being a dog is inheriting the property of number of legs being four to it therefore we can infer it. But there are some exceptions to it. For example, if now somebody says that Fido is a lame dog then might be Fido has got three legs and not four legs. therefore whenever we infer something using inheritance that means from the parent we extract that and use that information that is fine unless anything is known to the contrary. So, as we go down the hierarchy as we go down to the specialized cases like Fido if there is an exception then I could have said Fido has three legs. If nothing is told I will infer Fido has four legs because Fido is a dog. That is why in the earlier case we said that there can be exceptions to such inheritances. 
Here is another example semantic net. (Refer Slide Time 18:51)  

Here instead of is-a I have used specifically the relation subclass because by now you know that is-a essentially denoting a subclass. So we have got a root node animal and animal has a path head. Reptile is a subclass of animal, mammal is a subclass of animal, elephant is a subclass of mammal. Now elephant size is large and elephant lives in Africa so if I club together each of these relationships are equivalent to some sentence and the meaning of the sentence is very clear. That is why the name semantic which means meaning is used here, the word semantic net is present. For example, we can say if I cluster these two nodes together what it means? It means reptile is an animal, mammal is a type of animal, elephant is a mammal, elephant lives in Africa, size of elephant is large. And now Nellie is a instance of elephant so Nellie is a particular elephant just like Fido was a particular dog Nellie is a particular elephant and we could have done it with is-a and now if we say is-a then there is a problem. That is why we need to specify it a little bit. If I say all these are is-a then Nellie is a elephant or is an elephant and what would that mean? That would mean, is Nellie a class of white elephants which are rather small or which have got three tusks or something of that sort? Is it another variety of an elephant or is it a specific elephant? Now merely if we use the relation is-a that is not very much clear. Therefore is-a is being substituted here by two distinct relations; one is instance and one is subclass. There is a class, there can be subclasses of classes and there can be objects belonging to these classes and these objects are nothing but instances like Tom is a instance of a boy, boy is a subclass of person or subclass of male etc. So here instead of is-a we have used another relationship called instance so Nellie is an instance of elephant, Nellie is a particular elephant and Nellie likes apples. Now over here if we try to ask some questions as where Nellie lives we write a procedure that will search through this graph and we first start with Nellie because the query itself asks where Nellie lives. But right now we will search here for an answer and find whether it is lying at this level or whether there is any relation with Nellie saying Nellie lives in somewhere, but it is not available. Then we think, is Nellie a instance of some other class or is it a subclass of something? We find that Nellie is an instance of elephant so although I did not find the answer here I go one level up and see whether the answer is available then I can say Nellie is an instance of elephant and elephant lives in Africa so my answer will be Nellie lives in Africa. Now, if the question was asked does Nellie have a head then let us see how we can do it? We would have to start all over again from Nellie. Now, Nellie is an instance of an elephant and here we do not find any answer to this because here we do not find any relationship called as has part so I do not find the answer here whether it has got a head or not. I go up one level high because elephant is a subclass of mammal and I search for this and I do not have an answer here so I again go up to animal and there I find that animal has a part head therefore Nellie has a part head. (Refer Slide Time 24:45)  

Therefore how did we infer? We inferred by traversing the semantic network and with that we found that Nellie has a head and that was purely by inheritance. We inherited the property of Nellie having a head. The property was much higher and that has been inherited along this arc to Nellie. And also we can find that certain concepts are related in certain ways like apples and elephants. How are apples and elephants related? Nellie likes apples but it really does not say whether all elephants like apples. So, as we go up it is a generalization and as we go down the hierarchy it is a specialization. Although Nellie is an instance of elephant and is a particular elephant Nellie likes apple but that does not mean all other elephants will like apple. But meaning of the semantic network was not always well defined. Even in this case all the meanings are not very much clear from this description we have given. For example, we cannot really say are all elephants large or just typical elephants are large. Probably in the real world may be all elephants may not be very large but as we have done over here it is apprehend that we will infer all elephants to be large we have got no other option here. Do elephants live in the same Africa are some complicated meanings which you may like to ponder on. When we really write a program that will perform such inferences we cannot allow such ambiguities and we will have to do away with such ambiguities. So, in general I hope it is clear how the mechanism of inheritance works. To summarize a subclass can inherit the properties of the parent class. Now there can be instances or subclasses now instances or subclasses may have some additional properties which may be changed. For example, somebody inherits small house from his parent and then he can change it to a larger partner. Similarly the specializations or the subclasses can override or change the basic properties from those that were there with the ancestors. Now it is necessary (Refer Slide Time 28:06)  

to distinguish between two very important categories of properties. One is the default property and another is the generic property. Properties are always inherited for example Fido is a dog, dog is a animal but this is not a grammatical error because we are talking of is-a relationship, in English it should be dog is a animal. So hence Fido is a animal this is inherited. These properties are generic to the super class the original class and whatever is there in the original class if that property is generic then that value will be inherited and we do not change that. There can be three types of properties you can think of namely generic, default and other. Now generic properties are not usually changed unless there are some special circumstances. For example, if we have mammals then we know mammals are warm blooded species. Therefore for anything namely cat, dog, may be even human beings for that matter the generic property will be inherited. Besides that there can be default properties. What is the meaning of default property? Default means if nothing is known to the contrary inherit the value. Here is an example, if nothing is known to the contrary, I know Fido is a dog and suppose dog has a part tail. (Refer Slide Time 30:03)  

Therefore what should we infer? We will have to infer that Fido has tail so there is no exception to this. If we have this that dog has tail and I have got Fido is a dog then I can say that Fido has tail. Now this is the property link, has part is the property link and tail is the value of that property. Now Fido is a dog and has part tail is fine. Therefore if I look at Fido then I will inherit this and I will say it also has part tail. But if it is said Fido is a specific dog, now this is is-a but this is-a is an instance. Now this Fido being an instance of dog if we know that there is a special case that Fido’s tail has been chopped off then this does not hold. Therefore I inherited this part by default because nothing was known to the contrary. If anything is known to the contrary that will override whereas in the case of generic systems or generic properties we do not have this scope of overriding because that is generic to that class and any subclass will have to inherit that. So, under normal circumstance you would have inferred that Fido has part tail. Now if we know more details of this that Fido’s tail has been chopped off then we would have inferred Fido does not have tail. So (Refer Slide Time 32:33)  

we can supersede default values if we know more details of a subclass or in an instance the default value inherited can be overwritten. Now here is a classical example; birds, we know birds can fly, we know birds have feather birds are covered with feather and we also know that penguin is a bird, we know ostrich is a bird, we know eagle is a bird, sparrow is a bird. Now if we think of sparrow, sparrow is a subclass. Now here is a question, is sparrow an instance or a subclass? Obviously sparrow is a subclass of birds, it is not an instance because I can have ten sparrows in my cage and each of them is distinct. So sparrow is a subclass of birds will inherit the property has feather because all birds have feather. Now we also know birds can fly therefore we can also infer sparrow can fly because sparrow is a bird. Now, has feather is something common but can fly is not that generic because if we take the example of ostrich then we will see that ostrich cannot fly but ostrich has feathers. So ostrich being another subclass of birds will inherit the property of has feathers and will also automatically by mechanism it will inherit the property can fly but because we know something to the contrary specific to the subclass ostrich that will be overwritten and we will infer that ostrich cannot fly, so here is that example. Ostrich is a bird and by default we infer that ostrich can fly. But if at that ostrich subclass we know that it cannot fly then we modify the inherited inference to ostrich cannot fly. Now this is a very important and powerful means of reasoning and this one can be discussed in a little more detail. Two types of reasoning we often talk about; one is monotonic reasoning. Monotonic reasoning means whatever we infer at a particular point of time given some facts will hold. So anything that is inferred at some point of time T will not be invalidated by the arrival of some more information at a later point of time. So, if we go along this timeline and at this point we are inferring some x = 5 for example and now if I add this point of time which is coming then later I cannot invalidate that reasoning, it is not like a program. Instead of this we say we infer at this point that Tom is a man. The arrival of other facts is being inferred at this point of time during the reasoning process but never will revise this. We will add on new facts here but this will not be revised. on the other hand the other type of reasoning called non monotonic reasoning which means whatever I have inferred at this point of time with the arrival of new facts or information that I am generating this can be changed, it is possible to invalidate that and you will see that this default reasoning is nothing but an example of the same. Here we have done exactly the same thing that we had made an inference here but after that as we came to know more facts about ostrich we could override that. Now these default values can be overwritten but generic values cannot be overwritten. If we just think of a procedural form, suppose you want to write an inference machine if we want to use semantic nets as a valid knowledge representation scheme then we must have an inference mechanism for that. Therefore we will have to write a procedural code which will work on the semantic net and we will be able to infer new facts. Now what we want is we want to find the value (Refer Slide Time 39:00)  

of a attribute for an object. What is that object? For example, Nellie is an object or Fido is an object or may be mammal is an object. Now you want to know whether mammal has part head or Nellie lives in Africa. That means if I have a query like does Nellie live in Africa then in that case what I am trying to do is I am going to that particular object Nellie and I am trying to see whether it has got any relationship lives and if such a relationship exists there then is the value of that link is leading to another value which is Africa, that is how we go about. So essentially in order to answer the queries whatever queries you form we can form that in the form of an attributes value that we are seeking. For example, if we take an earlier example like has part, does elephant has head, so basically what we are trying to do is we are looking at the object elephant and we are trying to see whether it has got an attribute has part which is leading to head. Therefore in this case we are setting a list L to contain the node corresponding to O. We are setting a list linked list sort of which is containing the node corresponding to the object O and we are setting the current node to be the next node from L because we have to traverse that over a link. If there is an arc labeled a the attribute we are seeking then it points to the answer. We get the answer if we get it over there. Otherwise what I have to do is I will have to traverse that network. So in order to do that we will add all the nodes to L pointed to by the instance or subclass arcs coming from the current node. For example here I want to have some I have got a query where does Nellie live, now this question where does Nellie live can be translated in the form of a semantic net like I have got Nellie lives in some value which is not known, I could have put a query or could have put x so what is the value? I want to instantiate this particular value; I want to find the value of this value from the net. So this is my query semantic net you can think of as if it is a template with which I am going around in the semantic net and I am trying to find the best match. Now what would have happened? With this query I start my journey along this semantic net. Here unfortunately I do not find the match because here I am not finding any arc coming out of Nellie so I am making a list with Nellie and it is pointing to all the arcs which are connected by is-a or instance, subclass or instance type of relationships. So right now in that node we have got only elephant so now we come to this elephant and as we come to this elephant what are the next possible places to go? In that list I add mammal and if I do not find the answer here I have to go up and look into mammal whether that lives in link can be answered. (Refer Slide Time 45:11)  

 now here as i go in I find that the lives in link is available here and so my answer is Africa. if my query was does Nellie have head for example a funny query but then I would have started from here I would not have found the answer here but I would have gone up and again i would not have found the answer here so the link to the list would have included this node and would have gone up in this way till I find this answer. Let us again go back to that algorithm. If we do not find an answer we add to L all nodes pointed to by instance or subclass arcs coming from the current node. and if L is not empty that means there is a possibility of exploration we will set the current node and we will look into that current node to br the next node and will see if there is an arc labeled a at that point. This is the search mechanism we use in semantic net. Now here is another example; what is the hair color of Marcus? Now let us look at this semantic net what does it say? We have seen a similar example in the last lecture, man is a class; hair color of man is black is it generic or is it default? It is default because we are assuming that there are people with hair color brown and other types. Marcus is an instance of man and Marcus is married to Madonna. This is a semantic net, Marcus gave Madonna measles was an example we already saw. Now if in this case we want to answer the question what is the hair color of Marcus? So obviously the net that will be is Marcus with an edge hair color x which we do not know. And I make Marcus my current node, now no arc is coming out of Marcus with the relation hair color. But there is instance or subclass, I am not traversing this at all the pointers I am putting into the next possible nodes are either the instance or the subclass type of relationship. So here we take man and we find that man has got a hair color therefore Marcus is inheriting the answer and the hair color of Marcus is black. That is how (Refer Slide Time 48:30)  

we carry out inferencing in semantic nets. But there is a major problem over here and that is known as the case of multiple inheritance what is this multiple inheritance? The problem will occur if an object inherits the same attribute from two different classes. For example, here you see Fred is an instance of pet bird and is an instance of ostrich. So Fred is an ostrich that has been kept as a pet. Now ostrich has got the property locomotion is walking so ostrich walks. Pet bird is a subclass of bird and its locomotion is flying. Now Fred is inheriting from two subclasses ostrich and pet bird. Now how do we infer if we say does Fred fly how do you answer? Or rather in terms of this net if my query is formed as what is the locomotion of Fred? Now I can say I start with this I do not find any locomotion edge from here so I can go up this stream and come to ostrich and find that ostrich has got a locomotion edge so I infer walk. But what if I had chosen Fred to be an instance of pet bird and what would my algorithm lead to? I would have come here and here I do not find an edge therefore I have got no way of going down this level so I go up and there I find locomotion fly so I could have inferred that Fred flies so there can be different answers as to how to handle with this. One thing is suppose you know about different search mechanisms and if I had done depth first search then I would have either gone to this answer or to this answer. But if I had done breadth first search then I would have got both the answers such as fly through this path and walk through this path. Now my common sense would have told me that whatever is more specific is more correct. So what is more specific here is walk so that is one way of handling this situation. So, an inference technique option in such a scenario is something like this. To deal with multiple inheritance we need to add something (Refer Slide Time 50:06)  

to determine which class will take precedence. For example, in this case we can say that well pet bird will have precedence over ostrich or ostrich will have precedence over pet bird depending on the particular application you can think. You can decide on some policy to do that. in general we want the most specific class to be used so we could simply use the path length because the notion of inferential distance can also be explored so it could be something like this that the path length of fly is 1 2 3 or 2 at least 1 2 and the path length of walk is 1 so which ever path length is less is more specific and that is more valid we could have taken that. Now here there is no hard and fast answer and depending on the application often we have to select a particular policy. But still there can be always problems like here is another example; Rita a girl is an instance of teacher, Rita is a teacher and Rita is also a student. Rita teaches in a school and also goes to a B. Ed class or some postgraduate class where she is a student. Now the personality of Rita is confident when she is as a teacher in the class but the personality of Rita is rather shaky when she is a student. Now, if we have to answer the question (Refer Slide Time 52:00)  

is Rita confident or shaky it is really difficult because the path length will not lead us anywhere. And what is the personality of Rita? For this we are getting two different answers at the same distance so that problem will remain. So this is a problem of semantic nets and this is one of the problems, multiple inheritance has always been a problem and it is a very practical problem and there has been practical solutions to this. Now semantic nets as we have discussed is a very important basic tool. From semantic nets we will go to frames which were proposed by Minsky and frames can be considered as the predecessor of the object oriented systems and we will see how the notion of semantic nets has been applied in frames. Here is a quiz, draw a semantic net structure that depicts the following world knowledge. Birds and mammals are both animals, animals can walk, cats are animals and eat rodents. House cats are varieties of cats which eat mice. Morris is a house cat. Birds can fly. Penguin, sparrow, eagle are all birds. Penguins can also swim. Fred is a penguin. (Refer Slide Time 54:15)  

In that way I have just put in a number of facts. Depict it in the form of a semantic net and you will also find that it is possible to add on more sentences to this and enrich the semantic net. 

Before concluding this lecture let us quickly summarize what additional things we have seen in this lecture. We have first of all made a distinction; till now we were just talking of instance relationship but there can be subclass and instance relationship. And the major inferencing mechanism of semantic net is through inheritance mechanism. Now when we inherit the properties from the super class, inheritance mechanism means the properties of the super class is inherited by the subclass and the instances. But there are two different types of properties; generic properties and default properties. In the case of default properties if nothing is known to the contrary then the default values will hold but if at a lower level a specialized level the subclass level there is some overriding fact that will dominate and the default value will not hold. And the algorithm of inferencing is to make a template of query semantic net and try to find the best match by searching through the links in the semantic net and while we do this search what is natural is either the instance or the subclass links are traversed. And we can answer so many questions, if we represent the world facts in the form of semantic nets it is still being used in different forms and the frames and object oriented systems are two different products of the semantic net links. 

Artificial Intelligence
Prof. Anupam Basu

Lecture 20

Frames
(59:51)

inIn this lecture today we will start the discussions on a new knowledge representation scheme that is called frames. Ttill now we have looked into othersaw two very important knowledge representation schemes; one One is rules which is also known as production rules and based on production rules we have seen rule based systems and we have also seen how the inferencing is done in rule baseds systems. secondlySecondly we have seen semantic nets as a very fundamental knowledge representation methods. andAnd in the last two lectures we have also looked into the inferencing method that is adopted in the case of semantic nets. todayToday we will be discussing another knowledge representation scheme which is a little more systematic but fundamentally it has got many concepts in common with that of semantic nets. andAnd this knowledge representation scheme that we are will be discussing today is known as frames. frames are theThe systems which are based on theise frames are known as frame based systems. thisThis system was propounded proposed by a famous computer scientist and artificial intelligence researcher Marvin MLinsky and he proposed this scheme in order to represent common sense reasoning. ifIf you have noted in the in case of rules we have to enumerate all the possibilities to describe a particular state of affairs in it’s antecedent and if they match then it will fire the rule and the actions corresponding to the consequent will be taken. howHow ever this rule based system Rule Based System is also known as production rules and one school of thought stated that this is the way the human beings represent their knowledge and they take rule based decisions. (Refer Slide Time 04:00)  
 Hhow ever Marvin MLinsky and other A I decisions by our common sense we do really always fire the rules and match all the antecedents in order to see which rule is applicable in a particular scenario. thatThat was appearing very unnatural to them and so they thought of an alternative representation knowledge representation mechanism and consequently the frame system was proposed. nowNow what is a frame? aA frame let’s first look at the definition a frame is a prototype of a concept. aA very important point over here is prototype. all right aA prototype it is not the exact description of the concept we will see how what is meant by the prototypebut it is basically the schema the structure okay of a particular concept and what are there it denotes the attributes of the concept. everyEvery concept that we talk of, for example if we take the concept ball okay if we take the concept ball now there are so many attributes that we can specify about the ball. mayMay be it’s the color, it’s shape, it’s size or the diameter, may be the weight etc and so on and so forth. nowNow this shape in order to be a ball will be obviously spherical or round. all right mayMay be the material may vary, whether it’ is a cricket ball or whether it is a football or what ever. soSo these are the different attributes that are associated with this concept ball. now obviously you can see one of these attributes like whileWhile the other attributes can vary one of the attributes like the shape has to be round in order to be qualified as a ball as you see. okay  so So thatere is a particular importance that this attribute shape has. but But right now this issues we will be discussing later right now i would like to illustrate what concept now i note another thing that iI am not talking of a particular ball, this is the a concept ball, iI can have ten 10 different balls may be five 5 footballs, two 2 cricket balls etcetera but those are different instances of the same concept. similarlySimilarly we can define a particular concept like bird. (Refer Slide Time 06:30)  

so ifIf iI define a concept bird then i think you can start thinking of the different attributes of the bird may be the type or the name of the bird. nowNow you see when iI say the name of the bird iI am not talking of the particular name that iI can give to my pet bird but iI am talking of a particular species of the bird like say may be sparrow . similarlySimilarly there can be color of the bird the size of the bird okay and so on and so forth, now these are the attributes. okay anotherAnother attribute is can it fly? nowNow you can wonder that why all birds fly but that i’s not the case. forFor example, ostrich is the a bird that does not fly. so So, can it fly can have an answer yes or no. the color can be blue, can be yellow, the name can be sparrow, can be parrot so all right now you see the once that i am putting now putting in thisthese rectangles I am drawing here are these are the attributes of this particular concept and the other ones which iI am underlining are the different values. say yes all these things are so similarlySimilarly size can be any real number so these are the values but we are now considered about considering the attributes. soSo if iI want to describe the concept bird then that particular concept has associated with the it a number of attributes. all right so now let’s go back to this description

 aA frame is a prototype of a concept which will describe the attributes of the concept and by that iI mean the class of objects or concepts to which the concept in questions belongs. all right one thing that i have not said till now and some more things nowNow what we mean by this the class of objects or concepts to which the concept in question belongs. say iI again take the concept bird or say iI take the concept all right say the boy, now boy is a concept it has got a number of attributes which we have already given some examples of. nowNow besides this the boy suppose there are there is a class called human beings. nowNow boy is a human being and right similarly girl is another concept which has got which has got a set of attributes of itselfit but this is again related to another class another category human beings. Now, all boys are human beings, all girls are human beings, may be all men are human beings so all mothers are human beings and mothers have got their specific attributes. inIn order to call a lady that she is a mother obviously we look at some attributes. right andAnd again mother is a human being. Now and what is important over here is that if iI think of this human being to be a class then the concept boy is a subclass of this class boy this class human. soSo here is a super class and these are subclasses of the super class. okay so butBut also denote it’s class relationships that it is related to which super class it belongs to. which super class soSo if we just take a set theoretic view of the whole thing then we can also say that say human beings is a superset (Refer Slide Time 12:04)        

and boy belongs to that set. anyAny boy will belong to that set so this class boy is belonging to theis set. girlGirl is another concept which is a subclass of the class human beings or all the girls are subsets of thisthese class human beings as well. okay soSo that i’s what is meant over here the that the class of objects or concepts to which the concept in question belongs. theThe concept in question in our discussion was a boy or a girl okay and the class of objects class of objects or concepts like human beings to which this concept belongs. and will see that there are some more things theThe next important thing that we like to define is an instance. nowNow it is very important that we must be able to differentiate between an instance and a frame instance of a frame and a frame orand instance of a class or the class. Nnow iI will be using this terms class and instance versus frame where an and instance of a frame interchangeably because this has got a very close relationship with the object oriented definitions that we are using presently which is very popular now-a-days in different design domains and also in programming languages. andAnd instance of a frame is a representation of a specific object a particular object all right and that is the instantiation of a frame. nowNow let u’s look at this say let’s take this example; fidoFido is a dog, look at this white onesthese white ones and the yellow ones, fidoFido is a particular dog and dog is a concept. so So we have got the concept dog of and fidoFido is a particular dog. So  all right now fidoFido is aan instance of the class dog. now againAgain we can look at it in the way we are drawing. forFor example, now iI am making aa hierarchy and it’s all right (Refer Slide Time 15:01)  

now animals, now these animals  it is a class and animals can be mammals or those who are not mammals and and dog is a mammal so all these arcs iI can say dog is a mammal, mammal all mammals are animals so any mammal is an animal, we very frequently use this  i think you are familiar with this relation is is-a relation because we have very frequently used this in the case of semantic nets. now if we would like to again represent this okay before that sSo dog is a class because iI am not talking of any particular dog, all right mammals is a super class of dog because dog is a mammal. similarlySimilarly cat is a mammal so cat is also a mammal so that i’s another class. Therefore it is so it’s a super class of dog, cat etcetera and animals it is a super class of mammals. all right therefore so animals so Tthere can be different animals and some of them are mammals and again among different mammals there may be dogs, cats and other things. thereThere is a sub category which is a dog because there are some attributes which really distinguish a dog from a cat. okay andAnd if we say fidoFido is a particular dog, that is iI am talking of a particular dog and there may be other dogs which are also which are all dogs say may be Jjimmy is another dog. all right you see  iI could have made a further classification of these like you can say that iI can further break this up (Refer Slide Time 18:01)           

by doing it like this theat dog mammal then comes dog and then among them iI can say Spaniards, now what is a Spaniard? Spaniard is a class it i’s a subclass of dog, the Spaniard is a subclass of dog right so this is again a class. nowNow if iI say say Jjimmy is a Spaniard then jimmyJimmy is aan instance it’s a instance of relationship all right instance of a Spaniard. so in that way soSo it i’s a class and all Spaniard is also a dog.term  nowNow when we are talking of jimmyJimmy then we are talking of a particular element of this class. soSo the same thing we can draw in the form of a set so like the largest set is animal. inIn this there is a subset which is mammal in which there are talks dogs or there may be cats and there can be Spaniards and all those. nowNow when iI talk of jimmyJimmy and iI talk of fidoFido they are different elements of this set. soSo, in terms of the set theoretic notation we you can understand that this each of these elements areis particular instances of a class and that class or the particular set. nowNow anything that belongs to this set obviously belongs to this set. orOr if iI say dog and jimmyJimmy is aan instance of dog and dog is a mammal now this is a is a relation sorry this is aan instance relationship and this is a is- a relationship. right soSo jimmyJimmy is aan instance of dog and dog being a mammal jimmyJimmy is also a mammal so this is a transitive relation. okay so i hope now you have understood the difference between a instance and a frame instance This is the difference between instance of a class and the class itself. nowNow mby frame will be denoting the class and by instances of the frame will denote the instances. Now so now we can give you some quick examples but before that let u’s see how can wewe can represent the concept the notion of the feature dog. so look at this theThe description of a frame is very much like the structure data type that we use in the language C. 
so you see a frame consists of a number of slots. eachEach of these attributes dog is a attribute concept which has got a number of attributes. so what are theseThe attributes are: number of legs, type of teeth, has tail, color of skin, owner etc.etera okay  nowNow this slotsthese slots can have values and the values can be of different types. so So iI represent the frame or the concept, the frame representing the concept dog as a set of attributes okay it reads dog is a mammal that means it is saying that is a dogthe concept dog is connected to the concept mammal or the class dog is connected to the classuse mammal through the is- a relationship which we are showing besides that these attributes like number of legs, type of teeth, has tail, color of skin, (Refer Slide Time 23:02)  

owner etceteraetc. now you see Tthese are the slot names and theise slots can have values which are called the slot values. youYou may also note that the different values that this slotthese slots can have are not all of the same type. for example, dog is a mammal so this is a string a class name, number of legs it’sis  an integer, type of teeth is again a string, has teeth is a Boolean yes or no, color of skin will again be string, owner will be string and there can be owner of the dog which can be the string. now here you see this is a very important point look at this attribute has tail now whenWhen we are describing dog we are saying what does this mean dogs the frame dog it’s slot has tail has a value yes what does it mean itwhich  means dog has tail. butBut obviously there can be dogs which have got their tails cut we often see such dogs in different places. right thenThen is it the case that all dogs have tails? hereHere what we are saying is this value is a special type of value which is known as a default value that is which will discuss soon after that is unless we know that the tail of the dog has been chopped off we can safely assume and we inthen using our common sense we can assume that if it is a dog then obviously it has a tail. soSo through this description iI really build up the concept of the dog and iI need not fire ten rules in order to infer that dog has a tail. theThe notion of frame is that we it’s hypothesized it is felt by some people that we have different when we learn different concepts along with that we also learn the attributes of the concept and the different values of those attributes. okay so we al so So unless we know something different we have seen so many dogs from our childhood so and we have learnt from our basic science book that dog is a mammal and we have seen and learnt that the number of legs is four so as soon as the word dog is uttered all these things are activated in our mind. okay nowNow this owner iI could not put a value over here because dogs may have owners except for street dogs might be but for the concept dog there is no particular owner that is tied to it. right  forFor a particular dog it might have some owner. all right now weWe again describe fidoFido. nowNow what is fidoFido? fidoFido is a particular dog therefore fidoFido is an instance of the concept dog. so letLet u’s see how the values slot values have changed. fidoFido is a dog so iI could have written here also fidoFido is instance of dog that would have been more correct. Nnow number of legs is four because fidoFido is a dog so it has got number of legs four, type of teeth sharp, has tail yes, color of skin black, owner here, owner iI can put the value Rahim Rahim and even earlier also the color of skin was black because iI cannot attach any particular value of to the color of the skin of the concept dog. okay Nnow you see here now when iI describe this that is automtomatic so it is a constant and dog is a mammal all right and dog had attributes like type of teeth it wasand the value was sharp (Refer Slide Time 28:02)  

i would not use the so the value of teeth is sharpand the number of legs four, color of skin iI did not put any value, owner iI did not put any value. okay nowNow i say iI select another color say iand say that fidoFido is an instance of dog. nowNow when iI say this automtomatically iI specify many things without specifying them with fidoFido. hHere iI did not i do not need to specify that fidoFido has four legs, fidoFido’s teeth is sharp okay because since fidoFido is an instance of dog some of theise attributes which are specified will automtomatically be inherited in the instance. youYou are familiar with this term inheritance through our discussion on semantic nets. attributes which are not specified in the class definition so theseThese are automatics so these attributes like teeth, number of legs these are automtomatically inherited from here from the class definition. i did not specify those attribu Those attributes like owner which is not specified in the parent class or the color of skin which was not specified in the parent class can be specified here. iI can say it is black and owner may be Rahim. 

nowNow this is the advantage one very 

important advantage of frames and this advantage is coming is derived from the inheritance that wewe have just now shown. andAnd this is known as thisa advantage is a very important advantage for any knowledge representation is the economy of representation. when  (Refer Slide Time 30:57)    

i mean ifEven if iI can infer many things and yet iI did need not explicitly specify it all the time hence  then obviouslyobviously it i’s an advantage. The economy of representation is that and that is iI am specifying less what but iI can infer more that is the economy of representation and that’s and this seems to be  always attractive. okay so you see that allAll these need not be specified but they are inherited automtomatically and iI can build up a C structure like this with thiese values. anyAny slot value any slot which is not which does not have a value or we say that the slot is not instantiated that it means that this slot has not got a particular value as yet we will infer that to be unknown.  okay nowNow we come to a common notion which we have seen earlier that is the concept of object attribute value. we have whileWhile discussing rule based system Rule Based System and also while discussing aboutand semantic nets we had mentioned that we can represent many facts of the real life we can represent using this object attribute value triple. soSo an instance of a frame is denoting an object like frames like a dog. theThe slots are denoting the attributes of the object and the slots can be of different types as we have seen and the slots are instantiated with values. So, if we go up, here you see if iI just make a statement fidoFido is a dog and that is automtomatic here it is represented here because the object name is fidoFido, attribute name is- a and the value is dog. soSo you can also see how equivalently it translates into a proposition. say ifIf iI take this object or instance and take this attribute and take this value what would be the equivalent proposition? can you tell that is ifIf iI write in the well known propositional logic we will say number of legs of fidoFido is four right so you see that every object and we have seen that the slots can be of different types and the slots are instantiated with values. nowNow let us quickly look at the correlation between semantic nets and frames. hereHere is a semantic net that we had showedn earlier in the last lecture. Nellie now inIn semantic net we draw like this everything is like oval. so Nellie is a elephant, elephant is a animal, animal here look at this dotted edge in semantic nets we do not we draw everything in the same way, animal iI have intentionally not written thiese attributes because iI wish hope you will be able to say what this attribute is, animal and head how are they related? iI can say animal has part head, elephant in and Africa how are they related? theyThey are related with elephant found at or lives in Africa. and Nellie is a elephant, elephant is a animal. now if this information supposeSuppose iI label this edgesthese edges and if this information is to be translated to frames then what would be the class and what would be the instance? now please note it down because basedBased on this we will be now drawing the frame or we will show just what the slots are. nowNow if iI look at elephant it is a class all right so elephant is a what elephant is a according to this semantic net and so these are class relationships all right and there are other attributes and also elephant (Refer Slide Time 36:20)  

found in found in Africa, all right elephant has part head. all right now you see iI have intentionally used two different colors so here you see the class is a name of the instance the class is denoted by the frame name, the relations of the semantic net are the actually the slot names and we have got the slot values. all right soSo if we have Nellie which is an instance Nellie we will say it i’s an instance of elephant all right and other attributes. soSo if we go back here you see that the concepts are translating to either the frame or the instance of a frame Nellie is aan instance and the relations are translating to the slot names and we are putting in the slot values. nowNow this distinction was not there in the case of semantic nets. so we can now say thatNow what are slots? slots denotes attributes, attributes are typed because there can be different types, some values are defined in the frame definition and already instantiated like we have seen that in the case of fidoFido which and the dog that some are already known, some values are defined in the instances only, instantiated slots are defining facts or propositions which we have seen and which are usedthis may be used to answer queries. now this part i would like to mention say ifIf iI look at this instance then iI can answer the query if the question is put does fidoFido has tail? the answer will be iI will come to fidoFido iI will go to the has tail slot and iI will look at the value if there is no value iI will say unknown but okay if there is a value iI will say yes. whoWho is the owner of fidoFido ?who is the owner of fido soSo iI look at the object, what is the object? It is fidoFido and what is the attribute that iI am questioning? It is the  owner so iI come to this owner slot and what is the value of that? yes so it’s It is rahim Rahim and so therefore my answer to the question is that the owner of fidoFido is rahim Rahim. soSo this instances or this representation helps us to answer queries. (Refer Slide Time 40:02)  

nextNext ask the question what are there in a slot? obviously Obviously in a slot there will be values, there will be types which will say what type here iI am introducing two new things which you do not know that is constraints over possible values. iI cannot allow any value not only type there may be a range beyond which iI cannot put in the value. we will see examples of that alsoAlso we put in some predicates now these predicates are also known as daemons, they are actually functions. (Refer Slide Time 41:29)  

 ifIf you compare it with object oriented systems, let’ have a quick comparison with object oriented systems those of who we basically know that there are concepts of classes and objects of a class. nowNow if iI make a mapping then iI can say in an object oriented system object oriented system and side by side we haveI look at the frame based system 
so iI have got classes in object oriented systems and they are mapping to frames there is a mapping. iI have got objects in object oriented system and that is mapping to instances in the case of object oriented systems. over and above that those of you who are familiar withIn object oriented systems the objects communicate using messages and when aan object receives a message then it fires or activates some methods and these methods are equivalent to these daemons all right or these are also used as predicates. okay soSo there is a one to one correspondence between these. okay so slots can be now this is anotherAnother point is slots can be structured or there can be frames within the a particular slot and how do we denote that we will show with an example that we can use pointers to another frame. letLet us look at this example; now boy bBoy is a human being so is- a is a slot name and human being is a value. nameName is a string, now you see here for the boy the name part is not instantiated but the type is specified that what ever values it have it may have should be the a string, age will be an integer. now you see hereHere in order that iI can really call somebody a boy the age must be less than or equal to fifteen 15, height type is float and height should be less than seven 7 otherwise that i’s a spurious or wrong value that is coming, stays at i am sorry this stays at int all right no stays at it should be okay studies will be string here so there are no constraints over here.
now here you see Ttype is a pointer to result so result is another frame now that means what if iI need grades or the c g p then for that boy there is a grade and the grade can be computed from the results what ever marks he has got based on that iI can find out whether he gets a or b or c so it is a pointer and pointer to another frame which is known as results and that’ is also possible and that’ is what is meant by this last line slots can be structured that means within a slot iI can have a frame. theThe next thing is in addition to theise types Ii can put in  predicates. nowNow what are these predicates? thereThere are two types of predicates in general namely  if added and if needed. i will come back to this in a second now there are two types of predicates say general (Refer Slide Time 44:37)  

if needed and if added ifIf needed predicate state how to find compute a value. soSo if you need a particular value to be computed for some inference you will have to that particular predicate will tell you how you can compute it. andAnd if added predicates are used to check the constraints on the values that can be used for consistency checking also. soSo you see if iI need grades here is along with the slot i have put in a predicate. Iif needed that if at any particular point of time iI need this for inference iI will fire this function iI will activate this function compute c g p a a may be with parameter boy and results because iI will go to the result frame and will compute the compute the c g p a. suppose Suppose from some other inferencing iI get the value of height and the height is 7.5 “seven point five” feet that will violate the constraint that has been put in because if some value is put in iI want to check whether it i’s the correct value or not so iI have got a constraint here. (Refer Slide Time 46:01)  

soSo, if the height is added then iI have to check this constraint. all right nowNow let’ us look at tomTom. tomTom is again an instance of boy, name is tomTom smith, age is twelve s12 soee all these things are instantiated, height is five 5, stays at say fifteen15 park avenue, studies in bright academy, grades now grades iI will say if needed compute the c g p a of tomTom from results of tomTom. okay so letLet us quickly recapitulate what we have said. iIn a slot we have got the slot names, along with every slot we have got it’s type, we have got some constraints, the slots can be instantiated with values, we can have constraints with the slots, also we can have predicates with the slots, two types of predicates if needed and if added. Let us see how if needed predicates are very much we will see how they are usused for inferencing. okay nowNow, before going in to the inferencing we will quickly talk about two types of values. thereThere are some things called generic values. genericGeneric values arelike tomTom don’t drink milkmilk; do can we get the answer from this description of tomTom? No, there is no slot of tomTom so how we carry on inferencing. say iI have got a question does tomTom drink milk? iI do n’ot find any attribute about drinking over here but iI know i try instance of, so what would iI do? now say iI have got an instance tomTom, all right now here iI am pointing in the inference part also now tomTom has got so many attributes but drinking milk is not there. butBut iI know tomTom is an instance of boy all right now iI can go to boy and see if there are some attributes which are talking about some drinking. now unfortunatelyUnfortunately in the description of boy that we had shownwe showed there was no such attribute so we go on (Refer Slide Time 48:44)  

we go up and we find that boy is a baby human being and human beings drink milk because they are mammals.s drink say milk nowNow, all human beings drink milk all human beings drink milk because they are mammals. soSo this is a generic value to this of this class generic property of this class. thereforeTherefore so any property that is holding for all elements of this class is known as a generic property. so So although we do not know whether tomTom drinks milk and since Tom is an instance of a boy i will try the inference machine will try whether since tom is an instance of a boy whether boy drinks milk,  know the answer is not there so iI go up here and see that human beings boy is a human being and human being drinks milk. nowNow this information is true for all human beings so that is a generic value. all right now soSo what is the generic value? a generic value tomTom is an instance of a boy and also boy is a human being so in the description of human being we may have human beings drink milk andso since all human beings drink milk the answer can therefore be inherited and inferred through inheritance. this weWe have also seen this for semantic nets. i hope this is clear genericGeneric values are the values which will hold through for all sub classes. okay next now similarlySimilarly you can try, tomTom has a pen does he need ink? supposeSuppose tomTom has a pen does he need ink? nowNow how do you answer that?it now inIn the description of tomTom it is not whethere that tomTom needs ink but it iss just said tomTom has a pen. so we can thereforeTherefore we can go to the class pen and find out that pen needs ink. thereforeTherefore we inherit the answer down the line and we infer that tomTom needs ink. okay besidesBesides generic value there is a very important type of does tomTom drink write? againAgain the answer is not in the description of tomTom. whatWhat ever we have described for tomTom it was not there in that through which and derives hand he writes, it was not there right so what can we do? weWe will suppose in the description of boy we have another slot writes with right hand. now iI have put a star here to show that this value is a different type of value what is that? Iin general that is unless anything is known to the contrary (Refer Slide Time 52:27)  

iI will assume that a boy writes with right hand but it may not be true so theise type of values which we assume since we are not told that tomTom is left handed we will assume that tomTom writes with right hand. thisThis is known as the default value that we assume in the absence of any more information. nowNow again you can see that the default is with right hand. but if thereThere is a difference between generic values and default values, if while describing tomTom iI get some additional information that tomTom is left handed obviously the default value will be overwrittenridden. howHow ever the generic values cannot be overwritten if it is overwritten the generic values are tightly connected to the class, if the generic value is changed in the class definition changes. okay now so we that inheritanceInheritance is a very powerful mechanism and and i will briefly talk about inheritance because we have seen inheritance it so many times in semantic nets. theThe known values can be inherited from the parent’s unless more specific information is available with the child. i will just inIn the super class or the parent the generic values and the default values are specified. in the super class theThe generic values are retained and the default values may be overwritten asridden that we have already said seen. Multiple inheritance is one issue is there which is little difficult to solve. multiple inheritance thatThat is a class or instance may be a child of multiple parents. nowNow what do we need? infer forFor example say iI take define a class teacher politician. aA teacher politician is a teacher as well as a politician. nowNow for a teacher the top priority is qualification whereas for a politician the top priority is the vote bag. nowNow iI say Mr. xX is a teacher politician. soSo what is the top priority of Mr. xX? itIt is very difficult to say and thereso we have to take some policy and depending on the context we may decide which property which parent’s property we will inherit. okay now similarly you can see another example; say s scooter iI can say automtomobile and two wheeler and two wheelers have number of wheels two, automtomobile is four so if iI say scooter then how many wheels does scooter have? but But this is a wrong organization, this problem should not have occurred, we the correct organization should have been this: automobile two wheeler is a automtomobile, scooter is a two wheeler in that case when iI try to answer the query how many wheels does the scooter have iI will go to the top the first one and will try to inherit this. soSo what is the inference mechanism in frames? theThe inference mechanism is simple, look for a candidate frame or instances, look for the availability of values if the value is available
(Refer Slide Time 55:52)  

then resolve else go to the preceding level and so on if solved at a higher level try to have a more specific answer. nowNow iI will just illustrate this with this, say suppose how many wheels does scooter have? (Refer Slide Time 56:06)  

iI do n’ot find that value as an attribute but since iI do not specify everything everywhere iI go up one and find that two wheeler scooter is a two wheeler and it has got two wheels therefore iI infer by inheritance that scooter has two wheels. ifIf this was not specified iI would have gone one level higher and it would have taken the next the higher level information inferencing. howHow ever this is just to give you the overview of frames. thereThere are some subtleties in frame based system. (Refer Slide Time 56:43)  

there are so theseThese are very important in answering queries for example say with which hand does virendar Virendar sehwag Sehwag bat? now Now sehwag Sehwag has been described with ana batting average value that attribute is here. nNow sehwag Sehwag is the opener well here but here Ii do n’ot have anything saying with which hand did he bats, iI go up and opener is a batsmean and in batsmen I can say batsmen there is a default value is specified that is right handed and iI can specify that sehwag Sehwag is right handed. inIn the same way frames can be used for classification of objects (Refer Slide Time 59:04)    

and there are many other applications. 

butBut for this lecture iI think this is sufficient and iI will encourage you to describe different scenarios that cars automtomobiles or objects that you see around and try to make frame based representation and also utilize the examples that has been given in the frames.  

Transcripted by: R. Anjanaa lakshimi 
{Total slide time: 51:57}

Indian Institute Ofof Technology
Kharagpur

Artificial Intelligence
Prof. Sudeshna Sarkar
Department of Computer Science and Engineering, I.I.T., Kharagpur

Lecture #21
Planning - 1

 todayToday we will start on a new module. 
earlierEarlier we had looked at state space search and we also have looked at logic;
((propositional)) calculus and first order calculus.
todayToday we will look at a new module which comprises  planning problems.
soSo today we will give an introduction to the planning problem.
 andAnd in the subsequent lectures we will look at different algorithms
 which (( )) efficiently solve- the planning problem.
 
(fig., 01:15)
theThe instructional objective of today's class is 
 the student will get an introduction to planning problems.
givenGiven a situation which ((models)) planning 
they should be able to formulate the planning problems in terms off  
the representation formalisms that we will discuss.
todayToday we will look at the strips formalisms as well as situation calculus and 
certain other techniques. 
theThe students should be able to cast planning problems as search problems
and they should be able to cast it as a normal state space search problems.
butBut we will also explore why state space search solution techniques are 
not ideally suited for solving planning problems and we will discuss what other techniques can be used for searching in the planned space.
soSo this is a schematic overview of the intelligent agent that we discussed in the beginning of this course.				[Refer slide time: 02:35]
 
(fig., 02:33)
ifIf you remember this is the agent, this is the environment, 
 the agent interacts with the environment.
for simplicity initially we will assume atAt least for this course 
we will assume that the environment is static and the environment is fully observable.
theThe agent receives perceptions from the environment through it's sensory operators
and we assume that this perception is perfect.
thatThat is the environment is fully observable to the agent
 and the agent can take actions through it's ((effectors)) 
w which can change the state of the ((environment.))
 weWe will consider only deterministic actions. 
inIn the planning problem we will assume that the agent has a goal or goals 
that the agent wishes to achieve.
andAnd given the goals the agent wants to find a sequence of actions 
 that would let him or her achieve the goals.
andAnd the sort of question that we will ask is
g given the current situation what is the next action or 
what is the next action sequence that agent should take?	[Refer slide time: 04:18]
so inIn planning problems the task of the agent is to 
synthesize goal directed behavior.
andAnd the agent will have a goal which the agent wishes to 
 
(fig., 04:04)
the agenth as a goal which the agent wishes to achieve
 and the objective of the agent is to select a sequence of actions.
letLet us say a1 one a2 two a 3three up to a n
 so that as a result of executing the actions in this sequence 
 the agent will get to a state where the goal is achieved.
andAnd while determining the action sequence 
 the agent would have to consider the casual dependencies 
which betweenmean the different actions.
and as i mentioned for today's class and forFor our planning lectures 
we will primarily restrict ourselves to deterministic and fully observable environments.
thereThere are other types of planning problems where the environment may be ((stochastic)), 
the environment may be only partially observable,
 the actions may have non-deterministic effects
 and in such situations in stochastic environment 
it may not be possible or it may not be the best situation for the agent to plan 
 a sequence of actions. ratherRather the agent might like to determine a policy
 so that he can determine his current action and based on the feedback received for the next state the agent can decide the subsequent actions.
butBut for the deterministic and fully observable situations 
 to which we will confine ourselves today 
the solution to the planning problem would be a sequence of actions 
that achieve the goal.
nowNow let us try to understand why planning is important 
and where planning situations arise.
 thisThis course is about intelligent agents and intelligent agents need to operate in the world and to operate in the world the agent should be able to take actions.
theThe agent should be able to take actions which are appropriate for the situation 
that is at hand.
-itThe agent should also be able to take actions that change the state of the world
soso that the world achieves the goals th- the agent wishes to achieve 
they can be achieved in the world.
andAnd additionally the agent will like to compose more than one(( )) actions or a set of actions together to achieve complex goals.			[Refer slide time: 06:55]
 
(fig., 06:45)
so inIn order to change the world to suit the needs 
 agents will need to know ((but)) need to know the behavior of the world
 they should they need to know how the world changes as a consequence of the different actions that can be taken in the world.
andAnd reasoning from this the agent should be able to formulate a plan 
to take to achieve it's goals. andAnd in this –case 
while dealing with planning the agent needs to know about 
needs to reason about dynamic environments because 
 as the agent is taking an action the action can or 
is usually changing the state of the world. 
soTherefore in the dynamically changing world the agent has to reason about 
the effects of it's actionual sequence.
 
(fig., 07:54)
letLet us  lookus look at ((a))a very simple example
o-  every day sit-  example of a planning problem that
we have to often tackle.
 actuallyActually a lot of problems that we tackle in our everyday activity or in the long term- goals are planning problems.
forFor example, suppose my plan is to have a birthday party in my house tomorrow
and suppose the current situation is that iI am at home
t, there is flour at home, there is no butter, there is no sugar
a and I i wa- want to find a sequence of actions so theor plan 
so that iI can hold a birthday party tomorrow.
andAnd this may be the sequence of actions that iI wish- to perform
 that would lead to the realization of that code.
iI invite friends, iI go out and buy butter, iI go out and buy sugar,
 iI buy balloons, iI decorate the house, iI bake the cake and so on.
soSo this is a sequence of actions that iI carry out which realizes the objective 
that iI have in mind.
agentsAgents are robots and often they need to plan.
theThe-ir planning is involved in a lot of everyday activities that we do.
forFor example, if you want to navigate  and if i want to come to get to this room
 iI have to plan my- how iI would come to this place starting from my office
o or if iI want to deliver this course iI have to plan the sequence of lecture materials that iI will deliver to achieve the objective of teaching this course.
soSo planning is a part of everyday activities like communication, like navigation 
as well as many other activities.			[Refer slide time: 10:00]
letLet us look at some applications of planning; mobile robots.
inIn fact  mobilefact mobile robots where one of the initial goals of A I 
 
(fig., 10:18)
and mobile robots need to do planning on a very large scale 
 for navigation, for task planning etc. 
Hence different types of planning ((poor audio))are involved for mobile robots.
autonomousAutonomous agents also need to planning.
forFor example, NASA uses a deep space planning agent. 
weWe have earlier discussed about the different robots rovers that have been sent to
as have (( )) to space to mMars and these agents are involved in autonomous planning.
inIn simulated environments goal directed agents are needed for training,
for playing games, 
 in web and grid environments planning is needed for composing queries or services,
 to deliver a particular service.
inIn education or tutoring system planning is required to decide  
what material to deliver next to the student. 
planningPlanning is also used in scheduling problems with where
where we have to make choices of action as well as we have to handle different resources.
Hubble Space Telescope scheduler is an example of a very useful system
 
(fig., 11:18)
which uses planning.
actuallyActually now- a- days planning and scheduling go hand in hand.
then ((works)) forWorkflow management is a place where we need planning.
planningPlanning is beingas been  used in software test case generation,
 in plan based interfaces and many such applications.	[Refer slide time: 11:44]
S[] so let us see what is the essence of a planning problem.
inIn a planning problem we are given a way to describe the world
 
(fig., 11:44)
we are given a description of the initial state of the world
and we are given athe description of what the goal state should achieve.
andAnd our objective is to find a set of possible actions 
which when executed in sequence will change the world so that a goal can be achieved.
orOr rather we are given the initial state description, the goal description 
and the description of a set of possible actions that are there in the world
and we also know the effects of this actions. andAnd our objective is to find a sequence of a- actions or prescribe a sequence of actions that will change the initial state and take the agent to awards state where the goal will be achieved.
now planning problem thereThere is a lot of logical structure in the way a  (( ))
planning problem is formulated. andAnd our objective in designing planning algorithms
 is to design algorithms that take advantage of the logical structure of the problem instruction description and e in order to be able to do so
 we need to be able to represent the problem in a suitable logical language.
now let us see  very common followingFollowing a very common formalism of representation of planning problems 
((let us))let us see how the states, the actions, goals etcetera are represented.
soSo , in a planning problem typically states are modeled
 in terms of propositions or state variables or features of a state.
soSo we have different propositions and in a particular state  
these propositions have different binary values.
forFor example, suppose the propositions are; at home,  
havelf sugar, havelf butter, halfve money etcetera and  	[Refer slide time: 14:08]
the values of thiese propositions may be true or false in a given state.
forFor thean initial state we have a complete proposition description of the state.
 
(fig., 13:18)
we know which- are the propositions which are true in the initial state.
andAnd typically we have a partial description of the goal state.
weWe want to know whthoseat are the propositions that we want 
 should should be true in the goal state. 
soSo a state is typically represented as a conjunction of positive literals. 
soSo we have to specify the propositions which are true in a particular state
this will fully represent the description of a state.
forFor example, a state could  becould be lighted and hot and made of brass.
weWe can also use so we can use propositions as a conjunction of propositions to represent a state.
weWe can also use first order literals. 
forFor example we could use at robot one 1 kitchen and at robot two 2 garden.
nowNow notice that this is our predicate, at is a predicate 
but the arguments are ground literals. 
soSo we allow first order  literals to be used to describe the a state
 but these literals ((must)) be ground literals and they must be functioned free.-
 
(fig., 16:25)
soSo literals – must be ground and function free
 which means that we cannot use things like  at home –sorry at father Fred home. 
so thisThese sort of  firstof first order literals are not allowed because here we have a function. 
weWe also do not allow quantifications that is we do not allow at x home
where x is a variable which is either existentially qualified or (( )) quantified or for all quantified.
soSo these sort of first order literals are not allowed but we can have ground first order literals as in at robot one 1 kitchen.
so inIn a simple formalism we allow states to be represented as a conjunction of propositions or a conjunction of first order literals or a mixture of them.
nowNow there could be a large number of literals (( ))or propositions that are part of the planning problem.
weWe may specify the values of some of the literals.
forFor example, we may specify that lighted is true, hotter is true, made of brass is true.
butBut we may not have specified the values of the (( ))twenty other literals that occur in this planning problem or hundred other literals that occur in this planning problem.
inIn a closed world assumption we will assume that 
any condition that we have not mentioned are assumed false.
soSo in a closed world assumption we will say that whichever literals we have specifically stated to be true in a particular state will be true but 
 the other literals will all be false.
howHow is a goalld represented? 					[Refer slide time: 17:48]
 
(fig., 17:48)
a goldA goal is represented as a partially specified state.
forFor example, rich and famous and stays in Mumbai. 
thisThis is a restriction description of a goal. 
weWe say that a state s satisfies a goal g 
 if the state s contains all the propositions of goal g.
sSouppose the description of the state s is uh lighted and hot and made of brass
so this state satisfies the goal lighted and hot
because the propositions lighted and the proposition hot both occur in the goal state.
ifIf we take another example hmmm 
iif we say that rich and famous and miserable and stays ((in)) Mumbai and acts 
acts in movie M1, one 
 So that state will satisfy the goal rich and famous and stays in Mumbai.
so forFor a goal to satisfy a state all the propositions in the goal must be in the description of the state. 
 
(fig., 19;02)
so when we model planning problems 
 states are modeled in terms of  propositions
as we said complete initial state and partial goal state.
nowNow we will look at how actions are modeled. 
so Actions are typically modeled as state transformations. 
 To specify an action we specify what transformation to the state space 
 will be brought about by the execution of an action.
So, to represent actions different formalisms ((have been)) used. 
Today we will discuss about STRIPS which is a classical formalism for describing actions. 
We will later have  later describe A D L which is a more modeled and language which is less descriptive language which can
 
(fig., 20:03)
which has better expressive(( )) power than the STRIPS representation formalism.
so inIn STRIPS actions are specified by specifying two things.
oneOne is the precondition of the action 
 the other is the effect of the action.
theThe precondition of an action means the condition that must hold 
 so that the action can be executed.
(( )) sSuppose iI want to pick up an object
 the precondition is that my hand must be empty and 
the object which I wish to pick up must be  portable 
 and must be available in front of me.
soTherefore preconditions denote those conditions that must be satisfied 
 so that the action can be executed. 
secondlySecondly we need to  describeto describe the effects of an action.
whatWhat will happen, how will the state change as a result of taking the action?
now supposeSuppose the agent is in a state where certain propositions are true and when the agent executes an action the state may change. 
theThe change can be described by using two different things. 
someSome of the propositions which were true earlier would not be true 
 after the action is executed and some new propositions which may have been false earlier would be true when the action is executed.
soSo suppose let us take an example of an action fly
w which takes three arguments fly p from to. 
theThe precondition is that at p from can fly p airport from and airport to. 
so ifIf these four conditions are satisfied in the previous state 
then only this action can be executed.
andAnd ((poor audio))as a result of executing this action at p from will(( )) no longer be ttrue
but at p to will be now true in the next state.
soSo the effect of an action is described by a conjunction of positive and negative literals.
negativeNegative literals correspond to those propositions which have to be deleted 
from the previous state to get the current state.
theThe positive literals denote those propositions that w- will have to be added 
to the new state.
Now, the rest the other assumption that we will make is that the rest of the propositions that we have not specified in the preconditioning- sorry
in the effect list 					[Refer slide time: 22:48]
t those propositions their status will be unchanged.
ifIf the other propositions were true in the earlier state
 
(fig., 23:02)
they will continue to be true in the current state and
 if they were false in the earlier state they will continue to be false in the next state.
nowNow look at the action fly p from to.
nowNow p from to these are not ground literals but these are variables.
soSo fly p from to represents an action schema. 
anAn action schema can represent a number of different actions.
forFor example it can represent the action fly plane one Kolkata Delhi,
it can represent the action fly plane one 1 Kolkata Chennai,
it can represent the action fly plane one Kolkata Delhi, 
fly plane one 1 Delhi Kolkata fly plane one Delhi Chennai, 
fly plane two 2 Kolkata Delhi and so on.
so this actions (( ))Therefore these actions schemata can represent a class of actions and number of different actions.
theThe precondition of an action is a conjunction of function free positive literals.
theyThey denote what must be true in a state so that the action can be executed in that state.
thisThis action fly p from to can only be executed if can fly p is true,
p is at from, from is in airport, to is in airport. Thereforethen only
 these are the conjunctions conditions that needs to be true so that the action can be executed.
allAll actions cannot be executed at this state.
forFor example, if p is at from iI cannot execute the action fly p to from in the current state.
[breathing] asAs a result of the action the effect of the action is specified by the effect (( ))clause.
theThe effect says not at p from and at p to.
soSo the effect is a conjunction of function free literals not positive
 so this is a mistake. 
soSo, effect is a conjunction of function free literals which may contain a set of positive literals and a set of negative literals. andAnd 
they together represent how this state changes when the action is executed.
ifIf (( ))I have a negative literal it means that proposition is to be removed from the s- previous state to get the current state.
ifIf we have a positive literal it means that proposition has to be added to the new state.
nowNow actions represent change in the world and action changes the world. 
when we consider weWe want to know how an action will alter the world.
weWe also want to keep track of the history of the world states and answer questions about potential world states and i
inn order to do that we need to reason about actions.	[Refer slide time: 26:25]
nowNow you have studied  firststudied first order  porder p-redicate calculus earlier in this course.
 todayToday we will briefly talk about situation calculus which is a a formalismformalism 
 in first order predicate- calculus which allows us to reason about time and reason about situations.
 
(fig., 25:55)
 
(fig., 26:33)
situationSituation calculus was first discussed by McCarthy. 
itIt is a a formalismformalism to model dynamic worlds within first order logic.
soSo situation calculus is a formalism to model dynamic worlds. 
theThe key idea in situation calculus is that we represent the snapshot of the world
which we call- a- situation explicitly.
andAnd these situations index the different states of the world.
theThe states of the world change with time.
initially supposeSuppose the world is in a situation s0 zero which has a particular state
 andthen when the agent takes some action as a result the state changes and  
s1 one i becomes the new situation and
and we see that as as a result of taking an action the situation changes.
thatThat is, actions map situations to situations.
 
(fig., 27:33)
Fluents are properties that change from situation to situation.
thereThere are certain propositions which do not  ((poor audio))are
 independent of the situation but there is are certain Fluents which are those that change from situation to situation.			[Refer slide time: 27:48]
soSo in order to specify those propositions 
we need an extra argument to denote the situation. 
hmmm ((for))For example suppose let us take a proposition on,
 suppose we have a blocks world, 
a blocks world is a world where we are dealing with several blocks.
andAnd our objective is that suppose we have a particular configuration of the block
we wish to move these blocks so that we get to some other configuration of the blocks.
so thisThis is the block world. 
inIn block world the typical proposition that we use is 
whether a block is on top of another block.
forFor example we had like to say on A B. 
nowNow, on A B this proposition holds in this state or in this situation 
it does not hold in this situation.
soSo in this case we will specify with an extra argument that on A B,
 
(fig., 29:18)
suppose we call this situation 
suppose we call this situation s1 one
 and we call this situation s2 two 
so we can say on A B in situation s1,  one
onon B A in situation s2 two and not on A B in situation s2. two
soSo, on is a proposition which the whether on A B is true depends on the situation
 so we use a new situation argument in this proposition.
thenThen let us take the proposition clear.
so clearClear is also proposition and which is ve-
so clear is A whether A is clear or not depends on this situation
like in this case A is cleared, 
 clear A in s1 one but A is not clear in s2 two,
 C is clear in s2. one two 
soSo again to continue with the blocks world example 
so  here iI have a situation which we call s0 zero and ((we)) see that 
 in this situation B is cleared.
soSo clear B,  s0 zero on B, C,  s0 zero clear A, s0 zero and this is a robot hand
and we have a proposition corresponding to hand empty 
so hand empty at s0. zero. 
soSo these are t- this is the description of this state
this state in the blocks world.
nowNow let us look at possible actions in the blocks world.
Let us take a particular action pick up. 
soSo the action, pick up x					[Refer slide time: 30:44]
 
(fig., 30:20)
 
(fig., 30:44)
pick up x is the schema is the action schema,
 pick up B is the a particular instance of an action schema.
nowNow suppose the agent executes the action pick up B in the situation s zeros0
so we denote itd by do pick up B s zeros0
 so this action is executed at situation s zeros0.
theThe new situation that is –do pick up B, s zeros0 represents a new situation
 which arises out of executing the action pick up B on situation s zeros0.
soSo do pick up B s zeros0 ((results)) into a new situation s ones1. ok
soSo in the situation s ones1 on B, C will not be true
 so not on B, C in s ones1, 
clear C, s ones1 so cl- clear B will not be true but clear C will hold true in situation s ones1.
soSo s ones1 is a new situation which is a result of executing the action pick up B in situation s zeros0 and the situation s ones1 can be described by clear A clear C
 not hand empty and not clear B and not on B, C.
thisThis is another example suppose the situation s zeros0 corresponds to 
at home s zeros0 color door red.
nowNow notice that this proposition color it's value does not depend on the situation depend on the time. 
soSo for the proposition color we have not added the situation argument.
atAt is a proposition whose value changes depending on the situation.
soSo here we have added a situation argument but for the proposition color we have not added any situation argument.
soSo s zeros0 is described by at home s zeros0 color door red.
nowNow s ones1 is a situation which the agent gets to if the action go store is executed
so s ones1 is a result of oh sorry
so s one is a result of executing the action do go store the
 action go store in the situation s zeros0.		[Refer slide time: 32:29]
 
(fig., 32:29)

And the situation s1 is described by not at home s1 at store s1. So this is the effect of the action go store, not at home and at store. This proposition color door red is not affected by this action go store so if color door red holds in situation s0 it will continue to hold in the situation s1. 

Now in the initial state there coukldcould be, the entire problem descrioptiondescription could have 100s of different propositions and some of wehichwhich hold in the initial state and some of which do njotnot hold in the initial state. Now after every action we get to a state and if we have to describe wheteherwhether each of these propositions are turetrue or false in that state then that would be a very laboriuouslaborious process. 
And we also notice that typiocallytypically an action will only have a local effect, it only changes the value of certain propositions. So the vast amount of background information remain junchangedunchanged
This problem in AI is referred to as the frame problem. 
For example, suppose I am at home and there are different situations and different propositions that hold and gthethe current time.
35:08
For example, my son is at home, the store sells bread, my age is 38, and then so may other things, some of them hold in this state and some of them do not hold in this state. Now suppose I execute the action go store, now what will change is that, at home will no longer hold and at stroestore will hloldhold. However this vast amount of other propositions will continue to have the same value as they had earlier. And in this the background information does not change. This is known as the frame problem in AI.

35:33

And we want an efficient represenatationrepresentation of this franmeframe problem. And if we confine ourselves to the fram,eworkframework of logical reasoning then we haehave to capture the frame problem using a set of rules and these rules can be captuiresdcaptured by a set of actions called the successor state axioms.

So   successor state axioms encoppe the fact that normakllynormally things stay true from one state to the next unless an action changes them.
For example, suppose you consider the proposition at x. noweNow at x x will hold at the next situautonsituation which has a reason out of doing the action A in situation s. so So do A, S is the new situation which the agent finds itself in after carrying out the action A in situation S. at At x holds in this situation if and only if either A eqwualequal to go x if current action is go x then after this action at x will hold or at x will hold if the action is not go Y and the agent is already at x in the previous situation. If the agent is already at x and it does not execute a go action then the agent will continue to be at x. 
37:25
 
Or if the agent does execute the go x action then at the next situation it will be at x otherwise the agent will not be at x. 

37:36
This means that for every action or rather for every proposition that depends on the situation for every fluent we have to have one axiom like this axiom which recalls this successor state axiom. 

So in this case the fluent at x only changes the action go. There could be a fluent which changes as a result of more than one action. Suppose it changes with three different actions the successor state axiom for that fluent would involve all these three different actions. 
Now once we have wrote the problem in this form means we have written the initial states, the action representations and the successor state axioms then we can use theorem proving to find a solution to the planning problem. 
We have als\already seen how theorem proving works. Theorem proving can be used to find a solution to the planning problem. 
38:50  
However theorem proving may not always be a very efficient way of solving a planning problem so what we will do in this class is to explore different direct methods of solving the planning problem. 

So we will first look at strips. Representation for actions in strips which we discussed about earlier, every action is represented by the precondition of the actions which is a list of propositions that have to be true for the action to be executed.
The delete list and the add list so the effects of an action in strips are divided into two different lists. The add list is the list of propositions that become true after the action is executed and the delete list contains the list of propositions that becomes false after the action is executed. 

39:51

In the strips formalism the actions specify all the state variables whose values are changed by the action. But strips places certain restrictions on the preconditions list as well as the add and delete list. 
 Specifically strips does not allow disjunctions in the effect list that is the add or delete list. And strips only allow propositional preconditions at affects. 

We will later see that there are some other languages like ideal which relax this criteria a little bit. So let us look at an example problem; suppose the initial state is described by 40:43 at home, not have banana and not have money. 
 And the goal is have banana and have money and have at home. And the actions available are by x. the preconditions of by x are at store add list is add x. the other action is go X, Y, precondition is at x, delete list is at X, add list is at Y. 

41:18

Initially suppose we are at a situation S when we go from the home to the store we are at a new situation S prime. And in S as I said my son is at home the store sells bread and my age is 38 in the new situation S prime these propositions continue to hold. Now we want to know how we can efficiently represent everything that has not changed. 

Strips provide a good solution for simple actions. Strips just says that apart from those propositions we have listed out in the add list and delete list the rest of the propositions are maintained as it is in the next situation as in the previous situation. 

Now the ramification is that, suppose as a result of executing the action going to the store, suppose I am now in gold bazaar the number of people in the store went by 1 and the contents of my pockets are now in the store. So this is the effect of my action. 

Now do we have to encode all these when we describe the effects of action? 
In strips you do not have to specify all the effects of the action which can be inferred by some other technique. For example, 42:58
the fact that the number of people in the store go up by 1 if I go to the store that can be handled by some rules. 
Primitive facts like at home persist between states unless we specifically change this by executing a go action. 

Inferred facts are not carried over and must be re-inferred. Suppose the store currently has 5 people and if I go to the store the number of people becomes 6.
Slide: 43:40
So these things need not be specifically mentioned in the add and delete list but they can be inferred. And this use of rules avoids making of mistakes. 
Now let us see how planning can be looked upon as a search problem. We are given a representation of the initial state, a set of strips operators and a goal condition as we wish to achieve and a planning problem that we have to determine a sequence of actions which we apply to the initial state yields a state which satisfies the goal. 

44:15

Now we can look upon this as a search problem where the initial state is known to us, the actions or the possible operators that map a state to other states so we can have a state space graph and the goal is satisfied by any state that satisfies the goal. 

Suppose my initial state is at home, not have banana, not have money and the actions that I can execute is let us say go store, go office, go movie, go bank, cook dinner and so on. And as a result of the go store action we will have the initial state which will get transformed to a different state and from this state again different actions can be applied. 
So we see that we have a state space formalism. And we can search in this tree until we get a tree where the goal conditions are satisfied. 
45: 50

45:55
 So the planning problem is actually a search problem. This is another example of the planning problem. We have this initial state in this blocks world where B is on top of C and A is on the table. If we execute the action move A, B then we get to this particular case A is on top of B and B is on top of C if we execute the action move B, A then we come to this state. If we execute the action move B table we come to this state and if from this state we execute the action move C, A then we get to this state. Suppose our goal is represented by on C, A now we notice that in this state on C, A is true. So this is the state which satisfies the goal condition.  

Therefore this is the goal of the planning problem.
The planning problem can be caste as state space search problem and we can detect whether a goal situation has been achieved. 
But the main problem in casting the planning problem such as the state space search problem is that the state space can be very large because the number of actions we can execute in a state may be very large and the state may really explode. 

47:19

The search tree is generally quite large. Also the description of the planning problem suggests some structure of the problem because each action typically only affects all set of propositions. And actions depend on each other. 
Planning algorithms have to be designed to take advantage of this special nature of the representation. 
47:53
Normally we can start from the initial state and we can do a search in the forward direction which is known as forward state space search. 

Forward state space search means you search from what is known in the initial state and apply operators in the order that they are applied. 

And alternative to forward state space search is the backward state space search.
In backward state space search you do the search in the other way. That is, you start from the goal state and you try to find out the actions that would have led to such a state so you consider the actions in the reverse and the states in the reverse direction.

So you search from the description of the goal and identify the actions that help you to reach the goal. 
48:50

In forward search the branching factor can be extremely high. All applicable actions have to be considered from each state and typically they may include many actions which are irrelevant. 
Suppose my objective is to bake a cake, now going to a movie is an action which I can perform at the initial state but whose effects are irrelevant to the goal of baking a cake. Therefore if we have a forward search typically search could be very inefficient unless we use a very accurate heuristic. 

If we do backward search starting from the goal this allows us to consider relevant actions. 
49:39
However in backward search also we can have some actions which are irrelevant in the sense that we could not have arrived at the [ ]49:47 conditions starting from the initial state we have. Actually backward search is also called regression planars.
Planars which do backward search are called regression planars.

In contrast to this the forward search planars are called progression planars. So regression planars typically have smaller branching factor and for most problems they have a more focused search space than planars that search in the forward direction. However both backward and forward search may lead to the consideration of a lot of irrelevant action. 50:29

So if you really want to make the planning problem efficient you need to deploy very good heuristics. 

50:49

The questions for today’s lecture is, 
1)	Explain how planning systems differ from classical search techniques.
2)	Formulate the blocks world planning problem. 
Take a particular description of the blocks world problem, take a description of the goal state and represent the initial state, represent the goal state, list out the possible actions in the blocks world. For each action you give the precondition list and effect list and then you can take this example problem and draw the search space corresponding to this problem. 

Transcripted by: R. Anjanaa lakshimi 
{Total slide time: 55:37}

Indian Institute Of Technology
Kharagpur
Artificial Intelligence
Prof. Sudeshna Sarkar
Department of Computer Science and Engineering, I.I.T., Kharagpur

Lecture #22
Planning - 2

In the last class we had a first introduction to planning systems.
today'sToday’s class would be the second lecture on planning. 
 
(fig., 00:35)
huh theThe instructional objectives of today's lectures are the following:
weWe will look at into a little more detail into of forward search 
as used for planning problems, forward search or progression. 
thenThen we will look at backward search or regression.
weWe will also look at the planning algorithms used in the STRIPS system.
andAnd finally we will start with partial order planning the framingework
 and we will explain the basic partial order planning algorithm.
 
(fig., 01:18)
soSo let us huh get back to a quick introduction of the planning problem.
so asAs we have noticed earlier we have an agent which works in an environment.
theThe agent has goals, the agent takes actions and the actions affect the environment,
the agent receives percepts through its's sensors from the environment.
theThe objective of the environment is to plan the next action 
or the next sequence of actions so that it can achieve it's goals.
soSo this is the basic idea of a planning system.	[Refer slide time: 02:02]
 
 (fig., 02:18)
now beforeBefore we get into huh today's lecture
iI (( ))would like to briefly huh discuss the questions that we posed in the last lecture.
theThe first question was, explain how planning systems differ from classical search problems.
you see planningPlanning is also one type of searchsearch.
whatWhat (( )) sets apart from a general search problem is that 
in the case of planning we can decompose the goal
 we do n’ot look at the goal as a whole. 
soSo it is not just that we look to see whether a state satisfies the goal fully or not.
usuallyUsually as we have noted, a goal is described as a conjunction of different conditions
different propositions or conjunction of different literals
and so these can be looked upon sub-goals.
soSo instead of looking at the goal as a whole 
 it may be more useful if we look at the goal as a conjunction of sub-goals
 so that we can reason separately about solving each sub-code
 and then combine the results.
thisThis will make the planning problem easier to solve.
((secondlySecondly)) states are also not indivisibledual entities,
 states are decomposable. soSo states as we have seen is are described by a conjunction of those propositions that hold or that are true in the state.
soSo state is described as a set of propositions. andAnd actions are also 
explained in terms of what propositions must hold at the previous state
 in order to the action to be executed 
that is the preconditions of the actions and effect of the action.
soSo we concentrate on how the action changes the propositions in the state.
soSo this is what the sets a planning problem apart
 or w-hich distinguishes the planning problem from the more general search problems.
[breathing] and the (( ))And the algorithms  for planning thate
 we will consider we will look at theseis look at these characteristics of the planning problem
 in order to devise the right algorithms.		[Refer slide time: 04:29]
 
(fig., 06:45)
so let me briefly discuss now question two that we s- last week
i wanted you to formulateFormulate the blocks world planning problem.
now forFor the blocks world planning problem let us just look at a sample 
of a black blocks world planning problem.
supposeSuppose we have a the following situation: 
weWe have huh two blocks A and B,
 A then B is on top of A and we have a third block C.
soSo this is let us say this is the initial state of the planning problem.
howHow do we express this initial state? onOn table A,  on B, A, clear B,
on table C, clear C.
soSo the initial description of the initial state of the planning problem can be given as 
conjunction of on table A and on B, A and clear B and clear C 
{faintly} sorry 
and clear C and on table C.
so thisThis is how we can describe a particular initial state 
in terms of the propositions on table on and clear.
Tthen we have to specify a description of the goal state.
supposeSuppose in the goal state we need to only give a partial specification then  
for example Ii can say that B should be on top of C. ok
soSo this as we see is a partial specification of a goal state and there could be several states which satisfy this condition.
forFor example, this is a state which satisfies the goal condition B is on top of C, 
this is another state which satisfies the goal state,
t there are several states which can satisfy 
{faintly} the goal state.
thenThen we –can represent actions. Ffor example suppose in the blocks world
we can have actions like pick up, pick up x and 
we specify an action in the STRIPS domain by giving the preconditions of the actions and the effects of the action.
so- i will not formally write down the entire- system because (( )) covered it
and we will look at further examples later but 
forFor each action you need to specify the precondition.
can you tell me whatWhat would be the preconditions of the pick up x actions?
so forFor the pick up x action the preconditions could be hand empty and clear x.
ifIf x is clear and hand is empty then the pick up x action can be executed.
Now, if pick up x action can beis executed what would be the effect? 
theThe effect would be not, 
 so if x is on top of something suppose x is on top of y
if on x, y was true in the previous state on x, y will not be true in the next state
so that would be deleted hmm and hand empty will no longer be true
 after the pick up x action so not hand empty. ok
similarlySimilarly if we look at the actions stack (x, B) what would be the precondition
the precondition as to be that clear B so 
 B has to be clear and then hand should not be empty and the hand should be holding x. 
soSo if holding x and clear B then the actions stack (x, B) can be performed.
afterAfter the actions stack (x, B) is performed B will no longer be clear
x will be on (B on x, B) and x will be clear.
so [breathing] let us see
so far the action huh pick up x wait			[Refer slide time: 08:44]
 
(fig., 10:42)
[writing on the board]
sforo the action pickup x
 the precondition has to be that clear x hand empty and let us say on (x, y),
what will be the effects? 
effectsEffects will be not hand empty not on x, y and huh the effect will also be holding x.
forFor the action stack (x, y) what would be the preconditions?
theThe precondition has to be that holding x, not hand empty, clear y and
what will be the effects? 
theThe effects will be not holding x, not clear y, huh hand empty on (x, y) ok
now- lets huh get on with today's lecture
so asAs we have noted that planning problems can be ((cast)) as search problem
and in planning we are given an initial state and a goal state
 
(fig., 11:00)
and we have to generate a sequence of actions.
andAnd we have also discussed that there are two ways
two standard ways in which we can generate this sequence of actions.
oneOne is we can do forward state space planning 
 in which we go from the initial state towards the goal state in order to find the ((plan)) the plan
 or we can do backward planning.
 
(fig., 11:23)
soSo let us review the ((algorithm)) for forward planner.	[Refer slide time: 11:29]
inIn a forward planner initially we the agent is in the initial state and 
we maintain a search queue and we put the initial state in the search queue.
thenThen we have a loop.
inIn the loop the we pick a state
so first step is to pick a state from the search queue and examine the state.
ifIf it is a goal state then terminate and return the path from the initial state to this state.
otherwiseOtherwise if it is not a goal state we apply all the applicable actions in the state
to generate all successorses of theis state.
thisThis is called progressing the state through operator application.
andAnd each of the successor states so generated are put in the search queue.
soSo basically we can do a search breadth first search in the state space.
so weWe can go from one state to the states which are 
this isdistance one ahead and distance this is two ahead and so on
 until we get a state which satisfies the goal description.
thisThis is the general breadth first search framework and 
this is called forward planning algorithm.
nowNow this forward planning algorithm is also called progression 
 and we can write this algorithm in more detail as follows:
weWe are given an initial state I and we need to go to a goal state which satisfies G
so the progression of- from the current state to the goals 
given the set of actions available and the path so far.
So, if the current state satisfies the goals then we return the path
 otherwise we choose actions. 
chooseChoose from the actions those actions such that 
preconditions of a A is satisfied in the current state.
soSo actions is the set of all actions, 
 we choose a which are part of actions and those a-A for whose 
which the precondition is satisfied in state.
ifIf there is no such a andA then we cannot progress that state further.
butBut if such an a A exists or ((poor audio))if more than one A each such a exists then for each A 
we get the next state by applying the action a A to state S (( ))to the current state 
then the goals remain the same the actions remain the same and 
path is augmented by adding the current action.
soSo this is a- recursive formformulation- of the forward planning algorithm.
andAnd first call we call progress initial state G actions and then  ((empty path.rt))
nowNow, forward search huh the main problem with forward search is that in certain domains the number of possible actions that you can execute can be very high.
andAnd many of this actionsthese actions may have no bearing to achieve- the current goal.
soSo in the such systems forward search is not an effective mechanism.
so basicallyBasically in forward search the branching factor is the number of legal actions.
 
(fig., 15:02)	              [Refer slide time: 15:02]
theThe path length is the number of actions that are required to achieve the goal.
inIn any real world situation there are just too many applicable actions, 
there are too many things that you can do.
soSo if you are considering all possible actions then 
 the search space can become really- huge. 
so Therefore if you want to use forward planning you better use some good heuristics ((poor audio))to decide which actions you should use.

 aAnd the other alternative is do not search forward but
 search in the backward direction.
now lets see what that means
huh before that letLet us take a very simple example of forward search.
 
(fig., 15:44)
 
(fig., 15:55)
supposeSuppose initial state the agent is at Kharagpur
 and the goal state is the agent has to be at Leh
 and the actions available are; the agent can take a train from Kharagpur to Delhi,
take a train from Kharagpur to Chennai, from Kharagpur to Mumbai, Kharagpur to Howrah
so there are a huge number of possible actions that the agent can do.
agentAgent can take a train from to different places,
(( )) can take a flight to different places,
they can walk to different places so
 the total number of choices is just too huge.
 
(fig., 16:18)
so- huh someSome of these choices are applicable in the current state. 
evenEven that can be very large.
theThe agent can take a train from Kharagpur to Delhi, [Refer slide time: 16:29]
Kharagpur to Chennai, Kharagpur to Mumbai, Kharagpur to Howrah and so onand so on.

and thenThen for each such state consider what are the possible actions that it can take?
ifIf the agent is not huh directed by heuristics towards a particular huh motive
then t- as you can see the state space which is explored can be just too huge.
nowNow one possibility when the branching is very high is to use backward search.
whatWhat is does backward search mean?- 
backwardBackward search means instead of doing a search forward from the initial state
the agent  ((does athe)) search backward from the goal state.
soSo the agent finds out what he has to achieve and then finds what actions can achieve the goal.(( ))
ifIf there are- few actions only a few actions ((poor audio))with which he can achieve- the goal
 the branching factor can be small in the backward direction compared to the forward direction.
butBut backward huh search we have to take care of certain issues.
see youYou cannot straight forward huh do backward search from the goal 
just like we do forward search from the start state why
 because for one the goal does not uniquely specify a state 
 but it is only a partial description of the state.
remember inIn the goal state we just saidy 
 these are the conditions that must be true in the goal state right.
so weWe do not have the full goal state. ok
so butBut we are given a set of a set of sub-goals that we have to achieve. andAnd then 
we can consider each sub-goal separately and we consider actions that usually achieve one or more of theise sub-goals.
 
(fig., 18:18)
now inIn backward search we say that 
an action A is applicable in state S in the backward direction if 
the effect of A is consistent with S.
thereThere is at least one effect of A that is part of S 
and the state resulting from applying A in the reverse direction is 
called the result of regressing S through A.
so we will look at that later
firstFirstly of all let us see what which are the actions that we will consider from our goal.
According to the second point first of all you can see the second point is very clear
we must consider an action A whose effect makes (( )) 
at least one condition in the goal valid. ok
soSo- you achieve at least one condition of the goal
 so you must select such an action.
butBut you must be careful that the effects of the action is not consistent with 
what you have at the current goal state.
soSo the current goal state says clear C and your effect is not clear C
then that action cannot be executed. 			[Refer slide time: 19:29]
soSo an action can be executed only if it's effects are consistent with the current goal state
and it achieves at least one of the propositions in the current goal state.
soSo we are doing basically we are doing goal directed actions which achieves some goal.
you see ifIf the number of actions that can achieve a sub-goal is not very high
 in such cases backward search can turn out to be much more effective than 
{faintly} forward search.
((now)) lets come to theThe second question is:
howHow do we find the state resulting from a pplying- A in the reverse directiondirection?
 
(fig., 21:21)
soSo, that state is obtained by this.
firstFirst of all we collect the preconditions of A.
andAnd secondly we add to it the variable value assignment of every state variable
which are not in the precondition of A but in the current state.
soSo lets just huh try to see, we have
 this is my current state and we are considering the effect of the action A.
nowNow in this state what would be the propositionspropositions?
Number 1 number one all the preconditions of A,
 number two 2 all those propositions which are true in S.
butBut they are not in the preconditions of A we should also add here
so that would be the partial description of the state 
 which we get by regressing action A in state S. is this clear
finallyFinally in backward search when do we terminateterminate? 
inIn forward search we terminate when the agent gets to the goal.
inIn backward search the current backward state’s
we terminate when the current backward states partial assignment
 is consistent with the variable assignment in the initial state.
soSo –initial state we have a full description of the a ((set)) of propositions.
ifIf the current backward state is consistent with the initial state description 
{faintly} ((poor audio))				[Refer slide time: 22:02]
 
(fig., 25:18)
Then one can value it. 

so lets nowNow let us look at the algorithm for backward search which is called regression. 
so inIn regression we go from G the goal state to I the initial state.
so thisThis function Regress takes the following parameters:
initInit state is the initial state current goals are the goals actionsand
 current goals are the current state of the goals  
then we have a set of actions 
 then we have the current path.
nowNow in this function if init state satisfies the current goals 
then we have (( ))got to a state from G to I and
 we have finished so we determine the path. 
otherwiseOtherwise we choose an action choose those actions from the set of actions 
such that some effect there is at least one effect of this action 
which satisfies one of the current goals.
we choose those actions
weWe choose those actions such that some effect of action satisfies the current goals and the effects are not inconsistent with the current goals
 so we choose such actions all such actions.
ifIf there is no such action then we return failure.
weWe say that this goal is unachievable.
ifIf some effect of the action a huh so if some effect of action aA contradicts 
some of the current goals then also that action leads to an inconsistent state
 so it can be not be applied.
Therefore so you only consider those actions 
which achieve some sub-goal and which
whose effect does not contradict the current state.
so let nowNow we have to find out what is the state that we get 
by regressing the action in the state?
soSo let C G' prime be the set of current goals minus effects a A plus preconditions aA.
soSo the goal achieves some of the sub-goals. ok
so this action a as-  theseThese are the preconditions and these are the effects. ok
soSo we get the previous state by taking current goals minus what as been achieved 
as a result of the action a A plus we add the preconditions of aA.
nowNow we get C G'. prime. 
ifIf the current goals isgoal is a subset of the C G' prime then they we return failure
b because you see we do not achieve anything.
If -we get back C G' prime something which is equal to 
or more then the current goal then it is useless
so we do not consider such states and  
we break search aton those points. otherwiseOtherwise we again regress. 
weWe call regress with initial state, 
 the current goals become C G', prime 
 actions remain the same and the path
with path we concatenate a A.
 andAnd first time we call regress with 
the initial state, the goal, the set of actions and the ((empty path.)) is this clear
soSo this is the regression algorithm or the backward search algorithm.
noticeNotice that we searched from the goal and there is
so you have to handle the choosing the correct actions,
f finding out whether they are consistent 
the actions are consistent for the current state as
so if you have to look at that carefully and you have to know [Refer slide time: 25:44]
 
(fig., 25:55)
how to get back the regress state from the current state.
nowNow that we have- talked about progression and regression 
let us just go back a little bit and discuss the planning algorithm 
which is used in the STRIP  ((system.))
the Basically the STRIP system basically is a divide and conquer search system.
soSo the objective is to create a plan, to achieve a conjunction of –sub-goals
and this is done in this way.
weWe create a plan to achieve the goals in isolation
 that is each goal separately and then we combine this plan
 so we create a combined plan to achieve all the goals.
so whatWhat is done is usually you will serialize the goal.
firstFirst we achieve one goal then we have the problem of achieving the rest of the goals
unless until all the goals have been achieved.
soSo in STRIPS we sub- we divide the goals and then achieve one goal at a time.
nowNow how does the STRIPS planner achieve a list of goalsgoals?
itIt first chooses one of the goals to achieve. 
ifIf that goal is not already achieved then the STRIPS planner chooses an action 
that makes the goal true. ok
 
(fig., 27:38)
soSo it chooses one goal to achieve, 
 finds an action which makes the goal true backward search
 then it achieves the precondition of the action.
 soSo it finds the new goals that the system must achieve 
 and then it carries out the action and achieves the rest of the goals. ok
soSo this is the algorithm of the STRIPS planner.
nowNow let us see what are the characteristics of the algorithmthe characteristics of the algorithm  
 and what are the difficulties that we may face. 
nowNow the problem is that this algorithm is (( ))unsound
 because suppose the goal is written in terms of huh 
{very faintly} wait
suppose the goal G is specified as a conjunction of g1 one and g2 two and g3 three
 suppose the agent finds a plan to achieve g1 one 	[Refer slide time: 28:22]
 
(fig., 29:18)
so this is the initial state agent takes some actions and 
 as a result of the actions g1 one holds. 
nowNow the agent tries to satisfy theis rest of the goals g2 two and g3 three.
butBut while the agent finds a- plan to satisfy g2 two
 it may happen that in this state g1 one does not hold.
soSo, achieving g2 two may undo the effect of achieving g1 one right.
soSo we have to be careful that while achieving the goals in sequence 
the previous goals are not undone. 
soSo STRIPS does not take care of it properly and that is why 
the STRIPS planning algorithm is unsolved.
achievingAchievin oneg1 sub-goal may undo already achieved sub-goals.
inIn the example that we did, achieving sub-goal g2 two may undo 
the effect of achieving sub-goal g1 one
 because the objective of the agent is to go to one single state
where both g1  one and g 2two and g3 three hold.
 
(fig., 29:44)
nowNow how do we fix the STRIPS algorithm? out of them
thereThere are two ideas that have been tried to make STRIPS sound.
theThe first idea is to protect sub-goals.
thatThat is, once a sub-goal g1 one has been achieved 
do not take any action that undo’s the effect of g1. Therefore  one rightthe
so protect- so  first strategy to make STRIP sound is to protect sub-goals 
 which have been achieved. 
soSo once g1 one has been achieved do not allow any action that undoes g1 one.
unfortunatelyUnfortunately sometimes it will not be possible. 
So,- if you have to go to a state in g1 one 
it may be the case that you cannot get to state 
get to a state where g twog2 will be satisfied unless you undo the effects of g oneg1.
hmmm soSo such hmm such situations can arise ok
 that g twog2 cannot be achieved without undoing g oneg1. ok
forFor example suppose your objective is to hmm huh make  a cake – and buy balloons. 
right suppose the- huh supposeSuppose the objective is to make a cake and be at the park.
lets lets just revise the problem
the objective is to make cake and be at the park	[Refer slide time: 31:11]
supposeSuppose  ((you)) first try to achieve the goal of being at the park
 and then you want to achieve the goal of making cake.
nowNow in order to make cake you have to be at home.
ifIf you are at home you cannot be at the park.
soSo you cannot achieve the goal make cake 
without undoing the effect of being at the park. ok
-soSo this condition makes the STRIPS algorithm incomplete.
theThe second strategy to deal with STRIPS is to re-achieve sub-goals 
{faintly} that have been undone.
soTherefore if you first achieve g oneg1 and then try to achieve g twog2 and g3 three
but and while trying to achieve g twog2 [noise] if g oneg1 is undone 
 then your objective would be after g twog2 is completed re-achieve g oneg1.
Now, if you re-achieve sub-goals you might end up in finding longer plans.
 
(fig., 32:18)
now beforeBefore we move on to the next topic 
huh mainly partial order planning we will like to do a brief review of 
some of the characteristics of planning problems that we discussed in the earlier class.
The frame problem: so please recollect that we talked about the frame problem
inIn the frame problem we wanted to specify what does not change.
soSo, when the agent carries some act- huh carries out some action s 
then certain huh effects c- c- this action certainfor example the preconditions and effects of the action change
 but the rest of the world does not change.
thenThen we talked about the qualification problem.
inIn qualification problem we said it is hard to specify all possible preconditions
so we tryied to qualify them. and And thirdly we looked at the ramification problem
where we said that it is hard to specify all possible effects.
 
(fig., 33:28)
andAnd then we looked at situation calculus.
inIn situation calculus we looked at success-or state axioms.
weWe said that frame problem can be handled by using situation calculus (( ))such as
successor state axioms.
forFor every action let us say drop action 	[Refer slide time: 33:48]
we say that the effect of drop action is broken x.
butBut we also want to say that if the drop action has not be performed 
x will not be broken.
soSo we say broken x in the state resulting from doing action a A from state sS 
if the action is dropped and if x is fragile or 
 there exist b such that b as exploded and x is next to b and x is broken and we 
{faintly} cannot repair x.
soSo we will try to specify all actions which gives result
which results in broken x and we say no other condition x will be broken.
thisThis is a way of incorporating the successor the frame condition. ok
the we the c-Then  in the classical planning framework that we are following
 
(fig., 34:55)
we made certain assumptions.
weWe assumed that all actions have deterministic effect effect.
weWe assumed that the agent is omniscient and it is a sole agent of change 
 there are no other agents in the world and the agent has to achieve certain goals.
nowNow let us look at a very famous huh problem
 the blocks world planning problem and huh
what are the difficulties some algorithms face to solve this problem and
huh basically basically the problems that STRIPS face in solving this problem.
huh thisThis problem is called the Sussman blocks world problem
o or called the Sussman Anomaly. 
 
(fig., 35:36)
theThe initial state is huh given by the following diagram:
whereWhere A is on the table C is on A, C is clear-,
 B is on the table, B is clear.-
soSo this is the initial state and the goal state we want
A to be on top of B and 
B to be on top of C. ok
soSo this is the initial state and this is the goal state. 
now let us see how we can achieve
so theThe goal is on A B and on B C. right			[Refer slide time: 36:02]
nowNow suppose you take the first of this goals 
on huh not first the second of thiese goals,
that is you want to first achieve on B C.
 nowNow how can you achieve B C?
youYou can pick up B and s-tack B on top of C.
soSo, if you put B on top of C you can achieve on B C. ok
nowNow after you have achieved on B C
 if you are using the STRIP formalism 
you would like to achieve the other goal on A B.
nowNow after you have achieved on B C
 you get a situation like this B C A.
nowNow if you want to achieve on A B you cannot do it without undoing the effect of {faintly}the previous goal.
so Therefore this is a very famous problem for which the STRIPS algorithm
would run into difficulty.
nowNow let us look at the blocks world search space.
 
(fig., 37:18)
thisThis is the initial state of the Sussman Anomaly problem.
thisThis is the state where C is on top of A and B is there on the table 
and A is there on the table. and And this is the goal state that we wish to achieve. right
soSo if we are using forward huh search then what would happen?
theThe agent would get here here and then from here you he can go here, here, 
you he can go to all these places- etcetera
 so in (( )) forward search one can achieve the 
{very faintly} goal condition.
butBut if you want to serialize the sub-goals in a particular way 
it might be difficult to achieve
{faintly} huh this goal.
whatWhat we can do is, once we have this state space 		[Refer slide time: 38:11]and 
because it is a very small state space we can do forward search or backward search.
 
(fig., 38:11)
 
(fig., 38:18)
soSo let us look at an example of the progression algorithm applied to this problem. ok
nowNow, from the initial state the agent has re-course to two possible actions.
{referring fig., 37:18}
theThe agent can move C from the top of A to B
 and go to this state or the agent can move C to the table
, the agent has a third action of moving B to the top of C
 so these are the three possibilities that the agent can get to from the current state.
Now, suppose the agent chooses this action 
{faintly} and then the agent will go to this state. (( ))
nowNow at this state also the agent has how many actions6
one two three four five six huh possible actions from this state.
youHe  can move B to top of C, 
 you he can move huh B to top of A,
 you he can move C to top of B and so on
 so there are six 6 possible actions that the- agent can perform.
supposeSuppose the agent takes the action moving B to the top of C
 again from this state the agent has two possible actions ((poor audio))
three huh two possible actions this and this and
 if the agent takes th-is particular action and puts A on top of B 
then the agent reaches the goal state.
{referring fig., 38:18}
soSo, from the initial state by taking the actions
 move C from A to table then move B from table to C and then move A from table to B
 the agent can re- reach a state which huh in which the goal is huh satisfied
 and this is a case of success for the agent.
andAnd these choices however ((are non--))deterministic choices of the agent.
andAnd- in the actual practice if the huh the agent will have to do a lot of search.
soSo this is a particular problem where the state space is not very large. 
so huh Therefore this order problem can be executed by the agent 
 but for large problems search huh search can turn out to be very expensive.
nowNow let us just ((- free))briefly look at the regression algorithm that we discussed earlier
and see what happens when on the same state face space we use regression.
weWe start from the goal and then we apply action from the backward direction.
againAgain there is a particular sequence of actions move A from table to B 
 then huh move B from table to C and then move C from table to A.
soSo these actions achieve bring us back to the goal initial state.
butBut again these- are non deterministic choices.	[Refer slide time: 40:54]
 
(fig., 40:40)
Therefore if the agent takes th-ese choices
 the agent can get the initial state and but finding theise will involve a lot of search.hing
theThe challenge in finding a good planning algorithm is to make the search space smaller.
theThe reason why STRIPS tries to serialize the goal and achieve one goal at a time is
in order to make it easier to achieve the goal and cut down on the search space.
So, before ((we)) proceed further let us look at the comparison 
between progression and regression.
bothBoth the algorithms of progression and regression are sound
 
(fig., 41:44)
that is the resulting plan is valid,
 they are complete and if a if the v- plan valid plan exists they find one. 
butBut in these algorithms we need to do a non-deterministic choice e
and therefore you have to use 
{very faintly} search.
ifIf you are using search you can do different types of search such as 
that you have studied in this class
depth first search or breadth first search or iterative deepening search, 
A* star, I D A* star e etc.etera
butBut as you know the complexity of search is exponential in the worst case.
soSo in the worst case the time taken for search is O bn to the power n
 where b is the branching factor.
inIn regression usually the branching factor b is small. 
andAnd in progression we need a larger huh branching factor.
soSo in progression it is imperative to compute a good heuristics 
 in order to do progression search.
huh nowNow we will introduce you briefly to a different type of algorithm for finding plans.
insteadInstead of looking at state space search that we have been looking at so far
we have been looking at search in the space of plans
 or in the space of partial plans. 			[Refer slide time: 43:08]
soSo before we get into that let me explain what a partial plan means.
 
(fig., 43:18)
supposeSuppose this example is from ((poor audio))Russell and Norwich’s book 
Artificial Intelligence of ((Modern)) Approach.
theThe objective is to put on your socks and shoes 
every morning before you go out
you put on your socks and shoes. 
nowNow you can put on your socks and shoes in many different orders
 but you must put your right sock on before you put your right shoe on.
andAnd you must put your left socks on before you put your left shoe on.
nowNow here is a a plan to achieve right shoe on and right socks on
 and left shoe on and left sock on.
 
to achieve our (( )) theyThere are many ways in which you can achieve this.in which you can get achieve all this
soSo there are six different ways of achieving this plan.
((poor audio))These are called six different linearization’s of the plan.
youYou should e- you could either 
first put right sock then left sock then right shoe then left shoe
 or right sock right shoe left sock left shoe
 or there are six different choices as you can see in this diagram.
orOr we can represent these choices by the partial order plan.
thisThis partial order plan says that 
you must put your left shoe after your left sock and right shoe after your right sock.
butBut the- then the order between them may vary.
huh soSo this is the start state this is the finish state
 and these four actions have to be performed in any order 
 so long as they satisfy this constraint
 that left shoe is after left socks right shoe is after right sock. ok
soSo this is an example of a partial order plan.
thisThis is the least commitment plan or partial order plan
 which only expresses the constraints on the ordering of the different actions.
 
(fig., 45:18)    		  [Refer slide time: 48:25]
soSo in partial order planning the idea is that we have a search space of partial plans 
when we search in that space in order to find a partial plan to achieve the current goal.
nowNow when we talk about partial plans we can immediately handle 
 the problems that’s STRIP faced with serial solving of sub-goals.
weWe can elegantly solve that problem.- as we will see
 
Thereforeso in partial order planning which we call P O P
 the nodes are partial plans and the arcs or transitions are refinements to the plans
 and the solution is a plan not a p-
solution is a partial plan and is not a path.
In partial as i mentioned in partial plan 
we follow a principle of least commitment. 
that is, we do not commitment 
we do not commit to a particular order of the actions unless it is required.
nowNow let us see how we represent the partial plan in the P O P framework.
aA plan is represented by a three3- (( ))tuple A, O and L
 where A is the set of actions in the plan,
 O is the set of temporal orderings between the actions,
a A less than b B means a A must be executed before b B can be executed 
and L is a set of casual links linking actions via a literal.
weWe will see some examples.
thisThis is a notationnotation; this is a- example of a casual link
 
(fig., 47:18)
A p to A c to Q 
s o which means that the action A c as a precondition Q 
and this precondition is achieved by the action A p.
soSo the action A p achieves a condition Q which is required for the action A c to be executed.
soSo these are called these are called casual causal links.
Therefore
so  we have a set of nodes which correspond to partial plans,
 we have a set of temporal links and a set of casuusal links.
temporalTemporal link simply says that if we have huh two actions let us say 
A1 one and A3 three and we say that this is a temporal link that means
 A1 one must occur before A3. three.
casual Causal links A p to A c means that A p achieves some condition Q 
{faintly}  which is required as a precondition for A c.
forFor example, from the blocks world domain let us take the following example:
theThe action moves c from d to b.
hmm  supposeSuppose this is d, this is c
 we want to move c from d to b and it requires that b is clear,
 moving c from d to b requires that b is clear.		[Refer slide time: 48:44]
andAnd b is clear- is achieved by the action move a from b to table.
soSo this action achieves clear b and clear b is required for this action.
soSo we will have a casuusal link from the action move a from b to table
 to move c from d to b.
nowNow, suppose we have a casuusal link 
now if we a casual link from A p to A c we cannot have
we cannot insert an action another action or third action between these ((scope))
 two the actions if that action threatens this casusal link.
nowNow which other actions that can threaten this casusal link? aA step A t. 
soSo if we have a casual link from A p to A c on Q
 so A p achieves Q and A c requires Q. 
now if i put in- so supposeSuppose this is a temporal link and 
so iif iI put in an action A t between A p and A c 
A t is a temporal link such that A t has not 0Q as it's effect.
so So if A t has not 0Q as it's effect what will happen? if 
 
(fig., 50:02)
soSo A p achieves Q, A t achieves not 0Q. 
soSo, if A t achieves not 0Q A c cannot be executed immediately after A t
 because this Q which A c requires is gone.
soSo a step A t can threaten the casual link from A p to A c on Q 
if A t as not 0Q as an effect and in that case A t cannot come between A p and A c. ok
so for example letLet us look back at the example from the last slide.
{referring fig., 47:18}
moveMove A a from huh so we have move a from b to table achieves clear b
 and then we can execute move c from d to b.
nowNow can you tell me an action that threatens this casusal link? hmm can you 
so suppose we have between these two actions 
suppose we have a huh supposeSuppose between these two actions [
we (( ))….] 51:19 in another action which is move huh move x to b, 
if you have move x to b now what happens?
b is no longer clear as a result of this action so not clear b.
 soSo if b is not clear this action cannot be executed. ok
soSo, if you we have two casusal links and this action achieves 
the move a from b to table action achieves clear b
 which is required for move c from d to b action
 no other action should come in the middle that undo’es the effect of this a-
undo’s this action. ok
now what is the initial plan
so we As we said we will search in the plan space as we said 
 now what would be our initial plan?
so weWe will introduce two special initial plans.
 soSo initially we have two special actions.
oneOne we call A zero A0 and one we call A infinity.	[Refer slide time: 52:33]
 
(fig., 52:18)
weWe will ((put))put th-ese two special actions.
nowNow what is A zero A0?now A zero A0 is an actiona
 special action which as no precondition and the effects of A zero A0 is the initial state
is the description of the initial state.
((use))So it has null precondition and it's effect is the initial state.
weWe also introduced a- action A infinity 
whose preconditions are the goals and it has no effect.
soSo A zero A0 must be the first step in the plan and A infinity must be the last step in the plan.
soSo this is what we constraint any partial plan to have A zero A0 as the initial state
initial action and A infinity as the final action.
 
(fig., 53:53)
now- so- i- in the next class today we will not have much time
so inIn the next class we will cover the partial order planning algorithm in greater detail.
(( )) theThe basic idea in the partial order planning algorithm is that
huh we start with an initial empty plan which has only the initial action and the last action 
and no other commitment
then we try to insert other actions so that goals are achieved. right
andAnd where do we pick the goals from? 
weWe try to find out a sub-goal which has not ((yet been)) achieved
 and we try to find an action which satisfies some of the 
some of the sub-goals and we continue until we get a correct plan.
soSo we will discuss in detail the partial order planning algorithm in the next class
and then show how it works. 
so before i end i will like you to think of huh like you to answer these two questions
i will like you to comeCome up with some examples of planning problems for which forward planning is an appropriate choice 
planning problems for which progression can be used.
second question i will like you toThen  consider some examples
think of some example planning problems for which 	[Refer slide time: 55:25]
 
(fig., 55:18)
backward search or regression is the right choice.
 with that i end today's class
thank you
[Refer slide time: 55:30]

Transcripted by: R. Anjanaa lakshimi 
{Total slide time: 54:02}

Indian Institute of Technology
Kharagpur

Artificial Intelligence
Prof. Sudeshna Sarkar

Lecture #23
Planning - 3

Hello, welcome to the class. 
Today we will have the third lecture on planning.
In the last two classes we looked at planning in Artificial Intelligence and representations for planning problems.
We looked at STRIPS representation, the STRIPS algorithm for solving planning problems and we also looked at  regression planning and progression planning
 
(fig., 01:06)
which involve searching in the state space to find the plans.
Today in the third lecture we will explore further the details of Partial Order Planning.
As I mentioned in the previous class in Partial Order Planning we will be searching in the space of partial plans.
And our objective is given a problem situation we want to get a plan 
which makes least commitment about the ordering of the different actions.
So we will represent the plan as actions and the ordering between the actions
in order to get the final plan of the problem.
In any intermediate stage we will maintain consistent partial plans which may not be complete and we will presently see the details of what I mean by this. 
In Partial Order Planning the nodes would be partial plans.
And to get from one partial plan to another partial plan 
 
(fig., 02:18)    
the step is called plan refinement.				[Refer slide time: 02:33]
By taking a partial plan by doing plan refinement we get to another partial plan.
And the solution to the planning problem is a partial plan.
So the path to the solution is not important because all the relevant information is captured in the partial plan which is a node in this search problem.
That is, we obey the principles of least commitment.
We do not commit to any ordering of the actions which is not required.
Before we continue let us recall the problem we worked out in the last class of putting on the socks and shoes problem our objective was to put on the left sock left shoe right sock and right shoe.
So these are the four conditions that must be satisfied at the goal.
And we one cannot put on the right shoe before the right sock is put on
or the left shoe before the left sock is put on.
And a solution to such a problem would be, after this start the agent can put on either the left sock or the right sock.
Only after the left sock the agent can put on left shoe and only after the right sock the agent can put on right shoe.
When both left shoe and right shoe are on the plan is done or finished.
So this represents a partial plan for the socks and shoes problem.
The actual plan will of course not be such a partial order structure but
 
(fig., 04:33)
it will be a linear structure.
From this partial order structure we can get several linearizations.
The agent can put on left sock right sock left shoe right shoe or left sock left shoe right sock right shoe or right sock left sock left shoe right shoe or right sock left sock right shoe left shoe and so on.
Therefore how many linearizations can be possible given this partial plan.
			[Refer slide time: 05:29]
Let us get to certain terminologies that we will be using today.
 
(fig., 08:55)
We have a step in the partial plan and a step in the partial plan 
would be a particular action which is taken. So step is bound to a specific action.
Then in a partial plan we will have orderings, orderings of the form s1 before s2 which means s1 must precede s2.
If s1 is an action s2is another action, action s1 must come before the action s2
it may come immediately before the action s2 or sometimes before the action s2.
Thirdly let us see what we mean by open conditions.
Open conditions are the preconditions of the steps which are not yet been satisfied. 
Suppose we have a step and in order to execute the step we need certain preconditions to hold.
As you know in the STRIPS representation for every action we have preconditions like P1 and P2 are preconditions of action a and also as you know for every action we also have the add list and the delete list
suppose q1 and q2 are added and q3 is deleted.
So P1 and P2 are the preconditions of the action a.
Open conditions are those preconditions of the step a that have not yet been satisfied.
Then what is important is we need to know what we mean by causal link.
Now we will say that Ap to Ac on Q is a causal link on Q if the step Ac has a precondition of Q and this precondition is met by the step Ap. 
If the step Ap supplies the precondition Q which is a precondition of Ac
then Ap to Ac is a causal link on the condition Q.
So this is a commitment that the condition Q needed by Ac will be made true by Ap.
That is Ap must cost Ac. 
So Ap must have Q as an effect and Ac as Q as a precondition.
Now we say that Ap achieves Q for Ac.
	
If in our partial plan Ap achieves Q for Ac what it means is that no action can come between Ap and Ac which deletes Q.
So, while developing the partial plan if we want Ap to achieve Q for Ac we must make sure that no action would come between Ap and Ac which deletes Q. 
			[Refer slide time: 09:33]
 
(fig., 09:44)
There are more terminologies.
Unsafe link: Suppose there is a causal link s1 and s2 on the condition p and then we have another action s3.
Now if I put s3 between s1 and s2 and s3 can undo p then s3 will not be able to come between s1 and s2.
So if my plan permits or if my partial plan permits s3 to come between s1 and s2 and s3 deletes p then this is an unsafe link. 
This causal link s1 to s2 and p along with s3 together is an unsafe link.
So such unsafe links have to be taken care of before we get the final plan. 
Before we discuss the Partial Order Planning algorithm in order to develop the algorithm we will start with the initial state.
The initial state is a initial partial plan and we will start with this.
Empty plan is given by this.
In the empty plan we start with an initial state I, a goal state G and we assume that I precedes G.
This is an ordering between I and G and at G the preconditions that have to be satisfied are suppose G is g1 and g2 so g1 and g2 are the open preconditions that must be satisfied at G.
Initial plan: There are no causal links and there are no unsafe links.
But this plan is not complete because g1 and g2 are the open preconditions.
Now let us look at how we will represent partial plans.
A partial plan is represented by a 5-tuple A, O, L, OC and UL.
What is A? A is a set of actions in the plan. A is a set of steps in the plan.
So S0, S1, S2 up to Sinf let they be the steps in the plan.
So plan consists of the set of steps and as I mentioned a partial plan is not necessarily a total order plan.
That is it is not just a sequence of steps.
We can order the steps according to a partial order.
So we have a set of steps and we also specify the set of action ordering.
 So there is a partial ordering on the actions.
Si precedes Sj denotes that the step Si must occur before the state Sj in a realization of the plan.
So A is the set of steps, O is the set of action orderings and L is the set of the causal links.
And as we saw a causal link Si to Sj on condition p means Si achieves condition p for Sj. 
 
(fig., 13:55)
So Si has p as an effect and p is a precondition that is required by Sj.
 So, A, O and L specify the partial plan.
Now in a partial plan we have a set of open conditions which we denote by OC.
So OC at the sub-goals represent the sub-goals that remain to be satisfied.
They could be the sub-goals of the final goal or they could be the sub-goals of the intermediate state, the preconditions of an intermediate state which have not yet been satisfied.
So open condition means those conditions that are not satisfied therefore the plan is not yet complete.
Then we have a set of unsafe links. 
If we have the causal link Si achieves p for Sj and there is a step Sk which deletes p then Sk cannot come between Si and Sj.
So, if in a partial plan Sk can come between Si and Sj we have an unsafe link.
So what does a plan consist of?
 A plan is a 5-tuple so we have three things; A is a set of steps in the plan,
O is the set of partial order relations between the steps and L is the set of the causal links and if the plan is not complete what we will have in the plan is a set of open conditions which are the set of sub-goals that have not yet been satisfied and
we have some unsafe links those links which consists of causal links and 
a step which can come between the two steps in the causal link and 
delete the condition which is required.
When we have a partial plan that plan is flawed if there are some open conditions or there are some unsafe links.
And the solution plan would be a partial plan which has no remaining flaw.
That is, it achieves all the conditions and there is no unsafe links.
So in the final solution plan we would like there must be no remaining flaw.
That is every open condition must be satisfied by some action and then no unsafe links should exist.
Now this is a representation of the partial plan. 
We introduce two distinguished states S0 for the initial state and Sinf for the goal state.	[Refer slide time: 17:25]
Now we will consider that the initial state description is an effect of the initial step.
If in the problem the initial state is q1 q2
 
(fig., 19:18)
and the goal desired to be achieved is g1 g2 then we will have an initial state S0 whose effect will be q1 q2.
So q2 is another effect of S0. And G or Sinf is the goal state and for this goal state it has a set of preconditions.
The preconditions are the propositions which are true at the goal state.
So the preconditions here are g1 and g2.
In the initial plan that one can draw g1 and g2 are the open conditions.
So this is S0 and this is Sinfinity and g1 and g2 are the open conditions of G.
Now this is a partial plan where this open condition g1 is achieved by S3.
So S3 achieves g1 for Sinf. And let us say S1 achieves p for S3, I achieves q1 for S1.
Also suppose S2 achieves g2 for Sinf and now suppose S2 has an effect of 0p that is S2 deletes p and S1 achieves p for S3.
Can you tell me if there are any flaws in the plan?
There are two types of flaws in this plan.
Firstly S2 is a step which has two preconditions OC1 and OC2 which are not yet satisfied.
So OC1 and OC2 are the open preconditions of the state S2 so this is flaw number one. 
Secondly the flaw number two is S1 precedes S3, S3 precedes G and S2 precedes G
but we have not said anything in this partial plan about the relative ordering of S1, S2 and S3. 
Now if I put S2 between S1 and S3, so in the linearization if we have S0 to S1,
S1 to S2 and S2 to S3 and then S3 to done then what will be the problem?
The problem is S1 will have p as an effect but that p would be deleted by S2
so S3 will not get p as a precondition. 
So because S3 requires p as a precondition which is supplied as an effect of S1
we cannot put S2 between S1 and S3 so this is not correct so what can we do? 
S2 can occur either after S3 or it can occur before S1.
S2 which deletes p cannot in between S1 and S3 it can come before S1 or after S3.
Now let us look at a very basic version of the Partial Order Planning algorithm.
Let p be an initial plan. 
Now given an initial plan we first find out if the plan has any flaw.
So p is a plan we find out if p has a flaw.
So we choose a flaw f.
The flaw could be an open precondition or it could be an unsafe link. 
A partial plan may have several flaws, we choose one of them. 				[Refer slide time: 22:00]
Eventually we have to remove all the flaws in the planning algorithm.
And the order in which we choose the flaws may have a bearing on how fast we solve the problem.
But irrespective of which flaw we choose first we can choose the flaws in any order
and we will get the same solution.
So we have to choose a flaw and depending on the nature of the flaw we have to take certain steps.
So as we mentioned flaw is of two types.
An open condition is a flaw or unsafe link is also a flaw.
If we select a flaw which is an open condition then what should we do? 
We should choose an action S that achieves this open condition.
So if f is an open condition, suppose there is a state S' and this state as
f as its precondition. 
Now, in order to correct this flaw we must choose another state S1 which has f as an effect. 
If you do this then this open precondition is taken care of even though if other flaws are introduced.
Namely if the preconditions of S' are not satisfied there will be new flaws that are introduced.
Secondly a flaw could an unsafe link.
If the flaw is an unsafe link that is if we find that S2 deletes f then S2 cannot come between S1 and S' so S2 can either be promoted to come before S1or it can be demoted to come after S1.
So this is a protected interval between S1 and S' where S2 undoes the effect of f cannot come.
So S2 can be promoted or S2 can be demoted.	[Refer slide time: 24:43]
Now, after we have done that we must update the partial plan.
So, if we promote S2 then we must add a causal link from S2 to S1.
 
(fig., 25:25)
 
(fig., 26:44)
 
(fig., 26:55)

If I put S2 after S' then we must add a precedence link from S' to S2.
And while doing this if we find that there is no operator by which we can resolve the conflict then we return now.
Therefore this planning algorithm does not succeed. But if it does succeed and if we come to a point where there is no flaw left then we will return the current plan otherwise if there are more flaws left we will go to step two and select the next flaw. 
We start with an initial partial order plan which consists of the initial state I, the goal state G. 
The goal state has the desired propositions given as a goal as its open precondition
and the initial state has the initial state description as its effect.
Now our algorithm will choose a flaw.
 A flaw is either an open condition or it is an unsafe link.
If the flaw that we have chosen is an open condition we must choose an action S that achieves f.
If f is an unsafe link then we will choose to promote or demote the current step in the plan.
And once we have done this we update the current plan.
We return null if no resolution exist and if there is no flaw left we return p else we go back to state two and continue.
So this is the initial plan, S0 is the initial state, Sinf is the final state, g1 and g2 are the open conditions of Sinf.
This is an intermediate partial plan.
 Here we have Sinf, Sinf has the preconditions g1 and g2, S2 achieves g2, S2 deletes p, S3 achieves g1 for Sinf, S1 achieves p for S3, S1 has q1 as its prerequisite, S2 has OC1 and OC2 as its prerequisites.
So, if I take this open condition I must add a step which has OC1 as its effect.
If I look at this causal link from S1 to S3 on q and this node S2 if we put S2 between S1 and S3 then we can correct the flaw.
Therefore this process of flaw selection and flaw resolution is known as plan refinement.
So this is an example of a partial plan.
Apart from the initial state S0 and the final state, this is the initial state and the final state Sinf we have three other steps S1, S2 and S3.
S3 achieves g1, S2 achieves g2,		[Refer slide time: 28:18]
 
(fig., 28:18)
S2 deletes p, S1 achieves p which is required for S3. S1 has a precondition q1 
and S2 has two preconditions OC1 and OC2.
From this example let us find out the flaws in this partial plan.
Now there are several flaws in this partial plan. 
Firstly there are three open preconditions. 
S1 has the condition q1, S2 has the condition OC1 and OC2. Secondly there are some unsafe links.
S2 cannot come between S1 and S3 because S2 deletes p which is required by S3.
 
(fig., 29:11)
For flaw selection we have several choices.
The choices are the open conditions in the plan and the unsafe link.
The flaw resolution deals with how we select partial plans.
Here we can select a particular action and treat this as a backtrack point.
Or even for finding the unsafe links we can use backtrack search.
So this is the repetition of what I said earlier. 
A partial plan is described by the tuple A, O, L.
A is a set of actions in the plan, O is a set of temporal orderings of the form A precedes b and L is the causal linking actions via a literal.
This is an example of a causal link.
 
(fig., 30:40)
 
(fig., 30:45)
Ap achieves Q for Ac. 
Action Ac has precondition for Q that is established by the plan by Ap.
Another example; when one moves a from b to the table the effect is to clear b and clear b is required to move c from d to b.
Now, when we have an unsafe link as we noted there is a step which can come between a protective link.
So we say that a step At threatens a link Ap achieves Q for Ac if At has 0Q as an effect. 
Secondly, if At could come between Ap and Ac, that is if according to the partial plan we can put At between Ap and Ac then in both these places there is a problem with the partial plan it is not fully correct. 
Let us look at the initial plan. 
For uniformity we represent the initial state and the goal state with two special actions.
A0 is a special action or S0 which has no precondition which has initial state as effects and they must be of first step in the plan.
A, O, D has no effects, goals are preconditions and must be the last step in the plan process.
 
(fig., 32:18)
Now let us look at the Partial Order Planning algorithm in slightly greater detail
by seeing how we proceed.
So the Partial Order Planning problem P O P stands for POP 
takes three arguments A, O, L.
A is a set of steps, O is a set of temporal orderings between steps and L are causal links linking actions with a liberal.
So the P O P algorithm takes A, O, L as the argument.
In addition it takes two other arguments agenda and the actions.
Agenda are the set of flaws and actions are the set of actions that the agent can take.
If agenda is empty then return A, O, L so we have called P O P or Partial Order Planning on A O L has also agenda and actions. 					[Refer slide time: 33:18]
If agenda is null then we are done we return A O L, if agenda is null then we return minus A O L, aadd equal to chooses actions such that O is using effects. 
So, initially if the agenda is empty we return A O L otherwise we pick up a flaw from the agenda.
Let the flaw be (Q, a need) 
Now in order to satisfy this flaw we must choose an action such that this action requires Q.
So we must pick up an action which adds Q.
We have to have an action which adds Q so aadd is such an action.
If no such a exists then we return failure.
Otherwise we put L' = L union aadd where aadd is the new step we have obtained and Q is the something that aadd achieves and aneed is something that Q needs.
So, in other words you have an action aneed.
This step aneed has a precondition of Q which as not yet been achieved.
So we find a step aadd which has Q as its effect.
Now in this case we can say that aadd achieves Q for aneed.
Now aadd is a new step because it is a new step then we must add the preconditions of aadd which are not satisfied to the agenda.
Now how has the plan data structure changed?
First of all in the plan data structure we add a causal link from aadd to aneed on Q.
Secondly we update the partial ordering relations by adding this new relation 
aadd which must come before aneed. Then the new agenda is, that is one entry we have removed from the agenda is (Q, aneed) because the open precondition for Q has been achieved in this state so we remove it from the agenda.
If aadd is new then A equal to A union aadd.
That is, if we have got a new step then I will add it to my action list.
And then for all preconditions of aadd we must add (P, aadd) to the agenda.
Therefore we have taken care of open preconditions and next we have to take care of unsafe links.
Now for every action at we check if it threatens any causal link.
If it does threaten any causal link we must put the action at either before ap or after ac.
So in this diagram you have ap which achieves ac and it achieves it on the state variable Q.
Now when we want to bring in a new step I must either bring it before at or I must bring it after ac. 
Finally if the search succeeds at all points and we have an agenda which is empty
then we get the answer.
						[Refer slide time: 38:29]
 
(fig., 38:18)
 
(fig., 38:33)
This is the same algorithm written in a small fashion.
Initially we are given a Partial Order Planning problem consisting of A, O, L agenda and actions.
If agenda is empty initially then we return the current plan A, O, L otherwise we pick (Q, aneed) from agenda this is an empty condition.
Q is an empty precondition of aneed so we select our current goal then we choose an action which achieves this effect Q.
If there are several such actions this is a choice point.
If there is no such action of course the search will fail and then we say L' = L union aadd (Q, aneed).
So L is the set of causal links.
We add the causal need from aadd to aneed on Q to L.
So this is aadd, this is aneed and this is Q.
Now O' will be O' is the set of precedence constraints,
I add aadd comes before aneed.
From agenda I remove (Q, aneed).
If a new step is added then I union it with the step list and for each preconditions of the new state we add it to agenda.
For every action at that threatens any causal link we choose to add that at is before ap or ac is before at etc if neither choice is consistent fail.
Now this is the same algorithm. 
Initially I pick a goal then I choose one of the actions
then corresponding to the action I update the goals that is I remove aneed from the agenda then I add a new step I find the preconditions of the new step.       [Refer slide time: 41:18]
Suppose we find that there is a causal link from ap to ac on Q and if we find that there is an action at which threatens ap and ac
 
(fig., 41:44)
 
(fig., 42:02)
We say at must occur before ap or at must occur after ac.
If at cannot occur before ap or after ac if they are not consistent then we fail. 
And then we recursively call P O P on the action list on the causal link list and on the goal list.
So, as we can see the Partial Order Planning problem is sound and complete.
The P O P plan is a solution if all the preconditions are supported by causal links
that is there are no open conditions, there are no threats to any causal link 
and we have a consistent temporal ordering and by construction the P O P algorithm reaches a solution plan.
Now we will illustrate this P O P algorithm with a very simple example which is the blocks world example of the Sussman anomaly which we have discussed earlier.
In this problem the initial state consist of three blocks C A and B placed as follows A and B on the table C is on top of A.
The goal is to get the three blocks stacked like this: A on top of B, B on top of C.
Before we do our P O P algorithm on this problem Let us look at the representation of the initial state and the goal.
The goal is represented by A is on B and B is on C.
Note that we mentioned we can	[Refer slide time: 43:02]
 
(fig., 42:18)
represent a goal by a subset of the conditions that hold.
An initial state is specified by the exact behavior of all the variants.
In this case on table A and on C A and clear C on table B and clear B is the description of the initial state and ((on A B)) (on B C) is the description of the goal state.
Example of Sussman’s Anomaly:
A0 is the initial state or I and Ainf is the goal state or G.
It has two open preconditions ((on A B)) and (on B C).
So what are the flaws in this plan?
I has these as the effects.
So flaws are there are two open preconditions. 
Now what should we do? 
We will try to take one of the preconditions and try to satisfy it.
In the first step if we have an action or step A1 which involves moving B from table to C, if I move B from table to C what do we achieve?
We achieve the precondition (on B C).
(On B C) is an effect of the action move B from table to C and (on B C) is required by the goal.
So we have a causal link on (on B C).
Now let us see what the open conditions are and what flaws are there in this partial plan.
There are several open preconditions;
((on A B)) is open, clear B is achieved in the initial state but here it is open, clear C is achieved in the initial state but here it is open, on table B is achieved by the initial state.
Now what we will do is we will take up one of these open conditions.
Let us take up clear B and we see that clear B is satisfied at the initial state so we say that A0 achieves clear B for A1.
Next we look at the open condition (on A B) on this sub-goal
Now to achieve (on A B) we introduce a second step A2 which says move A from table to B.
Now move A from table to B as preconditions clear A clear B on table A and effects not on table A not clear B (on A B).
Now this (on A B) achieves this (on A B),
What are the open conditions? Clear A clear B on table A.
Now we notice that A0 achieves clear B for A1 but A2 removes clear B. 
So clear B is required by A1 and this clear B is achieved by A0.
Now this step A2 deletes clear B so A2 cannot come between A0 and A1.
Therefore A2 must come before A0 or after A1. 
A0 is the special first state A2 cannot come before A0 so A2 must come after A1.
So recognize this fact we add an ordering constraint from A1 to A2.
So A0 is followed by A1, A1 is followed by A2, A1 is followed by Ainf and A2 is also followed by Ainf.
 
(fig., 47:55)
So this is what this particular partial plan represents.
Now we pick up the open precondition clear A which is required by A2.
Now we introduce an action A3 that achieves clear A.
What is the action A3? Move C from A to table.
So if you move C from A to table then A becomes clear.
The other effects are not (on C A) on table C and clear A.
And the preconditions are (on C A) and clear C.
So this is another step A3.
We have introduced three different steps in our plan A1, A2 and A3.
The pink arrows are the causal links between the states and the white arrows are the ordering links between the states.
 
(fig., 48:44)
Finally we see that (on C A) is achieved in the initial state, clear C is achieved in the initial state that is for action A3.
For the action A1 clear B, clear C, on table B are all achieved in the initial state.
For action A2 clear A is achieved by A3, clear B by initial state, on table A by initial state and so on.
And we finally find that the ordering relations are A3 then A1 then A2.
Why it is A3 then A1? Actually A3 and A1 can come in any order. 
A1 deletes clear C, A3 requires clear C so we cannot put A1 between A0 and A3.
So to protect this unsafe link we must add a ordering constraint from A0 to A3.
So finally we get a plan A0 to A3 to A1 to A2 then to Ainf which is a plan which meets this goal.
So the steps are; move C from A to table.
If you recollect the planning problem the goal was A on top of B, B on top of C.				[Refer slide time: 50:33]
And in the initial state I think we had C on A and B on the table.
Now what are the steps?
First move C from A to table, then move B from table to C, move A from table to B and then we are done.
 
(fig., 51:02)
Now in summary, if we compare Partial Order Planning versus state space planning
the complexity of a search problem is Obn in the worst case where b is the branching factor and we have non-deterministic choices.
Now in progression planning and regression planning the depth of the plan is the number of actions in the plan.
In P O P the depth is the number of steps that we have to take to satisfy the flaws.
What are the flaws? The flaws are the open preconditions and the unsafe links.
Therefore n is related to the number of preconditions of the steps and the
number of link protection that we require.
And generally an action has several preconditions.
Now, if we look at the branching factor P O P usually has the smaller value of the branching factor.
But the main advantage of P O P is that we have no backtrack due to goal ordering.
When we have to satisfy an open condition there could be more than one action which satisfies the precondition so there is a choice point so there could be some backtracking on deciding which action to use to achieve this precondition.
But if we have two sub-goals g1 and g2 let us say (on A B) and (on B C) we do not have to backtrack on which one we should do first, whether we should do (on A B) or whether we should do (on B C). 
We can do both separately and then while deciding whether we should take action first or this action first we look at what conflicts they generate and we look at the unsafe links.
Therefore there is no backtrack due to goal ordering.
In Partial Order Planning as we have noted we have this list commitment that is we do not do any premature step ordering.
Usually the search involved is less if we do Partial Order Planning.  
		[Refer slide time: 54:00]

Indian Institute of Technology
Kharagpur

Artificial Intelligence
Prof. Sudeshna Sarkar
Department of Computer Science and Engineering, 

Lecture #24
Planning - 4

Welcome, today we will be having our last lecture on planning.
We already had taken three classes on planning and we have looked at 
plan formulation and several algorithms for planning problems.
We have looked at STRIPS, we have looked at forward planning as search problem, we have looked at backward planning or regression and in the last class we had looked at partial order planning. 
Today we will introduce another way of looking at planning problems
and we will introduce the concept of planning graphs and the algorithm graph planning.

(Refer Slide Time: 1:38)

 



The instructional objective of today’s lecture is:
We will discuss the data structure of a planning graph.
We will discuss the graph plan algorithm that uses the planning graph
and describes an algorithm for finding a plan.

Given a planning problem you should be able to construct the corresponding planning graph and starting from the last state you should know how to extract a solution plan from the planning graph.
As you have noted a big source of inefficiency in search algorithms is because of the branching factor.
If you are talking about either forward search or backward search what makes search algorithms expensive is the branching factor.
If the branching was one at every node then the algorithm would be a linear algorithm.
But otherwise if the average branching factor is B and the depth of s search tree is N you know that BN nodes have to be expanded.
So, to make a search problem more effective what we would like to do is to limit the branching factor or limit the number of children of each node.

(Refer Slide Time: 2:28)


 




When you look at backward planning you start from the goal and you find
all the actions that could have produced the goal state.
And then from each of the previous nodes you do the same thing.
The main problem is that you would be considering a lot of actions. 
And many of these actions may not be reachable, we may not be able to take the action starting from the initial state.
So our objective is to find out a subset of the actions which can be activated 
given the initial state that the agents start from.
And in this way we will like to limit the branching factor.
This was one of the motivations for this algorithm.


(Refer Slide Time: 4:24)


 


One way to reduce the branching factor is as follows:
First you consider a relaxed problem. 
So, given the original problem you take a relaxed version of the problem where you let go of some of the constraints.
That is, you remove some restrictions of the original problem.
And the relax problem that you get should be easy to solve.
And the solution to this relax problem will include the solution to the original problem.
So once you get this relax problem you can do a modified version of the original search.
And you restrict the search space to only those actions that are applicable
in the relax problem.
Now the basic idea of graph plan is you construct a graph that encodes 
some constraints on the possible plans.
You take some of the constraints from possible plans and incorporate them to create a graph.				
[Refer slide time: 05:55]

Now you will use this planning graph which we will refer to subsequently as PG.
We will take the planning graph PG to constrain the search for a valid plan.
The way we will construct the planning graph or PG is as follows: 


(Refer Slide Time: 5:15)
 

If any valid plan exists it must be a sub-graph of this PG.
The partial order plan that we have looked at can be expressed in the form of a directed acyclic graph or a diagram.
And we will construct a planning graph such that a valid plan will be a subset of this PG.
Now we will show that the planning graph can be built in polynomial time.
And once we build the planning graph we are able to obtain restrictions on actions which will make a search for a valid plan easily.
Now the planning graph we will consider will be a directed graph and it will have several levels.
And in this graph we will have two types of nodes namely proposition nodes and action nodes.
Therefore in the planning graph we have proposition nodes and then we have action nodes.
And in fact in the graph we will have several levels and the level with the proposition nodes will alternate with a level with the actions.
And we will also have three types of edges between levels.
There will be the precondition edges from propositions to actions, this proposition is a precondition for this action or these two propositions are the preconditions for this action, these are called precondition edges.
We have add edges from action to proposition, if this action is executed this proposition will be added.
And we have delete edges, if this action is executed certain propositions will be deleted.
So this may be q and this may be r, this may be 0r.

(Refer Slide Time: 6:46)
 


So we may have add edges and delete edges from action nodes to proposition nodes and we will have precondition edges from proposition nodes to action nodes.
And in the planning graph the proposition levels and action levels will alternate.
The action level will include actions whose preconditions are satisfied in the previous level plus maintenance actions.
 The propositions at a particular level will denote those propositions that can be valid at a particular state.
In a state where the subset of these propositions will be valid what are the actions that one can take from the state?
The actions that one can take from the state are those for which the preconditions are satisfied.
Also certain propositions will get carried over to the next state by virtue of the frame axioms as we have discussed earlier. 
As we have noted that there can be many propositions in the world and unless some proposition is explicitly added or deleted the other propositions which hold in a particular state continue to hold at the next state.
So corresponding to these propositions we will have maintenance actions 
or no-op actions.
Let us look at an example of the planning graph and then look at the algorithm graph plan.
So this is an example of a planning graph.

This is the initial state or S0, these are the propositions which hold at the initial state. These are some actions whose preconditions are satisfied in this initial state and this is the set of propositions that can hold in the next state.
So let P0 be the set of propositions that can hold in state S0 and let Pi be the possible set of propositions that can hold in state Si.
So this set is called Pi. 
Pi is the possible literals or propositions that can hold in state Si.
This represents Pj and Pj represents possible set of literals that can hold in state Sj and ai are the set of actions that can be taken at state Si.
This is an example of the planning graph.
These pink edges from a proposition to action means this is a precondition edge and edges from action to propositions mean they are add or delete edges.
	

(Refer Slide Time: 9:56)

 


They are add edges if they go to a positive literal and they are delete edges if they go to a negative literal and this grey edges are the maintenance actions or the no-op actions.
So this proposition continues to hold at the next state if the action does not add or delete that.
So this is an example of a planning graph.
We have come to the procedure graph plan now.
In the algorithm for graph plan there will be two phases.
There will be one phase for graph expansion and there will be another phase for solution extraction.
We start with k = 0 then k = 1 and then k = 2 and so on.
For every value of k we first do graph expansion then we check certain conditions in the graph and if the graph needs those conditions we do solution extraction. 

(Refer Slide Time: 9:52)

 


Procedure for graph plan:

For a particular value of k in the graph expansion phase we create a planning graph that contains k levels. 
So this represents a relaxed problem.
This planning graph we get incorporates some of the constraints of the problem we have but not necessarily all the constraints so this planning graph is a solution to the relaxed problem.
And the actual plan will be a sub-graph of this planning graph.
Once we have created the planning graph up to k levels we check whether the planning graph satisfies a necessary condition for plan existence.
Once we have a graph we check first by looking at the last level of the plan whether this plan would satisfy a necessary condition for the final plan.
If it does satisfy then we try to extract a solution.		

If we succeed in extracting a solution we output the solution. 
If we do not succeed in extracting a solution from the level k graph we increment the value of k and then go through the graph expansion phase again.
Therefore in the solution extraction phase what we do is we consider only the planning graph that we have and we do a backward search from the goal state.
So, in this backward search in the planning graph we consider only those actions that are present in the planning graph and this is what constraints the search space.
Now as a result of solution extraction by backward search through the planning graph if we find a solution then we return the solution.


And as we have seen earlier this is an example of the planning graph.
These pink edges denote the precondition edges and the add and delete edges, the grey edges denote the maintenance edges or the edges corresponding to no-op actions.

(Refer Slide Time: 16:10)



 


And Pi is the possible literals in state I, Pj is the possible literals in state j
so this is the notation that we could be using. 
Therefore in a planning graph there are alternating layers of ground literals and actions.
So at the action level i we have all actions that might possibly occur at that time step.
And at Pi we have all the literals that are asserted by these actions.
So, in the planning graph suppose this is the state level i – 1 which consists of the propositions Pi – 1 and suppose we have the state level i consisting of the literals Pi and this is the action level I, now this is state level 0, these are the propositions which hold in the initial state which are known to us and these are precondition edges.


(Refer Slide Time: 16:35)

 


So this is an action, this action has this and this has the precondition, this action has this and this has the precondition and these are the effect edges.
So these three are the effect edges and these are the precondition edges
and these grey lines correspond to maintenance actions.
That is, this proposition is carried over to the next state if an action does not affect that proposition.
So how do we construct the planning graph?
At level P1 we have all the literals which are given in the initial state.
In a planning problem you are given all the propositions that hold in the initial state and you are given a partial description of the goal state and you are required to find a plan which achieves the goal state.

(Refer Slide Time: 18:00)

 


Therefore P1 will include all literals in the initial state.
Then we will add an action in level Ai. 
Suppose we have Pi we will see how to construct Ai.
We will add an action in level Ai if all the preconditions of that action are present at level Pi.
We will add a precondition in level Pi if it is the effect of some action in level Ai – 1. 
So a proposition will be added in level Pi if it is the effect of some action in the level Ai – 1. 
And because we also have these maintenance actions we will carry over 
the propositions from Pi – 1 to Pi using maintenance actions.
And due to other actions we have some other propositions as effects which we will add to level i.
In addition we will maintain a set of exclusion relations called mutex relations. 
So these mutex relations will be used to eliminate incompatible propositions 
and actions.
And because of these mutex relations we will able to reduce the graph size work.
So the graph consists of alternate levels. 
We have a proposition level P1 followed by an action level A1 then a proposition level P2 followed by an action level A2 and so on.
 So this is a planning graph.					

(Refer Slide Time: 20:15)

 

Now what would be the mutual exclusion relations we will consider? We say two actions are mutex or two propositions are mutex.

So we have mutex relations between two propositions and we also have mutex relations between two actions.
Now two actions are mutex at some stage if no valid plan could contain both.
So if at a stage we find that both the actions cannot be together present in a valid plan we say that those two actions are mutex.
Similarly at a particular level if we find that two propositions cannot occur in a valid plan we will call them to be mutex.
Now we will look at certain conditions under which we can label two actions as mutex and then we will also find conditions under which we can label two propositions as mutex.
So we will study three different mutex situations between two actions.

Two actions are mutex.
We will consider inconsistent effects, interference and competing needs.
First we will look at what we mean by inconsistent effects.
So two actions a1 and a2 are mutex they cannot occur together. 
If an effect of one negates an effect of the other. 
Suppose the effect of a1 is P and the effect of a2 is 0P these two actions cannot come at this level of any valid plan.
So this is graphically illustrated here.			

(Refer Slide Time: 22:18)

 

We have a proposition level Pi and here we have the proposition level Pi +1 and these are the actions.
We say that action a1 and action a2 are mutex which we denote by this line between them a1 and a2 are mutex.
If the effect of a1 is inconsistent with the effect of a2 this could happen. Suppose q is the effect of a1 and 0q is the effect of a2 and since q and q2 are mutex they cannot occur together then a1 and a2 will be mutex.
Therefore this condition is called inconsistent effect condition.
Two actions will be mutex if their effects are inconsistent.

(Refer Slide Time: 23:21)



 

Secondly we will say that two actions are mutex under the interference condition.
That is, one of the actions deletes a precondition of the other action.
So this illustrates interference so we have these propositions at level Pi and this is Pi +1 and we say action a1 and a2 are mutex.
If action a1 has an effect 0q that is action a1 deletes q where q is a precondition of a2 so in this case we say a1 and a2 interfere.
Therefore these two actions will be mutex.
An example is the two actions stack a b and putdown a.
Now putdown a has holding a as precondition but stack a b is an action that deletes holding a.	


(Refer Slide Time: 24:18)

 


So stack a b deletes holding a and putdown a needs holding a.
Hence we cannot have these two actions together so they are mutex.
Thirdly two actions are mutex under the condition of competing needs.
That is if these actions have mutually exclusive preconditions, if the preconditions cannot occur together then at the next stage we cannot have those two actions together.
This is illustrated by the following diagram:
Suppose we have these two actions a1 and a2 and suppose this is the precondition of a1 and this it the precondition of a2 we say that if these two preconditions of a1 and a2 are mutex then these two actions must also be mutex.
For example, consider the action stack (a, b) and unstack a, b stack (a, b) requires that b must be clear and unstack a, b requires that a must be clear.
So if these two cannot be clear at the same time then these two actions cannot be executed in parallel.
Therefore under the condition of competing needs also these two actions are mutex.
So we have three conditions under which actions are mutex.
Two actions are mutex if they have inconsistent effects, if there is interference or if there are competing needs.
Now let us see when two propositions are said to be mutex.
These three conditions of action mutex can be found easily while constructing the planning graph. 
So we can label these inconsistencies easily.
It is also important to note here that these are only some of the mutex conditions.
But these mutex conditions are easy to find and they put some restrictions on the planning graph which goes towards reducing the amount of search that we need to carry out.

(Refer Slide Time: 26:55)

 



(Refer Slide Time: 27:44)

 


Now two propositions are said to be mutex if all ways of achieving the propositions are mutex.
Now when would we say this?				

(Refer Slide Time: 29:27)

 


For example suppose one proposition is the negation of the other so P and 0P are mutex because both of them cannot be simultaneously true at a state.
Two propositions are also mutex if all ways of achieving them are pair wise mutex.

This is illustrated by the following diagram:
We have these two propositions that we are considering,
we say that they are mutex.
These propositions can be achieved by action a1 or action a2.
This proposition can be achieved by action a3 or by this maintenance action.
Now a1 is mutex with a3, a2 is mutex with a3, a1 is mutex with this maintenance action, a2 is also mutex with this maintenance action so these two propositions can not be true simultaneously because you cannot achieve both of them simultaneously because all ways of achieving them are mutually mutex.
So there are two ways of achieving this proposition, there are two ways of achieving this proposition and they are pair wise mutex so each of these two ways is mutually exclusive with each of these two other ways so this is a case of inconsistent support.
Therefore these two propositions will be mutex.
Now let us look at an example to illustrate the construction of the planning graph
and then subsequently we will use the same example to run the graph plan 
algorithm.

(Refer Slide Time: 30:00)
 

Now this problem is proposed by Dana Nau.
Suppose you want to prepare dinner as a surprise for your sweetheart  
the initial state is as follows:
There is garbage, hands are clean and is quiet.
So garbage, clean hand, quiet is the characteristic of the initial state
and your goal is to prepare dinner have a present and there should be no garbage.
So, dinner and present and no garbage is the goal condition.

(Refer Slide Time: 30:36)


 
And these are the actions which are available to you:
Cook, wrap, carry and dolly.
So, cook is an action whose precondition is clean hands and effect is dinner.
Wrap is an action its precondition is quiet and effect is present.
Carry is an action which has no precondition and its effect is no garbage and not clean hands.
So hands become dirty but there is no garbage.
Dolly is an action which has no precondition its effect is no garbage and not quiet.
So we have these actions cook, wrap, carry and dolly and with these actions we have to see how we can achieve this goal of dinner and present and no garbage.

(Refer Slide Time: 31:55)

 


So this table has the same thing such as this action, precondition and the effect.
Cook has clean hands as precondition and dinner has effect, wrap has quiet as precondition and present as effect, carry has no precondition no garbage and no clean hands is the effect, dolly has no preconditions no garbage not quiet is the effect.
In addition to these four actions we also have all the maintenance actions.
It is one for each literal.
Let us see what it signifies?			


(Refer Slide Time: 32:20)

 


In the initial state we have garbage and then clean hands, quiet.
And you also know whatever propositions are not mentioned in the initial state
are assumed to be absent.
So in the initial state there is no dinner no present.
Now what are the actions that one can carry out in initial state?
One can do carry, one can do dolly, carry and dolly have no preconditions.
One can do cook the precondition is clean hands, one can do wrap the precondition is quiet.
And the effect of cook is dinner, effect of wrap is present, effect of carry is no garbage and no clean hands, effect of dolly is no garbage and not quiet.
Now let us see what sort of mutex hold between the actions.
In addition to these four actions we also have the maintenance conditions 
one for each literal.
For example, garbage to garbage there is a maintenance action.
From clean hands to clean hands there is a maintenance action.
Quiet to quiet similarly no dinner to no dinner no present to no present are the maintenance actions.
Now we notice that carry has no garbage as effect and the maintenance action corresponding to garbage has garbage as the effect.
Since garbage and no garbage are mutex they cannot occur together so carry is mutex with this maintenance action.

(Refer Slide Time: 34:24)

 


Therefore carry and this maintenance action are mutex.
Similarly dolly has no garbage as the effect. 
This no op has garbage as the effect so dolly is also mutex with the maintenance action.
So dolly has an effect of not quiet and quiet has the effect of quiet.
Since not quiet and quiet are mutex dolly cannot occur along with the maintenance action quiet.
Hence dolly and quiet are mutex.
Dolly is mutex with wrap because wrap requires quiet and dolly has an effect not quiet, this is an example of interference.
Dolly has effect not quiet wrap requires quiet so dolly and wrap cannot occur together.
Not quiet is inconsistent with present, cook is inconsistent with not present maintenance action, wrap is inconsistent with not present maintenance action.
So we start with the initial state garbage, clean hands, quiet, no dinner, no present. And these are the four possible actions carry, dolly, cook, wrap
and these are the maintenance actions.
And then we find all the mutex relations between these actions according to the three rules that we have looked at. 
And then carry is mutex with this maintenance action for garbage, dolly is mutex with the maintenance action from garbage, dolly is mutex with wrap, 
not quiet is mutex with present, cook is mutex with not dinner, wrap is mutex with not present.
So at state level 0 we have all the propositions of the atoms that are 
mentioned in initial state union the negations of all atoms which are not in S0.

(Refer Slide Time: 37:09)
 



At action level 1 we have all actions whose preconditions are satisfied in S0.
In state level 1 we have all effects of all the actions in action level one.
This is how we can construct the planning graph.
And by looking at the mutex conditions we can specify the mutual exclusive relations between them.

(Refer Slide Time: 37:40)

 


Here I have not put the maintenance actions for not dinner and not present
but the rest we have is this two level planning graph for this problem.
So the actions as we have noted are carry, dolly, cook and wrap.
These are the precondition links and these are the effect links.
These are the maintenance actions and then we have specified the mutex relations between the different actions.
We already saw how to have the mutex relation between the different actions.
And then we can also have mutex between the different propositions.
Therefore this is easy garbage mutex with no garb, clean hands is mutex with no clean hands, quiet is mutex with not quiet then dinner is mutex with not clean hands because dinner requires clean hands and dinner is mutex with not clean hands.
Present is mutex with not quiet because present requires quiet.
So these are the mutex relations between the different propositions.
Now, once we have this planning graph where we have state level 0, action level 1 and state level 1 now we need to check to see whether there is a possible plan of length 1 through this planning graph.
So the goal is not garbage, dinner and present.
All these three propositions are present in the goal and we want to see if a plan exists.
Now, not garbage, dinner, present all are present in state level one and they are not mutex with each other so this satisfies a necessary condition that a plan can exist.
Now we have to do plan extraction to find a solution.

(Refer Slide Time: 38:48)

 

Now how do we find a solution?
Not garb requires either carry or dolly, dinner requires cook, present requires wrap.
So, for present we have to take wrap, for dinner we have to take cook
and for not garbage we have to take either carry or dolly.
Now carry is mutex with cook so we cannot take carry then we have to consider dolly.
Dolly is mutex with wrap so we cannot take the action dolly.
So in none of these ways we can satisfy the not garb action.

(Refer Slide Time: 39:20)

 





(Refer Slide Time: 41:00)

 

Therefore this plan cannot be extracted starting from this planning graph because not grab can be achieved in two ways carry and dolly and carry is mutex with cook. 
Cook is required for dinner, present can only be achieved by wrap and wrap is mutex with dolly. So neither carry nor dolly can be executed which is consistent with these two.				

(Refer Slide Time: 41:55)

 





(Refer Slide Time: 42:25)

 

So neither of this work and so we have to abandon this plan so this plan does not exist.
Now what you do if this plan does not exist?
Now you have to consider expanding your planning graph further.
So you have to go back and expand the graph further to one more level.
So we generate one more action level.
We had already generated up to this, we generate one more action level action level two and then state level two.
And then we again try to see if state level two satisfies the necessary conditions for plan existence and then we try to extract the plan from state level two.
So this is the planning graph expanded up to state level two.
Here we see that the carry, dolly, cook and wrap actions are there and we have found all the mutex relations between the four actions and we have found the mutex relations between the propositions. 
Before we proceed further let us look at certain observations.		

(Refer Slide Time: 42:54)

 


(Refer Slide Time: 42:59)

 


(Refer Slide Time: 43:18)


 


So the first observation is that a set of propositions in state level zero, these are the set of propositions in P1, this is P2, P3 and we observe that the set of propositions are always monotonically increasing, the size of the set is increasing. 
We also note that the proposition mutex relationships are monotonically decreasing.
Whatever mutex relationships we had initially as we proceed further these relations can be dropped and no new mutex relations will arise.
Observation two is that since between two levels the number of propositions is increasing the number of actions which are applicable will also increase.
Therefore more actions will be applicable here than in this level.
Therefore the number of state variables is increasing, the number of propositions is increasing, the number of actions is increasing and then the mutex relationships between the actions are decreasing as we go.



(Refer Slide Time: 44:43)


 

From this we can conclude that as we construct the different levels of planning graph after some time the planning graph will level of because we are always increasing the propositions and after sometime we cannot increase any more propositions so the propositions that we had at level i will be the same as the propositions we will have at level i +1.
Therefore after sometime we will find that all the levels are identical.
So because we have a finite space the set of literals will never decrease 
and no mutexes will appear.
Hence this means after sometime our planning graph will reach a saturation point so the planning graph cannot go on for ever.

(Refer Slide Time: 44:56)

 



Now let us see what a valid plan is.
A valid plan is a planning graph where actions at the same level do not interfere.
Each action’s preconditions are made true by the plan and the goals are satisfied.
As we have noted a valid plan is a sub-graph of the planning graph we have. And It is that sub-graph where the actions at the same level do not interfere
that is they cannot be mutex.
Each action’s preconditions are made true by the plan and the goals are satisfied.

(Refer Slide Time: 45:41)


 

In the graph plan algorithm as we have noted earlier there are two phases.
In the first phase grow the planning graph this is called the graph expansion phase	and then we have the solution expansion phase where we search the planning graph for a valid plan.
Now if I do not find the valid plan we will add one new level to the planning graph and try again.
Before we proceed we note that the creation of the planning graph is a 
polynomial time algorithm.
The size of a t level planning graph and the time to create the planning graph are polynomial in the following: 

(Refer Slide Time: 46:15)

 

t is the number of levels, n is the number of objects, m is the number of actions and p is the number of propositions in the initial state.
And the size and the time of this planning graph is proportional to 
p + m × l × nk.
So, for k constant this is a polynomial. 
The maximum action nodes are m × nk where k is the largest number of action parameters we can have and k is usually constant, k is the number of actions in the possible actions planning problem that we are considering.
Once we have got the graph let us look at how we do solution extraction.
For solution extraction we do a search for a solution plan by backward chaining on the planning graph.
We first start at the last level of the planning graph where the goals are present and we try to achieve goals level by level.
At level k we pick a subset of non-mutex actions that can achieve the current goals.
Their preconditions of these actions become the goals for the k – 1 at level.

(Refer Slide Time: 47:03)

 


We build the goal subset by picking each goal and choosing one of the actions to add.
It could be that one action achieves two different goals.
So, if we had a goal g1 for which we chose action a1 and if g3 is also achieved by a1 then we can achieve g3 by a1. 
Then we continue on the remaining goals until we find a solution.
If we do not find a solution we backtrack and we check the alternate ways of achieving the unachieved goals.

(Refer Slide Time: 49:33)

 


In the plan graph search we are basically going in a loop. 
If goals are present at the current level and they are non-mutex with each other we choose action to achieve each goal and add preconditions to the next goal set at the previous level and then we continue recursively until we find a solution.
So this is what we execute in the different phases.
Therefore this is an example of a planning graph where suppose this and this are the goals these two propositions are the goals that we must satisfy.
And as you note, this proposition can be satisfied either by executing this action or by using this maintenance action.
This proposition can only be satisfied in one way.
So we must take this action it must be selected and out of this action and the no op we have to select one of them.

So this is the planning graph we start with and corresponding to these two possibilities we get two different planning graphs.
In this planning graph we have chosen this maintenance action to satisfy this goal.
And in both the planning graph we choose this action.
In this planning graph we choose this action to satisfy.

(Refer Slide Time: 50:07)

 

In this side of the search tree we have taken these two actions but here we have taken only this action. 
Now, in this case at the previous level we will require these two conditions to be satisfied and these two conditions are the preconditions of this action.
In this case where we have this maintenance action for this proposition
this proposition must have been true at the previous level.
So, if we choose this maintenance action then my goals are at the previous level.
On the other hand if I choose this action to achieve here I have these two actions to satisfy.
This action has these two as the preconditions; this action has these two as the preconditions so here I have to satisfy these four preconditions.
Therefore this is my goal at the next level.
Now, for each of these cases we will again consider these new set of goals
and try to see how we can achieve them.
By taking the previous example where we had these three preconditions
and for this precondition we have only this maintenance action, for this proposition this is the only action that achieves this proposition, for this proposition this is the only action that achieves this proposition.

(Refer Slide Time: 52:00)


 

So we take these two actions and we find that in order to do this at the previous level these three propositions are required.
Therefore we get the goal at the next level and thus we proceed until we come to the zeroth level.
If the propositions that we get at the zeroth level are a subset of the initial state description of the system then we have achieved a plan.
If we do not achieve a plan we backtrack and we explore other portions of the search tree.
Similarly from the other side we will have some other goal and thus we will proceed.
So graph plan proceeds like this.
How long will graph plan proceed?
After sometime graph plan will find that in the graph expansion phase at level k the propositions we have got is the same as what we get at level k +1 and that is saturation and we cannot expand the graph any further.	

(Refer Slide Time: 53:13)

 

(Refer Slide Time: 53:18)

 


If we have not found the solution then we stop.
Also graph can record the sets of unsolvable goals which can make it more efficient.
For solution extraction, in graph plan g is the set of goals we need to achieve and j is the level of this current state Sj.
If j is 0 that is we have reached the initial state we return the solution.
Otherwise for each literal l in g for each proposition which occurs in the current goal we non deterministically choose an action to use in state the previous state Sj – 1 that achieves l.
Once we have chosen actions for each of the literals and the goal if any pair of chosen actions are mutex then we backtrack.
Otherwise we get g' which is the set of preconditions of the chosen action
and then we recursively call solution extraction on g' and j – 1.
Therefore this is the solution extraction algorithm and this is the search algorithm but this search algorithm is constraint because the planning graph incorporates constraints on the problem.

Now let us come back to the planning graph we obtained. In this planning graph
at level two we find that the actions not garb, dinner, present are all present so we try to find the solution. 
And then in order to find the solution we see that there are three ways to achieve not garbage such as carry, dolly and the maintenance action for not garbage.
For dinner there are two ways; the maintenance action and cook, for present there are two ways; the maintenance action present and wrap.
Now we choose a way of achieving not garb by carry, we choose dinner to be achieved by the maintenance action dinner and we choose present to be achieved by the action wrap.
So this is one possibility by which we can achieve these goals.
In order to do this for wrap we have the precondition quiet, for dinner we need the precondition dinner, for not garb there is no precondition.
So at the level we have quiet and dinner as our goals.

(Refer Slide Time: 55:55)


 

To achieve quiet and dinner, quiet is achieved by the maintenance condition quiet and dinner is achieved by the action cook.
Cook requires clean hand as a prerequisite and quiet requires quiet as a prerequisite so clean hands and quiet are required in the initial state.
Now, clean hands and quiet are satisfied by the initial state description of the problem.
Therefore we have found a plan.
So this plan which is given in blue is a subset of the planning graph.
This is a partial order plan which is a subset of the planning graph.
Now, how does graph plan compare with partial order planning?

(Refer Slide Time: 56:30)


 

So because of the constraints of the graph plan imposes on the search space
it is faster than partial order plan.
Here is an exercise to try out.
So the domain we will be considering is a rocket domain where we have the following actions.
Move R, X, Y. Move rocket R from X to Y. 
Precondition is at R, X and has fuel R.
Effect is at R, Y and not at R, X and not has fuel R.
The second action is unload C, R, X which is unload cargo from rocket at X.

(Refer Slide Time: 56:35)

 

Precondition in C, R and R, X, rocket is at X and cargo is in R.
Effect: not in C, R and at C, X.
Third action is load C, R, X.
Precondition is at C, X and at R, X.
Effect is not at C, X and in C, R. 
So we have three actions; move rocket from X to Y, unload cargo from rocket at X and load cargo from rocket R at X.

(Refer Slide Time: 58:11)

 

Given this problem you need to apply graph plan to this domain with the cargo b and cargo c are in Delhi this is your goal. 
Initial situation: cargo b and cargo c are in Kolkata and rocket r is in Kolkata 
and r has fuel.
Therefore for this you have to execute graph plan and on the same problem you execute partial order planning.

                                       Indian Institute of Technology
Kharagpur

Artificial Intelligence
Prof. Anupam Basu
Department of Computer Science and Engineering, 
Lecture #25
Rule Based Expert System


We will be discussing about Rule-Based Expert System and we will also mention some things about certainty factors
Although we will have separate lectures on certainty factors that demands little more discussion. 
While discussing about Rule Based Systems we had talked about expert systems.

(Refer Slide Time: 2:47)

 

And if we quickly recapitulate what we learnt about Rule Based Systems
there are some salient points that we should remember.
First let us quickly have a look at what are the things we need to discuss about 
 Rule Based Systems.
First is the execution cycle. 
This is just for your recapitalization. 
This means that there are three phases; 
One is match, one is select and the other one is execute.
And after that we come back to the match phase only.
And the architecture of a Rule Based System consist of a knowledge base
which you can write in short as KB.
There is an inference mechanism which we are writing as IM, inference mechanism or control structure and there is a working memory.
In this working memory all the current facts are being stored. There is another user interface as it should be for any system and the user interactions are through this user interface. And that is communicating through the inference mechanism and the inference machine reads from and writes to the working memory. The knowledge base consists of rules in case of Rule Based System.
 And it is needless to say at this point that rules are of the structure if antecedent then consequent. 
 Now what is done in the match phase?
The inference mechanism looks at the data for the current problem from the working memory and tries to match them with the contents of the knowledge base to see which rules are enabled to be fired.
There may be more than one rule which is enabled to be fired.
When is a rule enabled to be fired?			

(Refer Slide Time: 04:18)


 

A rule is said to be enabled to be fired when the entire antecedents match or the consequents match depending on whether we are going to do forward chaining or backward chaining.
If more than one rules match or applicable at a particular cycle then in this phase the select phase which is also known as the conflict resolution phase a subset of rules are selected maybe one or maybe two.
And those rules which are selected are only fired and as this rules are fired by the inference machine new facts are generated which come to the working memory here.
That will enable other rules to match and this cycle will go on until no further rule matches or the goal with which we started the system is met.
these are the two points but besides this there are a couple of other points  which we need to recapitulate.
It is about the inference mechanism. 
There are two types of reasoning we talked about.
One is forward chaining and the other is backward chaining.
In forward chaining which is also known as data driven search we have got a rule like this; If a and b and c then p.
Now, given a rule like this and with whatever is there in the working memory say a b c we start matching the contents of the antecedents and if all of them match then we say this rule is enabled and if this rule is fired then the firing will produce this p as a new fact in the working memory and this can enable further search.
On the other hand in the case of backward chaining we start with the goal not the data. 

(Refer Slide Time: 07:32)

 


Unlike the forward chaining in the backward chaining case we start with, for example we have got a rule like backward chaining, If we have got a rule like if a and b and c then p 
and there are other rules also and we find that I have to prove p.
Then I start with p and that tells me I have to prove a b c.
And maybe in the working memory a and b are true so these are solved
but I have to find a rule which has got c in its consequent.
Suppose x is also there then c. then after this since c is my goal then I start with c and see whether this is proved or not.
For that I have to fire this rule c is true if x is true so in order to prove c you have to prove x but x is already proved so therefore c is proved therefore p is proved, that is backward chaining.
Now all these things we have discussed in our discussion about Rule Based System.

(Refer Slide Time: 9:08)

 

Now Rule Based Systems have been applied in expert systems.
And what are expert systems? 
Expert systems are said to be AI’s greatest commercial success.
An expert system uses knowledge specific to a problem domain. 
This is a very important point that you should try to understand that it uses knowledge that is very narrow specific to a particular problem maybe fault diagnosis of a two wheeler or maybe fault diagnosis of a particular type of car. 
So the narrower you make the domain the possibility of a success of an expert system is more because what we try to do in an expert system is to capture the human expert’s knowledge in the form of rules. 

(Refer Slide Time: 9:37)

 

Now human expert knowledge is a little complicated than can be really captured in the form of rules.
So, if we can restrict our domain then probably we can acquire the proper knowledge and encode it in the form of rules and then also we will find that 
for that small domain in order to capture all possible cases we can have a huge number of rules.
But we are not really trying to match the human expertise or the human capability in any way in expert systems then the question can be asked then why at all we go for an expert system?
The reason is, human expertise is scarce it is costly for all the aspects of problems you will not find enough number of experts who can cater to the large number of queries that come up.
Think of specialist doctors.				

Now specialist doctors are really scarce in number when compared to the demand that we have from a large number of patients for example, really good expert cardiologists. 
Now all the patients really do not need that level of expert advice all the time.
There are many cases which can be treated by less than a human expert. 
So if it be possible to capture the expert’s knowledge to some extent in such systems then obviously we can provide better service to the patients.
Everybody need not line up in front of the doctors chamber,
need not just wait for weeks and months to get an appointment. Maybe through some computer system they can be given routine advices wherever that is good.
Although expert systems aim or attempt to capture human expertise the gap will always be there.
Nevertheless, for many applications for example diagnosing the situation under a mine when we have put in some sensors and we really want to interpret the data either I take out the data at an upper level and try to look at it but proper interpretation of data at runtime may not be feasible for an expert to do. Therefore an expert system can do that job quite well provided we keep the domain narrow.

What is an expert system? 
Expert system is a system that uses knowledge specific to a problem domain to provide expert quality performance in that specific application area.
There have been a number of successful expert systems. 
Here we can see some of the names; 
Dendral which was developed in 1967 is an expert system that determines molecular structures based on mass spectrograms.
Mycin which is a very popular expert system developed at Stanford University by doctor E H Shortliffe in 1976 as part of his PhD dissertation. 
Mycin works for the area diagnosis and therapy recommendation for blood diseases.
And later on Mycin was done for different infectious diseases, bacteria, infections as well.
Prospector which came very close to Mycin in 1978 is an expert system that was used for mineral exploration. [Refer Slide Time: 15:48]
That means it tried to estimate where a particular mineral is likely to be there. And it found about a $100M ore deposit successfully.
Xcon is an expert system that was developed in 1984.
But the predecessors of Xcon are known as R1. And that expert system was again developed at a University and later on it was taken up by the Digital E- Corporation to configure how to install them large VAX and P D P 11 series computer systems. 
Installation of such large systems requires a lot of expertise about the layout, the cabling and other conditions.
But that can be done judiciously by Xcon. And this saved Digital Equipment Corporation DEC $70M per year.
That shows the commercial success. 
Now let us look at the common characteristics of expert systems. 
What are the common characteristics?
System performs at a level that is generally recognized as equivalent to an human expert in the field. 
It is better that you keep the domain narrow, keep the set of problems that an expert system can address limitedly rather than be too ambitious.
But the success of an expert system would be measured by if I had employed a skilled person and expert there paying a huge salary he who would have done some particular work and for that work if the expert system can do and show a performance that is close to this then the expert system is ok for that domain.
The reasons as i said human expertise is rare or expensive.
And there has to be a demand for the solution that justifies the cost and the effort of building the expert system.
Expert system building although we know the technology now the Rule Based System is one.
But it takes a lot of effort, time, man power, programming time, testing time 
in order to make a real successful expert system and so there has to be a demand for it. 

(Refer Slide Time: 18:37)

 

The system is highly domain specific because even in a narrow field 
lots of knowledge is there and we have to capture that.
And it should be narrow enough so that it does not depend so much on common sense.
Now we have to run it using a machine so it has to be amenable to symbolic reasoning. We should be able to reason using symbolic methods like predicate calculus, propositional calculus where we can represent the facts and rules in the form of symbols as we have seen earlier.
But it should also have a challenge that the solution is not straight forward.
You cannot employ an expert system to sort a file or sort an array or even do some pure image processing activities for which there are very good algorithms available.
The methodology is absolutely known, you really do not need much of decision making expertise. 
The rules in a Rule Based Expert System is if the situation is this then this 
will provide those critical judgment in an expert system.
If those critical judgments were not there in that case expert system is not required.
Again if you recollect that match, select, execute cycle of a rule based system
in the select phase we are doing conflict resolution that means we are selecting a subset of rules by some judgment by some heuristics.
We may assume that if we select this rule that is bringing me closer to the goal
so let me select this.
So it is not a straight forward algorithmic decision and so for all those cases expert systems or expert system approach can be used.
Presently there are systems which are a little hybrid in nature
in the sense that they use some part of rule based expert system and some parts are solved algorithmically and these two together provide a very good result. 
 System can explain its reasoning.
Think of a scenario that you have gone to an expert and the expert as told you that you have to undergo a surgery or you have to undergo a set of blood tests.
You can ask the expert why you want me to undergo this blood test.
The expert will be able to explain to you why or what is the reasoning that went behind him. Maybe that he is suspecting certain results, there are some possibilities in his mind and he wants to verify that or eliminate some of the possibilities and he will be able to explain that to you.
Just as a human expert will be able to do that it is expected that an expert system will also provide that sort of explanation at least close to that sort of explanation.
We should attempt that it should give similar explanations.
Therefore in order to be useful it must be able to justify its advice or 
conclusions.

(Refer Slide Time: 22:50)

 
	
The basic architecture that we had said about Rule Based System remains unaltered here.
Usually expert systems are rule based although there are expert systems which have been built on frames semantic nets and other technologies but mostly Rule Based Systems have found its popularity.
What are these rules for? 
The rules are to extract the expert knowledge in the form of facts and rules
and that is a real task.
Extracting knowledge from experts is really important and critical and this process is known as knowledge acquisition.
In order that an expert system becomes successful the knowledge as to be acquired properly and it is not a very straight forward case because no one is sure whether the human thinking moral is rule based or what sort of structure it maintains so there is a lot of debate about it.
So often the experts talk from a level which may be intuition or which may be very deeply embedded in their knowledge and they are so habituated they can click upon a particular suggestion and they say that.
But what is the reasoning process that went through to lead to this inference is often not very clear even to the experts themselves.
Therefore it is necessary that a knowledge engineer as we call it must talk to the experts, understand the domain and should be able to extract that knowledge and represent that knowledge in the form of rules so that it can be embedded in an expert system.
In the blocks there are:
The user interface, one is the inference engine, there is a general knowledge base and there is a case specific data.
Now you can think of where is the working memory.
The working memory is this for the time being.
This is the rule interpreter, inference engine, inference machine, control engine or whatever you call which is the path and this is the knowledge base.
And we have not deviated from the earlier architectures we showed except for this might be that the case specific data can be directly fed here but the interactions are the same that the inference machine will read the data, 
find out which rules are enabled, apply them and on application new data will be generated over here and that will come. 
So, user interface acquires information and displays results.
Inference engine performs deductions on the known facts and rules
that applies the knowledge base.
And knowledge base stores the domain specific facts and rules for solving problems in the domain and case specific data working memory or working memory stores information about the current deduction.
Therefore these are the four blocks.		

(Refer Slide Time: 26:44)


 

Now, as we know rule based expert systems have many options when applying rules.
At the beginning of this lecture I was mentioning about forward chaining and backward chaining which is also known as forward reasoning and backward reasoning. And in forward reasoning we start from the available facts and try to arrive at the goal.
On the other hand in case of backward reasoning we start with a conclusion and try to prove the conclusion by finding the sub goals.
If x and y and z then p means that in order to prove p I have to prove x, I have to prove y and I have to prove z.
So p is true if x is true, y is true and z is true. Might be x is already known to be true but for y and z I have to test them.
So again I will have to find a rule whose consequent has got y as I was explaining at the beginning of the lecture.
Similarly depth first, breadth first etc are again different ways in which this thing can be done.
What is a search space?
For example, I am in a particular state say S0 how is that state defined? 
That state is defined by the state of the working memory that is some of the facts I know of a, b, c etc.
Now given this state S0 where a b and c are known I may have a possibility of different rules to be fired R1, R2, R3. And firing each of these rules will lead me to different states S1, S2,, S3 because each of these rules will derive new inferences might be p will be known to be true if I fire this.
Now on this new state I will again have other rules to fire.
If I had selected this rule then probably I would have had some other different rules to fire which I have not traversed as yet. 
So after this I have got these three rules to fire, I fire this rule and I come to this state and that had three different paths and I select this state and come to another state etc. 
And while at every level and some paths I have left unexplored.
So this entire phase is the possible search space in which the path towards the solution lies and my job is to find that out.
And in order to search that particular path out of these three rules I first take this, then I take this and in this way I go on. And when I come to a dead end I have not found the solution I backtrack and follow the other unexplored paths.
That is the depth first.
In the case of breadth first what would be the case?	

(Refer Slide Time: 32:24)

 

At this level I will fire all these rules and come to this state.
Again at this level I will fire all the possibilities and come to this state and I will try to explore the space in a breadth-wise manner.
Obviously you know the difficulties of depth first that it will need a lot of nodes and lot of new children will be generated.
Another option is heuristic search which has been discussed in this course.
Now heuristics means with some partial information and knowledge 
I would like to find a better way possibly an intelligent way to explore the path.
Now in that case what do you do? 
In that case probably at this level I try to evaluate a heuristic function.
Now what that heuristic function will be is very much dependent on my application.
And based on the heuristic function I can select a particular path. 
For example, I have A on B and C is here and D is here.
It is some particular state say Sj this is the state of this and I want to come to another state that is my desired state which should be A and on that there should be C and D to go below B and now that is my objective. 
Now I have got different options to do that.
Suppose I want to come to this goal state Sg.
Now one way I can try is I cannot really take C over here C on top of B.
So one rule can be that you come to a state where A is here, B is here, C is here, D is here that is one possibility.
And from there you take C above this and take B above this.
Suppose if this be the case then I can straight away come to this where A will be here C is here and I directly put D on B.
Now, suppose my rules allow me that I can directly come over here
Therefore this is another state say Sm and this is Sn now which rule I should fire? 

(Refer Slide Time: 36:02)

 

If I have got a distance function then this Sn is closer to this because this part of the problem is already solved in one move and this will require more number of moves.
So, if that be my heuristic function I will apply this rule instead of this because that will reduce my distance to the goal.
Now these sort of judgments are used in order to have breadth first search.
Therefore applying the best rule is one approach.
Another approach is you can apply all applicable rules.

(Refer Slide Time: 36:29)

 

So here is an inference example.
Consider the following rules about diagnosis of an auto car problem.
The rule R1 says if there is gas in engine and it turns over.
That means when you put it in it is making that sound but it is not starting then the problem is with spark plugs.
As you turn the key it is not turning over so the initial sound is not coming and the lights are also not on then the problem is with battery.
If it does not turn over and the lights are on then the problem is with the starter.
Fourth rule says there is gas in tank and there is gas (petrol) in tank and gas in carburetor then there is gas in engine.
Suppose these four rules are known to us and this is the status of our database.
	

(Refer Slide Time: 37:29)

 






(Refer Slide Time: 38:51)

 



We know that there is gas in tank and there is gas in carburetor and that it is turning over so that sort of sound as been made.
Our goal is to find what the problem is.
And we have got all these four rules in our knowledge base so we have to find out what the underlying problem is.
Is it problem spark plugs, problem battery, problem starter or what is it?

Out of these rules we can always find out which rule will be enabled.
Now we can see that rule R1 is enabled.
Given this gas in tank, gas in carburetor and turning over scenario rule one is gas in engine because rule four says that gas in tank and gas in carburetor then gas in engine.
Therefore you first fire rule R4 then R1 then you know there is gas in engine and turns over.
Then we can find that using rule R1 here the problem is with spark plugs.
 Then we come up with a solution that x is spark plugs by R1 so we fire two rules. 
That is a very simplistic case of an expert system.
Although we solved this problem with forward chaining we started finding the rule that was matching with this gas in tank, gas in carburetor so R4 was enabled. And then in that cycle we came to know the gas in engine.
In the second cycle this R1 was gas in engine and turns over match
so R1 is fired.
But if we had done with backward chaining how would you have done it?
 To make it more specific, suppose our goal is to prove that the problem is with battery then with these same set of initial facts and rules provided to us we can try to solve it with backward chaining.
We will see how it works and you will have to draw the search space based on the rules fired.
You need not draw the entire search space but the path that is traversed.
But how did the problem get solved?
You need to draw the tree structure.
Now, as I say a very core part of developing any expert system is acquiring the knowledge from the expert and that process is known as knowledge engineering.
Knowledge acquisition is the bottleneck in developing expert systems.

(Refer Slide Time: 40:47)

 

For different reasons often it is difficult to codify knowledge as facts and rules that is first of all to extract them.
Extracting and formalizing or refining knowledge is a long difficult and laborious process.
And this process is known as knowledge engineering.

(Refer Slide Time: 41:37)

 

The other thing is explanation facilities.
These are imperative for acceptance of an expert system.
Now, the MYCIN expert system has nice front end developed known as TEIRESIAS that supported knowledge acquisition and explanation.
And it could answer questions like, why is that knowledge relevant?
How did that system come to that conclusion?
What is it currently trying to show?
And this interface also allows because when you start developing an expert system
It will be too much to expect that at the very initial phase your knowledge base will be so complete that you would not require any change or you would not need any knowledge to be put in.
We always learnt to do so therefore an expert system should also learn accordingly.
So there should always be a facility by which you can add knowledge
to an expert system.
And that addition can be done by TEIRESIAS in case of MYCIN.

(Refer Slide Time: 43:00)

 

 What is an expert system shell?
Now that is also another very important criterion.
As you can think of that an expert system consist of the knowledge base,
the inference mechanism, the working memory, user interface etc.
Now as we change the domain of application the first thing we will change for sure is the knowledge base.
But may be that when I have taken so much trouble in developing one expert system and developed its  inference mechanism, its conflict resolution strategy, its working memory structure and everything then I would not like to throw out everything instead I would rather like to change the knowledge base and apply in another application area if possible.
That will reduce the development life cycle of an expert system.
In order to meet this end expert system shells have been proposed. 
And expert system shells are expert systems where the knowledge base part is empty.
 So out of that you just take the knowledge base part make it empty and allow somebody else to put in some other knowledge base on that and that is expert system shell.
And such expert system shells have got the flexibility of giving you different conflict resolution strategies from which you can select depending on the domain such as different sorts of inferencing, forward and backward reasoning etc
And another thing we have not yet mentioned at all is representation of uncertainties.
The rules we use in an expert system are often very uncertain.
The logic is not as simple as 2 + 2 = 4.
For example given a particular soil quality a particular vegetable that will grow over there experts can say that this relationship of a particular soil quality 
a particular monsoon prediction will yield approximately this amount of  vegetable of a particular crop.
But all these are not absolutely certain and there are some uncertainties always involved.
The uncertainties come from different sources such as uncertainties in data, the uncertainties in our expertise also.
For example, when a patient comes to a doctor or doctor prescribes a medicine, he has got estimation from this expertise that this medicine will reduce the symptom within seven days to a particular level.	

But he cannot always be certain that within these seven days this will reduce
which is one uncertainty and the second thing is to how much it will reduce
is not always possible to quantify.
The frequency of fever or headache will come down but how much?
Therefore there are uncertainties in different levels.
But in order to be successful an expert system shell must provide some means of representing uncertainty.

(Refer Slide Time: 47:02)

 

Expertise2go: It is a free expert system shell where the shell provides the inference engine knowledge acquisition and explanation interface. 
The user must insert the relevant knowledge base/
Expertise2go Lite version is a java applet where a very nice GUI interface is available and there are features for knowledge acquisition, explanation, justification etc.

(Refer Slide Time: 47:45)

 

Here is a typical GUI where the expert system shell has been used to develop an automobile diagnostic system.
And this is a GUI which is shown to the user so the user can select the result of switching on the headlight is: 
We have just shown the example:
They light up, nothing happens, I do not know.
This sort of facility is also given.
Therefore your rule base must be able to capture those also.
Either you are certain or very uncertain you can submit your response.
You can also ask the system why you are being questioned like this and the system should be able to explain it to you. Therefore these are the typical GUI. 
In this expertise2go let us see typical rules that are present. 
This REM is a remark. 
We are showing a sample knowledge base here.
REM specifies a comment. 
 We also put in a comment because this is the rule.
The rule is going to check, is the battery dead?		

(Refer Slide Time: 48:55)

 

Look at these brackets which are the antecedents. If the result of switching on the headlights is equal to nothing happens. Now this is a peculiar case where we are allowing disjunctions also. The result of trying the starter equals nothing happens.
Then the recommendation actions are recharge or replace the battery.
Now depending on the shell that you are using the syntax of the rule will change.
This is a particular syntax here.
So in this particular case say the shell is providing me with some operators
like equals, not equals, equals one of or comparisons so look at the rule; is the car out of gas? 
That is what I want to test.
If the gas tank equals empty then the recommended action is refuel the car.
In that way there are rules.
And as the system fires the rules it also prompts for acquiring information, what sort of information are those?
Prompt: the result of trying the starter.

(Refer Slide Time: 50:32)

 
What happens when you turn the key to try to start the car?
The car cranks normally, the car cranks slowly and nothing happens.
This is being shown to the user and the user selects one as we have shown through the GUI.
For example it is prompting, a multiple choice like the smell of gasoline is present when trying the starter, not present when trying the starter so the user is asked to select. 
And these sort of prompt actions or [f…] are really activating this sort of GUI as shown here.

(Refer Slide Time: 51:08)

 

The inference engine in this e2glite shell is the inference engine works backwards from a goal, it is backward chaining.
Obviously it stops as soon as it finds the rule that concludes goal with desired certainty.
It handles uncertainties via certainty factors.
Now these certainty factors are really important. 	

(Refer Slide Time: 51:22)


 

This is a very important method.
But before that let me state that there are different ways of handling uncertainty.
In the following couple of lectures we will be dealing with different techniques of uncertainty management.
The different ways of handling uncertainty: 
One can be probabilities where we specify likelihood of a conclusion or apply Bayesian reasoning.
And certainty factor is an estimate of confidence in conclusions.
It is an estimate provided by the experts.
It is not as mathematically precise as is the case of probabilities but it is also very effective.


(Refer Slide Time: 52:46)

 

MYCIN is a medical expert system for analysis of meningitis infections.
MYCIN uses backward chaining rule based expert system.
It produces focused questions.
It also produces a natural language like interface for communication with the users.

(Refer Slide Time: 52:55)

 

Some typical rules of Mycin:

The syntax has now changed compared to the earlier one we saw.
If the stain of organism is gram positive and there is no disjunction law here and the morphology of the organism is coccus and the growth conformation of the organism is clumps. 				
If these three are true then remember it is a medical case we cannot serially with certainty so what we will say is there is suggestive evidence of some value .7 that the identity of the organism is staphylococcus.
That is a particular organism.
Now this rule in MYCIN can be translated in a Lisp syntax like structure but the premise of the rule is and there is a conjunction of in the same context
that is the organism and the strain of the organism is gram positive,
morphology is coccus and conformation is clumps.
Then action is conclude context and the conclude is the staphylococcus 
with a tally .7.
Now what is this .7?
That is a very key factor the certainty factor of Mycin.
But the beauty of MYCIN is the way it converses with the user.



(Refer Slide Time: 54:29)

 

What is the patient’s name?
The doctor types in John Doe.
Male or female and these italicized parts shown here is the system’s response.
The doctor says male.
What is the age? He is 55.
The system also understands this.
The doctor cannot write that to my mind he is approximately 55 so it is not a free flowing English but he can realize some restrictions that he can type in he is 55.
Have you obtained positive cultures indicating general type?
Yes the doctor says. 
What type of infection is it? It is primary bacteremia.
When did the symptoms first appear? May 5 1985.

(Refer Slide Time: 55:29)

 
		
Now it goes on.
Do you know the identity of the organism one? No
Is organism one a rod or a coccus or something else?
The doctor says rod.
In this way a very naturally flowing discussion goes on.

(Refer Slide Time: 56:26)

 


And ultimately MYCIN comes up with a recommendation. 
My recommendation will be based on the opinion that the identity of organism may be, note this may be Pseudomonas Aeruginosa, E. Coli and other such possibilities.
Then it furthers. So this is my provisional diagnosis.
On a scale of 0 to 4 it is asking the doctor where higher numbers indicate greater severity. 
How would you rate the patient’s degree of sickness?
The doctor says 3 and the MYCIN further goes on asking more questions and then comes up with a recommendation that there were so many possibilities and just as an expert doctor it says, in order to cover for items 1 2 3 5 and 6 give Gentamycin using a dose of 119 milligram 1.7 milligram/Kg for ten days, modify dose in the case of renal failure and in order to cover for item 4 give Cindamycin using a dose of so and so for 14 days.
Just see how naturally a system can communicate and that is what an expert system can do in order to come to a reasonable conclusion once  
it is nicely built and it is restricted in a proper domain.
 
Here is a quiz as an exercise.
What is an expert system shell and the second problem is really challenging on your part.			

(Refer Slide Time: 57:55)

 

Forward chaining and backward chaining concepts are made clear for your understanding. Now, Think how explanation facility can be incorporated in a rule based system.
By explanation facility find out that if at any particular point a rule has stated a conclusion.
You have got fever, you have got malaria for example.
Then if the user says why then the system explanation facility should be able to present that rule which really concluded malaria.
And might be that had some antecedents which have been found true then how did these antecedents were evaluated to be true or how is that these antecedents were found to be true that should also be found out such that which rules made these antecedents true.
What is the data structure you think will be helpful?
Data structure stack that you know of will be immensely helpful.

Transcripted by: R. Anjanaa lakshimi 
{Total slide time: 01:00:54}

Indian Institute of Technology
Kharagpur

Artificial Intelligence
Prof. Anupam Basu
Department of Computer Science and Engineering, IIT Kharagpur

Lecture #26
Reasoning with Uncertainty - I

In the last lecture we discussed about rule based expert systemsand we also gave the example of MYCIN. 
It was shown that MYCIN not only provides expert advice but also interacts with the user in a very natural way.
And often it is not possible to really distinguish between whether you are communicating with a real doctor or you are communicating with an expert system that is a computer program for that matter.
Today in our lecture we will start discussing about uncertainty management.
We have seen also in the case of MYCIN that MYCIN rules often put in some suggestive evidence. 
For example, if the symptoms are so and so then there is suggestive evidence .7
that this is this and so and so.
Now this suggestive evidence .7 or .8 whatever the number might be is actually talking of some uncertainty or some weakness in the strength of belief
of the expert in the conclusion if we were absolutely certain. 
For example, if the two sides of a triangle are equal then the triangle is isosceles
in that case there is no uncertainty in this conclusion.
However because of several reasons in real life scenarios like medical reasoning, automobile diagnosis and many other things we cannot be very sure about either the evidence or if the evidences are known and its relationship with the conclusion there is some uncertainty lying somewhere.
In the next few lectures we will see how such uncertainty is can be handled
in the case of Artificial Intelligence. 
To start with we will look into certainty factors because we have seen a glimpse of these.
So our lecture today will deal with reasoning under uncertainty.

(Refer slide time: 03:29)

 
		
There are different types of uncertainties. 
What are the different ways in which you can deal with that?
To start with we will be talking about certainty factors.

(Refer Slide Time: 4:44)

 

Let us look at this example.
We call it the doorbell problem.
The doorbell rang at 12 O’clock at midnight.
Now the question we want to answer is, 
was someone there at the door?
Was there anybody at the door?
And Mohan was sleeping in the room. 
Did Mohan wake up when the doorbell rang?
Suppose I want to answer these two questions.
My fact is that the doorbell rang at 12 O’clock in the midnight.
Therefore if we place the propositions in the logic form as we have already seen
at door x means if someone is at the door then he rings doorbell.
And there is another proposition, if there is doorbell then that wakes up Mohan. This should have gone here so wake Mohan.
So these are the two propositions.
If someone is at the door then he rings the doorbell.
If the doorbell rings that wakes up Mohan.
Now the problem is suppose the doorbell rang, we know that by the first proposition at door x implies doorbell.
If someone is at the door that comes from the previous proposition at door x implies doorbell
So the fact is that the doorbell ringed.		

(Refer slide time: 05:55)

 

Can we say that there is someone at the door?
If you recall the classical proposition and logic p implies q means if p is true then q is necessarily true but if p is false then q maybe true or q maybe false.
Therefore that is the implication and the implication operator we are showing here.
So, according to deductive reasoning if we know doorbell as rung then we cannot say for sure that there was someone at the door using normal implication because p implies q means if p is true then q is true.
But q can be true even if p is false so if q is true then p may or may not be true. 
According to deductive reasoning if there is doorbell ringing then we cannot for sure say at door x. 
But using our common sense or day-to-day style of reasoning often we tend to conclude if there is a doorbell then someone is at the door.
Although according to deductive reasoning that may not be true.
So this sort of reasoning where if there is something like p implies q
and we find q is true then we infer p. This may not be correct according to deduction but that is what we often do and this sort of reasoning is known as abductive reasoning.
Till now we have seen mostly deductive reasoning but in real intelligent systems often we take recourse to other forms of reasoning and all of you will realize and appreciate that when there is a doorbell then we immediately assume that there is someone at the door. 
So this sort of reasoning given this sort of p implies q type of implication
is known as abductive reasoning.

(Refer Slide Time: 8:17)

 

And whenever there is a doorbell you expect that someone is at the door and you rush to the door to open it.
Most of the time you are right but always you may not be right.
so this sort of reasoning is also very useful, abductive reasoning.
Later we will study about inductive reasoning.
Here you are introduced to abductive reasoning.
But then you may ask if there is a do doorbell then obviously there is someone at the door, but no, the doorbell might start ringing due to some other reason although less probable and mostly that may happen but still it is possible that the doorbell has rung for short circuit maybe because of wind or maybe the dog or some other animal just pressed around the doorbell or whatever there can be thousand and one reasons for that so what to do?
 How do you go about inferencing these things?
Again coming to the second question, given doorbell can we say that wakes Mohan up because we have already seen that a proposition is doorbell implies wake Mohan. 	

(Refer slide time: 10:37)

 

Now this is deductive reasoning.
Now we have got the doorbell so the p part of p implies q is true.
Therefore obviously it will wake up Mohan.
And deductive reasoning immediately tells you that, so it should always be true.
But if you think a little bit we can infer that the doorbell wakes up Mohan
when this implication is true doorbell implies wake up Mohan.
If this entire implication is true that whenever there is a doorbell that will wake up Mohan then obviously the doorbell will wake up Mohan.
But now we can also put this rule or this implication in question.
Is this implication doorbell implies wake up Mohan always true?
Maybe there may be cases that when Mohan is really tried which is not always the case and mostly that is not the case but if Mohan is really tired then even the doorbell will not wake him up.
Even if the doorbell rings Mohan may not always wake up.
So this implication that we are saying may be mostly correct but may not be always correct.
Therefore we cannot answer either of the questions with certainty.
What are the questions? 
Is someone at the door or did Mohan wake up? 
None of these we can answer with certainty.
The first one we cannot answer with certainty because that is an abductive reasoning.
Most probably the doorbell rings only when someone is at the door but there may be also chances of short circuit for which the doorbell rings automatically.
Relatively unnatural scenario can also wake up Mohan.
The second one is, does the doorbell wake up Mohan?

(Refer slide time: 12:29)


 
		
This cannot also be done with certainty because mostly a doorbell wakes up Mohan but this implication is not always true.
Sometimes it may not work.
Now proposition one was incomplete because the doorbell can ring mostly because of at door x but there can be several other reasons like short circuit, wind etc.
Now we could have modified the proposition in this way.
We can add on different other conditions at door x or short circuit or wind etc but it really does not help because there may be thousand and one reasons and how many reasons will you go on enumerating in order to make a complete proposition and mostly that is not possible.
Therefore this approach does not help because the list of possible causes here are huge in fact they can be infinite.
Now what was the second proposition? 
The second proposition was a doorbell implies wake up Mohan.
That is often true but that is not a tautology.
Tautology means something that is always true.
For example, Mortal x or not Mortal x that is always true irrespective of any interpretation.
These are called tautologies, A or 0A which is always true. 
These things are known as tautologies. 
So proposition two is, often true but is not a tautology may not be always true.

(Refer slide time: 16:50)

 

 How to do deal with this scenario?
Is there any way out?
But these sorts of problems like the doorbell problem is very common in fact
most of the real life problems we try to solve using Artificial Intelligence techniques are like this, the real world is like this.
It is not always 2 + 2 = 4.
So, in AI we often need to reason under such circumstances.
And in order to solve it we need to properly model uncertainty and impreciseness.
There is a subtle difference between these two words uncertainty and impreciseness. 
Uncertainty in a rule as, it rained, it will rain tomorrow, it will rain today, it rains in July etc are statements which are uncertain because although mostly it rains in July in the eastern parts of India but in many of the parts it is not always certain that everyday in July it will rain.
So these sorts of statements are always associated with some sort of uncertainty.
On the other hand impreciseness is inherent in most of the statements we make.
The boy is quite tall; it is quite likely that it will rain in July.
What do you mean by quite likely? The boy is very tall, what do you mean by very? 
I like him very much, how much do you like him? Is it 5, 10, 15? 
Now these are not precise statements. 
The height of the building is for example 20 ft, 200 ft or whatever then we quantify that and that is precise.
But mostly many of our statements are not precise.
Therefore in order to deal with real life problems we have to handle both uncertainty and impreciseness. And that can be handled using appropriate reasoning techniques.
Now let us look at the different sources of uncertainty.
We have seen the doorbell example.

(Refer slide time: 19:11)

 

Therefore implications may be weak. 
They are not absolutely certain that if there is a doorbell then that will wake up Mohan.
We can rather write the implication in this way; doorbell .8 implies wake up Mohan.
That means now this implication is being given strength of .8.
What does it mean? 
It may mean that eighty percent of the time doorbell rings Mohan wakes up.
So, if I have written doorbell implies wake Mohan then that is a certain proposition as every time the doorbell rings Mohan will wake up.
But since that is not the real life case we are putting up some quantification of the frequency with which the rule applies.
This rule is mostly applicable in eighty percent of the cases so I write down .8.
Now imprecise language like often rarely, I rarely meet Tom, how frequently? Sometimes, so all these typical words we use in our day-to-day life leads to impreciseness in the statement.
Now in order to really work in the AI area and to develop real applicable systems we have to quantify these imprecise terms in terms of frequencies might be.
One possible way is to code them in terms of frequencies.
The first thing is we need to quantify them and also we need to design rules for reasoning with these frequencies. 
We have to deal with the rules, we have to write rules in such a way that these uncertainties are captured in those cases.
Therefore this is one source of uncertainty, implications may be weak. 


(Refer slide time: 21:47)

 

There is another source of uncertainty.
It is really difficult to get precise information all the time.
We have to leave with imprecise information. 
Whenever we hear some sound like somebody speaking through the telephone the words are propagated as waves through your telephone line
through a channel where there is lot of noise. 
And there may be some words which are wobbled up and we are intelligent to really understand what  that sound was or even if that sound is disturbed or changed during transmission because we cannot expect that noiseless channel to propagate the wave.
Therefore to get the precise information is always difficult.
In the earlier case we have seen when does the doorbell ring?
In order to code that if we had to do really make it very precise we have to write down so many things, at door x or short circuit or wind etc and that is not possible so that is another source of uncertainty. 
Another problem that can occur is incomplete knowledge.
Knowledge as we say is always incomplete as there is no end to learning.
Whatever knowledge you put in your knowledge base that is never complete,
you may learn something new tomorrow. 
But even if whatever knowledge we put in our knowledge base and we start working with that our knowledge base will be incomplete and we will have to live with it.
We may not know or guess all the possible antecedents or consequents.
Maybe I thought that the doorbell rings besides somebody being at the door, short circuit or wind or animal but there can be some other reason, the bell rang due to some other reason and I really do not know. That means my knowledge is incomplete. 
So, in order to build a system robust and reasonably good and at least trying to mimic human intelligence we have to deal with such incomplete knowledge.

(Refer slide time: 25:40)


 

The third source of uncertainty comes from the fact that there are conflicting information. 
You take a patient with some complicated symptoms, you go to two different doctors and it is quite likely that the two doctors may differ in their opinion.
If the disease is not the regular things that we see and some very new peculiar symptoms that we have seen and there are different groups of doctors who may differ in their opinion.
So experts often provide conflicting information because the actual truth is not known with certainty.
I may make some statement with some belief from my experience
I put in that statement, I say I am eighty percent sure that this patient is suffering from Thalassemia but the twenty percent I am not sure so another doctor can say well I am seventy percent sure that this same patient is suffering from some other disease.
Maybe actually both the diseases may be there in the patient or either of them of these experts may not be correct so that is a reality.
 So experts often provide conflicting information so when the doctors or the experts speak about these they speak with a degree of belief so we have to quantify the measure of belief.
How strongly you feel about what you are saying?
How much sure are you?
It may be eighty percent it may be seventy percent but how much sure are you is measured of belief.
And there is another problem here that uncertainties are often propagated.
I had a scenario that a implies b and b implies c, if I perform chaining and suppose a is true then I come here and take this tool and infer b and since b is true I come to this rule and infer c.
This is chaining and in each of these implications that I had here it was a certain implication so it was with a belief one and this was also with a belief one.


(Refer slide time: 29:29)

 


(Refer slide time: 31:18)

 



But now if I say that a implies b with some certainty .7 and b implies c with some belief .6 and I know a then obviously the strength of belief with which I know b is less than it was here. 
So I infer b with a less belief and then I come to this rule and propagate so this thing is propagated, this lack of belief is propagated here and again this rule is not certain because it h- it has got a strength .6 so I infer c with even weaker belief.
Now let us take another scenario that a implies b. 
The strength of belief is .7 and a is known with some uncertainty, a is not known for sure and suppose a is known with some certainty of .8 now here this implication is a weak implication and further this antecedent is also not surely known so the strength with which b is inferred is also weaker than either .7 or .8.
So if I had b implies c then with some belief of .8 this implication then through this chain of reasoning c will be inferred with a much weaker belief.
This is what we mean by propagation of uncertainty.
So, in the absence of interdependencies of propagation of uncertain knowledge 
the uncertainty of the conclusions increase.
Here is an example.
Suppose I know tomorrow will be sunny with a belief .6 and tomorrow will be warm with a belief .8.
Now when I take them together; Tomorrow will be sunny and tomorrow will be warm what will be the certainty of this conjunction?
This is a conjunction of these two uncertain statements uncertain propositions
tomorrow is sunny tomorrow is warm. When I take them together what will be the strength of belief in this conjunction?
Now in order to handle such scenario certainty factors were proposed.  
This was proposed along with the development of MYCIN at the Stanford University so this is often known as Stanford certainty factors,
Stanford certainty algebra, etc. 

(Refer slide time: 33:05)

 

Now let us have a re-look at the MYCIN rules.
If it is one rule I am looking at the first antecedent is, if the stain of the organism is gram positive and the morphology of the organism is coccus and the growth conformation of the organism is clumps then there is suggestive evidence .7 that the identity of the organism is staphylococcus.
So the experts could conclude that the organism is staphylococcus with a belief of .7 assuming that all these three antecedents are true.
Now this .7 is the certainty factor.
What do certainty factors mean?

(Refer slide time: 34:18)


 

Now it is an expert estimate of the degree of belief or disbelief in an evidence hypothesis relationship.
We will be dealing with this sort of relationships now for a while quite frequently.
Here e implies h means given this evidence we conclude this evidence leads to the hypothesis h. 
The h stands for the hypothesis and e stands for evidence.
Now, if we go back to the earlier rule where from did this value come?
This value actually came from expert’s knowledge and subjective estimate of the relationship between the occurrences of these symptoms; Stain of organism being gram positive, the morphology of organism being coccus and the growth of organism being clumps and these three relates to the organism being staphylococcus with a strength of .7.
So these are subjective estimates, we are saying subjective probability estimate provided by the expert from his or experience.
So this .7 is actually a measure of belief that is writing it in this way MB means measure of belief of the hypothesis h given the evidence e.
Now this rule is supporting the fact that the identity of the organism is staphylococcus.
So this is the measure of belief. 
Similarly there can be measure of disbelief which I would have written MD(h, e) measure of disbelief say an evidence e relates to the hypothesis h with a negation.
That is, I do not believe that given this evidence this hypothesis will be supported so something like 0h.
If e tells 0h with some strength .6 then we can say measure of disbelief in the hypothesis h given the evidence e is .6. 
And let this be evidence e1 and some other expert given this e1 says this h with .8.
Then the measure of belief of h given e1 is .7.
Now certainty factor CF(h, e) is measure of belief in (h, e) minus measure of disbelief given (h, e).
So in this peculiar case certainty factor will be something like .8 – .6 = .2. 
So certainty factor is a measure of belief minus measure of disbelief.

(Refer slide time: 39:00)

 

So a rule can either say in favor of hypothesis or against a particular hypothesis.
A measure of belief like this is in favor whereas this one is against
so this is leading to MD whereas this is leading to MB. 
But the situation becomes a little more complicated when we have got more than one rule contributing to one hypothesis.
For example, there is a rule here A implies C and there is another rule B implies C, Now both this rules are pointing to C but with different degrees of uncertainty, different degrees of belief and disbelief. 
So this is the case a; several rules contribute to one hypothesis.

(Refer slide time: 39:45)

 

The second problem that can come up is, what is our belief if several propositions are taken together?
For example here A is a proposition which is an uncertain proposition and B is another proposition which is also an uncertain proposition, If I take them together A and B what would be my combined certainty about this?
Obviously that will be less.
The third case that can occur is, what is our belief in the result of rule chaining?
There is a rule A implies B with some uncertainty maybe in A in this fact itself or in the implication itself so there is some uncertainty here and there is some uncertainty here.
So when I chain them together what would be the certainty in this result C?
Obviously that will be even weaker but how do you compute that because ultimately we want to find some sort of a computable value.
So these are some typical cases which we will have to deal with.

(Refer slide time: 41:40)


 


Now certainty algebra is a heuristic or rather expert given approach for reasoning with certainty.
So measure of belief MB(h, e) now here you may face some rotational problems.

(Refer slide time: 43:11)

 




When we write MB(h, e) that means MB in h given e. 
That means if e is known then what is my measure of belief in h?
The same thing is written as MB h given e these two are equivalent and
the same thing is meant by both these notations.

Therefore measure of belief MB h given e lies between 1 and 0.
If MD(h) given e is 0 it will be some number.
Measure of disbelief is again between 1 and 0 when the other one is known to be 0.
And certainty factor of h given e is MB – MD.
So we start with a measure of belief which is MB(h, e),measure of disbelief is MD(h, e) and certainty factor of h given e is MB(h, e) – MD(h, e).
Now there may be multiple evidences which are supporting the hypothesis h.

(Refer slide time: 44:00)

 



I am given some measure of belief of h given e.
Now I can have additional evidence like here h is supported by e1 as well as e2 and there are different strengths of relations like e1 is supporting h to some degree, e2 is supporting h to some other degree.
Now what how can I compute MB(h, e1) and e2.	[
It is again the same scenario. 
Suppose I have got some e1 is supporting the hypothesis h with some strength .7 and there is another evidence e2 which is supporting h with some other strength .6, now what is my combined belief in h?
And this is what I want to know when I take both these evidences together.


(Refer slide time: 46:18)

 



So what is my measure of belief in h given e1 and e2? 
Now obviously if the measure of disbelief in h given e1 and e2 is 1. 
That means e1 and e2 taken together certainly makes me to disbelieve this
then obviously this 1 is 0. 
Otherwise what will happen?
Otherwise I will compute it as MB(h, e1) + MB(h, e2) times 1 – MB(h, e1).
Think of a space, this is my measure of belief in h given e1. 
And independently let me select another color, this is my measure of belief in h given e2.
Now what is my total belief given e1 and e2?
That would be this space. 
Now what is this space? 
This part is MB(h, e1), this blue part I add is MB(h, e2) but there is a part which is common here so I must subtract that.
And what is this part?
This part is coming twice so I will not take this entire part but a part of that
so times I multiply this with1 whichever is falling over here minus MB(h, e1).

(Refer slide time: 48:55)


 






(Refer slide time: 50:20)		

 

I can write this part as: 
MB(h, e1) + MB(h, e2) – MB(h, e1) MB(h, e2) this part is common.
This is the common part here.
If I take the common part out it will be MB(h, e1) + MB(h, e2) then this part will be1 – MB(h, e1). 
That is the expression.
Therefore this is the formula. 
Now, similarly for MD measure of disbelief will be similar.
MD(h, e1) and e2 will be 0 if the measure of belief is 1 otherwise it will be MD(h, e1) + MD(h, e2) times 1 – MD(h, e1). 

(Refer slide time: 50:55)


 

So it is the combination of hypothesis. 
The earlier one was combination of evidence.
Now we are looking at combination of hypothesis.
This is a little different. 
In the earlier part this is the combination of evidence.
When there are more than single evidence what is our strength in the hypothesis?

If there be multiple hypothesis h1 and h2 given e then e supports (h1, e) and supports h2 with different strengths.
Now what is common? If I take the conjunction what is the strength of that?
Now this is minimum of MB(h1, e) and (h2, e) that means I have got a hypothesis h1, 				

(Refer slide time: 52:02)

 


I have got a hypothesis h2, one evidence is supporting this and the same evidence is supporting this with .7 and this with .6 and I want to find whether e is supporting h1 and h2, what is the strength of that?
it will be the minimum of the weaker of these two links because both these will be true with the minimum value so this will be .6.
Similarly if I had or is implying either h1 or h2 then obviously this majority part will come here as .7.
So that is the formula that measure of belief of h1 and h2 will be the minimum of the individual beliefs and the disjunction h1 or h2 will be the max of these two h1 and h2.
Now, disbelief will be computed analogously in the same way.
In place of MB it will be MD.
Now this is needed for calculating the certainty factor of a rule antecedent where there are several clauses.
The other thing that remains here is chaining of rules. We have seen this chaining of rules several times.

(Refer slide time: 54:20)

 

Now let MB'(h, s)  s is  symptom and M B' note this prime be the measure of belief in h if we are sure about s.
But if we are unsure about s because s has been inferred by another evidence e so e led us to believe s and that was not certain so there was some weakness over there so when I do not have full confidence in this then this MB' will have to be modified.
MB' is the measure the belief in h if s is known for sure but s is not known for sure therefore given the uncertainty in s
we will have to modify the MB' in this way that will be multiplied with max of 0 and certainty factor of s given e.
Whatever this one is if this is 0 then this will be 0, this entire thing will be 0 if s is is known to be false for sure then this will be 0
otherwise whatever be the certainty factor here that factor will multiply MB' to give the measure of belief in s.

Note: Preview of next lecture not added in editing part.


Indian Institute of Technology
Kharagpur

Artificial Intelligence
Prof. Anupam Basu
Department of Computer Science and Engineering, 

Lecture #27
Reasoning with Uncertainty - II

In the last lecture we discussed about certainty factors and briefly introduced certainty factors.
In this lecture we will further deal with that.
We have seen the different scenarios.

(Refer Slide Time: 1:13)

 

We will first start with the last point where we left.
Certainty factors in the case of rule chaining.
Now what was really attempted to be explained here is something like this;
That I have got some antecedent a pointing to some other antecedent b
with some certainty say .7. 
This is equivalent to writing it as a implies b .7, what does it mean? 
It means that if a is known with certainty then I will infer b with confidence or belief of .7.
So I would say that MB(b/a) = .7 in the normal case but we know that this a may not be known with certainty there may be some uncertainty in this a itself.
If the certainty factor of a given whatever evidences E be 1 then in that case it is a certainty but suppose if the certainty factor of a given some evidence E is less than 1 then obviously this .7 I get here should be modified.
Therefore we say it is MB’. 
		

(Refer slide time: 4:59)

 


MB’ is the measure of belief in b if a is known with certainty.
But a is not known with certainty.
Therefore this needs to be modified and I must have some MB(b/a) where this MB’ b/a must be modified.
MB’ b/a must be modified is the measure of belief in b given a is known with certainty and that would have been .7.
But a is not known with certainty so this has to be modified and so we multiply this with max of 0 and certainty factor of a given some evidence E.
Suppose a was known this part was .8.
a is not known with certainty but a is known with belief of .8. In that case this would have been modified to this is .7 times max of 0 and .8 so it will be .8 so it will be .56.

(Refer Slide Time: 5:27)

 

Suppose e is the evidence that led us to believe s and the certainty factor of s given the evidence e was CF[s, e]
So we take the max of that and multiply MB’[h, s] with this.
That leads us to the modified measure of belief when the rules are chained.
Next let us look at some examples. 

(Refer Slide Time: 6:03)

 

This is the traditionally old example. 
A is implying C with a certainty of .3, B is implying C with a certainty of .2
now what would be the total certainty of C?
Suppose first it was only known that A is true, B is true is not yet known so this rule was enabled so A implies C has been inferred with .3, now forget about this part.
Now we get another evidence B and there is a rule which says if B is true then also C is true but that is even a weaker rule where this relation is strengthened with the certainty factor of .2.
So we have seen in our formulae we can see that MB(h/e1 and e2).
That means in our case given B and A we will have the measure of belief of h/A plus measure of belief of h/B times 1 – measure of belief of h/A because it is coming twice	.		

(Refer slide time: 07:54)

 

Then we are applying this formula and we get .3 + .2 times .7 that is .44. And none of these rules spoke against C.
Therefore measure of disbelief in C is 0.
Therefore certainty factor is MB – MD so that will be .44 – 0 = .4 4.
Let us take a little more complicated example. 

(Refer slide time: 8: 44)


 

Let us consider two rules where the first rule says if something some animal has hair then it is a mammal.
And this implication has got strength of .9.
The second rule is saying if the animal has forward bulging eyes and sharp teeth then it is also a mammal and the strength of this rule that means 
the strength of this implication is .7.
So we have got two rules and we have got some facts as well.
What are these facts?
Suppose from prior observations or information we have found that certainty factor of this antecedent has hair is .8. 
So we know this antecedent not for sure because it might be you were driving in a car and the animal just moved in front of your car and you could have only a glimpse of that and you have got some information and some degree of belief in what you saw.
 With eighty percent confidence you are saying that the animal that ran across had hair and had forward eyes and that was .7.				
Now note that might be you have looked at the hair part and said you are eighty percent confident that the animal had hair and your friend who was in the car took a glimpse of the eyes and said I am seventy five percent confident 
that it had forward eyes and another friend said I am thirty percent confident that it had sharp teeth.
So might be that from different sources you have got these information 
So the database is like this now; 
C F in has hair is .8, C F in forward eyes is .75, CF in sharp teeth is .3 and I have got these rules.
Note two factors; 
The rules are also not hundred percent certain, they are certainty factors with these implications and neither are these facts hundred percent certain.
So our problem is that, given multiple premises the different premises 1 2 3
how do I combine them into one certainty factor when I take this rule?
Now certainty factor of P1 or P2 is max of certainty factor of P1 and certainty factor of P2 and certainty factor of P1 and P2 is min of certainty factor of P1 and certainty factor of P2.
So if I consider rule R2 the certainty factor of forward eyes and sharp teeth 
if I apply this formula then I should take the min of sharp teeth and forward eyes confidence.
So min of .75 and .3 and min of .75 and .3 is .3.
So this antecedent part of the rule R2 is .3.
Now these are the rules has hair implies mammal with certainty factor .9, 
forward eyes and sharp teeth implies mammal with certainty factor .7. 
Now, has hair is a single antecedent so its certainty factor is already known 
it is .8.
Therefore now I know the certainty factor of all these antecedent parts
it is .8 and these 2 together is .3 it was .75 and .3 so since they are handed
I have taken the min of those so it is .3.
Now how do you combine with the certainty factor for the rule?

(Refer slide time: 13:34)

 
	
The formula is that certainty factor of a hypothesis given a rule.
Now this rule is evident.
We know that certainty factor of H/e is a certainty factor of the premise that is this part and this has to be multiplied with a certainty factor of the implication.
So, for the first one it is certainty factor of has hair times certainty factor of R1. 
So the certainty factor of has hair is .8 and the certainty factor of the rule is .9 so that is .7 2.
The first rule tells me that I infer that the animal that ran across the front of my car is a mammal with certainty .72.
Now if we look at the second rule that given R2 what is the certainty that it is a mammal?
Then we know that the certainty factor of the antecedent part the forward eyes and sharp teeth is .3 and the certainty factor of the rule itself is .7so if we multiply them it is .2 1. 
Now we come to a point when two rules are stating that it is a mammal, one with a confidence of .72 and another with a confidence of .21. 

(Refer slide time: 16:02)

 

Then we have got different rules with the same conclusion.
Note that it is the same as here we can consider the rules to be evidences,
my evidences are changing, when I had this has hair the CF in that was .8
then this was an evidence that was talking of the hypothesis that is mammal
so I combine them by multiplying. 
This was .9 so these two got multiplied and I got .72.
Similarly I had another rule forward eyes and sharp teeth so this was my e and that was giving to my hypothesis that is mammal and this came to .3 and this was something like .7 this implication was .7 so that got multiplied and I got .21.
Now this is again mammal so I have got R1 and R2. 
Both these rules are talking in favor of mammal.
So the common hypothesis h that is mammal is being supported by two2 rules R1 and R2.
So this is evidence one and this is evidence two.		

(Refer slide time: 18:02)


 


(Refer slide time: 20:02)

 

So the same thing can be applied .Combining these rules is the same as looking at it as e1 and e2 is leading to h or both R1 and R2 are leading to mammal and this came up with .72 and this came up with .21.
Now how do I combine these two rules?
 If we take the rule R1 then I will infer mammal with a confidence of .72.
But the second rule which has got in itself .21 will be added to this
but before adding it should be multiplied with 1 – MB[h, e1] the same old formula we did here.
Now by applying this formula we get .72 + .588 so it is .788.
Now a very relevant question can be asked.
If you recall the architecture of or the mode of working of an expert system rule based system there are different rules R1 R2 R3 and the inference machine will ultimately decide which rule to fire.
So this computation we have just now shown is, we first apply R1 and infer that  the animal is a mammal with a certainty of .7 2 by applying rule R1 and we apply rule R2 after that then we get this figure .788. [Refer slide time: 21:15]
But there is no assurance that the inference machine and the conflict resolution strategy will always follow the same order of firing the rules. 
What would happen if the rules are fired in the alternative way?
R2 is fired first and we infer that the animal is a mammal with certainty factor 
.21 and after that we fire rule R1.
Then what would be the computation? 
First if rule R2 is fired then we infer with a measure of belief that mammal given R2 to be .21.
Then we fire rule R1.
Then we have to combine these two and we will apply the formula that
 mammal given R1 and R2 to be MB(M/R2) + MB(M/R1)[1 – MB[M/R2]]

(Refer slide time: 23:35)

 

It is just the complementary, the opposite.
So here you see this is .21 + .72[1 – .21] = .2 1 + .72 times .79. 
And if you multiply that it will be .21 +.576 if you do it with .8 for example 
it will little less than this as shown.   
So if we combine this using the same formula we will get the same value if we do it in this way. 
Therefore what would this factor get changed to? It will be .79 so this should be .72 times .79 so this part up will add up to the same value so we will find that it will be approximately .788.
 So what does it mean?
This means that certainty factor should be independent.
The law of combination should be independent of the order in which 
the rules have been fired.
So certainty factor of some hypothesis given e1 and e2 should be the same as certainty factor of h/e2 and e1.
This means the combination rule should be such that it should be commutative.
 Also another property this combination rules should support is that it should be associative. 
that means I first do e1 e2 and then take e3 that means I am combining e 1 and e 2 first and then combining with e3 and given that the certainty factor of h given this should be the same as if I had combined it in a different way that is h/e1 I have just done and later on I do say e2 and e3 should be same. So that is the associative property. 


(Refer slide time: 26:02)

 

And the other one we discussed about is the commutative property.
Now these two properties must be satisfied by the certainty factor combination rules.
And unfortunately the certainty factor combination rules were designed in a way that these properties are satisfied.

So if we are given multiple premises how do you combine them into one certainty factor?
The summary of certainty factor algebra we have discussed till now is that if it is certainly true. 

(Refer slide time: 26:35)

 


We are probably familiar with the term conditional probability that given an evidence e, given a particular symptom headache what is the probability that the patient has got migraine? 
So if we know for sure in that case we can say that the probability of h/e is 1.
Now this is equivalent to in our certainty factor algebra that MB[h/e] is 1 and MD[h/e] is 0 and therefore the certainty factor of h/e is 1.
If it be certainly false then probability of h/e is 0 then MB is 0, MD is 1 and certainty factor is – 1. 
Therefore the range of certainty factor can be from 1 to – 1.
Whereas the range of measure of belief is from 0 to 1 and MD is also 0 to 1
but certainty factor is from – 1 to + 1. 
When I do not know the relationship between the evidence e and h then whatever probability h has is the thing and there is no question of any conditional probability. In that case I have got no measure of belief, measure of disbelief and the certainty factor is 0, therefore I cannot do anything. 
Combination of evidences is what we already saw, certainty factor of e1 and e2 is min of certainty factor of e1 and certainty factor of e2 or disjunction is max of e1 and e2. Then implication that is if e than h this CF[h, e] is CF[e] [CF[h/E] where E is the certainty with which we know h if this e was known with certainty.
Note that there is a case where we have done for rule chaining.
MB’ times the certainty factor the same thing.
This is the way the certainty factors are computed. 
Next let us have a revisit with a MYCIN rule because MYCIN first introduced certainty factors. 
So let us see how the MYCIN rules do the same thing with certainty factor. 

(Refer slide time: 30:30)

 

Here is a rule that we have seen earlier. 
If the stain of the organism is gram positive and the morphology of the organism is coccus and the growth conformation of the organism is chains
there is suggestive evidence with certainty factor .7 that the identity of the organism is streptococcus.
The earlier rule was staphylococcus and was clumps. But here it is chains and streptococcus. So this is altogether another rule. Probably this rule is a little different.
So we have got the certainty factor.
What is my certainty factor in this complete evidence?
Suppose this is know with .5, the stain of the organism is gram positive
that has been known from some other rule and that has been known with .5
and this has been known with .6 and this one has been known with .3. 
So the evidence of all these conjunction will be of .mean of .5 .6 .3 so it will be .3 so it is the belief of the antecedent part. 
Now this entire rule is saying that this conclusion is true with .7 provided these are known for sure, these are certainly true.
Unfortunately these have come from somewhere, the stain of the organism has been found through some test at some laboratory and you do not have hundred percent confidence in that so you had some .5 confidence into this.
Therefore the strength of belief in this conclusion that the identity of the organism is streptococcus has to be modified. 
It has to be modified again as certainty factor of h, e certainty factor of e times this should be multiplication of certainty factor of h, E.
That means if these evidences where known for sure then it will be .7.
But unfortunately these evidences are known with certainty of .3.
So ultimately MYCIN inference machine will infer that identity of the organism is streptococcus with a confidence of .21.
And if there is some other rule, for example there is a rule which is something like this, if streptococcus then XYZ and the confidence the certainty factor of that is .8.
Now from my earlier rule I have inferred streptococcus with a confidence of.21 and .8 will be .8 if this streptococcus was known with certainty but it should be multiplied by .21 and whatever comes through this multiplication that value will be the confidence the certainty factor that will be associated with XYZ in MYCIN.


(Refer slide time: 34:18)

 


Quiz:
Compute the following:
Suppose we are trying to confirm some hypothesis h the initial observation is that s1 confirms our belief in h with a measure of belief .3 that means there is some rule like this; s1 implies h with certainty factor .3.
Later on another test report comes from another lab and there is another rule s2 that is confirming the same hypothesis h with certainty factor .2.
So s1 confirmed h with .3 and later on another report comes where s2 confirms h with .2 and now as we have got both these you are asked to find the certainty factor of h/s1 and s2.
Now let us discuss a very important issue namely the MYCIN rules or in-certainty factors.
This factor is behind the success of certainty factor. 
Or you can say that it is also a limiting factor of application of certainty factors
known as indeendence assumption in MYCIN.

(Refer slide time: 37:02)

 


Let us look at this rule.
If the stain of organism is gram positive and the morphology of the organism is coccus and the growth conformation of the organism is chains then there is suggestive evidence with a certainty of .7 that the identity of the organism is streptococcus.
Now all these three antecedents have been combined into a rule because 
these are not independent. 			
Maybe that if it is gram positive then being gram positive also makes the morphology coccus so they are not independent. 
Since they are not independent they have been clubbed together and the expert has combined them together with a certainty factor of .7 
If we assumed that each of these antecedents were independent and all of them have got an independent certainty factor of .6 and certainty factor algebra was applied, now what do we mean by independent?
In that case that being gram positive does not affect causal, it is not something that the morphology being coccus.
And the morphology being coccus is not casually related to the growth conformation in change.
These are three independent observations. The boy is tall, or here is another example, if we say the father of the boy is rich and the boy has a car then in all priority these are causally related.
But if there be something like that the father of the boy is tall and the boy has a car then these two are two independent statements which are not causally related and I independently find the truth in each of these propositions.
So we say that two propositions are independent if they are not causally related.
If they are not casually related in that case and if we assume that each of them have got certainty factor .6 then let us see what happens and we apply certain factor algebra.
Then MB measure of belief in h/s1 and s2, s1 has got .6 and s2 is .6 so the combined belief is .84.

(Refer slide time: 40:25)


 

And if we take s3 so we are talking of this rule, this was .7, this is .6 and .6 
so when we combine these the first two then we get .84 and when we combine the third one with it then it becomes .936. 
The figures were .6 then .84 and then .936, let us see another interesting phenomena here. 
The first one here are independent, s1 supported h and it was with .6, and s1 was again with .6


(Refer slide time: 42:40)

 

now I got another .6 so from this .6 my belief went to .84 because these two together now supported h. 
But when I put in s3 obviously since that is another supporting evidence
which will further support this but this has gone up to .936.
So as I am putting in more and more evidence my confidence in the overall thing is going up exponentially but is gradually going towards saturation where ultimately it can saturate to the value 1
This is one observation that one additional evidence s2 along with s1 really can support this in favor or if it be MD in disfavor.
But gradually as I put on more and more evidences then it tries to saturate
and that is only natural.
So it is coming to .936 which means the overall rule is coming to this.
So obviously it is not tallying with what the expert had said that it should be .7
 the original rule has said that it is .7 but here if I take them independently then it is becoming different.
But in real life many of the things will not be independent so here comes the real expertise that I am clubbing together all the dependent clauses and using my expertise I am putting in one particular confidence factor into it that is .7. 
When I do it then it becomes as if e implies h and these two antecedents are taken together.
 Here is another example. Let us consider these three. 
Here three notations are used S, W and R.
These are three propositions.
S is stating that sprinkler was used last night. For example, you are maintaining a lawn and you want to keep the grass green so you have put in a sprinkler which sprinkles water over there.

(Refer slide time: 45:54)

 

Now S is sprinkler was on, it was put on last night and the statement W says grass is wet this morning today the grass is wet and R is another proposition it rained last night.
Now if I assume that all these are independent and I write a rule R1 if S that means is the sprinkler was on last night then W then today morning the grass will be wet. Independently this rule is fine there is no problem with this and I have put in a confidence .9 because I was not too sure how long the sprinkler will be on but if this sprinkler is on in the morning the grass will be wet.
Now suppose another rule has been written independently;
If I find the grass wet today morning then R that means then I will assume that it rained last night.
This rule is also right individually when these two are seen individually.
I am making a statement that, if the sprinkler was on yesterday night then today the grass will be wet and I am putting .9 degree of belief or certainty factor to this rule.
Another rule is saying that, if the grass is found wet today morning then I will assume that with .8 certainty that it rained yesterday night that is also very logical. 
Now the problem comes when we combine these rules.

(Refer slide time: 47:55)

 

If the rules are chained, if S then W and rule two is, if W then R then on chaining we will find a rule if S then R and what is the certainty of that? 

If I go on combining MBWS it was .8 this rule and I combine this with this rule that is MBRW which will be .7 2 then because R was it rained last night
I conclude that it rained last night because I found with .8 certainty that the grass was wet.
So I have concluded that it has rained with a finite certainty while the actual reason was the sprinkler being on.
So when the sprinkler was on I just looked at the grass and using my rule 
and assuming that these two rules are fine independently I combined them. Actually the sprinkler was on, my intermediate effect was the grass being wet and I inferred that it rained. This is the problem and this is not correct.
So we believe that it rained because the sprinkler was on.
Again this is a violation of the independent assumption.
So what is very important in order to do this is to understand the notion of causality and independence.
What is meant by causality here?
We can simply look at some event e1 causes another event e2.
Maybe another event e3 is causing e 4. 
So these are the causal links where one is causing the other e1 is causing e2 
although e3 and e2 were not causally linked.
So we must look at the scenario and if they are not causally related we should not combine them which is a very important consideration. 
So in this rule if the grass is wet, the action wet and rain should not be combined with this sprinkler business and if they have to be combined then this combined belief has to be given a separate certainty factor.
Now with this discussion of certainty factor next we will move to another very popular way of inferencing known as Probabilistic reasoning.
Now certainty factor is not strictly following the formal rigor of probability.
Later on we will once again see how it deals with uncertainty and then we will see how the theory of probability can be applied to deal with uncertainty.



Indian Institute of Technology
Kharagpur

Artificial Intelligence
Prof. Anupam Basu
Department of Computer Science and Engineering, 

Lecture #28
Reasoning with Uncertainty III

In the earlier lecture we discussed about reasoning with uncertainty 
and there we discussed about certainty factor algebra.
And we had also shown how MYCIN uses certainty factors in order to deal with uncertain reasoning.
A very powerful tool for reasoning with uncertainty is the probability theory and probabilistic techniques.
In another term that is called statistical reasoning. 
Many of you might be familiar with probability and we will have a brief recapitulation
of the basics of probability theory today and then from there we will proceed towards reasoning using probability of events.
We are having some confidence associated with some rules.
However, these confidences or the certainty factors or the measure of beliefs or disbeliefs were being attributed by subjective decisions taken by experts.
However, if we had actually conducted a number of experiments, for example, a particular rule a implies b, out of thousand times when a is true how many times did b become true?
If we could make a through experiment about this and based on the frequency of the times when a implies b really holds we could have really given a much more realistic value or a more realistic strength of belief to the rule.
That is the basic idea of probabilistic reasoning.
However there are associated problems and complications as well.
Before going towards the probabilistic reasoning we will re-look on the issue of uncertainty in decision making problem.

(Refer Slide Time: 3:47)
 

In the earlier lecture we had talked different sources of uncertainty, here we are looking at the same issues but they are being presented in a different way.
One issue is expressiveness. 
We have to deal with uncertain situations because of our inherent limitation of expressing very concretely some of the facts that we deal with.
So the question is, can concepts used by humans be represented adequately?
Can the confidence of the experts in their decisions be expressed?
The experts take some actions.
When they are faced with the situation they apply their intuition, their common sense, their experience and take some actions.
But what really led them to take that action is often not very clearly expressed.
If you recall while discussing about knowledge acquisition we mentioned the same thing that it is very difficult to extract expertise from the experts.
Now when the experts take a decision under an uncertain situation how do we ensure to extract the exact mechanism or confidence or the probability whatever the expert applies. 
The second issue is comprehensibility.
How do we represent the uncertainty?
First of all that uncertainty has to be represented and secondly that representation must be such that we will be able to utilize that in the standard reasoning method like Modus ponens or rule based inference or whatever.
If you recall we will not call an organization of information to be knowledge
unless that is usable to perform inferencing.
The same is true here.
We have to represent the uncertainty in a way which can be reasoned with it.
The third factor is correctness, how correct is it?
When we take probabilities obviously anything that is having a probability of .7
is not known with certainty.
So if I take this .7 probability as a measure or as a key factor to take any decision how correct would my decision be? 
a probability of .7 means that in 70% of the cases it is true but in 30% of the cases it is not.
So obviously there is a chance that the decision that I take will not be correct
if it falls in that 30%.
Therefore the probability must be very reliably calculated and gathered
from a huge set of data.
The other thing is relevance ranking and long inference chains.
We have seen in our earlier discussion about certainty factors that as we propagate through rule chaining for example a implies b, b implies c.
As we propagate or proceed through a longer chain and when our starting point a is known with some uncertainty which means not 100% certain
and a implies b is also an uncertain rule then obviously b is inferred with a lesser certainty.
So some error that may have got in when I have assigned that uncertainty in the rule or in the fact a we will propagate in b.
And then when I use the rule b implies c and take a uncertain decision about c
with some belief then if there were some error that error will be propagated. Therefore that is another issue. 
Another important thing is the computational complexity.
We may like to represent uncertainty in different ways but ultimately the computer will have to take the decisions.
So the computational complexity maybe of the order n or n2 
whether it is polynomial or whether it is exponential all these issues will rise because we have to take the decision in sometime.
In some cases we have to take it in real time but in some other cases also there must be a finite time we cannot spend a lot of time to just make a computation for one uncertainty handling and there will be so many other uncertain cases and for every time we will not be able to devote that much computational time.
Therefore computational complexity becomes an issue because we want to have fast decision making.

(Refer Slide Time: 10:00)

 

The other source of uncertainty is data.
Missing data:
Some data may not at all be available.
That is, suppose we have got sensors put in different places but there are some very difficult places where it is very difficult to approach and put in a sensor then we will not get data from there.
There may be that we are collecting data from hundred sensors but some of the sensors fail and we are not getting data.
But in the absence of that data also we have to carry out the reasoning. This is known as missing data.
The other thing is unreliable data. 
The data may be unreliable because when we receive the data the data is propagated through some channel and that channel may be noisy. 
For example, you are carrying out a blood test and whatever machine you are using for the blood test is not very reliable so you get a gross idea of the blood report but that is unreliable.
In some cases it is ambiguous and imprecise. 	
We do not very clearly specify those.
We have already seen some expressions like very tall, quite big, sometimes, frequently, seldom etc are some of the statements we make which are not precise, seldom, often, how often? How many times in a day? 
How many times in a week?
Sometimes the data may be inconsistent. 
You may get the data or the information from multiple sources 
and the data may be inconsistent.
Sometimes the data may be subjective and sometimes the data may be derived from defaults.
Now this requires a special mention.
What do we mean by default? 
Default means assuming something when nothing is known to the contrary.
If you recall in the passing 
I mentioned about abductive reasoning, and deductive reasoning has been discussed.
We have also mentioned about abductive reasoning.
There is another very interesting type of reasoning called default reasoning.


(Refer Slide Time: 14:02)

 

Now what does this default mean?
Default means assume something like x to be true when nothing is known
to the contrary. 
That means I do not know, I do not have any evidence, no evidence is right now available is right now available that contradicts x that says not x, there is nothing as such. So when nothing is known to the contrary then assume something. That is called default reasoning. 
Here is an example; Fido is a dog and you know dogs have four legs. 
Therefore I can infer that Fido has four legs.
Now if I know that Fido met with an accident and had one leg amputated 
Then obviously this reasoning will fail.
Now I am feeling tempted to ask you what is this sort of reasoning?
Fido is a dog so I can say dog Fido and dog x implies four legs x and from there I infer four legs Fido.
What is this form of reasoning? 			
This example relates to Modus ponens which looks like P implies Q and P is true so we assume Q is true.

(Refer Slide Time: 16:02)

 

Now if I had known that Fido met with an accident and had one leg amputated
then obviously this would have failed. 
But in the absence of that knowledge I will certainly apply this rule and assume that Fido has four legs.
Now say I have got some facts from the domain of cricket matches, so I want to know about batsman x and I have got some details about batman x, 
I know average runs scored by him, number of centuries he made etc. 
Suppose in my knowledge base I do not know whether he is a right handed batsman or a left handed batsman, it is not known nothing is there.
Now on a sudden a query comes as which hand does x uses while batting?
Normally given these facts I will not be able to answer these questions because I do not have that information. 
However if my reasoning mechanism says that for any such query
If nothing is known if not known assume right handed. Then with this power I will be able to answer this question and I will say right handed x.
However this may not be true but this is the default reasoning that we do
because might be now if here it is said left handed x then obviously this information which is present will overwrite this default and this information will not be holding any longer.
This is what is known as default reasoning. 
So whenever we take some conclusion based on default reasoning obviously some amount of uncertainty mixed in that.
The second point we are coming to is expert knowledge. 
As we are mentioning time and again there may be an inconsistency between different experts.
Often the experts talk about plausibility that is a best guess of experts, best guess so again it is not a certainty.

(Refer Slide Time: 18:45)

 

The other important factor is the quality the causal knowledge.
If we know the causality the cause effect relationship, for example the resistance of an electric circuit affects the current that is flowing and also the voltage causes the current to flow. 


(Refer Slide Time: 21:34)

 

Now this is the effect and if we really know for example this causality is known by the well known law of Physics the Ohms law.
We have got a deep understanding of the causality the relationship between the cause and the effect.
But in many cases that is not true.
We do not know the area to such depths. For example, psychology we are still trying to model the human brain.
We do not have any clear grasp on how the mind or how the psychology works
So we cannot make a very strong causal commitment over there.
Instead in many cases where the causal knowledge of the deep understanding is not available we go through statistical associations which are observations. We look at several cases and based on that we carry on our decisions.

(Refer Slide Time: 22:02)


 

Knowledge representation: This is another issue.
How do we represent knowledge?
When we discussed about knowledge base systems we said that we cannot really make a complete knowledge base.
Therefore in order to make it really good we have to narrow down the domain
so that we can go deep into it. Hence we actually work with a restricted model of the real system.
The actual system may be very diverse but we are working with a restricted version a narrow version of that and limited expressiveness of the representation mechanism.
Inference process is also another important issue.
We know what deductive reasoning or deduction is.
We know that there is sound so the derived result is formally correct but we may find that it is wrong in the real system because the rule may itself be wrong. 
There is a subtle difference, my reasoning is right but my rule is wrong.
So again coming back to our Modus ponens P implies Q and P I infer Q
this inference process is fine, this inference rule is correct but the correctness of this result depends on the correctness of this rule.
If this rule is not correct then obviously this inference also will not be correct.
so there is a difference between the soundness of the inference process and 
the correctness of the result.
Even if the inference process is sound the result may not be correct unless the axiom or the theorem or the rule that I have written over here is not correct.
Inductive reasoning: The new conclusions that we get are always not well founded. 
Again the different inference mechanisms with which we are by now familiar is deductive the other one is abductive. 
Now I introduce another one that is inductive reasoning.
And actually it is not new because you will see that day-to-day we often use this.
For example, to a child I show one ball a particular ball that is a round shaped object. 
So, a1 is some round shaped object and I tell the child that it is a ball so the child associates the round shaped object with ball.
Next I show him another ball which is another round shaped object of a different color and I say this is a ball.
I show him another might be a football but the earlier two were smaller balls 
like cricket ball, hockey ball etc. Now I show him a football which is also a round shaped object but only the size is different and I say this is a ball. 
Gradually the child learns that for any round object that is shown to him is a ball. 

Therefore all these are instances and based on that the child learns a rule that is for all a that is round for all round objects ai implies ball.

Now I show him an apple or an orange which is round a well shaped orange and the child will infer it as a ball because he has inferred that all the round shaped objects is a ball because I have shown him large number of instances
n number of instances and from there I have made a generalization.
Now if these instances are proper then for most practical purposes this will be a very useful way of reasoning. 
How do we infer? How do we learn? 
Actually most of the things we do is through induction.
This is the process that goes on.				
So this is known as inductive reasoning.

(Refer Slide Time: 28:18)

 

However as we have shown here this is not sound because there may be counter examples. 
The same unsoundness was there in the case of abductive reasoning also.
So inductive reasoning in that the new conclusions that we arrived at are not always well-founded.
The other thing is individual rules.
There can be errors in the individual rules, there can be representation errors, 
inappropriate application of the rules etc.
Now we have already seen this case that the likelihood association of the evidence we have seen just as we did in the case of certainty factors
we had put in several certainty factors with the premise for the conclusion
and the combination of evidence from multiple premises.
All these examples we have already seen while discussing about certainty factors.
In rule based system there can be another issue like uncertainty can come up when we have got multiple parts and there are multiple rules to use.
Therefore here we bring in conflict resolution which means from the set of multiple rules we select one or two and apply them.
Now which one is selected that may give rise to some sort of uncertainty.
The compatibility, the contradiction between rules, the subsumption where one rule may be more general than the other one, there may be missing rules. 
When we collect the data from multiple sources of different reliability 
we will again get a varied result, the reliability of the ultimate fused data set will not be the same.
Now, in order to handle such complicated scenario a probabilistic approach is required.
Basics of probability theory:
Probability theory is a mathematical approach for processing uncertain information.

(Refer Slide Time: 30:45)

 

And we talk about a sample space.
What is a sample space? 
 A sample space is the set of all possible events.
For example, when we deal with a dice when you toss a die which has got six faces the possible outcomes are 1 2 3 4 5 6 so that is a set. It can be discrete or continuous.
For example, when I think of the possible temperatures that can be there tomorrow 
the typical weather report what will be the maximum temperature? 
It can vary in a continuous range. 			
So, this sample space can be considered as a set.
And what is a probability? Probability is a real number P(xi) where xi represents any of these events.
So we call these to be the events and any of these events is associated with a real number which denotes the likelihood of the event to occur.
And as you know it is a non negative value between 0 and 1 and total probability of the sample space is 1.
These are the basics and here is an example.
We can always think about it as a set.
It is a set of possible events in the case of a dice throwing. 
The possible events are 1 2 3 4 5 6. 
So when I throw a die any of these discrete events will occur.
Either 4 will occur or 5 will occur and each of them have got a probability associated with it.
Here this is xi and if the dice is fair then you know that the probability of an event xi is 1/6 in this case because all of them are having equal probability.
And the total probability of the sample space is 1. That is, when all these are added together that will be 1.

(Refer Slide Time: 33:28)

 

 Now for mutually exclusive events, what is meant by mutually exclusive events?
Two events cannot take place together.
When I throw a coin it cannot be head and tail, it will be either head or tail.
The probability of at least one of them is the sum of their individual probabilities.
The probability can be of two types namely experimental probability or subjective probability.
Experimental probability is based on the frequency of events.
If I carry out a fair experiment of throwing the die or if I take a biased coin
and throw it ten times may be it will not be five times head and five times tail.
If it is biased then may be I will get seven times head and three times tail.
But with that experiment being done quite a number of times 
We can come to a fair degree of association of the frequency of the events.
The other thing is the subjective probability that is based on the experts assessment of the whole thing.
The next thing is compound probabilities. 
In the last lecture we tried to explain the idea of independent events. 
Suppose there are two sets here the possible outcomes are 1 2 3. 	

(Refer Slide Time: 35:25)


 


That is my space X and there is another space Y, when I press the bell 1
as this is a sort of a button or I can press any one of them, and here there are some other events say a b c that will appear as I press a button.
Now if there be a direct association that, if 1 is pressed then a will be displayed, when 2 is pressed b will be displayed then there is a direct causal relationship between these two and they are not independent.
On the other hand if it be the case that both the appearance of 1 2 or 3 or a b c are absolutely random based on their own probabilities. 
So there are some probabilities of P(A) may be .6, most of the time A occurs P(B ) is .3 and P(C) is .1 so all these are adding up to 1 for this space.
Most of the times probably I will get A irrespective of whether I press 1, 2 or 3.
So in that case the appearance of the pressing of the button 1, 2 or 3 and the appearance of the symbols a b c are absolutely independent, there is no causality among them. 
In such cases the probabilities do not affect each other in any way.
The joint probability of two independent events A and B is given by P
probability of A and B that is the number of intersections A and B divided by the total number of elements in the set and that can be computed as the probability of A times probability of B.
For example, again thinking of this same example 1 2 3 and a b c.
This is the display that will be displayed and this is the key that will be pressed.
Now I can compute the number of times that 1 as been pressed and a has appeared
Now if this be random it has got its own probability. Here we said that P(a) was .6, P(b) was .3 and P(c) was .1 and similarly here we have said P(1) is .6,
P(2) is .2 and P(3) where 3 is being pressed is also .2 because that should also add up to 1.			
Then the joint probability of that 1 will be pressed and a will appear will be probability of 1 multiplied by probability of a occurring that will be equal to .6 times .6 = .36.

(Refer Slide Time: 39:44)


 




(Refer Slide Time: 41:21)


 

Now this will hold only if this and this are independent.
They are absolutely independent, this is coming from one set and this is coming from an independent set.
That is known as the joint probability or compound probability.
Similarly A or B, that is the union probability of two independent events will be P(A) + P(B) the probability of A + probability of B – the joint probability when they are occurring together. 

In the earlier example they have got their probability 1, 2 and 3.
What is the probability that 1 will be pressed or a will appear?
Obviously 1 will be pressed has got the probability to be .6 as we have done earlier. 
The fact that a will appear has also got a probability .6.
But there is a joint probability in that I am already considering the cases where 1 and a have jointly occurred so that was .36 so – .36. 
Therefore it will come to 1.2 – .36.and that will be the union of this.
Basic axioms of probability: 
Probability is a value between 0 and 1. P(E) denotes the probability of the event E.
If the event is always true then P true is 1 that it is a certain event.

(Refer Slide Time: 41:48)

 



(Refer Slide Time: 43:29)

 

There is no uncertainty involved in it.
Similarly P(false) is 0 that is it is an impossible event.
P(A) there is a probability of A occurring is equal to 1 – probability of 0A occurring. 
Since the total space is 1 then obviously the probability of A occurring must be 1 – this.
Similarly again this example will tell you a b c and P(a) is .6, P(b) is .2, P(c) is .2 so what is the probability of 0a?
So probability of 0a is either probability of b or probability of c so that will be the union of this so it is 1 – the probability of a.
That is, basically if you look at the probability of b in this case the probability of c taken together so it will be 1 –.6 = .4.
The basics are P, A or B. These are the basic axioms of probability which will lead to use.

(Refer Slide Time: 43:48)

 

Till now we were discussing about independent events.
Now let us look into dependent events.
Dependent events mean they affect in some way.
One of the ways they can affect is the causality.
If the voltage across a circuit is increased the current flow will increase.
There are other ways also in which our reasoning can affect.
For example, if there is a murder then there will be some blood stains. 
So, if we see blood stains from there we can also reason back towards murder.
So evidences also sometimes help in reasoning and that is very useful
in the case of diagnosis. When we look at some fault we try to reason out the cause of the fault.
Now, if two events are dependent then that is the fact that one event has occurred will certainly influence the occurrence of another event.
This is known as conditional probability.
That is, conditional probability of an event A given that event B has already occurred that is given by P(A/B). 
Now here you will again find some similarity with our certainty factor case
where we are time and again writing MB(h/e).
Similarly here we are saying that the probability of an event A occurring if B has occurred.
What is the probability of this?
Now B has occurred or B has got a particular probability of occurrence.
In that case what is the probability of A’s occurring? 
That is known as conditional probability. And that is given by P(A) AND B. 
That is A has occurred and B has occurred divided by probability of B. 
It is P(A/B) = P(A AND B)/P(B).
Now there are two types of probability. Here you can see that the probability of A and B occurring is being divided by the probability of B.
Now the probability of B is known A-Priori. 
So here we have to talk about two types of probabilities. 
One is A-Priori probability.
That is the probability of A-Priori and we also write as A-Priori that is known before hand. 
P(B) we have computed irrespective of A.
Irrespective of A I have computed the P(B) through experiment.
And suppose I find that in general B occurs with a frequency of .6.
You go to a village and carry on some experiments just on the occurrence of malaria in that village.
So P(malaria) you find that out of every ten persons six persons are inflicted with malaria.
So the probability of malaria is .6. 
Now we would like to see probability of fever P(fever/malaria).
This may not be a very good example when they had malaria.
When they had given they have malaria what is the probability of fever?
How many of these people had fever?
Assume that all malaria cases are not giving rise to fever.
So in that case what we are trying to say it is the probability that somebody will have fever, he has fever and malaria and I find that the probability is .9 in those cases where he had fever and malaria. 
Now out of this malaria has got an independent probability A-Priori probability that every six people will have malaria.
So they got fever because of malaria not be right.
People can have fever for some other reason also.	
So we compute this by dividing with the A-Priori probability of malaria.
That is because they are not independent I divide it with .6 and whatever value comes up that is my conditional probability of fever given malaria. 

(Refer Slide Time: 49:40)

 



(Refer Slide Time: 49:55)

 

This is known as conditional probability.
A-Priori probability:
The prior probabilities of events A and B suppose we know P(A) and P(B)
we know the prior probabilities. 
We know the prior probabilities of P(A) and P(B). 
But if we know A for sure then probability of B might change.
If we know A for sure then this is represented as the conditional probability of B/A.
Now let us go back to the doorbell problem we had seen earlier.
Doorbell rings at midnight so it is D.
W is Mohan wakes up at midnight.
And suppose the A-priori probability of the doorbell ringing at midnight is 
.001 as the probability is really less and Mohan is waking up at midnight for different reasons such as for the doorbell or for some other reason maybe feeling very hot or uncomfortable etc is .01.
And we say that the probability of Mohan’s waking up with doorbell when the doorbell rings is .8.
Now this is a conditional probability. 
These are A-Priori probabilities that doorbell or no doorbell the probability of Mohan’s waking up is .01.
For any reason the A Priori probability of the doorbell ringing is .001.
And this is a conditional probability that if the doorbell rings then Mohan will wake up that is .8.
So there is a difference between the types of probabilities.
Now, in order to handle this sort of cases there is a very interesting and famous rule that is known as Bayes’ rule that is used for computing the conditional probabilities.
Probability of an event A/B that is if B is known then the probability of A can be computed by this formula as, here we are trying to find out the conditional probability.


(Refer Slide Time: 51:48)

 

Conditional of probability of A occurring when B is known.
Now if we know the A-Priori probability the reverse relationship 
I know the A-Priori P(A) and I know the A-Priori P(B) and I also know the A-Priori P(B) occurring when A occurs.
What we are trying to do is a cause effect relationship. 
Then this is the formula that gives me P(A/B) that can be derived with P(B/A) multiplied by the A Priori P(A)/A Priori P(B).
Now this can be easily derived like this:
P(A AND B) is, what is this? It is P(A/B) times P(B) and again P(A AND B) is P(B A) (P A) if I just change it. 
Now from this I can say this one is P(A AND B)/P(B) and 
P(A B) is P(B A)/times P(A).
So what is the implication of this?
Before going to the implication of this here is an example.
A Priori probability of doorbell ringing is .001, 
A Priori probability of Mohan’s waking is .01, 
A Priori probability we know that if the doorbell rings Mohan will wake up is .8 
then what is the probability that when Mohan woke up?
He woke up because the doorbell rang. 
I want to find the reverse thing.
Then by Bayes’ rule we will simply use this P(W D) multiplied by the A Priori probability of this denominator.
This is the event and this is the hypothesis. The event is 001 and that is divided by the A Priori probability of waking up because I have to divide by this because anyway that would have happened so I get a normalized probability of P(D W) which is 0.08
This is a very important technique and it has got far reaching implication.
We will see what it really implied.
We are given some event and from there we try to come to a hypothesis.
And I really want to find out the probability that this hypothesis is true given this event.			

(Refer Slide Time: 55:33)

 


For example, if this event is fever and if he has fever he had malaria.
What is the probability that if he has fever he will have malaria? 
I want to find that out that fever is implying malaria.
I want to find out the probability of this.
Now I know P(e/h). What does it mean?
This means that somebody who had malaria had fever also,
How many times it happened? 
I find that this probability is something like .4 so I know this thing A Priori and I also know the probability of fever in a particular locality, I also know the probability of malaria in a particular locality and these are the A Priori probabilities.
So using this one, this one and this one I am trying to find out this. 
So I know these A Priori probabilities and I also know that if somebody has malaria then he has fever, so this part is known. 
So, from the knowledge of this I am trying to derive this and that is being done using exactly what is being done in the case of Bayes’ theorem.
And this conditional probability is very important for all purpose for the simple reason as we will realize that it is a way of dealing with these rules that e implies h. 
This is nothing but a rule and I will have to reason through these implications and if I know the probability of this then it will be a much stronger measure 
than what we had in the case of certain defectors which was rather a subjective measure.
So, Bayes’ theorem forms a basis of probabilistic reasoning.
In the next lecture we will start with the Bayesian reasoning techniques using this Bayes’ theorem.
And one very popular powerful tool that has been designed for reasoning
in the probabilistic domain is the belief network.
But this forms the basis of the basic probability theory requirements. The
main thing is the idea about the conditional probability and the Bayes’ theory.


Indian Institute of Technology
Kharagpur

Artificial Intelligence
Prof. Anupam Basu
Department of Computer Science and Engineering, 

Lecture #29
Reasoning with Uncertainty IV
In the last lecture we introduced the concept of conditional probability and the Bayes’ rule.
 In this lecture we will first start with Bayes’ rule and then we will continue to see how the probabilistic reasoning can actually be implemented using the notion of conditional probability.
So, to start with we have a revisit of the Bayes’ rule. 

(Refer Slide Time: 2:12)

 

I want to know the probability of A given the probability of B’s occurrence.
When I know the probability of occurrence of B how would I compute the probability of A occurring? 
 That is expressed in terms of A Priori probabilities of B and A and also the probability of B/A.
So here we are trying to compute A/B in terms of the known probability of B/A
and here is an example of how we compute that.
The A Priori probability of the doorbell ringing is .001 and the A Priori probability of Mohan’s waking up is .01 and A Priori probability of waking up given the doorbell ringing is .8 and from there we compute the probability of doorbell ringing given Mohan’s waking up using Bayes’ rule as .8 times P(D)/P(W) so we get .08.
This is the basic notion of Bayes’ rule.
Such conditional probabilities are applied when the2 events are not independent. 

(Refer Slide Time: 3:49)

 

If 2 events are independent A and B 2 events are independent when do I call them independent?
In terms of conditional probability if A and B are independent then probability of A given B will be the same as the probability of A because A and B are actually independent.
And probability of B/A will be the probability of B itself because they are independent.
Therefore in case of independent events the joint probability P(A AND B) is P(A) times P(B).
Now here we can see a simple scenario that the P(A/B) is .2 
P(A/NOTB) is .3, P(NOTA/B) is point 1 and P(NOTA/NOTB) is .4.
It is interesting to note that since they are joint probability the sum of all these are coming to 1 
Therefore here there are some nice formulations about computing the probabilities of events. 
We can compute the P(A) if we know the P(B) in terms of Bayes’ rule and that is very much applicable in the case of rules where we deal with event e leading to hypothesis h so p(h/e) can be computed similarly.
But there are of course some advantages and also there are some problems.

(Refer Slide Time: 5:49)


 

For example, the advantages are; since it is based on the probability theory which has got years of formal mathematical foundation it is very rigorous and it is the reflection of a reality because we are computing a-posteriori probability.	
The word a-posteriori is just the opposite of A Priori. A Priori means something that is known beforehand and a-posteriori is something that we are trying to know and that is very much evident in the case of Bayes’ rule as we had done that we want to the find the probability of this. 
That is, we want to the find the P(h/e) we want to know this.
We know the A Priori probability of e, we know the A Priori probability of h
and we also know the A Priori P(e/h) that is same as h leads to e.
These are known A Priori, these are known beforehand A Priori and using Bayes’ rule using these three we are computing this one which is a-posteriori. 
That is a very nice thing to do.
However there are some inherent problems in the case of probabilistic approach.
The problems are, it may be inappropriate because the real world scenario is changing. 
Therefore the future may not always be similar to the past.
So we are taking A Priori probabilities which are based on the past information and we are going to predict the probability a-posteriori that is the futuristic probability. But that is valid as long as the world does not change.
But in reality the world is changing. Therefore this may not be always appropriate. 
The second thing is that it may be inexact or incorrect especially for subjective probabilities.
If we compute the probabilities based on a large number of experiments that we do then there is no problem with that.
But in many cases we cannot carry out so many experiments for all the rules.
Therefore we often have to go by some subjective measures and subjective inputs and that is where the inaccuracies may creep in.
 So in order to apply the probabilistic reasoning rigorously we need to carry out a huge set of experiments to find out all the possible of conditional and joint probabilities and that is really an overhead because if there be n events then for all possible combinations we need to find 2n number of probabilities.
The other thing is that knowledge may be represented implicitly.
In that case we really cannot find out the probability corresponding to this.


Now just to avoid this overhead of finding out objective probabilities always and the huge amount of computation required we sometimes feel like bypassing this computational overhead.
So we take recourse to some subjective measurements and have a quick but good enough solution using certainty factor approach. 
We have seen this is in MYCIN where it tries to avoid this overhead of the probabilistic approach or the Bayesian approach and take recourse to some subjective measurements or certainty factor algebra which assumed that all the probabilities which are dependent causally should be clubbed together in a rule. Therefore we will be dealing with only independent events then the computation becomes much simpler.
However, Bayesian approach demands discussion in its own right.
 In Bayesian approach the basic tenets or the steps of this we first derive the probability of a cause given a symptom. 
We get a symptom and we want to find out the probability of the cause.
Although it required a lot of computational power now it is gaining more and more importance because we are having more and more powerful machines.
So more computational power being available and better algorithmic methods being available 

(Refer Slide Time: 9:20)

 

Bayesian approach is becoming more popular very much in the case of machine learning, information retrieval and language processing etc.
The Bayesian approach is especially useful in diagnostic systems like medicine and computer diagnosis etc. Here the basic thing is, we are trying to find the inverse or a-posteriori probability.
That is, inverse of the conditional probability of an earlier event given that a later one occurred.
Now the Bayes’ theorem needs a little bit of extension when there is a large number of hypothesis as we had presented earlier.
So here let us see; let the evidence E be there with some probability.
We want to find the probability of a hypothesis of a particular disease given a symptom.
Given this evidence what is the probability of a particular hypothesis Hi?
So that is the probability that hypothesis Hi is true given the evidence E.
And here we are having the A Priori probability known beforehand that probability will always observe the evidence E given that the hypothesis Hi is true.
If the patient has malaria then always there will be fever.
What is the probability of that? 
P(Hi) is the A Priori probability that the hypothesis Hi is true in the absence of any specific evidence. 						

(Refer Slide Time: 12:05)

 


I go to a particular village and I straight away know that 60% of the people suffer from malnutrition.

So, for that I did not really do a lot of statistics. 
If I have got that data I can start with that, that P(Hi) is probability that somebody in that village will suffer from malnutrition is .6. 
Now we have got an evidence E which can support a number of hypothesis such as n number of or k number of hypothesis. 

Then this evidence E and its contribution to a particular hypothesis is distributed over this entire k hypothesis. 
So Bayes’ rule or Bayes’ theorem needs to be modified in this way
P(Hi/E) is P(E/Hi) that was there already the A Priori probability of the evidence given the hypothesis times P(Hi) divided by, and here n varies from 1 to k because I am talking of k hypothesis then P(E/Hn) E/Hn times P(Hn)
so earlier I was dividing at Bayes’ theorem.
I was just dividing by this evidence part or rather the B part. 
Now in this case this will be divided by the entire possible hypothesis that I know of so I am coming to this notion so this is the general structure of Bayes’ theorem. That is why we are calling it as extended Bayes’ theorem.
Next let us look at an interesting example which will reveal to you the interaction of the causes and effects.
You will see that a particular cause leads to an effect or a particular cause can lead to two possible effects and there is a very nice interaction among them.

(Refer Slide Time: 15:28)


 


(Refer Slide Time: 16:55)

 

S means the patient has spots or rashes, M means the patient has measles
and F is that the patient has high fever. 
Now look at this causal diagram.
If the cause is measles and measles leads to spots measles leads to fever. 
Both of these are effects.
Now suppose I observe the evidence spots, now these spots although it is an effect of measles obviously it will serve as an evidence for measles but it will also serve as an evidence for fever though there is no direct causality here
but the causality is coming in an indirect way because if somebody has spots  then he or she may have measles and if he or she has measles then they may have fever.
That is the way in which this is also serving as an evidence also for fever. 
In the earlier case we had assumed that my evidence was spots, spots were known. 
Next I see that measles is known the disease is known that somebody has Measles.
Now obviously if somebody has measles and if it is known then obviously we can infer this evidence that he will have spots. 
And once we infer spots then that is also supporting fever because then this is again serving as additional evidence and increases the belief in favor of fever.
If I know this then of course I know this.
So what happened is, if I start with spots I know I can infer measles as a causal or I also increase the belief in terms of fever.

(Refer Slide Time: 18:18)

 

If both spots and fever are present they could be present independently.
But if both of them spots and fever is present they provide evidence in favor of measles.
But unfortunately if they were independent I could have combined my belief as spot leading to measles and fever leading to measles independently as something like this.
if someone has measles he will have spots, if someone has measles he will have fever.
So using Bayesian I know spots independently and I know fever independently so I have got two evidences e1 and e2 and both of them are pointing to hypothesis h that would be a joint probability which we could have done but unfortunately if e1 and e2 were independent. 
But unfortunately as we have seen in this case spots and fevers are not independent because e1 is related to h and h is also related to e2.
So there is an unseen dependence here and these are not dependent.
Therefore I cannot straight away combine them which I could if they were independent.
So we cannot simply sum their effects but we have to take conditional probabilities of their conjunction into account. 

(Refer Slide Time: 20:29)

 

So in general given a prior body of evidence e and some new observation E we have to compute P(H). 
This is the new observation and the earlier observation was this. 
I knew that he had spots so I have inferred something about measles.
Now I also know about fever that is denoted by e here. 
That means now I have to compute this conditional probability as
P(H/E) times P, this is the dependence that if somebody has spots and measles then what is the probability of his fever? 
I have to multiply with this and then divide with the conditional probability of fever when somebody has spots.
I have to find out this conditional probability as well as this joint probability 
and in a real world the size of joint probabilities that we need to compute grows as 2n if there are n propositions to be considered.
And that is the real problem in applying the Bayesian approach. 	


(Refer Slide Time: 21:44)

 

However, in order to facilitate our computation some intelligent techniques have been devised.
Now we will talk of one such technique which is known as belief network.

What is a belief network? 
  A belief network is a representation of causal connections between different events with associated conditional probabilities.
Here is a picture of a belief network. 

(Refer Slide Time: 22:26)

 

All the nodes of a belief network are basically events and two nodes are connected by an edge if one node is the cause of the other.
This is a typical example of a burglar’s alarm case. 

(Refer Slide Time: 24:56)

 

The alarm this event has got a parent burglary that means burglary will cause the alarm, earthquake is another parent so earthquake will cause alarm and each of these edges show that there is a causal relationship.
Therefore nodes are representing events.
And that network is the representation of causal connections between different events.
Directed edge from one node to another node A to node B represents the fact that event B may be an effect of the event A. The usage of the word maybe has to be noted here. And we are storing conditional probabilities for a node and conditional probability of a node is provided for each possible value of its parents. 
That means, the conditional probability of A, think of this node John calls, so 
what is the probability of John calling? What is the parent of John calls? It is the alarm so corresponding to this node John calls in the belief network I am giving a least of probability table with a list of probabilities of John calling for every possible value of the alarm.
The alarm may ring that is true in that case John calls is .9 and if there is false it is .05.
Here for this conditional probability it may not add to 1. 
 With this definition now let us come to this example.
 There is a burglar’s alarm fixed at home.
Now this burglar’s alarm is fairly reliable. That means whenever there is a burglary the alarm will ring.
If a burglar breaks open your house it will start ringing.
However, it is not 100% accurate.
It also responds at times to minor earthquakes.
Whenever there is a minor earthquake and the door is shaking then also it starts ringing but that is a minor scenario.
The other thing is that there are two neighbors John and Mary who on hearing alarm calls the police. Now John always calls when the alarm rings. 
John is very particular about it. 
if the alarm rings John will call the police. 
But sometimes he confuses the telephone ring with the alarm and even if there is a telephone ring he thinks that it is an alarm and calls the police, that is his problem.
And Mary has got the habit of listening to loud music and sometimes misses the alarm altogether, just misses the alarm. 
But if she gets the alarm then certainly she will call the police.
That is the scenario. 
So here is the corresponding belief network burglary, earthquake, alarm, John calling, Mary calling which are the possible events and for each event we have made a node and for each of these nodes burglary we have put an A Priori probability of burglary as .001 and A Priori probability of an earthquake occurring is .002.
Now, alarm may be caused because of the burglary or because of the earthquake.
So, if both of these things happen then the probability of alarm ringing is .95.
If burglary occurs and there is no earthquake then also in 95% cases it will ring.
If there is an earthquake then also sometimes it rings .29 and if none of them is there then also sometimes for spooky reasons it rings.
So this is the probability of alarm ringing with respect to these conditions burglary and earthquake. But here you see these two which have got no parents are A Priori probabilities.
Now what is the probability of John calling? 
If there is an alarm he calls but sometimes he also calls when there is a telephone ring so no alarm but he has called. 	



And for Mary if there is an alarm and she really gets it she will call.
But sometimes even if there is no alarm she calls.
Therefore this is associated with this node and this is associated with this node and like that I have formed the belief network. 
Properties of the belief network:

(Refer Slide Time: 28:33)

 

As we have seen a belief network is a directed acyclic graph or in short DAG there is no cycle in the graph as you have seen here because in that case there would be a problem. 
If John calling again starts burglary or starts alarm then there would be a problem and it will be much more difficult to handle.
 Nodes which have no parents have prior probabilities as we have just now shown.
The nodes which have got no parents are given A Priori probabilities.
All other nodes have been given conditional probabilities.
In general there can be multiple directed paths between a given pair of nodes.
It is a possibility that there will be a multiple path from one node to another but that case will not consider.
We will consider the case when there is only one directed path or only one path between any pair of nodes and that scenario is known as a poly-tree. 

(Refer Slide Time: 29:44)


 

What is a poly-tree?
A poly-tree is a graph where there is at most one undirected path between any pair of nodes. 
Between these two nodes there is only one path either here or here and why is it undirected? 
From this node to this node also there is only one path whereas this is not a poly-tree because from this node to this node I have got two paths. 
Here of course there is only one path but from this node to this node
I have got two paths like this.
So this is a poly-tree and this is not a poly-tree.
We will restrict our cases only where it is a poly-tree. 
Given this scenario you can see that it is a poly-tree. What is the probability that the alarm has sounded?
But neither a burglary nor an earthquake has occurred and both Mary and John has called.
In this case I want to find the probability that John has called, Mary has called 
but burglary and earthquake none of these things occurred.
Now obviously John has called because is it just when there was no alarm
or John really heard that there was an alarm, what is it?
Was there an alarm or it was a false thing? 
None of these things occurred so how would I reason out this?

(Refer Slide Time: 32:44)

 

So I am trying to model this scenario.
Probability that there was no earthquake and there was no burglary and there was alarm, the alarm sounded, John called and Mary called.
This is the probability that I need to compute and that is equal to probability of John calling on alarm multiplied by probability of Mary calling on alarm. Probability of alarm ringing was when there was no earthquake and there is no burglary and A Priori probability that there is no earthquake and the A Priori probability that there is no burglary.
Now in this graph poly-tree we have got all these values so we will compute this from the same formula. 
We can see that probability of John calling on alarm was .9, probability of Mary calling on alarm was .7, and probability that the alarm has occurred when there was no burglary and there is no earthquake is this .001. 		

This figure I have to take and times this, when I take this joint I have to take 1 – .002 
There is a probability of NOTE because the probability of E was .002 
and probability of burglary was .001 so the probability of NOT burglary is 1 – .001 that is the basic of probability. So we find the value as.00062
That is how we compute in the case of using the belief network.

(Refer Slide Time: 34:11)
 
 

Now if the information about the parent node is not known then a node may not be conditionally independent from its ancestors. 
For example in general in a belief network I will consider the conditional probability for the nodes which has got some causal relationships and those which are not related I will not consider them.
For example, if I am in a scenario where I know that burglary has got some relationship with alarm and earthquake has got some relationship with siren or some other thing which is not related then in this graph I will just restrict myself only to the nodes where there is a conditional dependency. For example, John calls etc and I will not consider these nodes here.
But if I do not know anything that burglary occurred or John called if for this event I do not have any information about its parent then any of these can be participating in the decision making. 
Now our problem was that in the case of Bayesian approach we have to compute a huge number of probabilities. 
So, in a belief network there are so many nodes and edges.
Which of the parts we have to really look into and take into our computation?
In order to reduce the size of computation we can very intelligently restrict ourselves to a subset of the belief network.
In order to appreciate that, first let us look very carefully into these definitions.
A set of nodes E, some set of nodes E d-separates two set of nodes X and Y 
if every undirected path from a node on X to a node in Y is blocked given E. 

(Refer Slide Time: 36:24)


 


(Refer Slide Time: 37:44)

 


That means here is a belief network, malnutrition leads to underweight, 
malnutrition leads to lack of iodine, lack of iodine leads to goiter and hormonal imbalance also leads to goiter.
Now, this node is very critical because this node d-separates this node and this node.
Suppose if in this node I know that there is lack of iodine then in order to reason whether he has got goiter or not it is sufficient for me to look into whether hormonal imbalance is there and I need not look into malnutrition.
But unless I know this I could have tried with goiter whether he has got malnutrition and from there I would have started to reason. 
So I would have needed more conditional probabilities to compute.
But in this case I can simply separate out between two sets of nodes X and Y 
which is basically d-separated by this node lack of iodine. 

(Refer Slide Time: 38:44)


 

Another example is:
If I know that somebody has malnutrition then for reasoning about underweight I need not look into the probabilities of lack of iodine, hormonal imbalance or goiter.
Similarly for finding goiter I need not take underweight into consideration.
So this node d-separating is making two clusters this one and this one.  
Therefore lack of iodine is again clustering these.
Definition:
A set nodes d-separates, in this case lack of iodine was the case or malnutrition was the case which d-separates two sets of nodes X and Y if every undirected path from X to Y is blocked or goes through E as was the case here.
Every node from this set was going through this.
There was no node which was by-passing this.
Now when do we say it is blocked? 

(Refer Slide Time: 40:03)

 


A path is blocked given a set of nodes E if there exists a node Z in the path for which one of the following holds:
Z is in E and Z has one arrow on the path leading in and one leading out. 

This is node which is in E that I know and that is a blocking node.
So there is one edge coming in and one edge going out which is the first condition.
Another case is Z so what is that?
Z is a node that is given a set of nodes E Z is in E and Z has both path arrows leading out, this is the example.
Z is in E, in my evidence set Z is there, a particular node is there and all of them are going out from there. So obviously the descendants of this are separate. 
The third case is neither Z nor its descendants in E and both path arrows lead into Z.
This is the evidence.
Although this is the evidence but the blocking node is this Z which is separating out Y both the path arrows from these clusters it is coming into the goiter. 

So goiter is blocking out these two so there is no dependence between these.
This is a nice idea of d-separation and therefore I can restrict myself to the computation of the probability with respect to the d-separated clusters only. 

(Refer Slide Time: 42:24)

 

Now it is very important to learn how we build in the belief network?
When we build the belief network we have got a number of variables.
We have to select the variables in such a way that we will get a node or 
it will be possible to create d-separated clusters.
For our reasoning the computation of conditional probability becomes less
so we choose the set of variables that describe the domain like all those
basic nodes such as malnutrition, goiter, lack of iodine or for burglars alarm
you can think of burglary or earthquake, alarm, John calls, Mary calls are the variables I have to define.
Another very important step is to choose an ordering for the variables because I will first pick up a variable X and a node for it and set the parent of X to some minimal set of existing nodes such that conditional independence is satisfied. And then I will go on doing this and in this way I will define the probability table for X.
So, given this I have to select from the possibilities. 
 I will first select one node namely lack of iodine then I have the select parent in such a way so that I can create a conditional independence set and then we define the probability of this.
Therefore ordering is much important and this approach is based on the minimalist principle.

(Refer Slide Time: 44:18)


 
		

There is a smaller explanation.
This is a very general philosophical term and is also applicable to AI.
A smaller explanation should be preferred rather than increasing the number of premises.
For example, X has happened because A has happened, B has happened, C has happened, D has happened etc.
Instead if we could say K and P happened and therefore A happened. 
So, that is the minimalist approach and that is exactly what we are trying to do 
that smaller the network the better it is both from explanatory and as well as from computational aspects.
So, in order to really find a good belief network we have to choose several orderings of the variables and construct the belief network incrementally and ultimately keep the one with smallest number of edges. 
This is a very important step.

(Refer Slide Time: 45:33)

 

 Now how do we infer in belief network? 
The inferences in belief network can be divided into four categories.
One is diagnostic inferences.
That means from effects we want to go to the causes. 
Suppose given that John calls what is the probability of burglary?
So the evidence was John has called and what is the probability of burglary,
that is the diagnostics so I have seen the effect.
My car is not starting. 
What is the probability? There is some problem in the spark plug.
This is what we are trying to do so it is diagnostic.
What is the probability of burglary given John Calling?
The other thing is causal inferences from causes to effects.
Given burglary what is the probability that John will call or Mary will call, there is a probability of John calling given B. 
For example, if the spark plug of my car gets dirty then what is the probability that it will not start? That is causal. 
The third type is intercausal inference.
One common event can be given by multiple causes.

(Refer Slide Time: 46:55)

 


	
For example, alarm could be caused by burglary as well as earthquake.
Given alarm what is the probability of burglary?
But alarm can be caused by both earthquake and burglary from there if one is known then what is the probability of the other one.
Given alarm what is the probability of burglary?
It is that I know the probability of burglary given alarm.
Now given earthquake what is the probability of burglary? 
Alarm has been there so probability of A and E, what is the probability of B?
The other thing is mixed inference. 
Some things are causes and some of the effects are known.
For example, I know John has called and I know that is an effect and I also know that no earthquake has occurred that is the status of some cause I know so what is the probability of alarm?
There is a probability of alarm when John has not called and there is no earthquake.
So, figuratively we can see that there are four patterns.

(Refer Slide Time: 48:10)


 

This is the diagnostic and this evidence is known.
What is the probability of a particular cause to have occurred?
Causal is, I know that the voltage has increased, what is the probability that the current flow will increase?
That is probability one but I know the cause that with some probabilities burglary has occurred and what is the probability that John will call?
The third is intercausal. 
Something has happened and I know there are multiple causes
I know some probability of earthquake occurring and I also know the probability of alarm being sounded on earthquake.
What is the probability that burglary has also occurred?
So there are multiple causes and I want to find this one.
The other one is mixed inference. 
That is, I know some of the causes and some of the effects.
I know that John has called, I know that there has been no earthquake,
what is the probability of burglary?
This is mixed inference.
So there are four patterns. 
Now poly-tree we have already explained.	

		




(Refer Slide Time: 49:32)


 

Let us consider X to be a query variable, we want to know the probability of X given an evidence E.
So I know the probability of E and I want to know the probability of X/E.
And X is my query so I want to know the probability of X/E and suppose there are a set of nodes which are parents of X and a set of nodes which are children of X so I want to know the probability of this.
I can separate out, there can be some parts which are neither parent nor children of X, X is the query variable whose conditional probability I want to know so we separate out these two so I call one to be EX + and another to be EX –  these two sets. 
These are the effect set and you can say this is the cause set. 


(Refer Slide Time: 49:47)
 

(Refer Slide Time: 50:50)


 

So EX + is the set of causal support for X that the variables which are connected through its parents which are known whose status and probabilities are known and EX – is the set of evidential support for X that is variables below X are connected through its children. 

(Refer Slide Time: 51:18)

 

Therefore now I can compute P(X/E), P(X) I want to compute given EX + and EX – because other parts of the poly-tree are independent.
How you could I find out which are the independent parts because I have found out some nodes by which I could d-separate the entire belief network.
Now I have restricted myself only to a cluster after d-separation.
The other part is not important.
So I have reduced the number of variables. Therefore I have also reduced the number of parents and number of children corresponding to a particular node X.
So I have got a smaller set of EX + and a smaller set of EX –.
Now we can simply apply Bayes’ theorem that P(X/E) can be computed as P(EX –/X and EX + times P(X/EX +/P(EX –/EX +) that is standard Bayes’ theorem.

Since X is d-separating of EX – from EX + here you see X is also d-separating these two clusters because all of them are coming through X. Therefore I can further simplify it from this Bayes’ theorem part.
We can say P(X, E) will be, we just take the numerator P(EX –/X and P(X/EX +) and we multiply it with alpha where 1 – alpha is a constant representing the denominator because they are d-separated. 
EX + and EX – are d-separated so I can find out the independent A Priori probabilities useful for this. 
So basically if they are independent then probability of EX –/EX + is basically the P(EX –) if they are independent and then only this part is there and this independence is coming from this d-separation. 

(Refer Slide Time: 53:55)

 

Now both the terms P(X/EX –) and P(EX +/X) can be computed recursively 
using the conditional independence relations.
Now that is a simply way of dealing with the thing.
We have seen four types of inferencing. 
Now we will see an example of casual inferencing. 
What does it mean? 
I know that there has been a burglary, I know the cause.
I want to find the probability of the effect. So that is my query.
So this green part, burglary I know has occurred.
So that is my evidence and I want to know the probability, with which John called,
how would I compute this?
Obviously I will have a d-separated graph and I will compute it here out of this so what is EX – and what is EX +?


(Refer Slide Time: 55:18)

 

EX – is, for this query node there is no children.
This is my query node so this is my node X so there are no children. So EX – is null.
And what is EX +? J is in EX +. 
John called is in EX +. 
So we need to compute probability of John calling given burglary.
Now this alarm is d-separating B and J. 			

(Refer Slide Time: 55:55)

 

So, using the Bayes’ theorem we can write A can be true or false.
So probability of John calling given burglary is what I want to know.
Is probability of John calling NOTA given no alarm?
Probability of no alarm given B given there is a burglary plus the probability of John calling given the alarm and the probability of alarm given burglary.
So, I have restricted it to three nodes of the graph and I can compute it as
.858.
Even after this reduction there may be considerable amount of computation.
The fundamental thing is apply Bayes’ theorem, the Bayesian approach.
 Try to find out that d-separated part.
And only restrict yourself to this EX + and EX – sets and compute the 
Bayesian probabilistic value based on these nodes.
That is the basic approach of belief networks or Bayesian networks.
Here is an example of causal reasoning, causal inferencing. 
Here the scenario is John has called so it is diagnostic.
What is the probability of burglary? That is the thing we want to find out.
Here you get a confidence about applying Bayesian networks for managing uncertainty.
These are some of the apsproaches.
So we have seen certainty factors and the probabilistic approach and the Bayesian approach.


                                        Indian Institute of Technology 
                                                      Kharagpur

Artificial Intelligence
Prof. Anupam Basu
Department of Computer Science and Engineering, 
Lecture #30
Fuzzy Reasoning 1

In the last lecture we discussed about different kinds of uncertainty and we have seen how probabilistic reasoning helps in reasoning under uncertain circumstances.
Today we will discuss about a novel technique called fuzzy reasoning which is based on fuzzy set theory and consequently fuzzy logic.
This notion of fuzzy set theory was proposed by Lotfi A. Zadeh. 
We have already seen that in our day-to-day conversations and communications
we often use terms, words which are rather imprecise in nature.
It rained heavily, now what does the term heavily mean? 
The boy is very strong, how strong is heavy strong? 
The color of the curtain is more or less red or reddish. 
Now these terms do not find a direct mapping to any quantification like number.
When I say the boy is good, the boy is very good then immediately I obviously do not imply that the boy is 7 good or 8 good or something like that.
Now these sort of terms are very useful in our day-to-day communication 
but certainly poses a difficulty when you try to compute with these things.
 Fuzzy reasoning such as the fuzzy set theory and fuzzy logic deals exactly with this particular thing.
How to handle such imprecise scenario?
If you are shown this picture on the left side all of you will agree that it looks like Abraham Lincoln but it is more or less like Abraham Lincoln. 
Now how is it that we can certainly identify that this is the figure of Abraham Lincoln?
The complexity of the decisions from such subjective inputs the decision making that we do in our mind is really intriguing and often we do not really understand and often we cannot model them in quantified manner.
Now we have already seen what is meant by stochastic uncertainty.
Those are the cases which we handle with probability theory like the probability of hitting the target is .8.


(Refer Slide Time: 5:16)

 

That means we have carried out a number of experiments and based on those experiments we have seen that in 80% of the cases we could hit the target properly.
But there is another category of uncertainty which is called lexical uncertainty.
Now what is meant by lexical uncertainty? 
Lexicon means the dictionary or the set of words.
So the uncertainty that creeps in from our usage of day-to-day words those are subjective sometimes they are ambiguous and often they are pre- imprecise.
So, for that a number different types of uncertainty creep in.
For example we use tall men, hot days, and stable currencies.
Now you see everywhere tall, hot, stable etc so what do they really mean? 
We will probably have a successful business year.
Now here the term that we have used probably is again not very stochastic in nature.
It is just our guess or feeling that we will have a successful business year.
Now what do you mean by successful?
When would you say that your business has been successful in this year?
This word itself is very imprecise in nature.
The experience of expert A shows that B is likely to occur.
Now again B is likely to occur what degree of probability would you assign to the word likely? 
That is really imprecise. 
Expert C is convinced that this is not true.
Now here you see convinced and it is likely.
These are two different terms.
Convinced means you are sure that the certainty is one.
Now again in order to bring out the difference between probability and uncertainty in terms of the imprecise aspects such as the fuzzy weight aspects that we are talking about today.

(Refer Slide Time: 5:55)

 

We can look at this sentence: 
A person suffering from hepatitis shows in 60% of all cases a strong fever.
That means in 60% of all cases so this is rather probabilistic.				
But what is imprecise? Strong fever, when do you call a particular fever or temperature to be strong.
In 45% of all cases a yellowish colored skin. 
What inch of the skin would you say yellow?
Which tinge will you say to be surely yellow?
Which one is more or less yellow?
Which one is yellowish?
So this term yellowish is again imprecise and vague and in 30% of all cases they suffer from nausea.
Now 30% of all cases they suffer from nausea so the symptom nausea is present in 30% of all cases so this part is certainly probabilistic.
Whereas this first part, a person suffering from hepatitis shows in sixty percent of all cases.
Now this is probabilistic, strong fever is again imprecise so that is fuzzy.
In 45% of all cases it is yellowish colored skin.
This is also a mix of probabilistic and fuzzy.
So here we will show you a conventional Boolean set the conventional set. 
Now this slide has been taken from another source so these measures of temperatures are not like the ones that we do in India.
But in west they measure the body temperature in degree Celsius. So, 
if we have strong fever as a set then here you see 40.1° Celsius is certainly strong fever,  41.4° is strong fever, 42 is strong fever and 39.3 is strong fever, and 38.7 goes absolutely out of this set so this is not a strong fever.
But the question remains that when I say 39.3 to be strong fever
why should not I consider 39.2 to be strong fever as well?
And if I consider 39.2 to be strong fever why should not I consider 39.1 or 39 or 38.9 and in that way 38.7 also to be strong fever?
Can I make such a crisp boundary? 
In many cases where we use such terms it is very difficult to make such a crisp boundary.
On the other hand in this diagram you see that the boundary is not very crisp
it is gradually fading out so at the core it is certainly strong fever and as I go out it is becoming gradually faded so it is becoming less and less of an element of strong fever. 	
			
(Refer Slide Time: 07:57)

 

And this is a critical term that I have used.
It becomes less and less of an element.
Gradually its belonging to the set is becoming less.
Here either an element belongs to the set or it does not belong to the set.
It is a binary decision yes or no.
On the other hand here we can see that 40.1° is inside the set, 42 is inside the set, 41.4 is also inside the set, 39.3 is slightly in this shaded region and 38.7 is even farther.
So, in order to capture this scenario fuzzy set theory was proposed. 
 Always there is a very close correspondence between a set and a and logic because in logic we use set.
So when we say a particular set let us say we take any particular set
a set X in which there are elements a, b, c then obviously we can say that a belongs to X and suppose d is outside the set d does not belong to X.
So this can also be expressed as predicate that belongs to (a, X), does not belong (d, X). 
Might be d is belonging to another set Y so belongs (d, Y).
So this belongs is a predicate and therefore this belongingness of a set or definition of a set immediately maps to predicate.
So, from set we can always come to predicate logic.

(Refer Slide Time: 12:18)

 

Now we say that fuzzy logic deals with qualitative information, the information that is qualitative, we always do not have qualitative figures to deal with it.
This is more realistic than predicate calculus because in real life we need to deal with qualitative statements.
Here is an example from a chemical plant.
We often use rules like this:
If the temperature is moderately high and the pressure is medium then turn the knob slightly right.  
Now, if the temperature is moderately high that is this part, earlier we had dealt with predicates like; if age of Tom is 50 and weight of Tom is 65, here you see everything was precise.		

(Refer Slide Time: 13:48)

 

But here we are using, if temperature is moderately high.
That means what if I write in this way?
If- is temperature and then write moderately high. 
Now this is no longer a precise term.
So here we are facing a difficulty but it is nevertheless more realistic and the pressure is medium. 
Now which pressure is medium? 
Turn the knob slightly. 
What angle would make it slightly right or slightly left? 
So we use this sort of terminologies.
So dealing with precise numerical information is often inconvenient because it is not very suitable for human beings.


(Refer Slide Time: 16:07)

 

If you ask the expert to say what exactly the pressure should be
he will not say that because he has got a feeling that if the pressure is medium turn the knob slightly right, exactly how much will not be told like 42.7° or 4.56° not to that precision that is more realistic and so here are some more examples which deal with such imprecise statements and these impreciseness are coming from the power of language, linguistic variables these are called as.
 Natural language allows us to express in a much more comfortable way.
And in order to model it in the quantitative world often we have to translate that into a quantitative statement and that translation is often artificial.
So we are trying to be as natural as possible in this case.
So, weather is sunny today, what do we mean by sunny? 
It is very cold in the Himalayas.
Rich people have a lot of worries.
Now, when would you call a person to be rich? 
When would you call temperature to be very cold?
All these things are imprecise.

(Refer Slide Time: 16:42)

 

A fuzzy set differs from crisp set in terms of membership.


(Refer Slide Time: 16:48)

 

In the case of a crisp set whenever we look at a crisp set any particular set X which consists of 1, 2, 3, 4, 7, 9 then any of these elements either belongs to this set or it does not belong to the set.
So if I take a super set a universal set that is a set of all integers now given any u belonging to this.
So if I take any integer either that will belong to this X or it will not belong to this X.
So the membership of this in the case of a crisp set, membership of u in X is either 1 or 0.
That means belongs to or does not belong to and there is nothing in-between. 
Whereas in the case of fuzzy set the thing is different.
We will use the same notion of membership but if I now change this set to a fuzzy set then let us see how can we talk about it. 
Suppose now I say that it is a set height which is 5 ft, 5 ft 2”, 5 ft 4”, 5 ft 5”, 5 ft 6”, 5 ft 8” etc and I define a set medium height. 
Now this is my universal set U and this set is a conventional crisp set.
For example, 6 ft is not in this set and in this discourse; 5 ft 5” is there.
Now, what would that set medium height be?
Will 5 ft 4” be in medium height? Suppose you say it is there then 5 ft 5” you think is also in medium height?
5 ft 6” will also be in medium height, 5 ft 8” will also be in medium height, and you say that these two will not be there. These are outside the set so I have taken these four. 
But the question is, if you consider 5 ft 4” is it belonging to medium height as strong as 5 ft 8” or 5 ft 6”? Probably it is not.
This 5 t 4” has got a weaker belonging and this 5 ft 5” has less weaker belonging. Whereas 5 ft 6 and 5 ft 8 certainly belong to this set.
So the degree of belonging of these elements requires to be specified.

(Refer Slide Time: 21:21)
 

But this was not the case when we are discussing about crisp set.
So we can define with the example of a set 2, 3, 5 a, b, c and suppose we have got a universe of discourse now 1 does not belong to S and
a belongs to S so that is a crisp set.

(Refer Slide Time: 21:55)


 

But if we try to generalize it in the case of fuzzy set, in the crisp set the membership is either 1 or 0. 
But in the case of fuzzy set the membership of an element X in a set S is no longer 0 or 1 rather muX is between 0 and 1 and both are included. 
Now this muX requires a little bit of explanation. 
I take the example of medium height here.
Now I have got an element x belonging to a universal set X.
For example, some element x belonging to the set height which was a crisp set. 
Now I am defining a fuzzy set medium height or medium which is a subset of this universal set height or this thing X, these two are the same thing.
Now, when I define medium I can say that 5 ft 4” has got a belonging of point three in this set.
And 5 ft 5” has a belonging of .5, 5 ft 6” has a belonging of .8.
Now these are rather subjective and are coming from the users’ notion.
It is your own notion of medium height.
And 5 ft 10” is certainly medium height.
So in this set what we have actually done is, along with every element of X. I have put in a membership function mu of this particular element x in the fuzzy set medium and that is followed by that particular value of x.


(Refer Slide Time: 24:28)

 


Just as in the case of crisp set this mu(x) in a fuzzy set medium would be either 0 or 1 in the case of crisp set but for fuzzy set it will be in the interval 0 and 1.
It can be 0, 1 or any number in-between.
So this is mu(x) in the set M where M is a fuzzy set which means membership of x in M.
This is also known as degree of membership. 
So this is a generalization of the conventional set theory.
Here you can see that mu in a crisp set S1 is a predicate which denotes x to be an even number. 

(Refer Slide Time: 25:55)

 

So here is the final definition that I was trying to explain.
Boolean or crisp set A is a mapping for the elements of S to the set 0, 1.
That is, A is mapping elements of S to 0 and 1.
So a characteristic function for a crisp set is 1 if x is an element of the set A, 
0 if x is not an element of the set A.
On the other hand fuzzy set F is a mapping for the elements of S to the interval 0, 1.
So, characteristic function S a membership function of x belonging to F will lay between 0 and 1 where 1 means full membership, 
0 means no membership and anything in-between for example .5 is a graded membership that is telling us the degree of its belonging.

So we can say that mu(xs). 
Again x is an element of the crisp set S and this is also a predicate, it is a membership predicate. 
Now let us give some examples.		

(Refer Slide Time: 27:12)

 

(Refer Slide Time: 27:28)


 

Fuzzy sets and concepts are commonly used like for example; John is tall, Ravi is smart, Sajni is happy, and the room is hot.
Now for example the crisp set tall can be defined as x where from x is coming? 
So, x is coming from the set height and we say that the entire x which is greater than 1.8 meters will belong to the set tall.
So tall is a set consisting of all x where x is greater than 1.8 meters.
Now what sort of set is this? a crisp set or a fuzzy set?
If I define in this way then it is certainly a crisp set because any number any height that is greater than 1.8 meters will certainly belong to the set tall. And if it is less than 1.8 then it will not belong to that set.
But such definitions of tall would put us in trouble because what about a person of height 1.79 meters?
What about 1.78 meters?
What about 1.52 meters?
Obviously, logically you will think that 1.8 is in the set tall.
We should not be so much cruel to 1.79.
We should consider that to be also tall but not certainly, 1.78 should also be in the list tall but then how would I distinguish between somebody who is 1.8 and somebody who is 1.78?
So the crisp set notion does not allow us to do that.
On the other hand in a fuzzy set a person of height of 1.8 meters would be considered to be tall to a high degree and a person with a height of 1.7 meters would be considered tall to a lesser degree.
Therefore I can define a function to describe the fuzzy set.
For example, here is a set which is a typical graph. 
What is the x axis of this graph?

(Refer Slide Time: 29:33)

 

So on this side of the graph we have got height and on this side we have got the membership.
We are having two axes.
On this axis I plot the height in meters and this height is coming from the crisp set that is the universe of discourse.
Now it may be 1.2 meter, 1.4 meter, 1.5 meter, 1.6 meter, 1.7 meter, and 1.8 meter in meters.
On this axis I am plotting the membership of a particular height h in the set tall because I am going to define that set tall. 
So, in tall the graph may be something like this. 
Over here up to this I am not considering them to be tall but then it is going up and here it is saturating to 1 so this membership is 1.0 and here it is .9, .8, .7 and so on.
Therefore here this graph is mentioning my subjective interpretation of the concept tall.
Now if I come across some person who has got a height 1.7 then simply I will project this over here and also give that a membership and place in tall and but with a membership of .8.
If somebody comes with 1.6 then I will also give him a place maybe with a degree .4.
If 1.5 comes in he is also not deprived and probably get .1 and below that everybody is getting 0 or something.
Now this is my one particular definition of tall.
But again now let us say I am trying to select a basketball team and I have been told to select only tall people.

(Refer Slide Time: 33:22)
 

In that case my notion of this fuzzy set I may like to change because I may like to have again the same axis and on this side I will have height again like 1.8, 1.7 where all the same figures are here and on this side- side I am putting in mu(tall) u or anything maybe h.
Now I can redefine my fuzzy set in a different way because I am rather strict about the height so maybe I will make my curve something like this. 
I will make it very slow and make it 1.
So somebody who is 1.8 will certainly get a membership 1.
Somebody is 1.7 will get a very low membership. 
The earlier curve was a little different it was a little flattened something like this. 
So, there 1.8 also got a membership, 1.7 got a quite some membership but when my purpose is to select a basketball team I will go for a steeper curve like this.
That is what my notion of tall is when I select the basketball team.
That is how the fuzzy sets are defined. 

(Refer Slide Time: 35:25)

 

Now here some numbers and I want to say give me a number which is above 600, what do you mean by that? 
How much money would you need? 
i would need about 600 rupees.
Now what do you mean by about 600 rupees?
Now if I want to conceptualize this notion about 600 then I have to define this about 600 to be a fuzzy set over a crisp set of these numbers 500, 600, 700 etc and the shape of the curve will be like this because for 600 certainly it is the membership of 1, 600 has the membership of 1.
In the fuzzy set about 600 whereas 599 will have a very high membership maybe .97 or something here,
550 is it about 600? 
Well by looking here we can say it is around .5, it is about 600, its membership in the fuzzy set 600 is about .5.
And for 520 its membership is around .2, what about 650? Its membership is again .5.
So here this would be a very natural curve of distribution of the membership to the fuzzy set about 600. 
Here is another example about conceptualization.
I am defining a set. 
Here is a rather philosophical question, how true is true? 
That means I say well what you said is true or I say 550 belongs to the fuzzy set about 600 this statement is true.
In predicate logic we were saying that a predicate evaluates to true or false.
There the concept of true was 1 and the concept of false was 0.
Now how true is true? 550 belongs to about 600 is true, I make this statement.
Now about this truth should I give 1 or what?
In the earlier diagram we have seen that 550 has got a membership 
mu of 550 in about 600 in that fuzzy set was .5.
Now membership of .5 means its belonging was to the degree .5 then it belongs to about 600 is it true?
Now I can define truth in fuzzy term where here I put in mu of about 600 and the different values of this thing.
And here is the membership in a fuzzy set I define true to be a fuzzy set
mu true of v. 
Now this point five was somewhere here.
If I define the truth fuzzy set to be something like this and this curve is such that it is something like .6.

But you may be even stricter than me and say that this is not my notion of truth, my notion of truth is something different.
My notion of truth is this.
My notion of truth is very steep and this is dilated so your notion of truth is this. 
In that case in your notion of truth the statement 550 belongs to about 600
how true it would be? 
It will map here and its membership will come to .4. 
Whereas another person may be a bit more lenient because he feels that any membership that is more than .2 then I can say it belongs to it a particular set.
So we will go for this curve and the same membership of 550 in about 600 is point five and that will be taken up here and it will evaluate to might be 1.
So in this way the notion of truth can also be modeled in fuzzy set.

(Refer Slide Time: 41:25)

 


(Refer Slide Time: 41:45)

 

 Here is another description of a fuzzy set.
On the x axis we have plotted height, on the y axis we have plotted the membership of the different heights in different fuzzy sets.
Here we have defined three fuzzy sets; One is tall, one is medium, one is short.
Now if you follow the curve for short it is coming like this that is, height of 30 centimeter is certainly short but height of around this .50 centimeter in short but is up with membership 1 that is somewhere here, height of 80 centimeter its membership in short is .7, height of 100 centimeter its membership in short is .2 etc whereas in medium height the distribution is something like this whereas for tall the distribution is something like this.
So we capture the different notions.
Now you can ask that where from am I getting these figures?
Though there are different ways of doing it these figures are representing pure conceptualization of a particular term.
You say that the room is hot.
You are saying it from a feeling and your feeling may differ from mine.
So up to what room temperature would you really tolerate and after that you would say it is becoming hot and the AC should be turned on?
That is your notion. So based on your notion of the term you have to find a particular distribution of the membership values and that will define the fuzzy set. 
Here again we are revisiting that strong fever case.
In conventional Boolean set theory it was like this and in fuzzy set it is represented in this way.
	


(Refer Slide Time: 44:40)

 

Now we can define discrete definitions we can say strong fever. 
If I say strong fever is 35° these are 0s and these are discrete definitions and I can have a continuous definition like this. 
Discrete definition is 43° centigrade’ membership in strong fever is 1, 42° centigrade’ membership is 1, 41° centigrade’ membership is .9, 40° centigrade is .65 and so on and so forth.
So I have discreetly taken up all the values but in this way if I want to tabulate it will be difficult because since it is a continuous domain I have to write for 35.5°, 35.4° and everything else.
If I select some discrete values I can do it in this form and I can represent it in the form of a table.
Whereas this is a continuous definition here, this is a curve going up so we can take any particular value and come to this point and find out the membership.
Therefore 41° membership is somewhere here which is .9, 39° membership is somewhere here and 38.9° membership is somewhere here.

(Refer Slide Time: 45:45)


 
		
Since we have to work with computers how can we describe a set? 
A set can be described in two ways. 
One is by extension or by intension. 
Even in the case of a conventional set we are familiar with these notions that I can either extensively enumerate that my set X consists of 1, 5, 3, 7 in that way I can go on and this is one sort of definition.
Another sort of definition is I can define a set Y to be all x such that x is greater than 5.
That means I am defining a set 6 onwards, x is greater than 5 if I say only integers 6, 7 etc.
This is a definition by extension and this is an implicit definition.
Now this is also the case in case of fuzzy sets.
So we can define S2 to be a set of tall people and we can put in a membership to this.

(Refer Slide Time: 47:47)


 

Now in the case of fuzzy set we can define it in this form; tall 
1.0 its membership in tall is 0, 1.2 its membership is 0, 1.4 its membership is 0 so in that way it goes on. 
Now in some books you will find that this comma is replaced by this plus. 
Now this plus is not addition this means union. 
So I am talking of a set where this element union this element union this element so all these things are together. 
Now how do we represent a fuzzy set in a computer?
I am defining a set, for example I have got a set age where age is 5, 10, 20, 30, 40, 60, 80. 
Now I can define different fuzzy sets and also represent them in the form of table.
For example, I define one fuzzy set young and here I put in 5, 10, 20, 30, 40, 60, 80.
Now I put in the memberships that 5 is certainly young, 10 is certainly young, 20 is certainly young, 30 is .6, 40 is .5, 60 is .1 or .01 and 80 is 0 so I can also draw the same thing. 

(Refer Slide Time: 50:50)


 

This is my definition of [….. ]young. 
So here it is age and my values are here 5, 10, 20, 30 like that.
What are these curves then? 
Here is its membership so for 5, 10, 20 the value is 1 and after that it has come to .6 and after that it has come to .5 and then there is a short fall.
This is the graph that is representing young and the same thing can be represented in the form of a table.
That is one way we can represent the fuzzy set in a computer.
Now similarly I can define another fuzzy set say old where obviously 5 is not old, 10 is not old, 20 is not old, 30 is not old here you say .2, 60 is .5 and 80 is .9, 90 might be 1.
So if I describe this fuzzy set then also I have actually mapped a particular curve. 

(Refer Slide Time: 52:18)


 

What is this curve? 
If this be the age here and the membership on this side these are all 0s and after 40 it is .5 and then .9 and something of this sort.
Now this curve can be mapped to this form of a table.
So the curves that we are showing you till now can be represented in the form of table. 
Therefore it is amenable to be represented in a computer.
Now here define a fuzzy set adult.
Just put in the membership values here and that will define your notion of adult.
	

(Refer Slide Time: 53:00)

 

This is one way in which fuzzy sets can be represented in a computer.
So we always describe these things in terms of some profiles and these profiles reflect our conceptualization of the fuzzy term like tall.
So here is a profile of different heights and this is my notion of tall and what are my axes here? 
This is the membership one, this is the membership of h where h is any variable on this, membership of h is in the fuzzy set tall this is the profile of tall 
you must have realized this by now.

(Refer Slide Time: 53:35)


 

Now there are different shapes.
How do we get the shapes of profiles?
Shapes of profiles are obtained from experiments and expert judgment.
We sometimes carry out some statistical experiments like I fill up a questionnaire and go to 1000 people and say would you consider 6 ft to be tall, would you consider 5 ft 8” to be tall and then we see how many people have really said that 5 ft 8” should belong to tall and based on that we try to put in some measure of membership.
But however this profiling all said and done is some somewhat vague and this has to be done by the expert or the user who wants his or her notion of fuzzy set to be captured.

(Refer Slide Time: 54:33)

 

So there are different types of functions.
Here is another type of function which is called the S function.
This function this curve can also be expressed as for any x less than a particular value it will be 0.
Between two points a and b it will be computed by some function. 
Between two other points it will be computed by some other function and 
after that it is 1.
This is one type of profile. 
So this profile can be represented as a table or can be represented as this function which can be computed using a procedure in the computer.

(Refer Slide Time: 55:18)

 

Similarly here is a very interesting thing.
I say that a number which is close to a, now conceptually it is something like this; a is certainly close to a membership 1, something which is in-between these two points if it is at b either plus on the positive side or negative side to .5 then for anything in-between we will have a membership and in that way we can have a bell type of curve. This is another way in which the membership functions can be represented.
I would encourage you to find out a mathematical function and expression
to represent this sort of a curve close to a so that we can also use it. 
We can of course tabulate it but we can also use it for computation as a function 
as a procedure.

Trancripted by p.sasikala
Lecture time (59:05)

ARTIFICIAL INTELLEGENCE
By
Prof. Anupam basuBasu
Department of Computer sScience and  Engineeringand Engineering
IIT Kharagpur

Lecture – 31
Fuzzy reasoning – II (TIME 00:53)

inIn this lecture we will continue with our discussion that we had last time about fuzzy sets and fuzzy reasoning. wIn the last lecture ewe concluded with the an example membership function like this which was withis a bell shipaped curve
refer slide time (1:15)
 

which isre presenting some number like close to a. nowNow, as i was mentione1d earlier this can be represented in the computer as a table. forFor each of these values, for twenty 20 we stored them membership function atvalue .1, point one for 30thirty we stored the membership value .5point five, 
if for 40forty we stored the membership value .9, point nine
for fifty 50 we stored the membership value one 1 etclike that. we can watch storing thereThere is also another way of dealing with such curves and storing them in the computer that is as a function. 
forFor example, if iI ask you to represent a fuzzy set to represent a fuzzy set close to a 
the notion will be something the curve will be something like this where right but  this number is a and any number which is close in this zone1 they will have high membership values. 
soSo if iI want to store a curve which is of this shape iI can either put it as table or iI can also find out some function. forFor example, if iI write that the mew of any number there is a x any number x in the set close to a to be represented as one by say1/1 one plus + ( one x –minus a)2 whole square  then also we will be able to represent thise sort of the a curve just to verify this. suppose Suppose this a is
 zero0, suppose this is zero in that case mew of c-2-if i am writing short close to a(1) the membership of one1 in that set will be one1 +will be a is zero right if i assume a to be zero one plus one12 square that is point five.5, the square part will take care of minus– one1 also because minus– one1 will also be point five.5.
 weWe can see that of curve is asymmetric nature.
 similarlySimilarly for c-2-lose to a( two2) will be= one1/ plus one1 plus+ (two2)2 whole square that is four4 that i’s one1/ by five5 that’ is point two.2. soSo that is shortly quite shortlyquite sharply it is falling down. 
soSo, instead of storing all the possible values and their membership function is as a table we can decide on the nature of the curve and find an equivalent function which can be computed in the computer.
 now this was given to you is as an assignment asked i asked you to complete it  i hopes many of you could do this  
refer slide time (4:53)
 
now Now if iI really want to have a much more stricter sharper curve like instead of the bell like curve i meanI like to have some curve like this alright or even sharper rather sharp you have this shorsortt of curve you can try in that case at mew c-2-lose to a( of x) =  to be something like one1/ by one1 plus+ mode of x minus– a. Iif you try this you will find that the nature of the curve is becoming quite short alrightsharp.
 sSo we can have either in an explicit presentation of a fuzzy set which is in the form for of a table, data structure or an implicit presentation which is in the form of a function. theseThese are the twotwo possibilities of representing a set fuzzy set in a computer. now 
refer slide time (6:25)

  

we We usually prefer to have excuse me pieceeace wise linear like triangular or this is another sigmaoid but it is instead of being a very continuous curve it is a pieceeace wise linear curve. thisThis is often preferpreferred because it is easier to represent and also calculation is easier it saves the computation.
refer slide time (6:59)

 

nextNext we come to the notion of linguistic variable. whatWhat are linguistic variables? forFor example, we use the sentences  rRavi is tall. 
we install ok now hereHere tall is an adjective and set of all in English is an adjective. 
setSet of all tall people is fuzzy and statements like Europeans are mostly rich, now see this tall is  ais a linguistic variable, rich is a linguistic variable. nowNow here in this sentence we are using adverb which is qualifying this word rich, 
 Europeans are mostly rich. nowNow how to deal with mostly? thatThat is, of course just as we have some x as and square of x similar to that mostly adverb can be assumed to be an operator overproverb this adjective. alright spSo we can define a linguistic variable as a predicate of a sentence and is typically is an adjective ok which is often qualified an adverb.
 thisThis is an observation;  
A linguistic variable is amenable in order to be amenable to fuzzy logic representations must be related to some crisp set or on which we can define thee fuzzy set. 
forFor example, when you we say rich then we may have  a crisp set of possible incomes and we can define the set rich on that crisp set. unlessUnless we have got some underline numerical they finish enoughdefinition corresponding to crisp set we will not be able to define visit a fuzzy set over that. 	
please note that A fuzzy set is always define over a crisp set and straight said to be the subset of that crisp set a fuzzy sub set as it is said.
 now linguistic variable variables can we be of different types usually they areir terms and associatedl c d with every term we have got a degree of membership which you have seen and that degree of membership can be computed through a membershipas my functions and some phase variables s. now some hereHere are some interesting curves.
refer slide time(9:25)
 

hereHere you see that this is the underlined numerical set crisp set  of the different temperatures. ok and And on this side is a the membership function of the fuzzy set that with we defined over this crisp set. so So we can define low temperature as in this way which we can again translate in the form of for a table or some function if possible. is theThis is piece peace wise linear so iI can always have up to thirty 3six6 or less than equal to thirty 36less than it should be one1 otherwise it will be a straight line equation with the a given radiant up to thirty 3seven7. [noise]

thisThis are is a normal temperature that means from thirty 3six6 to thirty 3seven7 some so somewhere in between thirty 3six6. point  five5 is a normal  temperature raised temperature we start from here and strong fever starts from here goes up like this. so you see ov Therefore over the same crisp set we can define different fuzzy sets is based on different linguistic variables. soSo these are the different terms. correspondingCorresponding to each term we will have a different fuzzy set and they will be different membership functions which define a the degree of membership of the different values of the crisp set.
 now Now if you have thirty 3eight8° degree centigrade  as a particular value then obviously we will get membership around point eight.8 which we can say pt moisturizeis pretty much raised and again the same value maps to in strong feveris intersecting in a slightly strong fever here just slightly strong is oris our corresponding comment on this membership. 
so you seeTherefore from  a crisp value we are coming to fuzzy value so iI a m getting a membership function and corresponding to the membership function again Ii can have a corresponding again linguistic state. 

refer slide time (11:40)

 	 

these There is another term which that needs to be reduced introduced that s calledcalled hedges. hedgesHedges are entities withichh deal with adverb. For example, J like john is tall so we can define a fuzzy set tall,  ok so say this is the heighteight and this is the fuzzy set tall so on this we can say jack Jack is very tall, very tall is further qualification over this and so that curve will take place will be something like this. and And somewhat tall is somehow short of relaxationa sort of relaxation and will look like this. alright nowNow at this a point iI would like to mention about twotwo operators.

refer slide time (12:32)
 

mention about two operators oneOne is the concentration operator 
and the other is the dilation operator. 
supposeSuppose i have got for some value of x iI have got a membership in tall as  point five.5 and the fuzzy set tall will be looking like as something like this. right nowNow for a particular heighteight maybe say this site which may be  five5 fteet six6 inches Ii have got a membership value something like point five.5. obviouslyObviously for this heighteight the membership in a set very tall should be less. soSo intuitively we can think of curve that the set very tall will be something like this, alright this will be very tall. Now, according to z a it is  veryis very convenient to have the membership of very tall obtain weighveryt of x obtain from this as mew of tall x this whole square.
 soSo this operation square is known as concentration operation and this that is reflected in this curve. on the other hand if we look at the same heighteight and mew we want to define a set more or less tall then obviously the membership of five5 point six.6 should increase right intuitively you can think of that  so if that curve will be some how like this. alright soSo this is a dilated curve and  thisand this is known as dilation operation. ok
 
aAnd this oneone we can obtain as square root of mew tall x so this oneis one way, so this square root this rraised to the power half is  a standard dilation operation and squaringe is is a concentration dilation operation.
so Therefore by this concentration operation this curve will automtomatically be a membership of this five5 point six.6 in very tall will be point two.2 five5 where as it in some of more or less tall will be quite high. right soSo these are again twotwo very important operations in fuzzy set or fuzzy logic and this is used to really compute hedges. theThe boy is running,
 the boy is tall, the boy is very tall, so 
how do iI handle very tall when iI know the definition of the fuzzy set tall?

refer slide time (16:09)
 

so thatThat is how the hedges are represented. These are also called linguistic hedges. nowNow there are twotwo more important concepts before we move to fuzzy operators. One one is set of support. The set of support for a fuzzy set is defined as the set it is a crisp set, a crisp set  of the crisp set please note that is a crisp set of consisting of elements whose membership values in the corresponding fuzzy set is greater than zero0. alright so So all the elements say heighteight in tall so all the heighteights in the set tall, all the hheightseight which have got all the height which have got a membership value greater than zero0 in the fuzzy set tall will be called the set of support for the fuzzy set tall. another Another thing is alpha cut, it is threshold. thatThat is, we put up some threshold alpha say .3 and point three all the elements whose membership are less than alpha will be taken out of the   set. So, so if you look over here we have put in some alpha here. 
allAll the elements is whose membership value is less than alpha will go out of this alpha cut of the fuzzy set. so Therefore which are the values which will have all these elements in this zoneone and those values havinghave got membership value greater than equal to alpha will so only these values will be in the alpha cut of the fuzzy set a. al right 
soSo please note that the set of support and alpha cut which are derived from the fuzzy set a will be twotwo different crisp sets. alright   [refer slide time (18:38)]
 
soSo this is again a the alpha cut. nowNow quick let us look at probability and fuzziness. set
 so So when we throw a die alright then the die can have six6 phases one1 two2 three3 four4 five5 six6 and the probability of each phase will be one1/ by six6 one1/ by six6 like thatetc. but But if we say, from this dice if iI define a fuzzy now fuzzy set called small number then one1 will be a small number certainly so that is one1, two2 will have a membership value point five.5 in theis sets small number, three3 and all those things will be zero0. 
refer slide time(19:17)

 
soSo this will be a definition of small, similarly this will be a definition of medium anda  
this will be a definition of big number etc. that is onOn the other hand the probability -wise all of them are the same ok. Therefore so we must understand the difference between fuzzy set and fuzzy number and a priority well value, they are conceptually different. Now next week on our ultimate objective  is to go towards fuzzy inferencinge which will do. butBut in order to deal with fuzzy sets just as we do in conventional sets we need to carry out some set operations.  soSo here we are introducing some fuzzy logic operations just like and or notAND, OR, NOT as we did which we did in in the case of a normal set and for normal logic. we willLet us see what they are for fuzzy sets. forFor example, equality: Ttwowo fuzzy sets a and b are straight to be equal if for all x their memberships are the same.  ok theTheir membership of all x in the set a is the same as it is in the main set b. 
nowNow look at the difference between this. if inIn the case of a normal set we setsaid




refer slide time (20:35)
 
that twotwo sets that a and b are equal if all the elements in this set or are equal to this. butBut in this case say i have got all the elements are same but this is a fuzzy set and this is another fuzzy set then unless the membership of this is same is not the same as the membership of this the element membership of this is not the same as the membership of this element and similarly for all the elements,  
just the belongingnessness  will not do but the membership values should also be the same then only we can see that area okthe theory works. 
other The other thing is complement. inIn normal set and fuzzy set it is athe same the complement is here. theThe complement of fuzzy set a is denoted as a’ prime  or a complement, membership of a complement x is one1 minus– mew membership of x in a and that is for all x. 
containment Containment that is a is the sub set of b a is contained in b that is true in case i said if all the membership values for all the xes’ in a  ora or less than equal to membership value of the same element is same in the  in b. This is the way we are dealing with the membership values.
alright so you see how you are dealing with now with the mebership values
hereHere are twotwo important operators union and intersection. The like union of twotwo fuzzy sets a and b is defined as a news fuzzy set n where the membership value of those elements for all the elements here will be the max of max of their membership value elements in the individual sets. andAnd similarly in intersection the membership value in the intersection set will be the minimum of their membership in the individual sets.
 now we can quickly give an example to this 

refer slide time (23:05)
 

lets see here that a way a nice example here is an example; Wwe are looking at a fuzzy set a this is a fuzzy set a, alright this blue line is a fuzzy set a and then the compliment of fuzzy set a complimentonents will be the red line is defined by this red line, wherever its  wherever is membership is, suppose it isf membership of 20twenty is something like or . point four4 then the membership of compliment for twenty 20 of will be one1 minus– .four4 point six.to .6 so this is the compliment s et. 
look hereHere we are looking at the intersection, this blue line is this one that is the fuzzy set a and this green line is a fuzzy set b, alright this is b then the intersection of that is for all the x es’ which are common to both a and b and whose membership value say maybei take this this 40 forty the 40’s 
forties  membership value  in a ais some thing like point eight.8 and forties  40’s membership in b is something like point four.4 so in the intersection forties  membership will be the minimum of this point two.2 to point four.4 which so it should be point four.4 50,fifty its membership in a is  point six.6 five5, its membership in b is also point six.6  five5 so in the intersection set membership of fifty will be the minimum of this so the minimum of point six.6 five5 and point six.6 five5 point six.6 five5 so in this way with this red line we will define the intersection set.
 theThe union set on the other hand will look like this. thisThis was a and the green was b  andb and the union is red line where iI have taken max of the membership values. And 40’s forties  membership in b was point four.4 and in a it was something like point a 8 so iI have taken onethat point eight.8 alright so this is the union in fuzzy set okthis set. 
As we know now this we have already discussed that the we can modify the meaning of fuzzy set uses hedges like very more or less slightly. andAnd typically very f is nothing but f2 square like tall, is very tall and more or less is the square root of that. that we already discussed 
nextNext we come to a very important notion that is a fuzzy relation. now just gives you overview of relation whatWhat is the a relationrelation? 

refer slide time (26:00)
  
weWe know, if we have got a crisp set a in which we have got set on tamTom and johnJohn and in the set b we have got say marks like eight80y and seven70ty. alright then we can define a set a ×cross b which will be something like all the possible tuplesbules to Tomm that is oneone element from this and oneone element from this, johnJohn eight80,y tomTom seven70ty  and johnJohn seven70ty  so these are the possibilities.
 Thenand we define a  are relation r which is a subset of a ×cross b. Hence so r may be something like may be all the things are not there depending on what r is, tomTom eight80y and johnJohn seven70.ty and .
nowNow iI give a name to this relation marks obtained. a name to this relation soSo, marks obtained is the a relation, tomTom got eight80y and johnJohn got seven7ty0. so thatThat is what we know as aabout what  relation is all aboutn in the typical computer Computer sScience, district Discrete mathematics Mathematics sense the Discrete this is structuresStructures. that is all of you know that thisThis is the definition of a relation.
 so similarlySimilarly iI could have defined, if there was set c like maryMary and susanSusan iI could have defined a set likes the relation likes between  a ×cross c so tomTom likes may bMary and e johnJohn likes Maryso that could be a set ,
that could be relation.
 alright nowNow here you see that, in a relation either this tuplebule belongs to this lesson relation or it does not belong to this lessorelationn.  alright from this  athis a ×cross b either it belongs to this or it does not belong to this. so
 butBut in the case of a fuzzy relation this belongingness will again be fuzzy, there they will be degree or grade or belongings. soSo it will belong very strongly or it will belong very weakly. nowNow this relation tomTom johnJohn etc iI can further represent ited in a form of a graph


refer slide time (28:56)

 

where iI have got tomTom and johnJohn and here iI have got those marks say eight80y and seven70ty and the graph my relation was defined as the graph that is tomTom got eight80 and johnJohn got seven70.ty. alright nowNow the print strength of these links are oneone. say iI can also have another relation 
refer slide time (29:42)
 
	
say tomTom, and johnJohn another relation maryMary  
and susanSusan and a relation may be something like this this likes tomTom likes both maryMary and susanSusan and johnJohn likes maryMary. soSo this graph can be used to represent the relation.
Now the strength of relation of thesethat links which is denoting that relationship between from tomTom to maryMary areor all having a strength of oneone in the case of a crisp relation. butBut in the case of a fuzzy relation there is a degree of association or degree of belonging. soSo we define it as r the fuzzy relation with will be a gradeeat of membership of  of x all thisall these elements x one1, x two2 etcwhatever. 	
It is like x one1’s membership in this relationlesson, x two2’s membership in this relation etc.lesson  that way soSo this relation is actually associating the membership grade mew for of membership and the each tuple.bule.
 forFor example, friendship, so manas Manas and Ssuman are friendfriends; they me belong to the friendship relationship membership of point nine.9. Mmanas and Jjyoti well theyare friends but not that strong so that i’s point one.1, Aalex and mMike point eight.8, Aalex and Jjohn point three.3 so here you see here along with those hedges that we had shown we are also putting some rate grade of membership.
 nextNext operator that we are looking at that tis the typical and or nAND OR NOTot. 	
weWe know that not NOT of a is denoted by compliment of a is one1 minus– mew a (x), 
a and b which is the intersection essentially is when we have got min we have got twotwo memberships is that x has got only for the element which is there in both. ofOf course the membership that is minimum in a and b i itI take its membership in a and is membership b and take the minimum, a or b will be the maximum. alright now soSo, if iI just take the a crisp set 
that a and b can only have values   refer slide time(32:32)
 
zero0 and one1 which is the the truth table and you see if iI apply a min a b for n this is thethe case for of n alright one1 one1 is one1 and everything else is zero0 this is the truth table for conventional a and b. but But if iI apply min a b then also a and b will be zero0, min of zero0 and one1 will be zero0, min of one1 and zero0 will be zero0, min of one1 and one1 will be one1. similarlySimilarly for r the max of zero0 and zero0 will be zero0, zero0 and one1 will be one1,  
one1 and zero0 will be one1, max of one1 and one1 will be one1. so Therefore in this way you can see that the fuzzy logic is actually covering our conventional logic. soSo fuzzy logic can be considerconsidered to be a super set of the Boolean logic. 

 


nextNext in order to do some inferenceing what we need to do is, we have to look at if then type of rules. weWe use fuzzy sets and fuzzy operators as the subjects and verbs of the fuzzy logic to form the rules. nowNow consider this rule; if velocity is small then current needed is small. 
nowNow obviously this is a rule where the definitions are rather or statements  are different thaen what we had in the case of conventional rule because here we have got all thiese fuzzy terms here. 
soSo such rules can be written as if x is a then y is b. 
ifIf the boy is tall then the boy is strong. 
tall and strong a ifIf the boy ifs tall then a A is fuzzy set tall. alright soSo a A is a linguistic term defined by a fuzzy set tall, then y Y is strong, B then the a linguistic term which is a fuzzy set strong 
defined over some crisp set yY. alright soSo we can have a couple of more examples; if  temperature is high then fever is this looks little odd 
if temperature is high then put coolant as moderate.
 now hereHere if iI write this as, if x is a now the x is this linguistic term which is taking a value from a fuzzy set aA.
andAnd that fuzzy set a A is the fuzzy set high, then y is b B, y is the variable coolant and b B is again the fuzzy set moderate. soSo this is a fuzzy set and this is another fuzzy set. so So fuzzy rules can be generalized in this ways if x is a fuzzy set then y is b B. now Now the question is, if iI have a rule like this 
refer slide time (36:54)
 
if x is a A where A is a fuzzy set then x is fuzzy set then y is bB alright this is another fivefuzzy set. z nowNow, iI have to evaluate in order to fire this rule in the form of forward changing iI have to first evaluate first this antecedent and earlier we found that this antecedent usually is evaluated to true or false. nowNow, if  temperature is high now iI have got a fuzzy set high and suppose temperature is seven70°ty degrees  and in the fuzzy set high the membership of seven70°ty degree is mew of seven70ty in high is a point seven.7 alright  so this will evaluate to not one1 or zero0 but it will evaluate to point seven.7. Therefore un like unless the conventional rule ways base where the antecedent was evaluating to trueed through  it is now  also having a quantity 
of truth.
 iI can even make little morethis antecedent a little more complex. even let me make it complex let me make it say i now write iIf  x a is A is true then now what will be the value of this antecedent? So x is again temperature seven70°ty degree and a A is high so iI evaluate this in some fuzzy set high temperatures as something like this where iI got seven70°ty degree  and  in that iI found something a outits membership to be .  point seven7 so this oneone comes to point seven.7.
nowNow i take the point seven this is true now and for  that as we have shown in the last class last lecture we can define some truth so here is the mew value of x same in high and there we got point seven.7 and here is a this is my definition of for the value true, this is the fuzzy set true as alright something like this. soSo point seven.7 point seven  will have  ahave a membership say point five.5 here this is the twoa two level thing sothing. So this entire thing will now evaluate to point five.5. thisThis is how the antecedents are evaluated.  


refer slide time (40:07)
 
now again we can have some rules like; if x is a A and just a conjunction y is b B, if the temperature is high and the pressure is low and say x in heas in the  earlier case it was seven7ty this was high and so this value evaluated with point seven.7 and this was some value is a pressure fiftyof 56 p s iPSI and in the set low pressure thati’s some how evaluated with point eight.8 then what will be the evaluation? whatWhat will be the truth of this entire antecedent? weWe have to look into that will see that actually take the mean min of theise twotwo so mean min of this twothese two point seven.7 and point eight.8 we will take so it will become point seven.7 so we evaluate this conjunct antecedent on with a strength point seven.7. ok refer slide time(41:17)
 
ok so evaluationEvaluation of fuzzy rules: Iin Boolean logic we know that p implies  qimplies q if p is true then q is true.  
butBut in fuzzy logic if p is true to some 

refer slide time(41:34)
 

degree as we have seen x is a A true with point to tthe degree point seven.7 then q is true to some degree, it is not just as q is true absolutely. likeLike here simply if p is true with .5 then point five q will be assumed to be true  with point five.5. 
now maxMax-min mean rule of composition: which are just now discussed 
Ssuppose iI have got multiple rules, there
refer slide time(42:05)
 
are n observations e i  i mean e one1, e two2,  e three3 like that up to enok e and over x and we have got n different hypothesis ok for them.  nowNow iI want to evaluate the max, iI want to evaluate the strength of the hypothesis and
i will take if all of them true i will take mew here this can be a conjunction as youas we said x is a A and y is b B so there what we will do is you do we will take atake the min of that so we will get a min value of 

refer slide time(43:02)

 
e one1, iI will get min value of e one two2, I will get min value of e one n.
andAnd then out of those iI will take the max of those. so in order to make it a little more clear 
 
refer slide time(43:05)



 
Again the samelet me again write it down let me write same example 
for example; x is a A is the if part and y is b B then maybe something like b is q is oneone rule. thereThere is another rule say j is b B and k is Cc implies may be  r is may be q or l. 
something nowNow this onex is A is evaluated with point seven.7 
and y is B this one is evaluated with point eight.8. 
so So we took the meain of these twohis two and this antecedent is evaluated to point seven.7.
 supposeSuppose this part evaluated with point five.5 and this part evaluated with point nine.9 so ithen I will take the meain of this so it will be point five.5. nowNow, out of this point five.5 and point seven.7 iI will take the max of these twotwo  so iI will select this rule and iI will fire this concept, so this is one this one  approach of doing that.will ok [refer slide time (44:42)]


 
This is what is told here; the maok that’s exactly what is being said it here max of all this means mins. 
now eEvaluation of fuzzy rules: The most common way  is to chop -off the consequent. 
refer slide time(45:02)
 
so we have just now shownJust now we saw the example where we aggregate the outputs into a single fuzzy set outputs in to a single fuzzy set which combines their a decision. ok nowNow let us come to fuzzy inferencing problem. thereThere are some things we need to do for theis. The first thing is fuzzification. whatWhat is meant by fuzzification?      	
weWe are given some number, why do we need to fuzziyfy? 
weWe need to fuzzifyication because our inference rules are not crisp rules as we have shown in the conventional rule based expert systems. inIn this case our rules for are themselves fuzzy. ifIf the velocity is small then the current is quite small. in that way thereforeTherefore when we get some value with that maybe the velocity a is five5 meters per second now five5 meters per second is it really a small velocity? 
refer slide time(46:44)

 

whatWhat is its membership in the fuzzy set small is what we have to find outdo . andAnd this process is known as fuzzification. so So we get some crisp value converting a crisp value such as heighteight for example five5 feet six6 inches to a membership value of fuzzy set such as mediaum on very may be tall.
 thereThere are different ways of fuzzification. someSome are experimental and some are subjective. alright andAnd fuzzified values serve as the input to the fuzzy rules. so  fuzzificationSo fuzzification is converting a crisp value to a fuzzy membership. 

theThe other thing is defuzzification.
refer slide time(46:49)

 

whatWhat we mean by defuzzification? It is convertingconverting a fuzzy term such as small shift.  supposeSuppose some thing is like some rule is like this; if the temperature is moderate then if the turn the coolant control and give thatif the coolant control is a small shift. nowNow the rule is stating small shift. nowNow for the robot or arm or some automtomated machine we we will have to give the a small shift and then you will have to actually command using somefrom crisp term say five5° degree, 10°ten degree whatever it is. andAnd that this process of converting this small shift to a crisp value is known as defuzzification. alright 
nowNow there are different methods of defuzzification such as centre of gravity method. here i show that  
say For example, iI have got a some fuzzy set like this any arbitrary fuzzy set where this is the variable and here is the membership. nowNow iI get some crisp value after this seventy no i get some membership value alright so i get some membership value here. here nowNow this membership value will have to be translated to some crisp value which is known as theso this is the  process of fuzzification. 
Now, suppose this is a curve and a curve is of this form say as something was like this and a particular membership value is actually cutting this curve in twotwo points here and here so iI am getting this so which oneone will iI take? There are then a different ways, iI can take athe mid point of this or iI can take the center of gravity method it that means for each of theise points iI take membership and iI multiply them this value with a their membership and then divide  by the theise values so that iI get the average of this. soSo there are different methods like max, center of gravity methods etc. 
 those who are interested to pursue this  further thereThere is very good bookare a number of good  books to refer like cleed […..] 49:56 and Ffalger 
and the bobook by  stimuli Timothy Ross.


refer slide time (49:35)
 

 i all this books are very good and it will be useful to you go through them 
nowNow we will show the an example where we will be  applyingbe applying theis fuzzy the inferenceing. letLet me first explain the problem here. theThe problem is something like this; wewe have got we are going to control something. so So 
refer slide time (50:30)


 
we are given some measured value i did some value from a plant. iI get some numerical value but my inferencing method is fuzzy. thereforeTherefore first thing that iI have to do is fuzzify this and get the linguistics variable. thenThen using my fuzzy rules iI will get some output in terms of linguistics, in terms of say small giftgive a small shift or of something oflike that. 
thenThen iI will have to defuzzify that and iI will get a numerical value which iI will again feedback to this control.
soSo here you see iI am working at twotwo levels numerical and linguistic. 
nowNow here is a an interesting example. say thereThere is a crane moving along this axisccess and there is a container and inhanging from here. andAnd the control must leave this control container at a proper point of times so that it  falls on the ship it which is this 

refer slide time (51:38)

 

or it takes it from the ship and falls on the trailer. ok nowNow it is very difficult to model such terms systems using normal state phase approach. nowNow here there are twotwo measured variable and this oneone control variable. soSo what you do  in this casewe do is, we have got angle and distantce as two issues, now two 
 the question is, at which angle Iand like should leave it orand from which distance iI should leave it. 
now here you see theI have to give the container grain i have to give a particular power and depending on the power that iI give we the grain will move with different speed and accordingly the angle and distant vary so iI have to control to thatthat power that’s which is my objective. 
soSo iI read the numerical values the angle and distance the numerical value and then fuzzify it. y field not So I get  some numerical value or some linguistic values and then get the power and then defuzzify it and again actually put the real numerical power to the controller crane. thatThat is the problem that i amwe are trying to solve. 
so the first thing that iI have to do is to define a set of fuzzy sets like  distance, is far, medium, close, zero, negative close etc.something of this 
angle again  will be again positive bigAngle again may be positive big, positive small, negative small, negative big etc. power can be positive high, positive medium, zero zero, negative medium, negative high etc.
but Now these are very big but we have toso I have to actually define their  membership functions. alright now before

refer slide time (53:36)

  

say i can thereforeTherefore before I can define the negative big here on an angle, the negative big is this big fuzzy set, negative small is this fuzzy set, positive small is this fuzzy set in this way iI can define alrightall of them. 
andAnd given any particular values maybe 4° put degree of angles iI can find out the membership that its membership in zero0 is point two.2 and itsn membership in positive small is point eight.8. So, in that way iI can find out their memberships. soSo this is fuzzification. iI get an angle and get  theise membership values. 

refer slide time (54:44)
 





soSo this is how iI do the fuzzification first. afterAfter doing the fuzzification we will apply the rule. supposeSuppose there are couple of rules; if distance is medium and angle is positive small then power is positive medium. ifIf distance is medium and angle is zero0 then power is zero0. ifIf distance is far and angle zero0 then power is positive medium. So we need to do you have to do thethe two things. First we need to do the aggregation. thatThat means first you we have to do the computation of the  if part and composition is a the computation of the then part part.
theThe computation of  the then if part is what we have already discussed. and that is we knew about this 


refer slide time (55:01)



 

say In the first rule we get the distance medium in that it was point eight.8 and the angle positive small was point nine.9, the angle had the a membership point nine.9 here and the distance was point eight.8 at in medium so this thing will be the minean alright that we have already seen  so it is point eight.8. And so for the second rule suppose in   the same the two antecedents get give you point two.2 and for the third rule it is giving point one.1. So for each of theise rules we do the aggregation, we compute with the minean function the truth of their antecedents then iI will select to the compositeion so i will come we have seen all the different rules that all theise rules will give you power and to be positive medium powered to be zero0 powered to be positive medium. so 



refer slide time (56:02)



then when iI compose them i am getting that positive high is not coming with any membership, 
positive medium is point eight.8 because it was the max of point eight.8 and point one.1 where from this the point eight.8 and point one.1 come? again lets go back to this rule Again looking back, positive medium this was point eight.8 say the first rule was saying positive medium with point eight.8 
and the last rule was saying positive medium with point one.1. alright soSo iI have got these so iI have taken the max. Therefore so i have taken out of this iI have taken max so iI have got point eight.8.
 nowNow with point eight.8 what is the power positive medium power i have to give? butBut how would iI give that point eight.8? theSo point eight.8 is some where here, point eight.8 membership is is cutting is positive medium so point eight.8 is coming somewhere here so iI am getting it so iwill again compute the fiss crisp value corresponding to this using some method like center of the maximum and iI will get six6 point four.4Kw kilo watt then thisand that  is the power that iI will have to give. soSo this is the approach that we adopt in the case of fuzzy inferencing.e 

so summarizing let me just quickly summarize gGiven a set of fuzzy rules suppose you gat have a a set of fuzzy rules where so  the antecedents are fuzzy and the consequentces are  fuzzy so you have got some value, then the first thing you have to do is you have to  fuzzify. And in order to fuzzify you have to define the fuzzy sets before hand.ification andAnd those fuzzy sets can be stored as tables and or functions. soSo fuzzification means you will get some membership value. soSo, for every rule that is applicable you will compute the truth of their antecedents. howHow do you compute the truth of their antecedents? if there is a multiple if the antecedent is a conjunction then we will get take the minimum of those. Now, if there are more than one rules now out of the rules which are applicable out of thise minimum iI will take the maximum and that rule I will apply and that consequent will give mebe some fuzzy membership and with that fuzzy membership is positive medium it is saying ok or turn the knob slightly to the left, 
in that  slightly left membership you are getting a membership point four.4. nowNow you defuzzify thatand point four.4 is membership is leading to this which particular crisp value and that is the shift that we will give. nowNow this approach is rather effective and it is being applyied in several applications now-a-days.


Trancripted by p.sasikala
Lecture time (56:14)

ARTIFICIAL INTELLEGENCE
By
Prof. Sudeshna Sarkar
Department of Computer Science and Engineering
IIT Kharagpur

Lecture – 32
Introduction to learning – I (TIME 00:53)

In today lecture we start with the topic machine learning. We will have seven lectures of this series. Today we will give introduction to this topic. 
The instructional objectives of today’s lectures are the following. We will first look at what we mean by machine learning and several definitions of machine learning. 
refer slide time(01:23)

 

In this class the student will be introduced to different learning frameworks and then we will introduce some of the basic notations. Also the student will be made familiar with certain example applications of machine learning. Specifically we will give an introduction to the type of learning which we call concept learning. In this context the student will look at certain things; what are the concept of features used in a learning problem. We will look at what we mean by a hypothesis base or a set of hypothesis and what we mean by a hypothesis that we are trying to learn. We will introduce what we mean by a training set and a desk set. We will also talk about the instance space. Also we will briefly introduce the notion of inductive bias. On studying this lesson the student should be able to formulate a given concept learning problem. Given a problem they should be identify possible features that may be relevant to the problem and they should be able to get an idea of the hypothesis base that they need to consider.
 In subsequent lectures we will look at different algorithms and different types of representation issues. 
Let us look at the definition of learning. Learning covers a broad range of processes.  
refer slide time(03:11)


 
Leaning means to gain knowledge or an understanding of skill in something. When we say we want to learn then we want to gain knowledge or understanding or expertise in solving some problems. And this expertise or this knowledge can come by looking at the examples or by gaining example experience or this knowledge can come by studying the problem or by being told by instructions. 

refer slide time(03:59)
 

There are several parallels between the process of human learning and artificial or machine learning. Some techniques in machine learning derive from the efforts of psychologists to derive theories of human or animal learning through computational models. 
The field of cognitive psychology has dealt with in trying to understand how humans learn. And in some cases they have also tried to identify certain computational models to model the way humans think and the way humans learn. Machine learning on the other hand has dealt with certain techniques some of which I inspired by human learning techniques as derived from cognitive psychology, others by different symbolic techniques which are efficient to do by machines. 
But it is conceivable that the concepts and techniques being explored by researchers in machine learning may actually help understand certain biological processes. So there is a lot of cross partilisation between the wheels of cognitive psychology which tries to understand animal learning and the field of machine learning, their objective is to solely learn by the help of machines.
Before we formulate to define learning let us look at few of the definitions that people have put forward on machine learning. 

	
refer slide time(05:40)


 

The first definition we will take up is the definition by Donald Michie in 1991. This definition states that a learning system uses sample data to generate an updated basis for improved performance on subsequent data from the same source and expresses the new basis in intelligible symbolic form. 
So we have a learning system which uses sample data which we call a training example. So there is sample data or there is some experience to go by and on the basis of this experience the system tries to generate a new model so that it can lead to improved performance on subsequent examples. 
So, the system uses input data to get a model which can help it to improve its performance on new data. But data that is coming from similar sort of examples data from the same source and this model that has been learned is expressed in a symbolic form which can be understood and which can be manipulated. This is the definition by Donald Michie. 
Let us look at the definition by another pioneer field Herbert Simon. this definition says, learning denotes changes in the system that are adaptive in the sense that they enable  the system to do the same task or tasks drawn from the same population more effectively the next time. This is a similar definition as you can see. So this definition says, learning means change in the system and this change enables the system to perform better on similar tasks in future more effectively. 
[refer slide time(08:00)]

 
So we see that they get several other definitions by several other practitioners in the field but we see that this type of learning mainly talk about there being some input data or experience in which some data system learns and the system tries to improve its performance. 
And this improvement in performance must be measurable in some way 
so there must be performance measure which must be improved due to learning. 
Therefore by learning from examples the system is able to improve its performance. 
Secondly, some of these definitions also emphasize the comprehensibility. The new thing the system has learnt must be expressed in a form that will be understood. 

refer slide time(09:01)


 
Therefore now let us simply define a well posed learning problem. 
A computer program is set to learn a task t, so t is the task the system is trying to learn. 
And what is the basis by which the system is learning this task t? The system is given some data which the experience e and the system is learning to improve its performance in task t with respect to its performance metric p. that is, the system’s performance in the task t improves with experience e as measured by the performance metric p. 
Therefore this is the definition of learning we will accept. To put it in more natural form learning is the improvement of performance in some environment through the acquisition of knowledge resulting from experience in that environment. 
To understand this definition we need to go back to the framework of intelligent agent which we have been looking at in this course.
We have this structure, we have this agent and we have this environment. The agent takes action which changes the environment and the agent can sense the environment. 
The agent gets experience by interacting with the environment. And this results in acquisition of some knowledge and using this knowledge the agent can actually improve its performance in certain tasks. So this is the formal definition of learning. 
Now let us look at few examples of learning problem

refer slide time(11:21)


 
to make this more concrete.
Let us say that we want to know whether a particular given patient has a possibility of having brain tumor. 
What is the experience? We have the database of previous patients, the ones who have been diagnosed with tumor and ones who have been certified as having no tumor and we have accessed through the patient records which include different data about the patient as well as some images like MRI or something. 
So we have some data about past patients having tumor. 
And we wish to find out given a patient and the records we want to know whether this patient is likely to have tumor. 
The second example is, our task is to recognize speech. So the machine should be able to hear what we speak and it should able to recognize the speech and take dictations. So in this case what can be used as an experience is a database of speech which has been already recognized and their transcripts are available. So we have speech and their transcripts that constitute the experience. And as a result of this experience the system should be able to learn and how can we measure whether the system is successful? We measure the percentage of correctly recognized words. 
Therefore the success in the learning task can be measured by the accuracy or the precision. That is, number of examples which have been correctly labeled as correctly recognized.
And the error can be measured by looking at the number of misclassifications, the number of words which have been wrongly recognized.
So we want to recognize all correct cases. So, for tumor we want to label as positive all those patients who really have tumor and none of the patients who do not have tumor. We can measure it by accuracy or you can measure it by two terms procession and coverage.
So procession accuracy is the number of examples that we label as having tumor and we want to know the percentage of them who actually have tumor and coverage means out of the patients who actually have tumor how many of them do we correctly recognize as having tumor. These are the different measures we use for measuring the accuracy or the correctness of a learning task. 
So the learner is in an environment learner is tempting to learn something about environment so that it can perform some of its tasks well. The learner is linked to some knowledge base from which it can draw and which stores the acquired knowledge. Therefore the learner has a knowledge base, there could be some prior knowledge in the knowledge base but as a result of learning the learner is able to update its knowledge base. And this knowledge is usually stored in some form of internal data structure. 
You have studied logic and you have seen representational languages in logic,
first order logic and you have also looked at certain other representation schemes and frames etc. 
Knowledge is basically stored in one of these representation schemes. 
And what is experience? Experience is basically derived from the perceptual input of the agent and the agent can take some action which is the output of the agent. And the performance of the agent is measured quantitatively by several aspects and depending on the task we decide what to use. 
Let us just review why study of machine learning is so important. There are many tasks that require an adaptive system that require a system which can learn.


refer slide time (16:07)
 

For example, hand writing recognition, speech recognition are examples where an adaptive system is required. Learning is also useful as an alternative to hard coding a program. For example, suppose you want to develop a program which can play the game of chess. Now you could write a program in which for every different possible situation you can specify what move the agency. So you could hand code all the rules useful for playing chess. Therefore an alternative of writing such a program would be to provide the system with a database of chess, games and their outcomes
and maybe the system can apply the learning to learn to play a good game of chess without even instructing the system to make the right move at the right situation. But you give a large database of games by which the system can figure out. Providing database of games is usually easier than hand coding the rules. So you can save a lot of manual effort if a system is able to learn. Also the study of machine learning gives us an insight into human learning. For example, learning of language is a very non trivial task. 
So trying to understand how we can make a machine learn a language can give us a clue as to how humans acquire language. Machine learning has been very useful in the curb of data mining which helps systems to acquire hidden rules from data, which has opened a whole new area of applications. This is a new kind of capability that our systems are provided with. There are many types of learning and we can classify learning along different dimensions. 
Supervised learning:




refer slide time (18:39)

 

In supervised learning the system is given labeled training examples. 
In these labeled training examples we have a set of examples and we also have the labels. So we have the inputs and the outputs of the examples. 
And unsupervised learning is the learning where there are no labels given. We only have the examples which are not pre-classified. So we have unclassified training example and there are situations where we like to learn from them. 
For these lectures we will be mainly concerned with supervised machine learning where we are given labeled examples. For the concept learning we have a set of labeled examples during training using which we will learn to classify unknown example. There are also other types of learning where we do not have labeled examples but we try to learn. 
 A third type of learning is reinforcement learning. 
In reinforcement learning we are not given just examples and each one of them is labeled but we are given a sequence of examples and at some points the system gets some reward or some punishments called reinforcement.
For example, when a system is playing a game of chess the system does not get to know whether each move is good or bad each move is not rated. But at the end of the game the system knows whether it has won or the game was a draw or it lost. 


refer slide time (21:30)


 



That is the reinforcement which is available at only certain points in the game.
So, in reinforcement learning the system is trying to learn but it gets reinforcements only at a certain time.
 What are the different types of knowledge that can be acquired by learning? The types of knowledge can be declarative knowledge. Declarative knowledge can be expressed in terms of concepts, in terms of preferred value of parameters or in terms of grammar or in terms of taxonomies. 
The knowledge acquired by learning can alternatively be expressed as procedural knowledge. 
Procedural knowledge can be expressed in terms of rules, rule strengths, graphs or networks, computer programs and plans. 


refer slide time (21:38)


 
Certain data structures can be used for storing knowledge. 
Certain data structures are decision trees, logical expressions, neural networks, condition action rules, sets of rules, finite state automata and programs. For example, concepts
refer slide time (22:06)


 
when we want to represent concepts we can represent concepts by decision trees, or by logical expression or by neural networks. Behavioral rules can be expressed as condition action rules. 
Plans can be expressed as sets of rules or by finite set automata. Computer program can be expressed as c code. And then there could be different strategies by which these learning rules can be acquired. 
Inductive inference can be used to learn concepts and grammar. 
refer slide time (22:45)
 

Evolutionary learning or genetic algorithms can be used to learn certain preferred values of parameters. In unsupervised learning clustering can be used for learning taxonomies. 
refer slide time (23:02)

 

and then we can also have analogy or induction to learn rules, we can have reinforcement learning to learn plans or strategies or which you call policies, evolutionary learning, stochastic learning etc. There is a wide choice of learning strategies and learning programs. 
refer slide time (23:31)

 
How do we evaluate learner? Once we have a learning strategy typically what we do is we draw a learning curve. In a learning curve what we do is we plot the accuracy of the learner along the y axis, this axis is the accuracy and along the x axis we plot number for training example seen. 
So what we typically expect is that the accuracy of the learner will increase as the number of training examples increases. These are typical learning curves that we might get. So these learning curves plot the accuracy or precision in terms of the number of training example seen. Next we will talk about 
refer slide time (24:26)


 
inductive learning for classification also known as concept learning. In fact in most of the lectures on learning in this course we will mainly be looking at the idea of concept learning. 
What is concept learning? 
refer slide time (24:44)

 
Concept learning or classification means learning at description of a class of objects. So we have some concept or some object whose description we wish to learn and concept learning is learning this description.
And why do we want to learn this description? We want to use this description to predict the class of a new object. When we have a new example we want to know which class this new example belongs to so we try to learn the description of a class. So, if we are given an animal we want to know whether this animal can be classified as a tiger or not. And in the past we have seen several animals we are told they are tigers and we also have seen several animals for which we were told they are not tigers and from this we will form a model what is a tiger. And when given a new animal we will be able to know whether this animal is a tiger or not a tiger, so this is the essence of concept learning. 
There are other examples of concept learning tasks some of them are described here. Suppose our objective is to classify parts as defective parts or ok parts. 
Second example is mammogram analysis. We are given a mammogram and we want to classify whether the mammogram is either normal or precancerous or cancerous. 
Example 3; In document understanding we are given a rectangular region from a scanned image

refer slide time (26:35)

 
and we should be able to say whether this is a text region or graphics region. So we want to recognize a text region from a graphics region. These are certain examples of concept learning tasks. Now we will be using inductive inference for concept learning. And let us see what we mean by inductive inference. 
refer slide time (27:09)

 
Now, suppose H is a hypothesis and F is a set of facts, suppose we know that H implies F, if H implies F is valid this rule implies f is valid and H is known to be true then by Modus ponens F must be true. H implies F is valid, the antecedent H is known to be true then logically by Modus ponens it follows that F must be true. This is an example of logical deduction. So we can derive F from H. this process is truth preserving and this is called deductive inference. Let us take an example. You know that all men are mortal and you know that Socrates is a man and you can conclude that Socrates is mortal. 
This is an example of deductive inference. Now let us see next what we mean by inductive inference. suppose again let us assume that h is a hypothesis and f is a set of facts and again let us say that H implies F is known to be valid. Now suppose we know F, now knowing F in inductive inference we will try to derive H. now from F H does not follow by deductive inference. 
But if F is false then H must be false, if F is true deductively we cannot say that H must be true but if F is false we can say that H must be false.
So deriving H from F is falsity preserving but not truth preserving. So, if there are some facts which make f false then h must also be false. But in inductive inference we will derive H from F. Suppose for 10 days I have woken up and seen that in the morning the sky is blue and on the basis of this I form a hypothesis that in the morning the sky is always blue.
This is an example of an inductive inference. 
On the basis of 10 data of the sky being blue in the morning I am inferring that the sky is always blue. This is an example of inductive inference or induction. 
refer slide time (30:24)

 
Another example of induction is, suppose you see five white tigers, you see five tigers and all of them are white and you conclude all tigers are white. 
This is an example of inductive inference. 
 When can you be wrong? Suppose now you find a yellow tiger now this falsifies your conjunction. 
So inductive inference is not a logical inference so it does not preserve the truth but it is a useful leave to take, it gives you new knowledge, knowledge that you have not already seen. An example of deduction which I already mentioned 

refer slide time (31:10)

 
is suppose you told that all men are mortal and you know that you are man and you conclude that you are mortal. This is an example of deductive inference. 
What we saw earlier was an example of inductive inference. Now let us look at concept learning in more detail. 
refer slide time (31:30)

 
Suppose you have a goal concept that you are trying to learn so we will call this goal concept as a target concept. A target concept is a concept you are trying to learn. 
For example, you are trying to learn about a tiger so tiger is a target concept. 
And your guesses of the target concept is the hypotheses. So you are trying to learn the concept and you make hypothesis. You form a hypothesis about the description of the concept.
So a hypothesis is your guess or approximation of the target concept. And how do you form hypothesis? You form a hypothesis by looking at data, by looking at many instances of data or examples. So, an object which is used to help you learn the goal concept is called as instance or an example. 
An instance or an example is something which helps you learn the goal concept. 
Therefore typically an instance is described 
refer slide time (32:45)


 
by a vector of features also called attributes. Example of attributes; Suppose you are given a new animal and the attributes are color, number of legs, whether the animal has viscous, length of the body, whether the animal has fur and so on
so these are the features if you are given an animal whose color is white, it is furry, it has a tail, it is 6 ft long, it has viscors so this is a description of an instance of an animal. 
Therefore an instance is described by vector of features, features are also called attributes. So, for example the instance is given by x1, x2, xn and x1 is the value of the first feature, xn is the value of the nth feature. There are different types of features that we can have, features can be nominal. For example, color of the animal, 
refer slide time (34:53)
 
color can be red, blue, yellow, green so these are some specific values they are not directly linked to each other. Therefore color is a nominal attribute similarly suppose you are trying to learn an object whether the object is a table and one of the attributes it can have is from what material the object is made of? And the material can be wood, steel, glass etc. So these are nominal attributes.
Secondly, you can have numeric attributes. Usually the attributes can be ordered the other attributes are also in an order. Order means there is an ordering to the values. For example, length of the animal is the order attribute; it is 5 ft, 6 ft, 4 ft, 8 ft so they have some relationship within themselves. This is an order attribute. Temperature, weight, length are examples of order attributes. 
Thirdly attributes can be structured. 
There is some structure in the order of the values but they are not fully ordered. 
For example, if the values can be put into some sort of generalization hierarchy or some partial order. For example, if you consider the animal taxonomy, this is an example of animal taxonomy,
refer slide time (35:14)

  
Suppose you have the class vertebrates and you know that animals can be classified into invertebrates and among vertebrates you can have mammals, reptiles etc and among mammals birds, fish are vertebrates, so among mammals you can classify them as four legged mammals and two legged mammal, under four legged mammal we have tiger, mouse, deer etc and under two legged mammal we have humans, kangaroo etc. Therefore this is an example of classification hierarchy and this is also an example of a structured attribute and in this direction we have more general classes.
refer slide time (36:01)

  
So the concept learning problem formally can be described as follows: We are given labeling function f, f is a underlying function for describing the concept. This function f maps feature vectors into some classes. 
Therefore we have a discrete set of k classes that are k possible classes. 
For example, we have two classes tiger or not tiger or we can have three classes whether it is a tiger or a lion or a deer. So, in general we can have a finite set of classes so we have k classes and we have an actual underlying function which maps to instance to one of these classes. 
So the function f and maps an input instances to one of these classes. 
In a special case we may have only two classes and in such a case we may say that one of the classes is positive and the other one is negative. 
We are given some training examples. 
refer slide time (37:13)
 
Each training example is a pair, the instance and its classification. So each training example is given as a pair the instance and its class and we have a set of such training examples. 
So using these training examples we want to learn an approximation of f.
Therefore from the set of x f(x) pair we have got we want to learn the target concept f. Therefore f is what we wish to learn. 
So given a set of x of f(x) pairs we want to infer f we want to inductively infer f. Now if you are given a finite sample and you are not shown all the instances it is not really possible to guess the correct value of f with absolute certainty.
So we will apply inductive inference and we will try to find some pattern in the training examples and we will assume that this pattern will hold for future examples also. This is an example of training set, we have an instance x = 1 then the classification is 1, x = 2 f(x) = 3 f(x) = 9 x = 4, f(x) = 16 and we want to know if x is 5 then what is fx? This is an example of a training set that we are give. And if you want to learn the function f in this case one good guess is that the function f is a square function that is f(x) is x2 but this not really a discrete learning problem but this is an example of learning a function. 

refer slide time (39:19)
 
Another example; suppose you want to learn the concept of an apple whether an object is an apple or not, you are given some training examples. For training examples these are the features that are given. For every object that you are given you know the color, shape, diameter, whether it has a stem and then it is labeled whether the object is an apple or not. And you have got four examples. The first example the color is red, shape is round, it has a diameter of four inch, it has a stem and you know it is an apple. The second example is, the color is yellow, shape is round, diameter is 4.3 inch. It does not have a stem it is an apple. 
The third example; the color is green, shape is square, diameter is 5 inch, no stem, not an apple, this is a negative example of an apple. 
Fourth example; the color is green, shape is round, diameter is 3 inch, it has a stem and it is an apple. 
Now you could learn a rule or a set of rules to distinguish positive examples of apple from negative examples. These rules are called classification rules. For example the rules could be, round means apple or stem implies apple 



refer slide time (40:45)


 
or diameter less than 5 inch means apple or round and diameter less than 5 inch means apple. So these are some possible rules you can hypothesize about an apple.    (41:07)

 
In this case we are looking at rules expressed in a particular type of language. This rules are given in terms of the features. But these features are expressed in some language. For example, round and diameter is less than five inch so here we are expressing the antecedent as a conjunction of constraints and features. Similarly we could express rule as a disjunction. We could say round or diameter less than 5 inch implies apple. So this is the example of disjunctive rule. Or we could express it other forms. We could decide a language to express the rules. 
And this defines the set of hypothesis that we are considering. What type of hypothesis should we consider? 
Suppose f is the set of rules then the space of hypothesis is all rule sets. The space of hypothesis could be simple polynomials; the space of hypothesis could be decision trees, the space of hypothesis we are considering could be neural networks. Therefore we have a choice about what type of hypothesis we are going to consider. We are trying to learn a function f which we do not know and we are trying to find an approximation to f, we are trying to make a hypothesis. 
We will have to choose this hypothesis from a set of hypothesis that we will consider. This is called our hypothesis phase. 
And what set of hypothesis we should consider? This is what we must decide. So we will express by H the space of 
refer slide time (43:25)
 
all possible hypothesis that a learning program consider. Then our objective would be to find one hypothesis which is a member of this hypothesis space. We want to find that hypothesis which is the best. We want to find the best hypothesis of this hypothesis space. What is the best hypothesis? It is that which fits the given data the best. Therefore this is a process of search. In order to find its best hypothesis we will search through this hypothesis space to find that hypothesis which fits the data in the best possible manner. 


refer slide time (44:14)
 
This is an example of a hypothesis space. In this hypothesis space there is large number of hypothesis.  For example h1 h2 h3 h4 h5 are some of the hypothesis in this hypothesis space and we want to select the best of the hypothesis in this hypothesis space.
Suppose you want to learn the concept apple
refer slide time (44:14)
 



and this is an example of a hypothesis space and you have a hypothesis here like round and diameter less than 5 degree, red and round has stem, not a square, round are some of the hypothesis that are present in the hypothesis space and we want to select the best of these hypothesis. 

refer slide time (45:10)

 
Some definitions: The training set is the set of all training examples given to the learner, they called the training set. Therefore a training set is the set of all examples that are given to the learner. And a testing set is a set of examples on which the learner tests his hypothesis. So, on the basis of training set the learner will learn the hypothesis and the learner will try to evaluate how good the hypothesis is by looking at another set of examples which is called the test set. 
So the learner on looking at the training set will find a hypothesis. The hypothesis the learner has found will be set to be consistent if it is consistent with 
all the training examples. That is, the hypothesis must predict the correct label of all the training examples and such a hypothesis is called consistent hypothesis. 
We cannot guarantee that a consistent hypothesis will necessarily always give the correct labeling for each test example because we have not seen the text example but we can check whether the hypothesis correctly labels the training example. 
A testing set is the set of all examples that are given to the learner after it has learnt the hypothesis on which its accuracy will be tested. 
 


refer slide time (47:04)
7 

Suppose the examples are labeled plus and minus then the consistent hypothesis is a hypothesis that implies that labels are positive for all the plus examples and none of the minus examples so that is a consistent hypothesis. 
refer slide time (47:21)

 
Now let us look at this diagram. Suppose this one is the instance space which consists of all possible examples and suppose this is the true function f that our system is trying to learn. 
So this f labels the instances inside the circle as positive and the instances outside the circle as negative. Now this other circle is the hypothesis h that you learner finds. In this region the h and f agree the label of instances. h makes mistakes in this blue region and in this black region. So these are the two regions in which h makes a mistake out of this. An example is called to be a false negative if the hypothesis says it should be negative but it is actually positive. In this blue region your hypothesis states that these examples must be negative but these examples are actually positive according to f so these are falsely negative by your hypothesis. So this is a zone of error of your hypothesis. In this green region your hypothesis says that this instance should be positive but they are actually negative according to f. so this is another error zone and this is said to be the false positive for your hypothesis. So an example is a false negative for a hypothesis if the hypothesis says it would be negative but it is actually positive. An example is false positive for hypothesis if the hypothesis says it should be positive but it is negative. 
This big thing is the instance space, all the instances are here and this is a particular concept, inside the circle we have the positive example, outside the circle we have negative examples, and this other circle is your hypothesis. 
Inductive bias is another definition.
refer slide time (50:21)

 
When you are making an inductive inference on the basis of looking at some examples you are making a hypothesis. That hypothesis may not be fully correct. for example, if you are making a jump, you are making a conceptual jump, you are assuming something extreme so when you see some data you could infer different possible hypothesis. Now which one of them would you select? You could try to find out that hypothesis which is consistent but suppose you find several hypothesis which you are consistent with your training set, like in the apple example we could find
several hypothesis which are consistent the four examples so which one of them would you select? In order to select one of them we use the concept of inductive bias. 
Therefore inductive bias is a bias or a preference for one hypothesis over another. And there are different types of inductive bias which are used by different learners. For example, simplicity could be a bias or most general hypothesis could be a bias or choose the most specific hypothesis could be a bias. Therefore among hypothesis which are both consistent we choose one of them according to some bias that we have. 
refer slide time (51:59)

 

For example, suppose you want to learn the concept apple then you could learn a set of rules to distinguish positive from negative examples. 
These are called classification rules and these are rules we have already seen. And then you could say in order to select among rules you apply a bias this is called inductive bias. 
Now how do you choose the best hypothesis? 
One way in which you could choose the best hypothesis is, among two hypotheses which are equally consistent on the training examples you should choose a hypothesis that is likely to agree with new examples. So our real goal in machine learning 
refer slide time (52:55)

 
is to find a hypothesis f’ in h such that it will correctly classify new examples. 
So the ideal thing would be to choose that f’ so that probability that f’x does not agree with f(x) is smallest.
We want to choose that f’ so that it agrees with f most of the time. But we cannot compute this probability because we do not before hand know all the instances all the labels of instances etc. 
And how we will evaluate the performance of the learner?    refer slide time (53:35)

  
We will evaluate the performance of the learner by seeing how well the learned hypothesis can predict the classification of unseen examples, the
examples that we have not yet seen. 
refer slide time (53:53)

 
If we use the training set for classification we will not really understand whether hypothesis is performing well on unknown examples. 
So we should apply the hypothesis we learnt from the training set and a new data. 
So test set should be disjoint with the training set. 
So, in a concept learning our problem is to learn the target concept f from a set of training examples. 
And we evaluate we measure the predictive performance of the learned hypothesis on a set of test examples called the test set. And we require that the training and test set are different from each other so that there is no bias. 



refer slide time (54:54)

  

Few questions to consider:
1) Consider the problem of trying to recognize hand written digits. You have to formulate this as a concept learning problem. Specifically you have three tasks; a) Clearly specify what are the possible features? 
b) How do you get the training set and the test set? 
c) How will you measure the performance of your learning algorithm? 

refer slide time (55:36)

 

2) Consider the problem of trying to play a game of Ludo, formulate this as a learning problem. 
Clearly specify what your system will try to learn. 
b) How can you get the training examples for this system?  

Trancripted by p.sasikala
Lecture time (59:29)

ARTIFICIAL INTELLEGENCE
By
Prof.Sudeshna Sarkar
Department of Computer science and Engineering
IIT Kharagpur

Lecture – 33
Introduction to learning – II (TIME 00:01)

Welcome, today we start with second lecture on learning. In the last class we had given introduction to machine learning. Today we will look further at the problem of concept learning. And in the subsequent classes we will look at some machine learning algorithms. The instructional objectives of today’s class are as follows: 
refer slide time (1:32)
 
We will be reviewing concept learning or classifiers. We look more closely into the inductive learning hypothesis and explain the various types of inductive bias and also discuss why inductive bias is regret to select hypothesis. We will look at the definition of version space. We will discuss the concept of over-fitting and we will also look at the issue of cross-validation in order to find out the accuracy of classifier. And we will look at some versions of the version space learning algorithm. 
In the last class we discussed about concept learning or inductive learning for classification. 

refer slide time (2:30)
 
According to this problem the assumption is that you have a goal concept that you are trying to learn 
refer slide time (2:45)

 
and we call this a target concept . And as we discuss the target concept could be something like a description. You want to know whether the given object is an apple or not or if you are given an image you want to know whether this is an image of zero or not. 
So your objective is to learn a description of the target concept. And what you are given is a set of examples which we call the training set. An example or an instance x is described by a vector of features also called attributes. For example, suppose x is an instance x consists of n values x1 x2 xn then 
x1 is the value for attribute 1 for this instance, xn is the value of attribute n for this instance. 
Therefore, for the learning problem we have a number of attributes or features and each instance is described by its value for each of its attributes. Our training set consists of a set of such training examples. And there is an underlying labeling function f that maps an instance to a class. Sometimes we will be interested in two class learning problems. In those cases we will say that some of those instances are positive example of class and others are negative examples of the class. So there can be two classes positive and negative and in general there can be a finite set of k classes and each instance is labeled to belong to one of these classes. So f is a function which maps an instance
refer slide time (4:57)
 
to one of the classes from 0 to k – 1. Now our objective is, given a set of training examples that is a set of instances x and for each instance its label effect which is given to us we want to learn the target concept f. The function f is not known to us but we know f applied to the training instance. So we guess something which can possibly be f and these guesses are called the hypothesis that we form. 
So we talk about a hypothesis space.
 We are going to search for f and where are we going to search for f? 
We define a hypothesis space. once we have decided what are the attributes or features that we are using we will define a hypothesis space. And a hypothesis space is a collection of possible hypothesis. we will try to find good hypothesis out of those in the hypothesis space. So the hypothesis space h is a space of all possible hypothesis h1 h2 hn so h1 h2 hn are the different hypothesis in the hypothesis space. So, in the set of hypothesis suppose there are n hypothesis h1 h2 hn and we want to find one of these hypothesis which is possibly close to the target concept f that we are going to learn. 
Now it may be the case or it may not be the case that the target concept f is exactly one of the hypotheses in this space. So, if you do not choose the hypothesis space well or if the hypothesis space is not expressive enough the target concept f may not belong to the hypothesis space. Nevertheless your objective is to find a hypothesis of the hypothesis space which is close enough to the target concept. 
So the objective of the learner is to find a hypothesis h which is a member of the hypothesis space H which fits the training data the best. 
Therefore in order to identify a good hypothesis or the best fit hypothesis from the hypothesis space the learning algorithm has to carryout some form of search through this hypothesis space that is through the space of possible hypothesis. 

refer slide time (8:04)
 
Now, this concept learning problem is schematically represented in this figure. 
So we decide a hypothesis representation which defines the hypothesis space h. the way we are going to represent the hypothesis defines the hypothesis space.
Our objective is to find the desired hypothesis by searching from this hypothesis space. and the desired hypothesis is one which best fits the training example that we are given. 
The training examples are input to the learning algorithm. The learning algorithm searches through the hypothesis space to find one of the hypothesis which best fits the training example.  
 How do we get the hypothesis space? We have got a set of attributes or features let us say a1 a2 an are the n attributes. 
So the hypothesis space is defined in terms of these attributes. And the representation we choose to represent the hypothesis defines the hypothesis space. 
For example, if we consider that a hypothesis is a simple rule which can be expressed in predict calculus and the antecedent of the rule is a conjunction of literals so that could define an hypothesis space. Suppose that we have three features a1 a2 a3 then what is the possible hypothesis? The possible hypothesis could be a1 which is a possible hypothesis, a2 is a possible hypothesis, a3 is a possible hypothesis, a1bar is a hypothesis, a2bar is a hypothesis, a3bar is a hypothesis, a1 and a2 is a possible hypothesis, a1 and a2bar is a hypothesis, a1bar and a2 and a3bar is a hypothesis. 
So we can find out the number of such hypotheses. These are the possible hypothesis and we want to find one member of this hypothesis space that fits our training example. 
Now, if we consider a different class of hypothesis, suppose our hypothesis is a disjunction of exactly two literals then our hypothesis space will be different. What could be the possible hypothesis in this space? A1 or a2 is a hypothesis, a1bar or a2bar is a hypothesis, a1 or a3bar is a hypothesis, a2 and a3bar is a hypothesis and so on. So we will have a different set of hypothesis. So the attributes that we use and the way we represent a rule determines the hypothesis space. The hypothesis could also be describing other forms. 
For example, the hypothesis can be represented as a decision tree, the hypothesis can be represented as a neural network and so on.    (11:53)

  
Some more definitions to review:
A training set is a set of all training examples that are given to the learner. And on the basis of the training set the learner forms the hypothesis. Now after this hypothesis is formed it has to be evaluated. If you evaluate the hypothesis using the same examples on which you have trained you are likely not to get a good reflection because your hypothesis may try to fit the training data perfectly well but it may not work well for other the data that you have not seen.
So it is good if you can use a different set of examples for testing. So the set of examples that you use for testing is called the testing set. This example in the testing set should ideally be not known to the algorithm before it has learned the hypothesis. so the testing set is the set of all examples

refer slide time (13:03)
  
given to the learner after it has learned the hypothesis. This set is used to test the accuracy of the learned hypothesis over the unseen example. 
refer slide time (13:18)
 
Here the oval represents the set of all possible instances and instances are described in terms of the values of the attributes. 
A concept is a subset of the instance space. So, only a subset of instance space belongs to the concept class and the other instances do not belong to the concept class. 
So the concept can be represented as a set which is a sub set of the instance space. And our objective is to learn this concept. Now we find a hypothesis and this hypothesis in general may not be exactly the same as the concept. So when we have a hypothesis we see that there are some examples which are correctly classified by the hypothesis. For example, these examples are correctly
classified by the hypothesis because they are positive according to the concept and they are also positive according to the hypothesis. Whereas if you look at these examples, for these instances they are actually members of the concept path but they are not members of the hypothesis. So these are called false negative instances. Similarly these instances are classified as positive by the hypothesis but actually according to the concept they do not belong to the concept so they are called false positives. So this is the region of false negatives and this is the region of false positives and the middle region is labeled as positive by both the hypothesis as well as the concept and the rest of the instances are also labeled as negative by both the concept and the hypothesis. Therefore the false positives and false negatives are the two regions of error for the hypothesis. Now our objective as we said in inductive learning is given for a set of training examples we have to learn a description or we have to learn a hypothesis. And as we have noted that we cannot learn this deductively because we do not have access to information about all the instances we use the principle of inductive inference. 
And the inductive learning hypothesis states that 
refer slide time (16:26)
 
any hypothesis which is found to approximate the target function well over the training examples will also approximate the target function well over the unobserved examples. 
Now let us see the assumption behind this. If the training set and the test set belong to the same distribution, that is, if there is some similarity between them then by looking at the training set if we can identify a pattern we can infer inductively that such a pattern actually exists and it will be true of all instances. therefore this is the principle of inductive inference which helps us to make 
an inductive leap.
refer slide time (17:21)
 
Now you are given a set of training examples. On the basis of set of hypothesis you want to learn a hypothesis.  
There could be several competing hypothesis. Now what is the basis on which you decide which of the hypothesis to choose? The hypothesis you must choose is the one which has the minimum error and the highest accuracy. But you cannot measure the true accuracy of the hypothesis because you do not have access to all the training examples. Therefore you must use some sort of bias which is called inductive bias. There are many biases which have been used. For example, simplicity of hypothesis is a possible bias. Therefore a learning task is not always well formulated so (18:23)

 
there could be many functions that are consistent with all the training examples.
And the algorithm must have an inductive bias on the basis of which the algorithm will decide one of the hypotheses. 
There are several types of bias which could be used. There could be a bias while designing the hypothesis space. The set of hypothesis is defined by the hypothesis space and hypothesis space is defined by the data structure or representation that we used to describe the hypothesis. By restricting the language for the hypothesis space you have a restriction bias. Also, given a hypothesis space even in this hypothesis space there could be several competing hypothesis and the one which you choose among them would depend on your preference bias. For example you might say that if I have rule whose size is 1 then i will prefer that rule to a rule with size 3. So I might say I prefer a simpler hypothesis. So we might also have a preference path. There could be two types of biases. One is the restriction bias which restricts the hypothesis space and preference bias which selects among the possible hypothesis in the same hypothesis space. So, in hypothesis space restriction bias we restrain the language of the hypothesis space. 
refer slide time (20:03)
 
For example, we might consider that hypothesis of monomials. 
What is a monomial? Monomial is a conjunction of positive or negative literals. 
So monomials are a conjunction of literals. We might say that our hypothesis space must only be a monomial such as a1 and a2, a1 and a2bar and a3, a1bar and a4 and a3 and so on. These are examples of monomials. 
If we have a monomial we cannot have disjunctions. We cannot have hypothesis like a1 or a2, we cannot have hypothesis like a1 or a2 and a3. 
So, if we decide that our hypothesis space is the space of monomials we are ruling out other types of hypothesis. So what is the advantage of choosing a restricted hypothesis space? If we have a smaller hypothesis space the process of learning will be simpler. The size of the hypothesis space is smaller and the algorithm may be simpler. The disadvantage of choosing a small hypothesis space is that a small hypothesis space may not capture all  possible hypotheses so it may not be able to express all types of hypothesis. If you take an unrestricted hypothesis space we can represent all type of functions but then there will be a lot of functions which are consistent with the training examples and we have to give strong preference bias to choose one of them. And trying to search from a large hypothesis space we will not get a good algorithm to do that.
 (21:59)

    
 In preference bias we try to use a bias to decide which hypothesis is more useful. 
There are several types of preference bias that have been considered. One very popular type of preference bias is called the occam’s razor principle. 
 So occam’s razor is a very old principle and it actually means that things should not be multiplied unless it is necessary. So what it translates to is that we prefer simpler formula for our hypothesis. So if you have two competing formula and one is simpler we will prefer them. And there is some intuitive explanation of occam’s razor. There are fewer simpler hypothesis simpler patterns than more complex patterns and if a simpler pattern can fit our training data it has a better chance of fitting unseen data. And the second type of preference bias which is also very similar is the principle of minimum description length mdl which states that the best hypothesis is the one that minimizes the total length of the hypothesis and the description of the exceptions to the hypothesis. 
these are the some of biases that people have used. 
When we have a hypothesis space

refer slide time (23:38)

 
which consists of a set of hypothesis how do we search for the hypothesis that 
we want. The search could be carried out in many ways. We will just mention two simple possibilities.
One possibility is, you consider one hypothesis at a time which you call your current hypothesis. If the current hypothesis is not good enough you choose another hypothesis which is your next hypothesis.  
So this is the previous hypothesis and you try to go to a neighboring hypothesis   using some operators. So we could use certain operators and we can try to make the hypothesis general or more specific. Therefore our current best hypothesis search looks at one hypothesis at a time and if it goes to a point where there are no good alternatives then it might backtrack to the previous job.
Therefore several algorithms have been suggested to do current best hypothesis search. But the thing to note is that if the hypothesis space is very large this search will take a long time. The other type of search which has been advocated for theoretical learning algorithms is Least Commitment Search. 
Least Commitment Search considers multiple search at a time so it maintains a set of  possible of hypothesis and as it gets more training examples it tries to restrict the set of hypothesis or adds some more.


refer slide time (25:36)

 
So, one type of such algorithm uses the notion of what we call a version space. 
Suppose our original hypothesis space is H and H consists of n hypothesis H1, H2, Hn, some of these hypotheses is consistent with the example and some of them are not. If we remove these hypotheses that are not consistent with the examples we get the remaining hypothesis consistent with the current example. The hypothesis which is consistent with all current examples constitutes the version space. As we look at more examples the version spaces is further restricted. 
So our algorithm might maintain the current version of space and as it sees more examples the version space can get slowly restricted. This is called version space learning. 
refer slide time (26:42)
 
 Formally a hypothesis h is consistent with a set of training examples D if and only if h(x) and c(x) where c(x) or f(x) is the target concept that we trying to learn, 
c is the target concept. So, if a hypothesis h agrees with the target classification c for every training example that we have seen then that hypothesis is consistent with the training example. 
In other words h is said to be consistent with the training set d if for all instances x c(x) belonging to the training set D h(x) = c(x).
What is the version space? 
the version space is denoted by VSH,D so the version space with respect to the hypothesis space H and the training set D is the subset of hypothesis from the hypothesis space that are consistent with all the training examples. 
 
This is called the version space. 
refer slide time (28:04)

 
In other words version space VSH,D is a set of all hypothesis belonging to the hypothesis space such that it is consistent with the training example. 
With this definition we can outline a theoretically simple algorithm called a list then eliminate algorithm. In list then eliminate algorithm when we have not seen any examples when d is null we say that a version space is the entire hypothesis space. so we list all the hypothesis in the hypothesis space. Then as we take one example at a time we throw out all the hypotheses that are inconsistent in the training example and then we continue. Now this algorithm is very simple to describe but actually it is unrealistic because usually for any reasonable complex hypothesis the hypothesis space is really large and it is not practical to store all the hypothesis in the hypothesis space. 
Nevertheless the list then eliminate algorithm

refer slide time (29:20)
 
can be theoretically described as follows: We maintain a area of structure called the version space which is a list containing every hypothesis in the hypothesis space initially. 
Then we process each training example one at a time. We remove from the version space any hypothesis with the current training example. And at the end we output the list of the hypothesis is remaining in the version space. So this is the basic version space algorithm.

refer slide time (29:54)
 
Obviously this algorithm is not practical because the number of such hypothesis is very large.
 Now we will just briefly discuss a variation of the algorithm which instead of keeping the entire list of consistent hypothesis tries to represent them in a succinct manner. 
A version space can be represented by keeping a list of all consistent hypotheses. Additionally a version space can be represented by keeping the set of general hypothesis or a set of hypothesis. There are some hypotheses in the hypothesis space and there are inconsistent hypothesis. This inconsistent hypotheses is either more general than one of the hypothesis in the hypothesis space or more specific than one of the hypothesis in the hypothesis space. 
In many cases the version space can be succinctly represented by a set of most general hypothesis in the version space and by a set of more specific hypothesis in the version space. 
Any hypothesis will belong to the version space. The specific set is called the s set and the general set is called the g set. If a hypothesis is less general than one element of g set and most general element of s set then the hypothesis belongs to the version space. With this idea an algorithm called the version space algorithm or candidate elimination algorithm has been formed which maintains the g set and s set. 
refer slide time (31:52)
 
Now let us see what we mean by choosing the best hypothesis? We are given a set of training examples but our real goal in learning is to find a hypothesis f’ belonging in h that will correctly classify all examples. So the ideal thing would be to choose that hypothesis f’ that minimizes this probability that f’ misclassifies the concept. 

refer slide time (32:31)

 
So f is what we are trying to learn, f’ is what we have so we want f and f’ to be very close. But we cannot compute this probability because we do not have all the guesses with us. 

Now how do we measure the success of a learning algorithm?

refer slide time (32:53)

 
We have to measure how well the learned hypothesis predicts the classification of unseen examples. Now how do we therefore evaluate a classification? 
We have to measure the predictive performance of the hypothesis. if we learn on the basis of a training set and we test the performance on the same set we are cheating in some sense because our hypothesis may very well classify the training set but may not do so well on the test set because already we have seen the training set and we can be biased towards the training set. 
Therefore in order to make a better judgment of the accuracy of the classifier we must measure it using unseen example and this set of unseen examples is called the test set. 
And do the extent possible we must keep the training set and the test set disjoint. 
When we test the accuracy of the classifier


refer slide time (34:01)
 
we will get some sort of curve like this. So what we expect is as the size of the training set increases the accuracy of the classifier may increase. So we plot on the y axis the accuracy of the classifier and we measure the accuracy of classifier on the test set which is disjoint from the training set. And as we include more and more training examples we plot the accuracy of the classifier so we may typically get some curve like this and the accuracy may increase or decrease as the number of training set increases. So this sort of curve is called a learning curve. 
refer slide time (35:01)
 
So we want to fit the hypothesis to the training data. So we have some training examples, we have a space of possible hypothesis and we have an error function and we try to find a hypothesis which minimizes the error function. 
refer slide time (35:01)
 
There could be several error functions that we could use. For example our error function could be the number of examples which have been misclassified or our error function could be the sum of squared errors between f(x) and f’(x). If you test the accuracy on your training data 
refer slide time (35:51)
 
you will usually note that as you train more and more the accuracy on the training data increases. However if you try to go on increasing the accuracy on the set of training data what might happen is that the accuracy on unseen data increases up to a point and then reduces. 
So this is a common phenomenon you can observe that as your algorithm is tuned more and more the accuracy on the training set may go on increasing but after a point the accuracy on the test set might decrease. This is a typical behavior you might notice with many learning algorithms. And what is happening usually is that your algorithm is over fitting the data. It is fitting itself to all the idiosyncrasies in the training data which makes it incapable of generalizing over unseen data. 
So this phenomenon is called over fitting because more you train the better you fit the training data. Ultimately all the examples might be memorized. The data that you get may not be pure data there may be noise in the data. For example, suppose the color of apple is green it might appear as yellow. So, if you try to fit in the noise in the training data the algorithm will over fit the training data and work very well on the training data but it will not work well on the test data. 
This is an example to illustrate over fitting. 
refer slide time (37:51)
 
These black things here are the training examples and the curve you have got fits all the training examples. The green balls are the test data and this curve does not fit the test data very well, it has zero error on the training data but it has some error on the test data. 
For the same examples we have another curve
refer slide time (38:23)
 
which has from finite error on the training data but it fits the test data in a better fashion.
so how are you going to properly
refer slide time (38:40)

 
test the accuracy of your classifier. You are given a set of examples. Now what you should do is that instead of using all the examples for training you should keep aside some of the examples for testing. So you hold out some of the examples. You divide the total set of examples you have into two sets as the training set and the test set. On the basis of the training set you train your classifier and then you measure accuracy with the test set. So the hold out method advocates splitting the available data into two sets. The training set is used to train the classifier and after the classifier is trained testing set is used to estimate the error of the classifier.
refer slide time (39:35)
 
of course this hold out method requires that enough number of examples are available so that even after keeping aside some test set you have enough data remaining use for training. Usually for most algorithm the more data that you use for training the better you can expect the accuracy of the classifier will be. So this sort of hold out will only work well if there is sufficient amount of data. But some times what happens is that lot of data is not available. In certain fields it is not very easy to get data.
In those cases you have to use some intuition to optimally use the data that you have. So one of the techniques is called the k fold cross validation. The k fold cross validation is useful when you do not have a large amount of training data. 
So what you do is you partition the available data into k classes. 
For example, this is the case of four fold cross validation. 
We have divided the data into four classes and then we ran four experiments. 
In experiment one we keep aside the first class for testing and the remaining three we used for training. 
In the second experiment we keep the second data aside for testing and used the remaining three for training and so on. So we have four experiments and for each experiment we get accuracy and we get the estimate the accuracy of classifier an average of this accuracy. So this is called k fold cross validation. 
It could be used to conserve training data and make the best use of the training data.
And one extreme case of k fold cross validation is Leave One Out Cross Validation 
where if you have m data you divide the data into m sets of one example each and you run m experiments, in experiment i you leave aside one of the data i and use the rest for training and on the trained data you test one.
So this sort of approach is used when you do not have a large amount of examples available to you. 
Therefore in k fold cross validation    refer slide time (42:10)

 
the true error estimate is obtained as the average. 
You have k estimates of error and you take the average to obtain the average estimate of the error. 
refer slide time (42:10)
 
So, Leave One Out Cross Validation will often be referred to as LOOCV that stands for Leave One Out Cross Validation     
It is a case of k fold cross validation where k is chosen as the total number of examples. 
so if you have a data set of n examples n experiments are to be performed. For each of these experiments one example is kept aside for testing and n – 1 examples are used for training. 
refer slide time (42:10)
 

Here are some questions:
1) Suppose you have A2 class learning problem so the classes are plus and minus, the classes are referred to by q so q is the target concept that you are trying to learn. And you have got seven different examples and there are four attributes to this learning problem a1 one a2 two a3 three and a4 four and these attributes are Boolean attributes. And you have got these seven examples so you assume that there is a bias in favor of simpler hypothesis. 
So your objective would be to find a good hypothesis for this particular concept q. so the training data r first example is, true true false false it is negative, the second example is, a1 one is false a2 two true then false, true its class is negative. 
Third example: false true true true class positive. 
Example 4: true true true false class is positive. 
Example 5: false true true true class is positive. 
Example 6: false true false false class is negative. 
Example 7: true true false true class is negative. 
So, given these examples you try to find a simple hypothesis for q. and you can also try to find several hypotheses which are consistent with these training examples and find the simplest of them. Use your own measure of simplicity. 
2) Again you are given another training set. Now this training set is slightly more complicated. We have again four attributes a1 one a2 two a3 three a4 four and we have a target concept q. so again for these training example you must try to figure out different hypothesis and propose one of the hypothesis which is a good hypothesis which is consistent with all the training examples. Here again we have a ten training examples. Example 1: A1 is high A2 is red A3 is in A4 is NV and classification is positive.  

Example 2: A1 is high A2 is green A3 is out A4 is V and classification is positive.  
Third example: A1 low A2 green A3 in A4 is V and classification is negative. 
Example 4: A1 is high A2 is red A3 is in A4 is NV and Q is positive.   
Example 5: A1 is low A2 is green A3 is out A4 is NV Q is negative.  
Example 6: A1 is low A2 is red A3 is in A4 is V and Q is negative.  
Example 7: A1 is high A2 is red A3 is out A4 is NV and Q is negative.  
Example 8: A1 is low A2 is green A3 is in A4 is NV and Q is positive.  
Example 9: A1 is high A2 is red A3 is in A4 is V and Q is negative.  
Example 10: A1 is low A2 is blue A3 is in A4 is NV and Q is positive.  
Therefore your exercise would be given these training examples, find some competing hypothesis and choose one hypothesis. 





refer slide time (47:40)
 
Then the last question 3) you state weather true or false:
a) Over fitting is more likely to occur when the training set is large. 
B) A hypothesis is found that achieves 80% accuracy on the training set. The hypothesis is guaranteed to have a lower accuracy on any test set. State whether true or false, whether the accuracy on any test set will always be lower than the accuracy on the test set. 
    
Some questions from the previous lecture:
Consider the problem of trying to recognize handwritten digits. Formulate this as a concept learning problem. 
a) Clearly specify what can be the possible features.
b) How do you get the training set and the test set? 
c) How will you measure the performance of your learning algorithm? 
In order to recognize handwritten digit what is the input that you get? The input is an image. Suppose you write this digit 1 or write this digit 7 the input is an image containing this digit. And in this input usually when you are trying to recognize digits this image will be only in black and white in two colors, the background color and the pixel color of the digit. So, from this image your objective would be to find out whether this is 1 or 7 or 2 or 3 and so on. Therefore your raw input is an image like jpeg or bitmap etc. It is true from what you can get is the pixel value. Therefore you get the pixel value and you can do some sort of image processing so that you ultimately get the black and white image. 
So you binaries it and you get a black and white image where some pixels are black and some are white and from this image you want to know the identity of this digit. So the raw input is the set of pixels. But the number of pixels will depend on the size of the window. Suppose this window is 20 pixels by 20 pixels so you have 400 pixels where each pixel is either white or black, this is the raw image. 
Now you can try to get a hypothesis using these raw features. Therefore if you use these raw features your algorithm may not be able to get a good hypothesis unless all the possible ones are written exactly in the same way. Some people might write 1 like this, some people write 1 like this, some people write 1 like this some people write like this so all these have different pixel representations. Now if you want to do a very good job of recognizing these digits may be it will help if you can extract some more sophisticated features from this raw image that is available to you. For example, you could try to find whether there is a vertical line. 
You can try to look at the image and you can try to detect all vertical lines in the image or all possible lines in the image. You can try to get different transforms and you can find the moment of the image. So you can get set of more sophisticated features which can be used by a learning algorithm. So the raw input to this learning problem is a set of pixels whether it is white or black. 
Initially you put with an image which consists of colored and pixels after you do some preprocessing or binarisation you get some raw pixels and you can further make more pixels indicated by getting more sophisticated features. For example, the value of the first moment, the result of half transform, the different lines that are present in the image, you can use different masks. So those are the inputs to your learning problem. 
Now how would you get the training set and the test set? If handwritten digit recognition is the problem that you wish to address you should get several of your friends or to write down the different digits in their own handwriting. 
The style of writing the digits differs from person to person. Different digits may be expressed in different ways. So if you get different types of digits written by different people or friends then you can try to use these as your training set or the test set. or you could take certain places where you have access to handwritten digits, for example in the post office you could scan the pin code on the letters written by various and try to recognize those digits. So you collect a set of such samples.
And once you have collected sufficient number of samples you use some for training and the rest for testing. 
Now to measure the performance of learning algorithm you use a training set on the basis of which you train your classifier and evaluate the accuracy on an unknown set. 
refer slide time (55:07)
 
2) Consider the problem of trying to play a game of Ludo, formulate this as a learning problem. 
Clearly specify what your system will try to learn. 
How can you get the training examples for this system? 
Now, in order to learn to play a game of Ludo there are  the two things you could do. Either you could work it out in the supervised learning framework or you could work it out in the reinforcement learning framework. In supervised learning framework you might get a set database of boards, actually in Ludo you have at least two or more players and suppose there are two players player one and player two and there is a board and different pieces are in different positions and when it is your turn to play you roll a die and depending on the score of the die you have to play your move. So basically what you are trying to learn is, given the board and given the die what should be your next move is what you need to know. 
Now, in order to learn this if you have a set of training examples where for different board positions and for different rolls of die you have the recommended move to make given by an expert then you could try to learn the move from the board. 
In Ludo the number of possible board configurations is very large and for each configuration you have several possibilities of what the die throw turns out so the number of possible situation is very large and this is not a very easy learning problem unless you are able to generalize in some sense.
So using these raw features is not really very practical. 
Therefore you have to generalize the board in some manner. Now the second way by which you can learn is you can play automated games against each other by random moves and you can generate a huge number of database of such games and you can use reinforcement learning to learn. 
But even in the first case what you really need to do know is in order to make this learning program practical the raw board configuration is not very easy to deal with because there are so many raw board configurations. So, to handle games like Chess, Ludo, Backgammon where there are so many configurations what you try to do is you try to get certain features from analyzing the raw board. For example the features could be the number pieces of the opponent which are in his home base, number pieces of opponent which are in unprotected situations, whether your die roll is able to capture an opponent. Therefore like this you can set up some features. 
For example, if you have twenty or thirty features then for your current board position you can evaluate the values of these features and then from the examples you have you can abstract them in the features phase. For each instance you can find the values of these features and then you can see what the possible moves are and from this you can learn.  
Trancripted by p.sasikala
Lecture time (58:23)

ARTIFICIAL INTELLEGENCE
By
Prof.Sudeshna sarkar
Department of Computer science and  Engineering
IIT Kharagpur

Lecture – 34
Rule induction and decision trees – I (TIME 00:50)

Welcome, in the previous lectures we have looked at the general idea about what we mean by machine learning and we have also introduced you to the task of concept learning or classification. 
In today’s class we are going to study algorithms for learning decision trees which 
refer slide time (01:14)
 

can be used as classifiers for the concept learning task. In today’s lecture the objective is that the students  (01:27)

 
will become familiar with the definition of a decision tree as what a decision tree is and how a decision tree can be used as a classifier. 
The student will also learn an algorithm to construct a decision tree from a given set of training examples, induction of decision trees. We will also introduce the student to the concept of information gain and entropy and how these are used to construct a decision tree. 
Later we will review how learning algorithms are reviewed in terms of the training set error and test set error. We will also show how over-fitting occurs in decision trees and in a later class we will look at how over-fitting can be avoided in decision trees. 
So let us come to know what a decision tree is?
refer slide time (02:43)

  
A decision tree is a classifier in the form of a tree structure. A decision tree is a tree through which we make decision. So the decision tree has a number of nodes. The leaf nodes indicate the value of the class or the value of the target attribute. So you have a set of training examples, the training examples consist of number of features or attributes. 
One of the attributes is the target attribute which denotes the class that you are trying to learn. In the decision tree we will just look at examples of decision presently, the leaf nodes correspond to one of the classes the classifier is trying to learn. And the internal nodes are called decision nodes. The internal nodes usually specify some test to be carried out. And usually for most of the algorithms we will study the decision node usually tests the value of a single attribute. And corresponding to each value of the attribute you follow one of the branches that come out of this node. So let us now look at an example of 
 (04:01)

 
a decision tree. So this is a decision tree which has been constructed to decide whether for particular loan applicant whether the loan should be approved or rejected. 
So at the route the attribute employed is checked. So, employed in this case is a Boolean attribute. If employed is no if the person is not employed then the attribute credit score is checked. If the attribute credit score has value high then the loan is approved. If the attribute credit score has a value low then the loan is rejected. On the other hand if the person is employed then income is checked. 
If income is high the loan is approved if income is low the loan is rejected. So this is an example of a decision tree. As you can see there are four leaf nodes in this decision tree two of them correspond to approve and two of them correspond to reject.
There are three internal nodes particular decision tree corresponding to the attribute employed credit score and income. And from every internal node there are number of branches corresponding to the different values that this attribute can take. employed can take the value of yes or no, credit score can take the value of high or low, income can take the values of high or low. So this is an example of a decision tree. 
refer slide time (5:49)
 
this is an example of another decision tree and this decision tree has been constructed to find out whether a person likes a particular type of food. There are several attributes that we have used; some of this is taste, dry, hot and so on.
test can be sweet, sour or bitter; dry can be yes or no; hot can be yes or no. Now look at this decision tree, the other root we check the attribute taste. So, if taste is sweet then you check the attribute dry, if dry is yes then this food is liked 
the answer is yes, if dry is no answer is no, if taste is our then the attribute hot is checked, if hot is yes then the food is liked, if hot is no then you check the attribute dry, if dry is no the food is liked, if dry is yes the food is not liked, if taste is bitter the food is not liked. 
Therefore this is an example of another decision tree.
Here the depth of the different leaves vary. For example, the taste is bitter; this food is not liked irrespective of the other attributes. If the food is sour then if it is hot it is liked and if it is not hot but it is not dry also then it is liked otherwise it is not liked and so on. Now you note that each internal node in the decision tree tests an attribute, each leaf node assigns a classification and each branch corresponds to an attribute value. There are four internal nodes here 


refer slide time (08:05)
 
Taste, dry, hot and dry and each of them tests one attribute and each leaf node assigns a classification liked or not liked, each branch corresponds to an attribute value.
So let us come to the formal definition of a decision tree. A decision tree is a tree in which each non leaf node has associated with it an attribute or a feature.
refer slide time (08:33)


 
Each leaf node has associated with it a classification. In case the concept learning problem is a two class problem the classes can be plus or minus. In general there could be a finite number of classes. Each arc can be decision tree corresponds to one possible value of the attribute of its parent node. So, the decision tree is defined in terms of components, the internal nodes, the leaves and the branches. The leaves are the classes, the internal nodes are the decision nodes where an attribute value is checked, the branches coming out of the decision tree correspond to different values that the attribute can take. 
A decision tree can be used for classification. It is used to classify an example. So how do we classify      (09:36)

 

using a decision tree? You start from the root node, you follow the appropriate decision branches corresponding to the attribute values of the example that you have got. On reaching a leaf node the predicted class is update. 
For example let us look at this decision tree and let us look at this test data. We have test data whose taste is sour, dry is no, hot is no

refer slide time(10:13)
 
and color is red. Now for this test data let us apply this to the decision tree. So initially we come to the route; taste is sour, so we will follow this branch. So we first check the value of taste, taste is sour to follow this branch we check the value of hot, hot is no so we follow this branch check the value of dry, dry is no so we come to this leaf and this leaf is yes the food is liked. You note that in the training set there could be other attributes for example color which has been not been used in the decision tree because the decision tree finds the correct class without consulting this attribute tree either along this path or may be in the entire tree. For example, color is not used in this entire tree. Suppose there is another training set whose taste is sweet let us say dry is yes, hot is yes, color equal to yellow. Now to find the class of this training example using the decision tree one would follow this buffers test is checked and taste is sweet then dry is checked dry is yes so the food is liked. The attributes hot or color are not checked. 
Now with this introduction I will ask you to see what a decision tree can represent. What are the types of functions that can be represented by a decision tree? Let us assume for simplicity that all the attributes are Boolean attributes and let us also assume that the classification problem is a Boolean classification problem. 
In a decision tree you follow the different branches and there are some leaves at the bottom, now some of the leaves are classified as yes, some of the leaves are classified as no. So, if there are two yes leaf the concept is true if you either if you either reach this or that leaf. Therefore it is true if either of the leaves is reached.
Corresponding to each leaf you follow a path and you reach that leaf if the decision you make from the root to that leaf then all the decisions follow the particular branch. 
So a particular leaf can be represented as conjunction of values and if you have more than one leaf which amongst is positive you can treat them as a disjunction of those conjunctions. 

refer slide time(13:45)


  
So, you can easily see that a decision tree in general can represent a disjunction of conjunctions.
Any disjunction of conjunctions can be represented by a decision tree. 
As an exercise I will ask you to look at the following functions: 
For the first three assume that you have two attributes a and b. 
So you are required to represent the AND function, the OR function and the XOR functions for the attributes a  b. 
In other words you have to find a decision tree to represent a AND b, another decision tree to represent a OR b, another decision tree to represent a XOR b. 
For the fourth case I will ask you find a decision tree to represent the majority function given that you have five attributes a b c d and e. 
So you want to find the majority of these five attributes. 
Please try this and let us see whether you can come up with the decision tree.
Now let us go to the learning problem for the decision tree.
refer slide time(15:19)
 
so you have seen that given a decision tree you can use a decision tree so that given a training example you can find the class of the training example. Now given the training set how can you get a decision tree? In the learning problem you will be given the set training examples, by processing the training examples you will come up with the decision tree and you will use this decision tree to classify new training examples. 
So the technique that we will employ is a greedy technique to find a decision tree given the training data. Now the decision tree algorithms have been arrived at by different people in different fields because of lot of applications. The basic algorithm that we will talk about today is based on the ID3 algorithm which was developed by Ross Quinlan based on the CLS algorithm. 
refer slide time(16:29)
 

Now the algorithm that we will talk about is a greedy algorithm. You are given to training data and you want to come up with the decision tree. Now there could be different decision trees, a large number of decision trees that you could look for. So instead of searching the entire space of decision trees what we will do is use a simple greedy algorithm to induce decision tree. The algorithm is a top down greedy algorithm. 
It starts by deciding which node to make a root node.
Once the root node is fixed it tries to find the children of that node then fixes each of these nodes and tries to find out those nodes. 
So this algorithm is based on top down induction of decision trees. 
Now let us look at the basic steps in the algorithm. In the first step you try to find out the best decision attribute for the next node. So you have got with you a set of training examples let us say yes. Now based on yes you try to find that attribute A which is adjudged to be the best decision attribute for this node. 
Now you fix A to be the decision attribute of this node. 
So you make a the root of the decision tree of the current node. Now you find out that all possible values that a can take suppose a can take the values true and false for each value of A you create a new descendant.
Now these new descendant can be a leaf node or an internal node. Now, if all the training examples for which a is true if all these training examples belong to the same class that is all of them are positive or all of them are negative make this a leaf node and label it by the value of the target attribute. If you see that the examples which come to this branch are a mixture of positive and negative examples then you try to make another tree to fit here so you do this recursively..
So here you find the next attribute which is adjudged to be the best attribute for corresponding to the different values of the attribute you again have to construct a decision tree for its children.
So this is the sense of the decision tree learning algorithm. 
Step 1) Let a be the best decision attributer for the next node. Assign a as decision attribute for node. For each value of A create a new descendant. 
Initially you had the set of training examples S, you find that subset of 
S for which A is true they will come here you find that a subset of A for which A is false and they will come here. 
So you sort the training examples to the appropriate leaf node according to the attribute value of the branch. 
If all training examples are perfectly classified stop otherwise iterate this algorithm over the new nodes. 
So we will now describe the ID3 algorithm of Quinlan. This was one of the oldest algorithms devised by Quinlan. In the next class we will look at the newer versions of ID3 algorithm c 4.5, c 5.0 carts etc. 
So the ID3 algorithm has inputs, the set of attributes Atts, q is the target attribute and s is the set of training example. So you are given a set of training examples, you are given a set of non target attributes Atts and the target attribute is q. and the ID3 algorithm will take these inputs, construct a decision tree and return the decision tree. 
Now let us look at the details of this algorithm. 
  If S is empty

refer slide time(21:36)

 
that is there no examples left return a single node with value failure 
or return a single node with a default class. 
Step 2) If s is not empty but consists of records of the same class. 
So, if all the examples in S belong to the same class you make this node a leaf node with the value of that class and label it with the value of that class. 
Otherwise if Atts is empty that is there are no more attributes left then return a single node and label this node with the majority classification of the examples. So S is the set of examples that you have, you find a majority class that these example belong to and since there are no more attributes left to make a decision node you make a leaf there with the majority classification. 
Otherwise that is, if S is non empty but S is not homogeneous that is S consists of both positive as well as negative examples and Atts is not empty then what you do is you choose the best 



refer slide time(23:16)

 

attribute out of Atts. So Atts is the attribute you have left choose the best attribute out of Atts and let that be A and how do you choose the best attribute? You do it by processing the training set S. Remember, this is a greedy algorithm.
Now once you have chosen this decision attribute a you will try to construct a decision tree which will have A as its root. 
So you will construct a tree and initially let tree be the new decision tree whose root is A. Now for the attribute a you find all the value vj that a can take. 
Let Sj be the subset of S with A = vj. So you have all the training examples S you find all the values that this attribute can take and you make so many branches. In each branch you push those examples for which the attribute of A has the value of vj. 
So the training set that you have at the root will be partitioned into several subsets. 
So the subset sj will contain those training examples for which A = vj. Now at this branch we will make a subtree which you denote by sub t and you get it by recursively calling ID3 with the remaining attributes Atts – A. the target attribute Q and the subset of training examples are this node Sj. Why Atts – A? At the current node you have used up the attribute A so you have come to this branch for A = vj.
Now this attribute will no longer be available for further deciding the class. So you remove A from your attribute set and you call the decision tree algorithm recursively on the remaining examples Sj that come to this branch. Now you will add a branch to the tree with label vj and subtree sub t and you will return this tree. 
Therefore you start with a training examples S. 
You find that attribute A is the best attribute for this training example. 
Once you fix this attribute you look at different values this attribute can take. Suppose that you can take true and false. 
So what you will do is that corresponding to this you find out whether all the examples that come here so all the examples that come here let us say they are called S1 and S2. Now if S1 is only positive and only negative this will be a leaf node otherwise we select another attribute which is adjudged the best attribute with this node. And corresponding to that we will again sort the training example S1 into S1 1 and S1 2, S1 1 for D = true and S1 2 for D = false and then you will get this new node and corresponding to this new node you will construct a subtree and you will put this subtree on the left side here and corresponding to this you will construct another subtree which you will put here.
So this is how the decision tree learning proceeds. 
refer slide time(27:48)


 
refer slide time(27:56)
 
Thus this algorithm is a greedy algorithm. It does greedy selection. ID3 tries to find the attribute that best separates the training examples. You have some training examples and you try to find an attribute. Which attribute will you adjudge the best? The ID3 stops when you get to a leaf where all the examples have the same class. So, if S is a mixture of positive and negative examples you try to use an attribute which will ultimately put the positive and negative examples separately. So you try to select that attribute which can achieve a separation from positive and negative examples in the best possible manner. For this the algorithm uses a greedy search and the algorithm never backtracks. 
Now which attribute you want to select? There could be several strategies that you can employ. You can select the attribute completely randomly; you can use the heuristic list values. That is, you use the attribute with the smallest number of possible values. 
Another heuristic is you can choose max values. That is, choose the attributes which has the largest number of possible values. 
Or you can choose the heuristic max gain which corresponds to choosing the attribute with the largest expected information gain. 
So this max gain is the heuristic we will study and it has been found to be a very effective heuristic for learning decision trees. 
refer slide time(31:15)
 
What is the motivation behind this heuristic? In any inductive learning algorithm we use a bias which tells us whether to prefer one particular classifier over the other. The sort of bias that we use in this case is a preference for short classifier or a preference for short trees. So we will just recall a principle which has been widely known as the occam’s razor. 
Occam’s razor decrease that entities should not be multiplied unnecessarily. It means that the simplest hypothesis should be chosen over more complex ones if they have the same performance over the data. 
Suppose you have some training data and there are a number of decision trees possible which are all consist with the training data then you should select the one which is shortest which is simplest. in other words the smallest decision tree that correctly classifies all the training examples should be selected. Unfortunately trying to find the smallest decision tree that is consistent with examples is known to be an NP-hard problem. It is a very difficult problem and it is not easy to find the smallest decision tree. 
So what we do is we use a heuristic instead and this heuristic tries to find a smallest decision tree. And we try to use this idea use this heuristic in order to decide which attribute to select. So our goal is, we try to select the attribute that will result in the smallest expected size of the subtree rooted at the children.
refer slide time(32:00)

    
  So we have a choice of attributes and we try to select that attribute for which the subtrees rooted that the children will be small. 
And a heuristic that we will use is the information gain heuristic. 
The information gain heuristic is a good quantitative measure of the worth of an attribute. It measures how well a given attribute separates the training examples according to their target classification. Your objective is to achieve a separation from the positive and from the negative examples. So you try to select that attribute which achieves this separation maximally. So this is the heuristic that you use and information gain can do this quite effectively. Now let us look at
 (33:05)
 










some examples. Suppose you have some objects and you want to classify them as being a chair or not a chair. Suppose initially you have 50 positive examples and 50 negative examples and you have a choice of many attributes. Let us say two of the attributes are backrest and material. 
Suppose the attribute backrest has values yes and no. among objects of backrest yes there are 51 such objects 46 of them are chairs and 5 are non chairs. 
There are 49 objects with no backrest, 4 of them are chairs, 45 are non chairs. on the other hand if you look at the attribute material the value of the material can be wood or plastic. For material equal to wood there are 55 objects there are 35 positive and 20 negative and for material equal to plastic there are 45 objects in which 15 is positive and 30 negative. 
Now which of these attributes is better? Initially, the examples are a mixture; it is a uniform mixture of 50 positive and 50 negative examples. so each one being a chair or a non chair is equally likely. for backrest is equal to yes we see that a large majority are chairs or few small ones are non chairs. 
for backrest equal to no a large majority are non chairs and only a few are chairs. 
For material equal to wood the majority is chair 35 and 20 are non chairs but here the percentage of positive examples is not very high. 
Similarly here 15 + and 30 – more negative but this sort of distribution is less homogeneous than the distribution you get when you use backrest. 
So, if you really want to achieve a separation from the positive and negative examples may be this is likely to be a better option. 
A backrest is likely to be a better option to distinguish between the positive examples from negative examples. This however does not guarantee that inconjunction with another feature the material may prove to be a very good option. 
So we are really not looking ahead but only making a greedy choice based on the current information.
So we will use information gain to select the best attribute.
refer slide time(36:09)


Our goal is to construct a small decision tree that correctly classifies the training example. 
And we will look for the attribute for which we get examples with low information content. That is a more homogeneous example set. That is, most of the examples belong to the same class. Our heuristic is that if the examples we have mostly belong to the same class then the decision tree that will be needed to classify those examples would be smaller. So this is the idea based on which 	we use this sort of heuristic.  
refer slide time(37:05)


 
Now how will we determine the information gain on using a particular attribute? 
First of all we have to have a measure for the information content of the set of examples. 
We will presently see that we use the concept of entropy to find the information content or a measure of purity and a purity of the set of examples.
Now once we have the measure of information content we look at different situations that we will get by using different attributes at the route. So, when we use an attribute A at the root we split the examples into different classes corresponding to the different values of A.
Now we find the resulting sets and find the information content of each of the resulting sets. Take the weighted average and compare it with the original information content. 
If the information content becomes lower this is a reasonable attribute to take. 
So we find out that attribute for which the information content becomes the lowest after splitting on that attribute and we will select that attribute. 
So we are trying to determine the information gain on using a particular attribute and we will split the examples on the selected attributes possible values 
and we find the difference in the information content of a node and the information content after the node is split. 

refer slide time(38:58)
 
To measure the information content of a set of examples S we will use the concept of entropy. 
Suppose S is a sample of training examples that we have and let us say in this sample S p is the proportion of positive examples and n is the proportion of negative examples, so entropy measures the impurity of S, it is defined to be – p log b – n log n where p is the proportion of positive examples, n is the proportion of negative examples and the algorithm is base two. 
So this definition is if we have only two classes. In general if you have a number of classes then entropy of the set of examples is defined to be – sigma of pi log pi where pi denotes the fraction of examples which belong to class ci and we submit over all ci. Therefore this is the general definition of entropy when you have arbitrary number of classes. 
The entropy function can be plotted for the two class problem as shown here. This

refer slide time(40:38)
  
is an approximate shape of the entropy curve. We have plotted entropy on the y axis with the value of p on the x axis. We see that when p is 0 entropy is 0 when p is 1 entropy is 1 and the curve has a shape like this so we see that entropy is maximum when p is 1/2. When p is 1/2 entropy is 1 when p is 0 and 1 entropy is smallest. So the impurity of the data is maximum when p is 1/2 that is we have a exactly equal number of positive and negative examples then the impurity is maximum. When impurity is smallest then either p is 1 and all examples are positive and p is 0 where all examples are negative. If p is 1/2 we have a mixture so entropy is the measure of the impurity. 
In information theory entropy is actually            refer slide time(40:38)
 



interpreted as the expected number of bits to encode a class. Suppose you have two classes plus or minus and you get randomly drawn examples of S entropy denotes the expected number of bits to encode a class. Suppose all the class is positive then you require 0 bits to indicate whether the objective belongs to positive or negative class because you know all classes are positive. If all classes are negative also the same thing will happen. 
If classes can be either positive or negative with equal probability you can always use one bit to encode the classes as 0 or 1. 
Suppose positive classes are more frequent than negative classes what you can do is that you can choose an encoding so that positive class gives smaller number of bits than a negative class. So patterns involving positive classes will be more likely and to encode them you can use smaller number of bits.
So information theory we find optimal length code and it is found that if a message has probability p then you require a optimally – log p bits to encode that message. so the expected number of bits to encode a random member of S by information theory is given by – p log p  – n log n which is how entropy is defined. We will use this concept in deciding the information content of a set of examples that we get in a decision tree. 
So entropy characterizes the impurity of a collection. Just to give an example suppose S is a collection of 6 positive and 4 negative examples 
entropy of S can be computed as – 6/10 log2 (6/10) – 4/10 log2 4/10 which can be evaluated to be .97.   refer slide time(44:11)
 
refer slide time(44:18)
 
if the target attribute takes m different values that is there are m classes then the entropy of S with respect to the m wise classification is defined as – sigma pi log pi and this pi is the proportion of S belonging to class ci. Now how would we use information gain to select the best attribute. So we introduce the concept of remaindering. 
refer slide time(44:46)
 
Suppose we have a set of training examples S we can find its entropy. When we classify using A we split S into different subset. We define remainder A as the weighted sum of the information content of each of these subsets. So S is partitioned into subsets by possible values of the attribute, remainder A is the weighted sum of the entropy of the subsets. So remainder A can be taken to measure the total disorder or in-homogeneity of the children nodes when you classify using the attribute A. So the expected reduction in entropy caused by partitioning the examples using the attribute A is given by gain S, A.
refer slide time(45:36)

 
So the gain S, A entropy S – remainder A and remainder A is the weighted sum of entropy S, V. S, V is the subset of S for which the value is V and we sum it over all attribute values that A can take.
refer slide time(46:24)
 
 
So we define the best attribute to be the attribute for which gain S, A is maximum or equivalently for which remainder A is minimum. 
Now let us look at an example of decision tree. We have some training data; the training data has three attributes state, season and barometer 
refer slide time(46:44)
 
and the attribute which we are trying to learn is weather. State can be AK, HI, CA and season can be winter or summer, barometer can be down or up. And we are trying to predict whether the weather is snow or sun or rain. 
So the target attribute is weather and the other attributes are state, season and barometer.
refer slide time(47:10)


Now let us look at each of these attributes. The attribute state can have three values AK, HI and CA and in AK we find three examples 2 snow and 1 sun, in HI there are 3 sun, in CA there are 2 rain and 1 sun so entropy of AK is .92 of HI is 0 of CA is .92 so the average reduction in information content is given by .62 or average information content is given by .62. For the attribute season for the value winter there are 2 snow, 2 sun and 1 rain. 
Summer there are 3 sun and 1 rain, for CA there are 2 sun and 1 rain and the average entropy is 1.2. For barometer corresponding to down we have 1 snow 4 sun and corresponding to up we have 1 snow 1 sun 2 rain so the average entropy is 1.07. So we get the minimum entropy if we use state as the attribute 
and the maximum entropy if we use season as the attribute. So, 
refer slide time(48:41)

 
 you can use state to split the root. 
For state = AK we can split on season, For state = HI we get a leaf node, 
for state = CA we can split in barometer and we can finally get this decision tree, for state = AK we split on season,


refer slide time(48:51)

 
if it  is summer then sun, for winter it is snow, for state = HI it is always sun, for state = CA the barometer is down, for sun the barometer is up it is rain. 
So this is the decision tree that you get from the table by using the information gain heuristic the details you have to work out. 
 refer slide time(49:22)
 
Now we will briefly review how we will evaluate a decision tree. Once we have a constructed a decision tree we want to know how good the decision tree is. When you get a classifier you evaluate the classifier by computing its accuracy or equivalently by finding the error. 
Now how would you use the classifier? You would use the classifier to predict the classification of a new training example. 
So, given a distribution of possible examples we want a classifier for which the true error given the distribution of examples is smallest. But what you have with you is a set of training examples. You can evaluate the training error with respect to the training examples. 
This is what you can evaluate but you really want to find the true error. So, to review the true error of a hypothesis h with respect to target function f  
and a distribution D is the probability that h will misclassify an instance drawn randomly according to D. So the true error of the hypothesis h is the probability that h(x) does not agree with f(x). The sample error is what you get from the training example. 
The sample error of h with respect to target function f and data sample S is the proportion of examples that h misclassifies. 
 We do not have the true error with us but we will use a sample error as an indicator of the true error. But if we use the training error to learn our hypothesis and use the same training set to evaluate the hypothesis we are actually cheating because we have the scope to fit the classifier with the training data. 
In order to get a better idea of the true error what we do is we keep aside some of the data that we had  
refer slide time(52:01)
 
as the test set which we do not use to train the classifier. Typically this is the sort of behavior you get. If you are using the decision tree and as the decision tree size increases typically the accuracy on the training set increases. Initially you have a small accuracy but as you grow the decision tree the accuracy on the training set increases. 
However the accuracy on the test set initially increases after which it may go down. 
So this is a typical behavior that takes place and this is due to over-fitting. Later we will see how decision trees can be pruned to avoid over-fitting. 
But based on this curve this is the optimal tree size for which the test set accuracy is highest. So, one can think of stop growing the decision tree at this point and you can find it out you can keep aside test set and when you have the decision tree at various stages you check its accuracy on the test set and you select the tree for which the test set accuracy is highest so this is one method and the other method is to grow the entire decision tree and then prune the decision tree. 
refer slide time(53:48)
 
So accuracy on the training data increases as the decision tree grows and accuracy on the test data initially increases but then decreases. 
refer slide time (52:40)
 
Some practical applications of decision trees: 
Decision trees have been used for classifying patients by disease, 
They have been used for diagnosis for breast cancer. 
For a particular data set it was found that humans are correct, human doctors are correct 65% of the time and a decision tree classifier was found to have an accuracy of 75% higher than that of human experts.
Decision trees have been used for classifying equipment malfunctions, for classifying loan applications by level of risk. 
BP designed a decision tree for separating gas and oil for offshore oil platforms.
 refer slide time (53:40)
 
Another application was learning to fly a Cessna by observing pilots on a simulator. 
Data was generated by observing three skilled human pilots performing a fixed flight plan 30 times each and 90,000 training examples were obtained, 20 attributes, the class was the action taken like thrust, flap etc. The algorithm used was the learning algorithm c 4.5 and this tree was inserted into the flight simulators control loop and tested 



refer slide time (55:21)
 
the program learn to fly somewhat better than its teachers. The generalization process cleans up mistakes by humans. This was a quite successful exercise. refer slide time (55:40)
 
Some questions:
1) Give decision trees to represent the following Boolean functions:
a) NOT A and B or C and NOT D 
b) A AND B OR C 
3) Parity functions with four inputs A, B, C and D. 

refer slide time (56:16)

 
2) Given the following data set construct a decision tree show all the steps. 
refer slide time (56:23)


 
In this data set there are four attributes; district, house type, income, previous customer and the result is the outcome and this is what you are trying to learn. The outcome can be nothing or responded, district is suburban or rural or urban, 
house type is detached or semi detached or terrace, income is high or low, previous customer yes or no. So there are fourteen examples. 
Example 1) district is suburban, house type detached, income high, previous customer no, outcome nothing; 
2) District suburban, detached, high, yes, outcome nothing; 
3) Rural, detached, high, no, responded; 
4) Urban, semi detached, high, no, responded; 
5) Urban, semi detached, low, no, responded;
6) Urban, semi detached, low, yes, nothing; 
7) Rural, semi detached, low, yes, responded;
8) Suburban, terrace, high, no, nothing; 
9) Suburban, semi detached, low, no, responded;
10) Urban, terrace, low, no, responded;
11) Suburban, terrace, low, yes, responded;
 12) Rural, terrace, high, yes, responded; 
13) Rural, detached, low, no, responded; 
14) Urban, terrace, high, yes, nothing. 

Trancripted by p.sasikala
Lecture time (56:39)

ARTIFICIAL INTELLEGENCE
By
Prof.Sudeshna Sarkar
Department of Computer Science and Engineering
IIT Kharagpur

Lecture – 35
Rule induction and decision trees – II (TIME 00:50)

Welcome, today we start the second part of the lecture on decision trees.
In a last class we looked at the definition of the decision tree and we also looked at an algorithm ID3 which helps in constructing decision tree or inducing the decision tree given some training data. 
So today we will further consider the decision tree learning algorithm. The instructional objectives of 
refer slide time (1:21)
 
Today’s lectures are as follows. The student will learn how to evaluate the learning algorithm in terms of the error obtained on the training set and the error obtained on the test set. The student will learn how over-fitting takes place in decision trees and how over-fitting can be detected and avoided.
We will talk about different stopping criteria for decision tree building. 
We will look at2 different pruning techniques  
refer slide time (1:21)
 
for pruning of the decision tree. First we will look at reduce error pruning and then we will look at rule post pruning. 
We will discuss how a decision tree can be made to work with continuous valued attribute. We will also discuss how missing values are handled in training data where the data is incomplete so some attributes have missing values. 
Before we start let us review what a decision tree is and look at an example decision tree that we considered in the last class. If you look at this diagram
refer slide time (2:52)


 
every node in the decision tree the internal nodes in the decision tree test the value of an attribute. In this decision tree there are four internal nodes. This node looks at the attribute for test, this node for attribute of dry, this node attribute hot and this node attribute dry. 
Every leaf node corresponds to a classification. So this, this and this corresponds to yes values and this, this and this corresponds to no values. The branches that come out of an internal node correspond to the different values the attribute at its parents can take. Taste can take the values sweet, sour and bitter so we have three branches from test, hot can take values yes or no so we have two branches from hot. 
ID-3 is the simplest algorithm for learning decision trees. This algorithm was developed in 1989 by Ross Quinlan in the machine learning community and parallelly by Brian Minato in the field of statistics. So let us look back at ID3.          (4:32)

 
In ID3 we start with input, it is the set of training examples S. We have a set of attributes att, we have a training example S and Q is the attribute on which we are trying to classify the data. 
If the training example is empty ID3 does not proceed further. It just returns single node with a default plus or with the value failure. If S is not empty but 
all elements of S belongs to the same class, that is f is homogeneous we stop growing the tree and we label that leaf node with the class to which all the examples in which S belong to. Thirdly if atts is empty that is there are no more attribute left for testing then also we cannot grow the tree any further. In that case if S is not empty we label the leaf with the majority of the different elements of S belongs to otherwise we carry on this loop. 
So, when doeS1 stop growing the tree? We stop growing the tree when either S homogeneous or when atts is empty. We also stop if S is empty. Otherwise if these conditions are not satisfied we choose the best attribute A by looking at the examples S and the attributes left. And then we branch on the attribute and grow a decision tree recursively. There are several practical issues that come up in developing a decision tree some of which or not properly handled in ID3 algorithm.
Some of these issues are:
           refer slide time (7:11)

  
1) We have to choose an appropriate attribute selection measure. As we have seen there can be different techniques for selecting the attributes the one we have been talking about is using information gain and entropy. 
Secondly we have to decide how deep to grow the decision tree. The ID3algorithm that we discussed stops when either S is empty or atts is empty or when S is homogeneous. 
But we might decide to stop growing the earlier based on other criteria. 
ID3 will also fail if we reach a situation where atts is empty and S is not homogeneous. In such situation ID3 will fail and we have to take care of the situations. 
3) So far we have looked at how to handle attributes which have a finite number of values. Either they have Boolean values as true or false or they take nominal values, for example taste takes the values sweet, sour and bitter. But what about attributes which take real values? In most of the cases for many training sets the attributes will take real values e.g. the value of temperature, the value of height, value of income. They cannot be put in to a small number of classes. How do we handle continuous values? Decision tree algorithm can be modified to handle continuous values.
4) We want to handle training data with missing attribute values. 
Sometimes we get a data for which we do not get the values of all the features. 
Either they are corrupted by noise or the features could not be measured or they are unavailable. 
So we need to often deal with missing attribute values.
there are other issues which we will not talk about in this class. for example, sometimes attributes have differing causes. For example, in the decision tree once we have a decision tree, when we apply the decision tree we take the example and test with the root node of the decision tree then we follow the appropriate branch. 
Therefore trying to classify the example involves testing some of the attributes. 
Now it could be testing certain attributes are cheaper, testing certain attributes are more expensive. 
Consider a medical diagnosis system. 
Each attribute can correspond to some tests which have to be carried out. 
Some tests are cheaper, some tests are more expensive. 
So we would prefer to carry out tests which are cheaper. So, growing a decision tree and apart from looking at the simplicity or small decision trees it will also take into account the cost of the different tests. 
Improve computational efficiency:
These issues are dealt in the later versions of the decision tree algorithm. For example the C4.5 algorithm which was developed by Quinlan in 1993 and there are other versions of decision tree algorithm including C5.0 and then there is the cart algorithm. And then there are numerous other versions of decision tree algorithms which take care of these issues and grow beyond to the basic decision tree algorithm or ID3.
Before we discuss some of these issues let us discuss the issue of noise which we have not talked about so far. 
When you get the training data the data may not be pure there could be noise in the data. The noise can come in because of the error in obtaining the data or error in measuring the data or error in processing the data. 
So due to this error in measuring or obtaining the data   (11:55)


 
or data precision process can give us noisy data. For example, if the color is actually white it may appear to us as grey and so on 
Secondly, sometimes what happens is that we do not choose the feature set properly and our data set includes many features which are actually irrelevant to the classification task. 
The values of these features do not affect the actual classification. So such attributes actually introduce noise in the classification process. And because of the presence of irrelevant attributes or because of the error in measurement we may get2 examples which have exactly the same values of all the attributes but they belong to different classes. Certain classification problems by themselves are always noisy. If we are not able to look at all the features or if we skip some of the relevant features we might get2 data sets which agree with all the features that we have selected but have different classes. 
This can also happen if some data is noisy. So, in any case there is a situation where no matter what decision tree or what classifier we construct that classifier cannot achieve 100% accuracy on the training set because the training set containS2 examples which have identical values of the attributes for different classes. 
So any classifier that you construct will assign the same class will these two 
instances which will not be correct. 

refer slide time (14:07)


 
Let us review how we would estimate the accuracy of a classifier. 
As we have mentioned earlier we have some data d we divide d into two sets; the training set train and the test set test. 
We train the classifier using the training set and after we have learnt the classifier we apply it on the test set and because the classifier was trained as learned on the train set. It may be likely that the classifier is fit has been over-fitted to the training set. So to get better idea of the true accuracy of the classifier we should look at some unseen examples. And in order to get some examples we keep aside a portion of d to evaluate the classifier. 

Issue of over-fitting: 
Typically when we learn any classifier what happens is that as we increase the complexity,
 (15:44)

 
For example, in decision tree as we increase the decision tree size whereas we grow more nodes the training set accuracy increases. We start with a decision tree having a single node and we gradually add nodes. As we add nodes usually the training set accuracy goes up. 
However, if we test the accuracy on the test set we see that the test set accuracy initially goes up and then it can slowly come down. And this phenomenon is known as over-fitting. And we notice that based on the test set this is the point where the classifier has highest accuracy on the test set so this is the optimal tree size. 
Why does such over-fitting occur? Such over-fitting can occur because there could be some regularity which is discovered in the data.
As we have only a finite amount of data some regularity can be perceived in the data due to some random behavior of the irrelevant attributes. They can mislead us in making us think there is a pattern. And by taking advantage of this pattern our classifier can increase the accuracy of the training set but this pattern may be absent from the test set. Therefore the test set accuracy goes down. (17:16)
 
 When the attributes are selected initially near the root of the tree the better attributes gets selected immediately. So, the attributes which are highly predictive are already taken care of near the root of the decision tree. 
After we have looked at the more important attributes then we look for other attributes to grow the tree and then we try to include these attributes which have less productive power and many of them are actually noisy. 
Therefore the highly predictive attributes occur near the root of the decision tree and they are able to capture the more general patterns. The less predictive attributes added later and they mostly try to capture overfit the tree to statistical noise. 
Now how do we overcome the effect of over-fitting? There are two major approaches to take care of over-fitting. Firstly we can stop growing the decision tree before over-fitting 
refer slide time (17:16)
 
sort of kicks in and takes over the process. So we have to be careful to know how long to grow the tree. We should not grow the tree in all cases till completion as ID3 prescribes rather we might to decide stop growing the tree earlier. 
The second approach involves pruning. So, we grow the tree quite deep and then we eliminate some lower portions of the tree as a post processing step. 
Therefore stopping the growth of the tree while growing iS1 approach and first growing the tree and then pruning is the second approach. Therefore these are the two approaches used to stop the phenomenon of over-fitting. 
How over-fitting can be avoided? 
A tree or indeed any classifier is set to 
refer slide time (19:30)
 
overfit the data if we let this tree to grow deep enough so that it captures aberrations in the data. It tries to fit in the aberrations in the data to get perfect fit and this harms the predictive power on unseen examples. 
For example, suppose there is a feature called humidity and there is another feature called size, now we have two types of classes the blue class and the green class. So what happens is that in the decision tree the decision tree separates the feature space into different classes. In this class there are all blue except this green. 
Initially the decision tree gets this node consisting of all the six examples then it further divides the set to separate this lone green class. And it may be that this is over-fitting the data. Possibly this is just noise but the tree is grown deeper to capture this noise. This is another example of data that could be classified as noise. 
Now the point is that if we treat this whole thing as blue, this thing as green, this as green and this as white may be we will get a tree which has better generalization power which works better with unseen data. But trying to fit all the cases and zero error on the training set may give us a classifier which has lower accuracy on the test set. 
Formal definition of over-fitting: 
refer slide time (21:39)
 
We say a hypothesis h in H over-fits the data set. So H is the hypothesis space. 
We are trying to find the hypothesis ball h which belongs to the hypothesis space. We say the hypothesis h overfits the data set D. If there is another hypothesis h’ so h is the hypothesis that we have obtained and h’ is another hypothesis in the hypothesis space. 
If there exists an h’ which has worse classification accuracy on the data set but which has better actual classification accuracy, that is h has better accuracy than h’ on the current training set but h does not work very well on unseen data then we say that h has been over-fitted. 
We can see the phenomenon of over-fitting by inspecting the following curve. This curve is similar to the curve that we looked at earlier. As we have noted we plot accuracy along the y axis. So this is .5, this is .6, this .7, .8, .9 and 1.So these are the values of the accuracy. 
Now we see that the accuracy on the training set keeps going up as the size of the tree increases. But the accuracy on the test data initially goes up and then it goes down. So this illustrates the phenomenon of over-fitting. So beyond this point the decision tree has tried to overfit the data.
refer slide time (23:47)

 
What causes a hypothesis to overfit the data? As we have seen it may be due to random errors or noise or it can be due to coincidental patterns. So random errors or noise happens when examples have incorrect class label or incorrect attribute values due to error in measurement. 
Secondly there can be coincidental patterns. By chance the examples may seem to have to have a pattern due to the small size of the training sample. If you take a large training sample much patterns like to be present. But due to this phenomenon of over-fitting there can be strong performance degradation. So we have to deal with the effect of over-fitting for any learning algorithm that we inspect. 
 (25:00)

  
So, as we mentioned there are two ways of over-fitting; stopping and pruning. 
The simplest solution to over-fitting is stopping. We want to stop growing the tree before over-fitting takes over. When should we stop growing the tree? There could be several things that we can do. 
First of all we can grow the tree, we have to use ID3 we can keep growing the tree until we get a single data point .single data point obviously has a single class and we cannot proceed any further. But instead of doing that we can stop when the data contains examples which has identical values of atts. That is, we have some data which agree with the values of all the other attributes but they belong to different classes. In this case we do not get a homogeneous class and ID3 will fail but we have reached a lead and we cannot grow this tree further. In those situations what we do we is choose the majority class to label the node. So, in this node we have examples all of which belong to the same features but they have different class. We take the majority class and label it as the class of that node. Now, if you want to handle over-fitting we have to do something different. We have to stop growing the tree even earlier. In order to do that there are several techniques we can employ.   
refer slide time (26:40)
 
So, when we have a node we mention that we find the entropy of examples at this node and then for a particular attribute we use at the node we look at the children. So S is the data set here S has an entropy, entropy of S. Now due to this attribute A which takes values true and false we get the data sets S1 and S2 and S is the partitioned between S1 and S2. We find the entropy of S1, entropy of S2 and find the weighted average entropy of S1 and S2. 
 We ideally stop when the entropy of a data set is 0 that is they are all homogeneous. But instead of doing that we can stop when the entropy of the data set is low. So we can select a threshold which is likely more than 0 and we can stop growing the tree when the entropy is smaller than the threshold then we need not consider this division. 
Secondly, we can stop growing the tree when number of elements in the dataset is below a threshold. If our S contains only three or four data items we do not want to keep dividing the tree. So only if s is large then only we consider growing the tree so we stop when s is small. 
When s is small we stop, when entropy is low we stop and thirdly we also stop when the best next plate does not reduce the average entropy. So, this plate does not reduce the average entropy. Therefore we have reached a point where splitting does not help in the short run. 
So these are three techniques which can be used to stop growing the tree. Instead of stopping when entropy is zero that is the data set is fully homogeneous we stop when the data set is low entropy and secondly we stop when the size of the data set at that node is small, thirdly we stop when the best plate is not reduce the entropy. However, we can see that the third situation is not always wise. There are certain functions for which if we look at the immediate the gain in information that immediate gain is not there but there can be further gain as we go below the tree. For example, if we consider the XOR function, 
suppose you are trying to find a XOR b if you either r split an a or split b at the top level the entropy does not reduce the entropy remains the same. The entropy only reduces when after a if you test on b then entropy reduces or after b if you test a. after two levels the entropy reduces. So this is not a wise decision in many cases. 
refer slide time (30:27)
 
Pruning:
This is the second alternative. For pruning what we do is, when we prune a tree usually the error in the training set will go up. But why do we prune the tree? We prune the tree so that the true error should go down. So, how do we test the true error? We test error on the test set or call the validation set. So we keep aside a validation set we grow the tree on the training set and then prune it and then check it on the validation set. This validation set is called the holdout set. 
This validation set is slightly different from the test set. The test set is used to test the final tree after pruning. We have another set called holdout set of validation set which we use for validating the pruning. So we separate the training data into two parts. Most of the training data is used to build the tree and the holdout set or validation set is used to validate the accuracy. For pruning what we do is we grow tree as for as possible. Instead of stopping early we grow the tree quite deep. And then after going the tree we prune the tree until it keeps performing better on the held out data. 
refer slide time (30:27)
 

 So the idea behind pruning is that the portion of the tree that models general patterns should work well on the holdout set but the portion of the tree that fits noise should not work well on the holdout set. 
refer slide time (32:31)	
 
So this is the schematic. We have data set D we divide it into training and testing or the holdout and we will consider two different approaches to pruning; reduced error pruning and rule post pruning. First we will talk about reduced error pruning. In pruning what we 
refer slide time (32:55)
 
do usually is that we grow the tree to learn the training data and then after growing the tree we remove certain portions of the tree. So we prune certain portion of the tree and so we get this new pruned tree. This is the prune tree that we get. And the expectation is that the pruned tree will not overfit the data. So reduce the pruning we the techniques for pruning of decision tree. In reduced error pruning we first grow the tree and then we do the following: 
We consider all the internal nodes of the tree.
refer slide time (32:55)
 
For each node we check if removing it gives us any advantage in accuracy. So we consider all internal nodes in the tree. For each internal node we check if removing the node along with the subtree below the node and assigning the most common class to the node whether that improves accuracy on the validation set. So, we find out those intermediate nodes for which there is an improvement in accuracy and let n* be the node for which the increase in accuracy is maximum. We prune the tree below n* and then we go back to two until we get a situation when none of the nodes can be pruned to better accuracy. 
So this is the essence of the algorithm reduced error pruning. We consider all internal nodes. For each internal node we check if there is improvement accuracy by removing the tree below the node and by replacing it with the majority value. And for all such nodes for which there is improvement we choose the node with highest improvement which is n*. If no nodes show improvement we stop, if n* shows highest improvement that is the positive improvement then we prune below n* and we continue this algorithm. Therefore reduced error pruning can be illustrated by this diagram. 
Suppose this is the original tree it has seven nodes and three internal nodes. So what are the possible trees we can get after pruning? If we prune below this node we get this tree1.   
refer slide time (35:45)
 
If we prune below this node we get this tree2, if we prune below this node we get this tree t3. Now t4 involves pruning both below 1 as well as below 2. 
So we will consider this three trees t1, t2 and t3 as possible candidates. We find out whether for these trees the accuracy on the holdout set is better than the accuracy of the original tree. Among them we find the tree which has a highest accuracy. Suppose t2 has the highest accuracy which is better than the accuracy of t then we will choose this tree and then we will proceed. Suppose we prune below this node we get this as the prune tree so t’ is the prune tree. Now we will again consider (36:48)
 
pruning this tree further. We can prune below this node to get this tree t1, we can prune below this node to get this tree t2 and evaluate them with accuracy, take the one highest accuracy or if both of them have worst accuracy on the holdout set than t’ then we retain t’. 
This is how reduced error pruning proceeds. 
refer slide time (37:20)
 
So, this process continues until there is no improvement in the validation set. 
So this can be illustrated by this curve, this is the size of the tree and as we reduce the size of the tree we start with this tree where the accuracy is this as we prune that we see that initially the accuracy is going up and then the accuracy goes down so we stop pruning the tree here. 
Disadvantages of reduced error pruning: 
 (38:06)
 
Reduced error pruning is a very good technique but it cannot be used if the training data set is small. If the original data set is small if we keep aside something for the validation set the training dataset will be further reduced. So we have seen that cross validation can be used to take care of less amount of training data but in general reduced error in pruning data is not very good when we do not have enough data. Now we will discuss the second pruning method for pruning of decision trees which is called rule post pruning. 
refer slide time (38:06)
 
The basic idea behind rule post pruning is that after you construct the decision tree you dismantle the tree and write down a prevalent set of rules of the decision tree. A decision tree can be expressed as disjunction of all the paths along with a given class C. So we find that for all the leaves labeled by c we take the disjunction of all these paths. For each path the formula is the conjunction of all the attributes and the branch that was followed to get to this leaf. We can open out this tree in terms of each such path. So we can convert the decision tree into rules. 
We can have rules corresponding to the positive class as well as rules corresponding to the negative class and together we have a set of rules. After we get the set of rules we can prune the rules independently. So what we do is that we look at each rule and see if we can drop some conditions in that and achieve a higher accuracy on the validation set. Therefore we first unroll the tree and we write it in the terms of rules, we sort the rules according to the accuracy and then to use the rules first we will use the most accurate rules and keep using the rules until we get a rule match and then we get a classification. This is the idea of using a rule set on a decision tree. Then we can prune each of these trees. So, as an example let us look at this decision tree which has three internal nodes x1, x2 and x3 and three classes A which corresponds to 
refer slide time (40:57)
 
x1 = 0 and x2 = 0 or x1 = 1 and x3 = 0. 
There is the class B which corresponds to x1 = 0 and x2 = 0. 
There is a class C which corresponds to x1 = 1 and x3 = 1. So, corresponding to this decision tree we can write out the rules as NOTx1 and NOTx2 implies class A, also NOTx1 and x2 implies class B, x1 and NOTx3 is class A, x1 and x3 is class C. So we get this set of rules.
Now, at these four rules we try to see we can drop conditions from these rules and get a more accurate classifier. For example, we can drop the condition NOTx2 from this set so NOTx1 implies class A, we have not pruned NOTx1 and x2 so we prune this rule and we prune this rule so we drop the condition x1 from this rule and we drop this condition NOTx2 rule so we get this new set of rules which is smaller than the previous set of rules and we evaluate the accuracy of this old rule set on the holdout set. And if this rule set has higher accuracy we adopt the rule set. So we drop conditions to the rule so that the resulting rule set has higher accuracy on the holdout set. Can you spot the essential difference between reduced error pruning and rule possible pruning. In reduced error   pruning we only prune the bottom portion. When you prune a node we prune everything below that node. If you prune an intermediate node it can be shared by different paths. So when we prune the node we prune it along all the parts. When we write out a decision tree in the form of a number of rules and we look at each rule individually we can prune a variable. For example, we are pruning the variable x1 for this rule so we are dropping x1 from this rule.  
But we are not dropping x1 from the other rule. So we can independently drop conditions on different rules. And we can drop a condition at the top of a tree and not drop a condition at the bottom. In reduced error pruning when we drop a node we drop everything below that node..  
refer slide time (44:19)

 
What are the advantages of rule post pruning? The language is more expressive than a decision tree. 
When we get an arbitrary set of rules they cannot always be efficiently expressed using a decision tree. A decision tree can express a disjunction of rules. They can be expressed but not very efficiently. So when we get the set of rules we cannot reconstruct the decision tree back from the rules, we want to use these rules. Therefore this language is very expressive, rules are easy to interpret, pruning as we just noted is more flexible than we use rule pruning. And finally in practical application this method has been seen to have yielded high accuracy.  

refer slide time (45:16)

 
Now let us look back at the methods of validating the new tree that we get. So, the methods we have looked at so far are by using a validation set, the training set and validation set approach.
So the first method that we have been following involves dividing the dataset D into two sets train and test. We build a decision tree using train and we test the prune tree using test. 
The second method does not use a validation set. So this method 
 (46:06)
 
can be used even when you do not have enough data to keep aside a validation set. This second method uses a statistical test like the chi square test.  
The chi square test is an example, a statistical method people use than other tests. So, in this method we use all the data sets for training. 
And then we use a statistical test to decide whether we should expand the node or not. Suppose you have grown this tree and then you consider the tree statically in finding whether there is any benefit in expanding this node. 
refer slide time (47:04)
 
The third method is slightly different. It uses an encoding scheme to capture the size of the tree and the errors made by the tree. 
So we use something like the minimum description length principle. So the basic idea is that we use all the dataset D to construct the tree and we use the encoding scheme to know when stop growing the tree. Therefore this method is known as the mdl principle. In the mdl principle usually what we do is, we say that the best tree we want is the tree where the size of the tree in bits plus the size to represent the misclassified examples. So the sum of these two sizes should be minimum. Suppose you have a tree t which is larger in size suppose it contains 20 nodes and it misclassifies four examples four misclassifications. Suppose t’ has 6 nodes and it has six misclassifications so which one would you prefer? In order to compare these two schemes the mdl principle is used. You find out for any of these schemes the number of bits you need to represent both the tree as well as the misclassified examples. 
and of the different alternative you have you select the classifier for which this is minimum. So this is the mdl principle which is also inspired by the occam’s razor principle. 
Additional issues concerning decision trees:  
refer slide time (47:04)
 
So far we looked at attributes which have either Boolean values or fixed values. Now what to do when we have attributes that take continuous values. For example, suppose you take the attribute temperature. 
So temperature can take different values. Now, what we do in this case is that we try to discretize continuous attributes and in order to do that we select a split. Suppose we say temperature greater than 30, temperature less than equal to 30 so we split the values of temperature into two sets. Therefore in order to find out a good split what we do is we order all the values of temperature in the training set. we have ordered all the values and we find out for each value whether we have positive classes or negative classes. So we find that here there is a positive class, positive class and a positive class and then these are all negative examples, these are all positive examples, these are all negative examples. So these examples are negative whereas these examples are positive and these examples are positive. 
So, when we decide to discretize the attribute we select the possible points where we wish to discretize the values. Hence we only consider those cut points where there is a change of class. We choose the cut point that maximizes this information gain. 
To review what we do is, we order the data set in terms of temperature values. And for each instance we note the classification against the temperature value. Now, when we discretize the attribute we cut it on one point. At this point if we cut this we find the information gain with respect to this cut. So, temperature less than 99.0 will be on this side, t less than 99 is on this side and t greater than equal to 99 is on this side. Therefore this amounts to getting a Boolean feature and with respect to this Boolean feature we can find the information gain.
So we consider cutting at different points. We consider cutting at these points t less than 97.6, t greater than 99.6, t less than 100, t greater than 100. So we can consider cutting at each of these points for which the gain in information is maximum. 
Secondly, we wish to handle cases where attribute values are missing from the data set. 
For example, we may have a data set for which the value of mass is missing. 
 (52:49)
 
We have the other feature but the value of one of the features is missing, what we do in this case? There are several things we can do. 
refer slide time (53:06)
 
One possibility is to assign the most common value for that attribute in this node. Suppose Mars is missing we look at the other examples, we find out what is the most common value of Mars in the other examples we put that value in this example. This is one strategy. The second strategy is, we look at only those examples which have the same class as the current example. Suppose the example is n it has a class x we find all other examples which belong to class x. For those examples we find which value of Mars is most common we use that value of Mars in this example, this is the second strategy. 
The third possible strategy is; we assign a probability to each value of the attribute. Suppose Mars can take three different values m1, m2 and m3.
So, for each of these values of Mars we assign a probability and this probability is based on the frequency of those values you are going to take. 
Therefore each fraction is propagated down the tree. Suppose we find in the examples that Mars is the value of m1 with probability .7, m2 with probability .2, m3 with probability .1 we assign these values with this probability to the node n and we use that in the decision tree algorithm. This is another example of a continuous attribute. We have temperature, we have the class that we are trying to learn and it is the play tennis class,
refer slide time (55:03)
  
we have ordered temperature, sorted temperature and ascending order 15 to 18°, 19°, 20°, 24°, 27° and for each of these instances we have written down the class. We see that these are the points where the temperature value changes. So we will be taking one of these as the cut point. For example, if we take this as the cut point then we will test if temperature is less than 18.5°, if we cut at this point we will check if the temperature is less than 24.5° centigrade. For each of these we will find the information gain and select the one with the higher information gain. 
 (56:06)

Questions:
1) Given a training set which is noisy how will you decide when to stop growing the tree? 
2) We never test the same attribute twice along one path in a decision tree, explain why? 
3) If you have a continuous attribute do you think you can test the same attribute twice on the same path. 
4) Given training data is it always possible to obtain a tree which has zero error on the training set?

Trancripted by p.sasikala
Lecture time (1:25)

ARTIFICIAL INTELLEGENCE
By
Prof.Sudeshna Sarkar
Department of Computer Science and Engineering
IIT Kharagpur

Lecture – 36

Leavning Using neural Networks-I (TIME 00:50)

Welcome to the next lecture on Artificial Intelligence. Earlier we looked at the paradigm of machine learning and we discussed different learning problems. And specifically we talked about what concept learning is and we mentioned that different algorithms can be used for the concept learning task or for classification task. 
In the last two classes we discussed a particular type of learning algorithm which is induction using decision trees. We looked at algorithms to learn decision trees from data. 
Today we are going to look at another model using which we can do concept learning. The model that we will talk about is actually a general model and different variations of it can be used for many other tasks. What we are talking about is the connectionist paradigm which is used and which is more commonly known as neural networks. 
We will first briefly discuss what a neural network is and then we will see how some types of neural networks can be used for the concept learning task. The instructional objectives of today‘s lecture are as follows: (02:20)
 
The student will be introduced to the neural network framework. 
They will learn about a linear threshold unit or a perceptron which is a very simple model of neural network. The student will understand the type of functions that can be represented using a perceptron. 
The student will learn how a perceptron can be trained given some data
to fit the training data. 
refer slide time (03:00)
 
And then they will learn how radiant descent can be applied in order to learn a perceptron to learn a simple linear unit. They will see that the gradiant descent learning minimizes the sum of the squared error for a linear unit. 
And we will also introduce the students to a sigmoid unit which we will use as a basis for multilayered neural network. So, neural networks are actually different paradigm for computing just like the conventional computers that you have studied is based on the Von Neumann framework of computing. 
refer slide time (03:52)
 
The neural networks is based one a slightly network different idea of computing. This paradigm has been inspired biologically by trying to emulate the parallel architecture of human and animal brains. So, a neural network system comprises of many units which work in parallel. These units are individually simple units but together they can be varied to perform more complex tasks. So the main highlights of this framework are:  
refer slide time (04:39)
 
There are many simple processing elements, there is a high degree of interconnection between these processing units and there are simple messages which have been transferred to this interconnection and the interaction between these different elements is adaptive. This is the essence of the neural network paradigm.  Let us look at the history of neural computing. In 1943 McCulloch and Pitts did some work which is usually recognized to be the first work on neural networks. 
 (5:38)
 
In 1949 the first rule for learning neural network was devised.
In 1969 Minsky and Papert published a paper which highlighted the computational limitations or representational limitations of a perceptron unit. This leads to a stop of virtual decline in research world in artificial neural networks. 
Fortunately the 1980s saw a re-emergence of interest in artificial neural network and many researchers came up with more complex architectures in the form of 
multilayer networks that overcome the limitation of the perceptrons. And today research in the area of neural networks is quite active and they have been used or being used in a variety of application areas. 
refer slide time (6:50)


 
Now let us very briefly look at the difference in the architecture of conventional computing and neural computers. Conventional computers are known for the fast computations or fast arithmetic the number crunching that they can do and they are very good for delivering precise algorithmic solutions. Precise algorithmic solutions can be programmed in a conventional computing model. 
On the other hand the connection is paradigm and is very good for handling noisy data, for taking into account the massive amount of parallelism between different processors such computation are called fault tolerant and they are adaptive. 
The connectionist framework has been biologically inspired by the structure of animal or human brains. We all know that humans and animals are extremely intelligent and can do certain tasks remarkably well. And so far sophisticated computers have not been able to come up to the level of human or even animals in performing certain routine tasks. In the beginning lectures we looked at some such tasks as computer vision, image recognition, speech, language understanding and so on. (8:37)

 
Therefore many cognatic scientists and psychologists or any other people have extensively studied the brain in humans and some other animals. But it is really very complex like many other biological structures and functions. It is a very complex unit and it is very difficult to understand the brain completely. Actually the brain is composed of a number of units known as neurons. These neurons are also individually very complex units and together they constitute the brain. 
Therefore the basic structure of a neuron or nerve cell which occurs in the brain as well as in the other parts of the body
refer slide time (9:29)
 
is described in this diagram. The neuron has a nucleus and a cell body and the tail called axon. There are dendrites which are units that out from the cell body and at the end of the dendrites there is a synapse through which communication takes place between different neurons. 
Some salient features of the human brain: 
 (10:03)

  
1010 neurons, each neuron has several thousand connections, neuron switching times are of the order of 10–3 seconds that is 1 microsecond and neuron can perform hundreds of operations per second, also neurons die off frequently and some of them are never replaced. These are the salient features of human brain. And we all know the sort of tasks that human brain can perform. For example, we are very good at recognizing faces and typically recognizing faces takes us an average of .1 seconds. Now let us try to contrast this with the structure of what we have been able to achieve in modern computer. And we will notice that even though the number of gates or transistors in a modern day computer is large it is less than what you find in a human brain. 
However, the switching time of the devices that make up a computer are very fast and our processing elements can work faster than the processing units in the brain. 
Salient features:  
The brain is good for pattern recognition, for association like faces with names, complexity and they can tolerate noise to a large extent.  
A modern day computer or a machine on the other hand is good for calculation for precise answers and for being able to apply logic on rational process.  
 (12:09)
 
The Von Neumann architecture used in conventional computing usually comprises of a single processing unit which can perform tens of millions of operations per second and the arithmetic precision is very high.
On the other hand the human brain consists of many slow unreliable processors that act in parallel. And we know that the brain is still capable of doing many complex tasks. The idea between the connectionist paradigm is to try to emulate this structure of the brain and to devise a computing paradigm is wired by the which can perform complex tasks using this sort of brain work. 

Neural network comprises of many simple processing units. So we will first study a simple processing unit which we call perceptron and we will discuss what a perceptron can represent and how a perceptron can be learnt. So the unit that we are describing now is called a perceptron or a linear threshold unit. 
In a linear threshold unit the unit can take a number of inputs. And x1 x2 up to xn are the different inputs to this unit. These inputs are connected to this ltu by some arcs and there are ways associated with each of these arcs. For example, this arc has a weight of w1, this arc has a weight of w2 and this arc has a weight of wn. Now here what this unit does is it finds out the weighted sum of these inputs that is w1x1 + w2x2 up to wnxn and this input performs the function of thresholding. So this unit finds a weighted sum of inputs and then it applies the threshold. If the sum is greater than a threshold then the output of the perceptron is 1 otherwise the output of the perceptron is – 1. 

So this summation unit is followed by a thresholding unit. In this thresholding unit the threshold is applied. This threshold can have a special input x0 which = 1 and associated with it we can have a weight w0 for which we have to adjust to get the threshold. So this entire unit really computes this function sigma wi xi for i varying from 0 to n and if this summation is greater than 0 then the output is 1 otherwise the output is – 1. So this is the simple linear threshold unit. so this part is the linear part which computes the sum and this part applies a non linear step function on this sum and the output from this unit is either + 1 or – 1. So, typically in a neural network we have inputs. we can have more than one input. These inputs are fed to the network through what is called the input layer. And then each of these things is the output. The outputs are known as the output layer. To compute this output these inputs are fed to the outputs through these connections. In this case there are six inputs and all these six inputs are fed to each output unit and this output unit provides an unselect y1. This is the input x1, this is x2, this is x3, this x4, this is x5 and this is x6. And corresponding to these six inputs there are the six arcs with feed into the output unit and output of this unit is y1. There are weights on these arcs w1 w2 w3 and so on. Now these weights can be adjusted.
Hence in a network of perceptron the system works as follows: the weights on the arcs are changed. So what happens is that we want to adjust the weights on the perceptron so that the function we want to represent can be correctly represented. As in other concept learning tasks we start with the training data and then we find whether the function that we wish to compute at this  
refer slide time (18:50)

 
output is being indeed computed at these output units. So we try to find out the correct function is being computed. if the correct function is not being computed we will change the weights so that the correct function can be computed. So what we normally do in training and neural network is we start with an initially arbitrary value of the weights the way and we inspect the data we have got. If the data we got agrees perfectly with the current structure of the perceptron then we are very happy. But if does not then we change the values of the weight so that it represent the data in a better way. Hence this process is called perceptron learning or training the neural network. Suppose we are given some data or given one example data and according to the data there are some inputs. 

refer slide time (20:11)
 
So the inputs are x1, x2, xn and corresponding to this input the target output is t. so if our network correctly represents the function then the network for the input x1, x2, xn should give an output of t.
But if our network does not exactly represent the function it will give different output like o. Therefore if t and o are identical then we do not change the weights. But if t and o are different then we adjust the weights of the network so that o is closer to t. Therefore for a simple perceptron the general weight training rule is like this; after processing some training examples each weight is on arc is just as follows: wimew is old wi + delta wi. So to each weight we apply an incremental change of delta wi. 
Now what is delta wi? We take delta wi = eeta times t – o × xi. So t – o is the difference between the target and that is achieved out. If t = o then we do not change the weight delta wi =0. But if they are not same then we change delta wi proportional to t – o times the weight of the input. And along with it we use this parameter eeta which is called the learning rate. If the learning rate is high we make a bigger change and if eeta is small we make a smaller change. Suppose the target is 1 so for this input the output should have been 1 but o is 0. In that case what we are doing is if t is 1 and o is 0 t – o is positive and we are increasing delta wi. If we increase delta wi what will happen? Since our unit does a summation of wixi and if wi will increase and if xi is positive then the final output will increase, the target o was 0 so if the value of the input increases it will go towards 1 if xi is positive. 
If xi is negative then delta wi will be negative and then it will go towards the negative direction. Therefore what this training rule tries to do is, it tries to push o towards the t towards the target value. 
Hence the training rule is as follows: wi is changed by eeta t – 0 × xi where t is the target value, o is the perceptron output we have obtained and eeta is the small constant which is known as the learning rate.  
What does this training do? If t = o then do nothing and if t is not equal to o then the weights wi are changed 
refer slide time (24:14)

 
such that the output of the perceptrons for the new weights is closer to t. This algorithm converges to the correct classification under the following conditions: If the training data is linearly separable and if the value of the eeta is sufficiently small. So the basic idea is that some functions can be represented by a perceptron. And we will see that only linearly separable functions can be represented by perceptron.
if the function is not representable by the perceptron then this algorithm may not give you the correct result of the training. But if the function can be represented that is if it is linearly separable then if the learning rate we use is sufficiently small then this process converges. And with initial arbitrary value of the weights by applying the training rule over the data that we have seen we will be able to get finally a weight vector which is able to represent the correct function. The 

 
basic learning algorithm is as follows: The learning algorithm works in several phases, several ephocs.
in each epoch the entire training set is presented to the neural network. Based on the entire training set the error is computed and based on the error the weights are updated. And the error is the amount by which what is the output by the network differs from the target. 	 (26:15)

 
Suppose we have this perceptron structure where x and y are the two inputs and this is the output and the target is 0.0. 
Now we initialize the weights as random values. Suppose w0 = .3, w1 is .5 and w2 is – .4. Now suppose the function that we are trying to learn is the AND function for which the truth table is given here; if a is 0 and b is 0 then the output is 0 and if a is 0 and b is 1 then the output is 0 and only if a is 1 and b is 1 then the output is 1 and in all other cases it is 0. Now, given this network we have to see what the error of the network is for this which respect to this table. For example, if a is 0 and b is 0 then we can try to find out what would be the output. for a is 0 b is 0 what we get is, these links do not get contribute anything but only this link contributes so we get output as – .3 and the output should have been 0. For a is 0 b is 1 we get an output of – .7 and the output should have been 0, for a is 1 and b is 0 we get is .2 and the output should have been 1, for a is 1 b is 1 the output is – .1 the output should have been 0. 
So this is the current error we have on this perceptron and what we will do is adjust the weights using the weight training rule delta wi is eeta t – o xi is to adjust the weights. Now in order to analyze this training rule theoretically what we will look at a unit which is slightly different from the thresholding unit that we have looked at. So what we can do is, we will look at a simple linear unit which performs the summation but does not perform the threshold. 
So we use a simple linear unit and we want to find out the weight vector for which the linear unit is optimum. The linear unit also will give a value. The thresholding forces here forces the value to be – 1 or + 1. So what we will do instead is we will take a linear unit and we will require it to exactly be 0 under the zero conditions, 1 under condition one which we can use. 
For the linear unit the output is given by w0 + w1x1 up to wnxn. Our task is to learn the value of weights so that the error is minimized. 
refer slide time (30:03)
 

Definition of weight: This is the squared error which is a popular measure of error. It is the error with respect to the current value of the weight vector. The weight values are the ones which we are trying to adjust. For the given values of the weights we will have given value of the error and as we change the weights the error function will also change. So error is a function of the weights. And we use the following definition of error: ew is half of sum over all training examples (td – od)2. So td is the dth is the particular training example it is a target value and od is the output for that input by our network. So td – od is 0 means the network agrees with the example and if td – od is not 0 then it does not agree. So the summation of these (td – od)2 times half is the error function we will use and it is a very common function. 

refer slide time (32:27)

 
 Now what we will do is inspired by gradient ascent. Suppose we have a curve and this curve is a function of w, if we take a point on the curve and if we find the slope of that point the slope gives the direction in which the curve is moving. Now if we move towards the direction of the slope a little bit we will be going towards the top of the curve. And at the extremum points the slope is 0. If the slope dfdw is greater than 0 that means if we follow the curve we will move towards the peak of the curve. If dfdw = 0 we have reached the extremum position of the curve. 

Now our objective is, we are given an error function on the weights and we have to adjust the weight values in order to go to a region where the error is at minimum. therefore instead of doing gradient ascent we will try to do gradient descent on the error curve and our objective will be to start with the point and then move in the negative direction to the slope until we reach a point where the slope is equal to 0, that is we reach the minimum of the error function. The error depends on the weight vector.
Suppose we have a very simple system or simple learning problem where there are only two weights w0 and w1, now the error function can be given as 1/2sigma (tk – ok)2 and if we have a ok as a linear unit then we have a quadratic error function. 
And this quadratic function has the shape of a parabola. The characteristic of a parabola is that it has a unique minimum. When we start we have a particular set of weight values. Suppose this is our current weight value we want to find that combination of weights for which the error is minimum. So we want to reach this combination of weight values and we have started from this place in the error service. 
So what we will do is, we will compute the gradient at this point of the error surface and we will take a small step in the negative direction of the gradient so that we can descent the error surface and we will go on descending the error surface until we reach closer to the global minimum. And finally we might be able to reach the global minimum. For this surface because it is a parabolic surface there is only one minimum which we can reach. So the error on the weights is given by 1/2 sum over (tk – ok)2   
refer slide time (36:37)


 
And the gradient of this error function is, we can find the partial derivative of this error function with respect to each weight value. So grad ew can be written as del e del w0, del e del w1 and del e del wn. So we have the error slope along every dimension which gives us the gradient vector. Now our training rule will be to change each weight towards the negative direction of the gradient vector so that we can descent the error surface. Here w is the weight vector so delta w = – eeta grad ew. So grad ew is this quantity which is the partial derivative of the error vector. So, each individual wi is changed as follows: delta wi = – eeta del e del wi that is the partial derivative of e with respective of wi. 
So we have to find out what is delta wi for the linear unit that we are considering.

refer slide time (36:58)

 
Hence, in gradient descent we are trying to train the wis’ to minimize the squared error which is given by 1/2sigma over all training examples (td – od)2. As we saw in the last slide grad ew is a vector of the partial derivatives with respect to the weights and delta wi = – eeta del e/del wi. 
Now let us try to compute del e del wi for the error function that we have chosen. Now what is e? e is nothing but half sigma t d – od) 2  now what is od? 
od is nothing but sigma wixi. so we have to find the partial derivative with respect to wi from this function 1/2sigma (td – sigma wixi) 2 
now you know that the only portion of this function that depends on wi is this portion. the (td – sigma wixi) 2 when we are taking a particular wi only corresponding to this wi this term is the only important term.
so this derivative can be written as two times td – od × – xi so here the derivative is – xi change of variables, we have two times this function (td – sigma wixi) 2  which is 2 times td – od × – xi × 1/2 which gives us eeta sigma/d td – od × – xi the negative of this.
Therefore to sum it up we have the following algorithm for performing gradient descent on a linear unit given a set of training examples D and where we have n inputs. 
So the algorithm proceeds as follows:
refer slide time (39:26)

 
We initialize each wi to a small random value. Then we perform the following loop. Until the termination condition is met we do the following: initially we put delta wi as 0 that is we reset the value of delta wi. 
Then we pick up each training example from D. So each training example comprises of inputs x1, x2 and xn and the target value t. 
For each of these training examples what we do is we give this input to the linear unit and find the computed output o. 
the output o is simply sigmawixi. Now, for each linear weight what we do is modify delta wi as eeta times t – o times xi and then we say wi is wi + delta wi this is following a method that we found out that delta wi should be eeta × t – o times xi. We continue doing this until termination. In the termination condition we terminate when the error falls under a given threshold. 
So what we are doing is we are looking at all the training examples. For each training example what we are going to do is we are going to find out the error with respect to that training example. And we do it for all the training examples. So we basically finding the sum of this term over all the training examples that is why we have to do, so delta wi is previous delta wi + eeta times e – o times xi. And before we have started this we have put delta wi 0.
So what we are doing is basically we are finding out the sigma over all training examples eeta t – o xi. So we are doing  a version gradient descent which is called a batch gradient descent algorithm where we start with a particular vector we process all the training examples we find the cumulative error and accordingly we change the rates and again we continue this process and stop when the error is accepted.   
refer slide time (42:19)
 
This version of gradient descent this algorithm is called the batch mode gradient descent where the weights take care of the error over all the training examples. However this is expensive to apply after processing all the training example once we are able to adjust the weight values once and therefore this process can take a very long time. So what people often use instead is an incremental version of the gradient descent algorithm where for each example that is processed the weight vector is changed. And this incremental gradient descent is expected to be much faster than the batch mode gradient descent. 
In incremental gradient descent what we do is we modify the weight vector by processing a single training example. So we look at a single training example with respect to how we modify the weights, we look at one more training example then we modify the weights and so on. So this process is much faster because after every training example we modify the weights. Therefore the rate of change of weights is much faster. And the good news is that incremental gradient descent is an approximation to batch gradient descent. And if we keep the value of eeta small then the incremental gradient descent converges to some value which is very close to what you will get by using batch gradient descent and this is something we usually use in practice. 
Type of functions that can be represented by a linear unit or by a linear threshold unit: What we really want to know is what a decision boundary is. The perceptron will basically try to separate the positive examples from the negative examples. And in order to do that we use the geometric concept of decision boundary. So the decision boundary separates the positive from the negative example. For example, suppose we have these two features x1 and x2 and we have examples which belong to two different classes, these belong to a and these belong to b
refer slide time (42:19)

  
and we want to separate the a’s and b’s. And we can have the decision surface so that the a’s are on one side of the decision surface and the b’s are on the other side. Now in this two dimensional case we can draw a straight line separating the a’s from b’s. In all cases we will not be able to get a straight line separating the a’s and b’s. But in this case we can get and we say that this function is linearly separated. 
What if we have more than two input features? In that case if we have n input features and if we can have a n – 1 dimensional hyper plane that separates the two classes and then also we will say that the function is linearly separable. The perceptron learns a function of the form wix1, w2x2, 
refer slide time (46:29)

 
wnxn so this is a linear function which defines a hyper plane. So a perceptron can only successfully separate the classes between which there is a linear separator. Therefore this figure is an example of points which are linearly separable. In this slide the green points are separated from the yellow points by this line. Now here we have another example where we have two positive points here and two negative points here. And you can see that we cannot find a plane or find a straight line which separates the positive points from the negative points and such a straight line does not exist. So we say that this training set is not linearly separable. This training set is linearly separable because there exists this line that separates the positive points from the negative points. 
Now this is an example of a function. 
refer slide time (47:42)

 
which is not linearly separable. Now if you try to learn a function using a perceptron you can find that no straight line will be able to perfectly separate the positive points from the negative points that is the pink points from the green points. So you need a more complex decision surface to separate the positive points from the negative points. Perhaps this decision surface separates most of the pink points from the green point but there is something left over so you also need to incorporate this. 
So you need more complex decision surfaces to separate this problem. We need to look into neural paradigms where we can deal with such complex functions. Now, when we have threshold units we cannot do gradient descents because thresholding is a discontinuous function. This function the step function is not differentiable and we did the differentiation the derivative finding in order to find the slope. So, instead of dealing with threshold units we can look at linear units. 
refer slide time (49:12)

 
But if we use linear units a single linear unit can only give us linear decision surfaces not more complex decision surfaces. Now what we can do is we can think of having multilayer neural network so that we can cascade these different layer and we can build more complex networks. But if we make more complex networks out of linear units you can easily see that the result is again another linear network. So a linear function is not able to represent all types of functions. So what we want to do is we want to be able to represent more complex functions. And in order to do this we will be using multilayer neural networks. But we can not use linear units because that will not give us the power and we do not want to use thresholding units. Using thresholding units in multiple layers we will see that we can represent more complex functions but thresholding units are difficult to learn because we cannot use the trick of gradient descent. Therefore what we will do is we will try to look at other types of functions which are non linear functions but which are differentiable and using which we can represent more complex functions.                 refer slide time (51:01)

 
Multilayer neural networks: In a multilayer neural network this is the input layer and this is the output and between the input and the output we put other units where these units correspond to the hidden units. 
In a layered neural network we will put these units in layers such that the output from one layer feeds as the input of the next layer. This is an example of a layered network where the input goes through several layers and then goes through output. Now we will call this intermediate layers as the hidden layers because they are not part of what is the output. Now we will see that a layered network is able to represent the XOR function which you cannot represent by a perceptron.     refer slide time (51:01)

 
So, if you just think of a XOR function suppose you have two inputs x1 and x2 the XOR function has the following characteristic. if x1 is 1 or x2 is 0 then it is plus, if x2 is 0 and x1 is 1 then also it is plus, if both are 1 it is minus and if both are 0 then also it is minus and there is no line separating the pluses and the minuses. The XOR function can be represented by a two layered network as follows: The OR and the AND function can be represented by a simple perceptron. Find the neural networks that can represent the AND and OR function. Hence this layer can find the OR of x1 and x2 and can find the AND of x1 and x2 and this can compute the XOR function. So, the XOR function can be computed by a two layered network using three computing units. 
Thresholding units are not amenable to differentiation, linear units do not give us the power and we are going to look at non linear functions which are differentiable. And the sort of functions we will look at are the sigmoid function.
refer slide time (54:07)

  
The sigmoid function has a shape as follows: So sigmoid function is a S shaped function which is somewhat close to a thersholding function and it gives us a value between 0 and 1 and we will be using a network of sigmoidal units as our computing units. So, in a sigmoid function we have these inputs then there is first a summation of the inputs and then we apply the sigmoid function the sigmoid function of x is nothing but 1/1 + e–x. So, the output of this summation unit is net and the sigmoid function we find y = sigma of net that is y = 1/1 + e–net. This is also represented here as follows:   refer slide time (55:19)


 
We have this summation unit which is similar to


refer slide time (55:27)


 
linear threshold unit and then we apply the sigmoid function so output is sigma of net, net is sigma wixi and therefore output is 1/1+ e 2–sigmawixi. The sigmoid function as we said is differentiable, the shape is similar to the thresholding function a little bit and also sigmoid functions are easy to manipulate because these sigma xdx       (56:02)

 
gives us, so if you do the differentiation you will see d sigma dx is nothing but sigma x × 1 – sigma x. This makes it very easy to compute the differentiation of the sigmoid function. Later we will see how multilayered neural network using the sigmoid units can be learnt. 
So this is the example of sigmoid function. The sigmoid function is y = 1/1 + e–x. Now if we add another factor here k then if we vary, this function corresponds to k = 1 but if we vary the value of k we will get other functions as t per than the sigmoid function. 
So for k = 1 we get a function whose shape is this pink curve but for other values of k = 0..1 we get this curve. So, for different values of k we can find different shapes of the sigmoid curve.         (57:33)
 

Later on let us learn how to apply the sigmoid routine. 


Some questions:
In lecture 35 we asked you to represent these functions using decision tree. These things are very easy to represent using a decision tree. Suppose for example NOT a AND b OR c AND NOT d so to represented by decision tree what you can do is we can 
refer slide time (58:27)
 
initially test on b and if b is true then we test on a and if a is false then the answer is positive otherwise the answer is negative. If b is false then we test on c, if c is true then we test on d, if d is false it is positive and so on so we can build the decision tree corresponding to this function or this function. 
Take the same functions and try to represent them using neural network initially using a perceptron and if that fails try using a more complex network of two layers. 	
Parity function of A B C D: The decision tree will not be small it will be actually a full decision tree of four levels where we have to list out all the paths involving A B C D so you start with any of the attributes like a and the next level you check b and c then d you draw the full decision tree and you find out the leaves which will be positive and the leaves which will be negative.                  refer slide time (1:27)

 
So this decision tree going to be quite large. 
Questions on lecture 36:
The first question was, given a training set how will you decide when to stop growing the tree. 


 

ARTIFICIAL INTELLIGENCE
By
Prof. Sudeshna Sarkar
Transcript by sasikala .p
Department of Computer Engineering
			   IIT Kharagpur
Lecture – 37
			  Learning Using Neural Networks - II (time 0:46)


	Welcome, today we have a second lecture on neural networks.
 In the last class we had an introduction to neural network and we discussed linear threshold units or perceptrons and we looked at how to train neural networks using linear threshold units.  (refer slide time 1:13)

 

(refer slide time 1:16)

 

The objectives of today’s lecture are as follows:
 The student will learn what the limitations of single layer perceptron networks are.
We will introduce multilayer networks. The student will become familiar with a sigmoid unit and the properties of neural network unit which uses a sigmoid function. 
The student will learn how gradient descent works with sigmoid units. ( refer slide time 1:52)

 

Multilayered neural network with sigmoid units will be introduced. The back propagation algorithm will be described which is used for training multilayered neural network. 
The student will also be exposed to several issues in designing a neural network.
For example, selecting the topology or architecture of neural network and tuning the various parameters so that the network is able to work on a given problem. (refer slide time 3:07)

 

A neural network is a system which has been inspired biologically.  
We have a network of simple processing elements which are connected to each other via weighted links. The inputs are fed to the input units and as a result of the computation done in these units the outputs are produced.  
Some uses or applications of artificial neural network ANN: 
ANNs have been used for recognizing hand written letters, for predicting on-line the quality of welding spots, for identifying relevant documents within a corpus that is a large number of documents, the visualization of a high dimensional space (refer slide time 3:52)

 

and tracking on-line the position of robot arms. 
There are many uses that neural networks have been put into. These are a few of the utilities of neural network. 
When you are doing machine learning usually you will be given some problem. And you will have to find out what learning method is appropriate for tackling the problem. 
We have looked at a few learning methods namely the decision trees as well as now we are looking at neural networks. There are many other learning methods which have been used. 
Normally when you have a real life problem which is posed to you then you have to decide which learning method to apply. 
You have to guess which learning method will be most appropriate for a given type of problem. 
Usually what are the types of problems for which the ANN can be considered an appropriate to reply? 
Usually the problem is when we have got a lot of training data but we do not have a very good model of the data. Those are the problems for which we can apply artificial neural network. 
So problems that we cannot mathematically analyze or find a mathematical model to model the problems but we have a enough data then using neural networks is a good idea to model such problems. 
Neural networks also achieve nonlinear multidimensional input output mapping where the linear modeling does not work.  (refer slide tie 6:22)

 

And also, if we are trying to model a problem with neural network we have to spend some time in finding appropriate network and training the neural network. 
So, even after we have decided to use a neural network it will take some time to tune the neural network topology as well as parameters as well as the training of the networks. So, some time will be spent doing that. Neural networks are also able to handle noisy training data very well. 
Decision trees are very good for data which is not always noisy, a lot of the data is nominal and when we require learn a function which humans can interpret and understand or rules that we can understand. 
In neural network when we get a hypothesis the hypothesis is represented by a set of weight values. 
So the rules are not symbolically or understandably present to the human user. 
This is the reason why neural network methods or often referred to as black box methods because we cannot interpret easily the rules that neural networks learn. (refer slide time 7:45)

 

We looked at linear threshold units where we have a sigmoid unit which does a weighted summation of the inputs followed by application of a thresholding function or a step function. (refer slide time 8:05)

 

We also looked at, if we have a function which is continuous and differentiable how we can use Gradient descent to find out the set of weights for which the function has minimum error. 
So, if we have a thersholding unit which is a non differentiable function we will not be able to Gradient descent. But if we have simply the linear function, 
suppose we just have this summation unit and no thersholding unit in that case we can do Gradient descent on the weight surface and try to find the set of weights for which the error is minimum. (refer slide time 9:00)

 

Linear units or even linear thersholding units are able to classify correctly those instances which are linearly separable.
For example, here we have a set of points A and B. the A are the positive examples, B are the negative examples and we can find a line which separates the A points from the B points. 
Such problems can be tackled effectively by linear units or linear thresholding units.  
(refer slide time 9:48)

 

However, there are problems where the decision boundary is not linear where the linear decision boundary does not exist.
In that case we will not be able to find a linear decision surface. Suppose we try to find a linear decision surface we might be able to find some surface and we will notice that there are some errors like we are trying to separate the green balls from the pink balls. On the right hand side there are some green balls. 
So we have not been able to achieve correct full separation of the positive points from the negative points. 
So what sort of decisions surface would we need in this case? Suppose we had a decision surface which looked like this suppose we had this decision surface it can account for most of the pink points. It can separate most of the pink points from the green points even though there is one left over. 
So we can have the union of these two decision surfaces or we can only take this decision surface and ignore this smaller decision surface and treat it as noise. 
But in order to represent this decision surface a linear thresholding unit is not good enough. (refer slide time 11:21)

 

Decision regions: when we have a decision region where all the positive points are have this sort of shape that is all the positive points can be separated by a single line in those cases a linear perceptron does work. 
For problems like the exclusive OR we cannot have a good separation by a linear thersholding unit.  
There are problems when regions are meshed. And for such problems also we cannot use linear thresholding units. This is another example for problem. There we have two regions and with a hole in the region. 
So, only this portion is positive and the rest of the region the green region is all negative. 
So such problems cannot be separated by a linear unit and then problems where there is a positive region separated by negative region they also cannot be handled effectively by a linear thersholding unit. (refer slide time 12:42)

 

The Boolean XOR function is one function which cannot be represented by a single linear unit. 
However if we arrange the linear thersholding units in two layers, in this unit 
We compute x1 and x2 and here we can compute a XOR by setting these weights. 
So using these two layers of units we can compute the XOR function. 
So if we add a layer to the network that gives us computing power and which lets us go beyond linear decision surfaces. 
Multi-layer feed forward network: (refer slide time 13:59)

 

In multi layer feed forward network we have an input layer where the inputs are feed in, we have an output layer where the outputs go and within we have hidden layers. These are called the hidden layers which are not visible at the output and where the inputs also do not fit in. 
The hidden layers are the ones where this no input output but they help in computing the function at the output. (refer slide time 14:40)

 

So, if we had a hidden layer what can we represent? If we had one hidden layer that is if we have a two layered neural network such a network can represent any Boolean function. Any Boolean function can be represented by a two layered network. 
Any continuous differentiable function can be represented by a three layered network. That is the network with one output unit and two hidden units. 
Geometrically if we have one hidden layer that is a two layered neural network having one hidden layer we are able to separate positive regions that come within a convex hull. 
Suppose the green region is the negative region the pink region is the positive region and the positive region can be enclosed by a convex hull such a separation can be learnt if we have one hidden layer. if we use a two layered neural network comprising a perceptron they can model such learning problems. 
However, perceptrons are not easy to learn using gradient descent. When we use a thersholding function because the thersholding function is non differentiable we cannot use gradient descent if you are using perceptrons. 

Today we will look at the sigmoid function which is differentiable and using which we can learn functions that can be represented by convex hulls. 


If we have a two layered network then we can learn a collection of convex hulls. 
For example, suppose we have a problem where let us say the positive region is mapped by a number of convex hulls, so this is the positive region given in pink and the rest of the region is negative region given in green. 
Therefore such a decision surface can be learnt by a neural network having two hidden units. In the first hidden unit you can learn the individual convex hulls (refer slide time 17:41)

 

and in the second unit we can combine them. Therefore when we use a two layered neural network most problems can be represented. In theory if we add more than two hidden layers it does not gave us any representational power. 
But merely the fact that the two layered network or a three layered network has a certain expressivity it does not mean that the learning problem is simple. 
Given a learning problem one has to find out how many layers are needed for solving the problem and for each layer how many hidden units are needed, after that the training has to take place. Therefore when we go for multilayer neural network the training does not guarantee that we are able to learn the optimum network. So, the problem remains hard but a lot of problems have been solved successfully by neural networks. We discussed linear threshold unit and said that they can represent many types of functions but they are not
(refer slide time 19:11)

 
trainable by gradient descend. We looked at linear units we charge differentiable. Unfortunately when we want to go for multilayer neural networks and if we take several linear units together we do not add to representational power.




(refer slide time 19:51)

 
So, if we add several layers of linear units what we get is in effect another linear unit. Therefore it does not have the power to represent all types of decision surfaces. 
So, the type of functions we can represent by multilayer neural networks in a linear unit is [……] 19:55. So there are other activation functions 
(refer slide time 20:51)

 
that we can consider. We do not want to use linear threshold unit because it is not differentiable so we will try to use functions which are continuous and differentiable and at the same time which give us representational power. 
Hence the two such functions are; the sigmoidal function represented as y = 1/1 + e–h/p and the radial function or the Gaussian function which is given by y = 1/2/sigma e–h square/2sigma2.

So the sigmoidal function and the radial function are examples of two functions which people have used in multi layer neural network. 
Sigmoidal function:
The sigmoidal function has a shape which looks like this: 
(refer slide time 21:03)

 
So, as we can se it is an S shaped function but it very closely resembles the step function with the exception that it is continuous and differentiable. 
This is very similar to the state function. The Gaussian function on the other hand gives us a bell shaped curve which is also used in representing neural network. So this is the sigmoidal function again; y = 1/1 + e–x. 
(refer slide time 21:48)

 
The sigma x is the sigmoidal function and if we differentiate sigma x with respect to x we get, you can do the differentiation, it is an algebraic manipulation and you will get that ddx(sigma x) which is nothing but sigma x × 1 – sigma x. 
(refer slide time 22:15)

 
The mathematics becomes much simpler when we deal with sigmoid function because not only it is differentiable but we can express it in the form of a function itself. So the sort of unit we use for a neural network is called the sigmoid unit as follows: There is a summation unit followed by the application of the sigmoid unit. The summation unit computes sigma wixi and the sigmoid unit computes sigma of this quantity. Hence if net is sigma wixi the output is sigma of net which is 1/1 + e–net that is 1/1 + e–sigmawixi. 
Now a more general form of sigmoid function is this; 
(refer slide time 23:28)

 
1/1 + e–net – theta/tau. 
here theta is the threshold. If theta = 0 the S shaped function we get is placed around origin. If we put theta as some other value we will be able to give a shift to this function. And tau 1/1 + e–net/tau where tau is a sort of stiffness cost and varying the value of tau we can vary the slope of function that we get. For example, if tau = 1 this pink curve shows us the resulting sigmoid function. If tau = 0.1 then we get this white curve which is steeper than the curve with = 1. So, if I make tau less than 1 we get a function which more closely resembles the step function. So tau controls the slope of the sigmoid function.
(refer slide time 24:42)
  
Smaller the value of tau higher the slope, theta controls the horizontal offset of the function in a way is similar to threshold neurons. 
Sigmoidal neurons can accept any vector 
(refer slide time 24:42)
  
of real numbers as input and they output a real number between 0 and 1. 
A network of sigmoidal units with m input neurons and n output neurons realizes a function that maps rm-0 1n.
(refer slide time 25:21)
 
Now let us see how we can train using gradient descent. We have seen that d sigma x dx = sigma x × 1 – sigma x. 
Now we will try to derive the gradient descent rule when we have a single layer of sigmoidal units. Now what we will try to do is we will define the error function and try to find out the partial derivative of the error function with respect to each value of wi and we will try to compute those values. And we will see that value = –sigma over all training instances d td – od × od × 1 – od × xi. 
Here td is the target value of the dth training example and od is the actual output that we get using the neural network and xi is the ith input to which the corresponding weight is wi.
 Now here is the derivation for this:


(refer slide time 26:43)

 
del e/del wi is the partial derivative of the error function with respect to wi is del del wi and e is written as 1/2 (td – od)2 sigma summation(td – od)2. So this is the half of the sum of the square error this is the definition of error function.  
Now, if we differentiate it with respect to wI, first of all we can do some arithmetic manipulation we can bring half outside the sigma and we get 1/2sigma del del wi (td – od)2.
Then differentiating this we get two into td – od times del del wi td – od by change of a variable we get this. Then what is del del wi(td – od)? It is – del od/ delwi because td does not depend on the wi. But od depends on wi because od is obtained by doing this summation and then the sigmoid. So we have sigma summation of od td – od × del od/del wi. Now del od/del wi we can write using the chain rule as del od/del net d times del net d/del wi. And we can simplify this further; del od/del net d.





(refer slide time 28:38)


  
What is od? od is nothing but sigma net d. So del(sigma net d/del net d) is nothing but od times 1 – od because this is a sigmoid unit. And what is del net d/del wi? Net d is nothing but the dot product of w and x sigma wixi and net d is w. xd del wi.
 Now you see that in the w. xd we have w0x0, w1x1, w2x2 but now none of these terms depend on wi except the term from wixi. 
So this is nothing but xi for the dth training example. 
Therefore ultimately what we get is that the partial derivatives of the error function with respect to the weight wi is – of sigma over all training examples td – od × od × 1 – od × xi d. 

So this is the slope with respect to a particular weight wi. We can find with partial derivative with respect to all the weights and we can find the components in all directions of this slope so we can compute the slope. 
Once we compute the slope in gradient descent what we do is we find the direction opposite the slope. We want to climb down so we find the negative of this slope and we take a step in that direction. 
Now let us see how to train the weights of network. The basic idea is that we will use continuous differentiable activation function which is represented by a sigmoid unit. 


(refer slide time 30:39)


 
We will use the idea of gradient descent on the error surface and we will try to extend this to multiple layers. 
(refer slide time 30:49)
 
So this is a schematic diagram of multilayer network with an output layer, input layer and hidden layer. Here also we have two hidden layers.


(refer slide time 31:05)
 
We have been able to derive the gradient descent rule in one sigmoid function and we have seen that del e/del wi
(refer slide time 31:15)

  
is – of sigma/d td – od × od × 1 – od × xi .
Now, what if we have multiple layers of sigmoid units? If we have multiple layers we will use a technique which we call back propagation. 
Now, we will give the back propagation algorithm but before we do that we will try to give you simple idea of what is back propagation and how it works. 
So the basic idea is that, suppose you have some input units here and you have some outputs here and then you have some hidden layers here. Now, let us look at the output unit. 
When do you change the weights on the different arcs that lead to the output? We only change the weights if the output we get from the network does not agree with the target output. 
So, at the output unit we are able to recognize if there is some error. 
Error is recognizable at the output unit and we can try to change the weights of the different arcs that lead to the output unit. We change the weights so that this difference between the target value and the output value is minimized. But if you have two layered neural network how we know that there is an error at the hidden unit.
And how do we update the weight values because we do not know the target value at the hidden unit. At the output we know what the target value is. At a hidden unit we do not know what the target value is. And if we do not know what is the target value we will not know what the error is and we will have no basis for modifying the weights. 
So the basic idea behind back propagation here is that whatever error you observe at the output you try to allocate the error to the hidden units. If there is no error at the output that means both the outputs have no error you assume that there is no error at the hidden units also. 
But if there is some error here you try to allocate the error back to the hidden units from which it receives the input. Similarly if you have an error at this output unit you allocate this error to the units from which it receives input. So we propagate the error from the output backwards to the hidden units. So how we allocate the error? We allocate the error in proportion to the weights. So, once we allocate the error we know the target value that we have to minimize and therefore we are doing gradient descent. 
So this is the basic idea of back propagation.   
Back propagation algorithm:     (refer slide time 31:15)

 
In the first step we initialize the weights to some small random values. We first decide a topology of neural networks which has some arcs with each arcs there are some weights, we give them small randomly selected values. After that we feed the training example to the network. 
We take each training example and input it to the network and we compute the network outputs ok for each output unit k. And then we will do gradient descent with back propagation. 

For each output unit k we compute delta k to be ok × 1 – ok × t k – ok. 
For each hidden unit h we compute delta h to be oh times 1 – oh times sigma wh delta k. Therefore each hidden unit takes the burden of some of the error at each of the output units to which it is connected. 
So delta k is what is coming from the kth output unit and whk is the weight of the arc connecting this hidden unit with the kth output unit. 
So we compute delta k initially at the output nodes then at the node above the output node then at the node above that and so on. 
So, after we have computed delta k at all the units such as the output units as well as the hidden units then we update the network weight as follows:
 wij is updated as wij + delta w i j. and delta wij = eeta delta j times xij. 
Therefore the actual algorithm is quite simple to implement. so there is an initialization phase, after that there is a feeding phase where the training examples are fed to the neural networks and then at each unit starting from the output up towards the input we compute the value of delta i and then we modify the network weights and then we continue if the network is not satisfactory.
So, in back propagation we do gradient descent over the entire network weight vector.
(refer slide time 38:45)
 

 

And this back propagation can work not only for a neural network with one hidden layer but neural network with any number of hidden layers. In fact it can work even though we do not have layered neural networks but neural networks in the form of acyclic directed graph.
 We can still do this back propagation algorithm. 
The basic idea is that we start from the output and then find the delta value at those notes and then we find the delta value of those nodes for which downstream of the entire delta values have been computed. 
Therefore we compute the delta values backwards and then we can do the weight training. So, back propagation is not an optimum algorithm. By doing back propagation you cannot guarantee that the weight vector that you arrive at is the best weight vector. But in practice back propagation often works well. In fact what you can do is if you run back propagation once with some initial input values and you get a network which does not satisfy you completely you can still run back from back propagation several times with different initial weight values. 
There are some variations to back propagation.
(refer slide time 40:23)

  
Apart from the delta term that is eeta delta xij some people add another term called the momentum term. What is the momentum term? 
Momentum term takes contribution from the previous value of delta wij. Why is the momentum term used? Sometimes what happens is because there is a local minimum and as a result of back propagation the system may have a tendency to get stuck in the local minimum. 
So this is the local minimum and there is a scope of going further down in the error surface. 
So, to prevent your current weight vector to get stuck at the local minimum what we do is we keep track of what is the previous direction in which the error surface is moving. Now, even if we get stuck in the local minimum we try to have the momentum of the fall so to the new delta wij value we add the value of the previous slope so that our weight function can escape from the local minimum. So, by using the momentum term we can minimize the error in training examples. 
Now what we will do is we will study several issues concerning neural networks. 
For example, we wish to know that suppose we have trained a neural network using our training example how to find out whether works very well for unseen training examples. 
Secondly one thing we have to keep in mind when we use neural networks is that usually neural network training time can be quite high because we need several thousand iterations for training to converge. 
However, if we are able to train a neural network using the neural network is very fast. 
The feed forward nature using the neural network is very fast. 
Normally when we use a training example which we use for learning as training time increases the error reduces as shown by this graph.
(refer slide time 40:23)

  
With time as we train more and more typically the error reduces. However as we noted when we looked at decision trees if we look at new examples which we do not use for training then with them the error curve may be different. For example, with the new example usually error reduces some time and then the error starts increasing and this is due to over-fitting. 
We would like to detect when the error is at minimum and we would like to stop at the point where the error on the test set is minimum. Now how to detect this point? As we have done earlier what we can do is we can have a separate validation set or test set to decide when to stop training the network. Now suppose we have three points; if we have a very simple function connecting these three points often there is a greater chance that such a function can fit unknown examples better. If you have a more complex function



(refer slide time 44:56)

 
 fitting the points such a function maybe over-fitting the data. Similarly when we consider artificial neural networks sometimes we have to look for the simplest neural networks that can reasonably fit the data rather than a very complex topology which is able to fit the data perfectly. So, in a neural network when we have too few neurons that is if you have too few hidden units the network
(refer slide time 45:46)

  
may not have enough degrees of freedom to precisely approximate the desired function. 
But if the network has too many neurons it will learn the training examples perfectly. But due to these additional degrees of freedom it may be over-fitting the data. So it may be showing impossible behavior for unknown inputs. 
So when we design a neural network we have to be careful about that. Some other problems of neural network is that there are many parameters to be set. Suppose we are using the sigmoid function we have to decide the threshold of the sigmoid unit function, we have to decide eeta which is the learning rate, we have to decide alpha, the term associated with the momentum, we have to decide number of hidden units, we have to decide number of hidden layers and so on. Therefore in a neural network using a neural network takes a lot of time as you have to experiment with all these and try to find a good network to fit your data. And coupled with this is the long training time. For each configuration that you have to take you have to train it and training time is typically quite long. 
So what are the design steps for an artificial neural network?

(refer slide time 47:13)

 
first you have to set the architecture for the neural network that is you have the inputs and number of outputs, you have to decide which inputs to take how many outputs to take, number of hidden layer that you have, the number of neurons that you will have in each hidden layer then you have to run your gradient descent algorithms to optimize the weight vector values and finally you test the network 
and if the success and the network is satisfactory then you are done but if the network is not satisfactory then you have to go back to the previous steps so that you try with different values of parameters and may be even different topology. 
So there are a lot of parameters 
(refer slide time 48:00)
 
we have to decide when we are working with the neural network. 
(refer slide time 48:03)
 
And as we said over-fitting can usually occur. When do you terminate training? So normally you stop if the error fails to improve  

refer slide time (48:16)
 
or you stop if the rate of improvement stops below a certain level. Or you stop if the error reaches an acceptable level or you stop when a certain number of 
epochs have passed.
refer slide time (48:37)

Trancripted by p.sasikala
Lecture time (1:00)

ARTIFICIAL INTELLEGENCE
By
Prof.Sudeshna sarkar
Department of Computer science and Engineering
IIT Kharagpur

Lecture – 38
Probabilistic Learning (TIME 00:50)

Today we have our last lecture on machine learning. Today we will talk about probabilistic learning and may be discuss about computational learning theory.  Earlier you have learnt about uncertainty and probabilities. So what we discuss now is how Bayesian learning 
refer slide time (01:17)
 
can be used for the sort of concept learning tasks.
 You have already looked at some simple machine learning algorithms that are used for concept learning. For example, you have studied decision trees and looked at algorithms for top down training of decision trees. In the last two classes we have also looked at neural network so mainly multilayer feed forward neural networks and we have seen how to train the neural networks using algorithms like back propagation so that the neural network can recognize the concept. Today we will see that the Bayesian learning framework also provides some ways of concept learning and we will also discuss the applications of these things as well as their limitations. 
Why Bayesian learning is as important framework to study? Firstly Bayesian learning provides many practical learning algorithms. For example, today we will discuss a very simple learning called Naïve Bayes learning which is especially easy to learn and to train. And then we will also discuss how the surprising simple method works quite well for certain classification tasks and it is widely used for applications such as text classification and other areas. Then if you want to have more sophisticated methods you have already learnt about Bayesian belief networks it is possible to learn also Bayesian belief network. However we will discuss that in the Bayesian learning framework it is possible to easily combine prior knowledge. For example in the concept learning problem what you are trying to find out you are trying to learn a concept, you are trying to represent a concept and you are trying to find out an hypothesis which is a good representation of the concept. If you have some prior knowledge on which hypothesis are more likely for example we will see that the Bayesian framework allows you to integrate this prior knowledge into your learning methods so that the posterior hypothesis that you output depends on your prior probabilities or prior knowledge of the hypothesis.
Also Bayesian learning provides many foundations for machine learning. And the Bayesian description Bayesian approach can be used to evaluate learning algorithms including other learning algorithms. And it can be used to guide the design of new algorithms and it allows us to learn from models or meta learning. Bayesian learning is a very important concept. 
How is Bayesian classification useful? 
refer slide time (05:29)
 
Bayesian classification is useful for probabilistic learning. In probabilistic learning we calculate the explicit probabilities for hypothesis. For example, if you are / some training example you can output more than one hypothesis and you can attach probabilities with the different hypothesis. For example, let us say you have a decision tree so in neural network or decision tree you give only one hypothesis. When you are giving your training examples you come up with1 hypothesis. But in probabilistic learning framework it is possible to you to come with multiple hypotheses and also associate the confidence of probability that you have in each of these hypothesis. The second property of Bayesian classification is that it can be used in an incremental fashion. Each example can incrementally increase or decrease the probability of a hypothesis. You can find out for the posterior probability of the hypothesis and when you have processed a number of examples you have attached certain posterior probabilities with different hypothesis. For example, your probabilities can get updated incrementally. So, this is one advantage of Bayesian classification techniques. Thirdly it allows you to make probabilistic prediction. That is, you can predict multiple hypotheses and you can associate with each of them their probabilities. Also the Bayesian framework provides a standard of optimal decision making. Even when you are not using this technique but using some other technique you can try to evaluate how well the technique corresponds to this model. We can try to evaluate the type of the hypothesis that a learning algorithm outputs whether it satisfies the different criteria that we will study. 
refer slide time (07:51)
 
Some basic formula for probabilities: For example the product rule is used to find out the product of the probabilities of A and B.
Now P(A and B) can be written as the product of condition of P(A/B) times the P(B). As this is commutative the P(A and B) is the same as the P(B and A). Therefore it can also be written as P(B/A) times the P(A).  
We can combine this expression P(A/B) times P(B) = P(B/A) times P(A) to derive a very important learning hypothesis called the Bayesian hypothesis. Here are some other rules of combining probabilities. Sum rule allows you to find the probability of the sum of two random variables P(A + B) = P(A) + P(B) – P(A) and B or A intersection B. So this is called sum rule. 
We have the theorem of total probability. If we have events a1 a2 an and these events are mutually exclusive then we can say that P(B) = sum over all i’s P(B/Ai) times P(Ai). 
So we can condition on the different Ai’s and we can combine these the weighted sum to get the P(B). These three formulas are often used in different manipulation of probabilities. 
Another very important rule is the Bayes Rule. 
refer slide time (10:20)
 

This expression can be used to derive Bayes rule. From this expression we can write the P(A/B) = P(B/A) times P(A)/P(B). And this we can rewrite in the form of P(h/D) = P(D/h) times P(H)/P(D). This follows from the previous expression that we saw. Now let us see how we can interpret this. If you take that H represents the hypothesis and D represents the data or evidence. So, in the concept learning problem you are given some training examples or the evidence and you have to find out a hypothesis which describes the concept. So this can be represented probabilistically as finding out for different hypothesis. You can consider different hypothesis and find out the probability of that hypothesis given the data. And you can evaluate this probability for the entire possible hypothesis and output that hypothesis which is most likely. What we are trying to do is evaluate the P(A)/hypothesis/data or evidence that we have. Now we can rewrite P(h/D) in terms of probability D/h. Why we do this rewriting? It is often easier to evaluate the P(D/h). So, if you evaluate P(D/h) from that we can find P(h/D). But we need some other factors for example we need P(H) and P(D). Let us see what these mean. So P(H) can be interpreted as the prior probability of the hypothesis H. So, before you have got any evidence you have a set of possible hypothesis that you are considering and you have your initial belief about the probabilities of each of these hypothesis. 
Each of these hypothesis may be equally likely or may be you consider that some of these hypothesis are more likely than the other. So P(H) denotes your belief about the prior probability of the hypothesis H and we can refer to this as the prior. Now what does P(D) stands for? It is the prior probability of the data D or the evidence which is not very intuitively clear what it means. But P(D) is the prior probability of that data. Then what is P(h/D) signify? It is the probability of hypothesis given the data. So after you have seen the evidence what is your posterior probability? What is your current belief, posterior belief after seeing the data about the hypothesis? This is called P(h/D). After you have seen some training examples D you want to find out based on this training example what is the probability of each of the hypothesis. And as you increase your training examples this will be updated. Then what is P(D/h) signify? It means the likelihood of the (D/h). So you are / some hypothesis H. Now what is the likelihood that the hypothesis could have / rise to the data? The P(H/d) is  the posterior probability = P(h) the prior probability, this is the prior probability and this is the posterior probability. And what is P(D/h)? It is the likelihood of the data given the hypothesis and this P(d) is the evidence that you have. This is evidence and this is likelihood.
You can write it as posterior = likelihood × prior/evidence. So this is the base rule that we have P(H/d) = P(D/h) × P(H)/P(D). 
Generally we want most probable hypothesis given the training data, the hypothesis which has the maximum posterior probability. We refer to it as h map. H map is called the 
refer slide time (16:20)


 
map hypothesis. The map hypothesis is that hypothesis for which P(h/D) is maximum among all the hypothesis that you are considering. We know what a hypothesis space is. It is the set of hypothesis which you are considering. Your hypothesis space can finite and finite but suppose your hypotheses is finite and suppose there are hundred hypothesis in the hypothesis space. You want to find out the probabilities, the posterior probabilities for each of the hypothesis so that you can find out that hypothesis for which the posterior probability is the highest. So h map is that value of h for which the P(h/D) is maximized. This can be written as arg max (h/h) included in h, h included in the hypothesis space H P(H/d). 
Now if we apply Bayes Rule to this expression we can substitute the right hand side of this equation so we can say that h map is that hypothesis for which P(D/h) times P(H) times/P(D) is maximized. Now when you inspect this expression you are trying to find out that value of h for which the combination of these three terms is maximized out of which the particular hypothesis we are considering affects these two terms but not this term.. 
 So, that hypothesis for which this entire thing is maximized the same hypothesis for which only the numerator is maximized. So a map hypothesis is a hypothesis for which P(D/h) times probability h is maximized. (19:24)
 

Suppose you are given a learning problem and you are able to find out these different probabilities you would like to choose the maximum a-posteriori hypothesis h map and h map is given by arg max, h is included in the hypothesis space, P(D/h) times P(H).
 Now, as we discussed ph denotes the prior probability of the different hypothesis. 
Suppose that initially you consider all hypothesis to be equally likely then you can say that P(H) is the same for all h included in the hypothesis space. In that case h map would be that hypothesis for which P(D/h) is maximized if you consider P(H) is the same. So such a hypothesis for which only P(D/h) maximized is called the maximum likelihood hypothesis and this is called a maximum a-posteriori hypothesis. 
Now map and ML hypothesis gives you the same hypothesis if the prior probability of the entire hypothesis are equal otherwise they might give you different values. 



refer slide time (19:24)
 
Here is an illustration of what we mean by prior, posterior and how they affect the decision making. Here is a simple standard problem. In this problem your objective is to find out whether a patient has a malignant tumor or not. And you are / the following: 
A patient takes a lab test and the result comes back positive. The test returns a correct positive result in only 98% of the cases in which a malignant tumor is
actually present, and a correct negative result in only 97% of the cases in which it is not present. Furthermore, it is known that only .008 of the entire population have this particular tumor. So this is the data that you are given. What you have to find out? Whether the patient is likely to have malignant tumor. Here the two possible hypotheses are; the patient has malignant tumor and the patient does not have malignant tumor. You have to find out whether the probability of the patient having malignant tumor given your evidence is higher or the probability of not having malignant tumor given the evidence is higher. 
 What is the evidence? The evidence is that the patient takes a lab test for which the result is positive. So the result of the lab test is positive and the lab test is taken to identify whether the patient has tumor or not. However, the lab test is not 100% accurate. 
Now you are told that, if the patient does have malignant tumor the lab tests returns yes only in 98% of the cases. In 2% of the cases the lab test returns wrong result. It tells you that you do not have tumor but actually you have tumor. Also it tells us that if the malignant tumor is not present then in 97% of the cases the lab test returns negative. And 3% of the cases the lab test returns positive. Therefore based on this you have to find out if the result is positive how likely the patient is to have malignant tumor. In addition to this you are given some other data that your prior probability on malignant tumor and not malignant tumor are not equal. Hence, only .008 of the entire population has this tumor. That is, .992 of the population does not have tumor. 
So the prior probability of tumor is only .008 and the prior probability of no tumor is .992. 
Here is a way to proceed in this problem. 
From the description given in the previous slide we can write that P(tumor) = .008 so we can write the P(tumor) = .008 and P(not tumor) = .992. These are your prior probabilities. Now you are also given that, if there is tumor then P(lab test) being + is .98 and if there is tumor P(negative
refer slide time (24:15)


 
result) in the lab test is .02. You are further gven that if there is no tumor then P(+) in lab test is .03 and P(negative) in lab test is .97. Now, the evidence that you are given is that the lab test returns positive and you have to find out the posterior probability of the two possible hypothesis P(tumor/+) and P(no tumor/+). So P(tumor/+) can be written by using Bayes rule as P(+/tumor) P(tumor)/P(+) and P(not tumor/+) = P(+/not tumor) times P(not tumor/P(+). Now you can evaluate each of these values from the data you are /. What is P(+/tumor)? P(+/tumor) = .98 and P(tumor) = .008..
What is P(+/not tumor)? P(+/not tumor) = .03 and P(not tumor) = .992.
And what is P(+)? It is not known to us but it does not matter. When you have P(tumor/+) and when P(not tumor/+) we want to find out which1 of them is higher. 
You can find out which1 of them is higher by only evaluating the numerator. Also you know that probability of these two sum to 1. You know that any random variable P(A) and P(not A) = 1. So you can take this P(+) as some constant alpha and you can say alpha times .98 × .008 + alpha times .03 × .992 = 1, you can find out alpha and you can find the exact values of each of these probabilities. 
So you need not know the P(+). In this problem you will see that P(not tumor/+) is much higher and the reason to why this is the case is because the prior P(not tumor) is much higher. 

Prior P(not tumor) = .992 and P(tumor) is only  002. If these two prior probabilities were equal then P(+/tumor) = .098 and P(+/not tumor) = .03 so definitely this would have been more likely. The maximum likelihood hypothesis is that the patient has tumor. But if we combine it with the prior probabilities on the hypothesis whether a patient has tumor or not tumor we see that most likely the patient does not have tumor. So this example illustrates to you the role that prior probability plays in finding out the posterior probability.
So how does a map learner work? 
refer slide time (28:25)

 
You have the hypothesis space H and you are trying to find out for each hypothesis H in the hypothesis space you compute the posterior probability 
P(H/evidence) and you compute it using Bayes Rule and then you output that hypothesis for which this is highest.
 So the posterior probability h map is maximum and that hypothesis for which P(H/d) is maximum. This is called a map learner maximum a-posteriori probability learner. The Bayesian framework provides a survey of evaluating the probability of hypothesis. 
In practice the Bayesian method is not always possible to apply. The main reason is that in the particular case we considered we had only two possible hypothesis in the hypothesis space, the patient has tumor or the patient does not have tumor. Consider a case whether a number of possible hypothesis is very large. So, if you follow this procedure for each hypothesis you have to find out the posterior probability for each of the hypothesis given to it for each of the hypothesis. You have to do it for each of the hypothesis. If the number of hypothesis is very large it is not practical to enumerate each of the probabilities. 
Therefore this sort of rule is not applicable. But this nevertheless gives you a very good framework to show what is happening here. And if you study further into machine learning you will see that many of the learning algorithms are evaluated based on whether the hypothesis that they output is the maximum a-posterior hypothesis, whether it is a map hypothesis or ML hypothesis or whatever criteria that satisfies. Therefore this gives us standard for judging the efficacy of different learning algorithms and it can be applied when your hypothesis space is not very large.   
refer slide time (28:25)

   
So, the map learner is computationally intensive because you got to find out the posterior probability for each of the hypothesis. It provides a standard for judging the performance of learning algorithm. You can analytically find out whether a particular learning algorithm gives the map hypothesis or not. And, by choosing P(H) the prior reflects the prior knowledge about the learning task. And P(D/h) also can be computed in many cases depending on our knowledge about how the hypothesis affects the data.
So far we have looked at a map hypothesis. 
A map hypothesis gives you the hypothesis which is most likely. 
In some cases it is not the hypothesis that you really care about but you want to really find out the class. 
So you have a concept learning problem, you are given some data, you are / a new example and you want to find out the class of the data.  
If the hypothesis corresponds to the class then a map hypothesis is what you are looking for. 
However, consider these cases, if the hypothesis that is returned by the map learner is applied on the instance x

refer slide time (32:38)

 
does it give you the most likely classification of the instance x? It may not be the case. It depends on what form the hypothesis is in. For example, suppose you have some evidence D and you have three competing hypothesis.  
So your hypothesis space consists of three hypothesis h1, h2 and h3. 
And suppose you compute the posterior P(Ph1) to be .4 and that of h2 to be .3 and that of h3 to be. Now, which is the hypothesis with the highest posterior probability? Definitely it is h1 because P(h1/D) = .4 which is more than P(h2/D) or P(h3/D).
Now suppose we pick h1 and we apply h1(x). 
Suppose we find out that when we apply h1(x) h1 applied to x is + whereas h2 x is – h3 x is also –.
Now what is the most likely classification of x1? So you can just look at the examples and see that combined probability h1 h2 is .36 and they predict that x is – whereas h1 which has the single highest probability predicts that x is + but this probability is only .4. So in this example x is most likely best classified as minus. Therefore the map hypothesis applied to the instance does not necessarily give you the most probable classification of x. In such cases what we really want is to find out a Bayes optimal classifier. 
Now let us see what a Bayes Optimal Classifier is.

 Suppose you have different classes V so V is the set of possible plus. In your classification problem V where V is the set of possible classes. Therefore possible classes could be positive or negative in the case of the tumor example. 
Suppose you are trying to classify a news article as sports, politics, health and entertainment so your possible classes are sports, politics, health and entertainment.
These are the four possible classes. So what you are trying to find is the class. 
So the class vj which is an element of V, V is the set of all classes and you want to find out that class in V for which this quantity for which this is the highest. 
So you have different hypothesis h1, h2 etc and all these hypothesis belong to the hypothesis space H. So, for each hypothesis hi in the hypothesis space you find out the probability that the probability of a particular value of Vj sigma over all hypothesis P(Vj/hi) × P(hi/D) 
So this is the posterior probability of the hypothesis hi and this is the probability that the classification is Vj given the hypothesis i. 
Suppose you have three hypothesis h1, h2, h3 the posterior P(h1) = .4, h2 = .3,    refer slide time (36:58)

  
h3 = .3. Then what is the probability that your class is positive? It is p(+/h1) × .4 
+ P(+/h2) × .3 + P(+/h3) × .3. So this is the P(+). What is the P(–)? 
Similarly it is P(–/h1) into posterior P(h1) + P(–/h2) × posterior of (h2–/h3) × posterior(h3).

So let us try to apply this to the particular problem we saw in the previous slide.
We have already seen that P(h1/D) = .4, P(h2/D) = .3, P(h3/D) .3 and h1(x) is positive that means P(+/h1) = 1, P(–/h1) = 0.
 So this is given to us and we can therefore write as P(h2/D) = .2, P(–/h2) = 1,  
P(+/h2) = 0, P(–/h1) = 0, P(+/h1) = 1, h3 classifies instance as negative therefore P(–/h3) = 1 and P(+/h3) = 0.
So we can find out that P(+/hi) × P(hi/d) over all hi's gives us .4.
It is because there are three hypothesis h1, h2, h3 and only h1 classifies the positive. So this expression of P(+/h1) × P(h1/d +/h1) = 1 and P(h1/D) = .4 and the rest of the things are 0. So the P(+) is finally .4 whereas P(–) = .6. So this is the most likely classification therefore the most likely classification of instance is negative.
Therefore, once you have found out the posterior probability of hypothesis you can apply the Bayes Optimal Classifier to find out the most likely classification of examples.
 
Naive Bayes Classifier: This is a very simple classification algorithm but quite effective in especially problems where the number of attributes is very large.    refer slide time (40:16)


 
So the Naive Bayes Classifier is a popular simple learning algorithm. It works well when a moderate or a large training set is available. To explain the Naive Bayes Classifier we make a very restriction assumption and the assumption is that the attributes that describe instances are conditionally independent given classification. 
Suppose we have some attributes a1, a2, an for a classification problem
we say that the P(a1/c) is independent of a2 or of the other attributes. So the attributes are independent given the class. So this is an assumption that we make. This is a very restricting assumption. We are trying to say that this attribute features are completely independent features and they do not interact. In practice, in realistic learning problems it is very difficult to find examples of attributes whether all these attributes are independent.
However, even with this wrong assumption which does not hold certain realistic problems the output of the Naive Bayes Algorithm in practical problem sometimes is surprisingly good. And there are some applications on which Naive Bayes has been applied and gives quite good results. For example, in certain diagnosis problems.
And in text classification your objective is that you are given some text documents and you have some classes. For example, it could politics, sports, health, entertainment etc so you want to find out which class this article belongs to. And for this classification task Naive Bayes Algorithm has been quite successful in many such classification tasks. For example, there is a particular experiment people have performed on the 20,000 newsgroup data set where you have 20 newsgroups and you have 1000 documents per newsgroup. And on this Naive Bayes was applied and it gave a classification accuracy of 89% which is quite good. 
Now let us look at what the Naïve Bayes Classifier does. 
So we assume that we have a discrete target function which takes you from the attribute space x to the class c. 
And suppose each instance x is described by the attributes a1, a2, an so there are n attributes s sub 1, s sub 2, s sub n
refer slide time (43:20)

 
and you have a classification from. 
The c is the set of classes and x is the input instance. Now what is the most probable value of f(x)? It is given by the c map, c map is the class which has the highest probability. 
The class c with the maximum a-posterior probability is that class cj for which the following maximized. P(cj) given the attributes. 
So you want to find out c map which is that value of cj such that cj is included in C for which the P(cj/a1, a2, an) is maximized. 
We can rewrite P(cj/a1, a2, an) as P(a1, a2, an/cj) × P(cj/P(a1, a2, an) by applying Bayes Rule. 
So P(cj/a1, a2, an) is P(a1, a2, an/cj) times P(cj/P(a1, a2, an).
And since P(a1, a2, an) does not depend on cj we can say that c map is that class for which P(a1, a2, an/cj) times P(cj) is maximum. 
So c map is that class cj for which P(a1, a2, an/cj) times P(cj) is maximum. 
The P(cj)
refer slide time (43:20)

 
we can estimate from either we can have prior belief or we can estimate from the data that we have given. If you are given some training examples and the training examples we find out the proportion of examples belonging to the different classes we can estimate P(cj). 
Now the difficulty is to find out value of this expression: P(a1, a2, an/cj). This is a joint P(n) attributes. Now if we want to estimate the joint probability from the training data we will require a very large number of training data. 
It is because we have n attributes. Even in the simplest case when each attribute is Boolean that is each attribute can take two different values the number of combination of a1, a2, an is 2n. So the number of such joints is very large. We look at all combinations of the attribute values; the number of such combinations is very large. Now in order to estimate for each of them the probability requires very large corpus and also requires a large number of the values that you have to store and this is clearly not doable when n is large. 
Therefore in the Naïve Bayes assumption we make a very simplifying assumption. We assume that P(a1, a2, an/cj) is the product for P(ai/cj). 
We assume that (a1, a2, an) are conditionally independent given cj so this can be written as P(a1/cj) times P(a2/cj) times P(... an/cj). 
So we can write P(a1 a2 an/cj )as product of P(ai/ c j over all i’s.
Therefore c Naïve Bayes is that class for which this times P(cj) is maximized. 
So cnb is the class cj which is a member of C so that product over i P(ai/cj) × P(cj) is maximized. 
This is the Naives Bayes Classifier, it works under the assumption that the attributes are conditionally independent of each other given that we know the class. 
A recap: In Bayesian classification the classification problem may be formalized using a-posterior probabilities.       refer slide time (48:20)

 
We want to find out P(C/A) probability that the sample tuple is of class c. You are given an instance x and you want to find out the probability that instance belongs to class. 
For example, suppose you want to know given the attributes of a particular day whether it is a good day for playing. 
Suppose you are given that the outlook of the day is sunny, windy and so on I want to know whether it is a good day to play or bad day to play. So there are two classes yes and no. 
Therefore the idea is you want to assign to the sample x the class label c such that P(C/x0) is maximum. 
This is the problem we were trying to solve and Naives Bayes Classifier is one way of getting the solution provided the independent assumptions hold provided these attributes are independent given the class. And outlook and windy does not depend on each other so whether it is sunny or cloudy does not affect whether it is wind or not which may not be true. 
We have seen how to estimate a-posterior probabilities by
refer slide time (50:11)

 
using Bayes Theorem. Bayes Theorem is stated as P(C/X) = P(X/C) times P(C)/P(X); P(X) is constant for all classes and therefore we want to find out that class for which P(X/C) × P(C) is maximum. But we cannot compute P(X/C) because X is an instance which depends on potentially a large number of attributes and if you to estimate each of these possibilities the number of such possibility will be very large. 
So in Naïve Bayes Classification we have made a very restrictive assumption a naïve assumption which says attributes are independent. That is P(x1, x2, xk/c) is product of P(x1/c) (x2/c) (xk/c). 
refer slide time (51:12)

 
Suppose xi is a categorical attribute then how do I estimate P(xi/c)? What we can do is we can look at the training examples and find out for a given class how many times xi has a particular value. 
So P(xi/c) can be estimated from the training corpus as the relative frequency of those training examples that have value xi as the ith attribute in class c. 
But if your attribute is continuous, suppose you have an attribute like temperature which takes continuous values then (exi/c) is often estimated by assuming a probability distribution for these attributes. For example, often we assume a Gaussian density function from which P(xi/c) can be estimated. 
A recap: Naïve Bayes Algorithm works like this. In Naives Bayes learn you are given some examples. For each target value vj you estimate P(vj). Suppose you have this news group classification or news classification you have four classes sports, politics heath and entertainment. Now for each of these classes you  estimate P(vj) by looking at the corpus and finding out how many news you have from sports, how many from politics, how many from health, and how many from entertainment. So, from this you can estimate P(vj). Now for each attribute value you can estimate P(ai/vj). 
So vj is sports, you find out how many times this attribute a1 is true or a1 has a particular value. In a document classification we often use the words in the document as features. So we look at the set of words that occur in the document. So we want to know how many times the word referee is in a document given that it belongs to a sports domain or how many times the word referee is in the document that belongs to entertainment domain or the health domain.
refer slide time (54:57)

 
 
 For each attribute or each word that we are considering we can find out how many times that attribute takes a particular value for those documents that belong to that class. From this we can estimate P(ai/vj). 
Once we have estimated for all j P(vj) that is for all class the probabilities and all conditional P(ai/vj) we can use those values to classify a new instance. 
So the most likely classification of the new instance given by v is that value of vj for which P(vj) into product of (pi/vj) for all ai is maximum. This is Naive Bayes formula for classifying a new instance. 
So how do I compute the P(ai/vj)?
P(ai/vj) is, suppose  n denotes the number of examples in your training set for which the class is vj and nc denotes the number of examples in your training set for which the class is vj and for which ai has this particular value that is the attribute has the value of i. 
So P(ai/vj) can be estimated nc/n. 
However this sort of estimation has a problem. It could be that because you do not have sufficient number of examples in your corpus in your training data there is no example with class = vj in attribute = ai in which case this probability will be 0. And because of you are taking the product of different probabilities if one of the probabilities is 0 the entire value would be 0. Therefore in order to avoid this problem we do some form of smoothing. So, to ensure that the numerator is not 0

refer slide time (56:57)

 
we add p which is the prior estimate for P(ai/vj).
So we ensure the prior estimate is greater than 0. 
And if m is the weight given to the prior estimate then we add mp to the numerator and m to the denominator. So this is the smoothed value of P(ai/vj) to ensure that this never goes to 0. In practice we take p to be a very small value and m is the number of your examples. So let us just illustrate it with an example. Suppose we are given the following data:
 (56:57)
 
Suppose we have some training set and in the training set we have the data where we have the different attributes of a particular day. For example, what is the outlook of the day? Is it sunny or overcast or raining? Is the temperature hot mild or cool is the humidity high or normal is that windy or not? And we want to know whether it is a good day for sport or a bad day for sport. And from the data we find out from the P(ai/vj). If it is sunny then the P(good day) = 2/9 and bad day is 3/5 and so on. So we get this data of P(ai/vj) and when we are given a new instance the new instance (58:11)

 
is described by the values of the attributes. In this particular instance outlook is sunny, temperature is cool, humidity is high and wind is strong and we want to find out whether it is a good day for playing tennis or a bad day. 
i have fourteen examples in my training set and nine of them are good days five are bad days so P(yes) = 9/14 P(no) = 5/14 and then we use the value of the previous table to find out the Naïve Bayes example. 

The independence hypothesis we make in Naive Bayes makes computation simple but is seldom satisfied in practice. 
refer slide time (59:11)

 
And if we realistically want to model the problem we have to use joint probabilities which are not feasible. But the way out is to use Bayesian networks. You can have algorithms for learning Bayesian networks.  


Trancripted by p.sasikala
Lecture time (57:28)

ARTIFICIAL INTELLEGENCE
By
Prof.Anupam basu
Department of Computer Sscience and  Engineeringand Engineering
IIT Kharagpur

Lecture – 39
Natural language processing– I (TIME 00:53)

inIn the last two lectures of this course iI will be presenting to you one important aspect of intelligent behavior. 
aA machine is considered to be really intelligent only when it can understand or and interpret or speak for that matter natural language.   
naturalNatural language   is the  languagethe language   that we speak to day -to -day to ourselves. 
Different languages like English Hindi Bengali different languages that has evolved and through generations among the people through which they communicate is called natural language.    andAnd the capability to understand, interpret and communicate throughand true natural language is a very important criteriacriterion of intelligent behavior. s
oHere is an today i will be presenting to you an overview of natural language processing: ok so The first question that we need to answer is why should wewe should at all study this? whyWhy natural language processing is important important?
refer slide time(2:20)
 
ifIf we lookok from the practical sidte then the huge amounts of data available in the internet for example at least two point five 2.5bnbillion pages are available and thereat is lot of information that is spread all over which are is in natural language. 
So, if a computer has to learn it will have  to utilize and understand such expressions which are available in the form of natural language. 
andAnd there are applications for processing large amounts of text. weWe can see some of the typical applications here. see thereThere are so many text data available in the internet. 
isIs it possible to classify these  texts automtomatically into categories? forFor example,  all the texts on electricity for school student s. 
canCan the computer program really understand the extentd, the depth of the different texts and identify that well? Like this is on electricity and theise are the contentsdense of theis text or suitable for some school children. indexingIndexing and searching large text we need to search the texts and also another very important  application is automtomatic translation. weWe live in a multilingual world. 
So, if you consider the Indian scenario it is not the fact that all the Indians specially those who are deprived of good school education that all of them understand English. 
So, in order that they can also benefit from some of the knowledge that is available in English it needs to be translated to theiry localok a language that they understand. 
forFor India it is even more important because India is very much multilingual. 
ok speech Speech understanding is another important application where, see for example he here only one example is given; understanding telephone conversations but we sawbesides is that there can be so many other applications. if we could have a computer which could understand the speech of a person and say in hindiHindi and that is automtomatically translated in to text and it wilis beingl recorded in the database of a computer then it will be very much for useful not only for telephone conversations but also many other applications. 
similarlySimilarly if a computer can understand the text that is written and speak it out then for those who cannot speak it will be a really great benefit and this system is known as a text to speech system. similarlySimilarly speech recognition automtomatic speech recognition is s very important applications. Now-a-days information extraction has become is gaining more and more importance so that we can one application can be that we extract useful information for the different resume is that we are getting.
automaticAutomatic summarization of the text: Ssuppose the inter entire content of the bobookok if it could be summarized in say four or or five pages then somebobody can go to internet and have a brisk review of the basic content of the bobookok before he decides to the book  purchase that book or read it.d already nowadaysNow-a-days it is also it is being done but it is done manually. 
ifIf we had computers so who could really understand natural language then this thing could be done in an automtomated way. 
questionQuestion answering: is also a another very important application.s 
similarSimilar another applicationsother applications are knowledge applications acquisition. acquiringAcquiring knowledge from the experts, text generations, dialogsand dialogs and for all theise applications it is needed we require natural language processing expertise. 
nowNow why do we call some languages to be natural languagelanguage?
refer slide time (7:26)
 
theseThese are the languages as i said that we use in order to deriveour day-to-day life. itIt refers to the language spokoken by the people naturally. forFor example, English, Japanese, Swahili as is opposed to the artificial languages are which also we also called formal language languages like c plus plus+ +  java etc. 
now theseThese languages like c plus plus+   java+ java etc have got a very strict syntax. 
Iif iI say tomTom go to school you will understand that iI have made some grammatical mistake that instead of saying tomTom goes to school iI have said tomTom go to school but the meaning will be immediately clear to you. 
orOr if iI sayt gives me the glass of water then also you will give me the glass of water 
although the sentence iI have said is syntactically wrong. butBut someuch syntactically wrong statements will never be accepted in a formal language environment. 
ifIf you we just a miss a semi cololan in a c program they only will be passing error. alright soSo, the formal languages are strictly guided by the rules that are theyre in the grammar. inIn natural also we  need the rule we have got the grammar but we are more freefreer and monor minor syntactic errors does not deter the communication of the meaning ofr semantics. that is why we are interested in natural language processing and since there is i mean that is in our communication in natural language communication we can take recourse to some newer ambiguous statements still human beings can perceive it, understand it and can act accordingly and 
because of this flexibility natural language processing all the more difficult thean formal languages. 
now naturalNatural language process is required for applications that deal with natural language in some way or other. 
onOn the other hand we often heare the term computational linguistics.  
computationalComputational linguistics means carrying out the linguistic studies. It is studyingstudying the different features of a language using computers. thereThere is a shuttlubtle differentce between these two. thisThis is more on the linguistic side than on natural language processing but obviously they are very closely related. ok nowNow, if you we continue our discussion on why natural language processing let us lookok at some of these sentences.
refer slide time (10:00)
 
 
ifIf you we lookok at these sentences than they are not carrying in any meaning to you at all. 
theseThese are merenear symbobols which are not interpreted as yet. 
Now, when we feedfit such set of symbobols to a computer the computer will not understand anything.    However, whatever data we feedit to the computer you know or are symbobols and ultimately they get converted in to the directorsASCII characters and the computer interprets that. howeverHowever, in a c C language whenever you we type in the c C program what in goes in is a sof string of escii ASCII characters. right butBut in that case we have got a program that seeC compiler that is based on the strict basic grammar on of the c C language with which can interpret these strings of symbols. So substring also so, using that compiler based on that grammar we have imparted to the computer the knowledge of the c C language. Similarly, computers by itself lack the knowledge so they see the text in English the same way you have seen in the previous text, nothing but a bunch of symbobols and what is required is interpretation. howeverHowever people have no trouble in understanding the language, 
 we use common sense knowledge. see forFor example when this was shown you said this is all garbage. mightMight be if this statement is from some scribal tribal language iI do not know might be then they would have immediately understood that
 because of their common sense reasoning and because of their knowledge abobout that language. alright similarlySimilarly if you show me a friend French statement and if iI  can not recognize that iI will not be able to understand it. 
soSo what is required is essentially common sense knowledge, reasoning capacity and experience.  andAnd computers have by itself no common sense knowledge and no reasoning capacity unless we teach them. ok 
now thatThat is the overall scenario 
 so that makes a natural language processing a little difficult. Now, if we lookok at the different areas of computer Computer science Science research you will see thate computer Computer science Science deals with database, is algorithms, networking etc. and artificial Artificial iIntelligence that is the course that we are working on it 
now has got several components in it like robobotics, search and expert systems that we have seen and natural language processing is one very important area of artificial Artificial Iintelligence. and And why is natural language processing is important? It is important because of many important applications like information retrieval, machine translation, language analysis etc. now wWhen we carry out language analysis that again includes understanding the syntax of semantics of the language in of any text that is written 
refer slide time (14:00)
 
as well as parsing. justJust as we like write compilers to parse c C statements similarly we will see the difficulties that are inherently there for parsing natural language sentences. 
in this lecture and the next it will not be possible to cover the vastness of the different algorithms that have been developed for natural language processing 
the objective of the lecture is to introduce to theLet us look into the problems of natural language processing and some approaches that are adopted.
 
and show you some approach of the adopted and there is a vast open field lying head for those who are really interested in artificial intelligence and natural language processing to do degen more and see more of the subject so informationInformation retrieval, machine translation and
 understandingunderstanding the language togetherall this forms a part of natural language processing. 
now whenWhen we lookok at natural language processing there are different levels 
refer slide time (7:26)
  
in which we can think of it. weWe can say that these are the different  linguistic levels of analysis. say we need to speechSpeech is one level and that is at the top level. Iif we can generate text from spokoken speech then all theise entrances utterances the waveeb forms that we to generate when we talk have to be interpreted and captured and translated in the form of written text. thatThat is one level. evenEven after that we have got the written level. andAnd in the written language there areis again different components phonology. whatWhat is phonology? Phonology deals with sounds, letters, pronunciation etc. now wWhen we say god or got the meaning totally changes because of the last phoniphoneme.cs ok insteadInstead of pronouncing d we are pronouncing t and the entire meaning changes. theThe other thing is morphology. thatThat is the structure of the words. It is like child becomes children when we make it plural. But alright that bobookok becomes bobookoks. so So, based on this we have to also understand that this morphological change, morphology is the structure the lookok it has changed and  
alright with that changed the meaning has also changed. Therefore this ok so that is also another level.
 syntaxSyntax: now syntax Syntax is essentially dealing with grammar. howHow theise sequences are structured? andAnd the semantics are deals with the meaning of the strings. 
when we go on atAt the best ultimate level of understanding we have to understand the meaning of the strings.s theThe syntax is telling us what, wheather those strings the symbobol strings are well formed. weWe have to look into we have categorize in to different components of theose symbobols strings when we do syntacticmantic analysis.  	thatThat also helps in understanding the meaning. butBut with that we will understand some part of the meaning. and so w could say that there are two  Therefore we have two levels as i said at the abstract level speech and written language. And inside the  taking an insight written language there are the chronological aspects, morphologyical aspects, syntactical aspects and semantics aspects and there is a continuous interaction from among all theise levels. next we start refer slide time (18:19)
  


discussing about the issueIssues in syntax: ok say wWe take a very funny statement: Tthe dog ate my homework. nowNow iI am sure all of you will laugh at this sentence theso dog ate my home work. whatWhat does is it mean? butBut syntax per sayve is not dealing with the meaning of the sentence. itIt is trying to see wheather this sentence is syntactically or grammatically well formed. 
theThe meaning may be useless here we can but the syntax should allow meif it is a correct syntax then it should allow me to understand theare answer.s questionsQuestions like who did what? 
 whoWho ate? theThe dog ate, the dog ate what? theThe dog ate homework, whose homework homework? myMy homework. soSo, in all these you can see see that the components of the sentence have got a relationship among themselves and that relationship is preservedferred in theis sentence structure 
 so strictly speaking this sentence is syntactically correct. 
so So, when we perform syntactical analysis one very important parts is to identify the parts of speech 	 of a sentence. partsParts of speech is in school level all of you were exposed to that that when ever 
we talk abobout a sentence then we talk abobout noun, verb, pronoun, adjective, adverb etc all thiese are known of as parts of speech and they identify the relationship ofr the role of that word in that particular sentence. forFor example, in this case the dog is noun, ate is verb, homework is noun. nNow if iI could write a program that would accept a sentence and could and will be able to level each component of the sentence with the proper parts of speech then that program will be called a part of speech tagger, to poPOSs tagger or
refer slide time  (20:26)

  
part of speech tagging.y ok Iin English part of speech taggers exist with around ninety five percent95% accuracy but its work worse wit ish still going on where it can be improved. forFor Indian languages also now-a- days  we are working on part of speech taggingy and here also we are getting more than 95%ninety percent accuracy by now. 
alsoAlso the words may not always be should not always be lookoked at isolation looked in isolation, we have to we may in order to understand meaning we may some times require to lookok at group of a words. forFor example, if there are three words mother -in -law then we have to we can not just lookok at mother and try to find out its parts of speech mother is a noun in is  a preposition 
 and law is another noun. butBut in order to understand mother- in -law is actually mapping in to another concept. Ddog means something that i’s an animal, hot has got  a friend different concept but when in a sentence these two words are put side by side when they are allocated collocated then these should be lookoked upon together because hot dog is a particular type of fast food. So, in order to understand the sentences we have to understand the parts of speech. butBut also that is  not the only thing and there are many other issues involved; oone is colloacation and this identifying of collaocation is also known as chunking or local word grouping. now thereThere are more issues in 
refer slide time (22:57)
 
Ssyntax. Wwe would like to do i say we would like to do a shallow parsing where we just like to find out which one is the subject, which one is the noun pharase etc. sSo what we can do is, the dog chased the bear this is a sentence now when this sentence is shallow parsed we will first break up this sentence and identify that the this sentence the dog is the subject and chased the bear is the predicted ate. andAnd then the subject is the basic structure of theis subject is that it is a noun phraseframe the dog and predicate is the a verb pharase chase the bear. now sShallow parsing is often adopted when we try to understand a new language and now the works that on asare on,  you will see that in order to have a quick and good pharsing we need to have one very important approach is again is a statistical approach where we need a lot of training data to understand that given a word what is the its probability of being a noun, what is itsthe probability of becoming of mapping it to a particular concept. alright now thi s picture may not be a very clear to you but i will be working it out here is agaiHere is an an example of pharsing 
refer slide time (24:45)
 
of a sentence say johnJohn loves maryMary. so since the picture is not clear i will be working out myMy sentence with which we start which we start is johnJohn john loves maryMary. nowNow, when we pharse this sentence first thing is I will break it up as the a noun phrasearse. The parse the parts will break it up as a noun phrarse and a verb phraarse. and And this noun phrarse is this part johnJohn and the verb phrase is loves maryMary. 
andAnd this noun phrarse is further brokoken down as a noun and that noun is johnJohn. soSo that is coming to the terminal symbobol johnJohn   
and the verb parse can be further brokoken down in to the verb and the noun phrarse. and And this is verb is loves and the noun phrarse is the noun which is maryMary. all right soSo this isd the full pharse tree andso this is a terminal symbobol, this is a terminal symbobol, and this is a terminal symbol. all right soSo, this is the a pharse tree. nowNow one interesting thing is, lookok at this loves, we know the meaning of love as a verb. nowNow an intelligent system should be able to understand that when it is loves it is essentially a morphological change of this same verb love and because it is a third person singular number that why this addition of s has are common thesecome and these two are basically [….]. 27:02. now i will just give you another example of similar parts
refer slide time (27:07)
 
say Here is another example of similar parts: Tthe dog or the boboy went went to school. nowNow how do you pharse it? 
againAgain it will be brokoken down to noun the phrarse which is the noun pharse this is the boboy and it will be a verb phrarse which will be this went to school. andAnd this the boboy will be further can be further brokoken down as a determiner or article and a noun. andAnd theis determiner is the which is thethe terminal and the nouns is boboy. Tthe verb phrarse can be brokoken down to a verb which is went and it will have another noun phrase what is the noun pharse thiswhich is this two school alright and this noun phrarse can be further brokoken down as a prepositional phrarse and a noun. And noun and this noun is terminal school and this preposition replace asphrase has  got a preposition to. 
so Therefore this is how we pharse it but it is still there are many issues. because  forFor example, again let us see went, what is this went? Tthe went is the same thing as go in the past tense that has to be known that knowledge is required in order to understand thise sentence. butBut by this phrarsing what have you got? weWe have to  got the we have been able to understand the role of each of theise words that ok the boboy is a noun phrarse so that is tthe subject of the sentence. ok thisThis is the verb went is the verb. 
soSo pharsing is one, so this helps in automatically figuring out automatically questions like who did what and when. ifIf we ask johnJohn loves maryMary then if we ask the question who loves maryMary from this pharsing we can straightly away come to the subject and say the well johnJohn loves maryMary. 
thereThere are more issues in syntax. oneOne interesting thing is, this is a very interesting example anaphora resolution. weWe have got we always got have this sort of sentences; the dog entered my room it scared me. nowNow when youwe lookok at the it which is the a pronoun, 
refer slide time (30:17)

 

now to which noun does the pronoun all of you we know that a pronoun stand s for a noun. now inIn this case the system must if iI ask the question who scared you or who scared me the system should not answer it, the system should answer the dog. soSo the computer must understand the system must understand that this pronoun is actually related to this noun that 	noun the dog. ok 
soSo this is this sort of problems arethese sorts of problems are called the anaphora and when the second one is a more interesting problem; 
 iI saw the man in the park with a telescope. see  Therethis is a lot of problem here. iI saw the man with in thea park in thewith a telescope. iI can interpreted it in different ways that iI could see the man because iI had a telescope iI saw with a the telescope and i saw the man in the park. oOr this could be related to the man that iI saw the man in thewith a telescope in the part,  the a man was carrying the telescope. orOr the other thing is,that iI saw thethe man in the park with awhich had at a telescope. ok thTheise are the three possibilities. nowNow which one of these isare really meant? now evenEven for us there are two we can understand that in park withit is very improbable withto have a park with a telescope but of course i mean it is possible that you can have in a special park astro withfor some astronomic fair you can put in a telescope that is also possible. butBut probably iI saw the man with a telescope iI saw the with a telescope the man in the park, so iI used to a telescope to see the man or the other possibility was that iI saw the man was carrying a telescope. 
so you see thereTherefore the scope of lot of ambiguity is here. no there is lot of scope of lot of ambiguities here nowNow similarly there are several issues in the semantics. now theThe earlier one was syntax. But ok even if we resolve some parts of the syntax there are several issues in semantics. For example, say semantics means say 
refer slide time (32:57)
 
uUnderstanding the meaning of the sentence. howHow do we understand the meaning of the sentence? say theThe same word plant can mean an industrial plant, a chemical plant, electrical plant or whatever. plantPlant is also a living of organism. 
soSo often the words that we use are ambiguous. say whatWhat is t h e importance of semantics? unlessUnless we really understand the meaning then we will not be able to translate them there will be wrong translations. ifIf we try to carryout information retrieval there will be wrong information that will be extracting. andAnd if we can not do anaphora resolution properly then we will also  wronghave wrong reference. nowNow how do we learn the meaning of the wordswords? oneOne thing is, the from the dictionaries we can have the plants, plant works,  industrial plants all these things can be putting in as buildings for carrying on industrial labobor. Tthey buildt a large plant to manufacture automtomobiles. so thisTherefore these sort of sentences or glasses clauses we have will map to this. butBut again there is an entry in the dictionary plant, flora, plant life, a living organism lacking power of locomotion is a plant. 
nowNow when we say a sentence we give thestate a sentence they are producing abobout one thousand1,000 automtomobiles in the new plant 
refer slide time (34:00)
 
then which one of these my meaning would my system choose? iI have to resolve that and iI have to really understand that since iI am talking of automtomobiles and automtomobiles require some industrial labobor and a large plant obviously this new plant means an industrial plant where cars have beenare being manufactured. iI will obviously not interpret it as a plant in the form of a small tree. againAgain for this sentences the sea flora consists of thousandin 1,000 different plant species. nowNow we have to understand that thiese plants species is not the industrial plant but this sense of the plant. 
nowNow what happens when  is this the plant was close to the farm of animals? the plant itIt may be an industrial plant or iI am talking abobout a particular plant. 
so you see Therefore gradually the context is also becoming important. mea rely the roleAnd when iI do the phrarsing  
mearelymerely the role of the sentence is not enough. theThe role of the components of the sentences the part of the speech is not  enough. 
soSo what you do how do weyou really understand meanings? thereThere are several issues which are involved and it is rather complex. oneOne approach as iI said which is very popular now-a-days is the statistical approach. thereThere we start with a set of examples which are annotated. thatThat means in a sentence like this; they are producing aboboveut 1,000 thousand automtomobiles in the new plant.  
refer slide time (24:45)
  

inIn this sentence I manually ian notated it and say the in first meaning of this plant is actually industrial plant. not the second iI annotate many subch sentences. alright and And a system may be designed using a number of tools like them Hidden Markov mModel and other technology.  which will not cover on this course thereThere are technologyis technology available using this we canwhich  the system can learn the probability of a meaning given a particular structure of a sentence.
 so supposeSuppose we take hundred examples containing the word plant which are previously tagged by a human with  thea meanings then we train frame a learning algorithm.  there are several learning algorithms including the neural net algorithms there are different classification algorithm, there are mark ofMarkov M models, Hidden Markov Model algorithms etc where there arethat different approaches. soSo we will train a learning algorithm and using that algorithm when new sentences are fed we expect that this algorithm will try to find the closest meaning statistically. rightRight now the precision is in this is a little more than eighty percent80% than lot of research is going on in this area and how to choose a learning algorithm is an issue.
shown so oneOne thing you must realize is that natural and thisis sort of approaches requires the human tagging so you can not have any soft path to success. tThere is a necessity of putting in the labobor of tagging them properly at least a small set on which we can later on boputok scrap but we will have to make a small set properly annotated or on which we can run ourselves statistical learning algorithms. 
so anotherAnother reason issue is, how to obtain the hundred tagged examples? 
so now weWe were talking of learning using learning algorithms where we will learnlearnt the semantics. nowNow what are the issues in learning the semantics? weWe can assume a large amount of annotated data for training. alright we say  
refer slide time (39:28)
 
We train our all algorithm and then we take a set of new texts which is not annotated which a set of new text which will be used for the test. 
soSo our approach is learnlearning from the previous experience training to classify the new data.
theThe test data will be annotated automtomatically based on what ever our system  system has learnted from the corpus of already annotated data. alright soSo there are different algorithms for that like decision trees, memory based learning, neural net works etc. w hich some of them are covered in this course with machine learning have been discussed 
refer slide time (40:23)
 
besides thereThere are several other issues in information extraction. 
the purpose of the lecture as i said is to provide you a glimpse Here you would learn of the different aspects of natural language processing which are very useful now in the presented day a technology. say weWe are given a sentence; there was a group of abobout eight 8 to nine 9 people close to the entrance on highway seventy five75. 
Now, from this a computer program should be able to extract the information like the answer to the questions who? eight 8 to nine 9 people, where? onOn highway seventy five75. alright soSo this sort of answers should be extracted from this sentence where you see it is not just a very straight way case, where? close Close to the entrance on highway seventy five75 
exactExtract the information and also it requires detecting the detect new patterns. thereThere are issues in information retrieval as well. from the huge 
refer slide time (41:40)
 
fromFrom the huge bank of information that we have in the internet how do wewe extract out information that is relevant thati’s an issue to think of. say generalGeneral model of information retrieval is that we start with a huge collection of texts and we have got a query and our task is to find the documents that are relevant to the given query. weWe have to find out the documents which are relevant  torelevant to the given query. thereThere are different measures to really compute which are relevant. ok forFor example, there are text texts on cricket alright and iI want to find out the texts only which talk abobout sachin Sachin tendulkar Tendulkar. then one measureAnd because the my query in the case is sachin Sachin tendulkar Tendulkar in that case out of all these texts that are there iI have to evaluate the or rank all these called texts based on the keyword of my search which is sachin tendulkar Sachin Tendulkar. A veryof any common sense the approach would be the text which hasve got larger number of occurrences of the keyword sachin tendulkar Sachin Tendulkar will be more relevant. mMay be there is a text in which a lot of things about cricket hasve been said and sachin tendulkar Sachin Tendulkar’s name is have been mentioned only twice and it is talking abobout many other thing so that particular text will not be that relevant for me. 
so that is one approach to beTherefore this is a very simplistic approach that we select the keyword and based on that we find out which what one is relevant. soSo we usually find out the documents through creating the index like the index in a bobookok and application of such information retrieval we will find in
finding most of the search engines like googleGoogle, yahoo Yahoo, altavistaAltaVista etc.everything 
now The main issues in information retrieval are, we have to index the meaning because as we have seen one word can have multiple meanings. 
refer slide time (45:45)
 
inIn that case which meaning should iI should i assume this word for twoto refer to? forFor example, if iI say iI went to the bank to see the sunset, bank may be the typical bank where we keep money 
alright aand bank can be a river bank also. 
soSo this is one of ambiguities which is known as poly semi.
polyPoly semi means when a particular word can have multiple meanings.
 considerConsider the word bank 
you can see that the we will have different meanings like bank can be  will have bank can be say cache of money alright like the things that we havethe Sstate bBank of India, alright another thing can be a river bank,  
anotheranother thing can be  bankbe bank can mean depend for example iI banked on him alright i banked on himhim.
Now, when iI am given the sentence iI went to the bank to observe sunset then this is my sentence which i am trying to understand which is beingmy machine is trying to understand. nowNow you can see that just by syntactic analysis through the normal phrarsing iI will find out that this bank is basically a noun where as where the meaning of this meaning depend is  a verbword right so this meaning is not applicable. so you see here thatTherefore here you can find that by syntactic analysis itself without going in to a any semantics analysis we have been able to eliminate some of the meanings which are not applicable. ok so let me elimTherefore here we elimiinate this, iI eliminate is part, still iI an m left with the a problem of two possible meanings with  that both oof them are noun, this is noun, this is also noun and this part of speech is also noun. nowNow which of theise meanings should iI take? ok
Now hereHere iI need some more knowledge in order to solve this problem and one sort of knowledge is require that that knowledge will be 
refer slide time (47:43)
 
from the context ok that sunset is usuallyy what observed near a river or sea. soSo the probability of this bank being a river bank is higher in this context because this bank is associated with this phraseplace to observe sunset. alright ifIf the sentence was iI went ot to the bank to withdraw money then obviously if theis sentence was not there 	
andAnd instead if iI hadve written I went to the bank to with draw money then in that case this bank would have related to this meaning because of the association. nowNow one artificial intelligence knowledge discussion representation structure that we have already learnt and ccomes in over invery handy over here that is the a semantic net. theThe semantic net represents the meaning. Therefore if in our so if we know the knowledge base we have our d semantics  net then in that case we can handle theise sort out of situations. For fro example iI can have, say bank i hope you remember semantics you have forgotten tat is an say institution which stores money, bank say e is close to river, bank is visited by people,  
watch sunset. saySay iI have got this strength sort of structure;now i I went to the bank went to the bank to with draw money. whenWhen iI try to understand dealing the meaning of this sentence iI will traverse i ccan utilize this semantic structure and iI will find that there is a bitter relevance because it is leading me to the concept money which is already over here and so iI will take that meaning of bank in order to interpret the sentence. soSo this sort of knowledge is required for retrieving information from the retrieving information from the well web.or thereThere are other issues like this soame thing is can be very useful for question and answering. For example, say what is the height of Mmount Everest? eleven thousand11,000 ft. feet nowNow in the current state of the art of the information retrieval accurately where the decision precision and recall where toare the two very important measures which isis around forty fifty p40 to 50%. ercent thereThere is a lot of scope of research and lot of scope of work on in this area.
weWe improve the precision with the use  ofuse of more common sense knowledge and often we capture this in the form common sense knowledge knowledge in the form of knowledge representation. thereThere are other issues like cross language information retrieval. thatThat means we want to find the information across languages. 
forFor example, what is the minimum requirement for car rental in italyItaly? 
nowNow with this query when iI search the documents that are available in the web it is not to sufficient to restrict myself to the documents which are written in English only 
 because since iI want to get the information abobout some law in Italy there may be some Italian documents written in the Italian language which may come in very handy for me.from the 
butBut iI understand English  so and my query has been given in English. thereThere fore what is needed is that iI will accept the query in English and iI will go to the documents search documents which are English documents. iI will also search Italian documents. nowNow the query is in English so iI need to have some sort of translation at this point or do someI must use some other technique. 
So, from here iI will be getting some documents which are Italian documents and from here iI will be getting some English documents. 
nowNow my answer should be theise English documents and theise Italian documents will require to be translated m tMT means machine translation and ultimately iI want to have the answers in English because iI know only English. thisThis is cross language information retrieval. 
refer slide time (24:45)
 
ok soSo we will have to convert this to some Italian text in order to do this sort of translation. now just to complete the lectures i will mention about the some of the 
issuesIssues in machine translations: Tthere are text to text machine translations. thatThat is, givenn a English iI want to get hindiHindi or given hind iHindi I want to get theat document in English or speech to speech machine translation that is the ultimate dream that all of us cherish specially in a country like India. say For example,  i speech to speech 	
refer slide time (54:47)
 
i speech to speech that is the ideal but still we are still quite far from reaching the desired goal role here.
 say hereHere iI accept some hindiHindi in the speech alright and that hindiHindi speech is recognized by automtomatic speech recognizer ASR a s r and from there iI get hindiHindi text. and And that hindiHindi text can be translated by some machine translation to English and then the using some Ttext tTo sSpeech system t t sTTS English or may be other Indian languages like which is like bengaliBengali and there is a tText tTo Sspeech system t t sTTS that  stands for text to speech will convert will speak out theis English ofbut  the Bengali sentence to another person so iI am getting bengaliBengali speech or English speech. so So you just try to imagine if theise were possible then in that case a person speaking hindiHindi would be able to communicate with the  person speaking English and vice versa. That is, when he  from English speech when he speaks English then it goes to an English a s rASR and iI get a hindiHindi and that hindiHindi is again converted to a hindiHindi t t sTTS and that can be spokoken out as hindiHindi speech. 
youYou can see the advantage of this. that one touch of this and to speech to speech  
refer slide time (56:45)
 
Therefore we and as you have seen that in this case we also needed not only the we needed automtomatic speech recognition, we needed text to speech, we needed machine translation and all these are in the domain of natural language processing and our present day represent the challenges. 
so in the next lectures we will see we will concentrate on natural language understanding how some of the approaches how natural language can be understood 
thank you

Trancripted by p.sasikala
Lecture time (56:13)

ARTIFICIAL INTELLEGENCE
By
Prof.Anupam Bbasu
Department of Computer science and  Engineeringand Engineering
IIT Kharagpur

Lecture – 40
Natural language processing – II (TIME 00:44)

so thisThis will be last lecturecourse oofn this course Artificial Intelligence and ion this lecture we will be dealing with the remaining issues some of the issues of natural language processing.
 inIn the previous lecture on we were trying to bring out some of the issues.
 we will dwell on theLet us discuss issues on machine translation little bit today and then we will discussand discuss the  natural language understanding so today we will be discussing remaining part of natural language processing.
toTo start with let us have a look at the issues involved in machine translations 
because we have seen that machine translation is a very key very important aspect of many natural language applications like speech to speech, text to speech, even for text to speech from another language, information retrieval, cross language information retrieval. 
machine translation becomes very important issue and rightRight now in India which is a multilingual country a lot of effort has been extended panded towards development of machine translators. iitIIT Kkanpur has developed angloAngla bharathi a machine translatorion from English to hindiHindi and there are other efforts at different iitsIITs like iitIIT Kkharagpur and see thereC-DAC where machine translation action can activities have been be carried out machine translation and also at triple iitIIIT Hyderabad  about sSakthi a machine translatedor has 
been developed. now when youWhen we talk about machine translation it can be text to text machine translation. thatThat means say a text is given in hindiHindi and that has been translated into Bengali or it can be from speech to speech so speech to text and then machine translation at the text level and then again converting that to speech.ying them to speech discussed in the last lecture 
nowNow most of the work that has been done has addressed pairs of widely spread languages like pairs of language English French, English Chinese, English German, as i said English hindiHindi etc. 
Wwe are working on English to Bengali and thisat sort of activities are gone in .right Right now works are also been carried out from hindiHindi to English, hindiHindi to Bengali and government of India has taken a lot of initiate with in these activities. 
nowNow how to translate a text? thereThere are different approaches. Example, base translations where we have got some templates and given up a particular sentence we try to fit it to a template and that template will tell us the best way to translate it. butBut the most commonly used approach is statistical 
refer slide time (03:56)
 
where we learn again from the previously translate data. nowNow where from can we get previously translated data? 
forFor that we need parallel corporaparallel corpora.   whatWhat is the a parallel corpora?    aA parallel corpora   is a collection of the peair-wiseer white sentences.  forFor example, there is a English sentence and the corresponding hindiHindi translation ofn that which is se and also that is the sentence wise and  but also word level. everyEvery word having it is  a translated form so it may appear to be very simple then but it is not so because of the several reasons which we discussed earlier earlier lecturesthat  there are so many issues of resolving ambiguities and other things. So Ffror French and English and fromfor  Chinese to English there are required tools and there has been reasonable translations. When a  where trtranslation is being done by a machine we are not and now  expecting perfect translation, we are not also right now thinking of translating literature keeping the flavor which the human beings also cannot do most often. so butBut if we just restrict ourselves to some routine day- to- day sentences without special levels then we get reasonably good translations here. Chinese to hindiHindi for example there are no such tools there are efforts going on for translating for building corpora parallel corpora for English hindiHindi, Bengali hindiHindi etc. 
now a major level 

Now a major laborious notorious task is how to obtain the parallel texts? canCan you get it from the web? inIn the web most of the contents are in English so how do we get the contents from same contents in hindiHindi? thatThat is difficult but there are some cases where we get parallel corpora in many government records where those are also those are stored both in hindiHindi and English 
 so that may be one approach. 
now onceOnce we have the text how to get the most out of them? weWe have to do the word alignments. because weWe have to really align the hindiHindi word with the corresponding English word. weWe have to obtain the proper lexicons 
 and also we will have to input some knowledge from well studied language. as we will discus little later that 
inIn spite of all theise techniques the domain knowledge will be required and very helpful for better performance. next with this approach with this overview ofFrom  machine translation which is a very important activity  we now move on to the another respect of natural language processing that is natural language understanding. 
refer slide time (07:07)
 
now understandingUnderstanding natural language is very important. because if we want to youYou have seen thea dialog in my seen rightMYCIN. so that was not a inIn a very strict sense that was a not a natural language understanding program but in a restrictedive sense yes because for the small domain it interpreted in particular way in a very restricted format. but if we want to make a dialog system for example iI type in some query and the system will understand, it is not a quitethe sort of query that we give in Google, iI can type in natural language query like for example what would be the best for possible crane train if iI would like to go to Delhi may be through agraAgra or may be through Allahabad? It is this sort of construct.
that sort of construct Nnow, if we type that thing in and say let me or modify the sentence in little bita little bit, what will be the best way have to go from Delhi tohrough Bombay possibly through Allahabad or possible through some other place at a low cost?
 nowNow we would like to have the system, process this query understanding what exactly iI want and respond back to me. now naturalNatural language s often being very difficult natural language understanding can often be very difficult.
because will see why nowNow our objective is to make the computers react intelligently to human speech, speech is the ultimate but as an intermediate we can assume that we are typeing in text. thisThis is the most natural interface to computers. fFor example in database query now we type in standard s q lsql query in the s q l language but the ultimate aim could that be that iI am just asking for the query, i am just giving the speech query and iI am getting the answer or iI am writing the query in a free format of English or iand I want the answer. ok
refer slide time (09:12)

 
nowNow the research that is involved in theat belongs to 
refer slide time (09:40)
 
two disciplines; one One is a iAI who are focusing on programs that will react to language 
and linguistics isthat the discipline studying the human language used. bothBoth of them both thisthese disciplines must collaborates hand in hand in order to get a good result. verbalVerbal speech recognition that is related but it also requires some digital signal processing related and other things where we have to extract the speech signals.  
refer slide time (07:07)
 
must be rememberedThe point to remember is, that all natural languages are as complex as the other. theTheyre cannot be any language which is is very simple and the other is very complex. andAnd no natural language is best suited towards model ling or easier to process. 
andAnd the vocabulary of the different languages arevocabulary of the different languages is different and that is because of the different social factors. nowNow relatively easy to derive natural language parsers are grammar for constrained englishEnglish but not for general English however because in natural language we will get  a lot of flexibility and we can use some dictionary orand lexicon of words and their grammatical usage. 
grammarGrammar can both  recognize and generate sentences, we can use grammar to do that. 
anotherAnother very important issue is to understand the meaning of the sentences,  semantics.  thatThat is much more complex. asAs we have shown earlier that  are phrarsing a sentence helps in some way        refer slide time (11:24)
 
to understand which is part is the subject of the sentence, which part is the object and which part is the verb etc.  
asAs in the earlier example, we have shown in last lectures that from the sentence iI went to the bank to enjoy sunset. weWe could buy syntactic method we could eliminate the possibility of bank from being a verb like depend that was eliminated but true understanding requires much more common sense knowledge, the relation, the context  content and all these issues. 
say ifIf iI just take theis simple sentence maryMary ate spaghetti with georgeGeorge alright now this and if iI 
write maryMary ate spaghetti with chopstick now what is the difference between these two sentences? theThe structure is same, x ate y with z that is the structure butstructure. But since georgeGeorge is the a person  then Maryand basically it means Mary and George ate   spaghetti together 
refer slide time (12:42)
 
where as in the other case maryMary ate spaghetti using of a chopstick.s soSo what do we really mean by understanding a sentence? oneOne way we can say that whene can we listen to a sentence we probably make a picturizationeurizations make a perception of that. mayMay be if we think of a large semantics net describing the world in our mind some of the nodes of thosee semantics net gets activated and we get some sub tree model. butBut still it is aan open question that what we really mean by understanding a sentence?  And an indirect way might be that if iI really understand the a sentence properly then iI can reply to that or respond to that properly.
 nowNow we look at another sentence: when When the balloon touched the light bulb it broke. whatWhat broke? aA balloon touched the light bulb and the light bulb broke. or is it when the balloon touched the light bulb the balloon broke? ok exploreExplore it because it touched the hot light bulb.  nowNow how do you understand that? howHow the machine understands that? weWe immediately assume that the light bulb cannot be broken by the touch of a balloon but it is the balloon that got broken. butBut for in order to understand this we need so much of domain specific knowledge that are related that a balloon is a light object 
 and the a balloon can not be balloon is a light object and a balloon cannot break the a light bulb which is the a stronger object etc. so So how do you go about it? say again whenWhen the balloon touched the light bulb it broke. nowNow we are going to a discourse that is a set of sentences. thisThis caused the baby to cry. nowNow what is this and this? theThe balloon caused or the event of the balloon getting broken caused the baby to cry? obviouslyObviously as a human being we know that this the balloon got broken and that madke the baby cry. butBut how do you know that? make the baby cry weWe know that because we have got a domain knowledge which has got thea information that a baby likes the balloon. andAnd if any thing that is light is broken then the person feels sad. thatThat may be stored in semantic net or may be in  nature or in some other forms or may be in the form of rules. ok  now you see another pointAnother point is,  when the balloon touched the light bulb it broke. nowNow if there was some ambiguity to which there was amy system was was a little hesitant to really decide on what broke probably the second sentence actually helps to resolve this ambiguity in this case. But in some cases it can create further ambuiguhityies as well. 
maryMary gave johnJohn a dirty look and picked up the baby. probablyProbably you can picteurizze thate johnJohn is thea father and who led the don’t balloon go and it touched the light bulb and it broke. soSo the maryMary the mother was really annoyed with johnJohn and johnJohn shrugged and picked the balloon. 
now Now what does it mean? now withWith this sentence so you see there isare so many scope of interpretation but we are interpreting it so naturally because of the deep knowledge of the domain or the world that we have with us us.
nowNow let us see how the system will react to some questions. whichWhich one got broken? 
 supposeSuppose the system says balloon then all right who cried? theThe answer is baby, was anyone angry? nowNow say by tbyhe reasoning that i was orally describing where iI expected the background knowledge to be there iI can get the answers and these answers are directly this key words are directly in thise sentence. butBut was anyone? Nowhere  angry that is nowhere written that maryMary was angry and or johnJohn was angry. butBut that is subject to interpretation becausethat maryMary gave johnJohn a dirty look. and you see theThe deeper knowledge and or the more complicated scenario we are going on going in that giving a dirty look implies that some one is angry. 
nowNow the issue is, in the real world there are so many possibilities. inIn a rule based system is if you thing of can you really store all these possibilities? howHow much will you store? andAnd can you ever make it complete? soSo that makes athe entire scenario will be very difficult and this is one side. anotherAnother side is that natural language gradually evolves, new words are coming in, new phrases are coming in, new expressions are coming in so how tdo deal with it that? That is why there is an attempt to go towards statistical reasoning,  statistical method, statistical learning and translation which is helping to have a quick fit solution though not always very correct but mostly it works fine . did johnJohn care ? johnJohn shrugged and picked the balloon.
refer slide time (18:40)
 
soSo, johnJohn probably cared more for the balloon and not for the baby but that has to be inferred and this on the the inferences is quite complex.
theThe next thing is say the same thing and; was Was the light bulb hot? theThe answer is yes. nowNow how did you get it as yes? nowNow here it is also subject to interpretation and inference because it is coming from the domain ofr physics or domain of the real world that when a balloon when it touches the light bulb the light bulb  if on it was kept on then it is hot then only a balloon can break. alright ifIf the light bulb was off then obviously it will will not break. butBut here weather the light bulb was on or not is not the issue since it broke we infer that thee light bulb must be hot. wasWas the balloon inflated? yesYes and how did you get that otherwise if it is not inflated how do you say that it broke? wasWas the balloon exploded? i mean that s This is another question. wasWas johnJohn concerned? Questions as such  these are all subject to interpretation and complicated inference. whoWho was responsible for the baby crying? johnJohn? Iin a way yes 
but actually the balloon was responsible but since maryMary gave a johnJohn a dirty look so iit tells us that obviously johnJohn had something to do with the balloon going off and touching the light bulb. 
refer slide time (19:07)
 
ok soTherefore it is a much deeper inference that gives theis answer chance of johnJohn. 
soSo if we say that we were trying to parse the a paragraph 
refer slide time (20:22)
  
then all sentences we find are related to find one another to different degrees. andAnd we have to denote the relationship among the components that we are getting after parsing the individual sentences. 
see firstFirst of all we are getting the components as words and we try to find the partnerships.  alright butBut when theise words get embedded in a sentence then also there are some ambiguities and we have to resolve them. and when nowNow there is a paragraphcharacter so now we will have to discuss about inter sentence relationshiped to 
 and may be when we are going for discourse interesting consisting of a number of paragraphs we will have to move will have to establish inter paragraph relationships and in that way it gets more complicated.  inter complicated as we have shown that thereThere are some implicit knowledge  of about the universe and thus the knowledge representation is very crucial about the whole thing. Natural language systems like see early toy systems suchwere natural language system attempted for a long time. One of the one of the well known systems areis Eliza.  ok itIt pretends to be a psychologist and talks to the user. whatWhat does it do? it goes it do itIt parses a sentence and looks for key words and phrases. all right The user for example says iI feel angry today. 
nowNow, feel is the keyword that elizaEliza notes knows and feel so elizaEliza 
refer slide time (22:22)
 
can there fore respond as, tell me why you feel angry today? soSo it takes this the entire component theis entire part of the sentence feel angry today and just replaces iI with u so you feel angry today and this is a very routine mechanical way ofis  forming a question. tellTell me why you feel angry today? actuallyActually it is not deep understanding but in a way it can fool the naïve users and the naïve users can find it very interesting that Eliza is conversing to that. 
nowNow, in order to really build a really good knowledge to understanding system we require 
refer slide time (23:22) 

 

knowledge representation and we require knowledge structures. weWe have to have some internal representations for representing the meanings of theise sentences. 
weWe have to have some kind of inference generation, we have to do some syntax and semantics analysis. 
soSo if we look at sentence analysis here is a serial flow of control
refer slide time (23:52)
 
we need many things but here only a few things are enlisted. all right so weWe have got an input sentence here and that is undergoing syntactic analysis and for that we are using the grammar ok, this grammar is being used and consequently we are getting the parse tree ok we are getting the parse tree a as the out put of this syntactic analysis alright andanalysis. And this is followed, this parse tree as we saidat the syntactic level after doing some part of speech tagging we can handle or we can deal with some of the semantic issues. butBut there are many more semantic issues which cannot be handled at this level. thisThis is followed by semantic analysis. andAnd this is a complex problem where we will be needing features of the domain. ok we will be needing features of the domain andAnd after that we need some pragmatic analysis. Some thing’s are really not possible to be uttered in a particular context those are pragmatic considerations and alsothough  this we have to carry out inferences in order to discover the meanings. In the previous example of the  as we have shown in the earlier example that balloon breaking issue in that case we have already seen that we often need deep semantic understanding of semanticor we need a deeper inference to be carried out in order to understand because many of the things are implicit and need to be inferred and often these inferences and are quite complicated. all  right 
nowNow the point is that here we have shown a serial flow of control. Is this is serial flow of control enough? that iI will first do syntactic analysis then iI will do semantic analysis then pragmatic analysis then iI will do inference buts possibly not not always. 
forFor example, if you we look at these two sentences just have a look at these two sentences
refer slide time (25:53)
 
johnJohn took her flowers, ok john took her flowers what is it meanmeans? soSo somebody brought some flowers and johnJohn took that.
A or stranger took her money. 
nowNow theise two takes are different, this is acceptaccepted and this is steael. 
aA stranger stole her money. nowNow semantics and context are used to resolve the syntactic ambiguities printing mistake used to resolve the d should not be there the syntactic ambiguity because here you know this is subject to interpretation. 
say And here when the balloon touched the light bulb it broke this caused the baby to cry. 
maryMary gave johnJohn a dirty look and picked up the baby. 
refer slide time (26:58)
 

johnJohn shrugged and picked the balloon. nowNow we can observe theat seven explicit information are given 
 like:
 theThe balloon was the originally inflated, otherwise it would not have the balloon broke 
the light bulb was hot these are also not very explicit they have got some implicit some little bit of inferences required but that is quite straightforward, the balloon exploded that is straight awayover stated over here but there also you see it broke means what broke? Whether it is the bulb or the balloon? tha tThe anaphora reservation was required to find the proper reference. 
theThe explosion made a loud noise this is inferred nowhere it is said that there was athis noise but this caused may be the noise the baby to cry might be the noise. The baby was scared but that is relatively direct, crying may have direct relationship with being scared, the loud noise scared the baby, maryMary picked the baby to comfort it. 
Here so none of them are actually so much explicit it they require some sort of inference.s ok 
refer slide time (28:22) 
 
now thereThere have been different methods which haves been proposed to carryout an inference foran natural language understanding. 
oneOne way of doing that is the script language which is very helpful to encode stereotypic event sequences. see iI went to a movie last night. 
now goingGoing to a movie is a very stereotype scenario because all of us know that there are some routine things that we must do in order to go to a movie hall and see the movie.
 weWe must have money, we must go to the movie hall, we must get the ticket, and in order to get the ticket we must have enough money so that that they we can pay for the ticket 
and after we get the ticket we proceed towards us to the movie hall, 
 we have to show the ticket to the cashier usher
 and then we need to have to go and sit in the movie hall and watch the movie. and 
so when i come out of the movie hall iI also know that the amount of money that iI had when iI went in the movie hall and when I come out of the movie hall when i went out the amount will be less because iI had to pay for the ticket. now thisThis is a very stereotypic situation. 
anotherAnother very popular stereotypic situation is going to a restaurant. now when i go to a restaurant then i must go to the restaurant beforeBefore I go to a restaurant iI must have money, 
 iI must go to the restaurant, iI must find the seat in the restaurant, 
iI will have to order food and iI have to order food so that the cost of the food is less than the money that iI have, after eating iI have to pay for the food and when iI come out then iI will have less amount of money. theseThese are some various stereotype situations as if drama scriptit has been written. andAnd wheather iI go to a restaurant or you go to a restaurant or i your friend goes to a movie the steps that will be executed are very similar and so such 
that’ is why this is called a script so and as if we are following a script.
 so here i went to ifIf iI say i describe a stereotypic situation using the script language then as soon as iI write that iI went to a movie last night you understand that iI must have had money to buy the ticket 
thatThat is possible tovery infer very easily. 	t heThe ticket was purchased at the theatre in or in the movie hall, iI may have had to wait in the movie line for a bit before iI can go in to the theatre. 
 thatThat is a typical movie hall scenario thaere is at queue and you may have to wait, you can further infer more andone 
refer slide time (31:37)   
 
once inside the theater iI could have bought popcorn, candy or ice cream which are the typical things that are possible, you will not  and obviously you will not assume that inside the movie hall you will buy a chicken dish. alright 
iI exchanged the ticket with an usher who gave me a stub back etc and I etc ect you  can go on. Therefore this  so that is the script language. nowNow, using this sort of e script, language if we look at this balloon script here is a partial description of the 
refer slide time (32:12) 
  
balloon script. so only a partial part so itIt is possible that iI blow up the balloon by mouth and tied the balloon or iI could have bumped the balloon with helium and then tied the balloon. afterAfter tying the balloon the a couple of things can happen. eitherEither the balloon whitherwither away ok the balloon explodes or the balloon flies away. inIn that way we can describe the entire scenario this is very small part of balloon. andAnd we can go on keep building it a complete screen of the drama. ifIf we consider what happened with the child, maryMary, John and the balloon exploded as a drama we can write down a script in the form of some specific language as isf provided by the script language. 
butBut there are a couple of problems in this approach. firstFirst of all the script that you write will be very complicated and often very difficult to handle. anotherAnother issue is that the only script that we find is often not enough. 
inIn order to understand the scenario we have to have some basic idea of where we are trying to reach. and we have to already learned about plant 
refer slide time (33:36)
 
Some so some sort of goal basedest  planning is require d.all right 
butBut there are different levels of planning which will beare often fouind very useful to carry outharacterize stereotype situations 
 because if iI go to a movie hall iI have got a complicatedete script over there stereo typescript  there but my objective is to see the movie and get a seat.
andAnd out of the possible script actionsng that iI would like to take that i may possibility take iI will take the best one that will satisfy my goal faster. forFor example, just you want to go to the movie hall and we you have got different ways of going. Now, in order to understand suppose that a statement is there iI went to the movie hall yesterday night, iI started from my home at in the evening
i I reached the movie hall for the night show just five minutes before the show started and you go on saying different things. nowNow if somebody asks why very were you late? aAnd will the system be able to infer that he must have gone to the movie hall not by taking the a taxi or a cab but probably went by bicycle or by a slower vehicle or slower taken a longer route, and because why would a person choose a longer route, must be he did n’ot have enough money and we he wanted to save money. ifIf you have taken the a cab the amount of money that you would be left with would not be sufficient to buy a ticket. soSo these are all inferences and these inferences might be you can write everything out in the script but that becomes very common cumbersome
 instead if we have got a plan  and you applyied  the planning knowledge also over here you will be in a better situation to handle it. 
sSo the model moral of the story is, in order to build an  effective systemscript script script is one thing the domain knowledge is another thing may be the semantics net’s and all those things were are required, we need the lexicon and may be we need the planning structure and in that way there can be a numerous things 
refer slide time (36:22)
 
which will have to be used together in order to be an effective system. 
next we will discuss aboutHere is a particular approach to be discussed. It is that is not a unique approach but that is one of the approaches that or pronounare found to be very useful in natural language understanding 
that is knknown as case frame s or semantic frames.
 nowNow you have been you are exposed to the frame structure. ok just to have a quick review aA frame is a data structure like a frame boy will consist of different slots like age, height, weight, name etc these are the this has the  slots and boy is a and this is human etc, and in age since iI am saying a boy
 iI can also have some constraint that the age can be any value between say 5 five to fifteento 15 
 beyond which iI will not call him a boy. 
so when iI instantiate the frame the Ttom is a boy then we create an instance of this frame where that we talk about a particular boy tom Tom whose age will be say ten10 the exact data of Ttom maybe five feet5ft and weight may be sixty 60Kgs,kilo name is Ttom etc that is a frame.
 andAnd we have also seen that a frame can be used for and these are the constraints that we can associate to the different slots and we have also seen how the frame based inferencinference is done.e 
how it is done now inIn the case of natural language understanding you see whenever iI state 
refer slide time (38:22)
 
make any sentence the boy went to school now if we take the verb and went is a past tense of go so we can create a frame go which will be any verb and any verb will have different slots and what these slots will be? oneOne thing is is that in this case it is it can have different tenses alright but besides this so in this case went will be an instance of go with past. 
alright butBut going the action of going will require or can generate the question who went? let me write agent right now let me say iI can ask the questions relevant to going. 
whatWhat are the possible questions that iI can askask? 
iI can ask who wentwent. whereWhere did you he go? When? withWith whom etc, all thiese are possible questions I that  can ask.
 similarlySimilarly if iI consider the verb give then associated with give iI can have a number of queries 
refer slide time (40:22)
 
lLike, who gives? givesGives what? toTo whom? Where? How Eetc similarly similarly if iI take the verb eat then also iI can have queries like who eats? eatsEats what? Where? ok say withWith  what? tomTom ate spaghetti 

 with chopstick, with what etc these are some very relevant queries that associated with a particular verb.
refer slide time (41:22)
 
those of you have not forgotten your school level training in Bengali or Hindi or Indian languages you must have heard of the thing which is called the karakaKaraka: ok
inIn Indian languages we got six karakas 
and these are basically one karaka one karaka is mean determined by the relationship that a word with has got with a verb. 
likeLike Ttom went to school, who went to school? 
The answer is Ttom. so tThat the answer that is a nominative case in Indian languages katrikara karaka42:12.
Therefore who, what are like that who and what these are  different roles or in English grammar we also say theseat are different cases so  karaka and case are is the same thing. ok soSo this sort of frames like the frame for eatthese sorts of frames like the frame for eat  with starts with who eats? What Eetc 
these are different cases ok andand that’s why these are called the such frames are called case frames or they are also called semantic frames because in some way whenever we identify the karaka we can answer whenever we identify the case  we can answer some of the questions like say maryMary ate spaghetti with chopstick. now this sentenceAnd if iI have got a verb eat then this particular verb will be an instance of this instance of tense weathereat where the tense will be passed, ok who will be maryMary? whatWhat will be spaghetti?  withWith what will be chopstick chopstick?
now whenWhen we have got this case frame instantiated then obviously if iI ask the questions how did maryMary eat spaghetti? theThe answer will be here and so we can say that maryMary ate spaghetti with chopstick. ifIf we ask what did maryMary ate eat? theThe answer will come directly from the slot of this frame. That i’s why such case frames find 
refer slide time (44:29)
 
such case frames find a lot of applications in natural language understanding. so as i said 
refer slide time (47:22)
 
we were describing now frame knowledge is representationed is a good way to represent someof common sense
 and we can define some stereotypical aspects of that using frames 
refer slide time (47:40)
 
and sentences about theat domain can be parsed and their meaning can be extracted in terms of what the frames expect. What is important here is to know what the frames expect. 
and the meaning can be extracted in terms of what the frames expect because this is important what the frame expect say iI am in a scenario where say again i take the case eat, iI have got all the possible slots what, who, with what, where etc. now each of thiese slots have got an expectation that eating will so who eats this part what ever values fills it up this part who eats have a sentence that has to be an animate object. ok so So if i say the iI cannot just say the spaghetti ate many Mary because spaghetti is ann inanimated object, what eating this must be an in animate object so we know what is expected over here and this helps in dissolving some of the ambiguities alright where we expect a place here so that should be a some sort of a proper noun alright so we expect some nouns phrase here. And withwith what, we expect some instrument or some in animate od object here or it can be also if it is an animate object in that case that will relate to whom? say maryMary ate some things say spaghetti with johnJohn now since johnJohn is animate then obviously the relationship will be with who, that means the meaning will be this will no longer will be an instrument but will be a co agent. theThe agent of eating is maryMary and johnJohn was with him so that is a co agent. inIn that way by with looking at this possible constraintsthese possible constraints and the types of value of this filling of up these this blanks we can approach towards better understanding
 
of the sentences.
 so as as we are showing that isTherefore these case frames are or being builltd around verb phrarse. so So we start with the actions that is the activity of the sentence by the verb phrarse and the thematic role all thiese cases that iI was showing are essential also mentioned as thematic words roles. theThe different words in a sentence i have got a role to play in that entire sentences which could eb any sentence. any sentences it is very interesting way of say anyAny sentence that i say state builds upany sentence a picture. it youYou can think of a different ways, iI make a sentence and there was an expert artist probably heonly you would have drawn that on the canvas immediately 
 so that instead of listing to the sentence iI can look at the picture and understand the same thing. theThe other thing is when ever iI state a sentence since iI have got a good semantic net type of structure that sentenceand if iI understand that sentence the proper nodes of that semantic sentence net will be activated, ok  so this thati’s another way of looking at it. so whenever so iI am just saying in a different way. whenever i say a sentence that is that can always can be enacted soand  there can be different iI always talk about a theme and any theme will have different roles in it, any every component every word of the sentence playsce a particular role in that drama that that the sentence picteurizes. typical thematic roles or for the time being assume that whatWhat ever slots are being we are shshowingn who what these are different thematic roles andthat these are to be filled by a sentence 
and the task is to determineing the thematic roles of noun phrases. 
refer slide time (49:39)
 
now thereThere are many theories that define different semantic thematic roles. howeverHowever the common objective is to understand the sentence typically although is not complete. inIn Indian languages when you we work then we may have a different structure and a little bit of extension of the thematic roles but theise are more likeor less standard agent. 
refer slide time (50:16)
 
agentAgent is the passive or active entity that causes an action. forFor example, in the sentence Donald kicked the ball 
ok Donald is the agent, 
 co agent wisith the partner with the agent. 
forFor example, Donald kicked the ball with friend mickey Mickey.alright 
soSo here mickeyMickey is the co agent agent.
thereThere is a thematic object, what? Donald  kicked what? kickedKicked the ball and this there is the object undergoing change. oftenOften another way of looking at it is that any sentence if iI enact ited that will create a state change. 
Donald kicked the ball so the ball moved from one place to another to otherso the  over all state changed. 
thatThat is also a very interesting way of looking at the whole thing.s soSo the object that is undergoing change is the nothing but thematic object. 
The instrument is the tool used by the agent

 refer slide time (51:16)
 
like Donald kicked the ball with his foot. locationLocation is another thematic role where actionthat is the place where the action clock occurred. Donald kicked the ball on the field. andAnd in this way we can go on adding thematic rules. nowNow let us look at this type dialogue. 

 refer slide time (51:16)
 
again here the verb is give, who is giving? theThe kind man gave a breadbread to the beggar for his hungry child. So, giving iI start with giving that is the past tense, who gives? theThe kind man, hence so the agent slot the agent role is filledeld with the kind man, to whom? Who isich participating in the action? the beggar that fills up the co agent or the beneficiary it is not exactly a co agent but it is a beneficiary beneficial the beggar, gaive what? Tthat is the theme so alright bread fills up here the beggar comes up here or sometimes or sometimes we may like to put the beggar giave the bread to the beggar that is the co agent and for his hungry child who will be ultimately benefited, the hungry child will fill up here and when means the time, time is not specified.
 nowNow if we fill up this frame,  
so you see here that with this sentences hais not filled up all the possible frames. soSo the a partial instantiationaneous of the frame is possible and different verbs can have different frames. i am just showing the verb give theThe domain of a role is dependent on the verb.

 refer slide time (53:00)
 
alright now a role can be a single concept or composition based on a qualifier qualified structure like here: 
theThe kind man the agent part is beingin filled up with this entire structure the kind man.and  itIt is not only the man but it is a qualified sobut it is not a word but a structure. so itIt is not a single concept but a composition. 
now onceOnce this is filled up, it is this is partially filled up because this sentence does not talk about time. ok 
soSo, if we look at this frame then we can answer a couple of questions like who gave the breadbread. obviouslyObviously the answer is agent. Why nowNow iI am asking a difficult question, why  why did the kind man give the bread to the beggar? theThe answer will be the beneficiary, who is the beneficiary for his hungry child. soSo at least to a particular level we can answer some of the queries corresponding to this. 
Here is another so you can you are i would encourage you to take down this sentence: Robbie made coffee forro suzieSuzie with a percolator, you can make case frame structure, for make now this is made. 
refer slide time (54:39)
 

i will just give you a hint now made make make means making a thing ok, Hhe made a toy and make coffee that means prepare coffee. soSo you see the verb make can have multiple frames making coffee and making a machine ok makingmachine, making a noise, ok the child made noise in that case the frame of make will change. 
so So you have to make an appropriate frame for the particular semantics of the verb and then paharse this sentence and fill up the thematic roles manually and you will see that you will be able to unset answer some of the questions.
 nowNow this is just the tip of the iceberg. naturalNatural language understanding is a very interesting and very deep issue and subject and area of research.


 so it is not possible to describe everything here but my objective was to introduce the idea to you and i will encourage you to read about this more if you take up artificial intelligence for further studies natural language processing is a growing field and as i said before i conclude the natural language research is a very important area specially for countries like India where we live among the multilingual community 
thank you  



















	








       

                              





