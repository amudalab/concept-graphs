Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 2
    Stacks

Let us see about stacks. We will mainly see about stacks, besides we will talk about abstract data types, interfaces, exceptions, how stacks are implemented in java and application to the analysis of time series.  We will also talk about growable stacks, which do a little bit of amortized analysis and then we will talk about stacks in java virtual machine.

(Refer Slide Time: 1:16)

 

What is an abstract data type? It is basically a specification of the instances and the set of axioms that define the semantics of the operations on those instances. What does it all mean? You know the data types like integer, real numbers and so on. You can understand the notion of addition and that is the same way as you add 2 integers in mathematics. 











(Refer Slide Time: 1:51)

 

Similarly we will define data types and certain operations on those data types. Those operations would be defined through an interface which basically gives us the signature of the operation that is the parameters that operation requires and so on. We will also specify the results of those operations through a set of axioms. 

Just as in the case of integers, you know the sum of 2 integers as defined in mathematics. For example if you add a variable of type A and another variable of type B. If you sum them up, then the answer will be of type variable as you would know it from your mathematics class. We will be clear if we see an example. 

The operations that you have been talking about are essentially of three kinds. One would be just a constructor operation which is as same as the constructor method in java. 

Using this method you can create an instance of that particular data type. When you are talking about sophisticated data types, this method has to do a lot of work Access functions are the functions which let us to access elements of the data type and manipulation procedure would let us to manipulate or modify the data type.

Why are we talking about data types? Data types help us to identify the requirements for the building blocks of our algorithmic procedure. It provides a language which will help us to talk at a higher level of abstraction. As just as we are talking in terms of adding up of integers or in terms of stacks or queues or any of the advanced data type. 






(Refer Slide time: 3:17)

 


(Refer Slide Time: 3:57)

 

They encapsulate the data structure like how the data is organized and the algorithms that work on that data structures. Also they help us to separate the issues of correctness and efficiency. We will see more of this as we see the example of data types. Let me start by giving a simple example of the data type and that is a dynamic set. A set is defined as a collection of objects. Suppose we also had operations, which would let us modify that collection of objects, which means add or remove an object of that collection. Such a set we would call it as a dynamic set. 

(Refer Slide Time: 4:47)

 

We call it as dynamic, because we are changing the set which is the collection of objects. We will create data types for such dynamic sets. What are the kinds of methods that you have in a dynamic set? You would have a method to create a dynamic set, which would be a method new. There would be a method insert, to insert an element in to a dynamic set. S is the dynamic set and this method has two parameters let us say, the set s and the element. The result is an instance of the set itself which gives a new set, another set and also includes the element v in it. Similarly the delete method removes the element v from the set S. These are the two methods for updating the set. 

New method is for creating or constructing the set and IsIn is one of the access methods. 
All of it is telling us whether the element is in the set or not. The return value is of type Boolean. If v is in the set then it is true otherwise false. Axioms are the one which define how the operations should behave. We can write axioms in the following form. When I create a new set and if the set is empty then the answer should be always false, no matter what v is. If I have a dynamic set S and I insert an element v in it. Then the resulting set which has v in it should be true. 












(Refer Slide Time: 5:37)

 

If I have a set S and when I insert u in it, then the resulting set has u in it. Then if I where to ask whether v is in the resulting set, I will know that only if v was in the previous set S.  

Thus the answer to this operation IsIn (Delete(S, u), v), should be the same as the answer to this operation IsIn(S, v), provided v is different from u. IsIn (Delete(S, u), v) = IsIn(S, v), if v u Suppose I have a set S and I delete v from it.  If I ask whether v is in the resulting set, then the answer should be false. 

These are some basic axioms that define the nature of these operations and also the functionality of these operations. Still we did not specify how to do these operations or we did not talk about an algorithm or any procedure. 

At the least we have talked about the code for implementing the dynamic set. When you are talking about abstract data types, we are interested in more of the specification. That is what the instances would be like and what are the operations permitted on those instances, and the axioms that govern those operations. 











(Refer Slide Time: 6:46)

 

Some simple abstract data type that you may be familiar with is queue, but we will be doing it later. Let us see about stacks.

(Refer Slide Time: 8:52)

 

What is the stack? It is the collection of elements but this collection follows the last-in-first-out principle. What does it mean? It means that the element which is inserted last would be removed first. If I insert an element and then I remove an element from this collection. Then the element that would be removed was the one which was inserted at the last. The operation of inserting an element is called pushing onto the stack and the operation of removing an element is called popping off the stack.
(Refer Slide Time: 9:10)

 

Some of you might have seen this kind of toys. It has a collection of elements for instance may be stack of trays in your mess. What you do is when you put a tray, you put it on the top and when you remove it you would always remove the one which is at the top.

When you remove or pop of an element, it is always the one which you inserted at the last. We are going to define the abstract data type that is supported by four methods which are the key methods. 

The ?new? is a method to create a stack. In the push method when I specify an element o it adds this element to the abstract data type. It inserts an object o on to the top of the stack.

















(Refer Slide Time: 9:52)

 

Pop takes stack as the parameter and it does not take any parameter other than abstract data type. When I say pop the stack, it just removes the top element from the stack. If the stack is empty, they should flag an error stating that the stack is empty. The top operation returns the top element, it does not remove it and that is how it differs from the pop. Pop operation removes that element but the top tell us only about the top element. Again if the stack is empty then top does not make any sense, it should flag an error. 

(Refer Slide Time: 10:22)

 

We can also have some support methods which will help us do these operations. Size is one such method. Size tells us about how many elements are there in the stack and isEmpty tell us whether the stack is empty or not. The 6 methods that we saw are push, pop, new, top, size and isEmpty. These are all the methods and hope you all understood about what these methods are doing.

(Refer Slide Time: 11:34)

 

Axiom governs the behavior of these methods. If S is the stack, when I push an element on to S and then when I pop it, I should get back S. While doing a top operation, when I push an element on to a stack and then when I do a top operation I should get v, because v would be the top element of the stack. So far we have defined about the stack abstract data type, the methods and 2 axioms.

Axioms may not be complete but this is what the axioms would look like. How do we translate abstract data type into code? We need 2 constructs for that and they are the interfaces and exceptions. What is an interface? An interface is a way to declare about what a class has to do and what are the various methods associated with the class. It does not tell us about how those methods are done. That would be a part of the implementation of that interface or a class. 












(Refer Slide Time: 12:46)

 

For an interface we just right down the various names of the methods and the parameters it is going to take. In fact we do not even specify the names of parameter, we just have to specify the types of the parameter. When we write a class for an interface, we will actually provide the code for those various methods.

(Refer Slide Time: 13:16)

 

I might specify an interface for a stack and I am going to ask you to write the classes for that interface. Different people will write different classes to implement the interface in a completely different ways. I can still use your classes or any implementation of the interface, in a program that I have written, provided that must meet the interface specification which I have given to you. All I need to know is that the implementation meets the specification so that I can use that in the coding of my own program. It helps us to separate the implementation from the specification and that is why it is a very useful programming technique. 

(Refer Slide Time: 15:15)

 

Let us see about how a stack implementation looks like in java. Java has a built-in stack data structure but nevertheless we will define a stack interface. We just define the various methods that are going to be a part of this interface. 

There is one method called size, in which I need to specify the types of the parameters and the return type of the method. I have not specified how these methods are implemented. This is just an interface. 

In an interface we need to know the types of the parameter. When I am pushing, it takes a parameter of type object. Object is the generic type in java and all objects are derived from this type. 

The method isEmpty returns boolean. It just tells us whether the stack is empty or not. The top gives you the top element in the stack and it returns an object. It throws StackEmptyException, if this stack is empty then top () method should somehow signal that the stack is empty. We are going to do that using the notion of exceptions.

Void means it does not return any object or any value. It does not return a stack but it is a method which is executed on this stack and it modifies the stack. Thus stack cannot be considered as a parameter. What is an exception?

Exceptions are the mechanisms to handle errors. When we have an error or when we reach some exceptional condition or an exceptional case in the execution of program, we throw an exception. The term used in java is throw.

(Refer Slide Time: 18:07)

 

As soon as an exception is thrown, the flow of control moves from the current method to the point where the method was called. The idea essentially is that, when an exception occurs you delegate the responsibility of handling that exceptional case, to the procedure which called that particular method. 

You will be clear, if you see an example. I have two methods, one is an eat pizza method which throws a stomachache exception, also there is some dotted code. If you eat too much of pizza, then there is a problem and you throw StomachAcheException. 

The procedure public void eatPizza () throws was called in the method eatpizza (), which is inside the stimulate meeting procedure. When this StomachAcheException is thrown, the flow of control will come to TA.eatPizza (). Thus when this StomachAcheException is thrown, we will exist this method eatpizza () and go to TA.eatpizza (). 











(Refer slide time 19.07)

 

In the coding after {?} there are bunches of other statements that would not be executed. The flow of control would interrupt the dotted point and would reach TA.eatpizza (). There is also a notion of try and catch blocks. 

When the exception is thrown what happens to the variable that we have modified?
It depends upon the procedure call, think as if we are returning from this procedure StomachAcheException or a method. If those are local variables then you do not want to see them. If they are global variables and if it is modified in the if-loop, then those modifications are carried over to the TA.eatpizza () method. 

There is something called as a try and a catch block. If you think that there could be possible exception in this (TA.eatpizza ()) method, then you enclose the method within a try block. Start it with a try, open a bracket, and then include the method which you are calling and close it with a bracket. 

If there was no exception raised in TA.eatpizza () method or this particular exception StomachAcheException did not get raised in this method, then we will just skip the catch block, then go on to the statement, after the catch block.

If an exception was raised in this (TA.eatpizza ()) method, because this method might raise many exceptions. If this (StomachAcheException) exception was raised in the method, then we would come in to the catch block and execute the statements. 

If the method raises an exception, then if that exception is caught through a catch block, then we would execute the statements which are written inside the catch block. Any kind of statements can be written inside the catch block, not necessarily System.out.exception


(Refer Slide Time: 20:50)

 

What would happen, if I did not write the catch block? This procedure simulate meeting, would throw the exception to the point from where its parent procedure was called. When StomachAcheException throws an exception, the TA.eatpizza () method would also throw an exception, then the control will go to procedure from where simulate meeting is called. It is fine if it catches the exception at that point, if not it will throw an exception to the high level procedure and finally your procedure will stop with your exception appearing at your console. 

(Refer Slide Time: 22:25)

 
In this manner it is getting propagated all the way up to, where your procedure stops and the exception is shown to the user. System.out.println is just the method to print the statement.
 
(Refer Slide Time: 23:41)

 

An exception is really a java class in which I am creating an object or an instance for this class. Then I am initializing that instance with any parameter and I can specify some set of parameters in the statement given below.

StomachAcheException (?Ouch?); StomachAcheException itself is a class and for this class, I am creating an object by making a call to the statement. StomachAcheException (?Ouch?); When the catch statement is caught, e in that statement would get assigned to the object that is created by StomachAcheException (?Ouch?) statement. 

The try and catch block would come together. If the method were not enclosed between try and catch, then the exception would just get propagate upwards in the procedural hierarchy. StomachAcheException would throw an exception and the calling procedure of simulate Meeting would throw an exception, till it is caught at some point. If not it reaches the console. 

What does the name of the class followed by brackets and some parameters written would signify in java? For example: StomachAcheException (?Ouch?); In java it signifies, that you are creating an object for this class and you are invoking the constructor method with ?Ouch? as the parameters. 





(Refer Slide Time: 24:07)

 

The try and catch block are a method for listening exceptions and catching them. As I mentioned before, a catch block can contain anything. It does not mean that it should have only system.out.println, it can also throw an exception in turn. 

(Refer Slide Time: 26:16)

 

It also helps us to exit from the program when an exception occurs. If you throw an exception in any method, then you need to add a throws class next to the method name.  When we wrote the method eatPizza () we had, throws StomachAcheException. A method can throw more than one exception. 

In java everything is really an object. StomachAcheException is the name of the class. Public class StomachAcheException extends And the statement given below is the constructor method for the class. Thus the name of the constructor method is the same as the class name. Public StomachAcheException (string err) The constructor method takes a single parameter, which is a string. Super means that it is calling the super class with the same parameter. 

(Refer Slide Time: 28:00)

 

Again as I mentioned before, if you never catch an exception it will propagate upwards, along with the chain of method calls, till it reaches the console. Since the stomach ache exception is extending a run time exception, it will call the constructor method for the run time exception.

















(Refer Slide Time: 28:17)

 

If a particular method throws more than one exception, then you will have to specify all those exceptions which it throws, next to the method name. Even in the try block you can have many catch statements. First we can catch one particular exception followed by some other exception and so on. Look at your java book for more details.

(Refer Slide Time: 29:10)

 


Let us look at the stacks. We had created the interface for our stack. We are going to implement the methods and there are many ways of implementing a stack. First we are going to implement using an array. 
Let us say the maximum size of our stack is N and I am going to have an array of n elements of the stack. I am going to have a variable t, which will tell about the location of the top element of the stack. The variable t gives the index of the top element in the array S. The first element will be at location 0 and then when I push another element it will move to the next location and so on. 

(Refer Slide Time: 29:43)

 

I have actually listed out an entire implementation for our stack interface. My implementation is called array stack because I am using an array to implement the stack. 

The statement mentioned below says that I am implementing the stack interface. Public class ArrayStack implements Stack Implement stack means, it is implementing the stack interface that we provided. I have set with a default capacity for the stack which is 1024, otherwise the capacity of the stack would be in the variable N. Final is just specifying that the value of capacity is always a constant and it can never be changed.














(Refer Slide Time: 30:48)

 

S is an array which is going to hold the elements of the stack. Thus S is an array of object and t is the index of the top element. Initially t =-1, because there is nothing inside the t. t=0 means the top element is in the location 0 and when the stack is empty t =-1. 

     Public ArrayStack ()
     Public ArrayStack (int cap)

The above two statements are the constructor methods. If you do not specify anything or if you just call the array stack without any parameters, then I am going to create a stack whose capacity is 1024. If you call array stack with some number let us say 37, then I am going to create a stack of size 37. 

What should size do? Size should just return how many elements are there in my stack. If t is the index of the top element, then t+1 elements are there because we just started from zero. The stack is empty if t =-1 that is t <0.  If t <0 then isEmpty () method would return true, otherwise it returns false. 












(Refer Slide Time: 32:42)

 

If I want to push an object ob in to the stack and if the size of the stack already equals n, then I should throw a stack full exception. Else I should first increment t and then put the object at the new incremented location. S [++t] is the first increment, and then put the object at that location.

I have to give you the top element of the stack for that, I should check whether the stack is empty or not. If the stack is empty then I throw a stack empty exception. If the stack is empty then the flow of control would exit from throw new Stack Empty Exception (?Stack is empty?). And when the stack is not empty it just returns the top element of the stack S[t].
 
If I want to pop the stack, then once again I check if the stack is empty. If the stack is not empty then I save the top element in location elem. Then I decrement t, because I am removing the top element and to the location which I set earlier was set to null that is I dereference it. Because earlier at the top location t, it was initially 37 and I had an object in that location. I need to remove that object and decrement t to36. Then I return the top element. Pop also returns the top element.
           S [t--] =null;
           Return elem 









(Refer Slide Time: 34:02)

 

Stack Empty Exception is a class and I use new to create an object or an instance for this class. Why should we necessarily dereference the objects? It is best to deference, because you can remove those objects or you can get rid of those objects otherwise they will lie in your memory. Thus t is just an integer and it is a private member of this class, private because no one else knows about t and S is an array of objects and S can therefore access the  element of this array. 

(Refer Slide Time: 36:54)

 

The array implementation is very simple as all the operations were taking constant time. None of the operations required time propositional to the virtual dependent upon the number of elements in the array in this stack at that point. Each of those methods take O (1) time. The problem is that we are working with an upper bound on the size of the stack. This upper bound may have the default value1024, which we took in our example or it may be specified at the time of creation of stack. The problem is because you do not know the size of the stack. 

We might allocate a very large size for the stack, but it might be a waste of memory or we might allocate a very small stack in which we could not be able to run our procedure to complete, because we would soon have a stack full exception. Stack Empty Exception is the requirement of the interface, because the top and the pop methods are not defined, if the stack is empty.

(Refer Slide Time: 37:48)

 

It is the requirement of the interface but a stack full exception is an artifact of this implementation. If I had some other way of implementation, then I would never have to raise a stack full exception. Let us look at an example. We will see how to implement the stack and never to have a stack full exception, so that we can always grow the stack when needed.

Let us look at an application of stacks. We have the daily stock prices of a particular stock. If I give you the price on day 0, the price on day 1 and so on, then the span   of a stock price on a certain day i is defined as the maximum number of consecutive days that the price of the stock has been less than or equal to its price on day i. 

The following example would make it clear.   is the span of the stock price on day five and it is equal to the maximum number of days that the price of this stock has been less than or equal to   day price. For four days the price of this stock was less than or equal to   day price and so the span of the stock price on day five equals 4, inclusive of the current day. Following picture would make it clear hence we are counting 1,2,3,4 and for day six, it is 1, 2, 3,4,5,6 that is 6 days.

(Refer Slide Time: 39:25)

 

How can you compute the span in an array S, if I give you the stock prices in an array p. P is an array of numbers. To compute S[i], you are going to look at the price of the stock on day i, i-1, i-2 and so on. 

The index k will start from zero and it will keep going down till the price of the stock on day i- k is less than the price of the stock on day i. The moment you find a case such that the price of stock on day i- k is actually more than the price on day i, you stop the loop which is given below, otherwise you keep incrementing k. 

    If P [i-k]   P[i] then k? k+1

If this quantity P [i-k]   P[i] is less, then you increment k else you say done is true. Done will help us to exist the repeat-until loop. It will exit the repeat-until loop if done is true or if k equals i, which means that you have reached day 0. Then the span will be determined by the value of k, because k tell us the span of stock price on day i. Thus S[i] gets the value k, (S[i] ? k). This is one way of computing this span.









(Refer Slide Time: 41:07)

 

How much time does it take? It takes   time as we can see on the slide. Why should it take   time?  Because we are repeatedly comparing it  How many times the repeat-until loop can be executed in the worst case? It is i times, where i varying itself from 0 through n-1. The total number of times one of this statement get executed might be   or . Thus the running time of this algorithm is O ( ) in the worst case. 

Question is can we do something better? Yes, as we are talking of stacks, we can use a stack to do something better. To compute the span, we need to know the closest day preceding i, on which the stock price is greater than the price on day i. 

















(Refer Slide Time: 43:26)

 

In the example, for day 5 I need to know the closest day preceding day 5, on which the stock price is greater than the price on day 5. On day 1, it was greater. For day 5, the span would be 1,2,3,4. I am going to call the quantity h (i). h (i) is the closest day preceding i, on which the price is greater than the price on day i. Thus h (3) =2, h(2)=1, h(1)= 0, h(0)= -1 and h(4)=3, h(5)=1, h(6)=0. Once you computed h, how can you determine the span ? The span for the price on day 5 is 5-1 that is 4. If we can compute these h quantities, we can easily compute the span. 

(Refer Slide Time: 45:31)

 

How do we compute the h quantities? Suppose those 6 quantities which is in the slide, were the prices that I have given from day 1 through 6. Can h (7) be 1?  I have not told the price on day 7 but through the definition of h (7), it is the closest day preceding 7, on which the stock?s price is larger than price on day 7. But that day cannot be 1 at all, because the price on day 2 is larger than the price on day 1. Similarly that day cannot be 3 or 4.  What are the possible values that h (7) can take? 2, 5 and 6 are the only possible values that h (7) can take.

We will store the indices 2, 5, 6 in a stack. 2 will be at the bottom of the stack, 5 will be above that and 6 on the top. To determine h (7), first we need to compare the price on day 7 with price on day 6. Suppose the price on day 7 is less than the price on day 6 then what is the h (7). It is 6, but if the price on day 7 was greater than price on day 6, then I will compare with price on day 5. If it is greater than the price on day 5, then I will compare it with 2. If it is greater than 2, then it is minus one that is h (7) =-1. 

(Refer Slide Time: 46:41)

 

Suppose the new bar I have drawn is the price on day 7 and it is greater than the price on day 6 then h (7) is 5. The first price greater than the price on day 7 in the comparison gives me h (7). But once I know h (7), I should update my stack. Now what should my stack contain? Earlier it contains the indices 2, 5, 6 and now it contains 2, 5 and 7. It is clear because h (8) can never be 6, I should get rid of 6 by replacing it with 7. You can see that the stack would be the right way to do these things. 







(Refer Slide Time: 47:40)

 

Let us look at the procedure. Let d be the stack and initially it is empty. When I get a certain price, I am going to compare that price with the price on the top of the stack. If it is less than the price on the top of the stack that is on the top of the stack we had 2, 5, 6 and if the 7th bar is less than the 6 then it is just what is there on the top. Then we are done, the index on the top of the stack will give the h value. If it is more than the price on the top of the stack, then I will pop of or remove the top of the stack, because I need to compare with the next one. I pop of because I will not need that quantity any more. 

(Refer Slide Time: 48:36)

 

As you recall from the previous slide, since 7th bar was more than 6th bar, I can actually get rid of 6th bar because in the stack, I do not need that next time. We are going to go around the loop till either done becomes true. When it becomes true it says that I have found a price which is greater than the current day price. But if done never becomes true and the stack becomes empty then h=-1, exactly that is being set here in the below statement.
    If D.isEmpty () then h ?-1
                             Else h ? D.top ()
When I exit this (P[i]   P [D.top ()]) loop, the stack is empty then h=-1 else h is the top value of the stack. Once I know h, I can compute S[i] and keep it. I will push i back in because in my previous slide when I got this 7, I now push 7 in for my next computation. 
How much time does this take? May be n times. While loop might execute a lot of times and why this should take only n time. Is the worst case  or n? It may be . 

(Refer Slide Time: 49:52) 

 

How many elements do we pushed on the stack? Each element is pushed once and we have pushed at most n elements on to the stack, if there are n elements to begin with. Every time when the while loop is executed, we pop of one element from the stack. How many times the while loop gets executed? It is n times. Every time when the loop executes, we are removing an element from the stack. If the total number of elements ever we pushed on the stack was n, then how can we pop of more than n elements from the stack. It means that the total number of times the loop executed is not more than n.

How many times do the statements inside the for-loop execute? All most n times, because these are all a part of the for-loop. If statements execute exactly n times. What is the total time it will take? It is order n and not . I am saying the total number of times the loop executes when all iteration put together is no more than n. Because every time the loop executes and when we go through the loop, we remove one element stack from the stack. We never pushed more than n elements or the total number of elements we pushed on the stack is at most n.  

One problem with our stack implementation is that we had to give maximum size for the stack. We are going to look at an implementation in which the stack can grow, if it ever gets filled. When we are pushing an element, if the size of the stack is n then I create a new array of length f (N) and I copy all the elements of my original stack S in to A and then I rename it as S. It becomes my new stack with f (N) locations in it whose capacity is f (N). Thus f (N) will be larger than n for us and so we had to increase the size of the stack. I increment the top counter and I am trying to push the new object into my top location. 

How should we should f (N)? There are two strategies in which one could adopt. One could either have a tight strategy or a growth strategy. In a tight strategy, we always increment the size of the array by some constant c. We just increment that is additive increment. In the growth strategy, we double the size of the stack.   

(Refer Slide Time: 54:02)

 

We want to compare and see which of these is a better strategy. We are going to think push as of two kinds. One is regular push. In a regular push, there was just space in the stack and you just push the element and it takes one unit of time. A special push is one in which the stack is already full and you have to create a larger stack and copy the elements form the earlier stack to this larger stack and then push the element.






(Refer Slide Time: 55:40)

 

You created a stack of size f (N) that costs f (N) units and you copied the n elements that cost n units. And then you pushed one more element that cost one more unit. The total cost of this special push operation would be f (N) +N+1.
 
Let us see how the tight strategy behaves when we increment the size of the stack by c units. For example c is taken as 4. Initially I started with the array of size 0. When the first element came to push that was a, then I created a stack of size 4 and I push this first element in and the total cost was 4+1. 

When the second element came, I do not need to enlarge my stack, because I have space. It just cost one unit which is a regular push. The 3rd and 4th operation is also a regular push which costs 1 unit each.
 
The next one is a special push, I am not trying to push e because the stack is already full. I need to create an array of size 8 and I need to copy the 4 elements, then I need to push 1. The total cost becomes 8+4+1 and these are all the 3 regular pushes. Then once again when I fill it, I will create an array of size 12, because c is 4. I am incrementing the size of the array by 4 units every time. 










(Refer Slide Time: 56:07)

 

I create an array of size 12 and I copy the eight elements then I push one more element, so I get 12+8+1 and so on and finally I have 16+12+1. When the size of the array was 4, I am going to call it as phase 1 and when the size of the array is 8, I will call it as phase 2 and so on. Let us see the total cost of the procedure.

(Refer Slide Time: 58:18)

 

The pound symbol in the above slide is a multiplication operator. In phase I, the size of the array is c *  i. In phase 1 it is c, in phase 2 it is 2 c, in phase 3 it is 3c and so on. The size of the array is c*i. 

What is the total cost of phase i? t At the beginning of phase i, I first create an array of size c* i then I copy the elements of the previous array. Let us look at an example. I first create an array of size 8 then I copy the previous 4 elements. So I copy c* i-1 elements and it is the cost of copying the elements in to the new array. Then I will do c more pushes in this phase before the array gets filled. C is the total cost of the 4 regular pushes that I have been doing. The total cost is c* i + c*i-1+c which is 2 times ci. Thus the cost of phase i is 2ci.
 
In each phase I am doing c pushes, then if I have to do a total of n pushes then I need  phases. Total cost of  phases would be 2c (1+2+3+?+ ) because the cost of  phase is 2ci and this sum is roughly   times 2c and that is not t, it is approximately O ( ). 

So far we have seen the tight strategy. We could also take creation as order 1, in that case the analysis would change slightly. For the purposes of analysis I am just taking it as, if you are creating an array of size something, you take that much as the cost. 

Let us see the growth strategy. In the growth strategy, I start with an array of size 0, when I get the first element I create an array of size 1. When I am trying to push an element I would double this array, so I create an array size 2 and I push this element. When I try to push the element, I double this array and create an array of size 4 and push the element. I have space for one more element and this is the regular push while pushing d. When I try to push the 5th element, I will double the size of array again, so I create an array of size 8 and copy these elements and then push the 5th element and so on.
 
Once again we can analyze the cost. 1 is the cost of creating the array and you do not have to copy anything and 1 is the cost for pushing them. Here we created an array of cost 2, we copied 1 element and 1 was the cost of pushing. We created an array of size 4, 2 was the cost of copying, 1 was the cost of pushing the element c and here was the regular push so it was 1 and so on. 

We define a phase as when the size of the array was 1, we call it as Phase 0. When it is 2 we call it as phase 1. When it was 4, we call it phase 2 and when it was 8, we call it as phase 3 and so on. In phase i, the array has size . Phase 3 it has size 8 and phase 4 it has size 16 and so on. In the phase i, since the size is   I spent  units of time in creating the array. Then I have to copy elements of the previous array that is  elements.

How many elements are left after copying  elements?  elements are still left and they have to be pushed in, in this phase.   is the cost of pushing in those elements. The total cost of phase i is  + +  which is . If we do n push, we would have log n phases, because the way the array is growing. 
 
(Refer Slide Time: 1:00:55)

 

The total cost of n pushes is going to be 2+4+8+?+ , which is 4n. The total cost of n pushes is 4n growth strategy and in the tight strategy it is . Hence this is clearly a better strategy. 

Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 3
             Queues and Linked Lists

In the last lecture we looked at stacks as a data type. We saw how to implement stacks using an array. Today we are going to look at queues and linked list and in the later part of the class, we are going to do sequences. In particular the first part of the class I am going to do queues, linked list and double ended queues. 

(Refer Slide Time: 01:22)

 

What is the queue and how does it differ from the stack?  The stack followed the last-in first-out principle, the element that was inserted last in to stack was the one that was removed first.













(Refer Slide Time: 01:34)

 

Queue on the other hand follows the first-in-first-out principle. Whoever joins the queue earlier is the first to be removed from the queues that is first to be processed. You are all familiar with the queues. In a queue for instance there is a notion of a first element and the notion of the rear element. 

When an element is inserted in to the queue, it comes at the rear. If I remove an element from queue it is the element which is sitting at the front end would be removed. We always insert an element at the end and when we remove an element it is always the element at the front is removed.
















The queue is also an abstract data type and we can define a few methods on the queue. The methods given in the slide are the standard operations. The method new would create a queue and enqueue is the method to add an element to the queue and dequeue is to remove an element from the queue.
When you dequeue a queue or when you remove an element from the queue, you get another queue. 

(Refer Slide Time: 02:48)

 

The front is the method which gives the first element of the queue. 
How it does differ from dequeue?
It does not remove the front element, it only tell us which is the front element of the queue. 















(Refer Slide Time: 03:46)

 

We also have some other support methods, to implement the queue. One could be size and the other is IsEmpty. Size would tell us how many elements are there in the queue and IsEmpty would tell us whether the queue is empty or not. It would return true if the queue is empty else it would return false.

Just as we defined axioms for the stacks, you can define similar axioms for queues. If I create a new queue and I insert an element or enqueue an element v and then when I say what is the element at the front of the queue. It should be v and suppose I create a new queue and enqueue an element and then I dequeue an element, then I should get the empty queue which is the same as whatever obtained if I just called new. 

Similarly if I had a queue and I enqueued an element w, which means I added an element to the queue. Then I added another element v to the queue, thus w is ahead of v in the queue. If I call front, first I will get all the elements of the queue, followed by w and then followed by v. 

The element in the front of (Q, w) is the element at the front of the queue. 
Why I have written front of (Q, w) and not front of queue?
If I have just written front of queue, then it would not have been defined. If a queue is empty then there is no notion of front of the queue that is why I have written Front (Enqueue (Q, w)).




Same thing as before in which I had a queue Q, if I insert w in to the queue then I insert a v and then I removed an element. The element which was at the front of queue would be removed, if the queue was empty then it would have been w. The following operation is the same as in which I had a Q, I added w to the queue then I removed an element from the queue, then I added v again. The queue that I have obtained as the result of the below mentioned 2 procedures should be the same.
     Dequeue(Enqueue(Enqueue(Q,w),v))=Enqueue(Dequeue(Enqueue(Q,w)),v) 
Let us check out whether the result is true. 

Let us assume that the queue was initially empty. 
What does this statement Dequeue (Enqueue (Enqueue (Q, w), v)) gives?
First I added w then added v and then I removed an element. If I removed an element from the queue I would get w, that is w is removed. Then I get v as the remaining queue. 

Let us look at this Enqueue (Dequeue (Enqueue (Q, w)), v).
Queue is empty, I added w to the queue then I removed an element and once again I have left with an empty queue. Then I enqueue v, thus the queue has v in it. If queue is empty then in both the cases the queue will have only v in it at the end of the procedure.
 
If queue is not empty then I enqueued w, then I enqueued v again. Hence I have a queue in which first I have all the elements of Q, followed by w, followed by v. When I dequeued, I will be left with the original Q without the front element, followed by w and then followed by v.
 
Let us see if we get the same thing in Enqueue (Dequeue (Enqueue (Q, w)), v). 
I started with Q and I added w to it. Now I have queue which has Q and w. Then I dequeued which means I removed the front element of queue. The queue contains all the elements except the front element, then I have w in that queue and I added v at the end. Thus I get the same result. 

How do we implement a queue? 
We are going to use an array in a circular fashion to implement the queue. 
What does it mean?
Suppose someone tells that the queue is never going to be larger than n elements. I am going to allocate an array of size N. I am going to have 2 variables f and r, f for front and r for rear. f is the index of the front element that is f will be referring to the front element of the queue. r is an index which is the element following the rear element. The blue part is the one which is occupied by the queue.








  (Refer Slide Time: 08:11)

 


How did the queue reach the blue colored part?
I would have started with the front that is the first element I inserted must have come to the 0th location. Then the next element I inserted must have come to the 1st location and the 3rd element must have come to the 2nd location and so on. Then I also delete the elements. When I delete, an element goes away. In effect the elements in the queue drift right and hence the front and the rear element has moved to right. 
This implies that we have deleted f-1 elements. It is not completely accurate. I had said something like in a circular fashion. 

What does the circular fashion mean and why am I saying such a thing?
Let us say that we kept inserting the elements in queue. I insert another element then I insert another element and at one stage I cannot insert anymore elements because I have already reached the end of this array. But I have a space available in the front then I will wrap around and start inserting the elements. 










Your queue in some point will look like the one which is given in the slide below. The front was at the left of the rear but now front is at the right of the rear because we have queue which is now starting from the right side and going to the left side. When I insert an element it will still come to the  location and then to the next location and so on. 
(Refer Slide Time: 10:15)
 

When we started the front was referring to 0th location that is f should have been at minus one, because the front refers to the first element of the queue. If there is nothing in the queue then f should be minus one and rear refers to 0, because rear refers to an empty location. 
(Refer Slide Time: 10:59)

 

What does if at some point I reach a situation when f = r? 
Is that empty or full?

(Refer Slide Time: 11:41)

 

What will happen when it becomes empty? 
Suppose if I kept removing the elements starting from the  location, I did not add any other element and then I removed all the elements before  location. 

Where f would be located?
f would be increment to r, so f becomes r. When f is r, queue is empty. Suppose I kept adding the elements to the queue. When I add, r will move one step, another step and so on. When I add an element close to  location, then r would be referring to f. Again f equals r. We will add the  element. There is an ambiguity and we have to resolve it in some manner. 

f = r means, both empty and full. Since we will have a problem, if you do not know whether the queue is empty or full. We will try and ensure that we never had n elements to the queue. When the queue has only n-1 one elements, we will declare it before. That is what we are going to do.







Let us look at the code for enqueue.
This is just pseudo code. If the size of the queue or the number of elements in the queue is n-1, then we are going to stop and say that the queue is full and we will return the queue full exception. Otherwise if it is not the case then add the rear location, put the element that you are trying to insert and increment r. 

(Refer Slide Time: 13:30)

 

The modN is required, because we need to do the wrap around since it is circular. Indices goes from 0 through n-1 only, r is already n-1 and I increment r at this r ? (r+1) modN point. Then I do not want it to become n, but I want it to become 0 and hence modN is 0 and I will have it in r. In the pseudo-code, size is the method and it should have been enclosed in brackets.















(Refer Slide Time: 14:13)
 
What does the method size do?
It returns (N-f+r) value.
Why it should not return r-f.? 
r-f is negative in the setting which is given in the slide (Refer Slide Time: 15:04), but in the setting which is given in the slide (Refer Slide Time:: 10:59), r-f tells me exactly the number of elements in the queue. r-f is the correct thing except it might be negative. 
(Refer Slide Time: 15:04)
 

How many elements are there in the queue which is given in the above slide?
It should be n- r+f or n-f+r, which is anyone of these.    
The quantity (N-f+r) would be the number of elements that you would get and this quantity is always positive, because r-f can at worst be minus n. Thus N+r-f would always be a positive quantity. You can return this (N-f+r) as the size, as this will tell you the right number of elements. Check this out if you are confused.

isEmpty () is a method and we said queue is empty if f=r. There was an ambiguity and we never had more than n-1 elements into the queue. If f =r, that means the queue is empty and it is not full. Thus f=r returns empty also it returns true for this (Algorithm isEmpty ()) method. 

(Refer Slide Time: 14:35)

 

For front if the queue is empty then it raises an exception, otherwise just return a front element. We are not removing the front element as we are doing it in the dequeue method. In the case of dequeue method, we will increment the front index and remove the front element by setting Q (f) ? null. 












(Refer Slide Time: 16:39)

 

You can also implement the queue using a linked list. We saw an array to implement our queue. The disadvantage of using an array is fixed size. If you know the maximum size that the queue can take then it is ok, but if you have no idea about the maximum size, then you could either use the method which we did in the last class were in when the size  increases beyond what we have allotted, then we double the size of the queue. You could either do that or you could use an implementation which uses a linked list.
 
What is essentially a linked list?
It has nodes and it has pointers which are basically referring to the next nodes in the list. The first node is referred to as head of the list and the last node is referred to as the tail of list. Each of the nodes has some element or some data in it. 

If I am going to use a linked list to implement the queue, then the question is which should be the front of the queue, whether the head node should be the front of queue or the tail node should be the front of the queue. The head of the list should be the front of the queue, the tail of the list cannot be the front of the queue. 










(Refer Slide Time: 17:09)

 

Why the tail of the list cannot be the front of the queue? Why cannot I have my queue in which the 1st element is this, the 2nd second element is this and the third element is this? 
The problem is with removing, note that I cannot remove the torcezo element. The linked list does not permit me to do this. 

Can I remove the torcezo element from linked list?
Not directly, because to remove that element I have to change the 2nd pointer. But there is no way of accessing that pointer and hence I cannot remove that element. I can remove the rome element, there is no problem in it, but I cannot remove the torcezo element.

In a queue the removal is being done at the front that is we remove the element at the front of the queue. Since I cannot remove the element which is sitting at the last place and I cannot call this as the front of the queue. I would like to have rome as the front of my queue.












 

(Refer Slide Time: 18:38)

 

Let us see how we are going to implement our methods. 
Suppose I have to dequeue which means that the front of the queue is the one which is at your left. Head part is the front of the queue and the tail part is going to be the rear of the queue. If I have to remove the element at the front of the queue that is to dequeue, I should point the head to the 2nd node. 

Thus the front element will get removed and I just increment or just making the head point to 2nd node. In this manner I can delete the head element very easily and also I can insert a new element to the head easily. I just create a new node, connect the new node and make the head point to the new node. Thus inserting at the head is very easy. The head is the front of the queue, I can just move the head to one step right and in that manner, remove the front element of the queue.












(Refer Slide Time: 20:44)

 

If I have to add an element, enqueue has to be done at the rear of the queue. In the above slide, first diagram is my queue and the last element is the rear of the queue. I need to add a new element at the rear end of the queue. The pointer should now get modified to point to the newly added element and the tail should be update to the next node because that will become tail and the pointer after the rear element should be null.
 
I can always add an element at the tail but it is difficult to remove an element in constant time, because to remove the tail node, I need to access the previous node. The only way you can to do in this kind of list is to start from the beginning and move all the way to the right till you get to the tail node. Then you will be able to access the previous node. 

What is problem in removing in the tail node? 
The problem is that after I remove the tail node, what is the new tail of the list. It is the last before node, I have to make the tail point to that last before node. 

How do I get to that last before node?
I need to go through the entire list, to get to this node.  
I am not saying that it is not possible, but it is a very expensive operation. It is not worth while to remove at the tail and so we will remove at the head and add at the tail, which means the front of our queue will be at the head and the rear of the queue would be the tail. 




So far we have seen the queue data type. Now I am going to introduce another data type called double-ended queue. 

What is the double-ended queue? 
It is a queue in which we support, insert and delete operations at both the ends. We have Insert First which is to insert at the front of the queue, Insert Last is to insert at the end of the queue, Remove First is to remove at the front of the queue and Remove Last is to remove an element at the end of the queue. Also we have the first and the last operations. Such a thing is called double-ended queue, at both the ends we can do both the operations of insert and delete. 

(Refer Slide Time: 22:36)

 

A singly linked list is not a good idea to implement such a double-ended queue. Why because as I have said repeatedly, we cannot remove the element at the tail or it is very expensive.

What is the good solution to this problem? 
We are going to use doubly linked list to implement double-ended queues. 









(Refer Slide Time: 23:28)
  
What is the doubly linked list?
A doubly linked list has nodes with two pointers, one is next pointer and the other is the previous pointer. We are also going to have two sentinel nodes. Each node has two pointers, one pointing to the next and one pointing to the previous.
(Refer Slide Time: 24:55)
 
Using such a list we can implement all the operations of double-ended queue in constant time. The problem earlier was how to delete the node which is at the end. The head and the trailer nodes are the 2 sentinel nodes. I have a pointer to 2 sentinel nodes and to get to the last element, I just follow the pointer once and get to that element. To delete that node, move to the previous port and set its next pointer to trailer and send the previous pointer of trailer to that node. 
(Refer Slide Time: 25:15)
 
We need header and trailer nodes in a doubly linked list. These nodes are called sentinel nodes or dummy nodes because they do not contain any data inside them and they are just there to mark the start and the end. This is useful. 
How do you delete at the end? 
I have to delete San Francisco out of this list. All I have to do is make the sentinel node point to the previous node and make that previous node to point to the sentinel node. Then the last node is deleted and in the slide (Refer Slide Time: 25:52) the last one becomes my new list. That was the only thing I could not do in a singly linked list and I have shown it here. Hence all the other operations can be done in constant time.  
(Refer Slide Time: 25:52)
 

Thus using a doubly linked list, we can implement all the operations of double-ended queue in constant time. We can insert at the front, insert at the end, delete at the front or delete at the end all in constant time.
What is meant by constant time?
It is the time which is independent of number of elements in the list and your running time will not be depended upon the time.
Double-ended queue is a fairly generic data type, it can used to implement other data types also. Suppose you had an implementation of double-ended queue and you can use that to make a stack or a queue.
Let us see the implementation of a double-ended queue.
I can use the methods of this implementation to implement a stack. For instance in the method top (), the top element of the stack would correspond to the last element of our double-ended queue.

(Refer Slide Time: 27:17)

 

Thus the method top () would return the last element of the double-ended queue. The method push () would correspond to inserting at the end of my double ended queue and the method pop () would correspond to deleting at the end of my double ended queue. I could also make the last () to correspond to the front element of my double ended queue. In that case the last () would have been my front and insert Last (0) would have been insert Front () and remove Last () would have been my remove Front (). You can use it either way you like it.

Size () just corresponds to the size of my double ended queue and isEmpty () corresponds to isEmpty of my double-ended queue. Because these are only dependent upon the number of elements in the queue. 

Similarly I can use a double-ended queue to implement the queue. Front () gives the first element of the double-ended queue, enqueue () corresponds to last that is it inserts at the rear. When I say dequeue, it removes the first element of the double-ended queue. If I have a dequeue implementation, I can use the methods to implement a stack or a queue or one of these data types. 
(Refer Slide Time: 28:59)
 
We have used a double-ended queue to implement a stack or queue and this is an example of an adapter pattern. Thus adapter patterns implements a class using methods of another class. In general, adapter classes specialize general classes and we can have certain applications.
(Refer Slide Time: 29:39)

 
One application is that we can just implement by changing some methods. For example we can implement a stack by using a double-ended queue. Another application would be an implementation of a stack. We define an interface called stack and implemented it using an array. That implementation is called an array stack. 
What are the contents of array stack?
They are any arbitrary objects and I can adapt ArrayStack implementation to an implementation called IntegerArrayStack which only uses integer objects in it. All I have to do is suitably cast the type of the objects that I am pushing in to the stack or removing out of the stack. 
There is another data structures called circularly linked list and it is very simple. In that the last element is pointing to the first element of the list.
(Refer Slide Time: 31:38)

 

There are no 2 pointers head and tail that is there is only one pointer which is pointing to the start of the circular list and you can use the data structure which is given in the above slide to implement both queue and the stack.
How will you use this data structure to implement a queue? 
In a queue we will make the first node as the front of the queue and the last node as the rear of the queue. 
How will I add an element at the rear?
To add an element before the first node, make the pointer point to the first node and make the head point to, it is not straight forward because if you mean the big pointer then how you will make this to point to the new node you have just created. We want to create a new node at the end. Make the element which you are inserting to go into the new node and create a new node and copy the element Rome into the new node. Make the head point to that new node and copying is not costly because here you are copying only the reference. 

Think about the circular list and it is a very straight forward. In this manner you can insert an element in the queue, if you are using this circular list to implement the queue. Removing an element corresponds to removing the first one. 
How do you remove the first one? 
If I have to just remove the first element in the list, then how do I make the pointer from the last node to point to the 2nd node. There is a problem in doing this. 
What do you do again? 
Let us remove the 2nd node and copy the contents of that node to the 1st node. We have to remove the Rome. 
How do I remove the Rome?
I copy Seattle to Rome. Thus Rome has Seattle in it and I remove the 2nd node. Copying just means changing the reference. Hence we discusses about queues and double-ended queues. 
We are going to the second part where we will quickly look at some sequences. We are going to talk about vectors, positions, list and general sequences. We will be using the data structures like arrays and linked lists to implement these data types. 

(Refer Slide Time: 35:06)

 

What is the vector data type? 
Vector data type is a sequence of n elements that supports the following methods which are given in the slide below. These are indicative methods and not all the methods. 
Essentially in a vector it is a sequence where there is a notion of rank with every element of the sequence. Think of sequence of elements right 7,11,13,19. We know that 7 was the 1st element, 1l was the 2nd element, 13 was the 3rd element and 5 was the 4th element.



(Refer Slide Time: 35:34)

 

With each element there is a notion of rank, and then I can have methods like elemAtRank r. Rank here corresponds to let us say rank(r) integers. First element was the element at rank 1 and 2nd element was the element at rank 2 and so on. 
Suppose if I ask to give the element at rank r or replace the element at rank r by the element e, insert an element e at rank r or delete the element at rank r. I could have such methods. 

When I remove the element at rank r, for instance let us say the rank of the students in a particular class. There is a departmental rank 1, the departmental rank 2 and departmental rank 3 and so on. Suppose the departmental rank 4 changes and goes to some other department. The department rank 4 is the rank of the one who had the rank 5 before. The same notion follows and everyone would move up by 1 rank. 
 














Let us see how to implement the data type using arrays.
I am going to have an array, in which I will have the element with rank 1, rank 2 and rank 3 and so on. If I have to insert an element at rank r, I have to put an element in the  location, which means I have to shift all these elements to one step right. That is what I am doing and I put an element in that location.

(Refer Slide Time: 37:53)

 

In a for loop, first we are moving n-1 one step to the right by this statement S [i+1] ? S[i].  First we are doing this for n-1, then n-2 where n-2 is moved one step to the right till 
r is moved to the one step right. Finally element e is put at position r and the size is increased by 1 where n sores the size of the vector. 

   S[r] ? e
   n ? n+1

Similarly when I am removing an element at rank r, I am essentially shifting the entire elements one step to the left. All elements starting from r to n-2 and then S[i] gets S [i+1]. At the location r, I will get the element which was sitting at location r+1.

How expensive are these operations in the worst case? 
Order n in the worst case because we might have to shift up to n elements to the right
or to the left. This implementation is expensive from this point of view, if I have to do these two operations insert at a certain rank or remove at a certain rank. I have 2 in the worst case spent order n time. 


The other operations are faster. How much time does the elemAtRank(r) takes, because I just go to the  location in that array and retrieve the elements sitting there. replaceAtRank (r, e) again order one, because I just go to the  location and replace that element with element e.
(Refer Slide Time: 40:06)
 
The chart given below shows the time complexity of various methods. All methods except inserted at rank and remove at rank take constant time but these two methods could take order n time in the worst case. 
 (Refer Slide Time: 40:33)

 


(Refer Slide Time: 40:25)

 

Can you think of some other way of implementing this list?
We can implement through doubly linked list. 

Can you use a doubly linked list to implement a vector? 
I am showing you here the operation of inserting at a certain rank. 

There are 3 diagrams in the above slide. In the 1st diagram, the 1st node is the header and the next one is the element at rank 1. Following one is the element at rank 2 and the next one is the element at rank 3. Suppose I want to insert an element at rank 2, I have to make a new node and put it between 1 and 3.

How much time does it take?
Create the node and to insert it, I make a pointer point to the next node and make the previous pointer point to the previous node. This is how I insert newyork and the 3rd diagram is the one which I get after insertion. 












There are 2 issues. First if I know where I have to insert, then I take constant time but to find out where I have to insert takes order n times. Because if I have to insert at rank 17 then I have to step through that linked list till 17th position and then I would know to insert at that location. 

(Refer Slide Time: 41:03)

 

Once I know to insert at this location then it is easy. I will insert the element in 3 or 4 pointer changes. 

The following would be a java code for inserting at a rank.  
I am assuming the existence of the procedure nodeAtRank (rank). This is the method that I am going to be defining shortly.
What does this method do?
Given a rank, it tells me which is the node at that rank. 












(Refer Slide Time: 42:39)

 


For instance, to insert the node at rank 2, first I will call the procedure with rank 2 it will give me the 2nd node of the 1st diagram because that is the node at rank 2. 

(Refer Slide Time: 43:05)

 



I have to get to the previous node of that node. If I get to this node (next) at rank 2, then I get to the previous node (next.getPrev ()) and this is the node previous to rank 2 which is at rank 1. The new node that I have to insert has to be between next and prev. I create the new node and I set its previous field to refer to the previous node and I set its next field to refer to the next node.

   DLNode next=nodeAtRank (rank);
   DLNode prev=next.getPrev ();
   DLNode node=new DLNode (element, prev, next);

(Refer Slide Time: 43:25)

 
      
DLNode prev=next.getPrev (); ? this was the node at rank 1 and DLNode next=nodeAtRank (rank); was the node earlier at rank 2. In this manner I create the new node at the appropriate place and then I also need to check the previous and next field of the prev and next node. That is what I am doing here.

   next.setPrev (node);
   Prev.setNext (node);
   Size++;

Do not get intimated by this code, it is just doing what is shown in the picture. I am assuming the existence of this procedure DLNode next=nodeAtRank (rank) in which, the given rank will tell me which is the node at that rank in the original list.



(Refer Slide Time: 45:07)
 
I will show you the process of deletion. If I have to remove the element at rank 3, I will first find out the node which is at this rank so I get to the node which is selected in the 2nd diagram and then I have to go to the next node, go to the previous node and update their next and previous pointers. Thus the pointer will point to the next node and previous node and in this manner I will get rid of that node and at the end I will get the 3rd diagram as the final node.

Similarly I can write down the java code for doing this. Once again I am assuming the procedure nodeAtRank, which tells me about the node which is sitting at that rank. 
(Refer Slide Time: 45:27)

 
How do I implement this procedure nodeAtRank?
There is nothing else I can do except that I march to the list and keep incrementing my counter till I reach that rank. I have done essentially that except a small improvement, that if the rank is less than the number of the size of the list by 2, then I start from the header and if it is more than size by 2 I start from the tail. 

Just to small improvement nothing more you do such a thing, because if your list has hundred elements and you are looking for the element at rank 98, then there is no point to start from the header it is better to start from the tail. 

(Refer Slide Time: 45:34)

 

That is as far as the vector abstract data type is concerned except that when I say remove the element at a particular rank or insert the element at a particular rank. As you have seen both the implementations we have a problem. 

Whether we use an array or a list to do that implementation, we seem to require order n time in the worst case, just to be able to find out where the element correspond to that rank is. In an array, we know the element corresponding to that rank is and we have to move the elements when we insert or delete. 








Linked lists are better in supporting node based operations. I have a linked list and I tell you delete this node, if it is a doubly linked list you can delete that node in constant time. If I say this is a node and insert a new node after this node I could insert a new node after that node in constant time or if I say delete the inserted node before this node, again I can insert a node in constant time.

(Refer Slide Time: 47:11)

 

We have the data structure which is very efficient, which can do constant time operations provided that we give access to the node. Some how I access the particular node at which we want to insert or delete. That is what mentioned below. 

    removeAtNode (Node v) and
    insertAfterNode (Node v, Object e)

You can remove at a node or you can insert after a node and you can insert before a node all in constant time. However when I give you access to a particular node then in some sense, I am also telling you how I have implemented my list. Whether it is a doubly linked list or a singly linked list and what are the pointers and stuff like that. 









(Refer Slide Time: 47:56)

 

Suppose I want to hide all those information, so that you can still use node based operation without knowing the actual implementation of how the thing was done. So one can have different implementations. We are going to do this using a notion of positions. 
Position is an abstract data type which intuitively captures the place where a certain element is stored. In your data structure, there is only one method which is associated with the position and is the method element. 
(Refer Slide Time: 48:57)

 

Given an object of this data type position, I can only call this method element on that object and that will tell me about the element which is sitting at that particular position. If this is not making much sense, then think of position as reference to a particular node. Think of it as a pointer, because using that pointer you can access the element which is situated in the node and nothing else. 

You cannot use that pointer to update the next or the previous fields, or you do not even know how the node is implemented. You do not need to know whether the implementer has used a doubly linked list or singly linked list or a circular list. It is an abstract data type which hides all the details and you can only use the method element (), on the abstract data type position. 

(Refer Slide Time: 49:41)

 

With the notion of position, there will be a relative order of positions jus as in the case of a linked list. There is the 1st element in your linked list, 2nd element and the position is referring to the 1st element or the 1st node or the 2nd node or the 3rd node of the list.











(Refer Slide Time: 50:35)

 

Similarly 1st position, the 2nd position, the 3rd position and so on. Given a position that, there is the notion of the position before which refers to the node before that position and a position after that position. 
(Refer Slide Time: 51:23)

 

We can now define a list abstract datatype which uses the positions.


What would this abstract datatype have?
It would have generic methods like size () and isEmpty () and it could have query method, given a particular position I can have a method which asks is this the first position of my list. If it is this will say yes and otherwise say no and whether it is the last position of the list. I can have excessive methods like first (), last (), before (p) and after (p).
(Refer Slide Time: 53:11)

 
First will give me the first position, last would give me the last position, before (p) will give me the position before this position p and after will give me after this position p. I can have update methods like swapElements (p, q). 
What does this do?
Given a positions P and Q, it swaps the contents of these positions. Whatever may be the elements sitting at these 2 positions it swaps the contents. I can replace the element at position p with e (replaceElement (p, e)) and similarly I can insert the element e (insertFirst (e)) at the very first position. I can insert the element e (insertLast (e)) at the last position and so on. Using a doubly linked list you can actually implement all of these methods in constant time.

The list abstract datatype is just as the same as your linked list data structure except that we are getting an abstract datatype implementation of it. We are trying to capture all of those methods that you can do on a linked list as an abstract datatype. This datatype can be implemented using a double linked list and it can be implemented using a singly linked list except that it is more efficient if you implement it using a doubly linked list. In the doubly linked list all of these methods can be done at a constant time. Using a singly linked list some of these methods might take linear time in the worst case.  



Finally we have the notion of a sequence abstract data type. We talked of the vector abstract data type where there is a notion of rank associated with each element. Then there is a list data type where there is a notion of positions and the sequence abstract data type has both of these. It combines the vector and the list abstract data type and it inherits both of these interfaces and that is multiple inheritance.
 (Refer Slide Time: 53:19)

 

Besides the methods that are listed for vector and list abstract data type, it has two additional methods which helps you to go from one to other. Given a particular rank r, the method atRank(r) will return me the position corresponding to this rank. Given a position p the method rankOf (p) will tell me the rank corresponding to this position. 
 
You could have an implementation of the kind which was given in the slide for a sequence. 
In the above slide, given an array in which each element of the array refers to the position and the point 2 is same in both the cases. With the given particular location, I can identify the rank which it corresponds to by looking at the element. 

How is the method rankOf (p) implemented? 
P corresponds to a position, a position here is the thing which is given in the middle of the diagram. Given a particular position and how do I know the rank corresponding to that position. I just look in to the 3rd element that gives me the rank corresponding to that position. Given a particular rank how do I determine the position corresponding to that rank. Suppose you gave me rank 1, when I follow 1st reference, 1 is the position corresponding to this rank. At that position there is an element stored which is newyork. At the position besides the element, there is something else stored which is kind of provides cross reference.

At each of these positions I have an element stored and a rank of that element in my sequence. Suppose I had to insert an element at rank 2, I am going to create new position and the element would sit in that position and 2 would refer to that position and all of these will have to move to one step right. Not only have to move to the right, we have to change the ranks and update the position. Again inserting at the particular rank will take order n time of the worst case and similarly deleting an element. If I had given particular position and if I wanted to delete the element at that position.   

(Refer Slide Time: 56:07)

 

How do we delete an element at a certain position in the case of a doubly linked list?  
You need to think about this. So leave it as an exercise. 

This is a comparison of sequence operations, you can implement a sequence using an array in the picture I have shown you previously and you can also implement a sequence using a doubly linked list. This would be the worst case of running time.  

You can see in the case of an array implementation, if you want to insert an element at a certain rank or you want to remove an element at a certain rank. It will take order n time. If you want to insert after or insert before a certain position, this will also take order n time and if you need to remove an element at a certain position, this will also take order n time.  










(Refer Slide Time: 57:51)

 

Not so in the case of a doubly linked list because then you can just zap out the element from there. You can just update the pointers before and after and do these in constant time. But then what becomes more expensive is, because in a doubly linked list you cannot figure out the rank of an element. I have to go to the entire list to figure out the rank. Any rank based operation will take order n time, whether you want to find the rank of n element or you want to find out the element at a particular rank, find out the position corresponding to certain rank, all of these would take order n time.

We learnt about queues, double ended queues and also how to use linked list and doubly linked list to implement the these data types. Then we also looked at the vector abstract data type, the list abstract data type which is essentially a concretization of the linked list data structure and we also looked at sequence data types which is basically inheriting all the methods of your list data type and your vector data type.

Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 4
Dictionaries

We are going to look at the dictionary abstract data type. We are also going to see how binary search is done, what are all the analysis for binary search and then we will go on to hashing, hash table, how hashing is done, see the collision resolution techniques and then in the next class we will follow up with more hashing techniques. 

(Refer Slide Time: 1:17)

 

A dictionary is an abstract data type that stores elements which can be located very quickly. One example that we can have for a dictionary is to store bank accounts. 














(Refer Slide Time: 1:37)

 

What is the notion of the key, when you store bank accounts? The notion of a key is your account number, bank account has lots of information associated with it, but you are going to access the bank account or data associated with that account by using the account numbers. Thus the account number becomes the key. 

(Refer Slide Time: 1:53)

 



As I said account stores wealth of information, it could be your current balance, it could be the name and address of the account holder and it could be the list of transactions done in the last few days and so on.

When you have to access any of the above information you need the notion of the key. A dictionary is something that stores the elements. When we talk of an element we will mean all of the above additional information which is also given in the slide below. When we talk of key, the key would be the account number that helps us to access the particular information. 

Any application that wishes to do any kind of operation on an account, will have to provide the account number as key. The process cannot be continued without a key. 

(Refer Slide Time: 2:14)

 

The dictionary is basically an abstract model of a database. It is going to store the key-element pairs. The key could be an account number or it need not be an account number in all the case. Suppose if I have the student records then what would be the most natural notion of a key. Your entry number which is not an integer, you may also have characters and so on. Anything uniquely identifies particular student or particular account becomes the key. 









(Refer Slide Time: 3:03)

 

One of the main operations that is supported by a dictionary is searching by key. What are the kinds of method that we have in a dictionary abstract data type? The standard container methods that we have seen for queues and stacks and so on. One is size () which tell us how many elements are there in the dictionary, isEmpty () tells whether the dictionary is empty or not and elements () which returns all the elements in the dictionary. 
 
Then we will have query methods. Given a particular key find the element corresponding to this key (findElem (k)). In settings you might have the same key associated with many different elements and one could have such a kind of settings. We will see example of this kind later. Then given a particular key, you want to return all elements who have that key.

You could have update methods. Given I might want to insert an element e which has a key k, to insert that in to my dictionary, to remove an element with a certain key and to remove all elements which have a key k. 












(Refer Slide Time: 3:33)

 

Just you would think it as a standard thing, but now you should remember the notion of the key and that is crucial for help in searching. Without the key it will be very difficult for us to search through the database.

We will have a special element NIL, which will be returned by an unsuccessful search. It means that if I am searching for a particular key, there is no such element with that key in my dictionary and then my procedure will just return a nill. This is some special element.

(Refer Slide Time: 5:02)

 

One thing that we will keep in mind is that we only require comparison of keys for equality. Given two particular keys, we are not going to say one key is less than the other and one key is more than the other. This is not really required, because all you doing is searching for a certain key. All that is required is, given two keys you want to say whether they are the same or not.  

(Refer Slide Time: 5:31)

 

For instance again for student record, I could have name as your key in that case you know there is really no notion of taking two names and saying one name is smaller or larger than the other. Only operation we require is comparing keys for equality. We do not need any order on our keys.  

Dictionary is the abstract data type then in the next few lectures we will see how this particular abstract data type can be implemented. We are actually going to see many different ways of implementing this abstract data type. You already have seen some ways of implementing this abstract data type. 






 






(Refer Slide Time: 6:36)

 

For instance you could use an array or a linked list to implement a dictionary. Suppose if I had student records, how would you use a linked list to implement this dictionary? Every node would have the key which is the entry number and all the data associated with that particular entry number. It is a very inefficient but it is the way of doing things. There is no notion of a predecessor or successor. 

If we were using a linked list how would we organize it? That is up to you, you could connect them completely arbitrary manner. The nodes could be in completely arbitrary manner or you could might want to organize them some way but you could just throw them arbitrary in to the linked list. That is the implementation of the linked list and it is not very efficient. 

Today we are going to see the hash table and in later class we are going to look at binary tree, red or black trees, avl trees, B-trees. These are all the mechanisms of data structures to implement this dictionary abstract data type which is a very important data type. We are going to be spending quite some time on this particular data type in these lectures.

In java you have an abstract class called java.util.dictionary which lays out the specification also an interface called java.util.map. Before you get more into dictionary you have to understand about the abstract data type. Let us look at the problem of searching. This is the small aside but we will see why I am making this aside and how it will end up to the subsequent discussions. 






(Refer Slide Time: 8:23)

 

The problem of searching is if you are given a sequence of numbers lets say these are your keys in the database. I give you a single number which is my query. For example I could have 2, 4, 5,10,7 are the keys in my databases and I query 5.
What do you have to return? 

You have to return the position of the key 5 in the database where is it sitting. Index of the found number or nil. So 5 is sitting at position 2 and you return 2, while 9 does not appear anywhere and hence return nil. This is the problem and we call it as searching. 

Let us see how we can do that. You are going to see a technique called binary search. I think all of you have seen this before but again we are going to recall the technique and do an analysis of it. 

The key idea behind your binary search is divide and conquer. This is a design technique that we are going to see in future classes also and applied to different problems. In divide and search we really narrow down the range of elements in which we are searching for the query that is the key. Let me take you through an example.











(Refer Slide Time: 10:14)

 

Suppose the elements given in the slide are sitting in an array and these elements have to be in a sorted order, increasing or decreasing for binary search to work. Suppose I am searching for an element 22. 

What do I do in binary search? I will go and look at the middle element, in this case it is the element 14. I will compare 14 and 22. As 22 is larger than 14 that means if that 22 lies in this databases it would lye to the right of 14, because these set of elements are in increasing order. This means that the element 22 has to lie between 14 and 37. We use these 2 variables low and high to indicate the part of the array in which we have to search for our element. Initially we have to search for the element in this entire array but after looking at 14 we have figured out that we should search for the element only after 14 in the 2nd part of the array. 
 
Once again we have to repeat the same thing that is go to the middle element, compare 22 with this middle element. As 22 is less than the middle element which means the 22 has to be to the left of 25. This means we are searching for 22 between 17 and 22 which is given in the 3rd part of the diagram in the above slide. 
 
Once again we go to the middle element, we compare the middle element 19 with 22. 22 is larger and if 22 is their, then it has to be in the particular location and it is their. Hence we can say that 22 is at the location which is given in the slide and return this information. You all have seen binary search before, this is nothing new perhaps. 

I am going to write down a recursive procedure to do binary search. The procedure given below in the slide is just a pseudo code and all of you can read and understand this quickly. 


(Refer Slide Time: 12:15)

 

Once again we had the notion of low and high. Low was the lower end of the range and high was the higher end of the range which we have to search. In the procedure call A is the array and k is the element or the key which you are searching for, low is for lower end and high is for higher end. If low is more than high then basically that means you are invoking something wrong. You just return a null that is the key is not there. Else you go to the middle element which is obtained by taking the average of low and high and check if the middle element is the key you are looking for. If it is the key you are looking for then just return the position where you found the key, so you will return mid.
 
If the key you are looking for is less than the middle element then you know that you have to search in the left part of the array. The left part of the array has a staring location low and the ending location mid-1. You are going to search in that, and this (return BinarySearch (A, k, low, mid-1)) is your recursive call to that procedure. 
     
If it is not the case you come to the else and in this case what you have to do is to search in the right part of the array which means the mid+1 to high. Else return BinarySearch (A, k, mid+1, high) This is how you can do binary search for small pieces of code and this is recursive. 

You can also write an iterative procedure for this, there are exactly equivalent so that you know how to go from a recursive procedure to an iterative procedure. I have just written down an iterative procedure also. 

What is happening in our iterative procedure? The low to begin with 1 and high to begin with n. In the slide below blue color is the element 1 through n, let say in an array and we are doing the same thing roughly except now we are putting it in a loop and updating high and low every time.

After the first step low becomes mid+1, because the element was larger than the mid element A[mid] >k, low becomes mid +1. When the element is smaller than the mid element then high becomes mid-1 and we just go through this loop till we either find the key. In that case we just return the location where we found the key or low becomes larger than high in which case you would come out of this do-while loop and return NIL. 

(Refer Slide Time: 13:54)

 

I have just shown you how to write a binary search in two different ways. You can write it in a recursive procedure or in an iterative procedure to do the same thing. How much time does binary search take?  . You all know that but why does it takes   time. Exactly the size of the problem is halved at every step. Range of the items that we have to search in is halved after each comparison. If essentially the range of the elements in which I am searching for my key is n then after the first comparison it goes down to . After the second comparison the range goes down to   and so on. After log n comparison the range would go down to 1. When the range goes down to 1, either it is that element or it is not that element and you can stop. You will roughly require  comparisons. If you have an array implementation that is your elements sitting in an array, then you can go to which ever location you desire in constant time.








(Refer Slide Time: 15:59)

 

Then each of these comparisons can be done in one unit of time that is in constant time and you can do the entire process in only O (log n) time. When you do not write any base for log n, we will understand that it is base 2.

Suppose the numbers in that array were not in sorted order. Till now we assumed that the numbers were in increasing order. You can do binary search even if the numbers were in decreasing order but if they were in sorted order you can do binary search.

(Refer Slide Time: 16:52)

 

If they were not in sorted order and still you are searching for a key k. There is nothing else you can do, but to go through the entire array one element after the other and compare your key against that. That is the only thing you can really do. 

The worst case time, then becomes order n. If you are lucky you might get the element at the beginning of your search. On an average you will get it, it may be 1st time you got it at the very first position, some other element you searched and got at the 3rd position. Hence in an average you are going to spend some order n time.  

This is really the best you can do, if the array is sorted. You can see there is a huge difference coming up already. By sorting, if you were to sort the element to begin with, then you can search much more quickly. Given in the slide is a small pseudo code which is just saying that you run through the array one element at a time and compare your key against the element in the array.

(Refer Slide Time: 18:16)

 

That ends my aside on searching and I am going to go back to my dictionary problem. I am going to look at the setting where you are asked to implement a caller id facility for a large phone company. Given a particular phone number when a call comes in, based on the phone number you can figure out the name of the person who is making the call. 









(Refer Slide Time: 18:30)

 

That is what the company wants to do. Given the phone number you want to return the callers name. Let us assume that our phone numbers are all 8 digit numbers as in the case in Delhi. The range of phone numbers would be   that is of 100 million phone numbers. The number of different phone numbers is much less than this. The range is 100 million, but may be Delhi has only about a million phone numbers. That is because not all numbers are present. 

There are n phone numbers and n is much smaller than r. r is the range which is 100 million because the phone numbers are 8 digit numbers and n is the actual different number of elements in your database. You want to do this as efficiently as possible. 


















(Refer Slide Time: 19:14)

 

You can use an unordered sequence to do this. It means suppose the numbers given in the slide below were the phone numbers. I have not put down the 8 digit numbers but let say these were the phone numbers and you could just put them in a list in any arbitrary order.  

How much time does searching take? Order n, because you cannot do anything else but to traverse through this list. In the worst case you might have to go through the entire list. Hence it will take order n time to search for a particular phone number. Given a particular phone number, if you have to return the name of the person, you will have to take roughly order n time to do that.  



















(Refer Slide Time: 21:44)

 

The list in the above slide is an unordered list that is there is no particular order. How does one remove an element? Suppose a particular person decides to give up his connection. You have to remove that particular data record from this list. First you will search for the record then you will remove it. Searching itself takes order n time then removing also takes at least that much time. Once you found where the element is then you can do some small modification to do the entire thing in order n time.

Why does inserting take only constant time? Because it is an unordered list, you do not really care for where you are putting the elements. You might put at the very first location. Thus inserting takes very less time. It is not clear whether this particular implementation is good for this application. 

But there are certain applications in which, this way of doing things is faliable and one such application is where you have to maintain log files. When you have to maintain some kinds of log file for instance any kind of transactions that are happening in the database you try to maintain log files.  












(Refer Slide Time: 22:45)

 

If there is any problem you can figure out, you can revert the transaction whatever was done or for instance in your system administration you would keep track of all the various activities that were happening in your system and maintain the log of them. 

For log files, it is very rarely that you need to do search or removals but you need to add data frequently to your file. Every time when some transaction happens you need to add. Insertions are very frequent but searches and deletion are much rare. In that case this implementation is good because insertion takes only constant time. 

Really you have to see between these three operations (search, remove and insert) for which is the operation being performed more frequently, to decide what type of data structure used to implement the dictionary data type. 
















(Refer Slide Time: 23:24)

 

You can use ordered sequence also in that case let us say the elements were put in an increasing order of the key. Searching takes log n time. Log n provided, you had some kind of direct access mechanism into the thing which is used in array or some other thing which let you go to whichever element you wanted to go. 

Searching takes only O (log n) time, inserting and removing will take order n time because if all of them were put in an array. Then if I have to maintain the sorted order and to insert the element in a particular location, I have to shift everything to the right of the element. Insertion will take order n time in the worst case and similarly deletion you may have to move it back. We have seen examples in previous class.

Why does insertion take order n time? First we have to search for where the element has to be inserted and once we know the position then this is an array in which all the elements were put in an array. We have to create space there by moving everything to one step right. In the worst case we might have to move order n elements to the right. 
  
What is order n+ log n? It is order n, you have to recall your big-oh notation, order n+log n is order n. This would make sense, when you have to do a lot of searching in your dictionary but not many insertions and deletions from the dictionary. There is one other way which will be useful for our subsequent discussion and that is as follows. Let us say I take an array of size  which is a huge array. 







(Refer Slide Time: 25:28)

 

Ankur had a phone number of ?9635-8904 ?, I go to location ?9635-8904 ? in the array and put Ankur name there and the additional information associated with that in that array. At that very position which corresponds to Ankur phone number. All operations insert, search and delete will take only constant time.

Why because I just have to insert a caller id capability. What does that mean? Given a phone number, I want to know who is that person. Given the phone number I just go straight in to that location of the array and retrieve the name. Given a phone number I want to create a new phone connection, so I take the phone number and go to the particular location, put the name of the person who got the connection.
 
Similarly if I want to delete a particular phone connection, I just go to the particular location and I remove the element. All the operations take only constant time. What is the bad thing with this implementation? You are wasting a lot of space. It is not that you cannot do all of these operations very quickly. You can but here space is turning out be an issue.  

We are going to use what is called hashing. We are going to use a hash table which will tell us do the things quickly, I have said O (1) excepted time. It will not take two much space either and let us see the idea. 

What was the problem with the previous technique? In the previous technique we had 100 million phone numbers, so we had to create an array of size 100 million. But let us say there were only 1 million users, most of the array was getting wasted. There was nothing in their. 



(Refer Slide Time: 27:18)

 

Suppose I could create a smaller array with only 1 million locations in it and mapped those 1 million users to locations in that array. That is what we are going to do. 
 
Let us say in a hypothetical setting they were only small number of users. I was only trying to keep the phone numbers of 5 of my friends but still I wanted to do something fancy. I create an array of 5 elements and I take Ankur phone number and I compute this value 96358904 mod 5, which is the size of my array. I get a number between 0 and 4. In this particular case I would get 4. Depending upon what I get, I put Ankur at location 4 in my array. I am not using too much space and may get away with constant time for insertion and delete. Let us see whether we can or we cannot do that. 

Let me take another example just to make sure that you understand about the idea. Let us say keys are not phone numbers but entry numbers of students in this class. Your entry numbers looks something like this ?2004 or 3 or 2?, then you have 2 characters and then you have another 5 digits at the end. There are about 100 students in this class, the range of this numbers is huge. This is infact, I do not even know the range because they can take different set of values. But I would like to create a table of size of about 100, because there only 100 people in the class, why should I spend more space than that.










 (Refer Slide Time: 29:37)

 

Let me pick up a hash function. This function which I was using in the previous example mod 5 is also called as a hash function. I am going to pick up a hash function which does the following. It takes the last 2 digits of your entry number. In this case ?2004CS10110?would get mapped to location 10. It just picks up the last 2 digits of the entry number. That is what I am going to do.

I am going to take each one of your entry numbers, going to look at that last 2 digits and I just have a table of size 100. I am just going to put you in that location, depending upon the last 2 digits. What if I had a clash? Suppose I had another person with this entry number 2004CS50310. I do not know if they are in this class, there could be another person also. The problem is these 2 are going to the same location number 10. We will come to this problem but if this problem did not arise then did you see that we are in very grade shape. Then you can do insert, delete and search all in constant time because it is very much like the array implementation that I showed you. Except that it does not waste all the space. 













(Refer Slide Time: 31:12)

 

Let us see how we can address the problem of clash. If 2 elements are getting mapped to the same location in our hash table, this is called the collision. We have to find a way to address this issue. How do we deal with the 2 keys which mapped to the same spot in our hash table? We use the concept called chaining. There are many ways of addressing this issue and today in our class we are going to look at the very first, simple technique called chaining.

(Refer Slide Time: 32:07)

 

The blue color thing in the slide below is my hash table. I am not going to put the elements in this hash table but I am going to have a linked list starting at each of the locations. I am going to put the elements in the linked list. Suppose my hash table had only 5 locations in it. May be I was just using the hash function which was taking the key and computing modulo 5 or some other thing. 
 
If 2 or more keys were mapped to the 2nd location, I will just keep adding them to the linked list. As you can seen from the picture given in the slide below, it was the case 3 keys were getting mapped to location 2 and 1 key was getting mapped to location 4. There were no keys getting mapped to locations 1 and 3. This does address the problem of collision but what is the other problem does it create.

(Refer Slide Time: 32:34)

 

While we have resolved the collision problem and we are not able to do things in constant time any more. In the worst case all the keys get mapped to 1 location in this hash table. If all of them get mapped to the same location in the hash table then your data structure reduces to a linked list data structure which we know has the worst case time of order n for search and delete. Still insert has constant time. 

Whether each of the nodes in the linked list will contain both the identity in the phone number, caller id example? Each node will have both the phone number and all the data associated with that person who sits there. 









(Refer Slide Time: 34:07)

 

This is quick recap about how we are going to insert and delete of an element. For all of these three operations you have to do essentially the same thing. You have to use your hash function h, to determine where that key is in this table. We have seen 2 examples of hash functions. In one case we said we will just take the key modulo 5, in the other case we said we just take the last 2 digits of the key. But they could be many different kinds of hash function and in the next class we are going to see what are the different kinds of typically used hash functions. 

The last 2 digits can also be regarded as modulo 100. The reason I did not write modulo 100 because that was not an integer at all. It was your entry number and it had some characters in it. You are going to use your hash function to find the position of the key in the table. Then if you are going to search or insert or delete, do that in the linked list associated with that position. 

There are options that you might want to maintain the list which is in 2nd position in a sorted order, you might want to keep it in unordered. If you want to maintain in a sorted order then insertion is going to take more than a constant amount of time. If you want to keep it in unordered then insertion is going to take only constant amount of time because you can just insert at the very beginning or at very end of the linked list. 









(Refer Slide Time: 35:46)

 

If you want to insert an element at the very end of linked list then you do not need to traverse the entire list to reach the end. You can always have another pointer which always points to the end of the linked list and use that to update. Although there is no reason why you want to insert at the end. 

Suppose if you want to insert at the end, you could also do that in constant time always. By maintaining one pointer we will have two pointers from the 2nd location, one going to the front of this linked list and one were going to the tail of the linked list. Use that pointer to add an element at the end of the linked list. 

You can do ordering if you want to keep it ordered. We are not saying that in the hash table you have to keep anything ordered. If you want to keep it ordered you can do whichever if there is a notion of order on your keys then you can use that notion to order the elements. 

An element with key k is stored in the slot h (k) in which h is the hash function and h (k) is the value of hash function. The hash function is mapping the universe of all keys, let us say U to slots of the hash table. If the hash table was of size m, so it is a function which is mapping from U the universe to 0 through m-1. We are going to assume for the rest of the discussion that the time to compute the hash function for given key k is a constant time. Because quite often we just have to do simple arithmetic operations to compute the value of the hash function. We are going to assume that the time taken to compute the hash function is independent of the number of elements in the table.





(Refer Slide Time: 37:36)

 

As far as the choice of hash functions are concerned, we are going to see in the next class what are good choices of hash function. Lot of research has done in this and then we will see what are the kinds of hash function that people typically use. I just gave you 2 simple examples of hash function so as to motivate the concept. 
 
What is the good hash function? A good hash function is one which tries to distribute the keys uniformly over the table. It should not map all the keys to location 1 or location 2 or any such thing, because then there would be too many collisions, your data structure would start looking like a single linked list and that is not what you want to have. You want to have a hash function which distributes things uniformly over the table. Why uniformly, so that each of the list is small. 

















(Refer Slide Time: 38:53)

 

An ideal hash function would do something like the following. It would take an element and let us say I have a table of 100 locations. It will pick at random one of those 100 locations then throw the element there. This kind of it shows that every location would have roughly the same number of elements. But this is not a hash function what I just said, you can not have a hash function which takes a key and puts it at a random location. 

Why this is not a hash function? Because when I am searching for the element, where am I going to go and look. I do not know what random location it had picked at that point. While this is an ideal hash function, it is not really a hash function.
 
But for our analysis we are going to assume such a hash function. The hash function is just essentially does the following that it takes the element and throws it randomly, uniformly with same probability in one of those locations of the table. We will call this as simple uniform hash function and we are going to use this for analysis. 
 
We will use another term called the load factor of the table which is just a number of elements in the table divided by the number of slots, the size of the table and we will call this load factor alpha. 
               

What is going to happen when we are trying to search and our search is unsuccessful? It means that I took the element I computed the value of the hash function and I went to the particular slot in the hash table. I went through the entire linked list and did not find the element at all. 



(Refer Slide Time: 40:52)

 

How much time do you spend? I spend time propositional to the size of the linked list that I have to go through. Because I said computing the hash function takes constant time, you did not take any time to go to the right linked list but once you went to the right linked list you still have to step through the entire linked list, follow pointer by pointer till you reach the end. The time is propositional to the size of the linked list. 

What is the average size of your linked list? If there are n elements that are thrown in my table and m is the number of slots and if I had this simple hash function which was essentially distributing the things uniformly then on an average you would except that each linked list is of size   which was the load factor of the table. The excepted number of elements that need to be examined is  and the total search time where I am using this 1, 2 denote the time taken to compute the hash function is roughly 1+ .

This tells you that if your is let us say only a half than the excepted search time would be roughly a constant. O (1+ ) is excepted under the ideal hash functions. You can always create a bad hash function for which the time taken will be order n.  O (1+ ) represents the time that is spent in computing the hash function. 









(Refer Slide Time: 42:57)

 

We would not want a hash function for which every thing is getting mapped to one location. Because that is a linked list, why would you want to do something like that. This again brings back to the same question, the efficiency of this data structure relies critically on the hash function we choose. We will see what are the good hash functions in the next class.

Designing hash function is much more of an art than science. You have to really look at the data to design a good hash function. I am going to show you in the next class some principle behind the hash function that is what kind of hash functions one should use. But there is no theorem which says that this is the best hash function and you should always use this. 

What happens when we make a successful search?  That was for unsuccessful search but when I make a successful search, it means once again I took the key, computed the value of the hash function, went in to the appropriate linked list and then I am walking through the linked list. But I do not have to reach till the end of the linked list, at some point in the middle itself I may be able to find my element. 

How many elements do I have to traverse in this process? The position at which it was found but how do I know that. What is the excepted time I would take overall? Excepted time I mean, the average time I would take to search all those n elements that are there in the database.  

You can have many different ways of arguing it but let us do it in the following way. Suppose I was searching for the  element which was inserted in to my database. The element or the key that I am looking for was inserted in to the hash table when there were only 9 elements. This was the  element, if it was the  element that was inserted then the excepted length of the list in which it was inserted was . That is what we argued just now. 

(Refer Slide Time: 45:17)

 

Exactly m is the number of slots in the hash table. In the case of successful search excepted number of elements examined is 1 more then the number of elements examined when that particular element was inserted. When the  element was inserted i=10, I went through the linked list in which the element was inserted and appended at the very end. 

I had to compare that element with all the various elements. When I insert the element basically it is same as that the number of comparisons I have done is, 1 more than the number of comparisons I would have done in an unsuccessful search. We have to go through the entire linked list when we are inserting because to make sure that the element is not already their then we might insert at the end. We could also insert at the beginning but it is the same thing.  












(Refer Slide Time: 47:15)

 

One could have an analysis which looks like the following.  This is not critical you can all prove it in different manners. I am looking at the element 1 through n, there were n elements in my database. When the  element was inserted, then the excepted length of the linked list at the end of which it was inserted is roughly  and the 1 is for our hash function computation. 
This is (1+ ) roughly the excepted time required to insert at the  element. We are just summing this quantity up over all the n elements and taking the average. If you just go through this math, you will get something like the following and many of you could have figured this out on your own.
                     1+  

The average time would be roughly the excepted length of the list divided by 2. Whenever we are doing average time computations, when I said take a linked list and what is the average time to search for an element. You said I might have to go till the end of the linked list or find the element right at the beginning, on an average I will take half the length of the linked list. 








(Refer Slide Time: 48:27)

 

You are seeing a similar kind of behavior in this (1+ ), in which  is the very low order term which you can just ignore. What you are seeing is something like , we do not really have to go through this math but you can also follow it, this  is more intuitive. One could just say that the average time for successful search would be more like . Again it is O (1+ ) where 2 is not important. Both for successful and unsuccessful search we are taking a similar kind of time. 

What should   be that is what should be a good choice of ? Thus the   is the load factor of the table that is the number of elements in the table divided by the number of slots in the table.  













(Refer Slide Time: 48:52)

 

If I pick the size of hash table to be the number of elements that I am going to be inserting in the hash table, then   would be roughly a constant O(1). All your searching, insert and delete would take constant amount of time. In the excepted sense I mean when you have an ideal hash function which you can not really have. 

What if we did not know how many elements we have to insert then what should we do? 
With what size our hash table should start? We used a concept of growable stack, so the same idea is used in many of this data structures. You start with some thing small and if the number of elements you inserting becomes so large that the sizes of linked list become very large, then it perhaps time to move the entire set of element in to a larger hash table.

















(Refer Slide Time: 49:15)

 

Either you have to compute a new hash function or we will see how to modify these things. How you can do small modification to the hash function so that you can put it in the larger hash table. One should design your hash function keeping in mind that you might have to go from a smaller table to a larger table and even to a larger table and so on. 
 
What will happen to the space when the number of hash table slots was propositional to the number of elements?  It depends upon the big-oh. Let us say we pick the number of hash table slots to be equal to the number of elements. There is no problem, suppose n was a 1000 and m was also a 1000, this hash table can accommodate any number of elements. It not just that it can accommodate only 1000 elements. Why because in the linked list you can attach any number of elements that come. 

It is just that performance of the hash table would deteriorate if you had 10,000 elements coming because then each linked list would be of size 10 roughly, may be more or may be less. On an average the linked list length would be 10 in that case it make sense to move to a larger hash table.  











 (Refer Slide Time: 51:25)

 

If you know that the number of elements is only 1000 and you create a hash table of size 10,000 then there is wastage of space. You should always start with a small hash table and if need be grow it, rather than starting with a very large hash table and having wastage of space. 

Today we saw binary search which many of you have seen before, we also saw a little bit of hashing and we saw the dictionary abstract data type. In the next class we are going to continue with hashing c concepts of good hash function and see other ways of resolving collision.  



 





Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture? 5
Hashing (contd.)

Today we are going to continue our discussion on hashing. In the last class we saw about the hash table, the concept of hashing and also saw how to resolve collision in hashing using linked list. That method of collision is also called chaining. 
 
Today we are going to look at 2 other methods for collision resolution, linear probing and double hashing. We are also going to spend some more time discussing how the good hash function should look like. 

(Refer Slide Time: 01:34)

 

What is the good hash function? The function which can be computed quickly and as said in the previous class, it should distribute the keys uniformly over the hash table.  











(Refer Slide Time: 01:53)

 

All the keys should not get mapped to the same location because then the performance of hashing would become as worse as that of a linked list. Good hash functions are very rare and there is a famous paradox called birthday paradox. There would be about 35 or more students sitting in the class. There is a very high probability and you can actually compute that probability in which 2 of you would have the same birthday. Although you would think that there are 365 days in the year and if each one of you were to have one of these days as a birthday then there is very small probability that 2 would have the same day. But that is not the case, even with just 35 people you would have fairly high probability that 2 people would have same birthdays.  

(Refer Slide Time: 02:10)

 
The same kind of thing is happening here. Your days of the year corresponding to your slots in the hash table and even if I were to take a key and put it randomly in one of those slots there is very high probability that 2 keys would end up in the same slot that is birthday paradox.  

(Refer Slide Time: 02:50)

 

Collisions may take place in any kind of hash function you use. Then there is also a problem of how to deal with non-integer keys. In fact we saw an example in the last class where the keys were telephone numbers and we had returned the telephone numbers with hyphen. 

How did we treat telephone number as an integer? We just dropped the hyphen in between and then we thought of it as an integer. You are going to see some more techniques of converting non-integers keys in to integer ones. The other example that I had taken in the last class was your entry number where again the key was a non-integer because it had C S Y or some other thing. 













(Refer Slide Time: 03:22)

 

We have to convert in to integers and what we did in the last class was as I said, those keys are just going to take the last 2 digits as the hash function value. We are going to see some more techniques of converting non-integer keys into integer ones. Hash function can actually be thought of as being in 2 parts. There is a hash code map and there is a compression map and these 2 together make up a hash function. 

A hash function is basically a mapping of keys to indices of a hash table. Your hash code map, maps the key to an integer. If your key is already an integer then there is no need for this but when your keys are not integer keys then you will have to 1stconvert them in to integer keys. Key? integer, this integer could be from an arbitrary range but we need to bring it to the size of our hash table. 

















(Refer Slide Time: 04:17)

 

If n is my hash table then I need to bring this integer to the range of 0 through n-1, so that it can be mapped to an index of my table. That part we will call as compression map. We will see what kinds of functions are used for hash code map and compression map. Another important requirement of hash function is that if 1 key gets mapped to a certain index then the next time when I want to map a key it should get mapped to the same indexed location. It is not like, the next time it should get mapped to some other indexed location. 

(Refer Slide Time: 05:03)

 

In the last class we took an example of key which was 2004SA10110 and we mapped to location 10. I cannot have a hash function which sometimes maps to location 10 and sometimes maps to location 13. There could not be any kind of randomization happening there. Why is that because when I insert it, I may be mapped to location 10. When I try to retrieve or search for it then if it gets mapped to location 13, I would not know the location of the key. It should map equal keys to the same indices and of course and we should try to minimize the probability of collisions. 

Let us look at the popular hash-code maps. The hash-code map is the part which converts your key to an integer. One thing is that we could just take anything as the bit pattern and interpret it as an integer. If you have a numeric type of 32 bits or less, we can reinterpret the bits of the number as an integer. Your key which has more than 32 bits in it which is a long or a double real number which takes more than 4 bytes, then you can take it in chunks of 32 bits and add them up.

(Refer Slide Time: 06:14)

 

Take the first 4 bytes and add the next 4 bytes to it and so on to get eventually some 32 bit and that could be an integer you are working with. Such a kind of tree could also be used to compute the hash code map of a string. Suppose I was using the key as your name. Given a particular name, let us say Ankur I want to convert it to an integer. One possibility would be take the ASCII code of A, N, K, U, R add them up and that I will interpret as an integer. 

Why is this a bad strategy? Why would the number of collisions be high? Why would the sum of 2 different names be the same? Only if the order is different and that happens for many different words. It is not the case for the names, but many words in the English dictionary would be obtained from the same letters.

If you have 2 words such that the letters was same as g o d and d o g, then when you sum up the ASCII values they will be going to the same location only. We have to avoid such a kind of things. Even if the words were not the same but A was replaced by B and N was replaced by M even then we will end up with the same. These are all the reasons for why it is not a great strategy. Especially when you are trying to convert character strings in to an integer. 

One technique used in such settings is called as polynomial accumulation. You have a certain string and  is the ASCII code for the 1st character of the string and  is the ASCII code for the 2nd character and so on. You are going to think of it as a polynomial whose coefficients are ,  up to . 
                        
The above given expression is your polynomial and you are going to evaluate this polynomial at a certain value of x. The evaluated value is going to be the integer corresponding to this ( ) string. That integer might be from a large range then we will use the compression map to map it to the table. But 1stwe are looking at the hash code map were in we are trying to convert a string or a non-integer data in to an integer. We are looking at the setting where the string we have is this ( ) and we are trying to convert it to an integer. Evaluate the below given polynomial at some integer value. 
             

(Refer Slide Time: 09:24)

 

The value of x has been the experimental stuff, people have looked at and found that if you work with (x = 33, 37, 39 or 41) these values and if you take an English dictionary with about 50, 000 words in it and use this technique to convert your words in to integer. Then you will not get too many collisions. At a particular time you will have at most 6 collisions. There is no theory behind it, this has been observed experimentally. This is an experimental study in favour of this kind of a hash code map.  

(Refer Slide Time: 11:31)

 

Let us look at some compression map. Given an integer you have to map it to the small range of your table. One natural thing would be that k is your integer and your table is of size let us say little m. Just do k mod m and k mod m will give you some integer in the range 0 through m-1 where k is the key and m is the size of the table. 
 
Suppose you were to choose your m and let us say your table is of size 1024, m is basically . When I am taking some integer mod  then essentially that means I am taking the last 10 bits of that integer. Write the integer in its binary representation and then when I am taking mod 2 that means I am taking the last bit of the integer. If it is 0 then I get 0 always, if it is 1 I get 1. If I am taking mod 4, I am getting the last 2 bits. So if I am taking mod  then I am getting the last 10 bits. 

All the integers which have the same last 10 bits would get mapped to the same location. This is bad because we are forgetting the other bits of the integer. We are just taking some small set of bits that is the last 10 bits based on the hash function. Hence one should not do such a thing.

In this case if you are using the simple compression map then you should not pick up the size of your hash table to be some power of 2. In fact it helps, if you take the size of the hash table to be a prime number.  





(Refer Slide Time: 13:24)

 

Let us look at an example. Suppose I had 2000 strings and I am trying to put it in hash table. I will try to pick the size of my hash table let us say at 701 which is the prime number. This will ensure that on an average I would see only 3 strings per location that is 701 x 3 is roughly 2000. In my chaining, I would have 3 as the length of the linked list. 

One important thing is that one should not pick up the size of the hash table close to a power of 2, because the same kind of effect will start happening when you have the size of the hash table to be exactly the power of 2. If you are going to use that kind of a compression map which is just key mod m, then keep in mind that m should not be a power of 2 or even close to a power of 2 and preferably it should be a prime number.  

Things do not work when you see a lot of collisions happening. Lot of it depends upon the data and the keys you are trying to insert in to your hash table. These are generic principles which if you follow will improve the performance. There have been instances in which we did some experiment where it is better to take a number which is not necessarily a prime. 












(Refer Slide Time: 15:35) 

 
 
What are the other kinds of compression maps? There is other compression map you can use, essentially first I read out the 2nd part of the above slide. Suppose your keys are in the range of 0 through , recall now assuming that our keys are integers because we first used the hash code map to convert anything that was non-integral in to an integer. The keys are in the range 0 through max, so first covert them from this range (0? ) in to a range through  times A.

Essentially we multiply each key with A where A is some number between 0 and 1. First we converted to this range (0? A). Now we take the fractional part of the each key that corresponds to k A mod 1. As a consequence we get a number between 0 and 1 because we took the fractional part. We have to map it in to the range 0 through m-1 so I can just multiply that number I get between 0 and 1 by m. This number (kA mod 1) was between 0 and 1 and when I multiply by m I get fractional number. That is why I took the floor function which means round down. Thus I rounded that number down to the nearest integer. 

                            

I will repeat it again. You first took a key and multiplied by A where A is some number between 0 and 1. Then from that you took the fractional part of that number which is again something between 0 and 1 and then you rounded it down. This is another popular compression map. You could have done something different, for instance I could just take this (0?  A) and map it to (0?m-1) directly. Although it is not clear about how would you do it perhaps divide by m or some other thing. This is one of the popular ways of doing things. 

(Refer Slide Time: 17:41)

 

In the following case the choice of m is not critical. Even if m was the power of 2 now, the same kind of thing that happened before would not happen because we have done a lot of jugglery. We have taken that number, first we multiplied it by A which was a small fraction then we took the smaller fraction part and then plotted it to the range 0 through m. Here it is not critical that m not be a power of 2, we could use m as . Some evidence if we use A as something like  then it turns out to be good. If we use that value of A then it is called Fibonacci hashing.
 
Most of this is experimental without significant theory behind it. So if you might want to read more about hash function there is a nice book by Ronald Knuth on sorting and searching which covers hash functions in detail.  















(Refer Slide Time: 18:51)

 

There is another technique for a compression map called the Multiply, Add, and Divide which says the following, take your key multiply it by a and add b. Thus a and b are 2 fixed numbers. Then compute modulo N where N is the size of your hash table, sometimes I use m and sometimes N. The first technique was just k mod N but now we are doing something different. We are multiplying by a and adding b. 

Here a should not be a multiple of N. If a were a multiple of N then a mod N will be 0, so ak mod N is also 0. For any key you will always get mapped to the same location b. In fact a and N should be co-prime if possible to avoid any kind of patterns happening. Such a technique is used in your random number generator also. You might have used the function random as a part of your programming. If you specify the range it gives your random number in that range.

How does it come up with a random number? Many of the random number generators are based on the technique called linear congruential generators. They start with a certain seed. 












(Refer Slide Time: 20:51)

 

Seed is a starting value which could be user defined, you could provide what the seed is or it could be a random number generator which could just take the system time at that point or some other information and use that as a seed. That seed becomes the initial k value and then you compute this quantity ( ) and the value you get becomes your random number. 

                  

The above function will give random number in the range 0 through n-1. Then for the next random number, you are going to use k which is the last value you return. We will use the last random number generated as a value of k and once again compute . You will use the value you get for the next time and so on. This is how you generate random number. Such numbers are actually called pseudo random number because they are not truly random. Once you know the seed you can actually figure out all the numbers that you get.    











(Refer Slide Time: 21:39)

 

There is another technique called universal hashing which I am not going to go in much detail, I will just briefly tell you the idea. I pick up a hash function and tell you what the hash function is. You can always come up with set of keys such that all those keys using my hash function will get mapped to a very few locations. 

I think of you as an adversary who is trying to make life difficult for me let us say, by picking key which all get mapped to a very few locations in the hash table so that I have to spend a lot of time doing insertion, deletion and searching. 

One solution I can imply is that I do not even tell you the hash function which I am going to use. That means I am going to have a bunch of hash function let us say 15 different hash functions and before the process starts I am going to randomly pick 1 hash function out of these. Then with the keys that are given to me, I am going to use this hash function to put the keys in to the table. I have to use this same hash function for inserting all my keys, for doing the search, deletion and so on. 

For one run of the hash table implementation I have to use the same hash function. I cannot change the hash function in the midway but the next time when I invoke this program, I could perhaps use a different hash function because that I have picked up randomly from my set of hash function.  So even if you came up with the bad set of keys for one of my hash function, may be that is the hash function I did not pick up at all, when I was doing my implementation. 






(Refer Slide Time: 23:43)

 

There are some results which say that you can pick up a collection of hash function and such a collection of hash functions is called universal, such that for any 2 keys the probability that they get mapped to the same location is no more than  . 
                

As I said, this is just a brief idea about the universal hashing and I am not going to see in detail. When you do your next course on algorithms in the 3rd year you will see more of universal hashing. So that is as far as the hash function is concerned. When you use hashing you will get collision, there is no way around it and one technique we saw in the last class was to resolve collisions what we call chaining. 

If many keys go to the same location you just chain them up and put a linked list there. You can still do insert, search and delete by doing that operation in the linked list. You are going to see 2 other techniques today which fall under the general class of open addressing. One of these is called linear probing and the other is double hashing. 










(Refer Slide Time: 24:31)

 

Open addressing differs from chaining in the following key fact. Recall in chaining none of these elements were actually stored in the table.  

(Refer Slide Time: 25:06)

 

They were all stored outside the table, in the table all we had was a reference to the starting element of the linked list. The table was only storing the pointers or the references to the first element of the linked list. But now we are going to put all the elements in to the table itself. As I said hashing could map 2 elements to the same location in the table, we cannot put both of the elements to the same location. Still we want to put all the elements in the table, we will have to find some other locations for the element. Clearly if all elements have to reside in that table, then the number of elements that we are trying to put n has to be less than the size of the table which is m. 

(Refer Slide Time: 26:12)

 

I am going to work where m is the size of my table and n is the number of elements that I am trying to put. This was not a requirement for my chaining technique. I could have the number of elements as larger than the size of the table, because there the elements were not residing in the table. They were residing in the nodes which were a part of the linked list. Each entry of the table is now either going to contain an element or it is going to be null.  

It is going to be null which means that does not have any element in it. When we are searching or inserting or deleting, we have to probe the elements of the table in a suitable manner. 

We are going to think as if we are modifying the hash function a little bit. The U is the universe from which the keys are picked. Our hash function is mapping the keys, earlier this part {0, 1,? m-1} was not there. We were mapping the keys (U) to 0 through m-1 and that would tell us where this key sets, for instance in the case of chaining. We are going to have a second parameter and when I am trying to insert the key that will be my first probe.  








(Refer Slide Time: 26:59)

 

I will compute the value of the hash function for that key (k, 0) let us say for 0th probe and I obtained h (k, 0) as the value of my hash function. I look at the 0th location in the table, if that location is occupied then I have to look again. When I look up the next time I will have a value of 1 as the 2nd parameter. 

The 1st parameter is still the key k. I am going to compute the value of the hash function for (k, 1) which gives some other location in the hash table and so on. I am going to different location in the hash table till I find an empty location, if the operation was one of insertion.  

Depending upon the hash function we will have many different techniques. The hash function h is really determining sequence of slots which are examined for a certain key. The U was the range of the keys, U is the set which specifies the collections of keys that we have. 

The number of elements we are trying to insert in to the hash table should be less than the size of the hash table. If I try to insert all the 100 students of this class to a hash table that I create then clearly the size of the hash table has to be more than 100. Because each of this student has to go to 1 location of the hash table.  









(Refer Slide Time: 29:54)

 

The first technique under open addressing is called linear probing. I have the key k which I am trying to insert. I have a hash function h, I compute h (k). This probe =h (k) is the first place of the hash table that I am going to look at. If table [probe] is occupied then I just go to the next location. So probe is incremented by one and then once again I check if it is occupied. 

If it is occupied then I increment again till I find an empty location and at that point I will put the element k. This is the guiding principles that if the current location is used, just go to the next location. The mod m is used to do rap around, if you reach the end of the table then you start at the beginning. 

Your question is what happens when we retrieve the keys. We will come to that in a short while. When you are trying to insert, you compute the value of hash function and you go to a specific location as specified by the hash function for that key. If that location is occupied that is there is an element already sitting there, you go to the next location and if that is also occupied go to next location till you find the empty location. 

One advantage it has over chaining is that it uses less memory. In chaining you have to keep track of references. Each of your nodes should have place for the element that it is storing. But it should also have the reference to the next node so that space is wasted. But this technique might end up slightly slower than chaining.  







(Refer Slide Time: 32:19)

 

Let me show you an example. My hash function is k mod 13, a very simple hash function. My keys k are integers and I am trying to insert these keys in to the table. 13 is the size of my table and the location is from 0 to 12. The 18 mod 13 is 5, so 18 goes to location 5 because at that point the table was empty, so it can come there. 41 mod 13 is 2 so 41 goes to location 2, 22 mod 13 is 9 so 22 goes to location 9.

Till this there is no problem in inserting, as the table is empty. 44 mod 13 is 5, we want to put 44 in the 5th location. But this location is already occupied by 18, so 44 will have to search for the next location. As the 6th location is empty we put 44 there. 59 mod 13 is 7, we place 7 there as that location is empty. 32 mod 13 is 6, as 44 is sitting in 6 we go to the next location then 59 is sitting at that location, again we go to the next location and as that location is empty we put 32 there. 31 mod 13 is 5, so we should put it in 5th location but this location is occupied with 18 and the continuous locations are occupied by 44, 59, 32, and 22. 

So we go to the next location which is empty and we put 31 in that location. 73 mod 13 is 8, as 8th location is already occupied we check for the next locations and we put 73 in the 11th location. All the elements are sitting in their respective position that is 41 at location 2, 18 at location 5, 44 at location 6 and so on. This also shows you one problem with this technique. The elements tend to aggregate, form clusters so you might have to go through many locations while searching for an element. 

How would one search? The hash table is given in the slide below which is after inserting those elements. Suppose we are searching for key k, we are going to compute k mod 13 because that was our hash function. Then this (k mod 13) is the first location we go to and after that if we do not find the element we do not say that the element was not in the table, rather we go to the next location. 

(Refer Slide Time: 35:27)

 

If at the next location there is some element present then we go to the location following it and so on till we either find the element or we reach a an empty location. If we reach an empty location that means the element is not their in the table because if the element had been their in the table it would have been inserted at one of the locations that I have checked.  

Let us see. Suppose I am searching for 31 so we go to 31 mod 13 which is 5. I come to the 5th location in which 31 is not there, so I go to the next location and search the element till I find it. I found the element in the 10th location. When I did not find it, I can not say that the element is not their in the table. It could be their, infact it is their.

Suppose I am searching for 33 mod 13 which is 7, I would start searching it from the 7th location. Till 11th location the element is not present and the 12th location is empty. This means 33 could not be their at all in this table. Because if 33 had been there in the table, then by this time it would have been definitely inserted in to the table till the 12th position because this is an empty location.   












(Refer Slide Time: 36:32)

 

That is an unsuccessful search, in an unsuccessful search the search terminates when you reach an empty location but a successful search will terminate when it finds the element.  How do you delete? The following slide is my picture which is from the previous slide and I want to delete 32.  

(Refer Slide Time: 38:15)

 

First I have to search for 32, 32 mod 26 is 6. I come to the 6th location, it is not there. Then I go to the next location, also it is not there. Then I find the element 32 in the 8th location. Suppose I removed it by setting this location to null. I removed 32 from that location. 
Is this a good idea? No. Why this is not a good idea? Suppose now you search for 31. The 31 mod 13 is 5, so we come to the 5th location. But we did not find it there. Then we go to the next location for that element and we did not find it and at last we reached the empty location. Hence we will say that 31 is not their but still 31 is their. Why is a problem coming in? 

Because when 31 was inserted that was the full location. That is why 31 was inserted later, but if you delete the element in the 8th location then you have a problem. Some how we have to do something different because we cannot just set this location to null or we cannot mark this location empty also. Look up will declare that 31 is not present, which is wrong. How do we delete? Instead of setting this 8th location to null we will place a tombstone, actually an x. 

(Refer Slide Time: 39:49)

 

Tombstone is just a marker so you could set up a bit at that location which specifies that this location was occupied by some one. It is not always the case that this will be an empty location, at some point this was occupied by some one. How it will help us? When we are doing a look up and we encounter a tombstone, we do not declare that the search is ended and the element is not present but we continue. As before if I was searching for 31, 31 mod 13 is 5 so I would come to location 5 and go to the next location and at the 8th location I would see an x and not null which is a tombstone. 

So I continue till I find either a null location or 31. I found 31 and declare 31 is their. When a look up encounters a tombstone it ignores and continues. When an insert encounters a tombstone what does it do? It will put the element at that position. We have to reclaim this space. What happens if there are too many tombstones? You do not have elements in the table, those are actually empty locations but in your search you still have to go beyond them. The performance of your search degrades.  
If you have a lot of tombstones you should just rehash. Just remove all the elements and put them back again. The same kind of a technique you have to do when you grow the table. Now you are not growing the table, you have too many markers in the table so just do a rehash and that will create empty slots without the tombstones and your performance will increase again. 

I will come to the other open addressing techniques. We looked at linear probing, we compute the hash function we look at that location and next location and so on. In double hashing we have 2 hash functions h1 and h2. 

(Refer Slide Time: 42:31)

 

The value of h1 gives me the first position were I am going to look for the key k. Then h2 (k) will tell me the offset from the first position were I am going to look again for the key k.  

Let us look at the piece of code given in the above slide. Probe is set to h1 (k), so that is the first position I look at and offset is set to h2 (k). First I will look at the locations specified by probe and the table, if it is occupied then the next location I will look at is probe+ offset. Probe is set to probe + offset which means this is a next location I look at. If this is also occupied then the next location I will look is probe + offset + offset which mean offset is determining key with how much distance I am going to advance. 

Every time I do not see the element that I am searching for. For linear probing your offset is always 1. You were always going to the next location so that corresponds to an offset of 1. Instead of going to next location I jumped one location ahead that is I jumped 2 locations, then offset would have been 2 and so on. Offset in this case which is in the orange color in the slide below is determined by the hash function h2 (k). This offset could be different for different keys. 

(Refer Slide Time: 44:20)

 

We will look at an example of how double hashing works. If m is the prime then this technique will ensure that we look at all the locations of the table. In linear probing because the offset was one we would look at all the locations in the table. If there was an empty location you would always be able to insert the element. 

We would not like the following to happen. There are empty locations in the table but you start from a certain location, since the offset is 3 you go 3 units ahead and you keep finding everything is full and then you come back to the starting location. Because you will not be able to insert the element at all. May be all of these elements that you looked at were full but the other locations in the table where empty. 


















(Refer Slide Time: 44:41)

 

Some how you do not cycle back. When you will cycle back? When your offset divides the size of the table. If the size of your table was a prime number then your offset would never divide it and this kind of a thing would never happen. In fact you would look at all the elements of the table. This is the small fact you can go back and prove that if m is prime then I have given you the rough arguments for this case, but you can also prove it more formally. 

This has some of the same advantages and disadvantages as linear probing. One of it is it distributes keys more uniformly because you do not form clusters any more. These clusters were getting formed because you were just going one step at a time. If for some key you are going 7 steps ahead and for some other key you are going 13 steps ahead and for some other key you are going 2 steps ahead, then these clusters are not getting formed any more. That makes the performance better.

We will look at an example. I have 2 hash functions h1 and h2. The h1 is the same as before, k mod 13. The element is also as same as before, we have a table of size 13. The h2 (k) is my 2nd hash function and is 8- (k mod 8). It will always be a number between 1 and 8. It cannot be zero, because k mod 8 lies between 0 and 7, so it is between 1 and 8. 










(Refer Slide Time: 46:41)

 

The zero does not make any sense, if it is zero then we are in trouble. If h2 (k) is zero for some k then that means you are continuously looking at the same place and if that place were occupied then you cannot insert the element at all. Let us insert the first element 18, 18 mod 13 is 5 so it will go to location 5. The 41 mod 13 is 2 so it goes to location 2. The 22 mod 13 is 9 so it goes to location 9. The 44 mod 13 is 5 so it tries to go to location 5 but the location 5 is already occupied. We have to compute h2 (44).

What is h2 (44)? 8 ? (44 mod 8), 44 mod 8 is 4. So 8-4 is 4, I have to go 4 steps ahead. I will go to location 9 but that is also occupied, so I will go to location 0. That is empty so 44 will go to location 0. The 59 mod 13 is 7 so 59 will go to location 7. The 32 mod 13 is 6 so 32 will go to location 6. The 31 mod 13 is 5 so we go to location 5 but that is occupied. I compute h2 (31), 31 mod 8 is 7 and 8-7 is 1. So 31 will check for the location 6 but 6 is also occupied. We have to go to 7, it is also occupied so go to 8 and this is not occupied, thus 31 go to location 8.














(Refer Slide Time: 47:39)

 

The 73 mod 13 is 8, so it will try to go to 8 that is occupied. We compute h2 (73), 73 mod 8 is 1, h2 (73) is 7 so we will go to 8+7 =15. The 15 is 2 mod 13, we go to location 2 that is occupied so 2+7 is 9 where that is also occupied. The 9+7 is 16, 16 mod 13 is 3 so it goes to this location which is unoccupied. This is how the elements would be distributed in the table.  
 
We will do some analysis of double hashing. Recall that I am going to assume that the load factor is less than one. What is the load factor? The number of elements divided by the size of the hash table  that is less than one. I need it to be less than one otherwise more than 1 does not make any sense. We are talking of a scheme where all the elements have to sit inside the hash table. We are also going to assume, this is similar to the assumptions that we made in the last class that every time I probe, I actually look at a random element in the hash table which is uniformly random.

The first time I probe I will take a random location in the hash table and try to put the element their. If it is occupied then once again I will pick a random location in the hash table and try to put it their. If that is also occupied once again I pick a random location in the hash table and try to put the element their.  

Let us see how this performs, because we will only be able to analyze such a scheme. Because the other schemes are too dependent upon the hash function that we are using and we might not be able to analyze them. If   is the load factor then that means 1-  fraction of the table is empty. If  is half that means the number of elements divided by the size of the table is half. Which means only half the table is occupied and half the table is empty, 1- fraction of the table is empty. 

(Refer Slide Time: 50:27)

 

Suppose my search was an unsuccessful search.What does an unsuccessful search mean? That means the element is not in the table. When does an unsuccessful search stop? When I get an empty location. How many probes will be required before I get to an empty location?

The 1-  fraction of the table is empty let say  of table is empty and 90% of the table is full. That is 10% is empty. The expected number of probes required before I hit  fraction of the table which is empty would be roughly 10. Because the first time with  probability, I will get to an occupied location and so on. So roughly after 10 trails I will hit an empty location because only  of the table is empty.  













 (Refer Slide Time: 51:39)

 

If 1-   fraction of table is empty then roughly in an excepted sense  probes are required before I hit an empty location and declare it to be an unsuccessful search. This is the excepted numbers of probes required for an unsuccessful search. 
 
Let us look at a successful search. I am going to talk about the average number of probes required for a successful search, not for one particular search but if I were to look at all the successful searches.  

What are successful searches? Successful search are searches corresponding to the elements in the table. I have some number of elements in the table, let us say I search for the first element. Then how many probes are required? Suppose I search for the second element. How many probes are required and so on. Then I will take their average. 

Let us try and compute this quantity. If you recall from the last class the average number of probes required for a successful search is the average number of probes required to insert those elements. Because when we are inserting those elements we are essentially doing the same thing. It is the same as the average number of probes required to insert all these elements and this is the quantity I am going to compute. 









(Refer Slide Time: 53:11)

 

What is the average number of probes required to insert all the elements that I have in the table? When I am inserting an element I need to find an empty location again. Suppose I begin with an empty table and I am looking at the number of probes required to insert the first  elements. Size of the table is m, let us assume m is 100. I am talking of inserting the first 50 elements. Suppose I have already inserted 48, 49 elements and when I am trying to insert  element.  

What is the excepted number of probes that are required? The half of the table is empty, when I try once I may hit a full location. May be when I try again, in expectation I just need 2 probes to be able to insert this   element. For the other first 49 elements I might on an average even required less, but all I can say for sure that the average number of probes required for inserting these elements is  2. 














(Refer Slide Time: 53:51)

 

How many elements am I inserting? The   elements, on an average the total number of probes required is   m for these  elements. When I show you the rest, you will understand why I am doing this way. Suppose I have already inserted  elements in to my table and I am trying to insert the next  elements in to my table. When I am trying to insert the next  elements, just assume that I have already inserted   -1 and I am trying to insert this last element.  

How much of the table is already full when I try to insert this last element? The of table is already full. Only a  of the table is empty. So on an average I am going to require about 4 probes before I get to one of the empty location. I am searching for an empty location to put this element in. I need roughly 4 probes, infact I am just praising this as an upper bound and I need at most 4 probes to insert all of these  elements. The total number of probes required to insert these  elements is  times 4 which is no more than m. 



(Refer Slide Time: 53:14)

 

Similarly for these next  elements, when I am trying to insert the last of these  elements only   of the table is empty. On an average I require about 8 probes before I can get to one of those empty locations.  For these  elements or for any one of them I would not have required more than 8 probes. I would have required between 4 and 8 probes for these  elements.  

Because when I was inserting the first of these  elements only 3 quarters of the table was full. One quarter of it was empty, but I am just upper bounding it. I am just saying that no more than 8.  













(Refer Slide Time: 57:19)

 

What is the total number of probes required?  For  recall from previous slide I said m, for   this also I said m. What is the total required for these   elements? It is m times i (m x i). How many locations are empty in the table? What is the total number of elements in the table now? 

After I inserted   elements what fraction of the table was empty? It is half. After I inserted  +  how much of the table was empty? It is , so it is really this last number. After I inserted  how much was empty? It is . So after I inserted all of this that is  how much is empty?

It is which is  fraction that was empty. After I have inserted all of these fractions I have only  fraction of the table empty and the total number of probes required to insert these elements is m times I. We have a load factor of , we already inserted enough elements so that the load factor is . When the load factor is , 1- fraction of the table is empty. If I have 1-  fraction of table empty, then how many probes are required? If I have  fraction of the table empty then I require m x i probe. What is i? 
The i is basically minus log of this ( ) quantity. If I need to have 1-    fraction empty, so I just need ? m log (1-  ). These are the numbers of probes required.

(Refer Slide Time: 58:17)

 

If I have  fraction empty,  is the number smaller than one. So to get to this point I require m x i probes. So to get to a point where 1- fraction was empty, I need ? m log (1-  ) this many probes. The above what we saw was the total number of probes required and the average was just divided by n that is - .


We will be able to capture it to a table. For an unsuccessful and successful probes, when we had chaining it was 1+ . For probing, for an unsuccessful search it was ( ) and for a successful search what I just showed you is ( ). 












(Refer Slide Time: 01:00:14)

 

The last slide which shows how this performances of  changes.  

(Refer Slide Time: 01:00:38)

 
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 6
     Trees

Last class we discussed about hashing. We saw few collision resolution techniques, chaining, double hashing linear programming and you also did a little bit of analysis of these collision resolution techniques. Today we are going to talk about trees. We are also going to look at binary trees and some data structures for trees. 

(Refer Slide Time: 1:31)

 

What is a tree? Many of you might have come across a tree before, except this tree is going to be different from one that you have seen before. The root will be at the top. In most of the trees around, you do not see the root.













(Refer Slide Time: 1:37)

 

The root is going to be at the top of the tree. In the tree given in the slide above A is the root. There is a notion of parent and children, the node B is the parent of node D and E. By the same argument A is the parent of B and C, C is parent of F, G and H. A is a parent of B which in turn parent of D and E, so A is ancestor of D and E. A is also an ancestor of F, G and H. A is also an ancestor of I. A is a grandparent, sometimes we use the term grandparent. A is a grandparent of D, E, F, G and H. 

Hope you understand the difference between ancestor and grandparent. D and E are descendents of A, in fact B, C, D, E, F, G, H, I are all descendents of A. C and B are siblings because they have the same parent. B is a sibling of C and C is a sibling of B. G and E are not siblings but F, G and H are siblings. D and E are the children of node B. A is a parent of B, B is a parent of D and E, D and E are children of B, B and C are children of A and all of these are descendents of A. 

I have 3three ancestors H, C and A. H is the parent, C is the grandparent and A is the great grand parent but we do not use that term we just call it as an ancestor. The terms we defined till now were more in the nature of a family tree and then we will come to real trees. D, E, F, G, I are called the leaves of the tree. A is the root, if you just turn it upside down then the extremities should be the leaves.
 
What is the leaf? The generic term for A, B?I are also called nodes of a tree. A leaf is a node which has no children. If a node has no children then it is a leaf. H is not a leaf since H has a child but I, F, G, E and D are nodes which do not have any children and so they are called the leaves. 




(Refer Slide Time: 5:08)

 

A, B, C and H are called internal nodes, a node which is not a leaf is called an internal node. We associate a notion of level with each node, the root is at level 0. The children of the root are at level 1. The children of those nodes which are at level 1 are at level 2. D, E, F, G and H all are at level 2. It is not that H is at level 2, all of these nodes are at level 2. I is at level 3. 
 
Sometimes we also use the term depth, in which depth and level are the same thing. The level 0, level 1, level 2 are also called as depth 0, depth 1 and depth 2. The height of the tree is the maximum level of any node in the tree. What is the maximum level of any node in the tree? The height of this tree is 3.  
 
The degree of the node is the number of children it has. B has a degree 2, C has a degree 3 and H has a degree 1. The leaves have degree 0 because they do not have any children. Basic terminologies are quite intuitive. What are trees used for?

They can represent the hierarchy in an organization. For instance there is a company let us call electronics R Us which has some divisions. RND is 1 division, purchasing is another and manufacturing is  division.  Domestic and international are the sub-division for the sales. You could represent the organizational structure through a tree. You could also use a tree to represent the table of contents in a book. 

Let us say a book called student guide which has chapters on overview, grading, environment, programming and support code. The chapter grading has some sections called exams, homework and programs. They could also have some sub-sections and that would build up the tree. 



(Refer Slide Time: 6:56)

 

Your file system in which if you use the Unix environment or the Windows environment is also organized as a tree. The one in the level 0 is the root directory and in the 1stlevel are those 2 sub-directories. Then within the sub-directory I have some other sub-directories and within that I have homework, assignment and so on. Your file system is also organized like a tree. 

(Refer Slide Time: 7:57)

 

Today in our class we are going to see about definitions and then we start using those definitions in our later classes. 

(Refer Slide Time: 8:33)

 

An ordered tree is one in which the children of the each node are ordered. That means there is a notion in which we would like to put the left child in the 1stlevel to the right side. Suppose if you want to draw a family tree, you may want to draw the eldest child to the left and the younger child as you move from left to right. There is a notion of order there and some time you want to reflect that order in your tree. But there would be no notion of order in the following example.
 
The node which is in the level 0 is a directory and in the 1stlevel there are two sub-directories. Whether I place the left node to the right or the right node to the left, it does not really make any difference as far as the picture is concerned. Also it does not convey any additional information but sometimes you might have the notion of order between the children. Such a tree is called an ordered tree. 
















(Refer Slide Time: 9:37)

 

A binary tree is an ordered tree in which there is a notion of left child and a right child. Actually it is an ordered tree in which every node has at most 2 children. The diagram given in the slide below is an example of a binary tree. The root node has 2 children and the child node on the left has only 1 child and the following child on the right has only 1 child. The node which does not have child node are said to be leaves. We have 5 leaves which have no children. These nodes in the 1stlevel are ordered and there is a notion of left and right nodes. If I were to change the tree that is if I were to draw the left nodes on the right and right nodes on the left then I get a different binary tree. That would still be a binary tree but it would be different from this binary tree.

(Refer Slide Time: 11.06)

 
All of this is dependent upon the application you have. This is just a way of representing information. Sometimes the order has meaning to it, sometime it has no meaning to it. When it has some meaning to it then you would rather use an ordered binary tree and when you change the order, then you are representing something different. We will see more example of this and things would become clear.  

I can also define a binary tree in a recursive form as follows. A binary tree is just a single node or a leaf or it is an internal node which is the root to which I have attached 2 binary trees. In the following slide the nodes which are marked on the left side are called left subtree and the nodes marked on the right are called right subtree. I can construct any binary tree in this manner. 

I take a node and attach a left subtree and a right subtree. I get a left subtree and right subtree through recursive in which it is obtained by taking a node and attaching it to the left and right subtrees.

(Refer Slide Time: 12:12)

 

I have said and/or which means this left subtree might be null that is I might not attached anything or I might not attached anything to the right or I might have attached both the subtrees. Remember we have introduced other terms, left subtree and right subtree. The node to the left side of the root node is called the left subtree and the node on the right side is called right subtree.  What is the left subtree of the node which is in the 1stlevel? In the 2ndlevel, the node at the extreme left is the left subtree of the node. 






(Refer Slide Time: 12:49)

 

One example of a binary tree is the arithmetic expressions. I have an arithmetic expression which looks like the one given in the slide below.

(Refer Slide Time: 13:27)

 

I can represent this as a binary tree. Let us look at a parenthezisation of this expression. Suppose I have parenthesized in the manner like, which is given in the last line of the slide.  We have (4+6), the numbers here will be the leaves of my binary tree and the internal node would correspond to the operations. In fact this is also one way to evaluate this expression. You would take 4+6 and you would sum that. You would draw a tree which has 1 internal node and its two children are 4 and 6. The internal node would have plus operator in it. Whatever is the resulting value we are adding that to 1. I draw a tree whose root is a plus operator and one child is 1 and the other child is the subtree that is obtained from this operation and I could build this tree. This is just another way of representing arithmetic expression.  

Decision tree is another example of binary tree. The example given in the slide below is taken from the book. Star bucks, Caf? paragon and most of it would not make much sense, may be we would not come across them. What is the decision tree?

Each node in the decision tree corresponds to some decision that you want to make. You come to root node and ask whether you want a fast meal. The answer is yes then you come to the left node and whether you want coffee or not. The answer is yes then you go to star bucks. If the answer is no you may go to some other place and so on. Thus decision trees are another example of binary trees. Why because typically it is yes and no. You would follow the decision tree to get into a particular node. 

(Refer Slide Time: 14:51)

 

This was just more of terminology and examples. Let us see more concrete stuff. Let us define a complete binary tree. We are still at binary trees, as you can see every node in this tree has less than or equal to 2 children or at most 2 children. But I will call such a tree as a complete binary tree. We call a tree as a complete binary tree if at the  level there are  nodes. In some sense it is full and when every node has 2 children it does not give you a complete binary tree.
 





(Refer Slide Time: 16:06)

 

I will show you why it cannot be a complete binary tree. Let us look at the slide below and check whether every internal node have 2 children in this tree. Every node has a 2 children then that tree should also have leaves. It cannot be the case in which every node has 2 children, in some case there are no children. Just with the requirement that every node has 2 children, every node other than the leaf that means every internal node has 2 children does not implies it as a complete binary tree. This is a counter example to that in which every internal node has 2 children. This is not a complete binary tree. 

(Refer Slide Time: 17:00)

 

The following is an example of a complete binary tree. We want to say that at level i there are  node. The root node is at level 0 that is 1 node, at level 1 there are 2 nodes, at level 2 there are 4 and at level 3 there are 8. 

If h is the height of the tree, in the following example what is the height of the tree? We call height as the maximum level number so we should not count this as 4. Thus the height of the tree is 3. If h is the height of the tree that means all the leaves are at level h then by the definition of the binary tree we have said that the level i has  nodes that means there are  leaves. The number of leaves in a complete binary tree of height h is just . 
 
What is the number of internal nodes?  At level 0 we have 1 node, at level 1 we have 2 nodes and so on. Thus the sum is given as 1+2+ , because at level h all the nodes are leaf nodes. Thus the sum is , this is the number of internal nodes and the number of leaves is . The number of internal nodes is the number of leaves-1. This is for a complete binary tree. 

(Refer Slide Time: 17:34)

 

What is the total number of nodes in this tree? It is  which is the number of leaves +   which is the number of internal nodes, hence it becomes . Let us call this number as n. If I have a complete binary tree of n nodes, what is the height of this tree? 
Let us go one step at a time.  What is the number of leaves in this tree? If the number of nodes is n and the number of leaves was  which equals , just from this ( =n) expression. The number of leaves in a complete binary tree on n nodes is . If I have a complete binary tree on n nodes, half of the nodes are leaves and the remaining half are of internal nodes. Similarly I can say that if I have a tree on n nodes, then the height of the tree is  (no of leaves). I can evaluate h from ( =n), h will be log ( ) and so it is the log (no of leaves). Else we can go directly from , where the number of leaves is  and so h is log (no of leaves).  

(Refer Slide Time: 20:37)

 

You are just doing some simple counting here. If I give you a complete binary tree of height h then you should be able to say about the number of leaves and the number of internal nodes it has. When I give you a complete binary tree on n nodes, you should be able to say the height and so on. If you have a tree on n nodes then the height of the tree is log ( ). 

The other thing your have to keep in mind is that in such a tree the number of leaves is very large. It is roughly half the total number of nodes. It is very leafy kind of a tree. So far we have seen a complete binary tree but a binary tree is any tree in which every node has atmost 2 children. To get any binary tree, you can start with a suitably large complete binary tree and just cut it off. 








For instance if I were to cut off some pieces then I would get a binary tree as shown in the slide below. I can always do it, no matter about the tree I need. Take the binary tree on the right side as height 3 then I would start with the complete binary tree of height 3 which is on the left. Just cut off some pieces on the left side of the tree to get the tree which is on the right side. The picture given in the slide below is the proof.  

(Refer Slide Time: 22:07)

 

Let us use this fact that you can obtain any binary tree by just pruning of a complete binary tree. Take a complete binary tree, cut off some branches then you will get a binary tree. If I have a binary tree of height h then in a complete binary tree at level i there were atmost  nodes. In a binary tree at level i there will be atmost  nodes, there cannot be more than  nodes because the binary tree is obtained from a complete binary tree by pruning. 
















(Refer Slide Time: 23:14)

 

This is an important fact, atmost  nodes at level i implies that the total number of nodes in your binary tree of height h is atmost 1+2+ nodes. The last level is h, at level 0 there will be 1 node, at level 1 there is atmost 2 nodes, at level 2 there are atmost 4 nodes and so on. This is the maximum number of nodes that binary tree can have. 

(Refer Slide Time: 24:23)

 

Let us rewrite this. Suppose I told you that a tree has n nodes. Then n is less than or equal to this ( ) quantity, n <=   which means that the height of the tree is just rearranged and it is h >= . If I give you a binary tree with n nodes in it, its height is atleast  and there is a particular binary tree which achieves this equality and that is a complete binary tree. 

Think of a complete binary tree as a tree which acquires the smallest height. If I create a binary tree with the certain number of nodes, the one which has the shortest height will be a complete binary tree. Because there we are packing all the nodes as close to the root as possible by filling up all the levels to the maximum. That is the minimum height of the binary tree. I give you a binary tree on n nodes, its minimum possible height is .  

What is the maximum height that a binary tree on n nodes can have? A binary tree on n nodes has height atmost n-1. This is obtained when every node has exactly 1 child and the picture is given in the slide below. This would be a zigzag in any manner and the height is 8 since there are 9 nodes in it. 

(Refer Slide Time: 25:35)

 

In a binary tree on n nodes the minimum height is log (n) that is log ( ), but we say it as log (n) and the maximum height is n-1. That is the mistake many people make. They always assume that binary tree means height is log (n). But it is not the case, it could be anywhere between log n and n. How many leaves does the binary tree have?  



(Refer Slide Time: 26:44)

 

What is the minimum and the maximum number of leaves it can have? Let us figure it out. We will prove that the number of leaves in a tree is <=+ no of internal nodes. This is the useful inequality, in any binary tree the number of leaves is <+ the number of internal nodes or atmost the number of leaves in a tree can be 1 more than the number of internal nodes. How will you prove this? We will prove it by induction on the number of internal nodes. 

(Refer Slide Time: 27:19)

 

In a base case consider a tree with 1 node. If a tree has only 1 node how many internal nodes does it have?
It is 0, because that 1 node does not have any child so that is the leaf. Base case is when the number of internal nodes is 0, in which case the right hand side is 1 that is the number of leaves is 1 so the inequality is satisfied. 

We will assume that the statement is true for all trees with less than or equal to k-1 internal nodes.  This should be read as, the statement is true for trees with atmost k-1 internal nodes not just k-1 but anything even for less this statement is true. 
 
We will prove it for a tree with k internal nodes. Suppose I have a tree with k internal nodes, let us say on the left subtree I have  internal node. Then how many internal nodes do I have on the right subtree? It is exactly k- -1 and not atmost because all the internal nodes are either in the left subtree or in the right subtree or it is the root node. The minus one is because of the root node. This is the number of internal nodes in the right subtree. 
 
Let us apply the induction hypothesis.  is less than or equal to k-1 and the quantity (k- -1) is also less than or equal to k-1. We can use the induction hypothesis. In the left subtree which has   internal nodes, the number of leaves is less than or equal to  +1. In the right subtree the number of leaves is less than or equal to k- -1+1 which is k-  . The total number of leaves is just the sum of these two (( +1) + (k- )), all the leaves are either in the left subtree or in the right subtree. The total number of leaves is just the sum which is k+1 that is we wanted to prove. 
 
Since we started a tree with a k internal node, you have to show that the number of leaves is less than 1+k. This is a simple proof which shows the number of leaves is atmost 1+ the number of internal nodes. 

(Refer Slide Time: 27:44)

 
There was a tree in which we saw the number of leaves is equal to the number of internal nodes +1. It was in a complete binary tree.  What was the number of leaves in a complete binary tree? The number of leaves was , if h was the height of the tree and the number of internal nodes was 1+2+ nodes. There was exactly a difference of 1 between the number of leaves and the number of internal nodes. The complete binary tree once again achieves the equality. For any other tree the number of leaves will only be less than or equal to this sum. How small it can be? 

(Refer Slide Time: 30:53)

 

Let us look at that. For a binary tree on n nodes, the number of leaves + the number of internal nodes is n. Because every node is either a leaf or an internal node. Also we just saw that the number of leaves is less than or equal to the number of internal nodes +1. I will just rearrange, this implies that the number of leaves is . I have just rearranged, as the number of internal nodes is greater than or equal to the number of leaves -1. I replace that and get the number of leaves as   for any binary tree.
 
The another thing to keep in mind is for any binary tree the number of leaves will never be more than half the number of nodes in the tree. Again this equality ( ) was achieved for our complete binary tree, which is the most leafy tree. All others trees are dry and the minimum number of leaves that tree might have is just 1. The example for that is the same example that I have showed you before. The tree on 9 nodes has only 1leaf in it. 



Let us look at an abstract data type for trees. You would have the generic methods which you seen for all the abstract data types till now. The following are the generic container methods, size () which tell us about how many nodes are there in the tree, isEmpty () tells whether the tree is empty or not and the method elements () which list out all the elements of the tree. 

(Refer Slide Time: 32:22)

 

You can have position based container methods, it is as the kind we saw for the list or sequence data types. The swapElements (p, q) in which I have specified 2 positions p and q. Think of the positions as references in to the tree except that using the position data type I am not able to access anything else but the elements sitting at that position. 
 
The method positions () will specify all the positions in the tree. It will give you all the positions in the tree as a sequence. The positions method has no parameters, when you invoke it on a certain tree it will just give you a sequence of all the positions in the tree, references to all the nodes in the tree. Once you access a particular position then using the element method on that positions you can access the element in that node.

The swapElements (p, q) given 2 positions p and q, you are swapping the elements at these 2 positions. replaceElement (p, e) which means that given a position p you are replacing the element at that position with e. In query methods given a particular position isRoot (P) is this the root of the tree. Given a particular position isInternal (p) is this is an internal node, given a particular position isExternal (p) is this external or leaf. Sometimes we use external or sometimes we use leaf, does this position correspond to leaf. 

In accessor method when I call root it will return a position of the root, an object of type position. isRoot (p) is determined as given a position is it a root and root () returns the position of the root. Hope you understand the difference between the both. The position of the root means it is not a reference to that particular node but it is a reference of type position so that you cannot access anything except the element. This was the same as the type casting which we did earlier. The method parent (p), given a particular position returns the parent node. The children (p), given a particular position returns the children of this node. If it were children, there could be of many children for a certain node. 

How it will return the various children? It will return as a sequence, it will return a sequence of object type sequence which will contain the position of all the children. Position has an element method which will let you to access the data. The update methods are typically application specific and this would be the generic method for a tree.  

Binary tree should really be treated as a sub-class, as a derived class from a tree. All we need to do is to continue to have the same method as we described for the tree but we will have some additional methods. There would be a notion of a left child given a position give me the left child, give me the right child or give me a sibling. 

(Refer Slide Time: 36:37)

 

We will come to the update methods when we see the example of it. What is the node structure in a binary tree?  What are the kinds of data that you would be keeping in an object corresponding to a node of the binary tree? 









(Refer Slide Time: 37:14)

 

You would have the data, you would have a reference to the left child and a reference to the right child and you would have reference to the parent typically. You would also have a reference to key or data associated with this node, any element that is sitting in this node you would have reference to it and these were all sitting together. The reference to the node which is at the center will not be stored in the node and that does not make any sense. 

For instance if I access to this node, suppose this was the root node and I use the root method to get a position to this node then using that position I can now access the left child by invoking the left child method and in this manner I can get the position of any node. Once I have the position of a node I can then invoke element method and any method to get the data associated with that. A node in this case would definitely implement the position interface. In the slide given below, this is what the binary tree would look like if you look at the links and so on. 














(Refer Slide Time: 39:12)

 

The parent link would be null for a root node because it has no parent. Then it would have left child in which the left child would be referring to the node on the left and the right child would be referring to the node which is on the right and so on. In the above diagram the extreme right node does not have any right child, its right child member would be referring to null. That was for a binary tree. How do we take care of arbitrary trees?  Let us say unbounded trees. The root node has 3 children and its child which is on the left also has 3 children. 

(Refer Slide Time: 38:39)

 

Are we going to have 3 different data members to refer to 3 children? That is not clear about how to do it, because then if it has 4 children then how you would create space for another member. The way to do it is that you have a reference to 1of the child only and then all the children are in a linked list. Each child will have a reference to the parent, so all of these children would be pointing to the parent node. But the parent node would be pointing to only 1of them.  

Which would be the head of the linked list in the 2nd level? From the node which is at the 1st level, if I want to refer to the children, I can just come here essentially return all the elements of this linked list. How do I know that I have reached the last element of the linked list when the next is empty? 

The 1stfield of the node is empty because it does not have children. Every node still has only 3 data members, parent or 3 references 1 for the parent, 1 for left most child and 1 for the right sibling. The left node on the level 1 would refer to left most child and not to all the children because that we do not know how many are there. It will have 1 more to refer to the right sibling because for the left node at the level 2 should refer to the right sibling. 

You can do with only 3 references. The node in the level 2 has only 1 child and it is just pointing to that 1 child, there is no sense of left and right here. This is not a binary tree, left and right makes sense only in a binary tree. 

Actually I should not have written left child, I should have written 1st child. That is any 1 child then it just point to that 1 child and that let you to access its siblings through a linked list. From the 1st child you will go to the next child and to the next child and so on. You can step through all the various children throughout linked list. With that we will end our discussion on binary trees today. In the next class we are going to look at reversals of trees. 

Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 7
              Tree Walks / Traversals

In the last class we looked at definitions of trees, a binary tree and a complete binary tree and the height of these things. Today we are going to continue with our discussion on trees in particular we are going to talk about tree walk or tree traversals. 
 
A tree walk is a way of visiting the nodes of a tree in a specified order. There are 2 different tree walks that we will consider, one is called the preorder walk and the other is called postorder walk. In a preorder walk you first visit or process each node and then you go and process its children. I will show you an example to follow this and then you will be clear.  

(Refer Slide Time: 01:23)

 

In a post order you will first process all the children or visit all the children and after that only you would process the node. Let us see an example which will clarify this. I am looking at the examples of preorder tree walks. 

Suppose recall that we said, we can construct a tree out of a book or a paper. We can look at the organization of a book as a tree. Let us say a research paper. You have the paper and it has a certain sections, the first section is the title, second section is the abstract which discusses about what is their in the paper and then you have section 1, section 2, section 3 and then at the end of it you have references that is what are the books and the other papers that this particular publication has referenced.

(Refer Slide Time: 02:37)

 

The section 1 has 2 sub sections that is section 1.1 and 1.2. The section 2 has 3 sub sections, section 3 has 2 sub sections and so on. When you read the paper, this is the order you would go in. Suppose you are reading the paper end to end, first you will go to the title, then read the abstract then you will look at section 1 and then its sub section 1.1, 1.2 and so on. If you were to think of a book and how the table of contents of the book are listed. The way the tables of contents are listed is that first you have the chapter and then the sections within the chapter are listed. Then the next chapter and the sub sections within that chapter and so on.  

(Refer Slide Time: 04:04)

 
If I were to look at the above slide as the tree and these as the nodes of a tree. The first node that we are referring or accessing is the node 1. Then we go to the node 2 then 3, 4, 5, 6 and after we are done with 6, in a table of contents you will have 7, 8,9,10,11,12,13 and then 14. This is also called the preorder traversal of a tree. 

The pseudo-code for preorder traversal would look something like the one which is given in the above slide. If I have to do a preorder traversal of a node v in a tree, so to begin with I would call preorder traversal at the root of the tree. Then I would say first visit the node, visit here is a generic term and we will use it very often. All it means is that I am doing some computations in that node. In this particular case if I were listing out the book as table of contents then visit would correspond to print the title or print the heading of that particular node. 

For instance each node corresponds to a section or some thing then this (?visit? node v) would correspond to print out the name of the section. Then once you done that then you go to each of the children nodes and repeat the process. Repeat this same process there on each of the child nodes. Because this tree could be arbitrarily d and the subsection 1.1 could be as 3 sub sections 1.1.1, 1.1.2 and so on. 

(Refer Slide Time: 04:52)

 

If you were to do a preorder traversal then you would come to 1, then you would come to 1.1 then you would go to 1.1.1 and then 1.1.2, 1.1.3 and only then you would go to 1.2. That is the need for this kind of a recursive traversal. What you doing here? You first visit the node then visit all its children. So these are the 6 children of that node. We are saying visit the first node. What does visit correspond to? It does not have any children so it just means visit the first node. The second one corresponds to visit that node. The third one corresponds to visit that node and then visit its children. That is what we are doing, visit its children and then the next child and so on. This is what would be called a preorder traversal. 
I gave you the example this is like reading a document from beginning to end. We could also have what is called a post order traversal. In a post order traversal recall I said that we are going to visit the node at the end. We will first visit its children and only then we would visit the node. 

(Refer Slide Time: 06:43)

 

Let us say I have a directory structure like this. This is my root directory courses. This is an example from the book. There are 2 courses here CS 016 and CS 252. And then in that there are 2 sub directories, there is a file called grades. Within this sub directory there are 3 files, within this sub directory there are 3 files and so on. This is the directory structure. 

Suppose I want to compute the total space occupied by this file system or this entire directory. What would I do? I would compute the total space occupied by the subdirectory which is on the left and I would compute the total space occupied by the subdirectory which is on the right and then I would add these two up at the node which is at the center to obtain the total spaces required. So in some sense I am actually visiting the node which is at the center or doing some computation on this node after having done the computation at the 2 children nodes.  

After having computed the total spaces required by the sub directory on the left and after having computed the total spaces required by the subdirectory on the right, only then I am doing the node at the center. When you are doing a post order traversal of a node v, for every child of the node first we are going to perform a post order on that. In this example post order corresponds to finding the total space occupied by that sub directory. So to compute the total space occupied by the directory which is at the center, we are first going to compute the total space occupied by the sub directory on the left then the total space occupied by the directory on the right and having computed that, you are then going to compute the total space required by the directory at the center.  
In some sense the order in which computation is done is reverse from the pervious example. In fact this is the order in which the disk usage command in Unix, if you ever have used this particular command in Unix. What it does is, if you type in this command in a directory it tells you what is the total space occupied by the various subdirectory their. The way you list it out is, if you were to type the disk usage command in this sub directory, it was first going to list out the total space occupied in this directory on the left then the total space occupied in this directory on the right and then eventually at the end it list out the total space required here. Because it would have computed it only after it had done the computation on the left and right. How does this do the computation? In a recursive manner that is to compute the total space required by this directory(Refer Slide Time: 09:41), first it is going to compute the total space required here then the total space required here and then add them up to get the total space required here. So that would be a post order traversal.  

(Refer Slide Time: 07:05)

 

Which child we would visit it first? We are looking at ordered trees. There is a notion of a first child, second child, the third child and fourth child. So the first child is visited first and next is the second and so on. If you have drawn the tree in such a manner such that the first child is at the leftmost then we would say that the leftmost child is visited first and after that the other one on the right and so on. It depends upon how you have drawn your trees. What I had just shown you was traversal in general trees. If it is a preorder, first visit the node then visit the children nodes. In postorder first visit the children node and then visit the nodes. 

Let us look at how this specializes to the case of binary tree. In a preorder traversal what are we saying? So v is a node, if v is null then there is nothing to be done. If v is not null then in a preorder traversal we are first saying visit v. The visit is some generic computation we do not know what it really is. It depends upon what your particular application. First say visit, then do a pre order traversal on the left child and then do a pre order traversal on the right child. Note that this is a recursive procedure. We are calling preorder within the procedure itself. How does this (preorder (v.leftchild ())) work? This will work by making the call to itself, when we are doing a preorder traversal on the left child and on the right child. And the difference between pre order and post order here is that visit now comes at the very end. First you do a postorder traversal on the left child then you do a postorder traversal on the right child and then eventually you do visit v.

(Refer Slide Time: 10:54)

 

Let us see if you understood this. Let us look at an example. This is my tree. It is a binary tree. I want you to tell me what is the pre order and post order traversal of this tree. What we are doing when you visit a node is that we are just printing out the contents of the node. So let us first look at pre order. What do you think would be the first thing that would get printed if we are doing a pre order traversal of a tree? The a is the root. So we come here and we print a. Then we have to go and do a pre order traversal of a left sub tree. When we do a pre order traversal of the left sub tree we are going to come to the root of the left sub tree and first visit the node, visit corresponds to printing the content. We will just print it out as b. Then we will go to the left sub tree which is c. We will come here, we will look at the root node, first visit the node. Visit the node here means printing the contents, we will print c. Then we try to go to its left sub tree but its left sub tree is null. There is nothing there. So then we go to its right sub tree which is also null nothing to be done. Now we are done with the preorder traversal of this c. 








(Refer Slide Time: 11:57)

 

Where do we go now? To the right sub tree because first we went to visit the node then we did a pre order traversal of the left sub tree. Now we have to do a pre order traversal of right sub tree which means first visit the node here which is d then we go left which is f. Then again we try to go to left which is null, then go right which is also null, nothing to be done. Then we go to g and now we are done with the pre order traversal of this sub tree. We are done with a pre order traversal of the left sub tree. We are done with a pre order traversal of the right sub tree which means we are done with the pre order traversal of the entire sub tree. Now we would go to the right sub tree, the right sub tree has only e in it. So we would just print e. This would be the pre order traversal of this tree.  

Let us do a post order traversal. Which do you think is the first node that would get printed? It is c. Why c the right answer? Let us see. We come here to do a post order traversal. This a will be printed at the very end after I have done the post order traversal on the left side and post order traversal on the right. So I will first try to do a post order traversal of the nodes on the left side. When I try to do a post order traversal to b, I come here, first I will do a post order traversal of c then I will do a post order traversal of d, and then print the node b. I have to come to do a post order traversal of this node c. For doing that I will first do a post order traversal of its left child which is null, nothing to be done. I do a post order traversal of its right child, null nothing to be done. So I am ready to print the content of this node c. The first thing that will get printed is c. I am done with the post order traversal on the left subtree and now I come and do the post order traversal of the right sub tree. 

To do a post order traversal of the right sub tree I once again come to the root which is d. I first do the post order traversal of this left sub tree f then post order traversal of this right sub tree g and then print this content d. So post order traversal of f is a single node so it will just be f. The post order traversal of this right subtree would be g and then I would print the content of this which means d. What we will print now? So we have done the post order traversal of f and we have done the post order traversal of c so we can now print this node b. So we will print b. We have done with the post order traversal of the nodes on the left side so we go to the right sub tree e. Do the post order traversal here which means just print e and then we have done with the post order traversal here (Refer Slide Time: 15:45), we have done with the post order traversal here so we can now print the root which is a. So this would be the post order traversal. Is this clear to everyone, how the procedure works. 

(Refer Slide Time: 15:59)

 

I am showing another example for evaluating arithmetic expression. This is an arithmetic expression, we want to evaluate this expression. So how does one evaluate the expressions? This is minus, so in essence we have to compute the value of this quantity. What is the value of this sub expression? This corresponds to a sub expression so you have to compute the value of this sub expression which is on the left (Refer Slide Time: 16:36). We have to compute what the value of this sub expression on the right in the above slide. Whatever values we get we have to then take their difference. That will be the value of the entire thing. So as you can see it is like a post order traversal. First we have to compute the value of this which is on the left then the value of this on the right and then take the difference which is the operator sitting in this node at the center. 

How do I compute the value on the left side of the tree? I have to compute the value of this left sub tree, I have to compute the value of this right sub tree and then do the division because that is the operator sitting here. We can right a procedure something like this. Suppose I say evaluate the expression corresponding to v which is a node. Let say v is a root node here and I say evaluate this. If v is a leaf then I just return the variable stored at v because that is the value. The leaf corresponds to numbers in this expression. Else if v is not a leaf then that means we are at some internal node. So to evaluate the expression corresponding to this node v, I have to first evaluate the left part. Let say evaluate (v.leftChild ()). The arrow after x in the slide should be in the other direction so that x gets the value of that. The y gets the value of the right child when I evaluate on the right child. And if o is the operator then I just compute x o y whatever that operator o is and return that value. That will be the value of expression corresponding to node v. 
 
This is pseudo code, I hope you understand what I am trying to say here. This is like a post order traversal with a small modification. We are not going to be addressing that problem (Refer Slide Time: 18:29). The problem of generating this tree, given an arithmetic expression you will have to incorporate the priority rules to generate such a tree. We will not be worried about that for now. We are just looking at traversals. Given such a tree how can you evaluate the expression corresponding to this tree?  

(Refer Slide Time: 18:53)

 

For a binary tree we have seen a preorder traversal and a post order traversal. There is a third kind of traversal which is called an in order traversal. So recall that in a pre order traversal we visited the node first, then we went to the left then we went to the right. In a post order traversal we first went to the left then to the right then we visited the node. So the third possibility is if we just visit the node, between the visits to the left and the right. There should be an and between the visit to the left and right sub tree. 

So pseudo code for in order traversal would be such thing like the following. If v is null then we just get out, else we first do an in order on the left child then we visit the node and then we do an in order traversal on the right child. These are the only 3 possibilities. These are 3 binary trees, so you first go left. Where do you visit the node? Either you visit it before you visit both the left and the right or you visit it after you visit both the left and the right or you visit it in between the visits to the left and the right. These are the 3 possibilities and these are the 3 traversals that are known.  

Let us just look at an example and see that we have understood inorder traversal. Which is the first node that will get printed? Is that a? First to do an inorder traversal we come to the node a. We first do an inorder traversal of this left sub tree then we come to the node a. Then we do an inorder traversal of the right sub tree e. So to do an inorder traversal of the left sub tree, we will come to b. First we will do an inorder traversal of the left then we will print the content then go right. 

So to do an inorder traversal of the node c which is a single node, so it corresponds to printing c out. That is the first thing we will print. Then we will be printing the content of this which will be b and then we will have to do an inorder traversal of this right sub tree. For a right sub tree inorder traversal, once again we will first print out f then d and then g. Now we are done with the entire inorder traversal of left sub tree, so we will print out a and eventually we will print out e. That would be the inorder traversal of this tree. 

(Refer Slide Time: 20:07)

 

There is an another way of traversing a tree. That is called an Euler tour. Suppose this is a tree corresponding to an arithmetic expression. The tour is basically the one drawn in blue here. We start from here, keep going down. When we hit a leaf, we go up to his parent then go to the right sub tree and so on (Refer Slide Time: 21:54). It is a generic traversal of a binary tree. And all the 3 traversals that we have seen that is pre, post and inorder traversal can be viewed at as a special case of this Eulers tour traversal. Each node is basically getting visited thrice. Why because once we are coming like this (Refer Slide Time: 22:25) then other time we are going like this, touching this node. The third time we are coming like this and touching this node. So 3 times that we touch any node except the leaf nodes where you can count this as only once or thrice or what ever you want. But every internal node will be touched thrice. I should actually qualify, every internal node of degree which has 2 children, if the node has only one child then you will touch it only twice.  



(Refer Slide Time: 21:31)

 

Suppose this is the tree corresponding to a certain arithmetic expression which is given in the slide below. I want to print this arithmetic expression out with parenthesis. I want to draw the parenthesis, I want to print it out in this manner which is given in the below slide. So I can do an Euler walk to print this thing. Suppose I am here in the middle, before I start on the left sub tree I will print a left bracket. When I finish with the right sub tree I will print the right bracket. This right subtree corresponds to taking this path up (Refer Slide Time: 23:54), going up like this and when I am coming like this and touching this node will just print out the content of the node. Did you understand what I am saying? So recall that every node was visited thrice. 


So once when I am visiting it from the left then essentially I am going to print a left bracket. When I am touching it from the right essentially I am going to print the right bracket and when I am touching it from below I am going to print the content of this node. If you do that then you will get exactly the one which is given in the bottom of the below slide. First I will touch the node at the center from the left so first I will print left bracket then I will touch this node on the left I will print another left bracket, I touch this node on the left I print another left bracket and I touch this node on the left so I print another left bracket. So I get 4 left brackets to begin with. Then I come to the node 3. For the leaf I will just print the content of the leaf and do nothing else so I just print 3. Then I am going to touch this node from below (Refer Slide Time: 24:50), I am just going to print a star or a multiplication. Then I come and touch it from the left so I print a left bracket then I come here which is 1 and so on. So you can think of this as essentially printing out this arithmetic expression as some kind of Euler walk on this tree. 




(Refer Slide Time: 23:12)

 

We can actually write a generic method for tree traversal and then specialize it for whatever particular application you have. Whether you want to do a preorder, postorder or inorder traversal or any such thing. So this is just a small example. So you want to traverse a node specified at this position p. If this is an external node then you will call this method which is called external. You have not done anything here, we have just specified certain methods. External is a method that you will invoke if the node that you are trying to traverse is an external node. An external node is the same as a leaf node. So if it is a leaf node then that is the method you invoke. When you touch the node from the left then you will invoke this method called left. Here you continue with the left child. 

When you touch the node from below, you will invoke this method then you continue with right child. Then when you touch the node from the right you will invoke this method. Now you can specify what these methods are. So by specifying these methods you can create the traversal of choice, you can specialize this binary tree traversal, this generic tree traversal. If you want to go into java details this is an abstract class which means that these methods in particular init result, isexternal of course is specified but init result, external, left, below, and right are left unspecified. In a class you leave certain methods unspecified then it becomes an abstract class because you cannot really create an object of that class anymore. But then at some point you can specify those methods. And in that manner create a sub class of this abstract class which specifies these methods. And in that manner specializes this generic tree traversal procedure. This is a generic tree traversal procedure. So if I were to specify left, below and right in a certain manner then I could get a class for printing out arithmetic expressions.  





(Refer Slide Time: 25:16)

 

You want to know what the left result was. When I come back from the left child, may be I compute a certain result. In that example of disk usage we wanted to compute what is the total space occupied by that directory. We computed the space required by the left child, the directory in the left child. Corresponding to the left child we computed the space required by the directory corresponding to right child. When we computed that, those could be stored in r.left result and r.right result and then we would compute their sum. That would be your final value, which would be the value we would return.  

(Refer Slide Time: 28:48)

 

Let see how to specialize this for our printing arithmetic expression example. So recall that if the node is a leaf node then all we said was that we are going to print the content of that node. That is what we are saying, just print out the element in that node. P is a position just print out the element in that node, what ever the element may be. When we touch a node from the left, we said just print out a left bracket. So that is what we are saying just print out the left. When we touch a node from below, we just said that whatever is the operator present their just print that out. That is exactly what we are doing. When we touch it from the right, we just print out the right bracket. 

The PrintExpression Traversal is the class which is extending BinaryTree Traversal. When I invoke the traversal method it will now print out the arithmetic expression with the tree corresponding to arithmetic expression in parenthesized form. So I could specialize the same class BinaryTree Traversal to use it to compute the total space occupied by certain directories structure, by specializing these methods in a slightly different manner.  

Let us continue with our discussion on pre and in order. Suppose I give you the preorder and inorder traversal of a binary tree. I have mentioned this in the following slide. Can you use this to figure out what the tree is? Yes or no. Suppose this was the preorder traversal and this was the inorder traversal of the binary tree. I have given you these 2. Can you use these, to print out to tell me what the tree is? Yes you can. Why we can do that? 

(Refer Slide Time: 32:33)

 

So given this preorder traversal, what can I say first? I have marked a as the root. What should I do? The b is left child. Is this true that b is the left child? No, not necessarily. The root might not have a left child at all. That can happen, so we cannot say anything. All we can say is, a is the root. Let us find a in the inorder traversal. The a is the root, I know that let me just put down the node for the root. And I know that I will search for a in the inorder traversal. Now what do I know? I know that e is to the right that is e is the right sub tree. I know that this (e) is the inorder traversal of the right sub tree and this blue colored is the inorder traversal of the left sub tree of a. The green is the inorder traversal of the right sub tree and blue is the inorder traversal of the left sub tree.  

I know that the left sub tree has 5 elements in it. This is the information I know. So in the preorder traversal the 5 elements following a, would correspond to the preorder traversal of the left sub tree. The one element following that would be the preorder traversal of right sub tree. So in essence what have I managed to do? I have managed to identify what the left and right sub tree are and I know their preorder and inorder traversals. I know the preorder traversal of the left sub tree and I know the inorder traversal of the left sub tree. So my problem, I can use recursion now. I know e is the right sub tree and now I can basically work on this problem, where I am given the preorder and in order traversal of a tree and I need to figure out what the tree is. And whatever is the tree is, I will come and plug it as the left sub tree of a. 

Let us continue with the example and let see what we will do now. So b is the root of this left sub tree. Whatever argument we used before. So b is the root of the left sub tree, we are going to see where b is in the inorder traversal. The b is here and b is the root. So this (c) would be the left sub tree of b now. And f, d, g is going to be the right sub tree of b. So on the left I have only one element that would be c here. So c would be the left sub tree and this (d, g, f) would be the preorder traversal of the right sub tree. I figured out the c, since on the left there is only one so I can put that c down. I do not know what the right subtree is. I just know preorder traversal and inorder traversal. I know that it has 3 nodes, the right sub tree has 3 nodes. So the problem recursively reduces to this problem of given the preorder and inorder traversal of this 3 node tree, I need to figure out what the trees is. 
 
So once again I know that the root is going to be d. I look for d in the inorder traversal, it is there. I know on the left I have f and on the right I have g. I know the root is d and I know that the left sub tree would have f and the right would have g. I get something like the one which is given in the below slide.














(Refer Slide Time: 30:36)

 

You can translate this in to a piece of code. It will require some thought because you have to write it in a recursive manner. This is your second assignment which I have put it upon the web today. This would be some input, you will take from the user, the pre order and the inorder traversal. And you have to compute not the tree but you have to compute the post order traversal which is simple if you have computed the tree. Because you can do a post order traversal of this tree now and give the result. It might be possible that given any arbitrary sequences, it is not necessary there is a binary tree corresponding to that. You will also have to flag out an error, if the sequences that given to you are such that they could not possibly be the pre and inorder traversal of any binary tree. This is pre- and in- order. 

Given post and in also you should be able to compute the tree. Suppose I give you the postorder, inorder. Can you use that to compute the tree?  Recall what did we do? We try to first figure out where the root was. In the preorder the very first element is the root, in a postorder the last element is the root. So first, once you know the root then you will search for the root in your inorder traversal and wherever you find that root, that neatly divides the thing into a left sub tree and right sub tree. Once you know the number of nodes in the left sub tree, you can figure out what the post order traversal of the left sub tree. If there were 5 nodes in the left sub tree then the first 5 nodes of your post order traversal would be the post order traversal of the left sub tree. So in this manner again you can recursively work. You can recursively figure out what the left and right sub trees are and then plug them up to the tree. Given a post and in order traversal also you can do. What is the third question? Given pre and post can you do it? No. 





(Refer Slide Time: 36:15)

 

Given the pre and post order traversal of a binary tree you cannot uniquely determine the tree. And the reason for that is there can be 2 trees which have the same pre and post order traversal. 
               Preorder:   a b c
               Postorder: c b a
Suppose I gave you the above preorder traversal and postorder traversal that is a 3 node tree. What do you think is the tree? The tree given in the below slide is the tree with preorder traversal a b c. The postorder traversal will be c b a. This tree also has the same pre and post order traversal. The a b c is the preorder traversal, the postorder traversal is c b a. Which of this tree is a right one? Both of them. There is no unique tree, we could also have c as the right child. So these are only 2 but they could be many trees that you could construct. 
















(Refer Slide Time: 37:44)

 

That is the problem. Given a pre and post order traversal you cannot uniquely determine the tree. Because there could be many different trees with the same pre and post order traversals. But here note that some nodes in the trees had only one child. Suppose I gave you this information that every internal node of the tree has 2 children, not complete but every internal node of the tree has 2 children. What is that called? The below given slide is an example of a tree in which every internal node has 2 children. This is not a full tree, this is just a tree with every internal node having 2 children. There is no name to it.  

(Refer Slide Time: 39:32)

 

Suppose I gave you this information. Did you say an Indian tree? If each internal node of the binary tree has exactly 2 children. Then actually you can use the pre order and post order to determine the tree uniquely again. Let see just as an exercise, why this is true? I gave you the pre and post order traversal. What is the first thing you can say? The a is the root. We can quickly draw the root. What can you now? The b has to be the left child.  

(Refer Slide Time: 39:51)

 

(Refer Slide Time: 40:04)

 

It has 2 children. So we know that there is something on the left or actually I have done the example on the right. What can I say is the right child of a? The a has right child because every node has 2 children. What is the right child of a? It has to be e. Since my example, I have worked it out with that. The e, I know is the right child from the picture given in the above slide. I see e in the above picture. And after e nothing is there. What that it say? That says e is a leaf because in a preorder traversal I would have first visited e and then gone to its children. But there is nothing following it which means e has no descendants. 
 
Basically it means that the left sub tree has b c d f g. So e is the right child and the left sub tree has b c d f g as the preorder traversal and c f g d b as its postorder traversal. The same thing, since I have managed to divide, I am going to continue. So I have b c d f g as the preorder traversal of the left subtree and c f g d b as the postorder traversal of the left subtree. Once again I know that the root is b, I have drawn the root. What do I know about the right child of b? I know the right child is d. I see where d is in the case of preorder traversal. So d is the right child, every thing that follows d has to be in the right sub tree. Everything that follows the d in the pre order traversal has to be in the right sub tree. Why because in the right sub tree when I did a pre order traversal I have first visited the d and only then visited the other elements. So d f g forms the right sub tree and c forms the left sub tree. The left sub tree has only one element, that is what I am going to do here in the case of postorder traversal. 
 
This one element c would be my left sub tree and this f g d would be my right sub tree. I have figured out the left sub tree which has only single element, I can just draw it. And now I know that right sub tree has nodes d f g where preorder traversal is d f g and postorder is f g d. I know root is d, actually I have already drawn that out. I know the right child is g. The g is here and there is nothing beyond that means g has no children further. I can draw that out and then I know what remains. 

The left sub tree has only one node, it is in the left. You can do the same thing, I am just showing it to you at high level. In fact this is the level I will be following for all the algorithms we do in this class. You will have to translate it in to code. What you have to do is recursion or whatever it is. The idea of the assignment would be that. Many of these algorithms you should learn or you should figure out how to best program efficiently. You can write down the code also for this given the pre order and post order traversal of a binary tree every internal node of which has exactly 2 children, you can even use that to figure out what the tree is. 
 
Why did we work with 2 traversals? Why can not we just take pre order? So given just a pre order traversal can I use that to figure out what the tree is? This has the pre order traversal. The a, b, c, d, f, g, e was the pre order traversal of this tree. This is one tree with this as a pre order traversal. Can you think of another tree which has this as the pre order traversal? I could just have all of the nodes in one line to say, a b c and d below it, f after it g after at it, e at the very end. 





(Refer Slide Time: 41:04)

 

I could get huge number of different combinations all of which would give rise to the same preorder traversal. So with just the knowledge of one traversal you cannot really do much. Similarly for post order similarly for in order, even if I were to give you the in order traversal of this you can construct many different trees with the same in order traversal. So just using one, you cannot really do anything but 2 suffices for most purposes. In the only case when 2 does not suffices when you are given the pre order and the post order traversal. If some internal nodes of your binary tree could have only one child. But if you were told that every internal node has 2 children then even that is sufficient.

Can you count the number of binary trees given only lets say a preorder traversal? What do you think? At least 2 to the power n-1. We said a, b, c, d, e and f is my pre order traversal of a binary tree. 

We said a, b, c, d, e and f. This is one and all you are saying is I can make each one of them either a left or a right child of its parent. So this could be one the other option could be at the side a, b, c, d, e, and f and so on. So just since each of the nodes can be either the left or right child of its parent. But there are many other possibilities. This is absolute minimum. You can have many other possibilities. There could be lots and lots of things possible. Of course it will be finite because there are only finitely many different trees with 6 nodes on them. It will be of some finite number, it will be just a function of n exactly. But I do not know how to compute the close form expression for such things. 






(Refer Slide Time: 49:38)

 

So with that we will stop today?s class. What we looked at was tree traversals. How to traverse? So 3 different ways of traversing trees, in order, pre order and post order traversal for binary trees and for general trees there is no notion of an in order traversal as you perhaps understand. Why there is no notion of an in order traversal for general trees? Because if a node has 3 children then when do you visit the node itself? After visiting the first child or after visiting the second child or when? But in a binary tree there is a notion of left and a right. First you visit the left then you visit the node, then you visit the right. So there is also a notion of an in order traversal. And we saw some applications of these. And how given inorder, preorder and postorder traversal, two of these traversals you can figure out what the tree was which gave rise to those traversals.  
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 8
      Ordered Dictionaries

In today?s class we will be talking about ordered dictionaries. We will also be looking at binary search tree which is perhaps one of the simplest ways of implementing an ordered dictionary.

(Refer Slide Time 01:16)

 

So what is an ordered dictionary? It is essentially that you have the dictionary functionality. Recall dictionary was that you have key element pairs and you use the to insert an element, to search for an element or to delete an element. Besides that, in an ordered dictionary, you have the notion of the element with the minimum key, the element with the maximum key and the notion of predecessor and a successor. So when I say minimum key what then it means that there is some kind of a total order on the keys. This is different from when we have talked about hashing and dictionary. If we recall we said that the only operation that will ever need to do on the keys is to compare two keys for equality. Given two keys, we have to decide whether they are the same or not. So here we are going to be excepting more out of our keys.

Here we are going to assume that there is some kind of a ordering relation on the keys so that given two keys, I can either decide whether they are equal and if they are not equal then I can say one is less than the other one is larger than the other. There is ordering relation on the keys.





(Refer Slide Time 01:28)

 

Conversation between a student and professor: what are the parameters? You?re saying to the function predecessor and successor. I have just shown ?S? as the ordered dictionary as one of the parameters it need not be. So in particular you could think of a predecessor as taking only one parameter which is a particular key. So predecessor of k would give me the key which precedes k in the total order in the dictionary. 

(Refer Slide Time 02:50)

 

There are certain keys in dictionary s. Give me the one which precedes the key k and which lies in this dictionary. Keys are the particular field or the part of data on which the things are ordered. There is a total ordered on the keys. Similarly the successor function. 



(Refer Slide Time 03:34)

 

So what is one way you can implement such an ordered dictionary? There are two trivial ways of doing it where in both cases, we say using a list kind of data structure. So an unordered list would just keep all the elements in here. So all I have shown here are the keys. Since it is unordered, inserting takes only a constant amount of time. Searching will take ordered ?n? time in the worst case because there is no order. I might have to go through the entire list before I found out where the element is and how much time will deletion take? It takes order ?n? because first we have to search for what we are trying to delete. 

(Refer Slide Time 03:45)

 



How much time would the successor take here? Order n. not order one because suppose I say what is the successor of 12 in this dictionary? Successor of 12 is not 22. It is 14 which is the key following 12 in the order relation. The order relation I am assuming is just the order on the integers. So 12 is the smallest key, 14 is the key larger than that. 18, 22, 34. So 34 is the largest key in it. Given the key 12, I have to run through this entire thing to find out the one which was the smallest key larger than 12. Both successor and predecessor and min and max will all take order n time. So it?s fairly an efficient implementation. An ordered list on the other hand, let?s say we ordered the thing according to the total order on the keys. So now minimum takes only constant time. Maximum also we can organize so that it takes constant time. Let?s say I have a pointer reference to the end node in this list.

(Refer Slide Time 04:33)


 

Successor also takes constant time because given a particular node, I can just go to next one that will give me the successor. Let?s say predecessor takes constant time. It depends upon how you give the node. When I ask for successor of a node if you have to search for the node then of course it will take order n time because you might have to go through this entire list to reach that node. But if I tell you where the node is, let?s say I give you a reference to this particular node in this list, then you can compute the successor and predecessor in constant time. Inserting also takes order ?n? times because you have to find out where to insert. You have to find out the correct position for insertion. Searching takes order ?n? time because again we may have to run through the entire thing. If we use an array, searching can improve and you have seen an example of how to do this.








(Refer Slide Time 05:43)

 

 (Refer Slide Time 06:03)

 

We use binary search to do that. If you put the elements they are ordered. I put them in an array. Then we can do a binary search to find the element in log time. If we do binary search then now insertion and deletion still take order n time. 









(Refer Slide Time 06:33)

 

While we can find out what is the place to insert an element, you will have to shift all the elements to the right. So it takes lot of time to do the insertion. Similarly for deletion, we know where the element is which we are trying to delete. Then we have to shift everything to the left. Just to recap what binary search was, so you remember binary search. To search for 22, you go to the middle element. 22 is larger you will go to the right. 25 which is smaller go to the left so on and on. In that manner, you will eventually end up with the array location which has 22 or an array location which does not have 22. In that case able to say whether 22 is there or not and the number of comparisons you would take in this process is only logarithmic.
 
(Refer Slide Time 07:15)

 



(Refer Slide Time 07:43)

 

Every time you make a comparison the size of the array and which you are making the search ?halves? and number of times you can half n to get down to one has only log of n and so you will take order log n time to do the searching. Recall insertion and deletion I said that will take order n time. These are the trivial ways you adopt to implement an ordered dictionary. 
 
(Refer Slide Time 07:49)

 

But we are not here to talk about trivialities. So we are going to look at something more interesting and that?s called a binary search tree. What is a binary search tree? A binary search tree is a binary tree which has a search property on it. Recall what is the binary tree. Binary tree is a tree in which every node has at most two children. A node can have one child, two children or no children. No children means it is a leaf node and now there is a search property that we are talking out. 
So each node is going to contain a key and an element. In most discussions that follow, we will not be talking about the element at all. We are just interested in the keys.

(Refer Slide Time 08:32)

 

What is written on the nodes are the keys. Now what is a binary search property? The binary search property says the following. All the keys which are less than five will be in the left sub tree. All keys which are larger than five will be in the right sub tree and this property holds at every node. Not just at the root node. [HINDI] not all keys which are larger than 3. Of course there are keys larger than 3 which are here but basically if I look at the left sub tree, all the keys in the left sub tree will be less than this node. All the keys in the right sub tree will be larger than this node (Refer Slide Time: 09:47). Similarly for this tree. All the keys in the right sub tree as you can see are larger than this key value. If I look at this node, all keys in the left sub tree are less than 7. All keys in the right sub tree are larger than 7. This is called search property. 


















(Refer Slide Time 09:05)

 

A binary tree in which the nodes have keys which satisfy the search property. So the search property satisfied is called a binary search tree. So binary tree plus search property equals binary search tree. I have these set of keys: 2, 3, 5,5,7,8. This is the binary search tree with these keys in it. This is also another binary search tree with the same set of keys. 
You can have many different kinds of trees with the same set of keys. Both of them are binary search trees because they both are binary and satisfy the search property. 

(Refer Slide Time 10:26)

 

Conversation between Professor and student: It is implementation. Keys stored at the node in the left sub tree of v are less than or equal to k. (Refer Slide Time: 11:00) So there are couple of features here. Here I am assuming that you might have two keys which are the same. So quite often this doesn?t happen. So quite often you would be using dictionaries only in settings were the keys are unique. No two keys are the same. 
(Refer Slide Time 11:22)

 

Suppose you had the setting where two keys could be the same, suppose your key was the 
name of a student, you can define total order of names let?s say lexico graphic order or alphabetic order and then two names could be the same. So you could have settings in which the two keys are the same. Then we have to decide whether if a key is equal. Whether it should go to the left sub tree or it should go to the right sub tree and we can decide one way or other. Let?s say we go to the left. We could easily have said it goes to the right. There is no problem with that. Here actually I am permitting it to go both ways. Either go to the left or it could go to the right. 

(Refer Slide Time 11:54)

 

So for the rest of the discussion let me just assume the keys are unique. Otherwise it unnecessarily complicates matters. So we will just assume keys are unique, do the entire discussion see if there is a need of duplicate key. This is what should we do then. If there are duplicate keys how do you handle that now? Suppose you want to search in a binary search tree, given a particular key. So after all it?s a dictionary. We are implementing dictionary. So given a particular key I want to find out where the element is. So let me first show you an example.

(Refer Slide Time 12:50)

 

Suppose this is my tree and i want to search for 11. I come to the root.  I compare 11 with 5. 11 is larger than 5. So search property says that 11 has to be in the right sub tree. That was the search property. Keys which are larger than here will be in the right. Keys which are smaller will be in the left. So we go to the right sub tree. [HINDI] 11 is larger than 10. So once again if it is there at all in the tree, it has to be in the right sub tree. Hence we go the right sub tree and we compare 11 with this and we find 11. So we are done. We found 11. 

(Refer Slide Time 13:00)

 
That?s what was being said here. To find an element with a key k in a tree T, you will compare key k with the key in the root. If k is less than the key in the root then you will search for the key in the left sub tree. Otherwise you will search for k in the right sub tree.
 
(Refer Slide Time 13:54)

 

Suppose we have same example but now searching for 6. We come here 6 is larger so we go right. Then we compare 6 with 10. 6 is smaller. So we go left. Then we compare 6 with 7. 6 is smaller. So we try to go left. But the left child is null. So it?s not there. Because if it were there, it has to be here.

(Refer Slide Time 14:22)


 


It has to be in the right sub tree of this guy. Because 6 is larger than 5. It has to be in the left sub tree of this guy because it?s less than 10. It has to be in the left sub tree of 7 since it is less than 7. So if 6 were there, it had to be here and since it is not here, it?s not there. 

(Refer Slide Time 14:56)

 
             
So we can write the search procedure for binary search tree either as a recursive procedure or as an iterative procedure. So the recursive procedure is perhaps the simplest to understand. So you are searching for a key k in a tree t. So you look at the root of T. Let?s say that its x. If x equals nil which means there is no root or empty tree. Then you are essentially saying that nothing is there. So just forget this for now. If the key in this root node x is equal to the one you are searching for, then you just return the root node. If it is less than the key in the root node, if k is less than the key in the root node, then you have to recursively search in the left sub tree. So you are searching in the left sub tree of x. And what you are searching for? You are still searching for k.

















 
(Refer Slide Time 16:20)

 

So what is the left of x doing? This is all pseudo code. Let me come to what your question is. X is a reference to a particular node. So x to begin with refers to the root of the tree and then left of x is the reference to the root of the left sub tree. So it?s referring to that and we are searching for k in there. So that?s what we have to do and if key is larger than the key in the root, then we have to search for k in the right sub tree. This is clear with everyone. Let?s go to the iterative version. In the iterative version, we are not going to make recursive calls to the search procedure. What we are going to do is as we are doing in the search, we are just going keep matching down the same tree.

(Refer Slide Time 18:14)

 



So we start with x referring to the root of the tree. Well if x is nil which means that it?s an empty tree and k is not the key in the root. Then we will do something. What will we do if k is less than the key? Then we have to go left. So x now becomes the left child. x now gets the value of the left child of this current node.

So we started off with x, referring to this guy and then we said if k, so suppose there is key k1 here we are searching for the key k. If k is less than k1 then we go left, if k is more than k1 we go right and since we go left here we are now continuing the search here. So x is gets the new value which is either x dot left child either you want to call this way or actually in the code just showed you just now, I have written it as x gets left x.  

Pseudo code I can use any one of these so this is what it gets and we continue this search. We are basically may be the next step we go right and so on and on till we reach here, this is where our node x. So x will keep getting modified, first it will pointing to this node then its pointing to this node, pointing to this node and eventually pointing to this node. It?s keep getting modified in this manner.  

(Refer Slide Time 21:02)

 

How much time does search take? So let?s look at the iterative version of the search. What we did was that with each time we went through this while loop which is here, we came down one level in the tree. We went from a node to one of its child nodes either the left child of that node or the child of that node, [HINDI] we can came down one level.  










(Refer Slide Time 21:15)

 

We started at level zero which is the root then we came after one run of while loop, we came to one level after that we came to level 2, level 3 and so on. How many times we will execute this while loop, the maximum number of levels in the tree and what is a maximum number of levels in a tree, it?s the height of the tree. So if the height of the tree is h then the running time of procedure is no more than h. So order h is the running time of procedure. 

(Refer Slide Time 21:50)

 

Now h recall can be very large. I might have a tree on n nodes whose height is order n and we will see at what kind of situations this happens but note that the height of the tree could be very large. The search time is only order h the height of the tree, order the height of the tree but the height of the tree in particular can be has large as the number of nodes in the tree.
Let?s look at other procedure that of finding the minimum element in the tree.  

(Refer Slide Time 22:45)

 

Where do you think the minimum element in a binary search tree? Why left most tree? Left most node or left most leaf? Left most leaf. Let?s see why that?s wrong. Suppose I have a tree which looks like the following. I need to put up in some keys so that this looks like a binary search tree. So let?s put in 7, 5, 6, 12, 13, 11, 10. This is the binary search tree? Now which is the smallest node here? The leftmost leaf, I do not know the left most leafs but this must be the left most leafs but this is not the smallest one. Smallest node is 5, it?s really the left most node expect left most node also doesn?t too much sense. Last leaf, this is not a leaf at all. 

(Refer Slide Time 24:29)

 

Let?s not try to say which node? last internal node. This is an internal node, you want me to take more internal node without left children. No, I can create more internal node without left children. So this is also an internal node without any left children so don?t try to just give the half a sentence definition of which the smallest is, but let?s give a procedural definition.

(Refer Slide Time 25:02)

 

So which is how do you find the smallest? Start at the top, keep going left till left becomes null that is clear to everyone. So start from the top, keep going left till the left is null and that?s the smallest node. How about maximum? Keep going right till the right becomes null. What?s the proof? Why this is minimum? So the proof is very simple, the minimum has to be to the left of its root, so it has to be in the sub tree. Then you have smaller, so the smallest has to be left and there is nothing to the left so this has to be smallest, everything to the right has to be larger than this. So in one line that?s the proof for this fact.

















(Refer Slide Time 26:06)

 

So that?s the entire code essentially, you want to find the minimum in a tree x. So while the left of x is not null, just do x is left of x, keep going left of x. When you stop? When left of x is null at that point you will just return x or return the key x or return the element or whatever you want return. So how much time do you take again in this? 

(Refer Slide Time 26:10)

 

So why because of same argument, with every run of this while loop we are going down one step. So we can go down at most the number of the height of the tree and so that?s the maximum time taken. The same procedure can be used in small modification to compute the maximum. We just have to replace the left by right and then we will complete the maximum.

Let?s see how to compute the successor element of a tree? Successor, understand what successor means? Successor means given a particular key after find out the next one. So given x, find the node with the smallest key greater than key of x [Hindi]. Let?s see so there are 2 cases [Hindi] all of you saying key sub tree. [Hindi Conversation]. So there are two cases really so case one is in the right sub tree of x is non-empty, there is something in the right sub tree. So the picture is [Hindi] this is the node [Hindi] successor [Hindi] Right sub tree is non-empty, there is a right sub tree. It exist say [Hindi] so then we know that the key which is larger than this, all the keys in here are larger than this. We know that but why should the successor be lying here? Why can?t the successor be some where else? 

(Refer Slide Time 27:58)

 

It is greater than this. So I know that the keys here are larger than 5 but there are other keys which are larger than 5, the parent is one key which is larger than 5. So lots of confusion. So we need to look at it more carefully. So what I am way trying to say, let me draw this picture that was there on the fresh sheet of paper. So I have 5, I will put down 5 here in the center and we have 10 here, we have 7 here and we have 8 here, we have 1, 3. Now I have to find out the successor of this 5 (Refer Slide Time 30:17). I am trying to find out the successor of this node. 

This has the sub tree, now all of you are tempted to say that successor of this guy has to lie in the right sub tree, you want to say that the successor lies here, why? This is not the entire tree, please remember. It could have a parent, the parent is going to be larger than this 5. When it is larger than 5? When it is in the left sub tree. So there are two possibilities, the parent is larger than 5 which means 5 is in the left sub tree of the parent but than this guy will be larger than all of this guys because all of this guys then are in the left sub tree of the parent. The a successor [Hindi] this parent then cannot be the successor. 




(Refer Slide Time 31:31)

 

Why? That is the procedure which you have learnt but why is it that the successor only has to be in this right sub tree but nowhere else if the right sub tree is not null. Conversation between Professor and Student: Refer Slide Time: 31:58). But the next element which is larger than 5 could be let?s say this is my entire tree, it could be some where here [Hindi] why not?
 
(Refer Slide Time 32:17)

 

The easiest way to think of this is as follows. Suppose we were searching for some elements slightly larger than 5, we were searching for the successor of 5. The claim is that suppose we were searching for 5, the claim is that we must end up at this node that?s a point, whatever decision we are making, we would end up at this node. Now if the key we were searching for is slightly larger than 5 then that means we are searching for the successor of 5. 
Then that means we would second end up with this node and proceed because it is larger than 5 so which means that any successor of 5 really has to lie in this part of the tree. So in this part of the tree the successor is essentially be minimum node because all the keys here are larger than 5. So the minimum key here is the one we are looking for, we have already seen a procedure for completing the minimum which is that keep going left. We will go left, we will go left and we can?t go left any further so this becomes the successor of 5. 

(Refer Slide Time 34:22)

 

It remains me the remote that I have at home. So that?s what we have to do, if the right sub tree is non empty then we go right one step and keep going left. 

(Refer Slide Time 34:24)

 

The other case is in the right sub tree. Suppose I was trying to look for the successor of 3, if I am trying to look for the successor of 3 the right sub tree is empty so where is the successor of 3 now? Look at the parent. 

(Refer Slide Time 34:57)

 

The procedure is the following and we will see why it is a right procedure. What you are going to do is start going from this node to its parent and then to its grand parent and so on till you reach the node such that this key x that the successor you are looking for is in the left sub tree of that node. So you went up to here but note that this is in the right sub tree of this node. Then you went to its parent and here you found that 3 is in the left sub tree of 5 so we will stop here and this will comes the successor. You understand the procedure but why is that procedure correct? 

So recall the procedure is, I am searching for the successor of this node. The right sub tree is null, there is nothing to the right then I go to its parent and I go to its parent and I keep going up to the parent till I reach an ancestor such that this node whose successor we are trying to find is in the left sub tree of the ancestor. So this guy then going to be the successor of x, this node is going to be the successor of this (Refer Slide Time: 37:00). Why this is true? 

So this node has to be, is it larger than this? No it is smaller because it is in the right sub tree so it is smaller, this is also smaller, this is also smaller, this is larger (Refer Slide Time: 37:23). In fact this this guy is in the largest node in the entire sub tree because how did I find the largest node I come here and keep going right which is exactly what I do, so this is the largest in the entire sub tree and the node following this is then this guy. Yes or no? Can the successor could have been some where else, could it have been to the right of this guy because all of this going to be larger than this, can it be an ancestor of this guy. This ancestor is less than this but this ancestor if it is less than this it is also less than entire thing it?s not a successor. 


But if you are looking at this ancestor then this ancestor would be less than this and it would be less than entire thing so I was wrong, this ancestor is less than entire thing is not really a successor but this ancestor is greater than this entire thing but it is also greater than this. So this is only the successor so better for confusion but you understand what the arguments are (Refer Slide Time: 38:57).

(Refer Slide Time 39:01)

 

(Refer Slide Time 39:08)

 

So let?s see what code I can write for computing the successor. If this not null then I just return the minimum in the right sub tree otherwise I will come to this point I am looking at the parent, now so y is the parent and what I am doing here? If y is not null and x equals right of y, so when do I have to keep marching till x becomes the left. 


(Refer Slide Time 39:56) 

 

So essentially recall I am going up the tree and the first time I essentially I am at the node such that the node is the left child of the parent then I stop. The first time, so let me show you the previous slide. The first time so I keep going up here the first time I come to a node such that the parent such that this node is the left child of the parent then I stop.

(Refer Slide Time 40:22)

 

So for instance if I were looking for a successor of 8, I would go up like this and this is the first time the node is a left child of its parent and so I stop it. 





So that?s what we are doing here, so initially y is the parent of x and all we are doing is moving the pointers x and y. So y is the parent of x and then we are changing it so that x takes the value y and y takes the value of parent of y, so that it moves one step ahead.  

(Refer Slide Time 40:46)

 

So in some sense these are a pair of pointers which are moving up the tree with y always ahead of x, x y, the x moves and then y moves and x moves and then y moves and so on. When do we stop? When x is the one which is trailing, y is the one which is head when x is the left child of y. When x is the left child of y we will stop. So essentially we are continuing in the loop till x is the right child of y, when x becomes the left child of y we will get out of the loop or y becomes null that means I have gone beyond the root then we stop and we return. We return y which was the parent node. In case y hits null then we will return y which means null. If we return null which means it could not be found, so you can have a closer look at this code and convince yourself that it is correct. 

So once again what is the running time of the successor code? Order edge, in each of the cases, in one case what we are doing? We are going down the tree but we only go down as many levels as the height of the tree at most in other case we are going up the tree. So the maximum number of levels we can go up the tree is at most the height of the tree.













(Refer Slide Time 41:44)

 

So now look at the insertion procedure in a binary search tree. Till now we looked at search, how much time did search take? Order edge, height of the tree. We looked at minimum, we looked at maximum both of them take time proportional to the height of the tree, worst case time and we looked at successor and it took time proportional to the height of the tree. Can you also compute predecessor? 

(Refer Slide Time 42:50)


 

Similar idea, essentially interchange role of right and left, successor may [Hindi Conversation] and we will match up the tree and all of that is the same thing. 


Let?s look at insertion so that?s the other thing we will do today, we will take care of deletion in the next class. So insertion I have a binary search tree and I want to insert an elements so what should I do? Alpha search for the elements. 

(Refer Slide Time 44:00)

 

I first find out the place in the tree where the element should be. We are assuming distinct keys, we are assuming that node to elements of the same key. So we first find out where it should be and then we put it there so that it is actually there. So let?s look at an example, this is my binary search tree and I am trying to insert 8. So [Hindi] you recall this 8 is not there in the tree so I fist search for 8 which means I go down.

I come here go right, I go left and then I want to go right but right is null which means 8 is not in the tree, so I put 8 here which means that 8 becomes the right child of 7. At which ever point the search fails because we hit a null pointer and null reference we put the node there that?s all insertion is.

How much time does the insertion take them? Order h, which is same as searching essentially [Hindi Conversation]. So now that brings me to that question I had asked at the beginning of the class which is in what order should I been inserting the keys in a binary search tree so that its height? So we said the maximum height of a binary search tree is in or in minus one or one of those things. This is the maximum height of the binary search tree. Suppose I have n keys, in what order should I insert those keys so that I can get a tree of height n ? 1? Let?s say our keys were 1, 2, 3 up to n. So what order should I insert them, exactly this order or the descending order. So [Hindi Conversation] we will insert 4 at this place and in this manner we will create the chain of this kind and we will get a tree of height n ? 1.






(Refer Slide Time 46:03)

 

Similar would happen if I had inserted in the descending order. So which order should I insert it so that it can get small height? The first element I should insert is n by 2. Which is the next element I should insert? n by 2 ? 1? n by 4 and 3 n by 4 and then I would get something like this and now n by 8 and is it clear that n by 8 will come here? Why this because you will compare and put it here and which is the next one? 3n by 8, suppose I insert 7 n by 8, if I insert 7 n by 8 what happens? So can I insert 7 n by 8 at this point, can I insert 7 n by 8? No harm I can also insert it here.  

7 n by 8 will always come here, now I can insert my 3 n by 8 and now I have to insert 5n by 8 which will come here. Essentially I have to go level by level or I have to insert these nodes first and only then I have to insert later nodes otherwise. So in this manner I will end up with what kind of a tree? Height balanced, a complete binary tree I have not told you what height balanced trees are, you don?t know what the height balanced tree, you don?t even know the full binary trees is, we have learnt only what a complete binary tree. So this we will get is the complete binary tree, complete binary tree provided your n was some power of a 2 or 2 to the k ? 1, n has to be of the kind 2 to the k - 1 then you will get a complete binary tree. So in the next class we are going to ask the following question. 

If I was able to take a random permutation of these elements, if I put the elements in an ascending order then I get a bad tree. Why I am calling this a bad tree and this a good tree? Because of the height, this has the huge height, this has a small height why is height such so relevant because so all our operations were depended upon the height of the tree. The smaller the height of the tree, the faster the operation is so you would like to keep the tree height as small as possible.  







(Refer Slide Time 50:01)

 

So we don?t really want trees of this kind, of course you know with some effort you could figure out in what order should I insert these elements so that I get a small height tree and what is the bad order. Suppose I were to just take a random permutation of one through n, you understand the random permutation. Take one of the permutation there, where n factorial different permutation. Take one of them at random and insert elements in that order. What will be the height of the tree then? Depends on what you or and me? Who does the insertion on the permutation? It will depend up on the permutation so we have to talk of random variable. So the height of the tree now will become a random variable. You understand what a random variable means? Variable which takes many different minimum values so the height of the tree could take one of many different values.

(Refer Slide Time 50:06)

 

What are the many different values that the height of the tree can take? log n to n, log n to n - 1 or log n + 1 to n ? 1, I don?t know exactly which but one of those. log n to n so it will take one of these different values and it will take each of this value with a certain probability because after all I took a random permutation. So what we have to find out? So taking each of these values in the different probabilities so you have to find out what the expected value of these random variables. We will address this kind of a question in the next class.
 
(Refer Slide Time 51:47)

 

So this can be the code for insertion, so I am given a tree t [Hindi] I will just switch. This can be the code for the insertion, I am trying to insert this node z into a tree t and once again I am going to have two pointers x and y and what is the need for this 2 pointers x and y because recall when I am inserting a node so when I am inserting this node z, I am searching for z the key corresponding to z in the tree.


















(Refer Slide Time 52:21)

 

Then I hit this null and so we said we will put z at that place but to put z at that place what we will have to modify? The parent of z [Hindi] right child [Hindi] modify child a left child [Hindi] modify [Hindi] essentially parent pointer parent [Hindi] pointer [Hindi]. We will have two pointers x and y such that now the game is that y will be preceding x.

(Refer Slide Time 53:28)

 

So these two pointers will keep moving down such that y will always be the parent of x. So that?s what we are doing initially y is null, x is the root then y takes the value x and x goes to either left or to the right child of x and we keep doing this till we hit null, till x becomes null when x becomes null y is pointing to the  right node. [Hindi Conversation] so y is going to be the parent of new node we are inserting, so parent pointer of z so recall that the node is also have a left child and right child and then parent. So parent of z gets the value y and depending up on whether the key of z is less than the key of y or more than the key of y, if key of z is less than the key of y then z will be a left child of y. Then the left child of y becomes z and if key of z is more than the key of y then the right child of y becomes z. 

(Refer Slide Time 54:18)

 

That?s all you have to do but does everyone understand what the need for having these two references. You need to keep track of the parent because that?s the one you have to modify so you will go modify that and you will just set it appropriate to the left to the right. So already I discussed this, so in what sequence the insertion should be done, we saw that which should be worst case sequence. So with that we are going to stop our discussion. 

(Refer Slide Time 55:20)

 

On binary search tree we will continue this in the next class with the deletion procedure for binary search trees and will also look at this questions that I asked you today but if to insert elements in the random order then what is the time taken to do the insertion and what kind of the tree do you get as the result of that. 


Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 9
Deletion

Today we are going to start with deletion. In the last class we saw how to do insertion, search, computing successor, computing predecessor, computing minimum and maximum in a binary search tree. Today the only 1 operation that is left is the deletion and so we are going to see how to do deletion in a binary search tree. Then we are going to address the question that I had raised in the last class which was that if I were to insert some n elements into a binary search tree, suppose I were to randomly permute my elements and insert them. Then what can we say about the time that the insertion would take. Today we will see all of that in detail. 

(Refer Slide Time: 2:21)

 

We are given a node x in the tree and you have to delete the node. We will distinguish 3 cases. X has no children which will be the easiest case for deletion, x is the leaf in that case. If x has only 1 child even then the case will be very easy and we will see all of that and when x has 2 children it is slightly trickier but still fairly straight forward.
 
When x has no children then deletion is trivial. Why is that because the point B is the node x, it does not have any children so it is a leaf. I need to delete the node x. I should just cut this length which means we will have to change either the left child or the right child of this parent. 

In this case node B was the right child of the parent A, so we have to set the right child of A to null. If B were the left child then we would have set the left child to null. The 2nd diagram in the below slide is the tree that would be left after the deletion. If the node that you are trying to delete is the leaf node then the problem is trivial.

(Refer Slide Time: 3:09)

 

What happens if the node that you are trying to delete has only one child? From the slide given below, you are trying to delete A, it has only 1 child which is B. A does not have a left child. The operation is like that you can think the part ABD as a linked list. It is just like a linked list and you are trying to delete a node from the linked list. You take the previous node which will be D and you will make its left child point to B. 

(Refer Slide Time: 4:54)

 

Effectively you are removing the 2 links which is coming from A and you are establishing the link directly from D to B. Are you convinced that this would maintain the search property. Why should B be on the left of D and not on the right of D? Because B is less than D, and B is in the left sub tree of D. We would make B as the left child and everything that is below B that is all the descendants of B are also less than D. Because they are all in the left sub tree of D. We will just continue with them as there is no change. The only change that happens is that the 2 links from A go away and you set up 1 link from B to D. So very little has to be done in this case. In the above slide 2nd diagram will be the new setting or this is what will happen after the deletion. You have D, B and F but A goes away. 

The 3rd case is when the node to be deleted has 2 children. X is the reference to the node which is to be deleted. The node to be deleted is the one containing D and it has 2 children. Since it has 2 children let us do the following, it has a left sub tree and a right sub tree. We find the predecessor of D. We have seen how to do successor but we also said that you can equally find the predecessor. 

How do you find the predecessor? The predecessor of D would be the largest element in the left subtree. We just have to come left and keep going right, since B does not have right child so B is essentially the predecessor of D in this example. 

(Refer Slide Time: 07:40)  

 

How many children does B have? B can have 1, it can have a left child nothing is preventing us from B having a left child. I could have a situation in which let us say T, you could have something like T. But B does not have a right child, there is nothing in the right side of B. If B had a right child then it would not have been the predecessor of D, because we would have gone down further.

B has only 1 child or no child, the node T need not be there. That is what is being said here B or y is the same thing. The y is the reference and B is the content sometimes I am calling it B, sometimes y but this has atmost 1 child. We are going to delete B and essentially we are going to move B to D. That is we are not establishing such a link from B to D, but we are going to replace D with B and delete the node B.  

Why would the search property not get violated by moving the content of B to D? Since B was the successor of D, everything in this left subtree was less than D and B is the largest element in this left subtree. Everything in this left subtree is also less than B, if I move B to D there is no problem as far as the search property is concerned. Everything in the right subtree is more then D and so it is also more than B. By moving B to D again the search property is not violated. 

(Refer Slide Time: 08:38)

 

I can move B to D and can I delete B easily. Because B has either no child or 1 child. We have already seen how to delete a node which has no child or only 1 child. If B had a left subtree and suppose I decide to move B to D and there is nothing left so we will delete the node B. When we delete the node B, what happens we will make the right child of A point to that node. The node B gets deleted and the link corresponding to that goes away and we create 1 link and it goes to the point where it is directed and B moves up. 

That is the operation and now we have covered all the 3 cases. The 1st case was when there were no children, 2nd case was 1 child and 3rd case was 2 children. We said in the 3rd case we will have to move the content of B to D and then delete the node B. We worked with the predecessor but we could also have worked with the successor, since D has 2 children we can also find the successor of this node by going once right and then keep going left. The same kind of a thing can be done even there, we could have replaced D with its successor instead of its predecessor. But in this example I have shown the predecessor.  
What do you think is the running time of the delete operation? Suppose you do not even have to search for the node. I tell you this is the node and I give you the reference to the node that you want to delete. How much time does it takes to delete? In the 1st case when the node is a leaf, how much time it would take? Order one because it just needs to go the parent. There is a pointer in the node, reference to the parent node I just go to the parent and I just update the link that is the right or the left child of the parent. I am giving the reference of the particular node which has to be deleted. In every node we also have a parent, left child and a right child. We can always go back to the parent and update the content to show the deletion operation. 

In the 2nd case if we are deleting the node A, then once again we need go to the parent and modify its left child, again a constant time operation. In which case do we take more time? In the case 3, because here we have to find a predecessor or a successor. Predecessor can take time as large as the height of the tree because we will have to go once left and then keep going right. We can take in the worst case time proportional to the height of the tree. Case 3 takes more time and the time taken in the worst case is the order of the height of the tree. 

(Refer Slide Time: 13:15)

 

The pseudo-code for deletion is given in the below slide, so that you will understand roughly what is happening and we do not have to look in detail. If the left or the right is nil then we know that either it is a leaf node or it is a node with only 1 child. Then in that case, z was the node which I am trying to delete. So y ? z and we look at the TreeSuccessor (z). I am not completely sure that what I have is right, so let us just skip this. You would have understood the delete operation. I might have made some mistakes somewhere in the pseudocode. Predecessor or successor is the same thing, we can also work with the successor that is what I said. For the successor we have to search in the right tree. I do not think that it is very critical but let us skip this thing for now. You understood the delete operation and the pseudo code you can all write yourself. 
(Refer Slide Time: 13:51)

 

Suppose I give you a binary search tree and I do an in-order traversal of this binary search tree. What does in-order do? First the left then the 5th node then the right. First we will be printing out all the keys on the left. Suppose all I do when I visit a node is print out the key. That is what my traversal procedure is, first I will print out the keys which are on the left in some order. Then I will print the key which is in the center and then I will print out the keys on the right. This means that I will print out the root key 5 after I have print out the left and before I print out the right. All the keys on the left are less than or equal to 5 and all of the keys on the right are greater than or equal to 5, which means that 5 really comes at the right place in the ordering of these keys. 

Because everything which is less than 5 comes before 5 and everything that is more than 5 comes after 5. Hence 5 come really at the right place and the same argument can be set for every key not just the root key. When will you print 3 out? After I have printed everything to the left which means everything that is smaller than 3 will come before 3 and then I will print out things on the right.  













(Refer Slide Time: 16:54)

 

In in-order traversal, if you were to just to look at this and see in what order you would print out. First you will come to 3 and then go left, then come to 2 and print this one. Then you will print 3, 5 then you will come right and then go left and so on. This is the order in which you will print the keys. The property of the in-order traversal of a binary search tree is that it prints out the keys in increasing order. 

This can be a good method of sorting a bunch of keys. I will call this the binary search tree base sorting procedure. What is the method? You have a bunch of keys which you want to sort. You first insert all the keys into a binary search tree. That is what you are doing in the slide given below. You are taking all the keys and inserting them into a binary search tree and then just do in-order traversal of this tree. You will get all the keys in an increasing order that is you have sorted a set of keys.
















(Refer Slide Time: 19:31)

 

How much time does this procedure take? We have to insert all the keys and then we have to do an in-order tree work. Let us first look at how much time does the in-order tree work take. How much time does it take to traverse the nodes of a tree in in-order? It is order n. Why it is order n? We have to print all the node but I might take much more time. I need atleast order n time may be I need more time. We will look at this question later. 

How much time does it take to do an in-order tree work? Actually it just takes linear time that is it takes order n time. We will not do it in todays class, we will see how to argue that it just takes order n times. How much does the part which is given below takes, when I am inserting all the elements into a tree?
        for I ? 1 to n
           TreeInsert (T, A[i])
It is n log n.  But why it is n log n?














(Refer Slide Time: 20:14)

 

This is my BST sorting procedure. I want to sort the keys given in the above slide, I insert them into a binary search tree one after the other. First 5 then 10, 7 then when I insert 1 it will go as the left child of 5 then when I insert 3 it will go as the right child of 1. When I insert the other 1 it will go as the left child of 3. It depends upon how you are doing it, it could also have come as the left child of the other 1 and it is the same thing. And 8 would come as the right child of 7 and the final diagram in the above slide is the tree you would get. If you have to do any in-order tree work on this tree you would get exactly the sorted sequence.

How much time does it takes to insert all the n elements? It depends upon the sequence. If I were to sort let us say given a set of numbers 1 through n and I just want to sort them. The total time taken to insert this numbers is equal to the sum of the levels at which the nodes will come up because when I insert a particular number I come down the tree and I insert it at a particular place. The number of levels I traverse or the number of comparisons I do before I insert it is exactly equal to its level. Either level or level +1 or level -1 but you can think of it as the level for now.  

If I had inserted the numbers in sorted order, first I inserted 1 then I inserted 2, 3 and so on. What is the kind of a tree I would get? I would have 1 followed by 2 followed by 3 and so you remember that kind of a picture. I would get a tree some thing like the one which is given in the slide in blue color from 1 to n. What is the sum of levels? It is from 0+1+2+3? (n-1) and O ( ) is the series which sums to . That is not good, the time taken for insertion could be as bad as . 




(Refer Slide Time: 22:15)

 

Is this clear to everyone and this bring backs the question that we had asked in the last class. I take a random permutation of the elements. There are n factorial different permutations I take one of them at random. I insert the elements in this particular order. I want to compute the time it takes to insert. The time required for insertion is a random variable.  

We saw an example where the time required for insertion could be as bad as . That is what I showed you in the previous slide. In the best case it could be quite small and we will the best case. It is really a random variable it depends upon the sequence and the permutation in which we are inserting the elements. In fact we had discussed the best case in the previous class, we said the first element inserted should be  then it should be  and then   and so on. 













(Refer Slide Time: 24:52)

 

All the way we had not computed the total time for insertion. The total time for inserting these n elements is really a random variable and for random variables we compute what is the expectation of the random variable. Recall what is the expectation of a random variable? If a random variable takes many different values then the expectation of the random variable is obtained by taking the average if the probability of each value is the same. Otherwise you essentially do many trials which means that you compute the value of the random variable then you do the experiments once again and so on. Look at the value of the random variable that you get and take the average over all the trials. That is how you compute the expectation of the random variable.

In our case it is easier because each of the permutation is equally likely. We are saying we will just pick one of the permutations uniformly at random. Each of the permutation is equally likely so we are going to see how much time it takes to insert the elements in the order given by a certain permutations. How do we compute the average? We will take a permutation, insert the elements in that order and compute the time it takes to do that insertion. Then take another permutation compute the time it takes to insert the elements in that order, take the 3rd permutation compute the time it takes to insert the elements in that order and so on. 










(Refer Slide Time: 26:18)

 

There are n! different permutations, while compute the time it takes to insert the elements in the order specified by these n! different permutations. You will get the total time this huge quantity and will then divide by n! to compute the average and that could be the expected time it takes. Just to summarize, for each of this n! permutations we will compute the time taken to insert the keys in the order specified by that permutation and then we will compute the average. The average is computed over the n! permutations. We will denote this quantity by T (n), whatever is this value will be T (n). Let us see how to compute T (3). 

(Refer Slide Time: 28:50)

 

T (3) is the tree obtained or is the expected height of a tree on 3 nodes. What are the various possible trees on 3 nodes that we get? What are the various permutations of elements 1, 2, 3? We will have 6 different permutations as given in the slide. When we have this particular permutation what is the tree we would get? If we were to insert the elements in this order (1 2 3) and you will get a tree whose root is 1, 2 would come to the right and 3 would come after that. When the elements are inserted in this particular order (1 3 2), we would get a tree 1, 3 to the right and 2 to the left of 3. 

If the elements are inserted in this order (2 1 3), we will get a tree which is 2, 1 to the left and 3 to the right. When elements are inserted in this order (2 3 1), we get a tree which is 2, 3 to the right and 1to the left. When elements are inserted in this order (3 1 2), we get a tree which is 3, 1 to the left and 2 to the right of 1 but to the left of 3 and in this order (3 2 1), we get 3, 2 to the left and 1 to the left. 

What is the total time taken for insertion? We recall what we said, it is just the sum of the levels of the nodes. The levels of the nodes are 0, 1, 2 and so the sum is 3.  In the next one the levels of the nodes are again 0, 1, 2 so the sum is 3. For the next one it is 0 and 1 in which both 1 and 2 are at level 1, so the sum 2. For the next one also the sum is 2. For the 5th and 6th one the sum is 0, 1, 2 that is 3. Total of 16 and so the average is   which is 2.66 which is T (3). Hence we computed T (3) as 2.66. That is what being said here, for each of the n! permutations we are going to compute the time taken to insert the keys which is what we did for T (3). Then compute the average that gives us the value of T (3). But we cannot do the computations in this manner, we have to do it cleverly because we have to compute what T (n) is. Now am going to fix a particular element i and I am going to look at all permutations in which the element i is the very first element in the permutation. 

(Refer Slide Time: 29:10)

 

There are (n-1)! permutations in which i is the first element. In these (n-1)! permutations the tree that we obtain will have i as the root because it is the very first element and then the keys one through (i-1) will be in the left subtree of the root and the keys i+1 through n would be in the right subtree of the root. 

Let us look at one of this (n-1)! permutations and restrict our attention to the keys 1 through (i-1). They will be coming at many different places and at some places in this permutation. 

Let me look at another one of these (n-)! permutations. These keys 1 through (i-1) would be coming at some other place. If I look at all of these (n-1)! permutations they will induce permutations of keys 1 through (i-1) elements also. Every permutation of 1 through (i-1) would be there. 

(Refer Slide Time: 31:35)

 

In particular each permutation of 1 through (i-1) would occur how many times. There are (n-1)! permutations in all and there are (i-1)! permutations of the elements 1 through (i-1) and there is no reason why one of these permutation will occur more often than the other permutation. Each of them is occurring equally likely. How many times is each one of them occurring?

It is   times. You can also think of it in a slightly different manner. You have these (i-1) elements somewhere and the other (n-i) elements are getting inserted at different places. 


How many different permutations are there of those (n-i) elements that are getting inserted? You can compute that and it will be exactly this   quantity. This is a simpler argument, there are (n-1)! permutations in all and there are (i-1)! permutations of these elements 1 through (i-1) and there is no reason why one of these permutation should be more likely than the other one. They will all occur the same number of times. It is symmetric of law, each permutation therefore of these elements is occurring exactly  times.  

(Refer Slide Time: 33:12)

 

Suppose I had only (i-1) keys, 1 through (i-1). Then the average time taken to insert these keys is T (i-1), this is what we defined T (n). This average is the average taken over all the permutations of 1 through (i-1). The time taken to insert all these (i-1)! permutations are just (i-1)! T (i-1). The average time is T (i-1) and the average is taken by computing the total time to insert all the (i-1)! permutations divided by (i-1)! The total time to insert all the (i-1)! permutations are (i-1)! T (i-1). For instance in the example that we did earlier for T (3), the total time taken to insert all the permutations was 16 which we had computed, that was the average times 6.

When we are inserting the keys 1 through (i-1) into the left sub tree, we go back to the previous setting. Recall we are considering only permutations in which the first element is i. The keys 1 through (i-1) when we are inserting they will all be compared first against i and then we will go and put them in the left subtree.

Each key has to be compared with the root which is i. It is like as if I am creating a left subtree of (i-1) elements but I can not just count that as the average call. I am also counting one more because for each of those elements in the left subtree their level is actually one more than, if it were just the left subtree. 

(Refer Slide Time: 34:60)

 

Let me clarify what I am saying. In the above slide the small round in blue color is the i and the keys 1 through (i-1) are coming inside the triangle. If that triangle in blue color were my tree then I know that the average time to create this tree is T (i-1) but I am creating a tree of over n nodes. The keys 1 through (i-1) are coming in the first triangle and the other keys are coming in the next triangle.

How much time I am spending in creating this part (darker triangle)? It is the average for 1 through (i-1) plus one more for each one of these, because first I compare with i and then come down because the level of each of these is actually one more in tree i than it is in the tree (i-1).Then it has level 2 in the (i-1) tree and level 3 in the original tree. What is the total time to insert all the (i-1)! Permutations? 

Recall T (i-1) is the average time to insert (i-1) keys. In the tree that I am creating am taking one unit extra for every node that I am inserting,   because first I am comparing it against the node. (i-1)! (T (i-1) + (i-1)) is the time I am taking to insert T (i-1) keys and that is the average time I am taking to insert the (i-1) keys in my tree. 

What is the total time? I have my (i-1)! different permutations so the total time is this product (i-1)! (T (i-1) + (i-1)). Each of the permutations appears so many times that is  times. What is the total time I spend in inserting keys 1 through (i-1)? The total time to insert all the (i-1)! permutations is (i-1)! (T (i-1) + (i-1)). But each of the permutations is appearing  times. So it is just  times (i-1)! (T (i-1) + (i-1)) which is (n-1)! (T (i-1) + (i-1)). This (n-1)! (T (i-1) + (i-1)) is the total time I take to insert the keys 1 through (i-1) into my tree where this sum is taken over all the (n-1)! permutations and this is not actually the total time but this is the total time for permutations in which the first element is i. 

(Refer Slide Time: 36:59)

 

The time to insert keys 1 through (i-1) is (n-1)! (T (i-1) + (i-1)). Similarly the time to insert keys (i+1) through n will be similar to the one before.  Wherever there is (i-1) I will replace it by (n-i), because that is the number of keys I am inserting. That would give me this expression (n-1)! (T (n-i) + (n-i)) and this is to insert the keys in the right subtree. 

The total time to insert all the n keys is just the sum of the above 2 quantities.  (n-1)! (T (i-1) + T (n-i) + n-1) is the total time over all the permutations in which the first key is i. What is the total time to insert all the n keys in all the n! different permutations? It is just the sum of this quantity (n-1)! (T (i-1) + T (n-i) + n-1) as i goes from 1 through n because the first key can be 1 or 2 or 3.  (n-1)!  











 
(Refer Slide Time: 40:22)

 

All I am doing is trying to figure out the total time taken to insert all the n elements not in one permutations but over all n! permutations put together. I am trying to do it in a way so that am able to analyze it. There are (i-1)! different permutations of (i-1) elements. In the sequence of 1through n those (i-1) elements are present at different places. 

All possible permutations of (i-1)! different permutations of those (i-1) elements are there but each of those permutations itself is occurring (n-1)! over (i-1)! times. We have to keep that in mind. We are going to work with the expression which is given below. 
             (n-1)!   

The total time to insert all the n keys is above thing, this is exactly the 16 which we had computed for T (3). 















(Refer Slide Time: 40:22)

 

What is the average time to insert n keys? It is basically, this is (n-1)!  the total time, so this divided by n! would be the average. Below given is the one which I would get. Just to make sure this divided by n! will give me   and this expression I have written where i go from 1 through n. 
            
Note that each of these terms T (i-1) and T (n-1) will appear twice. If I look at T (3), it will appear once for i equals 4 and it will appear once for i equals (n-3). Each of the term will appear twice, so that this part of the sum ) is 2 times T (i) where I going from 0 through n-1. 
This part of (n-1) have moved out, so it is n-1 where I go from 0 to n divided by n, so it becomes just n-1. This n-1 is not multiplied by    and this   only multiplies the following part. 

Why do i go from 0 through n-1?
Because you will never generate T (n) neither from T (i-1) nor from T (n-i). You will generate T (0), when i equals 1 you will have T (0) and when i equals n you will again have T (0). 






(Refer Slide Time: 42:16)

 

What is T (0)? T (0) is zero, in fact T (1) is also zero. T (2) is not zero but T (1) is zero because it is just a single element. We say it is at level 0 so we just do not count it any time for inserting it. Such a thing is called recurrence relation.  

The function T is a function of n, we are expressing the value of this function at point n in terms of its values at previous points. I can use the below given relation to compute T (0), T (1), T (2) and so on.  

                   

T (0) is zero. What is T (1)? 
                   

When we substitute n=1 in the above equation, we get T (1) as zero which we argued it before. So T (0) and T (1) both are zero. I can use that to compute T (2). What will I get the value of T (2)? If I were to just use this expression I would get 1 which is explained in the slide given below. Thus T (2) turns out to be 1. If you are not convinced so far lets also compute T (3). T (3) will be   times T (0) which is zero so let me ignore that. T (1) which is also zero so let me ignore that and T (2) which is non-zero so let me leave that around plus n equals 3-1.




(Refer Slide Time: 43:43)

 
           
T (2) was 1 so this becomes   + 2 which is 2.66 that is exactly what we computed. You can use this recurrence to compute any T (I). We have done something fairly sophisticated. We have obtained some kind of a relation which tells us any T (n) provided we know the previous values. But we will do a little bit more, we will try to figure out how to solve this recurrence relation also. That is what we are going to do now. 

(Refer Slide Time: 45:20)

 

The T (n) is   times the sum of the previous T plus n-1. I can write T (n-1) is  times the sum of the previous T + (n-2). I just replace the n by n-1 and so I can just rearrange this quantity  or  times this sum equals , I have moved this n-2 on the other side so I get T (n-1) ? n+2 and I have scaled it by   because I am trying to compute    instead of  . 
                  =  (T (n-1)-n+2)
              
If the above equation were times, this would just have been this part (T (n-1)-n+2)), if you cancel out the n the   is exactly this part (T (n-1)-n+2)). I just want to compute this quantity , because now am going to substitute this in the previous expression that is .

I have computed 0 through n-2 as this quantity , so am going to substitute that. Thus T (n) then becomes this sum  +  T (n-1) +n-1. Then I just did some rearrangement, that is   and   that makes it . The   cancels out and so I am just left with 2 times . So we will work with the below equation.
               

All I have done is some rearrangement to simplify things. I have written T (n) in terms of T (n-1). T (n) was in terms of all the previous T. This is the simplification I have achieved with this point. 






(Refer Slide Time: 47:15)

 

Let us see. It is a bit of a dense slide but we will run through this very quickly. This is what we have . I will just replace that is   is less than 2 because n-1 is less than n. That is why the less than or equal to, actually it should be strictly less because   is strictly less than 1.I just simplify this a bit and now I am going to do the following. 


I am going to replace T (n-1) by using this recurrence relation that is by using this expression . The   which is  times T (n-2) +2, where the other +2 is coming from the before expression. All I have done is replacing T (n-1) with . 












(Refer Slide Time: 49:25)

 

Let me just simplify it. The n and n cancels in that expression so I get  and 2 is the same then . 
           
This is just the simplification and it is exactly equal. Once again I am going to do the same. I am replacing T (n-2) with the value of that. 


What is T (n-2)? It would be T (n-2)   , that is what I am going to replace it for T (n-2). When I do that (n-1) gets cancelled and I get  T (n-3). This  multiplies with this 2 and gives . This part  is the same as . When I replace it I get this  and this quantity gets decreasing with every step and the sum inside keeps increasing with every step. Next time I will add , the next time I will add   and so on. Eventually it goes down and when T (n-4) goes down to (n-n), I get a zero. And in the denominator I would get (n-n-1) so it is 1 and I just get (n+1). Inside I will get all the terms going up to  +2. Since T (0) is 0 that part goes away and I get exactly the equation which is given below.
                
                   
So I get T (n) is less than or equal to the above sum.  This is   the sum of a harmonic series and it is going up all the way up to . 

(Refer Slide Time: 52:47)

 

We are summing a harmonic series, we have to sum . One trick that we employ very often, you take this graph and this is a graph of a function . These are the points 1, 2, 3, 4 and let me draw line at 2. 
What is the height of this rectangle?
It is , the width of this rectangle is 1 and its area is . I have drawn a line at 3, so its area is . I am getting the terms so next one is ,   and the last one at n, I will draw it in this manner.  




(Refer Slide Time: 53:27)

 

This sum is less than or equal to the area under the curve f (x) =   and it is atmost the area under the curve in between the limits 1 through n. Recall T (n), it is   that is harmonic sum +2. I am going to just put that 2 (n+1) and that harmonic sum was the integral of   from the limits 1 through n +2. This integral part is just log (x). It will be just log of n and 2 (n+1) ln n +2 is exactly the sum which is O (n log n). 
            T (n)   O (n log n)
It is the expected time for inserting the n keys. We have done a very sophisticated computation.

We looked at all the n! different permutations possible and also we computed the total time taken to insert all the n! different permutations and we took the average. We got n log n. Recall we saw the worst case was   and there is a particular permutation which puts the elements in increasing order for that the total time for insertion is . But the average time is n log n. 

The best time is also n log n and the best time would happen when the tree was very shallow. If you have a tree on n nodes and it has a height atleast log n, log n-1 or roughly . If you take a complete binary tree of height log n the last level has   keys that is in the leaf there are   leaves. How much time you will take to insert each one of those leaves?
Log n, because you will have to come down the entire tree. So to insert each one of those   keys you are taking  time. Just to insert the leaves you take ( )  time which is   (n ) time a function which is growing faster than n . It is some constant time but it is n . 

(Refer Slide Time: 55:55)

 

In the best case you are taking n log n time, in the worst case you are taking  time but the average case is also n log n.  That is a very interesting part. If I would ask you the average case, you would just take the average of the above 2 cases and report it. Why it is turning out to be O (n logn)? What does it convey? Most of the trees are fairly shallow and do not have too much depth. Most of the trees have depth of only about  log n or some constant times n log n. So that the average is still only n log n.  

You have a bunch of numbers, the numbers here are the total time for insertion under the various permutations. The smallest of those numbers is n log n, the largest of those numbers is  but the average is also n logn or some constant times n log n. That means most of the numbers are closer to the minimum than to the maximum.  










(Refer Slide Time: 56:20)

 

That is all I was going to discuss in the class today. What we discussed today was the deletion procedure and then we also discussed this particular analysis which says that if you were to take a random permutation of n elements and insert them, then the average time or the expected time for insertion is n log n. In the worst case there could be a permutation for which the time taken is . We have seen examples of those, if the elements are in increasing order or in decreasing order then we take  time. The best possible permutations are also going to take at least n log n times or some constant times n log n. But the average case is also n log n.  
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 10
           Quick Sort 

Today we are going to talk about quick sort. This is the second sorting algorithm we are discussing in this series of lectures. The first one was insertion sort for which we had argued a worst case running time of O (n square). Today the quick sort algorithm which we are going to look at is also going to have a worst case running time of O (n square) but we will argue that on an average it takes only about n log n time. It is a quick algorithm, in practice it is very fast with very small constants.  

(Refer Slide Time: 01:35)

 

Another property of this algorithm is that it is in ?place? sorting algorithm. What is an in ?place? sorting algorithm? An algorithm is called in ?place? if you do not require any additional memory to do the sorting. We will assume that n numbers are given in an array, may be we need memory for 1 or more variables but we will not need any additional memory to do the sorting. That is called the in place sorting and that is typically good thing to have. Because its space is at a premium, especially when you are trying to sort a large collection of numbers. 

This algorithm falls into a paradigm which is called divide and conquer. I am not going to say too much about this because we are going to see a lot of divide and conquer algorithms in this course. At a very high level, the idea behind divide and conquer is the following that you are given a certain problem that you want to solve. You divide the problem up into 2 or more pieces. You solve the problem for those smaller pieces.  

The divide step in the case of quick sort would be partition our array. We will take the array which stores the n numbers and partitioned it; break it up into two parts. One is we will call it as the lower part and the other we will call the higher part. The property of the lower and the higher part would be that every element in the lower part, every number in the lower part would be less than every number in the higher part. 

How does this help us if we do such a partition? If I sort the lower part now and I sort the higher part and I put all elements of the lower part first and follow it up with all elements of the higher part then the entire thing is sorted. I have sorted the lower part and I have sorted the higher part, every elements here is less than the element in the higher part. So the entire thing is sorted. That is the combining part and in this case it is trivial because nothing needs to be done. But quite often in divide and conquer algorithm you have to do something to do the combine and we will see examples of these later. So we will go one step at a time. We will first understand how this partition is done.   

(Refer Slide Time: 4:10)

 

So we will give an algorithm to do the partitioning which will be a linear time procedure. So the partitioning is done around a pivot element. I will take one of these elements as a pivot and everything which is smaller than the pivot will be my lower half that is lower part of the array. And everything which is larger than the pivot will be the larger part. This is the procedure to do the partitioning. It takes the parameters, the array A and p, r are the limits of the array. The p here refers to this location and r refers to this location and this could be part of a larger array (Refer Slide Time: 05:10). This just says that partition the sub array from p to r. 

We will see what it will do at the end of the procedure. I am going to take A [r] which would be the element 10. I will put it into x which means x is going to be my pivot element. So x would contain 10. What is this doing? The i is getting the value p-1 so I is assigned to something before p. So this location is r and this location is p. And i is getting p-1 and j is getting r+1 and so which means i and j are just before and after that is the start and the end of the sub array that we are interested in. And now we are just going to go through this lop. The while TRUE which means we just continue going through this loop, till you break out to the loop. What is this doing? This is saying repeat j is j-1 until A [j] is less than or equal to k. So we are looking at this index and we are saying keep decrementing it till I reach a location which is less than or equal to 10. What was x? The pivot 10. So keep decrementing it, so I decrement j till I reach a location which has content less than or equal to 10. So already I reached such a location so I stopped decrementing. 

(Refer Slide Time: 08:40)

 

Now I go to the next loop where am incrementing i till I reach a locations which is greater than or equal to 10, so already I reached a location which is greater than or equal to 10. What am I going to do? Recall that I want everything in the left part to be less than 10 and everything in the right part to be more than 10. So these are in some sense culprits because this is more than 10 and it is in the left part and this is less than or equal to 10 and it is in the right part. So we will like to swap them and that is what we will do here. Exchange A[i], A[j] means just swap these contents. So I swapped these contents. So I will denote blue as everything which is in the left part and orange would be everything which is in the right part. So everything in the left part would be less than or equal to 10 which was my pivot element and everything in the right part which is the orange part will be greater than or equal to 10.

Recall, i is here at this point and j is here and once again I am going to keep decrementing j till I find an element which is smaller than 10. So I found one already and I keep incrementing i till I find an element which is larger than 10. So that actually I will find here and once again I will swap these things. I swapped them and I continue with j, so j will keep moving till I find something which is smaller than 10. So actually I found immediately one which is smaller than 10 at this location and it will keep moving till I find an element which is larger than 10. So this is not larger so actually I will end up till this location 19 and I will once again swap 8 and 19. I swap 8 and 19 and I get this. This is already smaller so this is also marked blue (Refer Slide Time: 07:38- 08:30).  

Now j would continue decrementing till I find something which is smaller than 10. The j is searching for something which is smaller than 10. So it will come to this position, this is the first element which is smaller than 10 (Refer Slide Time: 08:45). So it comes here and i will continue incrementing till I find something which is larger than 10. So it comes here and now i and j have crossed each other, which means our job is done. So if i is less than j then you do an exchange, if i is more than j which means they crossed each other then you exit which means return the procedure. What we are returning? We are returning the value of j which tells us about the boundary of the two halves. 

So this is my left half that is my left half is from p to j and my right half is from j+1 to r because am only returning j and not i. So j+1 to r is my right half. How much time does this procedure takes? It is order n. Is this clear that its taking order n time and no more because [Hindi Conversation] at every step we are decrementing j. How many times can j be decremented? It is at most 10 times the size of the array. And how many times can i be incremented? At most the size of the array, so this loop is done at most n times, this loop is done utmost n times and every time we increment or decrement we might have to do one exchange in the worst case so this is also never be done more than n times (Refer Slide Time:10:16).   

(Refer Slide Time: 12:30)

 

It is a simple way. Why do we do it in this manner, you could also have done it in a slightly different manner. After all we are taking the pivot and saying, compare every element with the pivot and put everything which is smaller in the first few locations and everything which is larger in the next few locations. If you try to do it, some other way you will require more memory. One way you could do is take one element at a time and put it in some other array and then copy that array back into this place. That would be one way of doing it. But that will take more memory, so we wanted to do it in in-place as to keep all the elements in this array and not to take up any additional memory space. 

We can do this partitioning in in-place that is no additional memory and in linear time which is the best pursue. I am saying let us look at this repeat until loop over all the while loops. Let me just look at this one statement, the total number of times this is executed not in one iteration of the while loop but all while loops put together. How many times is this statement executed? At most n times because I can decrement it only at most n times. 
That is not possible, because for one particular run of the while loop I might decrement it say for 3 times. Then for the next one I might decrement it for another 5 times and so on. But the sum over all will not be more than n because [student Conversation] At most n, that is all I am interested in. The total number of times this step is executed and this step is executed and this is executed is order n.  

This is my complete quick sort algorithm, I did my partitioning so I have to do quick sort on lets say this array, array A between limits p and r. When I do an initial call for quick sort I will do 1 to length of A, whatever is the length of A. So in general this would be the thing, so p to r. I need to do it only if p is less than r, if p=r then there is nothing to be done or if p is more than r then again it does not make any sense. So if p is less than r then i do something. What do I do? I first find the partition of this part p to r then I invoke the previous procedure. A, p, r what does it do? It rearranges the part of the array between p and r such that everything which is less than the pivot is in the initial part of the sub array and everything which is more than the pivot is in the later part of the sub array. 

(Refer Slide Time: 16:23)

 

And what does it return? It returns the demarcating lines, so the lower half is going from p to q  and the larger half is going from q+1 to r. So I need to sort the lower half and the upper half separately. So I recursively invoke quick sort on the lower half which is going from p to q  and on the larger half which is going from q+1 to r. The while TRUE means just keep doing the while loop forever. Where does this while loop stop? It will stop when you return out of this, it?s like an exit out of this while loop, like a break statement.  

So when this condition is met that i is more than j then you will return j and go out of this entire partition procedure itself. So which means that you also go out of the while loop. So when you call quick sort recursively, note that I do not have to copy the array A, it is the same array that is used. We might have to create more copies of the variables, actually we would only be creating additional copies of these parameters that we are passing. But that is the space created on this stack. We are ignoring this but we are not taking any additional memory for the elements that we have. They are all sitting in a single array. What element should be partitioned around? What should be our pivot element?    

Did every one understand the quick sort? You took a pivot element and you partitioned the array around the pivot element and you said let me sort this left half and let me sort this right half and then I am done. Then how do you solve the left and right half? Repeatedly by the same procedure. Since we have a notion of a left half and a right half therefore we need to write a quick sort procedure in this manner, because this will specify the limits of the sub array that you are sorting p, r. Let us try and analyze how much time does quick sort takes? We have only seen that the partition procedure takes order n times that is linear time but we do not know how much time quick sort takes. So the time taken by quick sort will depend upon how the split is happening. What do I mean by that now? I should not say left half but it is left part. So how many elements end up in the left part and how many elements end up in the right part? That is what we should find out and that will determine how much time quick sort is taking. Let?s see why. 

(Refer Slide Time: 17:00)

 

So if at every point, suppose the following was happening. At every point we were actually dividing the array up in to two equal halves which means that I started of with n elements, this here [Hindi Conversation] Somehow or I was lucky or whatever it is, my pivot was such that it was the median lets say. Which means that half the elements were less and half were more. So half the elements ended up in the left half and half elements ended up in the right part. Then when I did a quick sort on this again I was lucky, again I picked a pivot such that it divided up the thing in to two parts. 

And when I did my quick sort on this again I was lucky it just divided up in to two equal parts and so on. Suppose I was lucky at every step, then how much time am I taking? Let?s see. How much time did I take to divide this array up into two parts that is n/2 and n/2? It is n times, the size of this array. We saw the partition procedure took order n time. How much time did I take to divide this array of size n/2 into two parts that is n/4 and n/4? The n/2 here and n/2 here so that it becomes n. Then to divide this up into two parts that is n/8 and n/8, I again took n/4 here n/4 here n/4 and n/4 and so that is also n. So in each level of this tree that I have drawn am taking order n time to do the partition. And how many levels are there in this tree of mine? It is log n because eventually you will end up with one and there would be log n such levels. So the total time taken is order n log n. 

Why have I written theta of n and not o of n? Can you also say that the partition will take at least omega n time or at least will take n time? Yes, because when did we say that we would stop? When our i and j would inter change, so i has to go at least till some part and j has to go, so the total number of times I will increase i or decrease j is at least 10. Total number of times I will increase i, I am not saying that number of times I will increase i is at least half, it is not n/2. May be I increased i only n/4 times but then it means I decreased j at least 3n/4 times. So the sum is at least 10. I need at least n times which mean I need utmost m times and I need at least n times, so the exact time is really some constant time, theta of n. So I can actually say equality. 

(Refer Slide Time: 20:31)

 
What happens in general, if you are not lucky? What is the worst case? n squared, when we partition [Hindi Conversation]. Once side gets one element and the other side gets n-1 elements. We could have such a situation, T (1) + T (n-1). Again I am writing the recurrence relation. Time to quick sort n elements equals the time to partition. That was the very first step of our quick sort procedure plus the second step of our quick sort procedure was quick sort the left part lets say the left part has one element in it, so T(1) plus the time to quick sort the right part which has lets say n-1 elements, it is T(n-1). 

So this is our recurrence and let us solve this recurrence in exactly the same manner that we did earlier. So I am assuming T (1) as zero. So this is just T (n-1) + theta of n. If you have one element there is nothing to be done, it is sorted. So this is just T(n-1) plus theta of n. What is T (n-1)? It is T (n-2) plus theta of (n-1). So T (n-2) plus theta of n (n-1) and T (n-2) is T (n-3) plus theta of (n-2) and so on. So it has essentially become theta of k where k going from one through n. What all I am saying is, this is then equal to T (n-2) plus theta of (n-1) plus theta of n, which is T (n-3) plus theta of (n-2) plus this term which is equal to T (n-4) plus theta of (n-3) plus this entire thing and so on (Refer Slide Time: 23:54). So this is what you get which is basically theta of n squared. It is n squared, k going from one through n, sum of k is just n squared. That is why you get theta of n squared.
 
If this theta is bothering you, just replace it by c, some constant times n. So this would be the worst case. So the best case is when you do a half split and the worst case is when you know it is a skewed split. So this is what the worst case look like pictorially, n divided into 1 and n-1, n-1 divided into 1 and n-2, n-2 divided into 1 and n-3 and so on. And what will be the height of this tree now? It is n. In each step you are taking time n and again you are taking time n, n-1, n-2 and so on. All the way down to 1 so that makes it n squared. 

(Refer Slide Time: 23:46) 

 

When does the worst case appear? Suppose we were doing a following scheme, we were saying let me take the last element as the pivot always. That is what we did to begin with, let me take the last element as the pivot. So if my input is sorted already lets say in increasing order. Then I took the last element as the pivot. How many elements will be in my lower part? The n-1 elements in my lower part and only one element in my upper part because there is no element larger than the last element. And then once again to sort the lower part I took the last element as the pivot. So once again it will get divided in this manner.

(Refer Slide Time: 24:16)

 

So the worst case would happen when the input is already sorted in ascending order or in descending order. Even in descending order you will take n squared time. Because when it is in descending order you took the last element as the pivot, it is the smallest element [Hindi Conversation] there will be one element, right half will have all the n-1 elements in it. And you keep doing this and this is what will happen. Similar kind of a thing happened in insertion sort. In insertion sort we said the worst case would happen when it is in descending order. Because if you recall in insertion sort we were taking an element and figuring out the best place to put that element. And we would go from the end to find the best place. So if it is sorted in decreasing order then the best place is always the front of the array. So you will have to go all the way to the front of the array at every step. It is again 1+2+3+4 and so on all the way up to n. So you will get again the same n squared.  








(Refer Slide Time: 26:35)

 

But in insertion sort if the array was sorted in increasing order then how much time do you take? Then it is the best case because you do not have to move back anymore. Every element is in the right place, so it just takes constant amount of time. That is the comparison with insertion sort. But here both, whether it?s sorted increasing or sorted decreasing you might end up with something like n squared time. So worst case seems to occur more often. 

Let us continue with this analysis. We saw [Hindi Conversation] if the split was half and half at every step, then we are lucky and we get n log n time. Suppose it was not half and half but it was one tenth, nine tenth that is 10 percent of the elements end up on one side and 90 % of the elements change out. Suppose this was happening at every stage. You will not call this lucky because it?s not half and half but it?s still good which we will argue now. At the first stage this is what happens, n/10 elements on one side and 9n/10 elements on the other side. How much time did I take to do this partition? n time, then this n/10 got divided into one tenth on one side and nine tenth on the other side. One tenth means n/100 on one side and nine tenth of this is 9n/100 on the other side. And this (9/10) n also gets divided into 9n/10 on one side and nine tenths of this guy which is 81/100n on this side. How much time did it take to partition this into this and this (Refer Slide Time: 27:45) n/10 number of elements and so for this the total time is still n. 









(Refer Slide Time: 29:56)

 

So similarly the total time for every level will continue to be n. But now how many levels do I have? Let?s figure out that. Let?s just look at the largest number we have, as we go down one level at a time. We want all the numbers go down to one, so the largest number is the one which we have to say it goes down to a one. So at this route we have n, the largest number at this step will be 9n/10 because the other number is smaller. What will be the largest number at this step? It will be in the nine tenths of this guy which will be the largest one, so that will be 81/100n. 

The largest at the next step would be the nine tenths of this guy [Hindi Conversation] that will be the largest guy at this step which will be this and so on. So the largest number at every level is decreasing by a factor of 9/10. How many times can I decrease before it gets down to one? At most log of n to the base 10/9 because it is decrementing by a factor of 9, 10?s at every step. When it was decrementing by half at every step, we were saying log n base two. So if we just work out the math it will be log n to the base 10/9 which is order log n. It is just some constant, it is different constant times log n. So once again the height is order log n, so the total time taken is order n log n. Because at each level we are taking a total time of n and number of levels is log n. 
 
But even better, this is we are only providing an upper bound. Even if this split was in this strange manner one tenth and nine tenth or any constant fractions, there is nothing sacrosanct about one tenth and nine tenth. I could have said 36/37, even that is ok, it is not a problem. Even then we will be able to argue log n. There is nothing spectacular or special about 1/10. The important thing is we are saying a constant fraction of numbers go on one side. We cannot afford to say only one number go on one side that becomes bad for this. When only one number go on one side or only two numbers go on one side then we will end up taking n squared time. But if we say a constant fraction one tenth or one hundredth or one thousandth or one millionth the height would still be log n. 

We will do a formal analysis starting from the next slide, but just to give you a little more motivation on this. Suppose we alternate the lucky and unlucky cases, even then you can prove a log n depth. So what was our unlucky case? 1 and n-1, then what was the lucky case? n-1 divided into two, n-1/2 and n-1/2. How many operations did I take here to split n and how many operations did I take here to split n-1? So in total I took 2n-1 operation here and after 2n-1 operations or comparisons I managed to split it into n-1/2 and n-1/2 and 1 with 2n-1.  

(Refer Slide Time: 32:51)

 

So now I can again bring the same tree. I will get a recurrence like this. I can think of it as this with theta n or in particular 2n comparisons I managed to split it into n-1/2 on one side and lets say n-1/2+1 on the other side. Actually I managed a three way split because the 1 is coming here. So once again we will have a depth of only log n and I took 2n in each step, so its 2n times log n. We will not worry too much about this lets try and do this formally. So it seems that for many scenarios we will get a log n. We want to argue that this really is the case? 













(Refer Slide Time: 35:14)

 

So what is the best thing to do? We said we will be lucky when we always partition half and half. The best thing to do would be to find a median element. Pick the median element as the pivot and that will break up my array into two equal parts and that would be great. How do I find a median element? Sort the numbers and then find the median element. That would be one strategy except that sorting is what we are trying to do in that first place. So finding median element is not straight forward, you will see a procedure for doing it in your next algorithms course. [Student Conversation: not clear-some what close to the median by dividing the array into some odd number of small array then sorting these, as in like write it into small arrays of size five and sort them and then take the median of these. Now I repeat this till I get one number, that it is close to the median and we will take log n steps.] You can actually compute a median element in linear time but it is fairly an involved procedure which you will learn later. 

For now what you will do is, since you want to find the median element and you cannot find the median element. So you will just pick a random element and declare that. What we are going to do is we are just going to pick a random element as our pivot. We said we do not want to pick a specific element as the pivot. Why? Because if I always want to pick the last element as the pivot then if my sequence is in decreasing order or increasing order then I will struck with an n square running time. So I will just pick a random element as the pivot. So this is what we call a randomized algorithm. 

What is a randomized algorithm? An algorithm which is basically making some kind of random choices and we will analyze what this algorithm does. We will analyze the running time of this algorithm. So this is what a randomized quick sort is. We will assume all elements are distinct. We partition around a random element. A pivot is a random element and we just pick any element at random with the same probability. So what kind of splits we can get? We can get all kinds of different splits, if I have n elements I can get a split of 1n n-1; I can get a split of 2n n-2 and so on. What will be the probability of these splits? They all will be equal. They will all be with probability one over n. Did you understand why they would all be equal? Because it is a random element, each element can be picked with equal probability. 

So if I pick the f five n elements and I pick the tenth smallest element then I will get a 9 versus n-10 split or a 10 versus n-10 split. But what is the probability of picking the tenth smallest element? It is 1/n, because I could have picked any element. So I do not know what I am picking. The tenth smallest element is as likely as the 11th smallest element which is as likely as the 12th smallest element. The probability of each one of them is the same. We will see more examples of randomization in this course and it is a useful tool for designing algorithms.

(Refer Slide Time: 36:44)

 

So this is what our randomized quick sort is going to look like. So we are only going to modify the partitioned procedure and call it randomized partition instead. Once again we are trying to partition the array A between p and r, the sub array between locations p and r. What is random p, r? Random p, r generates a random number between p and r. Lets say that number is i, between p and r means including p and r. Lets say that number is i and we are just going to exchange i and the last element. Why am I doing this? Because this partition procedure, if you recall was taking the last element as the pivot. Now I want to put my pivot at the last location, so I just exchange the pivot element with the last location and then I call my partition procedure, the same partition procedure as before. So this becomes my randomized partition procedure.

And what does randomized quick sort do now? Instead of calling partition it calls randomized partition, the rest is the same. Did you understand what it is? Since our choice of pivot is crucial and we do not know really how to choose the pivot. We just pick a random element as the pivot. No, it would not be a random choice. So what is the difference between random choices here? So he has a very good question, he says if I partition it around the last element, why is that not a random choice? That is not a random choice because if I give you a specific input which is lets say increasing order then on that input you are going to take n square time. If you partition around the last element.
 
Now in randomized quick sort we are not partitioning around the last element. We are picking a random element to partition. When I pick a random element to partition then given the same sorted sequence as input, you are not going to take n squared time. In fact how much time are you going to take? We do not know how much time we are going to take. That is the interesting thing. Why we do not know how much time we are going to take? We do not know what the pivots are going to be, they are randomly selected. 

(Refer Slide Time: 41:00)

 

So it might happen that today you run the algorithm and it take some time and tomorrow you run the same algorithm and it takes a different time. Because it depends upon what random numbers are selected and those random numbers selected decide the pivot and the pivot as you see is crucial in deciding how much time we are taking. Because if that pivot was a nice one which was splitting the things evenly and if it is going to take less time and if the pivots were turning out to be bad ones fairly skewed, then I am going to take more time. So we are going to see all of that. So now if you do not know how much time the algorithm is going to take and it is going to take some time today and some time tomorrow, then what do we analyze? What is that we can say? The average time or expected, which is also called the expected time. 

What is the average we are doing over?  So are we averaging over all possible inputs? Sequence of random numbers is generated in some sense. So the way to think of it is, fix an input. You are not going to change the input; we will run the algorithm today and tomorrow and day after and so on till the end of this course. And then you are going to compute the time and take the average and that will be the expected time for sorting that specific input sequence.  
(Refer Slide Time: 41:35)

 

Let T (n) denote the expected number of comparisons required by quick sort. T (n) is a function of n, the number of comparisons required to sort n numbers will depend upon how many numbers you have depend upon it. Let us recall what quick sort does. Quick sort first partitions, if it has to partition n numbers no matter what the pivot is. It is always going to require the same number of comparisons. It is always going to require no more than n-1 comparisons. You can think of the partition process as every number is compared against the pivot and then all those that are less than the pivot are put on one side and all those that are more than pivot are put on the other side. So we always require n-1 comparisons. 

Every number has to be compared against the pivot otherwise we will not be able to decide whether it goes on the left side or on the right side. So this is the part of the partition. Depending upon what the pivot was, if the pivot was the ith smallest element then on one side how many elements am I going to get? Let?s say i-1 on one side and n-i on the other side and the ith element lets say I leave it at the right place. I have to quick sort those i-1 elements, I have to quick sort those n-i elements. How much expected time am I taking to quick sort those i-1 elements? T (i-1) is the expected time I take to quick sort these and this is the expected time I take to quick sort the n-i elements (Refer Slide time:42:53) So I will take this much amount of time in all. But what is i here? The i was the fact that the pivot was the ith smallest element that happens with the probability of one over n. So I am going to take this much time with the probability of one over n. 

I am going to take time T (7-1) + T (n-7) + n-1 with the probability of one over n. I am going to take time T (13-1) + T (n-13) + n-1 again with the probability of one over n and so on. So the expected time taken is basically this sum (Refer Slide Time: 43:45) this quantity summed over all choices of, so I have replaced i by the j here. So this i is the same as this j here and each one of them is being picked with the probability of one over n. This is how you compute expectation. Expectations, so for instance just to give you an examples those who are forgetting your expectations, I roll a dice. What is the expected value I see? How does one compute this? Each of the outcomes is equally likely, each occurs with the probability of one over six. I see one over one with the probability of one over six, two with the probability of one over six, three with the probability of one over six and all of that. So the expectation is one over six times one plus one over six times two plus one over six times three and so on. Whatever value we get that is the expected value you would see. What should I repeat?

(Refer Slide Time: 48:12)

 

Throw a dice so what are the possible values we get? It is 1, 2, 3, 4, 5, and 6. It is an unloaded dice, so each appears with the same probability. What is the expected value?  What is the random variable? Random variable is the number that comes, so the random variable is the number on the dice. This random variable lets call it x, this random variables takes six different values. Each value takes the probability of one over six. Probability x = i = 1/6, i between 1 and 6. What is the expectation? Think of the expectation as just the following. You keep throwing this dice and keep recording the outcome. Keep doing this forever and then just take the average. Suppose you threw this dice one billion times. How many times are you going to see a one? One billion by six times. This is what the probability means, the probability that it is 1/6.  

If you do the experiment sufficiently many times then this fraction of the times you are going to see this particular outcome. [Hindi Conversation] times your outcome was one. If I am looking at the sum of the outcomes this will be one sum. How many times do you see a two? How many times do you see a three and so on? So let me just complete this which is exactly the same as saying one by six into one, this quantity here. 

What is this quantity? (Refer Slide Time: 47:32) This is just the probability of the event that x i equals one. So this is 1/6 or I could even have written it as probability, the random variable takes the value one times this one which is the value of the random variable plus the probability 10 power 9/6 is the probability that it takes the random variable. The 10 power 9/6 by 10 power 9 is just the probability that it takes the value two. So probability x = 2 into 2 and so on. And probability x =6 into 6. So that is what expectation is. One way of thinking of expectation is, if a certain random variable is taking a set of discrete values then you just compute the value of the probability with which it takes the value, times the value summed over all the possible choices is the expectation of the random variable.  

So let?s revert to our slides. This is going to be taking this value with the probability of one over n. And so we have done just the probability times the value summed over all the possible choices which is j varying from one through n. And again as j varies from one through n, this quantity varies from T (0) to T (n-1) and this quantity also varies from T (n-1) to T (0) which means every term T (0) through T (n-1) appears twice. So I can write this part of the sum as this and this n-1, we are summing it n times and then dividing it by n and so it?s just plus n-1 separately. And this is a recurrence which we saw in the last class. It was the recurrence for the expected number of comparisons required to insert a randomly chosen permutation of n elements in a binary search tree. 

(Refer Slide Time: 49:45)

 

And what did we prove in the last class, we solved this recurrence and we showed the solution is n log n. Hence the expected number of comparisons required by randomized quick sort is n log n, the same recurrence. We solved it before and we are just using that fact. This is the analysis which we did and this is the expected number of comparisons required by quick sort. So let?s quickly summarize the time taken by quick sort. Worst case time we said was n squared, the best case time was n log n. We did not prove it formally? Why it should be n log n and why it can not be less, but intuitively you can understand that the best thing happens when the two are roughly equal and then we argued that it will take n log n. And the expected is once again n log n and this behavior is similar to what we also in the last class.  
(Refer Slide Time: 50:41)

 

When I am trying to insert n elements into a binary search tree, what is the worst case time? It is n squared, it happens when my sequence is sorted. What is the best case time? It is n log n again and if the first element I was inserting was roughly the median and the second element I was inserting was n/4 and so on. And the expected time was n log n. The crucial difference between what we are doing today and what we did in the last class.  
This is something that I have already said but let?s just recap. What we have done today is that the running time of quick sort depends upon the numbers that are getting generated. If the same random numbers were generated then the running time would be the same for a given input.

(Refer Slide Time: 51:19)

 
So the same thing I said, I fix an input the running time could be some value today, it could be some value tomorrow because the random numbers generated could be different. What we have done is we have taken expectations over these different random numbers that have been generated, by saying that today we will compute what the value is, tomorrow we will compute what the value is so on and take the average. We are doing this for one fixed input and we said if I fix an input and I compute this value it turns out to be n log n. That is what we did. 

And no way we used the fact that, what the input was really. Come to think of it, no matter what the input is, your expected time is turning out to be n log n. So some how this is actually what some of the slides were also saying before. When we said we would take a specific element as the pivot lets say the last element as the pivot. We saw that the running time would depend upon what input was given to us. If the input was sorted we would take n squared time, but if the input was such that the last element was the median element then once again we would get this half and half split and so on.  

So if I had taken a specific element as the pivot always then my running time would depend upon what the input sequence was or what the input order was. But I got around that somehow. I said let me randomly pick my pivot element and now what has happened? The running time does not really depend upon my input is, it depends upon what the random number choice is, which in expectation will give me a running time of only n log n. [Student Conversation: So how does that make things if better we do not make any fix element.????] the algorithm time is independent upon the input. How does, no. So what we are trying to say here is that when we say it is independent upon the input, we are saying that no matter what input the adversary gives us. You are trying to may be beat my algorithm.   

(Refer Slide Time: 56:27)

 

You want my algorithm to take as much time as possible. So the point here is that you cannot come up with any such sequence. No matter what sequence you come up, what sequence of numbers you come up, my algorithm will take a time which in expectation is n log n, which quit often will turn out to be n log n. [Student Conversation: I want to work somewhere. So there nobody is going to?We have to make an algorithm Particular kind of input may be if you look at this quantity that you want to make it independent of the input, then this kind we are making a random for that kind of input] 

If we knew what kinds of inputs we were getting then perhaps it makes sense to design the algorithm for those kinds of inputs. But this is not what we are doing here because we are not designing the algorithm for a specific input sequence or specific kinds of inputs. We are saying we want to be able to consider all possible inputs and in doing that we want an algorithm which really does not depend upon what the input is, its behavior is independent of that. What did we do in the last class for binary search tree? And I want to make this difference very clear. For the binary search tree we were doing an average over what? We said take a particular input sequence and it is going to take a specific amount of time, whether you run it today or you run it tomorrow its going to take the same amount of time. That was not a randomized algorithm it was a very specific algorithm were no random choices being made. If you take the same amount of time no matter when you run it, but if I take a different input sequence it would take a different amount of time and if I take a third input sequence it would take a third different amount of time and so on. 

(Refer Slide Time: 57:08)

 

And there what we were doing was, lets take the average over all input sequences that is over all possible n factorial different permutations. We took the average over all of them and then we got n log n. Did you understand the difference? The recurrence is the same but there is a vast difference between these two things that we have done. One was what you call an average case analysis; we looked at all possible inputs that can be there of numbers one through n, there are n factorial different permutations. So there are n factorial different inputs possible, we looked at all of them; we computed the time taken by the algorithm for each one of those inputs and took the average. Today on the other hand ours was a randomized algorithm, our algorithm was taking different times depending upon what the random numbers were. And today we were taking the average over the random numbers that were getting generated and not over the inputs, the input was fixed. With that I am going to end today?s lecture. We saw quick sort and we did the expected time analysis for randomized quick sort. 


Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 11
Avl trees

In this class we are going to talk about AVL trees. In the last class we have seen binary search tree data structure. One problem with the binary search tree if you recall is that the operations of insertion, deletion and search take time proportional to the height of the tree. Height of the tree can be very bad. We saw an example were the height of the tree could be as bad as order n or n-1.  We want to some how create a tree which does not have too bad a height. That is what we are going to do today. We are going to look at this data structure called AVL trees. What is an AVL tree? AVL trees are also called height balanced trees. Ignore the white spots that are showed on the slide below and they should not have shown here.  

(Refer Slide Time: 01:58)

 

This is the binary search tree and inside the nodes are the keys. Everything which is less than the root is to the left of the root and everything which is more than the root is to the right of it. The thing that is written next to each node is the height of a node. What we will call the height of a node? We have not defined this term yet. We will just say the height of a node is the height of the sub tree rooted at that node. 

For instance if I look at this node (78), all the things which is below it is the sub tree rooted at this node. What is the sub tree rooted at a node? It is just the set of descendents. I am looking at the tree which is on the right and in previous classes we have defined the height of such a tree as 2 and not 3. Because we had said that 78 is at level zero and 50 is at level 1 and 48 is at level 2 and so we called the height of the tree as 2. We will just modify, we will say that if it is the singleton node just one node then it is of height one instead of height zero as we have been calling it. So level numbers are beginning with 1. This sub tree 50 has height 2 and this sub tree 78 has height 3 and this entire tree has height 4. We are going to call this as height of the tree for the purpose of the AVL tree. 

With every node I have put down the height of that node. What is the height of the node? It is just the height of the sub tree rooted at that node. All the leaves will have height 1, the parents of the leaves will have height 2 and so on. Such a tree is called AVL tree if it is height balanced. What is height balanced? If I look at any node and its children then the difference in their height is at most one. There might be no difference in their heights, as in the case with the 50th node. Its 2 children have the same height. The node 78 has the difference, the left sub tree has more height than the right sub tree. The left sub tree has height 2 and the right sub tree has height 1. The node 44 also has a difference of one. The right sub tree has height 3 and the left sub tree has height 2. But the difference is no more than a one. This is the AVL tree. This is what our definition of an AVL tree would be. It is true for every node of the tree. The binary search tree has 2 properties. It has to be a binary search tree and for every internal node of the tree, the heights of the children differ by at most one. Why have I said internal? For a leaf node it has no children. It does not make any difference to talk about the height of the tree. So for this node 17 the right sub tree has height one. The left sub tree is missing so we call it height zero. Now you understand why I had made this change. If the tree is absent then I will denote the height as zero. And the single node will become height one. That is why I have to shift the definition a little bit. 

(Refer Slide Time: 6:04)

 

Let us see what is not an AVL tree? So recall that one of our binary tree which was very bad, which had a huge height was a tree like this. This is a binary search tree and I put some keys so that it looks like a binary search tree. This has height equal to n-1, if there were n nodes. Is this an AVL tree? No. Is the last node height balanced? Yes, since it is a leaf node it is height balanced. Is the next node height balanced? Yes, it is also height balanced. Is the node following the 2nd node height balanced? No because the right sub tree has height 2 and the left sub tree has height zero. Thus the height balanced property is violated here. It is also violated in the following nodes. Thus we will never have such kind of trees as AVL trees.

(Refer Slide Time: 07:22)

 

Since we said that we are not going to have such a kind of trees as AVL trees, let us try and figure out how bad the height of an AVL trees can be. Let say I have an AVL tree of n nodes, if its height can still be as bad as n-1 I have not gained anything. I would like to say that its height is no more than log n or something. We will figure that out and that is what we are going to prove in the next few minutes. The height of an AVL tree t which has n nodes in it is only order log n. Let see why this is true. I am not going to prove this claim directly, I am going to make a slightly different argument. Let us take an AVL tree of height h. Amongst all possible AVL trees of height h, let me see the one which has the smallest number of nodes. I defined this quantity n (h) as the minimum number of nodes in an AVL tree of height h. Let us figure out the quantity and then we will see how this implies the proposition.  

Given an AVL tree of height h, we want to find out what is the smallest number of nodes it has. Can it have only h nodes? Then we will be in trouble. We want to say it has many nodes, if you recall a binary search tree of height h can have only h+1 nodes like the example that I showed you. But a good tree which is like a complete binary tree of height h will have  nodes. What we would really like is that our AVL tree which was of height h has large number of nodes, not just h but more like  or something like that. That is what we are going to prove.

 






(Refer Slide Time: 11:31)

 

Let us understand the quantity. It is the minimum number of nodes in an AVL tree of height h. What is an AVL tree of height 1? It is just a singleton node and nothing else. It has only 1 node in it. If I have an AVL tree of height 2 then it has root and 1 node. But it can also be root and 2 children. Why have I written n (2) = 2 and not 3 because I am counting the minimum. That is why n (2) =2, the minimum number of nodes will be just 2 in an AVL tree of height 2. 

Suppose if I have an AVL tree of height 3 or more, it will contain 1 root node. Suppose if I have an AVL tree of height h, it will contain 1 root node and an AVL tree of height h-1 on one side and an AVL tree of height h-2 on the other side. Why h-1 and h-2? It has height h so its children can have height only h-1 and not more than h-1. They can have a difference of at most one. If one of them is h-1 the other one can only be h-2 or h-1. One of the sub tree has height h-1 and the other sub tree has height h-1 or h-2. But what will we pick? We would like that the other sub tree should have height h-2. Why? Because of minimum number of nodes. A tree which has smaller height will also have smaller number of nodes, so we would like that the height of the other sub tree to be h-2. 

If n (h) was the number of nodes in the tree of height h, then what is the number of nodes n (h) equal to? It is the number of nodes in a tree of height h-1 the smallest possible, because the left sub tree which is of height h-1 can have as small as little number of nodes as possible and in the right sub tree which is of height h-2 also has little number of nodes as possible. The number of nodes in the left sub tree is n (h-1), the number of nodes in the right sub tree is n (h-2). There is one root node and the recurrence relationship would look like this (n (h) = 1+n (h-1) + n (h-2)). Once again we are seeing the recurrence relation. This is what we have to solve today. What are the base conditions? We know n (1) is 1 and n (2) is 2. With that you can figure out what n (3) would be? n (3) would be 1+1+2 which is 4 and so on. But we would like a close form expression to do this. So we will solve this recurrence. 

We are not going to be solving this recurrence exactly. We are going to do it approximately. First we use the fact that n (h-1) is only going to be larger than n (h-2). Because as the height of the tree grows the number of nodes cannot reduce, it will only be more. So n (h-1) is at least as large as n (h-2). Then this implies what we had written earlier that is n (h) = n (h-1) + n (h-2) +1. This quantity is at least as large as 2n (h-2). Strictly larger because I also dropped the one. I have replaced this n (h-1) by n (h-2) and this (2n (h-2)) is what I get.
 
                   n (h) = n (h-1) + n (h-2) +1 > 2n (h-2)

This becomes the simple thing to solve, n (h) is more than 2n (h-2). This is what I will solve. So n (h) is more than 2n (h-2) and now n (h-2) is more than two times 2n (h-4). This implies the entire thing n (h) is more than 4n (h-4). Which implies that the entire thing is more than 8n (h-6). You understand how this comes n (h-4) is more than 2n (h-6) and so on, which will eventually take us to something like  after i steps n (h-2i). Suppose I pick , I am going to assume that this quantity is an integer. Let us assume that h was even to begin with, so this is an integer and for this value I will get n (h) > in which I replaced . Recall n (2) was 2, so it becomes . What does this say? We just argued that if your AVL tree has height h then it has at least  nodes. That is at least so many nodes. What is the maximum number of nodes it can have? Something like , one of those because it can be a complete binary tree. 

(Refer Slide Time: 16:34)

 



Suppose I were to take logarithms, what would I get? I would get h < 2log n. So n (h) is actually less than n because I have an AVL tree whose height is h and it has n nodes. Suppose I had an AVL tree of height h and n nodes then it will also satisfy this relation (h < 2log n (h)). It will satisfy the relation because n is only going to be larger than n (h). What was n (h)? n (h) was the minimum possible number of nodes. Any AVL tree on n nodes has height at most 2log n from this argument. 

The h < 2log n (h) is what we argued after taking algorithms. Let me take a tree of height h and n nodes. So n is going to be larger than n (h) because n (h) is the minimum number of nodes that are possible in a tree of height h, n (h) is that quantity. This n is just a function, do not confuse this n with the number of nodes. You can replace this n with something else. n (h) is the minimum number of nodes in a tree of height h. What we argued was that h < 2log n (h). Take an AVL tree of height h and m nodes. Its height is h and it has m nodes in it. What does this implies? The m > n (h), this follows from our definition of n (h). We know that h < 2log n (h) which is then < 2log m. This implies h = O (log m). 

(Refer Slide Time: 18:47)

 

The h is the height and m is the number of number of nodes. The height of an AVL tree on m nodes is less than two times log of the number of nodes. That is what being said here. We have shown that such a tree will have height no more than 2log n. The best possible tree could have height only log n if it were like a complete binary tree, very dense and every thing. But this has more height but not too much, just a factor of two more. Much better than having a height of an n. Let us try and solve this recurrence slightly better. This is more of an exercise also to show you how recurrences are solved. We did fairly crude analysis, we replaced n (h-1) with n (h-2) and then we did the steps and got the result. Let us try and get something better. It is just an exercise. 



(Refer Slide Time: 21:19)

 

We will show how to get a sharper bound on the height of an AVL tree. The bound we obtained is 2log n. Let see if we can get something better than that. We are going to use induction and we are going to do a tighter analysis of the same thing. We are going to show that the minimum number of nodes in an AVL tree of height h which was n (h) is at least c times h that is where c will be some constant more than one. What did we show in the previous slide? The n (h) was at least . What was the c?  It was , there we showed a c of . Let see if I can get a higher c that is a larger c more than . What would be the way of doing such a thing? We will assume that n (h) is at least as large as . 
 
We are going to prove this by induction. We will figure out what c is later. We are proving a certain statement without actually knowing exactly what the statement is because I am not telling what c is. But you will see what the c has to be for the statement to be true. What is the base case? h=1, say n (h) is 1. This statement n (h) >=  is true at h=1. We have said the number of nodes is going to be at least as large as. I assume that I made a mistake, let us come back this base case again. We will have to perhaps redefine the height of a tree. I think we should have  or some thing. Suppose the claim is true for all h < k and lets try and prove it for h =k. We have to prove that n (k) >= . We will come back to this base case in a minute. So recall this n (k) = n (k-1) + n (k-2) + 1 was our recurrence relation. Our induction hypothesis says that n (k-1) is at least , n (k-2) is at least and I have ignored this plus one. Actually I can say that this is strictly larger. 






(Refer Slide Time: 25:16)

 

I can show that n (k) is larger than if I can show this quantity ( ) is larger than . This is what I have to show  is lager than . What should be the value of c so that this ( >= ) is true. I just cancel out the terms appropriately and I get . If c satisfy this ( ) then this ( >= ) will also be true. Why because I just multiply both the side by  and I would get exactly that. If this ( >= ) is true then n (k) which is larger than this ( ) would also be larger than . I just have to pick c which will satisfy this ( ). You all know how to figure out c which will satisfy this.  

We will just solve this quadratic equation   and this has roots . This  is negative, so anything in between  would keep this less than zero. But I want as large as c as possible, so I will take  which is roughly 1.63. This quantity is also known as the golden ratio. Perhaps we will see this more often. This n (k) = n (k-1) + n (k-2) + 1 is not a fibonacci relation. If you add one to both sides, so n (k) would be with the fibonacci number minus one. You can also do that. We get a bound of roughly 1.63 that is c as 1.63. What is the mistake we have made? One thing is base case have not worked out. I guess this was the wrong thing to pick. (Refer Slide Time: 26:28) It should not be  but may be . So induction hypothesis should be . 




Let us take , it will not make a difference. We take the  so precisely I am dividing out by c then the base would have also be satisfied. If h=1, you would have more than one which is the case. And sorry about the base case, for the other two also it will be okay. Because for h=2, n (2) is 2 and this would become which is c. The c is less than 2 because we just argued it is 1.63. So please make that correction, we really require that the induction hypothesis is h-1. It will not make any difference on this (n (k) >= ) how ever. If this (n (h) >= ) become h-1 then this (n (k) >= ) will become k-1.

(Refer Slide Time: 27:05)

 

(Refer Slide Time: 28:05) 

 

This ( ) would continue as it is. This will become . We have to prove this ( ) is greater than or equal to , every where there will be a minus one. So that you will still get the same ( ) quadratic in equality. The value of the c would still turn out to be the same. That is  for n (h) please make that small correction. Thus the AVL tree on n nodes has height atmost  . We just do the same argument as before. I take a tree of height h and n nodes. We have just seen that , this is the tree with smallest possible number of nodes. So n is going to be only larger than this that is . 

(Refer Slide Time: 29:44)

 

Let us take log on both sides, we get  =h-1. I am just using the definition of log. I am taking , so I will get h-1. This implies h=  +1. We are able to prove this kind of a sharper bound. This equation also works for n=2, so that was our base case. Let us continue. I have shown you the 2 ways of solving this same recurrence. One was the much simpler way, actually both are very simple. The 2nd technique is also used quite often. You make a guess on what you think the right value should be. Then essentially you verify that. We said that suppose the right value is some  and then you figure out what your c should be. You can get something better, earlier we had  that is 1.414 and we could update to 1.63 by using this kind of a technique. 








(Refer Slide Time: 31:20)

 

Let us look at the structure of an AVL tree in detail. Once again I have an AVL tree on n nodes. Let me take the leaf of this tree which is closest to the root, which means whose level number is the smallest among all the leaves. Suppose this leaf is at level k. We can show that the height of the tree is at most 2k-1. This requires the proof and let us do that. I have an AVL tree which has n nodes in it, although the number of nodes in the tree is not going to be particularly important. This is some tree, I took that leaf of the tree which is closest to the root. Suppose the red dot is the leaf which is closest to the root. We said that it is at level k. So the other leaves could be at this level or could be below. 

In this class for AVL tree we work with level starting with one. It does not make a big difference, let us say we start with level one. We are going to prove that the height of this tree is at most 2k-1. So the height of this tree is  that is what we will prove. Let see why. I will draw this picture again. This is the leaf which I have colored red is at level k and it is the one which is the closest to the root. From the node which is next to k, there will be some sub tree hanging out. From the next node also there will be some sub tree hanging out and so on. 

The first node is my root at level 1. Let us look at this node which is at level k-1. What is the height of this node at level k-1? It has one child and this child has height one. The heights are in blue. This means this sub tree at level k-1 can have height at most 2. We want to get as larger height as possible for this tree. Whenever we say at most 2 will just take the largest value. This can have height 2, if this has height 2 then what is the height of this node? This sub tree will have height 3. If this sub tree has height 3, what is the largest height that the next sub tree can have? It can have 4. What is the height of this node? It is 5. 





(Refer Slide Time: 35:45)

 

What is the maximum height this sub tree can have? It is 6 and this node would be 7, 9 and so on. What will be the height of the root? In general given that this was k, just figure it out, it should be 2k-1. If it was just till the node 3 then it is basically k=2 height=3. If k=2 then the height =3, if k=3 then the height was 5. If k was 4 then height was 7 and so on. For arbitrary k this is 2k-1. It is a very simple argument which means that this entire tree can be no taller than 2k-1, if the closest leaf was at level k. This is the property of AVL tree and not a property of any arbitrary binary tree. 

(Refer Slide Time: 37:03) 

 

In an arbitrary binary tree you might have leaves at any level.  But the height of the tree could be as bad as you wanted. Here is a leaf at level 1, (hindi). (Refer Slide Time: 36:30) But for an AVL tree if there is a leaf at level k then the height of the tree can not be more than 2k. So in any AVL tree basically all our leaves will be in the shaded part of the above slide. This band whose width is as large as this roughly and both of them was k so I am ignoring that. I will just come back to this in a minute. We just argued that if the closest leaf is at the level k then the height of the tree is no more than 2k-1. (Refer Slide Time: 37:35)That is the largest possible height the tree can have.

Let us make another claim. If the closest leaf is at level k then all nodes at level 1 through k-2 have 2 children. Every node on these 1st k-2 levels should have 2 children. Why have I said k-2 and not k-1? Let us prove this by contradiction. What do we want to do contradict? Let us take some node at level k-2 which has only 1 child. The picture is given in the below slide. I have a node u at level k-2, it has only 1 child which is at level k-1. I have shown a node at level k-2 but the same argument would apply to any node at 1 through k-2. So v is at level k-1, it cannot be a leaf because our closest leaf was at level k.

(Refer Slide Time: 39:13) 

 

So it has to have another child. I have shown only 1 but it can also have 2 children. But this u has only 1 child. So sub tree rooted at v has height at least 2 because this should have 1 child, it cannot be a leaf. It has height at least 2 while the right sub tree here has height zero because there is nothing there. So we have a height imbalance at this node u. The height balance property is violated at u. Every node on these levels 1 through k-2 should have 2 children. At level k-1 how ever there can be nodes with only 1 child. 





This is level k, of course the tree extends. The dot on the left side is the level at which the closest leaf is situated. At level k-1, I can have a node with only 1 child and that child is the one which is in the middle. And provided it would not have any more descendants. It need not have descendants because it can be a leaf. This is completely okay but if it had more descendants then we would again have a problem in height balance property. This is okay which means that the node in the level in k-1 can have only 1 child. But everything which is in the 1st place should have at least 2 children. We said every node at level k-2 should have 2 children which means levels 1 through k-1 are full. It means they have as many nodes as possible on that level in a binary tree. This is after all a binary tree. So they are full.  

(Refer Slide Time: 40:58)

 

 (Refer Slide Time: 42:24)

 
What does that mean? That means the tree has at least  nodes. We also argued recall that the height of the tree is at most 2k-1. If the height of the tree is 2k-1 then it has at most  nodes. This implies the number of nodes in the tree which was n is between  and . Since we have been using h for the height, let us substitute h for 2k-1. Let us see how this equation would look like. This ( ) becomes  and this ( ) becomes . This is the same thing I am showing you again. What is this saying? If you have an AVL tree of height h then it has at least  which we had shown earlier. Now we are showing  just roughly the same thing nodes, all though we have proved the sharper bound. I am coming back to the older bound. The point is it has an exponential number of nodes, it has number of nodes which is some constant  an exponential. Because that gives the logarithmic height property. This is actually a third way of proving that the height of the tree is only log n. You can also use this as a proof. This did not require solving a recurrence relation. The other 2 methods we saw while solving the recurrence relation. But the sharpest bound we have seen so far is  that is .

(Refer Slide Time: 45:31)

 

Let us summarize what we have seen as the structure of an AVL tree is concerned. If the height of an AVL tree is h then the closest leaf can be at level . I have just changed things around, when I said when the closest was at k then the height was 2k-1. If the height is h, suppose I give you an AVL tree on n nodes of height h then the leaf which is closest to the root is actually pretty far from the root. It is atleast half the height away, it is at least  away. It does not require a proof, I am just rewording what I have said earlier. We also saw that on the first  levels the AVL tree is a complete binary tree. 

This is what an AVL tree looks like essentially. For the first half levels it is complete, very dense and then it starts thinning out. So it turn the tree around with the root at the bottom so initially it is dense and then it thins to the full height. But the fact that it is very dense for the first edge by 2 levels means it has a lot of nodes. It is a complete binary tree so it has  nodes straight away. That means that the height can not be too large, if I had n nodes the height can not be more than 2log n. Once again I have said that if number of nodes in the AVL tree is at least just this fact, since it is a complete binary tree on  levels it has at least  and at most  nodes because that is the height of the tree. 

This is the useful structural fact to keep in mind about AVL trees. Although we will not use it for any of our algorithms. But it just gives you some intuition of what the tree is and why is that this tree has only a logarithmic depth. We have looked at this height balance property, we said if this height balance property is there then it is nice the height of the tree is only algorithmic. We want to say that all our operations are only logarithmic because we still want to say that you can do a search, insert and delete in log n times. Search is easy there is no problem with search because after all it is a binary search tree. Forget the height balance property, it is just a binary search tree so you just do search as you do in a binary search tree. How much time will you take? Proportional to the height, order h. Height is log n so you will take only log n time. That is the best you can do in some sense. 

(Refer Slide Time: 49:16)
 
Suppose you were to try an insert. When you are going to do an insert what can go wrong. Recall for the tree to be height balanced, if the difference in the heights of its children is at most a one. When I insert a node it can change the height of some nodes and as a consequence the height balanced property might get violated. The first step of insertion would be the same as we did in the case of a binary search tree. 
How did we insert in a binary search tree? First you find the position. How do you find the position? You will just search for that element that you are trying to insert, that will tell you where the position is and just put the node there. And then you start marching up back to the route by following the parent pointers. As you march up you keep updating the heights of the various nodes you encountered because these are the only nodes whose heights could have changed and no one else. We will look at this again in more detail perhaps in the next class. I am just giving you the flavor of what needs to be done when we are doing an insertion. These are the nodes whose heights are going to change. 
So we are going to the first place where the height change appears, where the height imbalance happens. We are going to only start from the node where we inserted and move up the tree towards the root. Basically we keep going parent, parent till we hit the root. On this path that we follow, we find the first node which has the height imbalance property. Suppose that node is called z and its grandchild is called x. Let me skip this part and y is the node in the middle. So I think it is best if I show you the picture and that will give you an idea. 
(Refer Slide Time: 51:13)  
 
Suppose the 1st one was my tree, forget this empty node which is the last node for now. This was my tree originally. If this was my original tree, then is that an AVL tree? Height balance is satisfied in the node 50 because 48 is one and this 62 is one. The last node is not there, forget this type of node. This 78 is also height balanced because this 50 is 2 and this 88 is 1. This is also height balanced because this is 1 and this is 0. (Refer Slide Time: 49:46) This is height balanced because this is 2 and this is 3 initially. (Refer Slide Time: 49:47- 49:52) But now suppose I went and inserted a node 54 which came in here. The 54 would come here, I go right, left and then right here and left here. (Refer Slide Time: 49:56- 50:03) Now the height balance property is violated. What I am going to do? I am just going up the tree towards the root. Is the height balance property violated here? (Refer Slide Time: 50:16) No it is not. 

This is one this is zero. Is it violated here? (Refer Slide Time: 50:21) No, 1, 2. This is height 2, this is height 1. (Refer Slide Time: 50:27- 50:30) It is not violated here. Is it violated here? Yes because this is now 3 and this is 1, so these numbers are the new heights. (Refer Slide Time: 50:31- 50: 37) This is 3. (Refer Slide Time: 50:40) So this 78 is the first node at which the height balance property is violated. We call this node 78 as z, its child will be y and its grand child will be x. We wonder which child of this node will be y. The child on the path that we have taken. 
(Refer Slide Time: 51:20)
 
And now we need to do something to this tree to make it height balanced again. This is not height balanced tree. All the things we said about log n will go out of the window if you leave the tree like this. What are we going to do? We are going to do a kind of rotation operation and this 2nd picture in the above slide will become my new tree. In some sense what I have done is, I have moved 62 up and moved this 78 down and this 50 was here. It looks a bit mysterious. That is what we are going to do in the next class. Understand how this rotation operation is done. So as you can see now the height balance property is not violated at any node. It is not clearly in the node 50, its not here in 62 and also in 78. Both of them (50 and 78) have height 2. The 62 is at height 3 and this 17 is at height 2, so it is not violated. This is still a binary search tree with the same keys as before, we will not change the keys.
There are other ways also but you want an automated way of doing it, you do not have to draw the picture and then figure out what rotation have to be done. You will be able to do this program. This is what we are going to do in the next class. Look at insertion and look at how to do these rotations so that the height balance property is retained even after insertion. So we will look at both insertion and deletion in the next class. So in todays class we looked at AVL trees. We saw how AVL trees are defined and actually we proved a bound of  as the height of an AVL tree. We spent a lot of time figuring out how to solve that recurrence relation. We saw 2 ways solving that recurrence relation. We also looked at some structural property of the tree which also proved a similar bound and the height of the tree. With that we will end today?s class.   
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 12
Avl trees (Contd.)

Today we are going to continue our discussion on AVL trees. In particular we are going to look at the insertion and deletion procedure in an AVL trees. So we will begin with insertion.  

(Refer Slide Time: 01:24)

 

We had started this discussion on insertion in the last class also. Suppose I am trying to insert a node v into an AVL tree. Actually what I am trying to do is insert a key. What is the process of insertion in a binary search tree? First you find where the key is, you go to that place put the key there. Let say the node in which I put the key is v. This is something we had started the discussion, if as a consequence of this insertion it does not remain an AVL tree that is because the height balance property is violated. What are the nodes whose heights could change as a result of this insertion? Which of the nodes whose heights could change? Recall we defined the height of the node as the height of the sub tree rooted at that node. So this is large tree and some where below I insert a particular node. Which are the nodes whose heights could change? It could only be the ancestors of this particular node. Not because it is written here but you should also understand why it is only ancestors. Because it is only in the ancestors of that node whose subtree has changed as result of this insertion process. 

For any other node its sub tree has not changed it remains the same as before. So the ancestors of this node, their heights might change. And change means it will only increase. Because we have added a particular node. So it is the ancestor of these nodes whose height might increase as a consequence of this insertion. So if the insertion causes the tree to become imbalance or unbalanced, then some ancestor of this node v is the culprit and it is the place where one or more ancestors would have a height balance problem. 

(Refer Slide Time: 03:47)

 

Height problem means it will become height imbalanced. Height imbalance means one left sub tree and right sub tree, the difference in heights is more than a one. What are we going to do? We are going to essentially travel up the tree from this node v. Travel up the tree which means just keep following the parent pointer till we identify the node z which is unbalanced. So I have said, till we find the first node x whose grandparent is unbalanced, but you can also think of it as, I find the node z which is unbalanced and x is its grandchild. Which grandchild? The grandchild that we traversed or went through the grandchild on the path. Because the node can have many grandchildren. I will show you an example and this will be clear. We will call y as the parent of x. So y is the parent of x and then the child of z. 

Suppose this was the situation we had. This was an AVL tree which is given in the below slide and I insert let say 54 into this tree. So 54 would come here. Why, because it is larger than 44, smaller than 78, larger than 50, smaller than 62 and so it would come here (Refer Slide Time: 04:54). Now if there is a problem, actually this tree is not a height balanced any more. This is not an AVL tree any more. If there is a problem we said that the problem would be on one of these nodes. Note that these are the only nodes whose heights are changed. Earlier the node 62 had a height of zero, now it is one. The node 50 had a height of one, now it is 2. The node 78 had a height of 2, now it is 3. The node 44 had a height of 3, now it is 4. Which is the first node on this path which is now imbalanced? Is the node 50 imbalanced? No, the difference of height is 1.Is the node 78 imbalanced? Yes, the difference of height is 2, the node 88 is 1 and the node 50 is of height 3. The height of no other node has changed. The height of the node 17 is not changed.




(Refer Slide Time: 06:56)

 

Did you understand why the heights of these nodes on this path would change? The node 78 will be z and x would be its grandchild on the path and the parent would be y. Lets call that x is a node, y is a node and z is a node. So x is the node here in 62. So we travel up from 54, we find the first place where the imbalance happens let us call that z. And x is the grandchild of z, grandchild means child?s child. So child is y and its child is x. Now we are going to rebalance this tree, so to rebalance this tree in particular we are going to rebalance this sub tree. The sub tree rooted at z and we will do that by performing a rotation. This is what will happen after the rebalance, this is what the tree would look like. What we are going to do? Just understand how we came up with this picture. As you can see I have only changed this sub tree, the one containing these 6 nodes 48, 50, 54, 62, 78, 88. They are here the 6 nodes but organized in a manner so that this node 62 is not height imbalanced any more. And neither this node 44 is height imbalanced. We are going to understand this process today.  

Let us first understand what is a rotation. In the previous slide I used this term rotation. What does rotation mean? So rotation is the way of locally reorganizing a binary search tree. The picture shown in the below slide is a part of my binary search tree. This could be a huge tree but I just consider a part of it. So u is one node, v is its child and these are some sub trees. The is the sub tree rooted at the right child of u and this is the sub tree rooted at the right child of v. The is the sub tree rooted at the left child of u. This could be a null tree, no node or could be a huge tree, I do not care. What do I know? Because it is a binary search tree and I know all the keys in   are less than the key in v. All the keys in  are more than the key in v.

Keys in  are less than the key in u and keys in  are more than the key in u. This follows from the property of binary search tree. What is the rotation step going to be? First what we are going to do is let us just forget these links and let just look at this (Refer Slide Time: 09:02). And now this is what a rotation does. What has happened? V has become the parent of u. That is what a rotation is and will put the links back. What happened was v became a parent of u. The binary search tree property still holds by the way. Keys of   are less than v, keys of  are more than v so they come here and they are less than u so they come to the left of u. It is still a binary search tree but we have done some local reorganization and this will be very useful and we will see why. 

So  remains to the left of v.  was to the right of u, if you remember.  was to the right of u,  was to the left of v,  was to the right of v. So  remains to the right of u,  remains to the left of v but  moves from being to the right of v to the left of u. It is now the left child of u. Everyone follows what a rotation is. 

(Refer Slide Time: 07:32)

 

Now let us see how we will use these rotations to do our insertion. Suppose the insertion happens. This is the tree which is given in the below slide and I have not drawn the links. But it should be clear what the links are. Who can tell me what the links are? The y is the child of z, x is the child of y. These two ,  are the children of x,  is the right child of y and  is the right sub tree of z. So in the next few slides you will see pictures with out these links. But that is just to avoid the clutter. It should be completely clear about the relationships. Suppose I did an insertion in . And these are the x y z that we encountered in the procedure. May be insertion happens some where in some leaf, we went up along the path towards the root. The root may be some where here (Refer Slide Time: 11:48). And z was the first place at which we had an imbalance. And y was the child of z and x was the child of y on this path that we took.  





(Refer Slide Time: 16:29)

 

I have taken one picture but it could also be different. The y could have been the right child of z and x could have been the left or the right child of y. How many different cases are possible there? It is 4. The y could be the left of the right child of z and x could be the left of the right child of y. Two times two, 4 different cases. Am looking at one particular case now. That y is the left child of z and x is also a left child of y and the insertion happened in . I am using this ht ( ) to denote the height of the particular thing. The height of  let us say originally it was h and now because of the insertion it became h+1. It cannot increase by more than a one because after all I am just adding one node. So the increase in height can be at most a one and let say there was an increase in height. So there is an increase in height of this node x also. 

There is an increase in height of node y and there is also an increase in height of node z. That is why z became an imbalanced. If there was no increase in height of y then z would not become imbalanced. (Hindi Conversation) Everything is the same as before that means height of y is also increased. And height of y has increased because height of x has increased. Height of x has increased because height of   has increased and height of   let say increased from h to h+1. What can we say about the height of  now? What is the height of ? So x is balanced even after the insertion, because z was the first node which was imbalanced. So x was balanced after insertion. If x was balanced after insertion (Hindi Conversation) (Refer Slide Time: 14:20-14:52). So height of  is one of these 3. Which one? Can it be h+2? If it is h+2 then originally x is imbalanced, because originally height of  was h. If this ( ) is h and this ( ) is h+2 and then this (x) was imbalanced even to begin with, but that is not the case. Originally it was an AVL tree so the height of  cannot be h+2. 



Can it be h+1? If it is h+1 then height of x does not increase. Because this ( ) is h+1. Then that means what was the height x to begin with? It is h+2. If the height of this increased from h to h+1, even then its height remains h+2. The fact that the height of x has increased implies that this ( ) cannot be h+1. If this ( ) was h+1 then the height of x did not increase. It remained what it was before that is h+2. This implies height of  cannot be h+1. It has to be h. So height of   is h. If height of  is h and height of this ( ) has increased from h to h+1, then what about height of x? What was the original height of x? The original height was h+1 and now it has become h+2. The height of x has increased from h+1 to h +2. 

(Refer Slide Time: 20:05)

 

Let us continue this argument. So this is the picture so far which is given in the above slide. We have argued that the height of  has increased from h to h+1. Then height of  is h then the height of x is increased from h+1 to h+2. What about the height of ? Since y remains balanced, the new height of x is h+2. And this y is height balanced. The height of  is h+3 or h+2 or h+1. One of these 3, because the difference in heights can only be one. So its one of these . If it is h+3, we are repeating the argument roughly. If it is h+3 then that means y was originally imbalanced because original height of x was h+1. So y is originally imbalanced. Height of  cannot be h+3. If it is h+2 then that means that the height of y has not increased. Because if this ( ) was h+2 then the height of y that means originally was h+3 (Hindi Conversation). So height of   cannot be h+2. Height of   has to be h+1. So height of  is h+1. If the height of   is h+1, what is the height of y? Originally it was h+2 because both of these guys were h+1 (Refer Slide Time: 18:21). So this (y) was h+2 originally. And now it has become h+3 so it increased from h+2 to h+3. What about height of ? Note that z is imbalanced. 

The new height of y is h+3. So what should the height of   be?  Or it is h+5, initially it was balanced. Since z was balanced, height of  is h+1 or h+2 or h+3. Since this was originally h+2, this ( ) could only be h+1, h+2 or h+3 and since it is now unbalanced it cannot be h+2 or h+3, it has to be h+1. The height of  is h+1. What is the height of z initially? This was originally h+2, this ( ) was h+1, so this (y) was h+3 originally (Refer Slide Time: 20:02). And now of course its heights become h+4, but now we will do some rotation and stuff like that. So that will reduce its size, so its original height was h+3. 

(Refer Slide Time: 20:05)

 

So we will keep this picture. This is the final thing we argued. These are the heights of the various things. So when I said from here to here that is from h+1 to h+2, the first thing is, what it was originally and what it is now. We just need to look at the new values. We are going to do a rotation around this pair (y, z). What does rotation do? Rotation is going to rotate this, so that y is now going to become the parent of z. What do we want to do? We want to move this y up so that it will come here and y will become the parent of z. This is what the rotation is. 

Let me just draw it here. So y will now become the parent of z which is shown in the below slide. Where will I put these two? The x is one sub tree, this remains as it is. This will not be changed. So will remain at the right and  will come to the left and this big piece under x will remain as it is. That is what the rotation was. This is what will happen.







(Refer Slide Time: 22:07)  

 

The y has become the parent of z, these two   and  are the children of z. And this entire thing under x was to the left. Again I have not shown the links but you should be clear what the links are. I have written down the heights. The height of  was h+1,  was h,   was h+1 and  was h+1. After a rotation we already saw that the binary search tree properties are maintained. So this is still a binary search tree, but now we want to argue that the height balance property is also restored. (Hindi Conversation) 

(Refer Slide Time: 22:31)

 

Where was the imbalance happening? At z, as you can see this has height h+3,  has height h+1 so this is height imbalanced (Refer Slide Time: 23:20). What is the height of node x? It is h+2 because this ( ) is h+1 and this ( ) is h. What is the height of node z? It is h+2. What is the height of node y? It is h+3. Is everything balanced now? This x is balanced because there is only a difference in height of one. This z is balanced because there is no difference. This y is balanced because there is no difference. We have done a rotation. This is called a single rotation. You will soon see why it is called a single rotation. How much time does this operation take for just one rotation? We just have to do a constant number of operations. May be z will have to become a child of y. So there will be 3 or 4 different reference changes that you have to do. May be 5, may be 6 or some constant number, independent of the number of nodes in the tree. Now one interesting thing is happened. The original height of z was h+3. That is why we had written this h+3 here. After this rotation the height of this sub tree is also h+3. Whatever was the original height of this (z) is the new height of this sub tree (y) also. 

(Refer Slide Time: 25:01)

 

So the height of the sub tree remains the same after the rotation. The h+3 was the height before insertion, after we inserted and did the rotation the new height also becomes h+3. Why is this important? Because now I do not have to go up further. Because now any ancestor of this, its height would not change any more because whatever was the original height of this h+3 of this node is the new height of this sub tree also. So any of the ancestors of z, their heights would become the same as before. And so there will be no imbalance on them. After I did my insertion, I started moving up I find the first place where there was an imbalance. I did the rotation and I am done, I do not have to go up any further. We have actually considered only one case. One out of 4 different cases. Why one case? Because we said y is the left child of z and x is the left child of y. Now there is one symmetric case, which is the symmetric? Both are right, right. That I am not going to handle because I trust all of you can believe me that it is the symmetric case completely. 

The other case is this one. Where x is let say the right child of y which is the left child of z. This has the symmetric case that is, y is the right child of z and x is the left child of y. Again that is completely symmetric and will not handle that. So let us repeat the argument that we had. 

(Refer Slide Time: 31:18)

 

So once again I am assuming that the insertion happens in . It could happen in any one of these that is  and  but again it is symmetric. Let us assume it happens in . So this height went from h to h+1. What about the height of ? Since x is balanced, the height of  is either h+2 or h+1 or h. If it is h+2 then that means x was originally imbalanced. If it is h+1 then that means the height of x is not changed. So it has to be h. I am repeating the argument, it is the same as before. If  is h and the new height is h+1. What is the new height of x? It is h+2. What was the original height? It is h+1. So its height moved from h+1 to h+2. 

Now let us look at the height of . Since y is still balanced then that means the height of  is either h+3 or h+2 or h+1. If it is h+3 then that means y was originally not balanced. If it was h+2 then the height of y has not increased. It has to be h+1, which means the height of y has increased from h+2 to h+3 which means that now since z is imbalanced, the height of  has to be h+1. Which implies that the original height of z was h+3. Exactly the same as before, we do not make any difference. But now the rotations will have to be a bit different. In x, y and z, which of these 3 keys is the middle key? x, y or z which of these is middle. It is x. z is the largest, y is the smallest. If you recall in the previous rotation, we had x, y and z again. In the previous rotation which was the middle key? y, because they were all in a line. z was the top, y was its left child so it means y is less than z. And x was its left child so x is less than y is less than z. So after the rotation we ended up making y as the root. The middle key we ended up making the root. Here also we wanted to do something similar but except that the middle key is now x. So we are going to do a 2 step rotation. That is why it is called a double rotation. 
First I will rotate x y. Let see what will happen after I rotate x y. This is what it will look like. x has moved up y has moved down,  remains the left child of y,  remains the right sub tree of x and  switches loyalties from x to y. So earlier it was the left sub tree of x, now it is the right of sub tree of y and  remains as it is. And I have just copied the same height so has height h+1,  has height h+1,  has height h and  has height h+1. No difference. Is this balanced? Is this height balanced? Height of y is h+2 because both of these  and have height h+1. Height of x will be h+3, actually now there is an imbalance at x itself. Because y has h+2 and  has h. There is an imbalanced in x and height of z would be h+4 because height of x is h+3 that is one more than that. 

So this rotation has not done the job for us yet. We need to do one more rotation. What are the other rotation I need to do? Rotation (x, z). What will happen now? x will go up and z will come down. x will become the parent of z.  was the right sub tree of z, so it will remain the right subtree. y had  and  as its left, so they will remain as they are. And  which was the right sub tree of x now becomes the left sub tree of z, the same thing. Now let us compute heights. Height of y, h+2. Height of z, h+2. Height of x, h+3. Height balance happens, this is balanced in y, its balanced in x and its balanced in z. Further the height of this sub tree is the same as the original sub tree, h+3. So the final tree has the same height as the original tree. Hence we need not go further up the tree. 

Did you understand the need for the double rotation? We ended up doing the same thing, as I said the middle key ended up being at the top. Because we want to be able to split the thing uniformly. Why was the height imbalance happening? Because x was the middle key, it was coming way down. When I kind of split uniformly the heights reduced and there is a height balanced. It is roughly what is happening here. How much time does the double rotation take? Constant time. So just as a quick recap, we have 4 different ways to rotate nodes in an AVL tree. The single rotation was something like this given in the below slide. There were all in a line x y and z or they were like this x y and z (Refer Slide Time: 33:33). And after rotation this is the picture you get and here after rotation this is the picture you get.  















(Refer Slide Time: 33:30)

 

This is just a recap, you understand why we are doing this thing and why this picture is a height balanced picture and we also saw double rotations. So either like this, in which case after rotation you got something like this or it could be like this left and right and in which case again after rotation you got something like that. This is just to show you the picture, you do not have to understand much here. You are hopefully understood why the single and double rotations are done in the way they are done. 

(Refer Slide Time: 33:56)

 

Let us come to deletion because exactly the same principles are going to be used for deletion also. It is a binary tree. The difference between the height, does it become zero? We saw that. Did we see that? 

(Refer Slide Time: 37: 50)

 

Let us see here. Here the difference is zero but in this node z the difference is a one (Refer Slide Time: 34:50). Some nodes there would be a difference of a one, some other nodes there would be a difference of zero. Let us look at deletion. In a binary search tree when I delete a node, we will have 3 cases if you remember. When I am deleting the node which is a leaf or I am deleting a node which is only one child or I am deleting a node which has 2 children. When I delete a node which has 2 children, what did I do? I went to the successor of that node. I copied the content of their successor in to that node and I deleted the successor. The actual node I deleted was the successor node and the successor node has only one child or no children. Why does the successor has only one child? Because it does not have the left child, because if it had the left child then it would not be the successor. It has only one child or it has no children. So the actual node that you end up deleting is either a leaf node or a node with only one child. This is the actual node that you ended up deleting. 

What is a node which has only one child? In an AVL tree if I tell you, here is a node which has only one child. What can you say about that node? This is a node with only one child. Can it have another child? Can this node have the child? No, if it had a child like this, then what would be the problem. There would be a height imbalanced. So it cannot have this child or it cannot have the other child, which means this node is a leaf exactly. If in an AVL tree, a node has only one child, then that child is a leaf. What are we saying? When I am deleting in an AVL tree I am either deleting a leaf or I am deleting the parent of a leaf. If I am deleting a node with only one child, then it is a parent of a leaf. Which means that I am essentially deleting a leaf. If I am deleting the parent of a leaf them essentially what am I doing? I can just think of it as if I was deleting the leaf and copying the content of the leaf in to the parent. So I can always think of it as if I am deleting a leaf. So either deleting a leaf or parent, let us just keep that in mind. 
(Refer Slide Time: 39:46)

 

Let us say w is the node that we are deleting. We are going to define our x y and z slightly in a different way. So z is once again the first unbalanced node that we encountered as we go up from w towards the root. When I deleted w, once again what is going to happen? The ancestors of w, their height could reduce. So one of these ancestors will be unbalanced, if any are unbalanced then one of this will be unbalanced. Let us say z is the first unbalanced node encountered while we are traveling up the tree from w. Now y is not the child of z on the path but we are defining y as child of z with larger height and x is the child of y with larger height. z has 2 children, one of them has a larger height than the other one. So we take that one. Once again we will perform rotations to restore the height balance of the sub tree rooted at z. In the case of insertion what was happening is that once you did this rotation, we did not have to worry any more on the ancestor nodes. Everything was taken care of, we could stop after doing the rotation. In delete what we are going to see is that we might have to continue up and we will see what the reason for that. So you might have to continue up the tree, go to the ancestor of z and once again find the first node which is unbalanced and repeat the rotation there and after that go even further up. Find the first node which is unbalanced, repeat the rotation there and so on till you reach the root. 

Let us understand what is happening? The x is the child of y with larger height. If both of them have the same height, then we will say which of them should be x. We will say that in a minute. It could happen so this is a valid question. Both the children of y might have the same height. Then which is x? We will see which is x. The two children of z cannot have the same height because that is the imbalanced node. Ignore these h-1 or h-2 for a minute. They should have come at the end. This is the picture which is given below, I have z which is the first unbalanced node. y is the child of z which has larger height and x is the child of y which has larger height. I did a deletion in , I started going up the tree I found a z. Can y be this node here (Refer Slide Time: 42:18)? 


(Refer Slide Time: 45:49)
 

If y was here then its height would have actually decreased. What is a problem? Let see. Why did I draw y to be this? So w is some where here, the node I deleted is some where here. I started walking up and I came to z. And this was a first node I identified which had an imbalance. And then what did I say? This node z has 2 children, this is one child of z and the other child of z is y. Let us take the child of z which has larger height. Why could it not have been this node which has larger height? This is very simple actually. 

(Refer Slide Time: 46:32)

 

There is an imbalance that happened here. Earlier there was no imbalance. Imbalance happen because the height of this guy decreased (Refer Slide Time: 43:55). If there are 2 things which was balanced and one of them decreased then what can we say about the relationship between these two things initially? Could this have been smaller than this? No, if this was smaller than this then actually it will become more balanced (Refer Slide Time: 44:20). So this y must have been larger than this for imbalance to have happened. So this is y therefore. The height of  has reduced from h to h-1. What can I say about the height of y? It is h+1. It means that y must be h+1 or h+2. It cannot be h+2 because then originally also it was unbalanced. It cannot be h-3, because then initially it was unbalanced. It has to be h+1. So height of y is h+1. Of those 2, x is the one which has the larger height. The height of x is h. What can I say about of height of ? It can be h. Since this y is balanced this can be h or h-1 because y is balanced. So this is what we have argued so far. This goes from h to h-1, z is h+2, y is h+1, x is h,  is h or h-1. So the height of x is h and this is also balanced. So the heights of these two ( , ) are h-1 or h-2. One has to have a height of h-1, both cannot have a height of h-2. That is the only thing we can say. Both can have a height of h-1. You cannot say that only one can have a height of h-1. That is a wrong statement, at least one has a height of h-1. 

Let us do a rotation now to see what needs to be done. So these are the various heights that have seen which is given in the slide below. This is what we argued in the last 2 slides. What kind of a rotation should I do? I will do a rotation (y, z) once again. Similar to what I did in my insertion. So as a consequence you will have this kind of a picture now (Refer Slide Time: 47:12). y went up, z went down,  and became the 2 children of z,  and are the children of x. I have written down the heights h-1 or h-2, h-1 or h-2, h or h-1 and h-1. Because  went from h to h-1, so this is h-1. What is the height of x? This is h-1 or h-2, this is h-1 or h-2 but one of them is at least h-1, so x is h.

(Refer Slide Time: 48:59)

 

What is height of z? h or h+1. What is the height of y? h+1 or h+2. What was the original height of this tree? h+2. So if this is h+2 then we are okay, we do not have to continue. But if this is h+1 then we may have to continue because this now becomes the bigger tree (Refer Slide Time: 48:21) (Hindi Conversation). We will have to continue the argument as we go up. The way we said (Hindi Conversation) height has reduced from h to h -1, now we might have to say that this bigger thing height has reduced from h+2 to h+1 and we will have to repeat the argument at the next higher level and so on. 

(Refer Slide Time: 51:26)

 

After rotation the height of sub tree might be one less than the original height. In that case we have to continue up the tree. It is might be, you understand? Because it could not have reduced, in which case we can just stop. So this is single rotation, in the case when this was the picture which is given above. y was the left child of z and x was the left child of y.

But we could have this kind of a picture which is given above, that x is the right child of y. So the first part of the argument is the same. This  has gone from h to h-1, so we argued that the height of y is h+1. So height of y is h+1 and height of x is h because x is the one which has larger height and height of z is h+2 because height of y is h+1. This  is of height h or h-1. How about the height of ? So y is balanced so height of  is either h or h-1. Now if height of  was h then what I would do is, I would pick the root of  as x. So in that case I will be able to do that single rotation of mine, the previous case. 

If the height of  is h and the height of x is also h, the same question which he had asked earlier. Which do we pick? Which one will I pick? I will pick the one which will give me the single rotation case. I cannot say that I will pick the left child or the right child. I will pick the left child, if y is the left child of z. If y were a right child of z then I will pick the right child. Since such was not the case, if it was h then I would have picked x as the root of . So height of  is h-1. Since the height of x was h and these  , are the same as before h-1 or h-2 for both of them, with one of them at least being h-1. This is our new picture (Refer Slide Time: 51:35). Let me just copy it here. These are the heights of the various nodes and trees. Let us do the double rotation step. So first I am going to rotate as before (x, y), the same process as an insertion essentially.  

(Refer Slide Time: 52:03)

 

So with the rotation of (x, y) y will come down and x will move up I would get such a picture which is given in the right side of the above slide.  is the right child of x,  and  are the two children of y that is the left and right sub trees of y. The  is h-1,  is h-1 or h-2,  is h-1 or h-2 and  is h-1. What are the heights of these nodes? Height of y is h. What about the height of x? h+1 and there could be an imbalance at x if this where h-2 and height of z is h+2 because x is h+1. And there is an imbalance at this node z, so there could be here at x and there is here at z. There is none here at y. But we are not done so we will now do a rotation around (x, z). 

















(Refer Slide Time: 52:55)

 

What would happen? x moves up, z moves down , and  becomes the two children of the z.   has height of h-1 or h-2,  has height of h-1 and  has the height of  h-1 or h-2 and  has a height of h-1. What about the height of y? It is h. Is it balanced? Yes,  is h-1 and  is h-1 or h-2. What is the height of z? It is h. Is it balanced? Yes, h-1 and h-1 or h-2. What is the height of x? It is h+1, it is also balanced. But this height is now strictly one less than this (z). (Hindi Conversation) There is no might this time. Why did I make that argument (Hindi Conversation) height h-1 (Hindi Conversation) h we could have done single rotation (Hindi Conversation) and all that thing (Hindi Conversation) You understand y I had to make this kind of an argument? (Hindi Conversation) (Refer Slide Time: 54:00). So hopefully you all understand this. 


















(Refer Slide Time: 55:12)

 

What has happened is, final tree has height less than original tree, we need to continue up the tree. You understand the need for continuing up the tree. Because height has reduced by one, as the consequence they could still be imbalanced at the ancestors. Ancestors of this node, that is what ever is this node. This will require a proof even if it is correct, so think about it. It is a good question. He is asking me whether we can just be satisfied by checking the parent of this node. So think about this and we will answer it, may be in the next class or may be after the class. 

(Refer Slide Time: 56:38)

 


Let us quickly look at the running time of insert and delete. So for insertion we spent log n time in finding way to insert. Why log n? Height of the tree, we actually spent time proportional to the height of the tree which is the height we argued in the last class that is log n. So we spent log n time coming down then we spent log n time may be moving up. At most log n because that is the height and then we spent constant time in doing a rotation and one rotation and we are done. The entire thing is only log n.

Deletion, recall that in insertion you will first find the node in the binary search tree whether the insertion has to be done. You will insert the node then you will start moving up the tree to find the place where the imbalance occurs. The first place and then in that place we said we will just do a rotation and with one rotation you will be able to satisfy the height balance property once again. Insertion basically requires order log n time to insert the node and you might have to spend order log n time to move up and a constant time to do the rotation. So in all it just takes a log n order time. Deletion on the other hand also requires only order log n time but we need to do a little bit more work. The reason for that is to delete a node, recall that you have to identify which of those 3 cases the node is does in. Whether the node you are deleting is a leaf node or if it has only one child or if it has 2 children then we need to find the successor of the node. We need to go right and keep going left, find the successor, swap contents and then delete the successor node. 

Once you have deleted, now you have to move up the tree to find the first place where the imbalance occurs. Having found that you do a rotation, that rotation may or may not solve your problem. If it does not solve the problem of height balanced, it does not restore height balance then you might have to continue up from that from that node. And may be once again perform a rotation, if that is also the problem then you stop otherwise you will have to continue up. So in all the number of rotations you might require is as large as the height of the tree. Because with every rotation you are moving one level up. You might require as many as order log n rotations, but each of those rotations only taking a constant time. The total time required for all these rotations put together is only order log n. And we took order log n time to delete the node. All the rotations also took order log n time. The total time required for the entire delete operation is still order log n. So with that we will end today?s class. We saw how to do an insertion and deletion in AVL trees. We argued eventually that the total time taken for both insertion and deletion is only order log n. In the last class we had seen that the time taken for search is also only order log n in the case of an AVL tree. So all the 3 operations of insert, search and delete can be done in log n time in an AVL tree.  




Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
      Lecture ? 13
       (2, 4) Trees

In today?s class we are going to be talking about 2-4 trees. This is another way of representing a dictionary. So we are going to see the operation of insert, search and delete on this data structure and we are going to have the same kind of performance guarantees as the case in AVL trees. But in later classes we are going to see how this data structure is useful. So today I will just begin with this. 
 
(Refer Slide Time: 02:10) 

 

What are 2-4 trees? They are search trees, they are a kind of search trees but they are not binary search trees. So recall in a binary search tree what was happening? The tree was a binary tree with each node at most 2 children. So this not going to be a binary tree. That is a first point. Nodes can have more than 2 children now. So these 2-4 trees are also called 2-3-4 trees. I will tell you what this really means. So 2-3-4 actually refers to the number of children and node can have. So a node can have either 2, 3 or 4 children. Such trees in which a node can have many children but satisfy a certain kind of search properties are called multi-way search trees. 








(Refer Slide Time: 04:43)

 

So each internal node of a multi-way search tree has at least two children. It will have at least two which means it could have more than two children. Any number of children more than two. Each node of a tree also stores a collection of items of the form (key, element). In the binary search tree, each node was storing one key and the element there was let?s say a reference to the element or the element itself could be stored there. If the key was a student entry number, then the student record associated with the key could also be stored in the node itself. So in the similar way, we have that in the multi-way search tree you will have each node containing a pairs of this kind that is (key, element). And how many pairs there could be? It is more than one. In the binary search tree there is only one such pair in each node and in a multi-way search tree there could be more than one. 
 
In particular there could be d-1 such pairs or items, where d is the number of children that particular node has. So we are just generalizing the binary search trees. In the binary search trees each node has two children. Each node could have two children and then there is only one key that is kept in the node. Because that key helps us to determine whether we should go left or right. Similarly here we have d children, if d is the number of children then you really need to know in the search process whether you should go to the first child, second child, third child, fourth child and so on. So you will have d-1 different keys sitting in the node to help you determine that. I will soon show you an example and that will be clear. 

So this is an example of multi-way search tree (Refer Slide Time: 04:50). As you can see this node has two children. This has 3 children, this node has 2 children and this actually has 6 children. How many keys are there in a node? The number of keys in a node is one less than the number of children that node has. And why is that? So for instance this node (Refer Slide Time: 05:20) has three children and you need two keys in the node. The keys in the node determine what set of keys the various sub trees are going to have.  


(Refer Slide Time: 04:43)

 

So what I am trying to say here is, this is key 22. So this is also in the left sub tree. So everything in this left sub tree here is less than 22 and everything here is more than 22. Everything here is more than 22 and everything here is less than 22 (Refer Slide Time: 06:00). If you look at this key everything to the left of this is less than 5, so in the right sub tree we now have 3 children. So in this first sub tree everything would be less than 5. In the last sub tree everything will be more than 10 and those in between 5 and 10 would lie in this middle sub tree. So that is a concept. Now you understand why you need d-1 keys if you have d children. So everything less than the first key would be in the first sub tree. For that you have to follow the first child. Everything between the first key and the second key you will have to follow in the second child and so on. 

With that let me go back to the previous slide (Refer Slide Time: 04:43). So the children of each internal node are between the items. This is what I mean by between in code. So you have a certain node it has various keys or items. If you look at two consecutive keys then all the elements or all the items which have key value between the consecutive pairs would be in one sub tree. For that you will have to follow one child. So let?s get back to this. This is an example of a multi-way search tree. And how do you search in such a tree. Searching is similar to the binary search procedure as you did in the binary search tree. 

So suppose we are searching for 8. You come down here, compare 8 with 22 so 8 is less. So you go here, now you will have to find, so 8 is not less than 5 and 8 is not more than 10. But 8 lies between 5 and 10. So you will follow this and then you will find that 8 is sitting here. So it?s a successful search. So when you are searching for a key s you will compare it with k1. k1 is lets say the very first key in that node and k lets say kd-1 is the last key in that node. So you compare it with the very first key if it is less then, that means you have go to the left most sub tree. If it is more then kd-1 then you have to go to the right most sub tree.



(Refer Slide Time: 09:25)

 

So when you are searching for the node, for instance when we are searching here for 8, we came down, we went left because 8 is less than 22. Then 8 lies between 5 and 10. So we came down here and then we found 8 here. So when we are searching for 22, we came down the sequence of steps and we found that 12 was not there in the tree. So in particular when you are at a node, you have to determine that the key that you are searching for lies between which 2 keys and once you determine that you will follow the appropriate child. At the two extremities you will check whether it is less than the first key or it is larger than the last key. In which case you would follow either the left most child or right most child. So it is as simple as that. 

So what would an in order traversal in the tree would look like? That was the question we were at. So first what is the in order traversal in a tree? We recall in order traversal says left, then you print the data of the node and then you go right. But now there is no left and right, because a node can have many children. So what does an in order traversal here mean? So first go the left most then print the key, then go to the next child then print the key, then go to the third child then print the key, then go to the next child and so on. That would correspond to an in order traversal. So for instance here if were to do an in order traversal, what would I do? 

I would come down here, first go left. So first I will do an in order traversal on this part of the tree. Which means that I first come in here, I first go left I will do an in order traversal here, which means I come in here I first go left but there is nothing here. So then I print the key, that is 3. Then I go to the middle child, nothing there so then I print the next key 4. Then I go to the right child, nothing there. So that finishes the in order traversal on this node.

 

 


(Refer Slide Time: 11:45)

 

Having finished the in order traversal on this node, I go back to the parent. And then I will print this key, because first I went left then I print the key which means I print 5. Then having printed this key I will now do an in order traversal of this sub tree (Refer Slide Time: 11:15). So when I am doing in order traversal of this I will get 6 and 8, having finished that I go back and print this key now which gives me 10. And then I will do an in order traversal of this right sub tree. And that would give me 11 first, 13, 14 and then all of these. So 17, 18, 19, 20, 21 now I finish the in order traversal of this entire thing. So I print the key 22 and then I go right. So as we can see, you will get the keys in sorted order. That is also easy to prove. Why? Because in an in order traversal I will first print out all these keys and only then will I print out this key. 

So which means that in the order of printing all the keys which are less than this key will appear before and all the keys which have more than this key will appear after and this is true for every key. So which means what you get is a sorted order. Yes, 8 could have more than other children also. For instance, I could have something here let?s say 5.5. The 5.5 would be a valid node here? Instead of this I could have just one node with 5.5 here. That we could have organized in a different manner. But 5.5 is a valid node, there could be more nodes here. 

So now let us understand what 2-4 trees are. So 2-4 tree is something like this. What are the properties? Each node has at most 4 children. So first it is a multi way search tree. Multi way search tree which means every node has at least 2 children. Now we are saying each node has either 2, 3 or 4 children. That is why it is called a 2-4 tree or 2-3-4 tree. Each node has at most 2, 3 or 4 children. The second important property is that all the leaf nodes are at the same level. So the leaf node here are this, just forget this square boxes for now. So these are the leaf nodes and they are at the same level. 




(Refer Slide Time: 16:28)

 

They are all at level, suppose we are numbering level 0, 1, 2 again, so they are at level 2. These are the only 2 properties of a 2-4 tree. Of course it is multi-way search tree, so a 2-4 tree is a multi way search tree with these 2 additional properties. Search tree will have a property that, everything which is less than this key is going to be in the left and everything that is more is going to be in the right. This is an example of a 2-4 tree, as you can see this node has 3 children and this has 2. There is no node with 4 children but you could also have a node with 4 children in it.  

What is the height of a 2-4 tree? Why should the height of the tree be at least log4n and at most log2n? What is the worst case? When would the height of the tree be maximum? It is when everyone has 2 children. In that case everyone has 2 children and all the leafs are at the last level. Then it is exactly a complete binary tree. And in complete binary tree we argued that the height is log2n, there was plus one, minus one some where forget where it was, but it is some thing like that. That is a setting when the tree height is maximum. The tree height is minimum when every node has 4 children in it. Because then the nodes are closer to the root, you will have 4 and then 16 at the next level and then 16 times 4 that is 64 at the next level and so on. 

Once again if we do the same analysis you will find that the height of this tree is log4n. So height of the 2-4 tree on n nodes always lies between these two quantities. It is either log2n, it lies between log2n and log4n. log4n is essentially half of log2n. Basically the height of the 2-4 tree lies between half of log n and log n. How much time does it take to search in a 2-4 tree then? Why log n? How do we search in a 2-4 tree? It is a multi way search tree. So if I am searching for a particular key lets say suppose what do I want to search? I want to search for 11 lets say. I came here with 11, where would I go? Compare 11 with 12, I come here (Refer Slide Time: 17:10). I am comparing 11 with 10, so I go right and then I find 11 here. So I found 11.

So how much time does it take for me to search in a 2-4 tree? Height of the tree, is it something more that I need to do? It is correct, it is order height of the tree. I have to compare with in each node. Because when I am searching for 11, I have to essentially compare against, how many keys there could be in a node? A node we said has how many children? 4. If it has 4 children how many keys would it have? 3. The maximum number of keys therefore is 3. If it has 2 children how many keys do I require? 1. So a node has either 1, 2 or 3 keys. So when I search for the key and I come with key, then I have to compare it with this key, this key and this key (Refer Slide Time: 18:16). So I might require 3 comparisons in all to determine which particular branch to take out. So I might require 3 comparisons. 

So the time is 3 comparisons with in a node, times log n because that is log n is a number of node I would be visiting. Order log n. So order log n is correct but you have to be careful about this. Within each node you require more than one comparison. In a binary search tree you required only one comparison but now you could require up to 3 comparisons. Why 3 log n? He is asking me why did I say 3 log n. When I am searching, I start from here (Refer Slide Time: 19:06) start at the root and then whatever key I have I compare it with the keys in a node. Here this node has only one key but it could have 3 keys in it. Then I have to compare against each of those 3 keys to determine which particular branch to take out of that node. If it has 3 keys then there are 4 different branches, which should I take? To determine that I need to make 3 comparisons. 

Let us look at insertion in a 2-4 tree. I have this largest example that I am going to be using to show you the process. Is this a 2-4 tree? This has 4 children, this has 4, this has 2 children, this has 2 children and this last one also has 2 children (Refer Slide Time: 19:56). I have shown the node with 3 locations in it. So each node will have space for 3 keys and 4 pointers. 

(Refer Slide Time: 19:43)

 

So it has only one key but I have shown each node having space for 3. So the first element I am going to insert is 21. How do we insert? We insert just as in the case of the binary search tree. First we will search and wherever our search terminates, if we found that element then it would say that it already exist. You will not insert then but wherever the search terminates we would insert the element there. I am trying to insert 21. So 21, I come and compare. Here 21 lies between 13 and 22 which means I am going to take this branch out. So take this branch out, I compare it against 18. It is larger than 18 so I am going to take this branch out (Refer Slide Time: 21:00). So take this and it goes and sits in that particular node. Why does it go and sit in this node? Why did not I compare with 20 and say that let me go down further. This is a leaf node. I could also have said it, I compare it with 20 then I try to go right but right node is empty. The right pointer is a null pointer because it is not going down any further. So I know that this is the place where I have to insert and this is empty and there is space here so I just put it in. 
 
We would not put it next to 18. We would continue till we can not go any further. This is what happens in the binary search tree. (Hindi conversation) you keep comparing till you hit a null pointer and then you put it there. So till we hit a null pointer. We compared 21 with 20, let?s say we were trying to go right but this is a null pointer and so we put the node here. Now you are wondering how I am going to use this space. We will see how we are going to use this space. If this was already filled (Refer Slide Time: 22:22) (Hindi conversation) you will have to wait till the next slide. So if there is empty space no problem you can do the insertion.  

Let?s say now we try to insert 23. So 23 lies between 22 and 32. We are going to take this link out. We took this link out, 23 is less than 25 so we come down here. In a node we will try to keep the keys in a sorted order because only then. So 23 should come at this place. What should I do? Move 24 to the right and 23 will come at its place. Insertion actually happens at the very last node that is at the leaf nodes. The other way I could think of it is 24 was here, I compare 23 with it, I tried to go left that is null pointer. So that means I have to insert at that node itself. We are trying to insert 40. There should be no problem with 40. The 40 is more than 32 so I go right, I come here 40 is more than 35 so again go right and there is space here. So I compare 40 with 39 it is a null pointer which means I have to put it right here. There is space so I put it there. 

(Refer Slide Time: 23:48)

 



(Refer Slide Time: 23:58)

 

If I am trying to insert a key and there is no space available in the node in which the key should go. Then what do I do and that is an example. When I am inserting 29 that is the kind of thing would happen. So 29 between 22 and 32 so I follow this. 29 more than 25 so it wants to come and sit here between 28 and 30 except there is no space here.  

(Refer Slide Time: 24:20)

 

So this is what we are going to do. We are going to split the node. Which node are we going to split? The one containing 26, 28, 29 and 30. We are going to split it in to 2. Let?s say these are 4 keys, the two smaller one will go to the left and two larger one will go to the right and we will remove this node. We need to link up this node, this should be the children of this guy here. 
(Refer Slide Time: 24:53)

 

Because these are all originally children of this node. So this should also be a child of this node but now its going to have 3 children. But how many keys are there? One, so we need one more key. If it has 3 children, it should have 2 keys. So which key I should put here? I am going to promote (Hindi conversation). So it is best to just promote up 28. That is what I will promote 28 here.  

(Refer Slide Time: 25:54)

 

I could also have promoted up 29. You understand why 28 and why not 26. If I had promoted up 26 what would be the problem? Then the search property would not be valid. So I have to promote either the largest key from this node, up here or smallest from here. This will become the new structure. We have promoted one key to the parent and inserted that key. We could insert the key in to the parent because there was a space in the parent. But it might happen that when I am trying to insert the key in to the parent, the parent does not have any space. (Hindi conversation) 7 less than 13 so we have go to left. The 7 between 3 and 8 so we should follow the second pointer. It should come here and we want to put it here except that there is no space, so we will split this node. Two nodes created. 4, 5 go to the left node 6, 7 to the right node. We get rid off this. These are the 5 children of this node (Hindi conversation). 

(Refer Slide Time: 27:20)

 

So if the parent node does not have sufficient space then it is split. So we split the parent node in to two. 3 and 5 will go to the smaller one, 8 and 10 will go to the larger one that is to the other node. I have 1, 2, 3, 4, 5 children and they have to be made children of these guys. And one of the smaller (Hindi conversation) that has to be promoted up because when a split happens then we take the largest key of the smaller node and promote it up. The first two children would be made the children of this node. The right three would be made the children of this node. Why two of this and three of this? Because 5 is going to be promoted up. 

So that means there is only one key left here which means that this can have only 2 children. The first two children will go here, 5 is going to be promoted up so this will have two keys which means 3 children. So these 3 would be its children and we promote 5 up. So we split this node (Hindi conversation) that we split first then we went and split the parent and now we will see the split happening here also. Because this does not have any space. So we will split it in to 2. The 5 and 13 would go in to one node. 22 and 32 will go to the other node. This will disappear (Refer Slide Time: 29:41) and now 1, 2, 3, 4, 5 these are five children.





(Refer Slide Time: 27:35)

 

So 13 will get promoted up now. So the first two will become children of this and the next three would become children of this and 13 get promoted. But where does it get promoted? There is nothing above. So we will create a new root, eventually we may have to create a new root. That is what going to happen now. We create a new root, 13 goes up there and these two become the children of this. 

(Refer Slide Time: 29:40)

 




(Refer Slide Time: 30:05)

 

So if we create the new root the height of the tree increases by one. You understand the procedure so you first come down the tree till you hit a leaf. You will try to put the key there, if there is space nothing to be done. It is very simple. If there is no space then you split that node and then we decided that the two lower keys will go to one node and two higher keys will go to the other node. The largest key in the lower part would be promoted up. So when we split there are four keys in a node. There were four keys (Hindi conversation) to which means the second key of those four is the one which will get promoted up. Promoted up means we are trying to insert the key in to the parent node. If we are successful if there is a space no problem, otherwise repeat the split process of the parent node. So split it and this split might cascade all the way up to the root. If it cascades up to the root and the root also gets split then we have to create the new root. That is it. 

















(Refer Slide Time: 32:33)

 

How much time does insertion take? So search was very clear. For search we said it will take order log n time. How many splits can be there in the process of insertion? Its at each level we might be doing the split. How much time does one split take? How much time does it take for me to split a node? I will create some two nodes (Hindi conversation) constant time independent of the number of node. So each node split takes constant time. So the total time is order log n (Hindi conversation). 

(Refer Slide Time: 33:45)

 

So now let?s look at deletion. So suppose I wanted to delete 21. First as in the case of the binary search tree, first you have to search for 21. Find out where the key is. In the case of a binary search tree, recall deletion require 3 different cases. If the key was at the leaf then we just knock out the leaf, nothing to be done. If it was at the internal node then you had to distinguish between one child and two child. The one child case is not really happening here. So it is only two child case that we have to be worried about. If it is such an internal node with two children then what did we do? We found the successor or predecessor of that key lets say we found the predecessor and we move the predecessor to that and delete the predecessor. That is what we would mean. We are going to do something similar here.
 
Let us see. Suppose I wanted to delete 21. So 21 there is no problem because 21 is in a node. We will search for 21. We come down here, go right, go left, go right and I find 21 here. Why is there no problem in deleting 21? It is in a leaf node, I can just remove it and I can remove it without violating the property of the 2-4 tree. In a 2-4 tree we require each node has at least one key and at most three keys. So after deleting, this will still continue to have one key, so no problem. That is what is going to happen. I have not shown the process but this 21 will get deleted, we just knock it out from here, nothing to be done. If the key to be deleted is an internal node, is at an internal node. For instance I was trying to delete 25. I search for 25, I find 25 right here. What do I do? I am going to swap it with its predecessor. What is the predecessor of 25? How do I find the predecessor of 25 in the case of a 2-4 tree? I will go left and then keep going right (Hindi conversation) then I find the largest key in this node. 


(Refer Slide Time: 37:35)

 

I find the largest key its 24, so predecessor of 25 has to be 24. I am going to swap this two. Then I am going to remove the 25 from here. This is a same thing, I can remove the 25 from here. Why, because its a leaf already. There are two keys in the leaf, so if I remove one there is no problem. Note that the predecessor will always be in the leaf in this case. Not in the case of a binary search tree. Let us check this point out. In the case of the binary search tree, the predecessor of a node need not be a leaf node. Suppose this was my binary search tree, this was a node 10. I am finding the predecessor so I go left and then I go right. This could have been my binary search tree. What would be the predecessor of 10? It would be this guy (Refer Slide Time: 36:15). This is not a leaf node but here the predecessor will always be a leaf node. Why? How do I find the predecessor? I go left and then keep going right, keep taking the right most child. So when will I stop? When there is no right child. What does it mean when there is no right child? When my right child is null then that means all the children are null. Because I cannot have a situation in which there is a node which has a key, it has no right child but it has left child. This is not permitted at all.  

If there is a key then we will have two children, if there were two keys then we will have three children and so on. So my predecessor would always been a leaf, so I would just remove that leaf node and I would be done. So that is what I do. Recall I was deleting 25, so I swapped 24 and 25 and now I have to just get rid off 25. So I will get rid off 25 by that. So 25 disappeared from here. It is a very simple case. As you can imagine, problem were arising when I am in a leaf, I am trying to delete a key from a leaf which has only that one key in it. Then that leaf becomes empty so what do I do now? So let us look at that. 

If after deleting a key, a node becomes empty then we borrow a key from its sibling. Let us see what that means. Suppose I am trying to delete 20. So I search for 20, I come down in this manner. I reach here, now if I delete 20. So 20 is removed, problem is this is an empty node not permitted. What will I do? Borrow from sibling. What does borrow from sibling mean? This guy has only one sibling. So can I borrow 15 from here to here? No, it is disaster because search property is not going to be valid. So we are going to do something like a rotation like we did in an AVL tree. What does it mean? 15 goes up and 18 comes down. 

(Refer Slide Time: 38:23)

 

Then the next thing you are wondering is, if I can not borrow from my sibling. When can I not borrow from my sibling? When the sibling has only one key in it. For instance now if I were trying to delete 18 then that would be a problem. Let see. If sibling has only one key then we merge with the sibling that is we combine with the sibling. Suppose I was trying to delete 23. So 23 is here if I delete it this is an empty node. So I try to borrow from a sibling. 

(Refer Slide Time: 39: 44)

 

There is a small catch here (Hindi conversation) one and two, but I cannot borrow from this one. You see why it is. Because if I have to promote something then 28 is going to come down but this going to jumble this. So when I say borrow from a sibling I really mean and adjacent sibling. (Refer Slide Time: 40:35) I can borrow from here, so when I am here I can borrow from here, if I am here I can borrow from here. If I am here I can borrow from here or from here. But here I can only borrow from this guy. Did everyone understand why this is required? I try to borrow from here but if I borrow from here that is 26 goes up and 24 come down this is going to become empty. So that is not going to solve our problem. What we are going to do is merge. These two are going to merge, combine but if these two combine then the number of children here of this guy will become two which means (Hindi conversation). So the key in the parent node which separates these two siblings is this key, which is separating these two siblings is going to move down in to the merge node. 

Let us see, I create a new node which is the merge of these two nodes. This 24 moves down and this 26 also. These are the only keys in the new node. Because there was one here, there was none here and there was none here. Two keys in all so they come and sit here. These two will now disappear and this becomes the child of this node (Hindi conversation). Now what can happen? Essentially what we have done is we said we are going to one of the keys from the parent node is going to come down. But what if there was only one key in the parent node. The same as before (Hindi conversation) and so on. So moving the key down from the parent node corresponds to deleting the key from the parent node. 
  



(Refer Slide Time: 43:02)

 

This procedure will be the same as that we have done so far in this leaf node. But it can lead to the cascading. Cascading as in (Hindi conversation) we will see that happening in this example (Hindi conversation). This is the only child left of its parent (Hindi conversation) sibling is also only one key. So we are going to merge with the sibling. We are going to create a new node which is going to get the sibling key and the key from the parent which is 22. This is the situation now and we will delete this and this (Hindi conversation). 

Because of these deletions, height can reduce by one. After all we said height (Hindi conversation) log4 n is less than what it was, then that means height has to shrink and that would happen. There are just very few concepts that we really handled and insertion (Hindi conversation) or deletion (Hindi Conversation). (Hindi conversation) no point is doing that. So first you try to borrow if not successful then you merge. So let us conclude today?s discussion. Height of a 2-4 tree we have seen is log n. 















(Refer Slide Time: 48:15)

 

This would actually be a theta of log n because it is at least log4n and at most log2n. So as far as deletion was concerned we have not looked at running time for deletion yet. But you can see that is also log n. Why? It was first we come down the tree to search for the key that is log n and then we keep moving back up. At every step we might go all the way back up to the root. So another log n step. Each step where either borrowing one from the sibling or we are merging with the sibling. But all of them are constant time operations. Borrowing would correspond to (Hindi conversation) constant time (Hindi conversation). So what you have seen is that search, insertion and deletion all take order log n time in a 2-4 tree. So why did we come up with this very complicated data structure? Why are we doing all of this? There is another reason, we are going to see another data structure called red black trees. And that is also very fast data structure for implementing dictionaries. What we are going to see is that what we learnt about 2-4 trees today is going to be very helpful in understanding how the red black trees functions. So we are going to look at this in next class. So with we will end today?s discussion on 2-4 trees.  
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 14 
Red Black Trees 

In today?s class we are going to talking about red black trees. We are going to spend some time discussing about red black trees, what is the relation to 2-4 trees and what will discuss next and then we will see the process of deletion in red black trees. It is going to be slightly more involved process so we will be spending fair bit of class on deletion, in the next we are going to handle insertion in red black trees. 

(Refer Slide Time: 01:17)

 

So what is a red black tree? So red black tree first it?s a binary search tree. So we take the node of the binary search tree and we color them red and black and then it becomes the red black tree subject to certain properties. So first the root has to be colored black. That is important. A red node can have only black children that is another property that a red black tree has to have. 

If there is in a red node then its children have to be black. Now one thing we are going to do is in our binary search tree if a node does not have left or a right child, so we will create the left or a right child by creating what we will call an external node. And I will show you what I mean by that. We will create an additional left or right child, if it?s a leaf node of the binary search tree then it does not have either a left child or a right child. So in which case we will put in a two external nodes below that leaf node and make it the left and the right child.
 

(Refer Slide Time: 01:40)

 

So external nodes we are not going to color them. So it?s only the non-external node, original nodes of the binary search tree that we are going to color red or black. We define the black depth of an external node as the number of black, its number of black ancestor?s. So what that means is we are going to take an external node and we are going to walk up the external node towards the root and the number of black nodes we encounter is the black depth of this external node. And the one key property of the red black tree is going to be that every external node has the same black depth and that we will also refer to as the black height of this tree. So lots of definition but we will see what this means, a couple of examples.

(Refer Slide Time: 06:42) 

 
So this is two examples of red black trees. So let us look at the one here. So what do we have? This is the binary search tree on 5 nodes I have colored the nodes black and red. The root is black, a red node has only black children, black node can have black or red children. But a red node can have only black children. As you can see this node does not have any left or right child. This does not have any left right child; this does not have any left or right child. So I am going to add them (Refer Slide Time: 04:20). These I will call external nodes, they are not colored. So all the external nodes in this class today will be shown with the square boxes. Now what is the black depth of this external node? 1, 2. It is the 2 ancestors which are black, similarly the black depth of this are external node is 2, of this is 2, of this is 2, of this is 2 and of this is also 2. So this is a red black tree.

So we will add this external node also to node which has only one child. So let us look at this example and that will be clearer. So once again the root is black, a red node has only black children. That is the key, the red node can have only black children. Black node can have red children or black children. Now we are going to add the external node so as you can this node has only one child. So we will need to add an external node here also. Similarly it will need to add 2 external nodes here, one external node here and of course these will take 2 external nodes and so on. So these are the external nodes we will end of handing. [Refer Slide Time: 05:43] (Student Conversation). A black can have black children but red cannot have red children, it?s not symmetric. 

[Student Conversation: you going to take first case, staff: ya, Student: it has black and both the children left and right to be red right child is if we place the external node and we count the black height so it will be one Staff: red so then it will not be a red black tree that?s what you say. ] 

So if I had colored this red then this would not be a red black tree. The black height of this guy is 2. What is the black height of this guy, of this tree? 2, I will take any external node and look at how many ancestors are black and we will see that its 2 ancestors which are black. And this is true for every external node; if I look at this it also has two black ancestors, this also two black ancestor and so on. So this also has a black height of tree. So what are the key things we have to ensure? First that no red node has a red child, second root is black, third black height of the every external node is the same. Black height or depth I sometimes use the term depth to say how far it is from the top. So black depth of the every external node is the same, the black depth that?s what we will call the black height of the tree, depth of the node and height of the tree. 

And these are the two examples of trees which are not red black (Refer Slide Time: 07:24). Why this is not red black? This red child, this red node has a red child. So this is not, so even I have put in the external node like this, this will not be a red black tree. Why is this not a red black tree? So the problem here is what we call a double red problem. We will use the term quiet often, double red as in two reds occurring consecutively one after the other. 



(Refer Slide Time: 07:55)

 

Here I add the external nodes, the black height of this node is, the black depth of this node is 2, of this node 2, of all of these is 2. This is also 2, this is one, this is culprit, for this reason this is not a red black tree. So black height is not uniform, black depth is not uniform. So let?s look at what is the height of the red black tree is going to be? So let?s say h is the black height of a red black tree on n nodes. What does it mean? 

(Refer Slide Time: 08:23)

 

If I take any external node and I will look at, count its number of ancestor that it has exactly h black ancestor. When will the number of nodes in this tree, suppose I just tell you that this is the tree of black height h, when will the number of nodes in the tree be small? When the smallest? When everything is black, you don?t want any red. You want as few nodes as possible. So why include your red nodes and increase the number of nodes. So let?s have everything black, if you have everything black then this becomes a complete binary tree of height h or my red black tree why because all the external nodes have to be at the same level now. If they have to be at the same level all the external nodes then it will become a complete binary tree of height h. If I have a complete binary tree of height h then the number of nodes in the tree is 2 to the h-1. So the smallest number of nodes possible in a red black tree of black height h is 2 to the h-1. When is the number of nodes the largest? When every black node has 2 red children, but red node cannot have any red children. So then it will have to have black children which will have to have red children and so on. We will have these alternate layers of black and red. So we start with the black then we have red layer then we have black layer then red layer and so on.
 
If the number of black layer is h then the number of red layer is also h. So the total height of the tree becomes 2h and so number of nodes in the tree becomes 2 to the 2 h-1 which is about 4 to the h-1. So this implies that h lies between log of n to the base 4 and log of n to the base 2, just +1and -1you can figure out. So if I give you a red black tree on n nodes, so I just turn thing around then if I give you red black tree on n nodes then its height is at least log n to the base 4 and at most log n to the base 2. We have seen exactly of the same property for 2-4 trees. We said its minimum height is log n to the base 4 and maximum height is log n to the base 2. The same kind of the thing is happening here. 

So now you can immediately see that if I give you red black tree then to search in the tree, will take how much time now? Log n, why because first remember it is a binary search tree. So it has the search property, so I can do the search in this regular way that is start from the root, compare key with the root, go left or right and so on. So I can do my search and the time taken for that search is just the height of the tree and the height is no more than log n base 2. So that?s the time I take. 

Now let?s look at the correspondence between red black trees and 2-4 trees. So in particular I am going to say that given any red black tree, I can convert it into a 2-4 tree. So what are we going to do? We are going to take a black node, that black node, how many red children will it have? It?s a binary tree. So it might have both its children could be black, one child could be red, the other could be black or both could be red, both could be black, both could be red. One could be red, one could be black 	or you could have external node also, one could be external node but in any case the number of red nodes cannot exceed 2. It will be 0, 1 or 2. So we are going to take this black node and its red children and combine them in to one node. How many keys there will become in this one node? At most 3, when this node had 2 red children. Let?s see.  

So what I am saying? I am saying, we have a black node and we look at its red children. Suppose it had 2 red children so I am going to combine it in to one node, all these 3 I am going to combine in to one node. So this one node will have 3 children. Suppose it had only one red child and then it would be 2 keys in here and if it had no red child then there will be only one key here. 
(Refer Slide Time: 12:00)

 

So each node so formed has at least one and at most 3 keys. Furthermore black height of all the external node is the same, so in the resulting 2-4 tree all leaves will be the same, will be at the same level and this might has not too make sense. What are we saying? Black height of all external nodes is the same. So if I start from the external node and go up the tree, the number of black nodes I encounter would be the same. No matter which leaf I started from? So each of those black nodes is now part of a unique 2-4 node. Because I took a black node and I took its red children and I combine in to one. So each of a black node is a part of a unique node of a 2-4 tree. So as consequence when I start from a leaf and I go up to the root I encounter the same number of nodes in a 2-4 tree which means that all the leafs are in the same level of the 2-4 tree. 


















(Refer Slide Time: 17:23)

 

We will show an example with which this will become clear. So this is my (Refer Slide Time: 15:36) red black tree I am going to convert it in to a 2-4 tree. So I start with the root node, I will look at its red children it has two red children. All of these will combine in to single node of my 2-4 trees. Now I am going to look at another black node which is this, look at its red children it has again 2 red children so all of this combine in to one. And they combine in to one like this (Refer Slide Time: 16:16). So as you can see when these combine in to one, these 4 links that I am going out of this combination will be the 4 links going out of this node. When I took this entire thing let me go up, there are four, 1, 2, 3, 4 children of this (Refer Slide Time: 16:47) combined structure. They would be the 4 children of this node. So this is the first one, then I look at this. How many red children does it have? Only one, so they would combine in to one, so I get another node which is 5, 7. This becomes a second child, as you can see this is more than 4 so this will be more than 4 here. 

And what will be my third child? It would be just this black node, it has no red children. So I create a node with only 11 in it and the fourth one 19 and 17, so I create another node with just 17 and 19 in it. So this is the 2-4 tree I get. Now you can see the black height was 2, it was the same for every external that was the black tree and I get 2-4 tree of height 2. So I am giving a proof by example but this should be clear why I will get the 2-4 tree? So the property of a 2-4 tree, one critical property of the 2-4 tree is that all leaves are at the same level. Just saying that every node has 2, 3 or 4 children doesn?t make it a 2-4 tree. All leaves have to be at the same level and that is in ensured because of the fact that the black depth of every external node is the same. So this is how we can obtain a 2-4 tree from a red black tree.




(Refer Slide Time: 18:29)

 

We can also go the other way round, that is given a 2-4 tree you can make a red black tree. So what are we going to do? We are going to take a node of the 2-4 tree, replace it with 1 black node and then appropriate number of red nodes. What do you mean by appropriate number? If they were 2 keys in this 2-4 tree node then I will have 1 red node. If there were 3 keys then I will have 2 red nodes. If there was only one key then I will have no red nodes and these red nodes will be the children of the black node. So first I put down the black node then I put the appropriate number of red nodes. 

Now this ensures that I will not have the double red problem. Why? I will not create one red node as a child of another red node, because what am I doing. I am first taking a node of my 2-4 tree, first putting down a black node and then putting 0, 1 or 2 red children and then when I take the next node of a 2-4 tree which is the child of the previous one. Then once again I will first put down the black node. So I will not have 2 reds consecutively happening that we will see in the example shortly. Furthermore what?s going to happen is that for every node of the 2-4 tree I am putting in one black node.

So since all the leaves of the 2-4 tree are in the same level. The black height of the resulting red black tree would be uniform; the black depths of all the leaves would be the same. So let?s take an example. Ignore this for now, this should have come later. This is my 2-4 tree (Refer Slide Time: 20:21). 








(Refer Slide Time: 21:52)

 

As you can see each node has 1, 2 or 3 keys which mean 2, 3 or 4 children and the height of the tree is 3. Let?s see. So there is only one key here, I will just create a black node and nothing else no red nodes. Let me take this one, so I will create 1 black node and it will have 2 children, 2 red children. What should be the key in the black node be? 8, 3 and 10 means 2 children. Then I take this one, 18 just one black node with 18, no red children. Let me take 1, 2 so this would have 1 black node and 1 red node. I have an option; I might put 1 as the black and 2 as the red or the other way round. 
So I am going to put 2 as the black and 1 as the red. For 4, 5, 6; 5 will be the black and 4, 6 will be the red, black 9 nothing else. Here 12 black let?s say and 11 red, 14, 15 black, 14 red and 20. So now are you convinced that you will not have double red problem? Just by the way we are doing things. 

When I take a node and I create some thing in the red black tree I first put down the black node and only then I put down the red node. So at the next level I will first put down the black node. So it will never have two consecutive reds. Furthermore the black height of this red black trees so if I have to draw the external nodes and count their ancestors so if suppose there is an external node here, it has three black ancestors which is as same as the height of the red black tree because of that all of them would have same black depth. If I were to look at this external node which is here then it has black depths of 3. Everyone would have the same because if it is going from here to here then it corresponds to going from here to here because each of those black nodes is one level here. So the black height is same as the height of the 2-4 tree. So you understand what red black trees are and you understand how they are related to 2-4 tree. 

Basically there is 1 to 1 correspondence. Given any 2-4 tree I can create a red black tree, given any red black I can create a 2-4 tree. So in fact the operation of insertion and deletion in a red black tree are exactly the same as you do in a 2-4 tree. So we are just going to mimic those operations in this setting and that was one of the major reasons for doing the 2-4 tree. So we are going to mimic the operation but of course it will require to do it carefully. So I am going to look at the operation of deletion because this is slightly more tricky operation.

(Refer Slide Time: 28:22)

 

So how do we delete in a binary search tree? The first step of the deletion is the same as the binary search tree which is let we first search for the node, you are in to five other node. If the node is a leaf then you just deleted. If the node is an internal node then you find it successor or predecessor, you swap and then you delete the successor or predecessor. So now the successor or predecessors suppose we are talking about successor so successor is a node which does not have a right child. 

If it does not have a right child then that means we would have its right child which is an external node, we put an external node there. So that means the node that I am deleting is always the parent of some external node. If it?s a leaf even then its parent of an external node. Otherwise it doesn?t have a right child if you were talking of a successor. If it doesn?t have a right child we put in an external node. So it?s always parent of an external node. So these are the three settings we could have of the node that we are deleting, these two corresponding it to being a leaf.

So either if the node is a leaf either it is a red leaf or black leaf and the third setting is when the node that we are deleting does not have a right child. So it does not have a right child which means right child is an external node, if it does not have a right child but has a left child then this left child has to be red. Why? If it were to be black then black could not be remain consistence. Yes why? Because if I look at these external node verses this external node, [Hindi] you follow what I am saying. If this were black then if this node has 10 black ancestors then this one would have 11 black ancestors. Why? Because all its black ancestors are also ancestor of this guy and this has one more ancestor, this has to be red therefore.
If this is red then it cannot have any red children but it cannot have any black children either because if it had black children then once again we could have a same problem of black height not being the same. So this is the entire structure that we would have. If this is the node that I am trying to delete and it did not have a right child and it was not a leaf either then that means it?s right child is an external node, we said that already its left child will be a red node which would be a leaf. So then in this case the node that we are deleting is really parent of a leaf node. 

In this case the node we are deleting, in each case we are deleting the node here. In these two cases the node we are deleting or leaf node themselves and this case it?s a parent of a leaf node. If 19 is a red node. Can 19 be a red node and 17 be a black node, Black height would not be the same. He is saying suppose this is red and this is black, this is red and this is red, it cannot be. It?s a red double red problem. This is red and this is black. [Hindi]. So now this is an easy case to handle. If I am trying to delete this node what should I do? It?s a leaf; just delete it, nothing to be done. It cannot create a double red problem, if I delete this. That?s what I have done, just delete and just replace this entire thing with this external node. It cannot create a double red problem and it cannot also change the black height of any node because it?s a red node after all.
 
Similarly this case is also easy. What will I do? I am trying to delete this 19. So what should I do? 17 can move there and this entire thing can be removed. So I have not created a double red problem [Hindi] as it was before and this earlier had 10 black ancestors, it still has 10 black ancestors and this node also has same number of black ancestor. So the only tricky case is this one. By the way what do these two cases corresponds to in the case of 2-4 tree? This and this (Refer Slide Time: 30:30). What are we deleting? The key that we are removing is from a leaf always in a 2-4 tree and this corresponds to the case when the key that we are removing is part of node which has at least two keys. 

The key that we are removing is in a node which has atleast two keys and that was the very simple case there. If this was my 2-4 tree node and this had keys let?s say 3 and 5 and I was trying to remove 5 or 3 its very simple operation nothing needs to be done and this case as well as this case both of them correspond to this (Refer Slide Time: 30:54). 













(Refer Slide Time: 31:25)

 

Why does this corresponds to this? This is the node, even here this has to have a black parent and so this is the node then corresponding node in the 2-4 tree and this is a corresponding node, it might be even larger actually. It might have 3 keys ,this might also be red. [Hindi]. This however is a tricky case, this corresponds to a single key in a 2-4 tree node. So in the 2-4 tree node, if I remove this key then the node becomes empty. That?s a problem and here this would correspond to change in the black height. 

Hence we can assume that the door deleted is a black leaf that is this is the only case which is really interesting for us and we removing this reduces the black depth of an external node by one and so in general we are going to assume the following. We are going to look at the following step. There is some sub tree whose black height is reduced by one and we want to re organize the tree to take care of that. 

Why I am saying sub tree (Refer Slide Time: 31:25) where this is a sub tree. What is going to happen is that as we do reorganizing this tree could become larger and larger. This is a black height [Hindi] and we will see that. So it?s going to have a bunch of cases as procedure. 











(Refer Slide Time: 34:44)

 

So let me assume that this is a tree [Hindi] black height because of the deletion once [Hindi]. So what I am going to do, I am going to look at the parent. So this is the tree [Hindi] root has sub tree. I am going to look at its parent. The parent is let?s say [Hindi]. So first I will check whether a is a black or a is red depending upon this I will have two different cases happening. If a is red so this is a picture. What about the other child of a? It has to be black lets says its b. let?s look at the two children of b. Let?s call them c. 

You do not know whether they are black or red. a is just name for this node nothing else. This is the tree whose black height is reduced by one, the sub tree. I am just looking at the root of the sub tree and its parent (Refer Slide Time: 33:44). So a is the parent. Let me just clarify this point. So what you are saying is we are interested in when we are deleting this black node which has two external nodes and this has some parents somewhere.

When I delete this entire thing I replace it will one external node here. So essentially this sub tree, this node and its two external node children [Hindi]. It is getting replaced by this, black height zero [Hindi]. The problem why we have to formulate this way is that this is going to in some cases what is going to happen is this tree is going to expand. So just take this for now and we will see [Hindi] if you are confused then just think of this has the single node with two children like this picture. You can think of this blue triangle as that, its black height is reduced by one. So this is one possibility. If this is red and this child is black and we do not know what its two children colored are. The other is that if this is black, a is black then b could be red or black. 

So depending upon what the color of b is we will have different cases here, depending upon what the color of c is we will have different cases. So these are two cases either both of these are black. That could be one case.


(Refer Slide Time: 37:30)

 

So I have not shown them black, I am just showing you what the various cases are then we are going to handle these cases separately one after the other. So one case when both of these are black, the other case is when one of these is red. This is let?s say red and these is one case for us. This is just a part of the tree.  

There would be a left sub tree here, there would be some sub tree is hanging here and so on and of course this will have something, this will have a parent and more happening here (Refer Slide Time: 36:28). This is the case when both are black so I have not shown them here but this will be another case. Let?s come here. Now depending upon what b is we will have two case red or black. So b is red then this is the red node then it will have a right child which is black, it?s right child cannot be a red and this will be black and now I am going to look at two children and depending upon what the two children are I am going to have two more cases. The other option is when b is black in which case we will have two children and depending upon what these two children are once again we will have two cases. So here there is one case when one of this two is red. 

So that is the case like this and the other is when both of them are black which means I will just look at the scan structure. Similarly here, there would be one case when both of them are black. When one of them is red then I will have this and when both are black I will have this. So in this manner, this is just a kind of a starting picture to show you that there will be six cases that we will be looking at and we will see what the tree organization, re organization has to be done. So I am going to refer this as case 1.1, 1.2 this will be 2.1.1. So this is the first case we are going to look at. So this is what the picture would be. So a b c and basically a b c and this is the tree sub tree whose height is gone from h to h - 1 let see the black height is gone from h to h ? 1. 



(Refer Slide Time: 39:53)

 

Now what will be the black height? So all these heights are now black heights. What is the black height of this sub tree? I have written h ? 1. Why is it h ? 1? Earlier this had a black height of h, what should be the black height of this? h - 1 because there is one black node here already. So this will have the black height of h ? 1.

Similarly this will have black height of h -1, this will have the black height of h -1 and if a node is red then both its children have to be black. Red node cannot have a red child; it?s a double red problem. The other possibilities c is the left child of b so it?s a symmetric thing. Now how are we going to reorganize this tree? we are going to reorganize this tree in this manner. So it?s basically a rotation, b goes to the top, c goes here, a comes there and these have not labeled these things but you can understand 1, 2, 3, 4; 1, 2, 3, 4 in this order will set there.

We will see in this kind of reorganization when you are taking of AVL trees. Now what does this correspond in the case of a 2-4 tree? Let?s look at it. So what this corresponds to? This picture is basically that this node was empty that?s why the height went down. In the parent node I have an a and it is red which means the parent node in the 2-4 tree has at least two keys. [Hindi] This will be always black [Hindi] why is this rotation? Student: c is greater than b. You are right perhaps, so you are saying a is here [Hindi], so it will become a c and b. So ignore this one, it?s for this one, this picture (Refer Slide Time: 41:10). 

Now let?s see what this corresponds to in the case of 2-4 tree because that?s where all of this motivation is coming from. So this is a red node which means its parent is a black. So when I had created the 2-4 tree node I would have at least two keys in the 2-4 need tree node corresponding to this guy, at least two, may be three but at least two. So that is this one, so it could be either two or three I put the a here. Now in the 2-4 tree node corresponding to this guy, actually I took this one, so I would have b and I would have c (Refer Slide Time: 41:47). so I would have b and c may be there is another key if this guy was red. So this is what the picture is and then what do we do in the case of a 2-4 tree? I have a problem because I don?t have any key in this node, what do I do? I borrow from my sibling. My sibling can lend me because it has at least two keys. 

So it is going to lend me one. So one of the key is going to go up from here to here and one is going to come down from here to here. So that?s exactly what is happening. So [Hindi] one goes up, let?s say the b goes up, c remains here. When I am looking at b c, so this corresponds to this. So let me correct this in a second (Refer Slide Time: 43:00). So what we are saying? Suppose we were to keep this one in picture. So lets remove this one now and then this should be c and this should be b. 

Now it?s okay. So let?s keep this in mind. Now what?s going to happen, we have c and we have b here so which is the one which is going to go up? c and a is going to come down. So in the new node that I get here, I would have only one key a. In node that I have here c is not going to be there and in this node a will get replaced by the c. In this node a is the only key which means a is black. In this node b is one of the key and if there were one other key here either b is only key in this node or if there was one other key here then it is already sitting here, we don?t have to worry about that. So b becomes the black node and this c why should c be a red node because there are more keys here. So c can continue to be a red node. so this is just for motivation but if you have look at this. This is good enough. 

We have taken care of the height problem, this is my new re coloring, this was red, this is also red so they cannot be a double red problem. Why? Because there is a double red problem if the parent is red but the parent is not red. If the parent is red then there is a double red but if the parent is red then the parent of this was also red which means there was already a double red problem. So there is no double red problem. Let check the height business. So black height is h -1 and now what is the new black height of this sub tree? 

So for all these external node it will be h, for all of these also it will be h, for all these will be h, for this it will be h, the black height is h (Refer Slide Time: 45:15). What was the black height of this guy? h - 1 + 1, h - 1 + 1, h ? 1+ 1 and this was h to begin with [Hindi]. So the problem is taken care of. You have not introduced double red and the black height has been restored. What case was that? So that was very first case. Let me just show you so it was this one (Refer Slide Time: 45:58), so we have to go through this one (Refer Slide Time: 46:02). So we quickly now start going through them (Refer Slide Time: 39:53).

This is when parent is a red node, so this is what is written. So parent by parent I mean this is a sub tree whose height is decreased, I am looking at the parent of this root it?s a red node. It has a child which is black then black child has the red child. This is what we do. So the other thing is when it does not have any red child, b does not have any red child that would be in a second case. So let?s look at that.

(Refer Slide Time: 47:55)

 

So b has no red child. In this case the picture is a b and both of this sub tree have as the root black children. Now this went from the h to h - 1. So this has to have a black height of h - 1 and this is also have a black height of h -1. Why because originally black height of entire tree was h. So this must be h - 1, this must be h - 1. Now what are we going to do? Just the re coloring will solve the problem because I make this red and make this black. What is the black height of the resulting tree now? H, which is what the original height was. Why this was not introduced the double red? I made this red, no this could be a red. That?s why we have taken care of the other case first. 

If any one of these where red [Hindi] and what does this corresponds to for our 2-4 tree picture? This is the node [Hindi], this is the only key in this node sitting all alone and then what do we do? If this goes up then this guy becomes empty. So what do we do then? Merge, these two combine in to one single node and one key comes down from the above. So what will the key?s in the new node be? b from here, a from here which comes down from the above. So this new node is going to have b a in it which is what is being done here, this new node has b a in it. This will be one node, I take a black and look at all its red children. This will be one node with be a in it. 

So this is one to one corresponding between what we did in the case of 2-4 tree and what we are doing here and that helps to clear things. So we are looked at both the cases when this guy, when the parent was the red if then the parent was the red then we know this is a black. If this had any red children we have taken care of it in previous case, if this has no red children then we taken care of it now. So now we go to the red next set of cases when this parent is a black. So parent is a black node.




(Refer Slide Time: 52:54)

 

If parent is a black node then this could be a red or could be a black? So first considering the case that it has the red child b. If this is red then this has to be black and if this is black then this may be red may not be red. Now again some two condition I am making or some two assumption making of; this is red first and this is red. In the next case I am going to consider this was not red, this was black and this was black. This node had no red children and you see why this is required, you already seen one reason why to consider whether node has red child or not. 

So this is a picture. This height went down from h to h -1. So what is the black height of this tree? This entire black height that means was originally h +1. [Hindi] first let?s make sure that this is correct and then we will see why we came up with this and why we came with this is justified by our 2-4 tree. So let?s just check it is correct. So first you can see here that b is less than a, b is on the left c.  

C is more than b less than a so c lies between b and a and d is more than b but less than c, so d is more then b and less than c. So search property is okay and 1, 2, 3, 4, 5; 1, 2, 3, 4, 5, I just organize as before. They just go in a same manner. Now this one is a this one, it has now black height of h -1. This guy is this, its black height of h - 1 and this one is a very first one, its black height of h. So now if you look at the black depth of the external node so these guys will have black depth of h + 1. this will have h minus one plus one plus one, h +1 h -1 + 1 + 1 h -1 +1+1 (Refer Slide Time: 52:15). 

So the black depth of all the external nodes are the same so that thing is taken care of and far as a double red problem is concerned the only thing could be that this is red [Hindi]. It was only a red black tree and it had only made changes in this sub tree. So this is a valid thing and in takes care of the problem. Now where is this coming from? What is the motivation? Again this was an empty node, this corresponds to an empty node, this is a parent node, this has a and this has red child so it means that two keys a and b and this has one child here which has two keys c and d. 

So that?s what happening here. This corresponds to this node and a b corresponds to parent node and what happens now? I can, because my sibling has enough keys it will lend me one. So one of the key will go up and a would come down. Which is the key which would go up, the largest one. C would go up and a would come down, this is the picture would have b c d a and what is this corresponds to? This corresponds to exactly this b c here corresponding to this node, a this one and d this one. So everything is coming from the 2-4 tree. What would you have done in this case? This is direct one to one correspondence, you just use this thing to decide what to do here. 

So what is the case we considered here that this is black. We also considered the case when this parent was the red and then we made the assumption that it has the red child then this has to have the black child and we made the assumption that this is red. So now we will get rid of the last assumption that this is red that means this guy has no red child, c has no red child which means both of these are black (Refer Slide Time: 54:20). So we come to that key. 

(Refer Slide Time: 56:03)

 

c has no red child and so this is the picture. So this are two sub tree both of whose two root are black and so this has to have black height of a h -1 because the entire black height was h +1 to begin with same as before [Hindi] and we will see the motivation once again coming from the 2-4 tree. Though once again c is between b and a so b here, a is larger than b, c is more then b but less then a so binary search tree property is okay and the same t1 t2 t3 t4 [Hindi] t1 t2 t3 t4 [Hindi] black height check [Hindi] black depth [Hindi]. Double red problem [Hindi], these are both black [Hindi]. Where is this coming from once again? This is the node which is getting empty. The parent has a and b in it and then this has one child which has only c in it because both of its children?s are black. So now once again we cannot borrow, we have to merge and merge may c [Hindi] which corresponds to this [Hindi]. So c has no red child. Now what is the case left? We started assumption that the b is red. So now we need to work with assumption that b is black that was assumption b is red. So we come to b is black [Hindi]. This is the node we are talking of, this is black.

(Refer Slide Time: 58:07)

 

So now again the next case that it has a red child and one possibility and the other is it does not have a red child. So if it have a red child lets say this is a red [Hindi] d is between c and a. So d is between c and a, where is it coming from? This is empty, a does not have any red child which means its only key in its node and this has a red child so that means there are two here. So once again it is going to borrow one from here. So d is going to go up, a is going to come down and we will have this picture, so d is the only key in its parent, this may or may not be the only key in its node because this could also be a red but we are not bother about this [Hindi] but this only be the one key in its node because this node was earlier empty [Hindi] c does not have the red child at all. a c c no red child[Hindi].












(Refer Slide Time: 1:00:00) 

 

So now what do we have to do? [Hindi] h -1 +1h [Hindi]. I worked with the assumption that this is the sub tree [Hindi] black [Hindi]. What case does this correspond to? This has only one single key, this neighbor also has only one single key. So you kind of combine both of them, they come here and the parents becomes empty. so the parents becomes empty so you have to then repeat the process for 2-4 tree deletion in the parent which means it either has to borrow from one of its siblings or it has to merge with one of its sibling and all of that. So there was the same kind of thing happening in the case of 2-4 trees there. 

That the process continued up, the deletion process. The parent became empty now so you have to do something there and may be then again the parent became empty so you have to do something there and so on and all the way up to the root in which case you reduce the height of the tree at the end of the thing. The same thing could be happening here except that here all we are doing is re coloring a node. [Hindi]. Essentially one bit of information [Hindi]. In one of the six cases we have to go up but when we have to go up we don?t have to do too much work. We just have to do one recolor. So that?s the summary. So in all cases except the last case 2.2.2.










 
(Refer Slide Time: 1:01:45)

 

So deletion can be completed by either some reorganizing, by some rotation or by some re coloring [Hindi]. In this particular case the height of the sub tree reduces and so we need to proceed up the tree but in this case we are only re coloring the node [Hindi]. So what is happening is why is this is a fast procedure because you have to do essentially one rotation only [Hindi]. It?s a very short, so which is what makes the process really fast except for the last case in all case the height preserving [student] [Hindi].

You have to check that [Hindi], you have to check this last thing if you have to convince but this is the entire process of deletion. Next class we are going to look at the process of insertion, it?s much simpler than this. So this was the harder one but keep in mind you don?t have to remember it, if you remember the 2-4 tree which was much simpler to understand conceptually. If you remember that you will also be able to remember this process. 
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 15
Insertion in Red Black Trees 

In the last class we saw a red black tree, the correspondence of red black trees and 2-4 trees then we saw the deletion process in red black trees. This was the extensive process with 6 cases and so on. Today we are going to see how to insert the key in the red black tree also going to introduce the notion of an a-b tree. First we will define the a-b tree and then we are going to see the process of insertion and deletion in a-b tree.

(Refer Slide Time: 01:40)

 

We get to insertion. Suppose we are trying to insert a key k in to red black tree. After all a red black tree is a binary search tree. Since it is a binary search tree, first the insertion process would be like the binary search tree which means that we would try to find whether the key already exist in the tree. If it exists then we would not insert it. If it does not then we should be able to identify the place for the key to be inserted. 

We create a node with that key, we put it at that location and we have to color this node. Because a red black tree differs from a binary search tree in the fact that each node is colored and this coloring obeys certain properties.






(Refer Slide Time: 01:50)

 

Now we color this node and we are going to begin by coloring this node red. Let us say that in the slide given below the red colored node in the left side is the node that I inserted and it has a key k in it.  Which means in my binary search tree I must have come up to the black colored node and gone right, because k was larger then this key and found that this was an external node. 

Earlier in the binary search tree this was an external node. I decided to put my node to the right side of my black colored node and I color it red. I will create 2 external nodes which will be the children of this node. If the parent of this node is black then we have no problem because this node is colored red, the black height of the tree has not changed.
 
(Refer Slide Time: 02:31)  

 
The black height of the external nodes of the children is the same as the black height of the external node which was sitting earlier at the location of the key and which was the same as the black height of the other external node in this tree. Which means that the black height of the 2 external nodes of the children is the same as the black height of the all the other external nodes. That property of the red black tree continues to hold, that is primarily because we have introduced a red node and not a black node. 

The property that the black height of all the external nodes should be the same and it continues to hold. The problem could how ever be the double red colored node and that happens if the parent of the red colored node in the left side is red as it is shown in the right side of the above slide. The red colored node with key k inside it at the right side of the slide is the node I created. If its parent is red then we have the double red problem and we have to handle this problem. 

Remember that in the case of deletion the problem was arising because the black height was changing and all along we were trying to take care of that problem. We never encountered a double red problem in the case of deletion. In the case of insertion how ever we will never have the problem of black heights not being uniform. The black heights of all the external nodes will be the same. No problem on that front but the problem will be one of a double red. 

We have a double red problem. Let us see how to take care of this problem. Just concentrate on the left picture in the slide below. The k is the node that I am inserting and in the previous picture I had shown the 2 children who are external nodes and now I am just replace that by sub trees. You will see the reason for this in a short while. 

(Refer Slide Time: 04:54)

 

We would have a double red problem if the parent of the node that we inserted is red. This parent which is node a is colored red. Clearly the parent of the node a must be a black. If the parent of this node was red then there was already a double red problem in my tree. So the node b must be a black and the first case that I am considering is when its other child is black which means that the sibling. 

The k is the node that I am inserting, the sibling of its parent node a is black. That is the case I am considering. We just do a simple rotation. Note that a is larger then b and a is smaller then k so I can put a in the middle, b on the left and k on the right. I get a kind of a tree which is on the right side in the slide above. The a will be colored black, b and k would be colored red.  

What is the black height of the tree on the left? The black height of the tree on the left is same as the black height of the tree on the right. If you take any external node then its black height was the black height of the external node plus one. And for the external nodes on the right, its height is still the same as the black height of those external nodes plus one. 
 
(Refer Slide Time: 07:48) 

 

Would the black height of all of the nodes be the same? The black height of all the external nodes is indicated in the slide above. You can see that the black depth of all the external nodes. Anything below the node k will be (h+1) +1(on the node a) which will be h+2 and below the node c will be h+1(on the node c) +1(on the node b) which is h+2. So the black height problem not there at all, it is uniform. 

Only thing we have to worry about this transformation is whether we have introduced a new double red problem, and we have not. We would have introduced a new double red problem if the root of any one of these 3 sub tree under node a and k was red. But if the root any one of these 3 sub tree was red then that means we had 2 double red problems and not 1 double red problem. If the external node under the node a was red and also the node a was red, then it is a double red problem. If the external node under k was red and also the node k was red, then it is a double red problem. We have more then one double red problem. 

But we have introduced only one double red problem by inserting that node, so this cannot happen. In this case we are inserting k and for now we assume that the nodes under the node k are external nodes. In the next slide I will come to why I have drawn this 2 sub trees. 

If I were to think of this as a 2-4 tree then that means that I have a node which has these 3 keys (a, b and k) in it. Earlier it had keys b and a in it, c was another node and k can be accommodated with b and a without any problem. 

(Refer Slide Time: 09:56)

 

This is what we said but we are getting a double red problem. Why is that? It is because this (b, a, k) is not formed in a right manner. If I had 3 keys in a 2-4 node then the middle key is the one which is to be set black, while it is not getting done in the left side tree on the above slide. Just that simple rotation takes care of this. So it is not that we are changing the 2-4 trees in the any manner. This (a, b, k in the left side) is the node corresponding to the 2-4 tree and these 3 goes together in to one 2-4 tree. These 3 nodes (b, a, k in the right side) still go together in the node of the 2-4 tree as before. It is just that we are reorganizing it, so that it is now in the form of our red black tree. So that was one case when the parent of the inserted node, has the sibling which is black. So the other cases when c is red and we will look at that now. 

So this is the second case, the parent of the inserted node (a) is red and the other child of b is also red. We have this double red problem and we need to take care of this. What should I do now? Can I do which I did on the previous slide? Why not? Because then I would get a double red problem on the other side. 

If I look at what is happening in the 2-4 tree that means in the 2-4 tree this (c, b, a) is the corresponding node of the 2-4 tree. Recall how I get the node of 2-4 trees, I take a black and look at all of its red children. So the nodes which are marked in the below slide is the corresponding node. It already has c, b and a in it and I am trying to bring in k in to this where clearly there is no space. 
(Refer Slide Time: 11:48)

 

So we split it into 2. That is what we are going to see in the following slide.

(Refer Slide Time: 12:26)

 

This is what the transformation we are going to do. No transformation but we just recolor the nodes and this corresponds to split and let us see why it happens. The left side tree in the above slide is the transformation that I have done. I have colored the node a and c to black. In the above picture (c, b, a) was sitting in one node and k is trying to come in. How do we split? We split with c on one side; a, k on the other side and then b goes up. Since c on one side that corresponds to a single black node, a and k on the other side corresponds to a and k on the right side of the tree and b goes up, so this is now trying to go up to the parent. 

Does this take care of all problems? There are 2 issues. First have we created a black height problem? No, so let us say what should be the initial black height of all these sub trees? 

(Refer Slide Time: 14:37)

 

If the node below k is h, then all the black height of these entire sub trees which is on the left side would be h+1. This means the external nodes on the right side are all h and now the black height of these entire sub trees is h+1. So no problem, it was as before. But I have a red node b and its parent could have been a red. Thus I have a double red problem and this is the same thing. We have managed to move the double red problem one level up. And now you see why I had these sub trees hanging out of here. When this moves one level up, the rounded part in the above slide will be one sub tree hanging from the node b and the one which is marked on the right side would be the other sub tree hanging from here. This is the continuation, the parent of b could also be red and that case the double red problem moves up one level. 














 
(Refer Slide Time: 14:53)

 

We will repeat this process at the next level, we will consider the 2 cases. If we can by rotation take care of it we would have done it, if not then the double red problem will move even one level up and so on. Eventually we will end up coloring the root which was originally black as it was in a red black tree. We will end up coloring it red but if the root is colored red everything else is okay. That is the root is colored red. How do I take care of it? Just color it black again and that will increase the black depth of all the external nodes by one but it remains the same. We are not saying that the black depth of the all the external nodes should remain the same, we just say it should remain uniform.  

The black depth of the one external node and the other external should be the same. If I color the root black, it will just affect all the external nodes by one. So there will be no problem. This essentially corresponds to moving all the way up and splitting the root. When we split the root in the case of a 2-4 tree, even then the height of 2-4 tree went up by one and so we are seeing that the height of the red black tree is correspondingly increasing by one when we do such thing. So again what we have seen in this insertion process is that either we have to do one rotation to take care of the problem and if we could not take care of the problem by one rotation then we have to move the problem up to the next level. 











(Refer Slide Time: 16:31)

 

But when we move the problem up to the next level we just did a recoloring of the nodes. Lets see, when we move the problem up to the next level in this case all I did was, change the reds to black and change the black to red. And now it is moved up to the next high level and may be if it has to moved up to further higher level it will just corresponds to recoloring of nodes.
 
(Refer Slide Time: 18:59)

 

Suppose in the above slide, the parent of the red colored node b was red, then the other child of this has to be black, it cannot be red clearly. Other wise there was already a double red problem. But it is not that the one we are worried about. Its basically the node on the extreme right which we are worried about, whether it is red or black.  
If we look at the previous case, we looked at the parent nodes sibling whether it is red or black. So it is the extreme right node whose color we are worried about and still this node can be red or black. So in insertion we just have to do one rotation. If we move up the tree, we just have to do recoloring. The same was happening in the case of deletion also and I had mentioned this very clearly that if we move up, we just had to do some recoloring. The moment you do one rotation the process ends, other wise it just does recoloring. This is what it makes the process very fast because recoloring is just one bit of information really in each node. The 1 or 0 will just tell you whether it is red or black. You just need to quickly change those bits if you are moving up the tree. 

Rotation is slightly more expensive because it requires some pointer changes, 6 or 7 pointers have to be changed. And this is why the insertion and deletion in red black tree is faster than in the case of AVL trees. In AVL trees recall that we have to do more then one rotation. We did a rotation then we moved up, perhaps you have to do another rotation and so on. Although for both of the data structures, the worst case time for insertion and deletion is log n. Because even there you were doing only log n rotations, but the constant behind that log n are much larger in the case of AVL tree than in the case of the red black tree. 

So even with in the log n, this would be a faster process for both insertion and deletion than in the case of an AVL tree. (Refer Slide Time: 16:31) That is all we wanted to discuss about the red black trees. We looked at search, insert and delete. All of them take log n time, you can also think of other operation like if I say find the minimum element in a red black tree. How much time do you think its going to take? Minimum means just keep going left. So time is height, height is log n then its just log n. You can do all such kind of operations in log n time. Most of those operations are not changing the tree. It is much easier, the 2 operations where we changed the tree are insert and delete but we seen that you can still take care of them in log n time. 

(Refer Slide Time: 23:49) 

 

We will come to this notion of a-b trees and this is a generalized idea of 2-4 trees. 
What is an a-b tree? I have drawn an a-b tree and actually this is the same picture that I used for the 2-4 trees if you remember. An a-b tree is a multi-way search tree. Each node has at least a and at most b children. When a is 2 and b is 4 then you get 2-4 tree. If it has at least a children and at most b children then how many keys are there inside? a-1 to b-1. What do you mean by b-a+1? If it has a children then it has a-1 keys, if it has b children then it has b-1 keys. So the number of keys is between a-1 and b-1. The one node which does not satisfy this property is the root node. The root node can have only 2 children. If a is 3 or 7 or some other thing then its not that the root also should have at least 3 or 7 children. Root is out of this definition, so root can have only 2 children. Root has at least 2 and at most b children. For the root the requirement is from 2 to b and we will see what is the need for this requirement. Again all leaf nodes are at the same level. 

What is the height of an a-b tree? We have seen this before,   is the minimum height and   is the maximum height. You can context this to the little bit because the root has the only 2 children. When the root has 2 children every one else has a children, so there would be a plus one in that. (Refer Slide Time: 24:15). But this ( , ) would be the roughly the bounds. As you can see in the above slide, every node has at least 2 and at most 3 children, so this is an example of 2-3 tree. We can talk of 2-3 tree, 2-4 tree, 2-5 trees and so on, for any choice of a and b. I will correct the statement as this discussion proceeds. It is not any choice of a and b, we will see what are our requirements on the relation between a and b. This will not work on any choice of a and b, but for now we will just assume it as any choice of a and b. 

(Refer Slide Time: 25:04)

 

In insertion so as you can imagine this is going to be essentially a repetition of what we did for 2-4 trees and it is with small modifications. I am trying to insert the key 21. As it is a multi way search tree, I will find a position where this has to go. The 21 does not go between 13 and 22 so come down. It is more then 18 so go right and it can fit in to that position next to 20. No problem if the node has an empty space. Similarly for 23, as it is less than 25 it comes to this space and in a node we are just keeping in order. So 24 will make way and 23 will come at the place.  
 
If there is no space in the node that we are trying to put the key in, then the node gets split. Let us see 29, 29 compared with 22 and it is more than 22 so it goes right, more than 25 and again goes right. I am looking at the 2-3 tree in this picture. We are talking of a-b but I did not want to make a and b very large because it would not fit on the slide. So I am just looking at a=2 and b=3. And the concepts are the same. For an a-b tree all the leaves have to be at the same height as in the case of 2-4 tree. 

This is essentially 2-3 tree. Basically 2-3 tree means each node has between 2 and 3 children which means that each node can have 1 key or 2 keys only which is why each of those has been made with space for 2 keys. Each could have 1key or 2 keys. 


This can have only 2 keys but now I am coming with another one that is the third one. If there is insufficient space then split the node. I am going to split this node and the median key is promoted to the parent. Thus 28 is the median of these 3, so 28 will get promoted to the parent. (Refer Slide Time: 27:54) So I split 26 goes down 29 goes down and 28 goes up. And the lines disappear and these become the children of this.  

The split can cascade and we will see the example of that. When I am trying to insert 7 I will compare 7 with the first key. The 7 is less than 13 so I will go left, 7 lies between 3 and 8. So I take the middle path, come down and 7 tries to come to this node except this node does not have enough space.  

(Refer Slide Time: 27:54)

 

This node gets split in to 2. The 4 will go down, 7 will go to the adjacent node and median is 5 so it will go up. As nothing is left here because we just remove this node and we will remove it shortly. (Refer Slide Time: 28:51) This 5 is trying to go in to this (3, 8) node, except there is no space here either and once again this gets split in to 2. The 3 goes to the upper node and 8 to the adjacent node and the median 5 get promoted to the parent. 

It is same as in the red black tree, we have not done anything which is different from the red black tree. But what I am pointing out through this is that we need not have a 2-4 tree, we could also have 2-3 tree. In the red black tree every node had space for 3 keys, even if every node had space for 2 keys we can still make it work. That is what happens, 3 and 8 get splitted and 5 goes to the top. 
 
Once again 5 is trying to enter here, but there is no space for 5. First let us just reorganize this. These 4 children have to be children of these 2 nodes (3 and 8).  The 2 left will go to the left of 3 and 2 right would go to its right. Let us take care of this node (5). The 5 try to go between 13 and 22, but not able to go. So this gets split in to 2 and 1 key gets promoted to the parent except there is no parent this is the root so we create a new root and we would go like that. (Refer Slide Time: 30:00) These are the 4 children (3, 8, 18, 25 28) they would have to become children of this two nodes (5 and 22). So 2 left most go to this (5) and 2 right most will go to this (22) and the new root (13) will have to this 2 children (5 and 22).

(Refer Slide Time: 35:47)

 
 
Exactly the same thing would happen for any a-b tree. I am trying to insert, if there is space put it there, if there is no space split and move the median up. The median might not be unique like in the case of the 2-4 tree, there were 4 keys there. The median could be the second element or third element. We can not insert in to an empty node. What is this statement? There would be no empty node. 
 
At least a children, at most b children mean every node has at least a-1keys and at most b-1keys. So this is the property. We can also rephrase it in this way. Every node has at least a-1keys and at most b-1keys. If your tree had less than a-1keys in it, then all of them would basically sit in the root. This property is not true for the root, root can have as small as 1 key only. Because it can have as small as 2 children. So root can have only 1key. 

We are saying every node has at least a children and at most b children which mean that every node has at least a-1keys and at most b-1keys. This property of at least a children and at most b children does not apply to the root node. The root node has at least 2 and at most b. The root node has at least 1key and at most b-1keys. Why we said that this property does not apply to the root, because when we went in this manner and inserted and we ended up splitting the root node in to 2 and we created the new root node then this new root has only 2 children. We have to permit the root to have only 2 children. This is why this requirement, the root can have as small as 2 children. If we insist the root have to have at least a children then we might not be able to do this at all. 

We are going back from 2-3 trees to a-b trees. So in a-b trees we said, we are trying to insert in a node if it has space, we put it there. How much space does an a-b tree node have? Space for b-1keys, if it has space we will put it. If it does not have space, what is that mean? They were already b-1keys there and I am trying to insert one more key,  key. Then we will split in to 2, one of the keys will go up and from the remaining b-1keys, half will go on one side and half will go on other side. That is exactly what is being said here. A node is split when it has exactly b keys. Why exactly b? Because earlier it had b-1, I was trying to put in one more there, then it means it has exactly b.  

One of these is promoted to the parent and the remaining are split in to 2. What is the remaining? b-1, the b-1is getting split in to 2. One part gets  and the other part gets . The , if it is an integer then both of these are the same. If it is not an integer, like in the case of a 2-4 tree. The b is 4 in the case of a 2-4 tree. You had   which is 1.5, which means one side was getting 2 keys and the other side was getting 1 key. Thus 1.5 rounded up is 2 and 1.5 rounded down is 1. One was getting 2 and the other 1. So one node gets  this many keys and the other gets  this many keys. But after the split, these 2 nodes are valid nodes of the key. So one node is getting so many keys  which means that a-1this quantity should be less then or equal to . 







(Refer Slide Time: 36:16)

 

I have corrected in the above slide which is given below. a-1<=   Why this is coming, because I am creating a node with  keys which means I have a requirement that every node has at least a-1keys in it. This quantity should be greater than or equal to a-1. Otherwise if this was less than a-1, then there would be a problem because this node that I am creating is not a valid node. 

Let us look at the deletion. Deletion again is the same as the case of a 2-4tree. The simple case is when I am deleting a certain node and there are more than one node in that. For instance if I were deleting 12 and there is nothing to be done, just delete 12 and still there is one key left. Suppose if I was deleting 20, then the problem would be that when I am removing 20, this node becomes empty and node has to have at least 1key and at most 2 keys. So it has to have at least one. 
 
If the node becomes empty then I first try to borrow a key from its sibling. The sibling of 20 is 14 and 15. I will try to borrow one from that and recall the way we borrowed was one key went up and the other one came down. That is exactly 20 disappears and 15 goes up and 18 comes down. This is valid 2-3 tree. 

 What happens if the sibling has only one key? For instance if I was trying to delete 23, 23 goes away. I try to borrow one key from its sibling, 26 is the only sibling I can borrow from. I cannot borrow from the next one and you remember the reason for this. I am trying to borrow it from 26 but this has only 1key. If I borrow from 26, this becomes empty. In this case of a 2-4 tree we merge. First try to borrow if not possible then merge.  



(Refer Slide Time: 38:33)

 

We will merge this and we will create a new node. And this will get the key 26 and one key will come down from the parent. The 24 will come down and 26 will come from here and this will become the merge node. Thus only one key left there, so it goes to the left and these become the 2 children of this node. We also saw that this process could go up and up. Because what we had effectively done is that we have removed one key from this 28th node. But if this node had only one key then it would have become empty. 

If this 28 was not there then this would have become empty. Then we would have tried to borrow one from 18, but we cannot borrow because there is only one key. This 18 would have merged which means this 22 would have come down and we would have created one node with 18 and 22. Which means I would have deleted something from 22 but if I am deleting something from here this becomes empty. So again it will merge and in this manner eventually the root has to be removed and the height of the tree reduces by one. We have seen this example from the case of the 2-4 tree, exactly the same thing is happening. Let us continue. In an a-b tree we will merge a node with its sibling. Now we are coming back to the a-b tree. When do I merge a node with its sibling?  












(Refer Slide Time: 45:36)

 

The minimum number of keys in a node is a-1, which means it had a-2 keys. I am trying to borrow one from its sibling but I am merging with the sibling which means that even the sibling does not have anything to lend me. Which means how much does the sibling have? The minimum number a-1. 

If I am merging the node with the sibling then that means the sibling has a-1keys and the node itself has a-2keys. After merging the new node that gets created, how many key will it have? The sum of these two (a-2 and a-1) plus one. Why plus one? It will have 2a-1keys. Now 2(a=1) better be less than b-1. This is the same as saying that a-1 is better be less than . 

Why have I set flow? The a-1has to be an integer. As  and since a-1 is an integer it has to be less than or equal to the floor of , floor means rounded down. This symbol means, if this is not an integer round it down to the nearest integer. 

This ( ) is the property that your a and b should satisfied. Let us just quickly see what this just means. Given the particular value of a, what are the different values b can take? You just have to look at this (2(a-1) <=b-1) again. So b can take a value 2a-1 or more. The b should be greater than or equal to 2a-1. When a was 2, what are the values b can take? 3, 4 and so on. That is the thing that we need to keep in mind. If we have a as 5, then b have to be at least 9. So you cannot have a (5, 7) tree, this will not work where (5, 9) tree is okay. 


(Refer Slide Time: 45:36)

 

We will see a quick summary. For insertion and deletion, we saw insertion and deletion in a-b trees. The height of an a-b tree we saw is log n and so insertion and deletion both take order log n time. The reason for that is the same as in the case of a 2-4 tree. 

The reason why the insertion and deletion was taking log n time in 2-4 tree was the height was log n and we might have done some number of operations. But we were doing the operation proportional to the height. We might move up all the way, so first we move down in the case of insertion and deletion so that height is order log n. Then sometimes you are borrowing in the case of insertion, sometimes splitting, but the number of times you have to split is at most the height. In the case of deletion we would either borrow the key or we would merge or again the number of times we would have to do this is proportional to the height because every time we did one of these operation, we moved one level up or we just stop the entire process. So both of these operation take order log n time. 

The other thing we saw was, what should be the relation between a and b for this to work. That is as far as this a-b tree was concern. The other thing we did today was red black tree and for that we saw the process of insertion. We saw that it takes only some number of re coloring and one rotation to complete an insertion. That is the key thing about the red black tree, they only require one rotation. Some number of re coloring or might have to re color many nodes but only one rotation is required and this is what gives the power. This is what makes them very fast in practice and they are faster then AVL trees for this reason. So the next class we are going to see the particular kind of the a-b tree which is called b-tree and what role does it plays specially in searching very large databases. That is what we are going to do in the next class.   





Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 16
Disk Based Data Structures 

So today we are going to be talking about disk based data structures. In last class we looked at ab trees. 

(Refer Slide Time: 01:17) 

 

So these were the extension of the 2-4 trees that we had seen. Today we are going to look at disk based data structure and in particular we are going to look at b trees and we will see that they are very similar, they are in fact ab trees for specific value of b. So now we are looking at is when we have a large amount of data over which to search. Till now all our search trees were limited to main memory, in the sense you build a binary search tree or build a 2-4 tree or a red black tree so we had this nodes which where objects in memory and you know the references corresponds to pointer addresses or memory addresses.

Now we are looking at the setting where we have the huge amount data. Let?s say some kind of transaction data which could be bank share markets, you know setting we are large amount of data get generate and such huge amount of data is not stored in the main memory of computer. This is typically stored on disk and now you want to be able to search through this data or insert something in to this data or modify this data. How do you do that? So you just imagine the setting where you have let?s say particular bank which has records of each of its customers. So they could be a million customers each of those records would be huge. The data should set in the each account should be huge because it would have all the transaction data associated with what has been history of transaction and so on. You can?t expect all of that data to decide in the main memory of the computer so it would be kept on disk and now suppose type in particular account number you should be able to retrieve that particular account and the data associated with that account. 

So question is how are going to do this? So we want to make a search tree which is some sense secondary storage in avl tree which will help you even when the data most of the data is stored on disk will still be able to search through it. Now the problem here is that the data is stored on disk that?s fine but even the index that we are building. So what do you mean by index? The search tree, search that we have so huge that we cannot expect the entire search tree to fit in to the main memory. We will come to this in to the short while. So what?s the problem in you have think on the disk verses when thinks are the main memory of the computer.
 
(Refer Slide Time 04:01)

 

One big problem is that disk access is very slow. How is disk accessed? You have let?s say this is the disk, so you typically have bunch of disk in stack one and top of each other. Each one of them has its own read write head, head which will traverse which can move along this disk and get to a particular track. So these are the track on the disk the disk rotates in one particular direction and these read write head can decide that it wants to go on this track or this track or this track and so on. When you have to read a particular location so a track is further divided in to sectors. So when you have to read a particular sector, the disk rotates and this read write head gets to the appropriate tag and then starts reading the data from there. 

So significant fraction of the time is spent in this read write head moving determining which track it has go to and moving and the rotation. So one of this is called seek latency, so this that time that is required for the head to get to the appropriate tag its called seek latency and the other one is called rotation latency. The time required for the disk to rotate so that the head is positioned at the right place, the right sector and once you have write at the right place then the entire sector is typically read. So one sector or let?s say one let?s call it as page so the data then is read in units in larger units. You don?t read one byte of data from the disk. Why don?t you read one byte of data from the disk? Because you already spend so much in getting to that one byte then might it is well read whole lot of bytes. So you typically read or write in units called pages which are you know it could depend upon the particular computer system but it could be in this range 2 to 16 kilo bytes. 
So you read this much amount so this could be one page setting here and you read this much amount of data and it will be moved in to the main memory. This is reading, when you writing similarly you would write back and entire page. What are the problems associated with this? We have to organize our search tree in such a manner that first it can sit suitably on this disk as I said or search structure itself is going to be so large that it cannot fit in to the main memory and that?s what we are going to do in next slides. So when you have the disk based algorithm like this, the running time is going to be measured in terms of the time taken by the CPU and the number of disk access. 

(Refer Slide Time 07:11)

 

Now this time, the time taken by the CPU is insignificant compared to the number of disk access in the time spend in each disk access. So what you would do in most of these algorithm is to organize the data in such a manner that there is a number of times you have to access the disk is very small and what I have said is you know till now we had seen the algorithm which are called main memory algorithm where the entire data sets in the main memory. So they cannot be easily ported to this module when part of the data sit on the disk, it cannot be ported in the straight forward way. What are the problems that are going to happen now that we have data setting on the disk? So one is as for as this pointer business is concern. Pointer is to same as references to objects. So till now we had reference to an object, we knew that correspondence to the address in the main memory. So you would go and access that location in the main memory. 

Now if part of your data structure is sitting on the disk, so just imagine that your red black tree or whatever it was, part of it was in the main memory and part it was sitting on the disk. One of those pointer, one of those references is referring to a location on the disk now. So if it is referring to something in the main memory then you can go and get it at that particular memory location but if it is referring to something on the disk then you will have to do something lets use the operation disk read to read the particular data from the disk and disk write to write bind of the disk. We have to use such kind of operation to be able to access the data, I will come to what key means in a short while. 


(Refer Slide Time 09:15)

 

So one big problem is that you know the pointer now have to be translated suitably. If the pointer is just pointing to a memory location then it is easy, you just get to the memory location and do your job 
but if its not pointing to a  memory location, if it is referring to something that sits in to the disk then we have to first fetch that block of data, that page of data from the disk in to the main memory and then you can access that particular object. So typical pattern would look like something.

(Refer Slide Time 10:01)

 

So x is the pointer to some object so you would read this object from the disk you will do some operation on this object and then you will eventually write it back and you might omit this step if you did not modify the object at all. 
So we are going work with two operation today, disk read and disk write, reading a block from disk and writing block back to disk. So now let?s come to what a, b tree is.

(Refer Slide Time 10:32)

 

B tree is a same as your ab tree. Here I have drawn an example where in this ab tree each node has 1000 keys so you value of a here is at least thousand lets say is a 1000. So recall in an ab tree, each node has at least a children and at most b children. So here I have taken my value of a as 1001 so each node has at least 1001 children. So here I have just tried to illustrate why is this data structure now useful for disk based access. So you have this first node which has 1001 children right and since it has 1001 children how many keys would it have inside it? 

1000 keys and just I have organize it very uniformly so that then this each of this each node has also 1000 children and then further more these leaves so its just two level trees these leaves also have 1000 keys in them. So how many keys are there in all in this? So that?s a 1000 plus million plus a billion. So there will be so many keys in this entire two levels structure. So let?s say I had a data base with so many records, so many different accounts so I could put those of this each of this accounts, records you know lets say the key just the account number so i could put all of those keys in to this kind of a structure. Now each node had contain 1000 keys. Now this entire structure cannot fit in to main memory, you can see this is already 10 to the 9 keys write which each of this keys there is associated with the pointer. Each of the key itself could be let?s say 4 bytes of memory, so 4 bytes of memory let?s say 4 bytes of pointers is about 8 bytes per key so that?s 8 bytes times a billion that?s 8 giga bytes.

So you need that kind of space just to keep this data structure in to a main memory and here we have not said anything about the data associated with this records. Each of those data themselves could be 100 mega bytes because my account has listed with all transaction that have been done on the that account lets say the last three years or four years or some such thing that?s huge amount of data that?s lets says stored on the disk we are not even bringing that in to the main memory. But what I am pointing out first is even if the actual data was stored on the disk just to be able to access the data, just using keys and pointers is so huge that you cannot have all of it in to the main memory. 

Now why is this kind of structure useful? We are talking about this base, so what we are going to have now is that we are going to have this structure this called b tree. This structure itself would be kept on the disk, so each of these nodes would now be one page on that disk and it is just this very top node, the root node which will be kept in the main memory. Yes, now what happens if you have to search, what would you do? You will determine which of this 1000 keys between which two keys your particular key lies in. 
Suppose it lies so and then go to the appropriate child node. 

Suppose this is a child we have to go to, now what will you do? You will access this node from the disk and bring it in to memory. from here you will determine key which should be your next node suppose it is this node so you will access this node from disk and bring it in to main memory and from here you will be able to find your account number and then you will have to follow the account data, let?s see. So how many disk access would you need? One to get this, one to get this and one to may be get the actual data associated the information associated with that particular record that you are trying to access.  

So how much disk access, how many disk access do you need? Height of the tree. So to reduce the height of the tree what do you want? So you want as little height as possible for the tree, yes so how can you reduce the height of the tree? Increase the value of a, increase your whatever the number of children each of this nodes have (Refer Slide Time 15:50) but what is limiting you from number of children each of this nodes and page size?  Instead of 1000 why didn?t I put in 10000 key here or 100000 keys in here? That would have been even better, that would reduce the height even further. If I had, that would reduce the height even further but i can even put too many because each of those keys is taking certain amount of space and i can only put as many keys as can fit in to one page and what is the page? Page is a unit of transfer between the disk and the main memory. 

So that is define for the particular computer system. [Hindi conversation]. So depending upon if each of your keys comma pointer per taking let?s say 8 bytes, your page size is 16 kilo bytes each of key comma pointer pair is taking 8 bytes that you can put at most 2000 keys in to one page and so that determine [Hindi conversation]. So this is just read writing what the node?  

So x is now referring to an particular node so what does the node x have? n [x] is the number of keys in that node and then once again the first key is going to be less then the second key is going to be less than the, so key one of the node is less than the key two of the node and so and on. 













(Refer Slide Time 17:08)

 

So there are n of x keys and they will have lets say methods or particular bit in a node which specify whether the node is a leaf node or not. Whether it is has any further children, so leaf if this is true then that means leaf node other wise not and if its not a leaf node it?s a internal node then we will have, if there are n of x keys then will have n of x +1 children of that node and this we all know we have seen this before if k sub i is in z key in the sub tree c sub i in the case of i is less than the ith key in the node and so on. So it?s the same.

(Refer Slide Time 18:05)

 



So b tree is the essentially same structure as you ab tree. Now only difference is so for as ab tree we say we provided a lower bound on the value of b, we said b is should be at least 2a-1. What are we saying today we want that b should be? So if a node has degree t so t is the same as that we had talked off all nodes except the root had between t and 2 t. So this so we want that b, so today we want basically this is out bound for b so we were working with b equals two times a exactly. So b tree is essentially special kind of ab tree with ab with bb exactly two times a. So each node has between t and 2 t children and so it has at least, it may be between t -1 and 2 t minus one keys. So the other way in which b tree differs from a, ab tree is while ab tree is meant to be a data structure meant for an internal memory, b tree is a data structure meant for secondary storage. So you really have to choose large value of b so that height of the tree is smallest possible and the entire node, all the key is in the node can fit in to one page and the same as before the root node has between 0 and 2 t children. Actually i have said zero but you know, it need not be zero it could be 2 between 2 and 2t children, let?s make it 2 and 2 t. 

 (Refer Slide Time 20:08)

 

So the root node in this example has exactly two children and then this is t -1 keys and let?s say t children. So this is the setting in which the height of the tree is as large as possible because each of the node has only t children. So if this is all just making sure that you remember things from the previous classes. There are one node here, two nodes here, 2 t nodes here and so on. So this is goes up to height h, you can compute you know this would be the total number of nodes that would had and since n is equal to this that will give you the height of the tree as this quantity. So I will just show you some pseudo code also for searching so that you know that these things are completely clear in your head. 








(Refer Slide Time 20:53)

 

Suppose I am searching for key k in a node x. So this will be a recursive search procedure. What am I going to do? I am going to first find out the key. What should I do? I first have to find out the key, the first key which is larger than k, the keys are arranging let?s say increasing order in a node. So you have to find a first key which is larger the k so that?s what I do here, I keep moving till I find a key which is larger than k then I come out. So if I found a key which is larger than k let?s say that will give me the ith key. So if  I found exactly the key I was searching for then I return if the node that I am in was leaf node that I can?t procedure any further then I return else either then the key is not there else I fetch the next node. I fetch the next node and i continue my search in the next node. What you have to do here? You were searching, this is let?s say ci (x), we have to know excess the particular node. So ci (x) is just the reference to the child node, the appropriate child node, you just have to access that. I am going to skip this.

















(Refer Slide Time 22:54)

 

So splitting nodes the same idea is before. When you split nodes? When they are full so how many keys can sit in a node? We said 2t, 2t-1. It can have 2t children so it can have 2t -1 keys. So if it has 2 t - 1 keys and new key if we are trying to insert a new key then we split the node and what was the process of splitting, if this is the node that I am splitting let say t q value is 4. So this already has seven keys if I am trying to put one more here then I need to split it so s goes up and this gets split in to these. So I have put down the code here, you can have a look at this slides separately to understand this code. I am not going to spend too much time during it here. 

(Refer Slide Time 23:38)

 

It?s very straight forward what you have to do, this is a procedure for splitting a node y whose parent is x and this is lets say the ith child of this parent. So i refers to that and what you do, you create a new node z and then first you copy the appropriate number of keys t - 1 keys from y to z that?s what being done here and then you also, if this is not a leaf node then you will copy the children also from y to z that?s being done here and then coping that. Here you are moving so this has to be promoted up here which means that these keys in here have to be moved one step right in the array that?s what is being done here and then this key that is to be moved up copied in to the appropriate place there and then we have these three describe operation because we have modified this node and this node and the new node we created. 

So they all have to be return back to disk, so that is being done here. So the same thing is before the only catch today is that we are doing this disk operation also, you can look at this slides and understand the code. So how much time does it take for? So now today we are going to be measuring running time in terms of the number of disk access that we have to do. So in doing the split the number of disk access we have to do was three because we did three input outputs at the end. (Refer Slide Time 25:37) The CPU time which will now be proportional to the number of keys in that node because we have to moves certain keys and so on is going to be fairly small compared to this. So we will actually be counting time in terms of the number of disk access. 

So today one variant of binary of b trees also works with, 	so if you recall inserting when you did inserting in two four trees or in ab trees, we went down the tree, found out the place where we have to insert the element then try to put the element there and if that resulted in the node getting split, we split that node and that may have to insert the key in the parent node and that could lead to another split and so on and on. There is another way of doing this thing which is that we start from the top. So that is called the two parse operation where in first you come down and then you go up the tree. One possibility is to do the entire thing is only one pass, what is that mean? That is as you are starting from the top you check to see if the node that you are looking at already has its full quota of keys. What is the full quota? 2 t -1, if it?s already has 2 t - 1 keys then you are going to split that node right there and then and only then proceed down the tree, does everyone follow. 

So we are going to start from the root and we are going to recursively travel all the way up to the leaf but before we descend to a lower level we make sure that the node contains strictly less than 2 t - 1 keys. If it has 2 t - 1 keys then we split the node right there. Let?s understand this.














(Refer Slide Time 27:33)

 

So I will come back to the slide later, let?s say this was my root node. I am not yet inserted the key, this was the very first root node that I already counted this is already fill up I am working with t =4. So this has 2t-1, 7 keys in it. This is already full, so before descending down I see this is already full, I write at this step split this node in to a d f l n p and h moves up and now I will continue down the tree whatever key I am trying to insert I will now continue. So let me see, lets show you an example I will skip this slide and come back to this later. So let?s take an example and then I will look at the code again. So suppose this is the tree, I am trying to t is the value of three I am trying to insert b so first I come here this node is not full, t is three so it node is full when it has five keys in it. 

(Refer Slide Time 28:33)

 

This is not yet full, so I can come down and b is inserted at this place. Suppose I am trying to insert q so once again I come to this node. This is not yet full, so I go down so actually this should b here I come here this is not yet full I come down here and then I put q here but this is full already so which means it will call this is split. So q r s would go on one side u v on the other and t gets promoted here. Note the very interesting thing why did this node not get split? What we had seen in the two four trees or ab trees what was happening was that when I inserted an element in to the parent, the parent could also gets split and that?s not going to happen here anymore. Why? Because I came down from the parent only when the parent had room in it, if the parent had room then when I came down and I split this node and I put the one key in to the parent, the parent is now not going to get split. Let?s with this happening again, so now when I am trying to insert the next key lets say l so now when I come to the root node, I will straight away split this because this already full, doesn?t matter where l is going l is you know l m n o m l m n, it will come here this node is not full so but I am then I am trying to insert l I will right away split in to g m t x p and then come and insert l. 

So now let?s say when I try to insert f, I come here this is not full so I can continue this is not full so I can continue then I come here insert it here, split it and one guy gets promoted here. So the fact that this is not full lets me accommodate this addition key here without crossing ripple effect in the splitting process. 

(Refer Slide Time 31:11) 

 

So in some this is like a one pass, you know in just one way down we have kind of access all the nodes. In just single pass we have access to all the nodes of the tree and whatever splitting then it will be done. So how many disk i was required? basically it just move down the tree once, we have to access every node and then every time you split the node, you have to right down that down back to the disk. Its parent node back to the disk and one new node that you create back to the disk. So for every time you split you might have to write down three nodes back to the disk. So you will have to read as many nodes as the height of the tree and how many nodes will you have to write back? You will have to write back at most three times the number of splits that many nodes you will have to write back to the disk. (Student Conversation-Refer Slide Time: 32:28). So towards the end I will discuss what are potential disadvantages of doing in this way. 
If you keep splitting because you will have to now see what I am going to do when I have to delete a key. There I will try and let?s see what I try and do that and then it will be clear. So when I am doing deletion, some going to actually skip the code I showed you, so you can look at those slides and understand them on your own.  

(Refer Slide Time 32:50)

 

As far as deleting key is concern. Once again we recall the earlier procedure, we went down the tree we deleted the key, when we deleted the key that node could have less number of keys than it was supposed to have in which case we first try to borrow, if not successful merge. If we merge we have to remove one key from the parent that could cause the ripple effect, the cascading effect. So that we ended up doing all the way up to the top. Now we are again trying to do the single pass delete procedure which means that we just going to down from the top and do the deletes. So now what we are going to do is if we encounter a node which has already the minimum number of keys.

What is the minimum of keys in a node? t -1, if it just t -1 keys then we are going to make a effort to let it have more than t -1 keys strictly more, at least t. So what are we going to try the same thing as before? First we try to borrow from a sibling, if we are successful with that great. If not then that means the sibling also has t - 1 then we merge and when we merge we bring one from above and why can we now successfully bring one from above because the upper has strictly more than t ? 1. So you can do the entire thing in a single pass in exactly this manner. So that?s what we being said here before descending to lower level in the tree make sure that the node contains at least t keys. 

In the case of insertion we require it contains strictly less than 2 t - 1 keys now we say it contain strictly more than t keys. Suppose this is the situation. So we are once again working with the t equals three, t equals three means each node has to have at least two keys. So this node it?s okay, there is no problem with this node why because it has three keys so I can continue down. I can continue down I come to this node and f is deleted from here and there is no problem. But if I was trying to delete something from here then when I encounter this node which has only two key the minimum number then I will try to do something write then and there before proceeding down. So you know I have actually put down all the cases here where you know the key that we are trying to delete not in the leaf then you have to do all of the snaps so you seen all of these before so lets skip see some of this and this was the case when we have to merge. So sorry just once again let me make sure what right.

(Refer Slide Time 36:12)

 

So if this is the setting if the particular node has only 2 - 1 keys then we have take action to ensure that it has at least t keys before we continue down and first thing is that we can borrow from the sibling and if not then we merge so this is the picture.

(Refer Slide Time 36:43) 

 

Let?s look at this one, we trying to delete b. We come here there is no problem here and when I am trying to delete here I have borrowed one from the sibling, you may have seen many example of this by now. So how do you borrow, one goes up and one comes down so this is the situation happening here and if we cannot borrow then you merge same as before you are deleting d. 

So you come here C L when you are deleting d this is already has problem because it has minimum number of nodes so you going trying borrow one from the sibling but you are not going to be successful in borrowing one because that also has minimum so you merge and you get c l p t x bringing one down and then you go head and you delete d from here. So this is the situation that?s going to happen. Then this also illustrates why this is the bads key, who can tell me why so this is answering the question that he had raised? Why did we do the same thing for the red black tree is and the ab trees? Student: so we could have delete either we will have? Staff: okay but [student conversation]. okay I am looking for one other answer which is perhaps who can [student conversation] good, split again see what?s going to happen I just deleted and this is the picture I got, suppose I insert now what?s going to happen I am going to go back this one, suppose I delete I am going to come back to this. Suppose I insert, this kind of going to go back and forth between this and I am spending lot of works in doing this because splitting the node lots of pointer moments and all of that. 

So this key does not work in very well then this we have this kind of things happening. So insert and deletes are very highly interspersed, if you have a block of insert and block of delete happening then it?s okay. Then you can still work with the scheme fine. Because if you had just deletes happening [Hindi conversation] so they would be no problem. This has large number of keys now so it can handle whole lots of delete without significant trouble. So if you had large sequence of delete then this is a fine strategy or if you had long sequence of insert but if you had these things alternating vary often then would be in some trouble. So once again what are the disk, I was going to look like? 

(Refer Slide Time 39:52)

 

So we are going to read as many nodes as the height of the tree and each point when I am borrowing one from the sibling or merging one from the sibling and basically modify only my sibling node. So if I am doing any of those operation I might have to right back my sibling, my one node and the parent node, by the parent node because even when I borrow from the sibling one key goes up and one key comes down from the parent. 

So its parent nodes also gets modified or even then merging again the parent gets modified, so every time I do some borrow or merge with the sibling I will have to right back three node and so that gives the number of disk right that we have to do a thing. So again this two pass operation we have actually seen before.

(Refer Slide Time 40:35)

 

So this was single pass we have doing things, the two pass we have already seen. So you first go down and then you go up as much as necessary so to say. But in the case of disk access you have to think carefully or you have to organize the things more carefully because if you first make one pass down and then you make a pass all the way back up then you spending twice as much time as you should have by just having made a single pass. So one thing you can do is that when you making first pass down you keeps all those blocks read in memory. How many blocks is that? How many pages is that just the height of the tree which is not too much. You keep them in memory. Why do we keep them in memory, because you might require them on the way back in second pass. So those are in optimization that you can do to try and reduce the type because as I said most of the time here spend in the disk access, you have to reduce the number of disk access as much as possible. So with that we are going to end this class today so this was mainly meant as a recap of a data structure that we saw today was just small extension of the ab tree but it is specifically for disk based accesses and this useful in that setting. 

Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 17
Case Study: Searching for Patterns

So today we are going to be talking about pattern searching or pattern matching. The setting is that we are given a piece of text. Let?s say ?t? is our text which is of length ?n?. So it is ?n? characters over some alphabets. So for today let?s assume that it?s the English alphabet. We have to search for a pattern ?p? in this text. You have all come across this problem. You are using a certain editor. You want to find out all occurrences of a certain work. Most editors will provide you the facility.

(Refer Slide Time 01:17)

 

The browser provides you the facility. Many things provide this kind of facility that you can search for a particular pattern. Not one occurrence but all occurrences of the pattern. So question is: how are we going to do it? This is also the exact matching problem. There are versions where you cannot do an exact matching. As in you can search for the pattern in an approximate manner. That means occurrences of the pattern in that text where it matches most of the texts. If there is difference in one character, it is okay. Difference in two characters, it is okay. But at most other places it matches.

So how would one solve this problem? What are the applications? This comes up in text editing. As you can imagine, it comes in information retrieval. One big application is bio informatics where your text would be a large database with sequence of nucleotides in it and you are searching for a particular gene or a particular sequence of nucleotides and you want to find out where all they occur. So this is an example where the alphabet that you have is small. If it?s a DNA then the alphabet has 4 bases in it. But if it is a protein sequence, then it has 20 bases or it is an alphabet of size 20. So what is the na?ve method of doing this? Suppose this is the text and I have a certain pattern let?s say the pattern is ag ag. This is the text. 

(Refer Slide Time: 03:29)

 

So what do you mean by the naive method? I will put the pattern at the first place and I will start matching the pattern against the text. In the third place, I find the mismatch. So what do I do? I start again from the second element. Is it clear to everyone why I should start from the second position and not from the fifth position? What we are going to do is move the pattern by one step. We will see examples of why this one step was critical and not more. Clearly by moving it by one step, we are not losing anything. We will be able to find all the occurrences of the particular pattern. So we do this again. There is mismatch right at the very first step. So we will move forward by one step and now here you see this is the mismatch of the fourth position. So how much should I move it? We will keep moving by one. Even here there is a mismatch at this place and so you will say that this is one occurrence. 

So this is the question. Why are we moving by 1 and not by 4? If we were moving by 4, we could have skipped this thing. So suppose I moved at 4 at this position, then where would I end up in? It is a g a g here and then if I moved at 4 again, then I would end up a g a g and so I would have missed the occurrence of the pattern because the pattern is not occurring at multiples of 4 or such things. It could occur anywhere in the text. So moving by 4 is a wrong thing to do. 









(Refer Slide Time: 06:40) 

 

How much time does it take? n m. So for every position that I put the pattern, I might have to match up to n places. It might be that only at the very last place, we find out the mismatch in which case I am matching it at all the places of the pattern. How long was the pattern? So for each location of the pattern we might have to do ?m? matches and how many different positions of the patterns are there? n different positions. So I might be spending as much as m n time. The time complexity then is order m n. space complexity is also going to be an issue. How much space we require? Is this clear that we require only m. we just need to store the text. We need to store the pattern. We don?t need to any other additional space. We don?t need another array to move the pattern in it. 

This moving the pattern is only to show it to you here that you can always increment your index appropriately. So this unfortunately is not too good. Why is it not good? It?s too high. So if you have a huge document, m is very large and your pattern is also reasonably sized. Let?s say 20 or 30 characters, then you are going to spend a lot of time. So this is not the way it is done. We are going to look at another solution today. We are going to look at a third solution in the next class. So this will be the sequence of improvements. We will improve the time complexity today from m n to something else. I will see what that is. So how can we do better?











(Refer Slide Time: 07:48)

 

So when a mismatch is detected at a certain position, let?s say at position ?k? in the pattern string, what do we know? We know that we have matched k-1 characters before that and I will try and take advantage of this fact that we have already matched k-1 characters. Let?s see what I mean by this. So here the mismatch is detected at position 3 of the pattern. So what do I know? I know that the first two characters of the pattern are the same as the last two characters of the text. So I can use this information to try and move the pattern not by one step but by more steps. Why? Let?s look at the second. So this is c. There is a mismatch here. So what do I know? Even if I did not know what was here, I know since this is the mismatch I know that here, at this position in text it is a ?g? and this position text it is an ?a?. 

Now if I shift the pattern by one unit what happens? I know that in this position in the text it is a ?g?. You understand why I know that? I don?t have to look at the text. Now that although the text is here in front of you, you can erase these two things and yet you know that this is ?g? and this is ?a? and since this is a ?g?, there is no pointer in shifting in one step because then this would never match with this. We will have to shift by one step if this were also ?a. If this were in ?a?, then we would have shifted it at by one step. So that?s the idea and we will see how to implement this idea. Now again what do we see? We see that the mismatch at the very first step. The mismatch is at the fourth position of the pattern. Now should I shift it by one position? Should I shift it by two positions or three positions? 









(Refer Slide Time: 11:41)

 

Now what do I know? So what I claim is you don?t have to look at the pattern here. You know that the last three characters of the pattern are the same as these three characters here because after all you know the mismatch was happening at this position. So this has to be an ?a?. This has to be a ?g? and this has to be an ?a?. Now if I were to shift it by one position, then that means that I have to move by one position. So there would be a mismatch. If I were to shift it by two positions, then I know this is an ?a? this is an ?a? and what else do I know? I know this will become a g. but the g was already mismatching with this. Actually I don?t really know that. The shift will depend upon the pattern of course and we have to determine what the shift will be. So here it?s completely clear that we would not want to shift it by one unit. But whether we wanted to shift it by two units or not, we don?t know. I am shifting it by two units. We will see that exact procedure very shortly. That?s what the purpose of this lecture is. 

Once again we see that there is a match here and then there is a mismatch here. So mismatch is at the second position. Should I shift it by one position? I will shift it by one position again. Now I see a complete match. There is nothing to be done here but I have to continue. I can?t stop. How much should I shift it forward by? 2 or 4? What do you mean by split pattern? What do you mean by mix pattern? If there was another ag here, this are two occurrences there. 

This would be one occurrence and this would be another occurrence. If this were a and g, it?s all occurrence of this pattern. I am not saying all disjoint occurrence of the pattern. I should be shifting by two units. So this is what is known as The Knuth-Morris-Pratt Algorithm. It?s very famous algorithm so key idea is when mismatch occurs we need not restart the computation all the way from the back. We can use some information about the pattern. Some information obtained from the pattern to determine how many steps we should shift forward. What we are going to do is we are going to construct an array ?h?. That?s going to determine how many characters to shift the pattern to the right in case there is a mismatch. So we are going to store this information of how many characters we have to shift right in an array which we will call ?h? and we will see what the array ?h? would be.
(Refer slide time 13:50)

 

What is the key idea now? So I guess I need to explain it more clearly. So recall we have our text. We have a pattern. We have seen mismatch at a certain position. That means this is not the same as this. Suppose if we have successfully matched the prefix P1 through P (i) -1 of the pattern with the substring T (j- i +1?? j -1), so let?s label these things. So this would be location j. T?his would be i. So I successfully match P1 through i-1. So this would be location i-1. I successfully match P1 through P (i) - 1 of the pattern. With what part of the text? It?s j-1. This would be j-1 and basically we are counting i-1 locations here. So that would be location j-i+1 to j-1. So this is the part of the text which is the same as this pattern. That I know because this is the first place where the mismatch happens and pi pattern this is not equal to tj, then we did not reprocess any of the suffix. 

So what we are saying now is that we need to determine how much we need to shift. So we should shift in such a manner so that this part matches with this. So now this is the shifted pattern. I need to shift in such a manner so that these two parts are the same. This part matches up completely. Why would this part match completely? This is because then this would also match up with this here because I know this is the same as this. I will repeat it with the next slide. So with each position of the pattern, we are going to define. So recall we said that there is an array ?h? which determines how much we are going to shift the pattern right.










(Refer Slide Time 16:53) 
 
 

What is the information ?hi?? hi is the extent to which I will shift the pattern, if there is a mismatch at position i+1, that?s going to be the semantic. So we will understand this. How much I am going to shift the pattern right we will of course try to make the smallest possible shift. We will come to all of that. So we are going to make a certain amount of shift. We will see what the right shift is. Smallest or largest because it?s both smallest and largest. Smallest so that you don?t miss out any pattern. Largest so that you don?t waste any comparisons. So it?s both smallest and largest. So you can pick. Suppose this is what is happening, this is my pattern, I am now not showing you the text at all. But I tell you just the following information that the mismatch happened at position 11. So you don?t know what the text is but the mismatch happened at position 11. 

How much should you shift the pattern by? Let?s draw a picture and let?s see how much should you shift the pattern by. Let?s say I shifted the pattern by one unit. Then what will happen? I will have a b a a. This is useless. I am now comparing the pattern against the pattern itself. Why because you know that text is the same here. That was I said I don?t know what the text is but I know now what the text is. It has to be this. This is one key observation. So now if I shifted the pattern by one unit, then clearly it could not match against the text. So one is useless. Let?s shift it by two units. If I shift it by 2, I get a b. It?s of no use again. Let me draw the shift by one. Also a b a a. Useless again.









(Refer Slide Time: 20:54)  

 

Shift by two? Useless because this is not matching this. Shift by 3. a b a a. Useless. Shift by 4? a. Useless. Shift by 5? So I can keep playing this game. a b a a a. We have shift by 5. (Hindi) So we are trying to shift by 6. But that is also useless. 7 and 8 are also useless. Actually I want to make i + 1 = l2. So 12 is the first place at which the mismatch occurs. 5th is okay. It was a b a a b a (Hindi) which is not the same as this. So this is acceptable because (Hindi). So we will say that 5 is the reasonable value to shift by. So we will come to what h11 or the 11th location of this array would be. So I have defined hi to be the length of the longest proper suffix of P(1) through P(i) that matches the prefix of P. let?s understand what this is. What is the suffix of P (1) through P (i)? Now recall we said this would always be an ?a? and there is a mismatch here (Refer Slide Time: 25:22).


















(Refer slide time 27:45)

 

So P (1) through P (i) is this part of the pattern (Refer Slide Time: 25:29). What is the suffix of the P (1) through P (i)? Prefix is contiguous parts before. So ?a? is a prefix. ?ab b? is a prefix. ?ab a? is a prefix. ?ab a? is a prefix and so on. What is a suffix? ?a? is a suffix. ?ab b? is a suffix. ?ab a? is a suffix. ?a b a a b a? is a suffix. So I am finding the longest proper suffix. Proper suffix means suffix which does not include the entire text. Like proper subset means strictly smaller. So that?s what proper suffix of P(1) through P(i) means that matches the prefix of P. So I have to find out length of the longest suffix of P(1) through P(i) that matches the prefix of P which is exactly this. This is the one we showed here. Why should the suffix match the prefix of P? Because when we shifted, it is the prefix of P which comes towards the suffix. I have to find out the longest match because that determines how many we have to shift without missing out on a possible match somewhere. So I have to find out the longest possible match and then there is the additional congestion here that the i+1th character, so ?i? is the length. I am finding out hi - the length of the longest suffix which matches the prefix of P and then I want that (i + 1)th character of this should not be the same as the (hi + 1) th character of the pattern because hi this (Refer Slide Time: 27:31). So hi then is 1, 2, 3, 4, 5, 6.  

So I don?t want that the 7th the same as P12 and why did we require this? Because we know that there is a mismatch happening here. So we want that better be different. So this is the definition for our hi. So it?s a bit of a tricky definition but you understand what the argument is. So it?s not really how much you are shifting by. If the pattern has matched the first 11 positions, then h11 says that ?okay, if there is a mismatch in the next position, then what is the shift?? so after the shift how many characters still continue to match? 6 characters and what is the extent of the shift? 11 ? 6 = 5. The logic is very simple. You are trying to determine what is the extent to which you can shift the pattern. You are trying to determine what is the smallest extent by which you can shift the pattern. You will say smallest is one but smallest is not one because one is quite meaningless. You know that clearly there will be mismatch happening by shifting the pattern by one. So what is the smallest extent by which you can shift the pattern without you doing a necessary work? You know that there are certain mismatches happening. So you want to figure out the right extent to shift but you don?t want to shift by huge amount either. Why did I make shift by 11 units all together because then I would have missed out on certain possible matches.

Let?s continue. so this (Refer slide time 30:14) part of the pattern what we already discussed in the same as this one and 7th position is not the same as the 12th position which means that with the value of hi  - h 11 as 6, then this condition is true. This would be 7. The 7th position is not same as the 12th position. i is 11 and hi is 6. If there is no proper suffix of P (1) through i with this above property then what should the value of h (i) be? What is h (i) capturing? (Hindi). So let?s take some examples and then this will be clear. So this is my text (Refer slide time 31:35) and this is my pattern. Where is the mismatch happening? In ?c? or ?a?. 

(Refer slide time 31:35)

 

This is position 12 of the text. So till 11 positions, matches happen. So the value of i is 11 again. h 11 is 6. By how many are we shifting? 5. So that?s what is happening. (Hindi). So last six characters and first six characters are the same. You can actually move the pattern all the way up to here and further this is different from this. So there is a potential this guy matches with this. It does not that there is a different thing. There is a potential that this guy matches with this. So we will shift it by so much amount. So this is the shifted pattern. h11 is 6. So you were shifting P to the right so that P (1) through hi - 1 through 6 aligns with this as the suffix of text. So k is the position of mismatch. k -1 is before this and k - hi is this part. So it aligns with this perfectly.









(Refer Slide Time 32:14) 

 

These parts should match. Why? Because this is the same as this which is the same as this. So they match. So in other words we are shifting (P i ? hi) places to the right. Suppose the pattern was only up to here, a b will not be the part of the pattern. So that means that till the 11th position everything is fine. How much should I shift forward by? Again the same i - hi because by doing that, we would not be missing out on any other patterns then. So everyone understands why we are looking at longest such matching part? So that minimum shift occurs or maximum shift occurs? So that minimum shift occurs. That way the minimum amount of shift is happening. But why did we want a minimum amount? We want the largest possible shift. So the right answer should not be so that minimum shift occurs. It is so that we don?t miss out any patterns. That?s why we are looking at the longest prefix. If I were to take shorter prefix, I might end up missing out on some guys. Why does this work? Why is it that we are not missing out on any pattern by doing this? We have to argue correctness of the algorithm. 
















(Refer Slide Time 35:19)

 

So this is my piece of text. The mismatch was at position k of that text and position I + 1 of the pattern. This is position I + 1 of the pattern this is the position k of the text (Refer Slide Time: 35:43). In this case k and I + 1 are turning out to be the same but there could be in some part of the text even before this case even before this I + 1 and k would not be the same. This is before the shift and these 6 character are same as this character which is why we could move it this way. So this is the situation after the shift. So if this part is beta, this substring is also the same as beta. Suppose this was not the case. That is, the KMP algorithm missed out on certain patterns, they were certain patterns in the text which we could not find the procedure at all, we will argue that certain things cannot happen. 

Suppose there is this missed occurrence of the pattern, (Hindi), these are the same as this. But we missed out on this one. This guy mismatched only at this. This better be the same as this. So this is also alpha. So these alpha characters, this prefix of the string of length alpha matches the suffix of P 1 through Pi of length alpha and alpha is more than beta. So this violates our definition. Alpha is more than beta which is hi. These are longer. These matches this. These is a longer prefix. Yes agreed. But this character is also the same as this. (Hindi). So P l = tk because they matched but tk is not equal to pi plus one because there was a mismatch. So pl is not equal to pi + 1. 
So it could not be that this and this are the same for which reason we did not take this longer suffix or prefix. So it is also the case this and this are the different. There was one requirement that we are looking for the longest prefix which matches the suffix of P 1 through P i and P i +1 is not equal to P of hi + 1. Both conditions need to be satisfied. (Hindi). Here hi value was not correct and this establishes the correctness of the algorithm. So if you have the correct value of hi, these things would work fine. So let?s take an example. This would now be clear after this. I have a certain (Refer slide time 40:16) pattern which is 13 character long. This is the input string.



(Refer Slide Time: 40:25)
 
 

We will expand it out. This is the array h. Suppose I have calculated it in some manner, array h can be calculated for every I with some amount of effort. You can figure out what h of I should be. We will see how to do that also. Suppose I have figured this out, for instance this is the same pattern, at 11 you have 6. Let?s see how we are going to use this information. Let?s do a complete example. So this is my pattern. This is the text. The first mismatch occurs at position 12 which means till position 11 there was a match. So I go to position 11 in the array. I see the 6 written there. There was a 6 written in position 11. So what does it mean? I have to shift by 11 - 6 = 5 units. This 6 is telling me how much of the pattern is going to remain matched after the shift. So 6 characters of the pattern will remain matched. (Hindi). h11 is 6. So I - hi is 11- 6 which is 5. So I have to shift by 5 units. As you can see, the pattern has been shifted by 5 units. 

The pattern is the one on top and the text is below. Now again I will continue to look at this position. Note that I don?t have to try and match the first 6 positions because they are now already matched by the definition of the function h. They are matched. But this 12th position (Hindi). But what position in the pattern is it? This is position 7 in the pattern. The value of I is 6. Now I go to 6 and I see. So h6 is 3. So now I am going to shift by I - h6.  6 - h6 = 3 units. I shift by additional 3 units and this is the situation I have now (Hindi) which we have to shift by 2 units.










(Refer Slide Time 43:15)

 

So we shift 2 units to the right and so we will get a b after the shift we will get a b and something but b and c still mismatch which means we will now shift by 1 unit. a and c mismatch. So we will shift by another 1 unit (Hindi). When you shifted beyond the c, then there was the match from when you found the match. So if you had the h function, you can use that to determine how much you should shift by at every step so that the match can be found and this shift function is designed in such a manner that you will never miss out on any potential match. Every one followed this 210 business? We come to calculation of h. How much time does our algorithm take?

(Refer Slide Time 45:23) 

 

Suppose h is given. We will see how to do the h. (Hindi). We will count the number of comparisons. (Hindi) this was the first step at which we had the mismatch. (Hindi) because it did not result in the match. (Hindi) every time we were comparing with c. So how many times would we be comparing with a certain character? Many times up to the length of the pattern. But that doesn?t sound very good because that would mean that the number of comparisons is huge. For each character, the maximum number of comparisons could be large. But let?s look at the total number of comparisons put together. So let?s call this unsuccessful comparisons. What is an unsuccessful comparison? Comparisons that result in a mismatch. Now how many unsuccessful comparisons are there, over all put together? (Hindi) every time I do an unsuccessful comparison, I shift the pattern. (Hindi) at most, the number of the size of the text types. So the number of unsuccessful comparisons is at most the size of the text which is ?n?. So unsuccessful comparison are not too many. 

How many successful comparisons are there? (Hindi) these were all the successful comparisons. After I shift, these characters are a part of successful comparisons. We will never compare against these characters again. (Hindi) we will never compare this part of the pattern; the part before this red line with the text. So each character of the text which is ever part of the successful comparison is compared only once. There?s no back track happening. The number of successful comparison is at most the size of the text. The total number of comparisons is the number of shifts plus the number of unsuccessful comparisons. So we broke up our comparisons into two sets: successful and unsuccessful. 

Once a character is part of the successful comparison it?s never compare again. If a character is part of the unsuccessful comparison, it might be compared again. But it results in a shift.  So every unsuccessful comparison results in a shift. So total number of shifts since it can be no more than the size of the text. No two successful comparisons can be more than the size of the text. So total number of comparisons therefore it at most two times the size the text. So total number of comparison that are required is the size of the text. 

(Refer Slide Time 50:48)

 
But what about computing the array h? How do we compute the various hi?s and how much time does it take? So we are going to call that preprocessing time. So recall that the size of the pattern was n. What one can argue is that the time required to compute the array h is only order m - the size of the pattern and the time required to do the searching is order n - the size of the text. So the total time therefore is order (m + n). I have not told you this as yet: ?y? is computing the array h. ?y? can we done in order m time. Let?s see why. So we have to compute array h. 

(Refer Slide Time 55:09)

 

What was our definition of an array h? Suppose I have to compute h (i), I look at my pattern. I was certain pattern. This is the ith position. What is h of I, who can remain me what h of I is? The longest proper prefix of the pattern which is also a suffix of 1 through I and that next character should be different [Hindi]. So hi is the value, it?s the longest suffix, longest prefix or longest suffix whatever longest prefix of the pattern. So what are the potential value of hi, what are the different value hi can take? zero to m, zero to I -1 because we also said proper. What is the proper subset? When is the subset proper, when it?s not equal to the set. So that?s what the proper suffixes, when it?s not in the entire string. 

So essentially it can take i different values, give or take a one and then I can try out all those various values. What is that mean? Suppose I am trying out the value h (i) =3. What should I do to try this out? I should look out the last three character here and the first three character and see if the other same. If this is the same as this, if this as the same as this and if this as the same as this then h (i) can be 3, hi is more than or equal to 3 (Refer Slide Time: 53:43). So I will try out 3, 4, 5, 6, 7 all the way up to i-1 and whichever are the possible value I just take the largest among them. So how much time does this take? So how much time do I take to compute hi, I will try out all the i different values. I will do all the so many comparison, so in particular if I choose a value of h (i) equals j then how time do I need to do check whether j is the candidate value? j time. I will take j, j going from 1 through i-1, zero through i-1, so this is i square roughly. So for h (i) I will take i square time, so for entire array h, I will take summation i square I going from 1 through m which is m cube time, huge amount of time. This should be o of m cube; divide by what? (Refer Slide Time: 55:11) We just taking computing the total time (Refer Slide Time: 55:26) huge amount of time right. So we want to do something better. We can use the previous value of h (i) which have been calculated how can we use that? So that?s a nice idea in fact that?s the right idea. So to compute h (i) so we are going to assume the following that when you are trying to compute h (i) you already have the previous values h 1 through h (i) -1. These are already there.

(Refer slide time 1:00:00)

 

Note that in doing this computation of array h we don?t require the text at all. It?s being done just on the pattern. You can take your pattern, compute your h and then you can match it again which ever text you want. So the text can be whatever you want, once you computed this then you can just go head and work with the text. Suppose I am trying to compute h (i) again and I know all the values of h 1 through h (i) -1. How can I use these to compute h (i)? Who can tell me? Span of the stock using the stack. Now what can we do? I don?t think that is the same idea but can someone tell me what should I do here?

So for h (i) -1 we know so if h (i) -1 was let?s say 5, so we know that the first five match with these 5. So these 5 match with these. So these are the first 5 of the pattern, these is the pattern p these also the pattern p. So the first 5 of the pattern match with i-1, i-2, i-3 basically i-5 to i-1 of the text of this pattern itself. Now how do I know what h (i) is? These doesn?t solve the h (i) problem. Now if this 6 character of the pattern matches this then can I say that h (i) is equal to 6. Can I say that? Why not? So I claim the following, if p (i) = p (6), the 6 character of the pattern. [Student: it is atleast h (i-1) so we will try of z assuming that it is at least five, so that?s what we have to do we have to do something like (Refer Slide Time: 59:56). There was zeros also so it need not be an increase, no but it need not be equal to h (i-1). It could be zero also. We are saying but it could potentially if this and this match now, I am sorry I am confusing my array h with my (Refer Slide Time: 1:00:23). Student: h (i+1) is not equal to h (i+1). Staff: so they would not match so it has to be less than 5, so they would be not match. Student: so it has to be less than five greater than six or. Staff: okay so that?s a good question we will take this off in the next class. So today we looked at pattern matching in a text. So we looked at very simple algorithm for doing that which had an order mn time then we looked at the knuth-morris-pratt algorithm which we argued has the running time of order m + n. So that was the improvement, we went from a mn to m + n this is the modulo fact that you can compute the array h in order m time. So we shown you how to compute the array h and order m cube time and we are going to see how to do that in order m time in the next class.  
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 18
     Tries

Today we are going to be talking about another data structure called ?tries? and we are going to see it?s used in pattern matching. so I am going to be starting off with what are called ?Standard Tries? which is the plain version of tries and we are going to move on to ?Compressed Tries?. This is a space sufficient way of keeping tries and the last topic we are going to look at today is what are called ?Suffix Trees?.  So first 2 terms have tries in them and the 3rd has trees in them. Recall in the last class we were looking at pattern matching. Given a piece of text, we were interested in matching patterns. Finding out at what all places certain pattern appears in the text and what we had done there if you recall was that we had preprocessed the pattern. That is, we took the pattern. We computed this failure function h on the pattern and then we used that information to search for the pattern in the text and the Time we took was proportional to the size of the text. 

(Refer Slide Time: 02:45)

 

So this preprocessing the pattern speeded up the Time it took to match the pattern. If we did not compute the failure function h, then we just had this brute force method of matching the pattern which took order m n time. ?m? size of text and ?n? size of the pattern. So after processing the pattern in Time proportional to the length of the pattern, the Knuth-Morris-Pratt Algorithm searches an arbitrary text in time proportional to the length of the text. Now if the text is very large, this is not a very good situation to have.




(Refer Slide Time 02:54) 
 
 

So I have a very large piece of text which doesn?t change and I am searching for patterns in the text. Every Time I search for a small pattern, if I am going to spend Time proportional to the size of the text, that?s a lot of Time. so you have let?s say, collected works of Shakespeare in which you want to search for ?veronica? and you are going to spend Time proportional to the  size of the text which is very huge. Now what we want to do today is to process the text so that I can search for the pattern in Time proportional to the length of the pattern.  see this becomes great because now patterns are typically very small; 7 characters, 15 characters, something like that while your text could be a million characters large. You don?t want to spend that much time every time you are searching for a text. But here today we are going to require that we will preprocess that text. We will work on the text initially and we will have created some data structure on that so that when a pattern comes, we can search for that pattern. All occurrences of that pattern we can spot in a very little Time proportional to the length of the pattern, no matter what pattern comes. In the previous KMP algorithm, it was the other way around. You processed the pattern. You did a preprocessing on the pattern so that no matter what text came, you could search on that text. But there you were taking Time proportional to the length of the text which is quite expensive. We will come to the notion of tries today. 

What is a trie? I will perhaps show you a picture and I will explain it through this picture. So trie is a data structure to maintain a set of strings. Let?s say I have a set ?S? of strings. S = (bear, bell, bid, bull, buy, sell, stock, stop). Now I am going to create a tree here. Now this is not a binary tree. In fact the number of number of children that particular node can have. It can be as large as the size of the alphabet. We are working with the English alphabet. Let?s all our strings are lower case characters. So the size of the alphabet is 26. Each node can have up to 26 children. Now the children of the node are ordered alphabetically. What does it mean? Each node is going to have a particular character in it and if I look at all the children of this particular node, then those are going to be ordered alphabetically. So b will proceed c if c were there and c would come after b and so on. I have just 2 b and s. so b comes to the left of s. this had 3 e, I and u. so they come one after the other in this order. So it?s an ordered tree. 


(Refer Slide Time 06:44)

 

This is read left to. Now how is this organized? Suppose I have to start from here and I have to follow a path in this tree. Suppose I came this way ?b e a r?. (Refer Slide Time: 07:05). Bear is one of the words here. Suppose I were to take some other path ?s e l l?. Sell is another one. ?s t o c k? ? stock. This is another word here. So now you can build this thing. If I give you set of words can you build this trie. It?s straight forward.  What am I doing? At the very first level I am looking at the first characters of all my words and see what are the various occurrences. If you look at the first character I have just b?s and s?s. So there will be one node corresponding to b and one corresponding to s and within this b. then this b has only 5 words associated with it. What are the second character of these words. e i u. so that?s why e i u is the children of b and so on. 

The square here just reflects it?s a leaf node corresponds to a word. Suppose I had built such a trie, how much Time does it take to search for a word here? Suppose I give you a word. How much Time does it take? Suppose I said ?bed?, how much Time does it take to search for bed here? First I come here. So this has two children.  How is this organized? How do you think what kind of data structure would I have to organize this? It?s a multi way search tree. It is in some sense but each node can have up to 26 children as a set. So one way of organizing it is each of the nodes has an array of size 26 sitting inside it. The first location of the array points to the node corresponding to a. second to the node corresponding to b. third to the node corresponding to c and so on. If you organize it in an array, you waste space. Each of the nodes has 2 to 3 children here. In that case, instead of an array you could keep a linked list. 

Ordered according in the alphabetical order in which case you know you will have two nodes here. the first nodes will be pointing to b and the second node will be pointing to s. you will say that this is b and this is s. now given this, how  much Time does it take to search? Suppose I had a link list at each nodes, why would this change the search Time? It is 26 Times the length of the word I am searching for. so I have a link list sitting here (Refer Slide Time: 10:38) and in each of the nodes of the link list, there is a particular character which says that if you are searching for this word and if this is its first character, then follow this pointer. If you are looking for a word beginning with ?S?, you will have to run through the link list first to get to s and then follow the pointer. If s is not there you can stop away but if s is there in which case you will have to follow the pointer and repeat these things. So how much Time does it take in the worst case? You might have to traverse 26 nodes of the linked list into the length of the word. 

(Refers Slide Time 12:32)

 

Let?s look at the operation of find. How much Time does find take? I am using let?s say linked list of presentation on each other. So 26 is the alphabet size. I am using d to denote it. May be the alphabet was not 26 large. You may have a smaller alphabet or a larger alphabet and m is the size of the string of the word that I am searching for. So that?s the Time for find. can you see that this is also the Time for insert and for delete? When you know you are searching, you keep coming down and then you don?t find it any more. If you don?t find it any more, what you do is insert. You will create that letter, put it in the linked list, make a pointer down and may be you will have to create new nodes. You want me to show how you do this?

Suppose we were trying to insert let me quickly do it. What do what do? I want to insert bed. What will I do? I will search for bed. I will come down here. ?d? found. Here I would have a node. I would have seen a ?b?. So I come down here then I would search for a ?e?. I come down here. Here I am searching for a ?d? in the linked list here. There is no ?d?. So I create a node. Now it will be a square node because it?s the end of the word and this would have ?d? written in it. But if I had ?b e d s?, then I would create one circular node and then a square node below. I might have to create such a longer change here. In any case total Time taken would be proportional to the length of the word. We will see later when one of the words is the prefix of the other. That?s what you worried about. (Refer Slide Time 12:32)


So find, insert and delete all take the same Time order dm but one thing is bad with this data structure and that?s the space requirement of the data structure. How much space does it take? 26 Times the number of nodes. How many nodes are in this tree that we have created? Total number of characters in the entire text which is the size of the text. That?s the worst case and it can be close to the worst case. I have let?s say 10 words. Let?s say I have 10 words, each beginning with a different character. The first word begins with the ?a? second with the b third with the c the fourth with the d and so and on and you can make a long chain below this depending upon what the size of the word is. There can be as many as many as total. Not total number of words which is exactly total size of the strings in it. by size I mean put all the characters together and their total size. Let?s call that double. So that?s the space required which is too large. So we got to do something about this one. Before that, let?s see applications. So this actually does our task what we started off with.  

(Refer Slide Time 17:07)

 

So suppose I give you a peace of text and we take all the words in the text and throw them into a trie. I make a trie out of all the words in the text. Now if I have to search for a particular word, I can search for the word in Time proportional to the length of the word. This is what we started off today. So I can do word matching. Find the first occurrence of word ?x? in the text. Why have I said first occurrence? It will be inserted only once. So one occurrence I can detect by doing that. We can also do all occurrences. We come to all of this in a second. Let me show you an example and you will see that we can actually even do all occurrences. So each of these operations of matching is done by tracing the path corresponding to that word in this trie. So let?s look at an example. This is a piece of the text. 














(Refer Slide Time 18:27)

 

So you have a bunch of words. ?See? for instance appears twice. so I am looking at all  the distinct words that are there in this piece of text which is ?see?, ?bear?, ?sell stock?, ?buy stock?, ?bull?, ?bid? and ?bell?. So these are all the words I threw them into a trie. This is the trie I get and as you can see, this leaf corresponds to ?b e a r? (Refer Slide Time: 19:15) and bear occurs at position six that text that is so with this leaf I store 6. 

Let?s look at the bid. Bid is occurring at two places. Perhaps starting at 47 and starting at 58. So I will store both 47 and 58 here (Refer Slide Time: 19:42). This is what we call preprocessing the text. I took my initial text and did something, built this trie on it, stored this information in each of these leaves, so that now if you come with queries like where does this particular word appear, I can quickly tell you. How much Time do I need? It?s very little. It?s just proportional to the length of the word and I will be able to tell you all the places where this word is, by looking at this number down here. This doesn?t really solve the problem that we were talking of in the last class which was that I give you piece of text and I give you pattern and find where all the pattern appears in the text. Because my text need not be a collection of words. As I said you know my text could be let?s say, sequence of basis in a gene database. So I have just a long sequence of ?A C T G? that kind of thing and I am searching for a particular sequence in there. so here we have a separate notion of words and if we are searching for words, that?s  okay. Suppose I was searching for ?a r blank s e?, then I cannot search for patterns here. So if I have a pattern like that. So if I don?t know if I think of these blanks also as some special character of my alphabet, then I cannot really search for anything. 

So the reason I can search here is because there are well defined boundaries. My pattern has to begin with the boundary and end with the boundary. That?s why I can search. So now first we will address the issue of the large size that this trie has. Let?s try and reduce the size of the trie first. We will do the following.



(Refer Slide Time 24:19) 

 

We are going to look at all nodes of the trie which have degree only one and remove those nodes. By degree one I mean who have only one child and I am going to compress those nodes. So if this is my standard trie, see that there are whole lot of nodes here which have only one child. (Refer Slide Time: 23:30 to 23:40) this node for instance this node this node this node in fact this node as well as this node. This node also has only one child this has only one child. So I am going to compress that. By compress I mean I am going to take the child and collapse it in to the parent and if the resulting node also has only one child, I am again going to take the child and collapse it in to the parent and keep doing this. In my previous example, I had said I have trie in which there were a bunch of words, each of which begins with a different character. So I had created a long chain like this. Now if I compress them in to a single thing, then this entire thing becomes just one node. So I will just show that to you in a second. This is what my compressed trie would look like (Refer Slide Time: 24:39).

















(Refer Slide Time 24:36)

 

b and s are the same. ?i d? collapse in to one. Now a node doesn?t have one single character but a string as its labeled.  This e a r collapses here. l collapses there and so on. As you can see, the compressed trie is smaller than the standard trie. Why would this take less space now? What is the number of nodes in this thing now? 

(Refer Slide Time 25:29) 

 

So this is a fact which will prove for yourself. It?s very simple. Suppose I have a tree which has ?l? leafs in it and every node of the tree has at least two children. Every internal node of the tree has at least two children. Then the number of internal nodes cannot be more then l -1. a tree in which every node has at least two children, by that I mean except leaf node clearly. Leaf nodes don?t have any children. Every node has at least two children. It has at most l -1 internal nodes where ?l? is the number of leaves. If every node has at least two children, then the number of internal nodes is not too much. This is a very simple thing. You can prove it by induction. How are we going to use this? How many leaves are there in my trie? It?s the number of words. This says that the number of internal nodes is going to be (number of words ? 1) at most. So the number of nodes in a compressed trie is order of s where s is the number of words. S was the set of string. So s is the number of words. This is the number of nodes in a compressed trie. This doesn?t solve our problem completely. Why because each node now has a longer label inside it. We will also have to store that label. We need space to store that label. From where do we get that space now? 

We are going to store labels not as labels but as numbers. Let?s see what I mean by that. Let?s look at this label ?i d?.  This was the last two characters of the word ?b I d?. 

(Refer Slide Time 27:27)

 

So ?bid? is the 6th word in my collection. I have kept all the words in some array in some arrays and id is the last two characters. So in the 6th word, ?i d? begins at position one and ends at position 2.  So each of these labels no matter how long they are, can be stored as three numbers. 

This is because each of these labels will be a substring of one of these words. Do you follow what I mean by sub-string of one of the words? Not necessarily a suffix or the prefix although in this example it looks like a suffix while it is not necessarily.  For instance ?to? is not a suffix. I am not saying prefix or the suffix. I am saying it?s a sub string. It is a contiguous part of the string.  Now what is the space used by the trie? You will have to store these words somewhere. This is your input. This is stored somewhere. So we are just trying to figure out how much additional space we are taking for the data structure there. Then how much additional space are we taking? Now this space I am taking by this data structure is number of nodes. Number of nodes is two times number of words at most into three for three integers each because then there are some pointers. How does searching happen? Let?s look at that. Now how does insertion and deletion happen in a compressed type?

(Refer Slide Time 32:52)  

 

Suppose this is the trie I have. So the labels that I have put at a node, we can also think of that they are the labels on the parent edge of that node. It?s one and the same thing you understand what I mean? There is also a subtle reason why I am doing it this way and we will see why. I am searching for this string ?b b a a b b?. I start searching. So conceptually it is the same as saying I see a ?b? here and come down. Then the first here is the a. this is a b. so I should go this way and I come down here (Refer Slide Time: 32:04 to 32:29). Now the third character I have is an ?a?. So I am looking for an ?a?. So the first character here is an ?a?. The first character here is a ?b?.

 So I should come down to ?b? and then I start matching this with this the label here. We are not doing anything sophisticated. We should now get familiar with this.  We are searching for this pattern. We are moving down the tree. (Hindi) degree is the number of children. So we are inserting b b a a bb which means we first search for it. We search for it. We reach the middle of this edge. Till the middle of this edge, we have matched b b a a. (hindi) next character is b (hindi). Red node is the node I?ve inserted (hindi). This is how you will insert. Now how will you delete? We proceed, we find the node and we delete. Now something else has to be done. Suppose I have to delete b b a a b b. I will come here and I will delete this guy. Now I look at the parent. If the parent has only one child left now, collapse the child with the parent and you might have to do this multiple times. So it?s a very simple data structure. I am leaving out the implementation details. You will have to figure a few things out. Tries are very useful they are used in web search because you can imagine why.









(Refer Slide Time 36:59)  

 

Imagine you are going to Google. You type a word and it retrieves to you all the web pages that have that word. That part which helps you retrieve all the things is called the index of the search engine. So it is stored as the compressed trie typically. I am not saying that Google doesn?t this way. This is what generic search engine do and each leaf of the trie is associated with the word. (hindi).  That?s called the ?occurrence list?. The trie is kept in an internal memory. The list can be very long. If you type a word like ?computer?, you imagine the number of URLs pages that could contain that word. So this occurrence list will be huge. So that?s why it?s not kept in the main memory. It?s kept on disk. 

Now suppose you wrote ?computer and music?, so now it will search for computer and it will search for music. It will get two occurrence lists. Now it has to take their intersection. So Boolean queries corresponds to set operation of this occurrence list. If it is ?and? it is union and if it is ?or?, it is the intersection. Of course there are lots and lots of techniques that go into speed up the thing. You eliminate stop words and many other things. We will not go into that.  They are also used in internet routers.















(Refer Slide Time 39:02)

 

Now you are all perhaps familiar that each computer on internet has an internet or an IP address which is a 32-bit number. So type google.com. You can use nslookup to find out the IP address. So a particular organization just uses the subset which are all related in a certain manner. For instance, all IIT Delhi address will look something like 10. Now how is routing done? In a router when packet comes in, it has an ip address written to it. It doesn?t say if this is the IP address, send it here. Router is a bunch of links coming in links and going out. So packets comes on one of the links and the router has to figure out which links to send it out to. There are 232 IP addresses. 

It says take the IP address of the packet and find out the longest match. Your table would have the following. Anything that begins with a 10, send it here. Anything that begins with the 10.27, send it here, anything that begins with the 10.27.36, send it here. So now what is the router going to do? It?s going to find out the best possible match of these three. It will try to find out the longest match. So if the packet had 10.27.36, then it will go on the 36 route. But if it was 10.27.34, it will take the 10.27 route. If it was 10.28, it will take the 10 route. This is the way routing tables are organized. So they are also tries I used to do this. Tries could be one way of doing it. So we will come back to pattern matching now.













(Refer Slide Time 40:01) 

 

We saw compressed tries are doing the job reasonably well provided there was the notion of words or delimiter and our pattern started and ended at the delimiter. But suppose you are as I gave you an example if you are searching in a biological data base there is no notion for the delimiter there. What do you do then? So this is something we said before. Instead of preprocessing the pattern, we are going to be preprocessing the text. Now what we are going to do is the notion of what?s called the suffix tree. We will take all suffixes of the text and organize them in to a tree and you will see what I am trying to say in a second. Let?s see piece of text x a b x a c (Refer Slide Time: 41:09). 

(Refer Slide Time: 41:09)

 

How many suffixes does it have? There are 6 suffixes. I am not saying proper suffix. I am going to take them as my words. x a b x a c is a one word. a b x  c is another. b x c is another. x a c is the 4th. a c is the 5th and c is the 6th. There are 6 words and I am going to create a trie of these words, in particular a compressed trie and this is what the structure is. 

So let?s see why it?s a trie. Here if it were a ?b?, I would go this way. If it were a ?c?, I would go this way. If it were an ?a?, I would go this way. If it were an ?x?, I would go this way (Refer Slide Time: 42:06). (hindi) numbers are the starting position of that suffix. So this corresponds to x a b x a c. what is the starting position? It?s one. This corresponds to x a c. Its starting position is 4. This corresponds to a b x a c. The starting position is 2. The starting position for a c is 5 and so on and so 4th. So put all our suffixes in a trie. So it?s essentially a compressed trie for all the suffixes of the text.

(Refer Slide Time 44:09)

 

So it seems it would be huge but why should it be huge? How many suffixes are there? There are as many as length of text. So we will have that many words. Recall that the size of the trie is just the number of words order number of words. So its order length of the text. (hindi) so this size of the trie is not too much. So suffix tree for us text x of size n from an alphabet of size d stores all the n suffixes of x in order n. d is typically small. So it doesn?t require too much space. (hindi)We will come to why we are doing suffixes. Can someone think of why suffixes? So I was searching for a b. what will happen if I am searching for a b? Suppose I start searching for a b. I will come at ?a? here and then I will come at ?b? here and I will stop in the middle but can I say something now? I did not find. That?s what you will be tempted to say.  If the pattern appears in the text then there is some suffix whose prefix is that pattern. That means that there is some word in the collection of words that I have thrown in whose prefix is that pattern which means that when I am searching for the pattern, that initial part of the word will match up and I will be able to do something with that. Many of you can see what I will be able to do. I will just look at the leaves of that sub tree and identify. We will come to all of that. That?s the remaining of this lecture. So let?s say I had this word ?minimized?.


(Refer Slide Time 46:49)  

 

I don?t make the suffix tree for each word in the text. I make a suffix tree for the entire text. So if this is my entire text I make a suffix tree for it. There would be 8 suffixes. This would be the corresponding suffix tree I would get.  Once again I have collapsed my nodes.  You can all make this suffix tree and now we want to do a compact representation once again.

(Refer Slide Time 47:58)

 


How much space do I require? So instead of storing labels, there are big labels here. We don?t want to store labels. So once again we can store by numbers. Once again each one of them is a sub string. So I just need to know what the start and the end position of the sub string is. I don?t even need three integers now. I just need two because they are all part of one single text. So this is for instance what would happen. m I n I m i z e - m i n i m i z e starts at position 2 and ends at position 7. So I can store it very efficiently.  Now this is the key thing which we are using in pattern matching.

(Refer Slide Time 48:22) 

 

So if I have two suffixes ?x a b? ? x a c? and they have the same prefix ?x a?, their corresponding paths are the same at the beginning and it?s just the concatenation of the edge labels of the mutual parts. So x a b x a c x a b x a c, its common part is ?x a? and it comes here (48:53). This is going to be crucial in a short while because now if I was searching for x a, I would end up here (Refer Slide Time: 49:12). So I have to actually report all occurrences and this will help me do that. So what do I have to do report all occurrences basically I have to look at its children. Look at the leaves in the sub tree and that will give me the position. We will come to all of that. This was the problem that some one had pointed out very briefly in the beginning. If one word in my trie is contained in another word, what happens? Suppose my text is x a b x a, now what is going to happen? I have one suffix which is x a and another suffix which is x a b x a. this suffix x a is a prefix of the other suffix. So in my trie, what is going to happen? You know one of the words is going to end up at some internal node. You don?t see a big problem with this? Let me quickly show you what I am trying to say. 












(Refer Slide Time 50:54) 

 

Let?s call it ?x a? and ?b x a?. There are 2 suffixes ?x a? and ? x a b x a?. We are ignoring the other suffixes. (hindi) what is special about dollar? There is nothing special about dollar. It just is a character which is not part of an original alphabet. (hindi) so now how does one build a suffix tree? 

(Refer Slide Time 53:10)

 


We start with one initial - one suffix. Let?s say this is the entire text. So that basically is one edge and then we keep breaking this edge.  So we will search for the next suffix. (hindi). starting at the root, find the longest path from the root whose label matches a prefix of si through n. at some point if no matches are possible and if this point is at the node, then we denote this by a ?w?. If it is in the middle of an edge, we insert a new node and then call this node ?w? and we create an edge running from the root to the suffix that we create. So we can take an example quickly. One suffix is ? x a b x a c?. (hindi)

(Refer Slide Time 55:16)

 

So this will take time proportional to the length of the text. If the length of my text was n then it takes time order summation n2. It is a bit more but we will see what we can do about this one.

(Refer Slide Time 55:29)

 

This is the key idea that we are exploiting in pattern matching. So given a pattern ?p? and has text t, our aim is to find all occurrences of pattern p in the text. So the idea of algorithm is that every occurrences of p in t is a prefix of a suffix of t. (hindi) thus an occurrence of p can be obtained as concatenation of the labels of edges of the path beginning at the root. So how do we do pattern matching? We build a suffix tree for the text, match the characters of the pattern along the path beginning at the root until the pattern is exhausted.

(Refer Slide Time 56:25)   

 

If the pattern is exhausted completely, that means that we have found a match. If no more matches are possible, then that means that pattern does not exist. So 2 in this case, p does not appear anywhere in the text. In case 1, p is the prefix of a suffix of a certain suffix which is obtained by extending the path and till we reach a leaf. Each extension gives a suffix. All the leaves we can reach from there will tell us the occurrences of the pattern. Each extension provides af occurrence of the p in t. what are the extensions? They are basically all the leaves below that. (hindi) let?s quickly see an example.


















(Refer Slide Time 57:44) 

 

This corresponds to this suffix. This corresponds to this suffix this corresponds to this suffix (Refer Slide Time: 58:08). The number that you write here is the starting position of this pattern of this suffix. So we write a 7 here. We saw only an order n2 algorithm for constructing the suffix tree. 

(Refer Slide Time 58:41)

 

It can actually be done in order n time but that?s a fairly complicated algorithm. We will not be doing it in this class. So that gives us the total complexity of pattern matching.



(Refer Slide Time 58:56)

 

So preprocessing which means building the suffix tree. We said it can be done in order n times proportional to the size of the text although we saw only an n2 algorithm today. And for searching I?ve said size of the pattern plus ?k?- number of occurrences of the pattern in this. 

This is completely essential. If a pattern is only 3 characters long but occurs one thousand times in the text and you have to say all the times it appear then clearly you are going to take time proportional to 1000. So this is clearly a requirement and why is this coming up? This is because we have to report all the leaves of this node.

(Refer Slide Time 59:41)

 

If there are k leaves we have to go and report all the k leaves. Have many internal nodes are there in this sub-tree? There are (k ? 1) nodes. What is the size of the entire sub-tree? It?s of order k. 

(Refer Slide Time 1:00:42)

 

That gives us the total complexity of pattern matching. Let me go the last side. The total space we require is the proportional to the size of pattern to store the pattern. So with this I end today?s lectures. So we looked at a faster, faster in the sense now we decide to preprocess the text and to search for the pattern we just need time proportional to the length of the pattern. 
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 19
Data Compression

 Today we are going to be talking about Data compression.

 (Refer Slide Time 01:17) 

 

We will begin with what the idea behind file compression is and then we are going to be talking about Huffman Tries which is the way of doing data compression. We are going to see how ?ABRACADABRA? translate into these sequence of 0?s and 1?s. So what is file compression? As you know, if you have a piece of text, it?s stored as bits in your computer and what is typically done is that, for each character, you have what?s called an ?ASCII code?. So if you were to go into a unique shell and type <man ASCII>, then that will give you the ASCII code for all the various characters.  













(Refer Slide Time 03:31)

 

The ASCII code is an 8 bit code which means that every character is stored as 8 bits. So this is what is called ?fixed-length encoding?. why fixed length because for each character, I have the same number of bits but our idea today is to try and reduce the amount of space required to encode a piece of text. If each character I am going to use 8 bits then the total number of bits required will be 8 times the number of characters in the piece of text. But suppose I don?t have to do fixed-length coding. 

You know some character might have two bits associated with them. Some character will be encoded using three bits, some using four and so on, can we exploit this and the fact that some characters occur more frequently than others to design a coding screen which will represent the same piece of text using lesser number of bits using lesser number of bits. You understand the need for doing this kind of compression?  Clearly the lesser memory you require, you know if you are transmitting the file, you have to send less number of bits.

If you are storing, you will have to store less number of bits and so on. So it?s very useful to be able to compress the information that you have. That will bring us to what we call variable-length coding. So the number of bits used to represent each character would be different. In particular, character which occur more frequently, we are going to represent them using less number of bits and use that. Characters which appear very infrequently, let?s say x or z in the English alphabet, we can have longer sequences. May be more than 8 bits. Let?s see how this is done. So let?s say my piece of text is just 4 characters java and I decide to encode ?a?. How many characters are there? 








(Refer Slide Time 07:00)

 

Suppose my alphabets were just a, j and v.  The only things that ever encoded were strings on this alphabet a, j and b. So java is1 example of such a string and suppose I were doing fixed length encoding, how many bits should I associate with each of these? I will need at least two. I can?t do with just1 because there are three different characters and there are only two possible values if I choose1 bit. So I need at least 2 bits and if I take 2 bits then how many bits do I need to represents java? 2 times 4 is 8. As straight forward as that. But suppose I decide to use 0, just single bit for ?a? and11 for j and 10 for v and I will tell you why I am doing this. Then java can be encoded as110100.  That would be the encoding and it will take only 6 bits. It will take the lesser number of bits. Then you can ask me, ?well, why did I do j as11 and v as10?? so the problem with variable length decoding - variable length encoding is that of decoding. How do you decode? Given a sequence of bits you want to decode it uniquely. So suppose I gave you a sequence of bits, then you should be able to retrieve java from this. Of course I?ve told what the codes were. You should be able to get back to java. Suppose for instance, I had used this as my encoding, for ?a? I use 0, ?j? is 0 1 and ?v? is00. So still it should take 6 bits only and the encoding would be 01000 and 0.  From here can I get back to java given this code? Well 01 could be either ?a? or it could be a ?j?. It has to be a ?j? because we are using this ?1?. So what would you do with these 4 0?s? It could be ?java?, it could be ?j v v? or it could be ?ja?. It?s ambiguous. You see the problem?  If you have to use variable length encoding, then you could have this problem of ambiguity while decoding. 

So you have to be careful when you are using variable length codes. This problem would not arise when you have fixed length codes. You understand why because you will take those many bits and then you now determine what exactly the character was. So to prevent ambiguities in decoding, we will ensure that our encoding satisfies what?s called ?the prefix rule? which is very simple. It says that no code is a prefix of another code. By code, I mean the bits I use for a particular character. When this was our code, you can see 0 was not a prefix of either j or v. j and v were also not prefixes of each other. The encoding arising out of this would be unambiguous and we will see an example to show that to illustrate that. But the encoding arising out of this (Refer Slide Time: 08:15) will be ambiguous because ?a? is the prefix of these (Refer Slide Time: 08:21) and I will show you an example. You must understand what the prefix rule is. So if your codes satisfy the prefix rule, then decoding will be unambiguous. But if it does not, then you will have ambiguity in decoding. So I will come back to the prefix rule in the next slide. Code is the collection of code words.

(Refer Slide Time 08:48)  

 

What I have written out here is a code. Each one of these is called a code word. For each character the sequence of bits what?s called the code word and the entire thing is called a code. So code which satisfies the prefix rule can be represented as a tree. In particular as a trie. So recall the ?trie? that we have discussed in the last class, the branching was a 26 way branching. But here branching will only be a 2 way branching. Each node will have only a 2 children. Here my alphabets are a b c d r. five characters only.

The characters will be stored at the leaves or the external nodes and every for every node the left edge will label with 0 and the right edge will be labeled with 1. Now a = 010 because you know if you look at this, from root two A, you will encounter 010. When you are coming to R, the code for R will be 011. Can you see that if I drew such a picture for you, then the code word for any character will not be a prefix of the codeword for some other character.  Otherwise it could have ended midway. But it?s not because everything is a leaf. Each character corresponds to a leaf. That?s the very first statement here. So the code word for one character would not be a prefix of code word for another character. We will represent our codes using such a we trace a from the root to the particular leaf to determine the code word for each character. I need not have shown this picture at all. I could have just drawn this trie and from this you can figure out what is the code word corresponding to this.  Now how do we do the decoding? Suppose this is my trie and these are the code words (Refer Slide Time: 11:33).







(Refer Slide Time 11:28)

 

Now I give you a sequence of bits that?s this is my encoded text and I have to decode this text (Refer Slide Time: 12:01). As you can see, the code satisfies the prefix rule. So how do I do the decoding? So I start from the beginning. You always start from the beginning. You start from the beginning 010. So you will trace out 010. You will get a leaf. You stop and these 3 characters go away and I get an ?a?. So I have struck off these 3 and I have written down an ?a?. Now I will take 1 and the next 1 is also1. So I get a ?b? and I will strike these two off. I have taken care of these two. Now I get a 011. Its 01.1 so 0 remains. As you can imagine it will come to the same ?abracadabra?. So that?s what this will turn out to be and we can decode it and see.

(Refer Slide Time 13:01)

 

Suppose I give you this trie and I give you this encoded text, how much time does it take to decode? Clearly I am just looking at a bit and I am going to spend 1 unit of time with every bit. I just look at that bit and go down one level in that trie.so it?s basically length which you have to spend clearly. So this is another trie (Refer Slide Time 13:55) and you know this is a long piece of encoded text and you can figure out what this is.

(Refer Slide Time 13:55)  

 

You can take this is an exercise. So what is our aim in doing this?

(Refer Slide Time 15:52)
  
 

Recall we wanted to build up a code in such a manner such that the total length of the encoding is as small as possible.  Suppose this trie was specifying my code and once again I was encoding abracadabra, this has 29 bits. How will you compute such a thing? Well let?s quickly do that to make sure that you understand.  What is the frequency of each character? A ? 5,B - 2, C ? 1,D ? 1 and R ? 2. Im using 3 bits for A, 2 for B, 2 for C, 2 for D and 3 for R. I am just counting the number of bits. So this becomes 15, 4,2 and 6.29 totally. Now suppose I had another trie say this one (Refer Slide Time 16:06), you think this will have less or more? 

(Refer Slide Time 16:34)

 

This will have less.  ?A? was occurring 5 times. Here I?m using 3 bits and I am using 2 bits here. I?m saving 5 bits for ?a?. Of course, I will have to compensate elsewhere.  c and d, there are 2 here and 3 there.  But c and d occur very infrequently. So it?s good. This will have less than 29.  We are also saving on R. this you can check. It will require 24 bits. We have to design this in such a manner so that the number of bits required it as little as possible. So let?s try and understand. What is it that we are given? We are given the frequency of that character. Suppose I give you a piece of text. You will count the frequency of characters and we are trying to compute the trie so that the length of the encoding we get is as small as possible.












(Refer Slide Time 17:36)

 

Recall that each character is a leaf in our tree. Number of bits used to encode the character is its level number where I?m assuming that the root is number 0. So if the ith character has frequency of f i and has level number of  l i, then what is it that we are trying to minimize? It is summation fi li. so we have to choose a tree so that this quantity is minimized. Our tree will determine the li?s. fi is given to us.  We cannot change the fi?s. We can pick a tree so that we can get appropriate  li?s.  So summation fi li is called the ?total external weighted path length? of a tree. It should be very easy to see why.

(Refer Slide Time: 19:51)

 


External because we are talking about external nodes which are our leaves. What is its path length? The length of the path from the root to the leaf which is basically the number of levels. ?Weighted? because we are multiplying it by the frequency and ?total? because we are summing it up.  So we are viewing each leaf as having a weight which is equal to the frequency of the corresponding character.  The weights in the weighted are referring to this frequency.  From now on, I might call the same thing is weight or frequency. This means the same thing. Here for instance given these weights or frequencies, f1 through fn, we wish to find the tree whose total weighted external path length is minimum. 

That?s what we want to do. We are given the weight on the leaf. We want to build a tree whose leaves will be these and who?s weighted external path length will be minimum and will denote this minimum weighted external path length by this quantity. so given ?n? leaves with weights f1, f 2, f 3,? fn; your problem is to build a binary tree  whose leaves will be these ?n? leaves and which will have a minimum total weighted external path length. So we have managed to translate our question of finding the minimum length in encoding such that the length of the encoded message minimum to that of designing an appropriate tree.  One thing I am going to assume is that when I write it in this way, f1 is smaller than f2 and these are in increasing order. Now let me show you what the algorithm is. Once again, my text is the same ?abracadabra?. There are 5 characters and I have put down the frequency of these 5 characters.  

(Refer Slide Time 20:58)

 

 









(Refer Slide Time 22:13) 

 

I have put red boxes around them. These will have to be the leaves of my tree. Now what I am going to do at the very step is I am going to take the 2 with the smallest value which are the ones in the ends. These are the two smallest ones. I am going to combine them. How am doing that? I am going to create another node. I am building up a tree now. These are my leaves. I make another node as the parent of these two and this node gets the value of weight of sum of these two. Now these two will disappear from the picture and I will just be left with 4 nodes and I will repeat the process. So what is the process? Take the two smallest and combine them together into one. Take the two smallest and combine them together into a node and when you combine, you basically sum up their weights. So at the next, we have an option .we can either combine b and r or we can combine r with this one (Refer Slide Time: 22:13) that we have created. Let?s see which one I take. I decide b and r to be combined into one.  When you have an option we can pick whichever you feel like.  

So they have combined into one and what we are left is only three node. Now many times I will have to do this process? If I started off with ?n? leaves, how many times will I have to do this process? Every time you are reducing a node by 1. so it is (n -1) times not n by2. Now which will we combine? Two and four clearly because they are the two smallest ones.  

So you combine them into one and we get a six. Finally we have only two nodes left ? 5 & 6. They will get combined. This is the picture. We will combine them into one and these are 11.now how do I label? I can just label whichever way I like to. It really does not make a difference. The length of the encoding was determined by the depth of these things. This becomes encoding now and the claim is this is the best. This will give you the same minimum.  Whatever was the minimum for abracadabra will be achieved by this. As you can see ?a? which was occurring five times is getting only1 bit. Looks like it is the right thing to do. So this is our final trie.




(Refer Slide Time 24:07)  

 

This is my text. This is the corresponding code. As you can see for ?a? you have 0. For b you have 100, the next three characters. For r you have 101, the next 3. For c you have 3 and so on and on. There should be a gap between this 101 and 0 because ?a? corresponds to the last one and these are only 23 bits. These is even better than the previous code which was 24 and this is the best possible which is what we will argue in this class. Can you do better than this?  Let me take the same example and then build another trie.  How can I build another trie? We recall that there was an option at each point. Let me take the other side of the option. Let me see what trie I get now. Here there are 2 minimums. Here there are 3 2?s (Refer Slide Time: 25:44). First I combined these two. Let me do that. I decide to combine r with this one (Refer Slide Time: 25:49). I get a four.  

(Refer Slide Time 25:50)

 
Now which do I have to combine? This four with this two (Refer Slide Time: 25:55). I combine the four and the two and then I get six. Finally I am going to combine this and this.  So this is the final trie I get. It?s the same piece of text. Once again ?a? gets only 1 bit. b gets 2. r gets 3. ?a? gets 1. ?c? gets 4 now and d also gets 4.  You think it will be different from 23? It should not be.  Otherwise the theorem I am claiming is false. It should be the same.  You can count. It will be 23 because this algorithm is computing the tree with the minimum weighted external path length. Since it?s the minimum, it cannot be smaller or larger than the other one because they are both the minimum. So we now need to argue correctness. Why is this computing the minimum? 

(Refer Slide Time 27:40)

 

Why does this algorithm compute a tree with the minimum weighted external path length? So there?s no reason to believe its too simple to do anything useful. Let us see what the argument for this one is. We will be proving it by using induction on the number of leaves which is same as the number of characters.  Suppose I gave you only two leaves, then what is the algorithm going to do? It will just combine them into one and so it will basically give you a tree with the three nodes, one root. this will be 0 and this will be 1 (Refer Slide Time: 27:54) and this will be something and this will be something and clearly it?s using 1 bit for each of the character, you cannot do better. You cannot take 0 bits for a character. so it?s true when you have only two leaves. So we are going to assume the claim is true when you have (n -1) characters or leaves and we are going to show that it?s going to be true when you have ?n? characters or leaves. So when we have ?n? characters, what are we doing at the very first step? We are taking the two characters with the smallest frequencies and combining them into one.








(Refer Slide Time 29:22)

 

We are taking the two characters frequencies f1 and f 2 and we are replacing it with one node of weight f1 + f 2. It?s as if you have one character now of frequency (f1+f2). so once it does that, beyond this point the behavior is as if it was given (n-1) characters with frequencies as f1 + f 2, f3, f 4, f5 upto fn.  Beyond that the point it?s the same behavior. so beyond the point the algorithm behaves as if it had only (n-1) characters with frequencies f1+f2, f3 all the way up to fn. using our induction hypothesis it would have computed the best possible tree because these are only (n-1) characters now and it would have computed a tree on these (n-1) characters with total weighted external path length as this quantity.  This was the minimum quantity. This was the notation we used for the minimum. The tree computed by this algorithm has weighted external path length f1 + f2+ this quantity (Refer Slide Time: 32:02).  This is what our algorithm has computed. This is the weighted external path length computed by this Huffmen?s Algorithm. Now we have to argue that this is the minimum possible. It will not go lesser than this. What we will argue is that the best solutions for f1 through f n equals f1 + f 2 + exactly the quantity that the algorithm had computed. We will argue this now.  















(Refer Slide Time 32:56)

 

This quantity that algorithm has computed is actually the best. It is the minimum weighted external path length when your weights are from f1 through fn. How do we argue this? This will follow from the fact that in the optimum tree, by optimum tree I mean that tree whose weighted external path length is minimum. So let?s prove this fact.  Suppose this factor is true, how does this implies this? Why does this imply? Let?s do one step at a time.

Suppose had proved this factor that in the best possible tree suppose we had prove that in the best possible tree that two lowest weights are siblings. We will do that in a slightly more formal way. Let?s assume that the two minimum are always siblings. Let?s argue that if this is true, then it will imply this (Refer Slide Time: 34:18). We have our best possible tree and the minimum are siblings. Why does it imply that we found the best> That?s because it implies the best over f1 through f n equals this. This is the best tree. we are going from here to here (Refer Slide Time: 35:28) which means that we have assumed this statement and we are proving that the best tree over f1 through f n has weighted external path length equal to f1 + f 2 + the weighted external path length of the best tree over f1 + f2 through fn. so now we are kind of trying to mimic what we have already done. let me now look at this tree. Now I am just looking at the remaining tree. This is a tree (Refer Slide Time: 36:08) and let me give this node weight equal to f1 + f 2. Now this is the tree over leaves f1 + f2, f 3 up to fn. What will its minimum weighted external path length become? If this is the best tree for the entire thing, for these values or leaves, this should be the best tree. We are looking at the best tree for f1 through fn.  I am saying the following. 

Let?s cut off the two leaves and just keep that. Let?s give this a name of f1 + f 2. Then this tree is the best tree for these choice of weights also. Suppose there was something smaller possible, then this (Refer Slide Time: 37:21) blue would not have been and best tree for these guys. The best would be a green tree.  So suppose the black tree was the best, better than this red one. (Hindi ) This tree is the best possible tree when you have leaves with weights f1+ f 2, f3, f4, f5, f6. This is the best possible tree. So this red weighted external path length is this quantity then and so blue total external path length is red external path length + f1 + f 2.
(Refer Slide Time 40:51)

 

So which means blue weighted external path length which is this leaf is equal to this leaf which is equal to the best possible (Refer Slide Time: 41:28). What did we get in our previous algorithm? We got the right hand side exactly.  So the algorithm is completing the best possible. If this black is better than the red, then what have I done? I have only increased the black by a quantity equal f1 + f 2 while this blue also differs from the red by this quantity f1 + f 2. If this black is better than this red, then this bigger black is also better than the bigger blue which violates our optimal case. But why is this statement true? 

Why should it be the case that in the optimum tree, the leaves with those two lowest frequencies are siblings? Let?s take the leaf with the lowest weight. It will have the maximum level number. Suppose this leaf with the lowest weight comes in between and there is another leaf with a higher weight, this is not optimum. So we have to swap these two. So the total external weighted path length becomes minimum. So the leaf with the smallest weight has to be at the last level. Let?s look at its parent and let?s look at its siblings which is this (Refer Slide Time: 43:53) leaf. By the same argument this leaf is the second smallest weight because if it for anything else then once again you can swap and reduce. So the leaves with the two smallest weights are actually at the very last level. In all our examples you must have seen that happening. They are all at the last level and they are siblings. You just use this fact, make them siblings and then the problem reduces by 1 because now you have only (n -1) nodes. How do we take care of (n -1) nodes? It is the same way we took care of them.  Once again you take the two smallest ones, make them siblings and continue. So just this one property being exploited in this algorithm. We got the very simple algorithm to compute the best possible trap. So with that I am going to stop today?s class. After we have done priority queues, we are we are going to analyze these particular algorithm to compute its running time. So I?m leaving the bit about computing its running time today and I will take it up after we have developed the notion of paradigms. 



Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 20
Priority Queues
Today we are going to look at the priority queue abstract data type. I am going to motivate it with the scheduling example then we are going to look at what the data type is, we are going to see how to implement the priority queue with the sequence then the key thing today we are going to do is to introduce the concept of binary heap. We are going to see how to do procedures for insertion in a heap and procedure called heapify which will be using in subsequent classes to see how to find to delete the minimum element in the binary heap and for other operations on heap.
(Refer Slide Time: 01:48)
 
So we have a multi user computer system, that?s the system in which there was multiple users who can submit jobs at different points in time. 






(Refer Slide Time: 03:16)
 
We assume that when a job arrives, we know in advance how much time the job is going to take on the particular system that we have. Further we are going to assume that the jobs can get preempted that is that job is running, it can be stopped some other jobs can be schedule on the processors and then later this job which was stopped can be resumed. One policy which minimizes the average waiting time of job is what is called the srpt. The srpt rule, its stands for shortest remaining processing time. What this policy says is that the processor schedules the job which is the smallest at any point in time which has smallest remaining processing time at any point in time. 
So to begin with this processor has the bunch of job, it will take the smallest job amongst that bunch and schedule it. May be the job finishes then it picks up the next smallest jobs in this collection it has and schedules that one but suppose you have a job which is running and then in mid way some other job comes in which has the processing time which is smaller than the remaining processing time of this job, the one that is currently running. If such is a case then we are going to interrupt the currently running job and schedule this new job that?s given. This is the setting in which interruption happens in which preemption happens. This is just a motivation we are not going to be looking at this srpt rule any more carefully than this but if you were to implement this kind of a policy, what kind of a data structure would you need that?s what we are going to look at. 





(Refer Slide Time: 04:02)
 
First we some how need to maintain the remaining processing time of the unfinished job, jobs that have not been completely finished. They are still with us, they still need to be scheduled for some period of time. We need to maintain that collection. Amongst these set of jobs, we need to find out the one which has the shortest remaining processing time because we need that so that according we can schedule the job according to the rule that we are following. When a finishes we would like to remove it from this collection and when a new job arrives we would like to add it to this collection. 
These are the kind of operations that we need to do on our data structures to be able to implement the srpt policies. One data type which will let us do all of this, what?s called the priority queues. It is data type to maintain a set of elements each of which has an associated priority.  The priority queue data type supports the following operations. The first operation is one of inserting an element. The x is an element, we are going to insert it to our collection s, so that new collection is now going to be s union x. Of course this element has certain priority. When we are inserting the element, we will also have to take care of the priority of that element. The minimum operation returns the element of s with this smallest priority. 
Priority perhaps is not the best word here because when you say each element has a priority, you would like to say that you would like to get the element of the largest priority.  It?s a bit of misnomer here but that?s what we will work with, so in the scheduling example it made sense to remove the one with the minimum processing time remaining. We will continue with the minimum, the priority really is the misnomer here and then we have the delete min operation. The delete main operation returns the element with the minimum priority and also removes it from the collection. 
The way minimum and delete min differs is that while minimum only returns the minimum element it doesn?t delete it from the collection, delete min also deletes it from collection. These are the three basic operations that the priority queue is supposed to support. Of course there could be many other operations, like for all other container classes that we have seen so far we have seen methods like is empty, is size and so on or size, is empty and so on. One thing that we require on the priorities is a total order, let me come to that. 
(Refer Slide Time: 08:11)
 
Recall the priority queue ranks its element by priority. Every element has a priority, priority need not necessarily be unique but they are totally ordered. That is given two priorities I can compare them, I can decide whether one is smaller than the other or larger than the other or they are both equal. There is the total order which will lets say denote by less than or equal to on the priorities. These priorities need not be integer, they could be anything at all but there is definitely some kind of a total order on these priorities. In particular this relation less than or equal to is reflexive that is priority k is less than or equal to k. it is antisymmetric which means that if k1 is less than k2 and k2 is less than k1 then k1 is also less than or equal to k2. The k1 is equal to k2, this is the error here and k1 k2 should be the same and its transitive. If k1 is less than k2 and k2 is less than k three then k1 less than k3. 
These are the properties of total order relation and we will assume that the priorities that we have satisfies the total order deletion. The most general and reusable form of priority queue uses comparator objects. What do I means by this? Recall that we said that in a priority queue, each element has priority associated with it and the priorities there is a total order on the priorities. If I want to put any kind of an object in to my priority queue, I should have a mechanism of comparing the priorities of the objects in the queue. 






(Refer Slide Time: 10:05)
 
We can have such a comparator object which will help us to do this comparison. So that comparator object all it does is it specifies the methods which will help us compare to priorities. In this manner we can ensure that our priority queue that we would have one implementation of priority queue and we can use same implementation to store any kind of object because we would also specify this comparator object which would help us compare the priorities of any two object that we decide to put in the collection. 
The comparator abstract data type would include methods like is less than, is less than or equal to, is equal to, is greater than, is greater than or equal to and whether it is comparable or not. Let?s first look at an implementation of priority queue using an unsorted sequence. Recall the items that we are trying to insert in to our sequence or pairs of priority and our element. We can implement the insert by using just say insert last. 








(Refer Slide Time: 11:03)
 
The insert operation of the priority queue, to do this we will just insert the element that is the item at the very end of our sequence. In this case six was the element that was to be inserted. Let?s say we went and put it at the very end. This takes only the constant amount of time but if I do such a thing, if we always insert at the end irrespective of the value of the priority then our sequence is not ordered any more. Since it is not ordered according to the priorities, we are going to have problems when we are trying to find a minimum or when we are trying to delete the minimum element, the element with the minimum priority from this collection. In that case we will have to look at all the elements of these sequences. For instance if I have to find the minimum then I would have to start right from here and traverse the sequence till I reach the element with the minimum priority which is let?s say one here. That?s to find the minimum. 
Similarly for delete min because I will have to get to here, I will have to remove the element and then I would have to find out the new minimum element. So worst case time complexity of minimum and delete min therefore becomes ordering. That?s not very good, here all the insertion takes constant time we are saying that both for minimum and for delete min we are going to take order n time. How about using the sorted sequence to try and implement a priority queue.






(Refer Slide Time: 13:20)
 
Now we are going to have a sequence in which the elements are sorted with increasing priorities that is the elements with the smallest priority is straight sitting right at the front of our sequence. In which case minimum and delete min I am just going to take a constant amount of time. The minimum element as I said is the element at the front of our sequence. So you just be able to retrieve the minimum element in constant time. 
Similarly to delete it, you just have to delete the minimum element and then delete the minimum element from the front of the sequence. This is what your sequence would look like now, note that the element with the minimum priority is sitting right at the front here but now the problem is if you have to do the insert operation. If you have to insert lets say an element with priority 7, I will have to traverse the sequence till I come to the right position and put the element there. In the worst case insert could now take order end time. In this example I am inserting 8 which can be inserted either before this 8 or after this 8. In this case I am taking time proportional to the length of the sequence. Priority queues find many applications.







(Refer Slide Time: 13:34)
 
I gave you an example of scheduling. They are also used in discreet event simulation and they are used as a building block for many other algorithms. we are going to be seeing more algorithm in this course lets says dijkstra?s algorithm which require the priority queue data structure to be able to efficiently implement the algorithm. Today what we are going to see is how to use a data structure called a heap, to implement a priority queue. 
We saw the two methods to implement the priority queue and unsorted sequence and sorted sequence and both of them had their problems. In case of unsorted sequence the delete min operation and minimum operation would take order n time and in the case of a sorted sequence, the insert operation would take order in time while the other operations are constant time operation. We would like to have a data structure which would do all these three operations insert, delete min and minimum very efficiently. 








(Refer Slide Time: 14:39)
 
What is a heap? When we say heap we typically mean what?s called a binary heap. Binary heap is a binary tree that stores the priorities or the priority element pairs at the nodes. In the rest of the class I am just going to show the nodes as containing the priority of the element and the element is sitting somewhere here also. They could be storing only priorities of the priority element pairs. Now heap has to two properties. One is the structure properties. In this binary tree all levels are full. This is level zero, it can have only one node, it has one node. Level 1 can have 2 nodes, it has 2 nodes. Level 2 can have 4 nodes, it has 4 nodes. Level 3 can have 8 nodes it has only 5 nodes. 
So all levels except the last levels are full and the last level is what we will call left fill which means that all the nodes on the last level are as left as possible. They could be 8 elements there. There only 5 but these 5 nodes would be the ones which are the first five if I were to move left to right. That?s the structural property of a binary. The other properties what I call a heap property. The heap property simply says the following. The priority of the node is at least as largest that of its parent. For any node its priority has to be larger than the priorities of its parent. 
This picture here that satisfy both of the properties, structure as well as heap. Consider any node, its priorities is at least as larger then the priorities of its parent. You can take any node. I can restate this, I can also say it in the following manner. The priority of any node is less than or equal to the priorities of its children. Sometimes we will also think of it in this manner, priority of any node is less than or equal to the priorities of its children. There might be two children, there might be only one child as is the case here. In this case priority of this node has to be less than the priority of its lone child. Let?s look at example which are not heap.



(Refer Slide Time: 17:01)
 
This is not a heap because if you look at this node, we require that it have a priority which is larger than the priority of its parent, at least as large as the priority of its parent but its priority is 18, its priority is 19. This violates the heap property. This does not violate the heap property but as you can see this node is empty, so this last level is not left filled. We would like that if there were only 4 nodes at the last level then they should appear as the first 4 nodes of this binary tree. 
(Refer Slide Time: 17:24)
 
This appears as the first four nodes of the last level of this binary tree (Refer Slide Time: 17:51). How does one find the minimum element in a heap? Suppose I give you a heap. What?s a heap? Recall as this two properties, one is the structural property the other is the heap property. Suppose I give you a heap, how does one found the minimum element? The claim is the minimum element, the element with the smallest priority always sits at the top of the heap. The top of the heap I mean the node here at the root and as you can see such as the case in the picture. The element with this minimum priority is the element by 11 and its sitting in top of the tree. 
Why is this the case? Why can?t the minimum not be some where else? If it is not at the root of the heap or at the top of the heap, it could be somewhere in the middle, somewhere inside the heap but then it has a parent and the heap property says that the priority of the parent is at least as large as the priority of the particular node then the parent would have a larger priority. The heap property says that the priority of the parent is no larger than the priority of the node. Now if such is the case then the parent would have even smaller priority and so the element that I was talking of was not the one with the smallest priority. 
Just recap what I said, if the element with the smallest priority was sitting else where in the heap then it would have a parent with smaller priority and this would violate the heap property. This larger should really leads smaller, it would have parent with the smaller priority and that would violate the heap property. The minimum can be done in constant time because we just need to go to the root element to find the element with the smallest priority. What is the height of the heap? Suppose I give you a heap on n nodes, what is its height going to be?
(Refer Slide Time: 20:08)
 
Recall our discussion on binary tree, if you have a complete binary tree of height h then it has exactly two to the h plus one minus one nodes. If we have a heap of n nodes with height h, note that since the height of the heap is h, it has more nodes than a complete binary tree of height h minus one. A complete binary tree of height h -1 has so many nodes in it. The number of nodes in the heap is larger than the number of nodes in the complete binary tree of height h -1. 
A complete binary tree of height h has 2 to the h plus one minus one nodes and the number of nodes in the heap is less than or equal to this quantity. This inequality holds, n is strictly larger than the two to the h minus one and less than or equal to two to the h plus one minus one. I can replace this as n equals the floor of the log of h. This notation here really means that log of h which need not be an integer, we are just rounding it down to the nearest integer. Hope this is clear to everyone. This is the mistake, this should read h equals log of n. The height of a heap is the log of the number of nodes. Just interchange this h and n, this is error on this slide. If n lies between two to the h and two to the h plus one minus one then h is log of n. How does one implement the heap? 
(Refer Slide Time: 22:04) 
 
Recall that till now we were saying that heap is this kind of a binary tree. We can of course implement the heap using the binary tree but it?s much simpler to implement the heap using an array. Let?s see what I mean here. This is level zero of my heap, this is level one, level two and level three of the binary tree corresponding to this heap. I am going to put these nodes of binary tree in to an array with the root node sitting at the first location in the array. My array is going to be indexed starting from location one. So 11 comes here 17 and 13, the next level would follow and we would go left to right. 17 would be the next element, 13 would be one after that then 18, 21, 19, 17, 43, 23, 26. 
This is level zero of the binary tree, this is level one of the binary tree, this is level two, this is level and this is level three of the binary tree. What?s the advantage of putting the nodes of a heap in to an array like this? Suppose I look at a node at location five lets say, node 21 and I want to find out the parent of this node in the heap. Then all I have to do is take 5 divided by 2 take the floors of 5 by 2 floor is 2. The parent of 21 in this heap is going to be 17 which is the case here. We can do it for any other node let?s take node 26. The 26 is at location 10, 10 by 2 is 5 whose floor is also 5. The parent of 26 that location 5 and is 21. You can quickly determine the parent of any node, if you know the location of the node. Parent is just at location i by two if the node is at location i. 
Conversely if the node is at location i and I want to figure out its two children then the two children, the left child would be at location two i then the right child would be location two i +1.  We can work out this math it will be very easy, I will just show it with an example here. 
If I look at node thirteen it?s at location three, its two children then should be location 6 and 7. Two children are 19 and 17 and they are at location 6 and 7. Given the location of any particular node, I can just quickly figure out what is the location of the parent, what are the locations of the two children of that node. Those are the only thing that are really need to do in heap and the heap property is really saying that if i is the location, parent of i is the location of its parent and a of parent of i, a is the array which is holding these priorities. So its priority of the parent of i. The priority of the parent is less than or equal to the priority of the node itself. That?s what the property is. We implicitly are maintaining the tree links. The parent child relationships are getting implicitly maintained. the children of a node i are at location two i and two i plus one and this is very useful in binary, in multiplying by two just corresponds to a left shift and multiplying by two and adding one is equally simple. 
(Refer Slide Time: 25:53)
 
One can quickly figure out the children of a node or also the parent of the node by just doing one left or right shift and an increment operation. This makes the operation really fast, when you trying to access the parent or the children of a particular node. Let?s now see how one would insert an element in to the heap. 







(Refer Slide Time: 26:54)
 
Till now the only operation we have seen is to find the minimum element in the heap and that we said can be done in constant time and because minimum element in a heap always sits at the root of the heap. Suppose I wanted to insert twelve in to this heap. What would I do? Right now this heap as 3, 7 and 12 nodes in it. If I insert 12 this heap is going to have 13 elements in it. If it has 13 elements in it then structurally I should have another node here. That?s the first thing I would do. I would first create a node here and let me now put 12 in to it.
(Refer Slide Time: 27:28)
 
Of course structurally this does satisfy the structural property of heap but it is not heap because it violates the heap property. This node should have lesser priority than its two children it does not, it has higher priority than this child. We need to take care of that. Since this is lesser priority than this we need to swap this thing.
(Refer Slide Time: 27:55)
 
(Refer Slide Time: 28:08)
 
We do that in this manner we swap this two element. Now the heap property is satisfied here, no problem. This has lower priority than its two children but it?s violated here. This does not have lower priority than its two children. We should then swap this and this, we do that. Heap property also now satisfied here. Since we change the content of this node, could it be that the heap property is violated at this node. Let?s look this has priority eleven, this has 17, this has 12. This has lesser priority than both its children, the heap property is satisfied here no problem at all. We are done with the insertion of 12. All we did was create a new node, put twelve here and then move twelve up, so as to maintain the heap property. We saw that 12 had a lower priority than its parent. So we swap 12 and its parent then once again we saw that 12 had reached here. 12 had a lower priority than its parent so once again we swapped 12 and its parent. 12 now reached here, but now 12 does not have lower priority than its parent, so we stop. Let?s see if we were to insert another element. Let me insert 8 in to this heap.
(Refer Slide Time: 29:24)
 
Once again 8 would come at this location, so I have put in 8 here. 8 has a lower priority than its parent this violates the heap property, so we would swap 8 and 17.
(Refer Slide Time: 29:34)
 
Once again 8 has the lower priority than its parent, so we would swap 8 and 12. Once again this has lower priority than its parent so we swap 8 and 11 and now we were done. This is now a heap with 8, the element that we inserted last. Now coming at the root of the heap and in fact it should come there because this is the minimum priority element in the entire thing. Let?s look at insertion again from the slightly different point of view. 
(Refer Slide Time: 31:48)
 
We again have the same heap as before, we are trying to insert 12. First we enlarge the heap which means that we create the new node where we create the new node because we know that the new heap now has 13 elements in it and so this is what it structure should be like. Now we consider the path from the root to inserted node which is this slight colored path. So this was the inserted node and we just march up, we just follow, go from this node to its parent, to its parent and to its parent. This is path from the root to the inserted node and on this path, we find the top most element with the priority higher than the priority of the inserted element. What are the nodes on this path? These are these three nodes, they have priorities 19, 13 and 11. 
The top most element on this path with higher priority, the element we are inserting is priority 12. This is the top most element but it has priority lesser than 12. This is the next highest element and this has priority more than 12. This is the top most element of the path with higher priority than that of the inserted element. This is the element, so what are we going to do now. We are going to insert 12 at the location but where would 13 go? 13 would get pushed down and where would 19 go? 19 would get pushed down like this. 



(Refer Slide Time: 32:03)
 
This is exactly the same procedure as before. If you look at the slide that we got earlier, the elements where at the same location except there we started 12 here and we bubbled it up. Now I say that I am going to directly figure out where 12 is going to come and I am going to move elements down. Let?s argue correctness. Let?s argue that the procedure of the insertion is really correct and for this we look at this other view of insertion that we saw in the previous slide. 
(Refer Slide Time: 33:37)
 
The first claim is that the only nodes whose contents changes on the ones on this path. This is the path that we considered which is the path from the newly inserted node to the root and we are changing only the contents of this. We are in fact only changing the contents of this part of the path. If the heap property is getting violated, the heap property can be violated only at the children of these nodes. It could that for this node, since we are modifying this content it might be that the new priority of the parent is larger than the priority of this guy or new priority of the parent of this node is larger than the priority of this node that would be violation of the heap property. So it is only for these two pink color nodes that the heap properties might be violated. 
(Refer Slide Time:  33:45)
 
What is happening? What are the new contents of the parent of 31 and 17? 31 is 19, what is the new contents of this going to be? If you recall 19 moved down, 13 was coming here and 12 was reaching there. The new content of this node is 13 which is only going to be less than this. The new content of any node are only going to be either the newly inserted element or the parent of that node. The new content of this is going to be the content of its parent and the content of the parent is already smaller and has the lower priority that is the priority of this guy. 
The priority of each node is only going to be reduce, so the priority of these guy is going to reduce because 13 is going to move here. The priority of this guy is going to reduce because newly inserted element 12 is going to come here and recall from our choice of path that this was picked as a node with higher priority than the priority of the element that was getting inserted. This is higher than this. The priority of this is also going to reduce as the consequence of insertion procedure. The priority of all of these nodes only going to reduce the consequence of insertion procedure and ones the heap property would not get violated.

 


(Refer Slide Time: 35:56)
 
These are going to be the new priorities of these nodes. We said that heap property could be violated for this guy or for this guy. This should also have been pink, it could be violated for this or this but earlier its parent had a priority 19. Now it has a priority 13. Earlier the parent of this guy had a priority of 13 now it has a priority of 12, the priority is only reducing. If there were only reducing then earlier if this had a priority which was smaller than the priority of this guy, it continues to have priority which is smaller than the priority of this guy.  
If earlier this had a priority which is smaller than that of this guy, it continues to have a priority which is smaller than the priority of this guy. Heap property is not violated at all and so what we get after insertion is still a heap. I am now going to look at another procedure called heapify which we are going to use as I mentioned at the beginning of the class, we are going to use for the other methods that we have to do on a  heap. 








(Refer Slide Time: 36:04)
 
I am now assuming that my heap is kept in an array A and i is an index in to this array so heapify take the parameter as an index in to the array A. Then the binary tree is rooted at the two children of i, left i and right i are the two children of i. The binary trees rooted at these two locations are already heaps but it might be the case that the heap property is violated at this node i that is A[i] might be larger than its children. A[i] might be larger than its children and this violates the heap property. 
Now heapify is the method which tries to maintain the heap property, makes the binary tree rooted at i a heap by suitable modification and will see what this is in a second. Let?s look at this structure. This is not a heap because this node right here at the very top has priority which is larger than the priorities of its two children.









(Refer Slide Time: 38:47)
 
But this binary tree, if I were to just restrict myself to this part which is a heap. Why it does satisfy the heap property? Every node has a priority which is smaller than the priority of its two children. This has smaller priority than its two children, this has smaller priority than its two children, this has smaller priority than its two children. Similarly this here is a heap, structurally as well as it does satisfy the heap property. Each node has priority smaller than its two children. These two are already heaps but this entire thing is not a heap because the heap property is getting violated at this node (Refer Slide Time: 38:23). 
This does not have priority smaller than the priority of its two children and some how we want to make this entire thing a heap. We can invoke the heapify procedure. Heap property is violated at node with index one which is this node but the sub tree is rooted at two, two is this and three is this. So sub tree is rooted at two which is this and this are heaps. We would invoke the heapify procedure with one because we want to make this entire thing a heap. For heapify remember we require that whichever node heapify is invoked on, the sub tree is rooted at the two children of that node are already heaps. That?s a very crucial part for heapify only then we would heapify it. If heapify is invoked at one, it can be invoked at one only because the sub trees rooted at two children of node one, this and this the sub tree is rooted here are already heaps. 
Let?s see how heapify works. Heapify is going to look at the two children of this node where the heap property is correctly violated. You have to look at the two children which are these 10 and 11 and it would take this smaller of these two children and swap it with 17.
The smaller is 10, it would swap 10 with 17. Now the heap property heap property is valid at this node. This has priority less than the priority of its two children but the heap property is violated at this node because this does not have priority smaller than the priority of these two children. Once again we are going to do the same thing. We are going to look at the two children, take the smaller one amongst them which is 16 and swap it with this. The heap property is now valid at this node but is the heap property valid at this node? (Refer Slide Time: 40:39) Because we also changed the content of this node. It is, because now when I look at the two children of this, they are 23 and 43 both of which have priority higher than 17 and so the heap property is violated here. Now this entire thing is heap, this is the heap because the heap property is now valid at every node in this tree. Recall, we have changed only the contents of these nodes. That?s the heapify procedure. 
Once again to recap, the heapify procedure can be invoked at the node i, only if the two children of this node or the sub tree rooted at the two children of this node are already heaps. Once again we will present the second view of heapify which will help us prove the correctness of heapify very easily. 
(Refer Slide Time:  43:55)
 
Heapify essentially is tracing a path down the tree. In our previous example, this path was this blue colored nodes. These were the node whose context we modified. If I look at the last node on the path suppose I call it node j then the left and the right child of j have priority which was larger than the priority of node i. 
i is this node (Refer Slide Time: 42:22), i is in our example is one so 43 and 23 are both larger than 17. All elements on this path have lower priority than their siblings. What are the siblings? Sibling of 16 is 21, sibling of 10 is 11, 17 does not really have a sibling. 10 is less than 11, 16 is less than 21 and why is this true and why is that the case that the priorities for all nodes on this path have priorities less than the priority of the sibling. That?s because of our choice of the path. We came left because we compared 10 and 11 and we picked 10, so that we could then swap it with here and then when we came here, we compared 16 and 21 and we picked 16. 
Just from the way we constructed our path this statement really follows. What are they really doing in heapify? In heapify what we are really doing is that we are moving these elements up, let me show you how. I am just showing it in one step now but what we saw earlier was a sequence of step. This was the net result, 10 moved up, 16 moved up and this 17 really came at location j. It came at this location. This was the net result of our heapify step. 

(Refer Slide Time: 44:00)
 
Now once again the same thing is happening. 16 was here, it was less than its sibling and it moved up here. Now 16 is less than 21, similarly 10. 10 was less than 11, 10 was less than its sibling, 10 moved up here. Now it has priority less than its child and 17 which was here because of this property that the last node on the path in which both of its children have probability which was larger than the A[i]. The 17 is going to have lower priority than both of these. 
While the contents of these three nodes have changed, the heap property is satisfied at all of these three nodes. This is going to have lesser priority than its two children because of this property that we mention here. This is going to have lesser priority than its two children because of this priority, this 16 which was here which was earlier a sibling of 21, had a lesser priority than 21. Now it has become a parent and similarly for 10. 10 will have a lesser priority than this guy because this was the smaller priority node amongst the two siblings, 10 and 11.








(Refer Slide Time: 45:36)
 
This essentially establishes the correctness of the heapify procedure. What you get as a result of heapify is still a heap. Heapify is a really a mechanism to rebuild a heap. If due to our operations, the heap property is violated at a particular node then we can heapify at that particular node. Assuming of course that the two sub trees, the two child sub trees which are rooted at that node have the heap property then we can do heapify on that node and get a larger heap. Let?s do a quick runtime analysis of the various procedures that we have seen today. 
(Refer Slide Time: 47:11)
 
Recall that a heap of n nodes has height order log n. when we are inserting an element, the insertion procedure was that we would create a new node and then we would move the element up the heap. In the worst case, the element might move all the way up to the top. if its moves all the way up to the top then we require time proportional to the height of the heap but an n node heap has the height at most log n. We require at most order log n steps to do an insertion. The other procedure we looked at today was the heapify procedure. 
In the heapify procedure on the other hand, the element moved down the heap. We took the element so if we are invoking heapify at node i, we looked at node i we looked at two children of node i, we took the smaller of the two children and swapped it with node i and continued the process till the element came down and the heap property was eventually satisfied. In the worst case the element might moved down as many steps as the height of the heap. The height of the heap is only order log n. In the worst case, the time taken for the heapify is also order log n. 
The two operation that we looked today insert and heapify, both require only order log n time. We also looked at the operation of minimum which was just to find the minimum element in the heap without removing it. Just to find the minimum element, we could do it in constant time because all we have do was to go to the root element of the heap and return the value of that, return the element stored at the root. Minimum takes constant time and insertion takes order log n time. Heapify also takes order log n time and in the next class we are going to use heapify to remove the minimum element from the heap and we will be able to do that in order log n time too. 
With that we end to today?s discussion. In the next class we are going to see how to use heapify to remove the minimum element, the delete min operation. We also going to see how to create a heap quickly in order n time and we are going to see how heaps can be used to do sorting efficiently.Thank you.


Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 21
          Binary Heaps 

We will continue our discussion of binary heaps in this class. Recall that in the last class we saw what a binary heap was, we also looked at the operations of insertion and heapify on a binary heap. To recall a binary heap has two properties two critical properties, one is the structural property where we require that the structure of the heap be similar to that of a complete binary tree. So all the levels except the last level of full and even the last level is what we called left full that is all the nodes in the last level are as left as possible. The other property was the heap property which was that for any node, the priority of that node should be less than or equal to the priority of its two children.

(Refer Slide Time: 02:05)

 

So today we are going to look at the operations of deleting the minimum element from a binary heap and building a heap. It is very easy to build a heap using repeated insertions but today we are going to do a build operation, it just takes linear time. Then we are also going to see how to use binary heaps to do sorting and that procedure is called heap sort. So recall that the minimum element is the one at the top of the heap.





 

(Refer Slide Time: 02:35)

 

So one way of doing a delete min is to just remove this element and then we have an empty space at the root. So we have to fill up this empty space and to fill up the empty space it would be natural to promote one of the two children of this root element to fill this empty space. So if you were to do that then this empty space would move down into the tree, move down the tree and it might end up at any particular location in the last level. So that the resulting tree might not be left filled. So just to illustrate what I am saying here let us look at this picture.

(Refer Slide Time: 03:14)

 

We are trying to delete a minimum element from the heap which is eight. So suppose I were to do that. So this creates an empty location here, now I am going to take the smaller of the two children element and push it up here, move it up here. So 10 is the smaller one I move 10 up there, so I have an empty location now here.

(Refer Slide Time: 3:36)

 

So it is natural to take the smaller of these two and move it up there and this moves the empty location here and now it is natural to take the smaller of these two, 23 and 43 and move it up there at that empty location. So that now we have an empty location here. 




















(Refer Slide Time: 03:59)

 

Now while this does satisfy the heap property. This is not a heap because it doesn?t satisfy the structural property of a heap. The elements at the level are not left filled. There is an empty slot here. So we can really do delete min in this manner but this is close to what we will be doing in our delete min and let?s see what is the right procedure for doing a delete min. So we need to get rid of 8.
 
(Refer Slide Time: 04:35)

 

Now when we get rid of 8, the number of elements in this heap is going to reduce. So structurally this heap should not be having this node any more here. 
So it makes sense to move this last element here, so we knock off eight and then we move this last element at this place and remove this node. Now structurally this is a heap but it doesn?t satisfy the heap property now. So to make sure if it satisfy the heap property we have to adjust the contents of the various nodes but now note the following interesting thing. This sub tree is a heap and so is this sub tree is a heap. The heap property is violated only at this node, this does not have a priority less than the priority of two children. So but we know for procedure for taking care of this problem. If this is a heap and this is a heap, all we have to do is to run a heapify procedure on this particular node. So we just have to do heapify one and recall what does heapify one do. Heapify one we saw also saw this in the last class. Heapify one would take the minimum of these two which is 10, swap 10 and 17. The heap property is valid at this node but it?s now violated at this node.

So once again we are going to take the smaller of its two children 16 and swap it with 17. 
Now the heap property is valid at this node but while it?s also valid at this node because if I look at the two children, they are both larger than 17. So this entire thing is now a heap and we have deleted the minimum element. So this is the delete min procedure. 

To recap in the delete min procedure we are going to remove the minimum element, take the last element of the heap which means go to the last level and take the right most element. In the array implementation this just corresponds to the last element in the array. 
Take that element and put it at the root and then just do a heapify of the root, heapify one. So this would make this entire thing a heap once again.

(Refer Slide Time: 07:00)

 

So this was the delete min procedure. Fairly simple all it required was the heapify procedure. We are now going to see another application of heapify procedure and that is to create a heap. 
One way of building a heap is to just repeatedly insert the elements in the heap. So we could insert the first element, second element and the third element and so on and on and how much time does this procedure take? So recall that to insert an element we take log of the number of elements that are already in the heap. So to insert the first element we will take order log one time, for the second element we would take order log two time, for the third we will take order log 3 time and so on and on all the way up to n, if you were to insert n elements the last element would take order log n time to insert. So if you some up this series it?s exactly log of n factorial which is the same as n log n. This simple minded method of creating a heap by repeated insertion takes order n log n time. 

So we are going to look at this other method of creating a heap which is going to take only linear time and the key to this is to create the heap bottom up. So note that this point, this is not the heap. These are the elements that were given to me, I just put them at arbitrary locations in this heap. So in the array so it just means that since we implementing heaps using array, so we just put all the elements that we have to make into a heap, we just put them into the array. Now we are going to create the heap bottom up, so note that these are already heaps. 

Look at this sub tree rooted here which is just this node itself, this is a heap because it doesn?t have a child, any children. So it does satisfy the heap property. So these individual leaves are heaps so no problem here. Now what we are going to do when we say we are going to create the heaps bottom up is we are going to make a heap out of this. 
So we would want that the sub tree rooted at this node also becomes a heap. What is the sub tree rooted at this node? It?s this sub tree. How will we make this into a heap? This is already a heap, we have to make this entire thing into a heap. So we will just run a heapify procedure on this and that is what happening here. This element would be at location n by 2, floor of n by 2. We are going to run a heapify procedure on this file. 

What would heapify do? Heapify would just compare this with its two children whichever is the smaller child, so it has only one child, so it will just take this smaller child and swap it here. Now this entire thing is a heap. Now we are going to look at this element, so we are going to make a heap out of all of these four sub trees now. So we are basically go to this element, we are going to look at the sub tree rooted at this element and make it a heap. So what is a sub tree rooted at this element? It has these 3 nodes in it 13, 11 and 19. How do I make a heap? I run heapify on this. To run heapify the first thing that heapify does is it takes this, looks at the two children, takes the smaller of T children and swaps it with this. So I am going to make a heap out of this. The smaller of the two children is 11 so I am going to swap 11 and 13. So this also now becomes a heap.

Similarly I now need to make this, the sub tree rooted here a heap. The smaller of the two children is 8, I swap 8 and 21. Now I need to make the sub tree rooted at this node a heap, these are the two children I want to make this a heap but note this is already a heap. This is a heap, this is a heap and this entire thing is a heap because the heap property is also satisfied here (Refer Slide Time: 11:15). So now I have these 4 heaps, these are all heaps. Now I am in a position to run a heapify operation here. 
Why run a heapify operation here? This is a heap, this is a heap so I can now run a heapify operation on this one and make this entire thing a heap. To see the advantage of heapify, this is already a heap, this is already a heap so I can make a heap out of this. So to make a heap out of this when I run heapify what does it do? So I am going to make the sub tree rooted at this node a heap now. To do that I have to take the smaller of these two and swap it there so that is 11 and 26 swapped and now recall we are doing heapify. So heapify would bubble up the element all the way to the bottom if need be. So as a consequence of this swap, the heap property is violated at this one. This node now has a larger priority than its two children. 

So once again we are going to take smaller of these two and swap it with this and now this entire thing is a heap sort. This was just a heapify operation on this node. The heapify operation on this node just doesn?t stop with one swap. It will swap and if need be bubble the element down and that?s what happened here. Now I need to run a heapify operation on this node to make this entire thing a heap. This is already a heap, this is already a heap, I need to make this entire thing a heap. So once again it will take the smaller of these two which is 8 in this case and swap it with 43 and now this is not a heap because the heap property is violated here. So I need to run a heapify essentially on this one or actually we are just part of the larger heapify, so we are doing the heapify. We took the smaller of the two children swapped, now we come to this node.

If the heap property is validated here which it is, I will take the smaller of the two children, swap with it 43, so 21 and 43 get swapped. So now this is a heap, this entire thing is a heap and all that remains is to make this entire thing a heap. The heap property is violated here so I run a heapify operation on this. The heapify operation will take the smaller of these two nodes and swap it with 23. So it?s going to happen now, 8 and 23 get exchanged.
 
Now the heapify continues it just doesn?t stop with this because now this is not a heap we have changed the content of this node. So we are going to take the smaller of these two and swap it with 23 and then now this is not a heap. So we go and take the smaller of these two and swap it with 23 so we get that. Since this does not have any other children we are done. So this entire thing now becomes a heap. So that was the build heap procedure. The build heap procedure all it is saying is essentially just go down, so recall this was 1, 2, 3, 4, 5, 6, this is how the elements are laid out in the array. If this is element n then recall that its parent is going to be at location n by 2. So this element on which we first have to run the heapify procedure is at location n by 2. 

So we first run the heapify on this then we run heapify on this, then we run the heapify here, then we run the heapify here and after we have done all of these we know that these are already heaps. So we can now go and run the heapify here and then the heapify here and then the heapify here. The right order of running heapify is really to first run the heapify on all of these nodes then on all of these nodes and then on all of these nodes and that is exactly what is being done here through this single for loop. 

We first run the heapify on all of these then on all of these then all of these. Of course this here is saying that you first run the heapify here then here, then here, then here (Refer Slide Time: 15:10) this is not necessarily required. You could also run the heapify in this order but remember that the heapify has to be run first on this level only then can you proceed to run the heapify here. Why is that? That is required because to run the heapify on this node these two should already be heaps and this can be a heap only after you have run a heapify here. So this is the entire build heap procedure. We are using the heapify sub routine crucially which takes as parameter the location at which you want to run heapify.
 
(Refer Slide Time: 15:53)

 

So let?s analyze the build heap procedure so to prove correctness and I am not going to discuss it in detail here. You can do an induction on i and the induction claim would be that all trees rooted at locations m which are more than i are already heaps. So given this induction statement, with this induction hypothesis you can do the induction step and prove the correctness of the build heap procedure. A simple minded approach to do the running time computation would just be as follows. There are n calls that we make to the heapify procedure or n by 2 calls that we make to the heapify procedure. Each one of them we saw in the last class takes in the worst case order log n time. So the total time taken by the build heap procedure is n log n but we said the build heap takes order n time and we can actually prove a better bound of order n. The intuition for this is most often we are doing the heapify procedure on heaps which are very small and let us see what this really means. So let?s define the height of a node as the length of the longest path from the node to a particular leaf.





(Refer Slide Time: 17:22)

 

So the height of this node is one, the height of this node is 2 and the height of this node is 3. We will call the leaves at height zero, we will say that the height of the leaves is zero. 
So 1, 2 and 3 and the height of the tree is just the maximum height of any node which is therefore 3. So the height of the tree is the same as the height of the root and its 3 in that example. 

Now the time for heapify, if I do a heapify on i on location i, the time for heapify is just the height of the sub tree rooted at that node i because in heapify as you recall, we might have to move the element all the way down but if the height of the sub tree is some quantity h then we can only move it h levels down and so the time is proportional to just the height of the sub tree. This we will have to remember that the time for heapify on i is just the height of the sub tree rooted at i. We are also going to assume that the number of nodes in the heap is of the form 2 to the k - 1 so that it is a complete binary tree. This will only help us simply the analysis, it is not really required to we can also do without this.














(Refer Slide Time: 18:48)

 

If the number of nodes was of the kind 2 to the k-1 then we know that there are roughly n by 2 nodes of height one and for each of these nodes, we require only one swap. These are height one nodes, the sub tree rooted at these nodes is height one. So we require only n by two swaps. For the n by 4 nodes at height 2, the number of nodes is half the number of the nodes at height 1, height 3 the number of nodes is half the number of nodes at height 2 and so on and on. So at height i there are n by 2 to the i nodes and for each of these nodes you require at most i swaps. So the total number of swaps that are required by swaps I mean the time, so you can count the time required by heapify in terms of the number of times you have to swap the location of two elements. 

The time required by the heapify procedure is just proportional to this to the number of swaps. So we are just counting the number of swaps. What is the total number of swaps required then? It is n by 2 times 1+ n by 4 times 2+ n by 2 to the i times i. So this is what the sum would look like, it is basically n +1 or n times summation i over 2 to the i, as i goes from 1 through log n. Why log n, because that?s the height of the heap. 

It is this sum that we are really interested in and now this summation here, summation i going from 1 through log n. i over 2 to the i is just 2 and I will show you why in a second. So if this is just a constant then this entire thing just becomes order n. So this is what we said earlier, heapify all though we are making n calls to the heapify procedure, most of the calls are being done on heaps which are very small. n by 2 calls have been done on heaps of size of height 1, n by 4 calls have been done on heaps of height 2 and since the time taken for heapify is proportional to the height of the tree on which the heapify procedure is being called much less time is spent here then just saying n by 4 log n which is a crude upper bound. 



(Refer Slide Time: 21:37)

 

I have to argue that the summation i over to 2 to the i is 2 and this is a simple argument for that. So recall that summation x to the i, i going from zero through infinity is 1 over 1 - x, if x is less than one. Now if I just differentiate this I get i times x to the i -1, i going from one through infinity. I have dropped the i equal zero term because that would now contribute to zero. The differential of the right hand side is 1 over 1- x square.
 
Now I multiply both sides by x to get i times x to the i equals x over 1- x square and now i plug in x equals half. So I get i over 2 to the i, i going from one through infinity equals 1 over 2 divide 1 over 4 which is equal to 2 and if you recall what we required was that this sum go from one through log n. So that then is only going to be less than or equal to 2 or strictly less than 2. So the sum from 1 through infinity is 2. So that completes analysis to show that one can build a heap in just order n time. The key thing here was that we build heap procedure went bottom up, it first created smaller heaps and then combine them into larger heaps by just using the heapify procedure repeatedly.


 











(Refer Slide Time: 23:09)

 

Today we have seen how to delete the minimum element from a heap. We have also seen how to build a heap in linear time, in order n time. Now we are going to see how to sort using heaps. So give you a bunch of elements and I want to put those elements in increasing order. So what would be one way of doing it using a heap? I could just do the following, I could take those elements, build a heap using them. Then I could repeatedly remove the smaller element from the heap. So that is exactly what I have said here. I first create a heap, this can be done in order n time as we just saw then I repeatedly remove the minimum element from the heap till the heap becomes empty. So how many elements would I have to remove in all? There are n elements in the heap, I am going to be removing n elements in all and each of the delete min procedure, each of the delete min steps requires order log n time. So I do n steps here, so the total required would be order n log n and this requires order n times, so the total time required is order n log n.
 
Now suppose you have to do an in place sort by in place sort I mean that you are given an array of n elements and you are given no other space. This is all the space you have and you just want to swap the elements in this array so that finally you have a sorted sequence. Recall that our heaps are implemented using arrays. So we first create a heap, for that we did not require any additional space. Starting with the initial array, just create a heap in that array. 

Now the contents of the array are basically 8, 10, 11, 12 so on and on, in this order sitting in the array. Now when I delete the minimum element, where does it go? Where would I put this element, because I don?t have any additional space. So now what we are going to do is when we delete this element we are going to put it at the end of the heap. So recall that for deleting the element, I am going to move this element here to the top, so which means that this location in the array is now available to me. 

It is free so I am going to use this location to move 8 to put 8 in. Essentially I am swapping 8 and 31 and now this location is really not part of the heap. The heap is this now and actually this is not a heap because it doesn?t satisfy the heap property. So to do a heapify, to make this into a heap so which means that the same as before. I take the smaller of the two children?s, swap the element with that now again heap property validated here, smaller of these two and swap and now this is again a heap. So all I have done is, so this was essentially the delete min procedure. I have done a delete min, I have removed 8 but I have kept the 8 in my array at the very end. 

So this is in place sorting. We are not using any additional space, we delete the element but then we keep it in the array in an empty location and now once again we do a delete min. so we delete 10 and 19 is going to come here. So this location is going to become empty so ten and nineteen essentially are getting swapped. This location now goes away from the heap, so this is not marked which essentially means that this is the part of my heap.
 
Of course I once again need to ensure that the heap property is satisfied, so I am going to do the swaps, 11 and 19 get swapped, 13 and 19 get swapped and 10 and 19 are not going to get swapped because this is not part of the heap any more, the heap is just this. So the heap property is satisfied here because this is only one child and it has priority less than the priority of its child, its lone child. So this is now a heap.

So once again I do the delete min, so 11 is going to be deleted, 26 is going to come here, 11 is going to come here because this location is now going to be empty and so this is what is going to happen. This location is going to go out of the heap now because the heap is only this part and once again we are going to do the swaps. We are going to do a heapify at the root so as to convert this into a heap and that is what we are doing now and now this is a heap. 

As you can see the last element is the minimum element. The second last element is the second minimum and so on and on. Eventually this is what is going to happen, we are going to have a sorted sequence but in decreasing order and that can easily be reversed in linear time to get a sorted sequence in increasing order. So I will just continue the procedure, we swap the last element with the root and then this element goes away from the heap. Now we are just going to do the swapping so 13 and 29 get swapped, here the smaller of these two, 17 and 19; 17 is going to get swapped with 29 and now this is again a heap furnace. 

Now the minimum element is sitting here, 13 is going to swapped with the last element, this element is going to go out of the heap now. It fades away out of the heap and now we are going to ensure the heap property by doing the necessary swaps. So smaller is 16, it got swapped, smaller is 21 it got swapped and now this is a heap because these two are not part of the heap. So heap is only this thing, this is the smart. So once again you swap the last element in the heap with that root element. So 16 comes here and 26 comes there, this goes away. It is not part of the heap any more and then we are going to ensure the heap property by doing the necessary swaps. 
So 17 and 26 get swapped and now the heap property is violated here because this has higher priority than one of its children so 19 and 26 are going to get swapped and now this is a heap. This again is now going to swap with the last element, 17 and 31 get swapped, 17 is going to drop off from the heap, 17 drops off and now we are going to ensure that this entire thing is a heap by taking the smaller of its children, swapping heap property validated, take the smaller of these two and swap.

The heap property is now valid. This is a heap so this is a minimum element, swap it with the last element, 19 and 29 get swapped. This element goes away from the array, gets dropped and now we have to ensure that this is a heap. The heap property is violated here, take the smaller of these two, 21 and 29 are swapped. The heap property is still violated; take the smaller of these two, 23 swap it with 29. This entire thing is now a heap. So we are going to remove the minimum element which is 21 we remove it, 31 goes in this place and since this location is now empty, 21 comes at this location.
 
Now we need to ensure that this is a heap, 21 dropped away now take the smaller of these two children twenty three, thirty one. Swap 23 and 21 and then take the smaller of the two children and swap it with 31. So now this is a heap, so the minimum element is 23 we are now going to remove the minimum element which essentially means swap it with the last. This is not part of the heap anymore and once again ensure the heap property by swapping and this is now a heap. The smallest element is 26, exchange 26 and 31 remove it from the heap and now ensure the heap by doing the necessary swaps. So the smaller of these two children?s is 29, so 29 and 31 are going to be swapped. This is a heap now, smallest element is 29. 

So I am going to remove this and 43 is going to come at this location, so 43 comes here 29comes here, this is not part of the heap anymore so I drop this off and now I need to ensure that this is a heap. So this now has only one child, this is the remaining part of the heap, this has only one child and which is smaller, so I need to swap them. So this is a heap now. So the minimum element is 31, I remove this minimum element and the last element is 43, it will come at this place. So 43 comes here, 31 is removed and it is put at the location of the last place because this location now gets empty. So essentially that corresponds to 31 and 43 getting swapped again and this element going away from the heap. Now this is the only element left in the heap and so it is a heap. I do a delete min which means I remove this element and it?s not part of the heap anymore. So this is what we get, as you can see this is a sorted sequence in decreasing order now if you read it like this and this is how we do heap sort.
 








(Refer Slide Time: 33:40)

 

So let?s quickly summarize the running times of the various heap operations that we have seen so far. The last thing we saw was the heap sort which takes a total time of order n log n. Why did it take a time of n log n? This was because it was a two step process, first we created a heap. This took only order n time and then we did delete min repeatedly till the heap becomes empty. So the first time we did the delete min operation we spent log n time, the second time we did a delete min operation we spent order log n - 1 time let?s say because the size of the heap reduces and so once again we have a series of this kind log n plus log n -1 plus log n-2 plus log n -3 going all the way down to a one but the sum of this series is log n factorial which is the same as n log n. The total time taken by this system is order n log n. 

So while this is only order n, the total time taken by this step is order n log n and that implies that heap sort takes a total time of order n log n. Building a heap, we saw a bottom up procedure for building a heap. So there are two ways of building a heap, one is repeated insertion. Repeated insertion we insert one element at a time and we argued that takes total time of n log n and you can actually come up with the examples were it takes that kind of time. So repeated insertions would take n log n time but if we did this bottom up process of building the heap where in the leaf elements are already heap, the sub trees of height one you made them a heap then you made the sub trees of height two a heap then you made the sub trees of height 3 a heap and so on and you repeatedly use the heapify procedure to be able to do that. 

So if you were to do it this way this bottom up construction of a heap this takes order n time. The delete min operation which we also sorted is takes only order log n time this was because the delete min operation is a two-step thing. First we remove the minimum element which is sitting at the root, take the last element in the heap. 

So in the array implementation this corresponds to the very last element in the array and put it at the root location, put it at the location of the first element which we have removed which was the minimum element. So once you did that, now this is not a heap because the heap property could be violated at the root but the two sub trees the two children of the root, the left child and the right child and the two sub trees rooted at this two children are heaps. So we can invoke the heapify procedure on the root. 

Heapify we have already discussed and we are going to recap today, a recap just now takes only order log n time. So the total time taken for delete min then is log n for the heapify and constant time to do this swap of moving the last element to the very first location. So total time taken is order log n so this was delete min where you are removing the minimum element. If you just wanted to find what the element was, the element with the least priority that we said is the element sitting at the root node. Now that we can just directly access and so finding the minimum element just take constant time. 

In the last class we saw the heapify operation we also saw it repeatedly in the delete min and the build heap procedure today and also in heap sort actually. Heapify is really crucial operations and we saw that it takes only log n time this is because heapify we are bubbling the element down the tree and the worst case we might have to bubble it all the way down but since the height of the tree is no more than log n, it will take no more than log n steps to do that.

In the insertion process on the other hand we are moving the element up the tree, so first we decide what the new structure of the tree is, so we add an additional node, we put the element there and then we keep moving it up till the property at all the node is not satisfied. So we keep moving it up so it bubbles up the tree. Since once again the height of the tree is only log n. in the worst case we might be bubbling up the tree at most log n levels and so the total time taken by the insert procedure is at most order log n. So as you can see, if i use a heap to implement the priority queue data structure then the worst case time complexity of any of the operations, so forget heap sort because the typical operations that we are doing are insert, find min and delete min. These were the three operations we started of with and while find min is done very quickly, it?s just constant time. Insert and delete min also don?t take too much time they take only order log n time.

Compare this with the implementation we had done using a sorted sequence in an unsorted sequence. In the case of an unsorted sequence we said insert would take constant time but find min and delete min will both take order n time. In the case of a sorted sequence we said that insert would take order n time while both find min and delete min could be done in constant time. So it?s not the case, in some settings you might be interested in implementing a heap, implementing a priority queue using a sorted sequence and which would those settings be. If you were implementing it using a sorted sequence then as I said both find min and delete min just take constant amount of time but insert takes a lot of time. It takes order n time. If you had a setting where you were doing a lot of find min operations, very few insert operations then it might make sense to use a sorted sequence.

So depending upon the application one has, depending upon the settings one is in, one might have to choose between I mean the different ways of implementing a priority queue. So we have looked at three ways heap, sorted sequences and unsorted sequences and depending upon which operation of occurring more often, one might have to choose an appropriate implementation. With this I am going to end today?s class. Today we looked at the other operations on heap. In particular we looked at the delete min operation and the operation for building a heap in a linear time. We also saw how to use a heap to do sorting in order n log n time only. This sorting that we saw was an in place sorting algorithm.
Thank you. 
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture # 22
           Why Sorting?

Today we are going to be looking at sorting. Sorting is actually one operation that computers love to spend their time on. You know it is estimated that 90 % of the time spent by all computers is on sorting numbers.

(Refer Slide Time: 01.26)

		 

So it is really a problem which requires attention, requires consideration and so we need to develop good algorithms for it. So in the last class before you have looked at heap sort. so today we are going to look at another sorting algorithm. So where does sorting occur? Most oftenly in databases. When you are searching in the databases you want to keep data sorted in a certain manner so that the search becomes efficient. Recall the search is more efficient when you have the data sorted. There are lot of settings in computation geometry, computer graphics where sorting is essential. So I am not going to more of these applications. But we are going to look at certain sorting algorithms. 









(Refer Slide Time: 02.14)

		 

Sorting algorithms also give us an idea of different algorithm design techniques. So actually this is a point we have not spent any time on yet. What is an algorithm? It is a way of solving a problem and there are certain techniques that people adopt to design an algorithm. Given a particular problem, how do you design an algorithm for it? So there are certain standard of techniques in sorting. It shows you some of those techniques. So today we are going to look at one such algorithm design technique through a sorting algorithm. 

There is also what a lower bound on sorting is. It means there is no sorting algorithm which requires comparisons. No sorting algorithm can sort n numbers in less than n log n comparisons. There cannot exist any sorting algorithm. So we are also not going to be spending time on this. But this fact is useful to prove lower bounds for other problems as well. This was as far as motivation for why are we spending so much time on sorting is concerned.














(Refer Slide Time: 3.32)

		 

So far we have seen insertion sort and selection sort. These are two sorting algorithm with the worst case running time of n2. There are two algorithms. I should also have written quick sort. Actually heap sort has a worst case running time of n log n and one algorithm that is missing from the list is quick sort which has the worst case running time of n2 but an average case running time of n log n. So today we are going to see another algorithm and the algorithm design paradigm that is used here, what is called divide and conquer. 

So some of you might have seen divide and conquer in your earlier CS Course. Basically what the idea is that, given a certain problem, to solve the problem you first divide the problem up into two or more parts. Then you recursively solve those sub problems. So that?s what is called the conquer step. So recursively solve the problems on those parts. When I say recursively solve, how do you solve the problems on those parts? By once again dividing them into smaller parts and solving the problems on those smaller parts and so on and on. So use the same algorithm to solve the problem on the smaller pieces and once you have the solutions for the smaller pieces, you need to some how combine the result to get the solution for the original problem. So that is what is called combine step. We will see through an algorithm for the sorting problem. 










(Refer Slide Time: 05.02)

		 
 
We are going to look at what is called the merge sort algorithm. Some of you again must have seen this before. If you have seen it, it is ok because we will see at least a different way of analyzing the algorithm. So what is sorting? Basically you are given n numbers which you need to put in increasing order, lets say. lets say S is the set of n numbers. What we do is we would split ?n? into two sets; S 1 and S 2. So this is the divide step. The split would be such that S 1 and S 2 would contain roughly half of the elements of S.  Now recall this should look very similar to another algorithm; the quick sort algorithm. there also we did a division at the very first step. But how was the division done there? It was not equal. So in the divide step there, we were dividing the problem into two sub problems but those two sub problems were not equal in size. 

One could be smaller than the other. But here we would make sure that we divide it into two equal sub problems. Equal I mean, the numbers of elements in the two sub problems are equal. So if S 1contains the first one, lets say n/2 elements, then S 2 contains n/2 floor elements. What do you mean by first n/2 elements? Not the first n/2 in a sorted order because I don?t know what the sorted order is. S is just some arbitrary collection. I just take the first n /2, call that S1 the next n/2, call that S 2. Now conquer step is that I would sort these two sequences. I would sort S1. I would sort S 2. So recursively I am solving the same problem and then once S 1 and S2 are already sorted, I have to combine. I wanted a sorted sequence for S. I have S 1 as one sorted sequence, S 2 as another sorted sequence .but what is S ? it is not just S 1 followed by S 2. No guarantee that is sorted. So I need to what is called ?merge? S 1 and S 2 into one sorted sequence. That is called the merge step or the combine step here. Now who can tell me, in the case of quick sort, the divide step was, I take an element and I compare. That?s the pivot element and I compare every element against this element. 



We had this procedure called partition which was helping us do that. that was a more involved procedure because we were trying to do it in place. So we were kind of just swapping elements till we got two sequences. One containing all elements less than the pivot and another containing all elements greater than or equal to the pivot. That was the division step. How much time did we spend in division there? Order n because we have to compare every element against the pivot. How much time do we spent in the division step here? Constant time because we are just saying if it is an array we just say this is S 1 & this is S2.  We are not copying anything from anywhere to anywhere. That is one difference. We will come back to the conquer thing. In the case of quick sort, we were doing the same thing. We partitioned. Then we said we will quick sort one part, quick sort the other part and then we will combine. Did we have to do combine there? No because we could just take the first sorted sequence, put the pivot element and then take the second sorted sequence and that would be the sorted sequence for the entire thing. So how much time was combining there taking? Constant time. Here combining will not take constant time. Here we have two sequences and we have to combine them. We will see that it will take order n time. This is also called the merge step and the name merge sort is deriving from this step. So every one understands what the algorithm is. 

(Refer Slide 09.24)

		 

So for instance, this is the pseudo code for the algorithm. I give you an array A, p and r. identify the part of the array, the lower and the upper bound which needs to be merge sorted. if p is less than r, only then we need to do something. p is the lower and r is the upper index. So q is the mid point p plus r by two. So this is the division step. These two are the recursively solving the conquer steps. I am saying from p to q sort the first part of the array, from q plus one to r sort the second part of the array .sort using same merger sort procedure and once you have this part and this part sorted you have to combine. For that I have another procedure called merge to which is specifying p q and r. 

So this procedure the definition is this following.  This is what it is doing but what is the definition of merge? It is going to take the two sequences. One sequence is from p to q the other is from q plus one to r and merge them and make one sorted sequence which will sit in p through r. That is merge and how does merge proceed? That?s essentially what the merge algorithm is doing. We are going to see examples of it. So don?t worry but next is read through this and you will understand a little bit of what merge is going to do. it is going to take the smallest of the two top most elements of these two sequences. What do I mean by the smallest of two top most elements? What is the top most element of this sequence? The first element. The top most of the first element which is p or the element at the location p :A (P) and the top most element of this sequence is A of q +1. I am going to take the smaller of these two and put it into the resulting sequence whichever is the smaller when I have to merge. 

So if I have to merge these two sequences which will be the smallest element of these two sequences. It will be the smallest of this and these two the smallest of these two numbers. I don?t have to worry about any other numbers because this is already sorted and this is already sorted in increasing order. Take the smaller of these two and that gives me the first element of my resulting sequence. In this manner I can continue building up the resulting sequence and we will see how that?s done.

(Refer Slide Time: 12.07)

		 

That?s merge. Now let?s look at an example for merge sort. This is my sequence. As you can see, it?s not sorted. It?s an 8 element sequence. So the first step is a divide step. You will divide this into two parts. So one is this and the other part is this. This is the division. 
i have shown it lower down because I am kind of doing a recursive call of merge sort. so this corresponds to one level of recursive call. So first do a merge sort here. This is another merge sort call. Then I will do another merge sort call here and so on. This is a recursive call on this part of the sequence on these 4 elements (Refer Slide Time: 12:55) and how is this done?
This is done by once again dividing this into two parts: 58, 24 is one part and the other is the other part and I do a recursive call on this part now first. First I do the division and then I do a recursive call on the first half. Then I do a recursive call on the second half. [HINDI] we just seem to be doing divisions and doing recursive calls as you can see. How do I sort this part? 

Once again I divide it into two parts - 85 and 24 and I do a recursive call on 85. Just a single element sequence and what is the solution for this when there is only single element? That itself is a sorted sequence. So 85 goes back up. So to say we go back to the parent calling procedure. So that is 85. So this is the sorted first half and this goes down. This is also sorted. So this is the sorted list and now we have to do the merge at this call. so we have to merge this sorted sequence which is just 85. Sitting in this blue oval are two sort sequences. We have to merge them. So right now we will just do the merge. So the result as you can imagine is 24, 85 and this is one sorted sequence. So now we have finished the merge sort for this call that we had done ? 85, 24. So now this can go back up. We have finished that merge sort. Now we are getting ready to do the merge sort on this second half. Let me show you the procedure once again. First we divide. Then we merge sort the left half and then we merge sort the right half and when I merge sort the left half, what happens?

 I once again divide and then I merge sort the left half and then I merge sort the right half. Then I merge sort the left half. What do I do? I once again divide and so on and on. So in some in some sense, first do a merge sort on this and nothing is to be done here so far and then how do I do this? Once again I divide and then I do a merge sort on these two but first I do it here. Only when I have finished this merge sort, will I come back to this one and how am I doing the merge sort here? Once again I am dividing. This is the trivial case. So this is easy to do. This is the trivial case. I get this sorted sequence. So I have finished the merge sort for this part now. So this goes back. Now I have to do the merge sort for this part. Once again I am doing a recursive call on this. I do a call, divide trivial merge; 45, 63 and now this is my sorted sequence. So I am now ready to go up and this is now the two sorted sequences that I get. Now I can merge this. So I will show you the merge step again more clearly but you can see which should be the first element. It should be 24. So this is the sorted sequence or the merge sequence. 

Now I am done with this part. This merge sort call has finished. The merge sort call that I have made on these four elements that has finished. So it goes back. Now I do a merge sort call on this. So I do one more merge sort call here. I am not showing you the entire thing. I am just showing you that this entire thing would happen in exactly the same way. 17, 30, 1, 96, 30 would sort and you get 17, 30, 1, 50, 96. This goes back and then we will merge these two and this is the resulting sequence. So these are two sorted sequences we can merge them. I will show you how to merge them very quickly and that will give us the resulting sequence. So who can tell me how many merge sort calls have I made in all? How can you count the number of merge sort calls you have made? In this example how many merge sort calls have I made? Order n. Why because each of these blocks correspond to one merge sort call. [hindi] so the number of merge sort calls is just the number of nodes in this tree and how many nodes are there in this tree? 
Well you can see that this is the tree? How many leaves are there in this tree? n leaves. There are n leaves. At this level there will be n/2 elements. at the next level there will be n/4, n/8 and so on and on. Some of this sequence n by two n by four n by eight n by sixteen so on is n minus one you can check that. There are many other ways of thinking of it. You can think of it has a complete binary tree on n leaves and you can apply those formulas that you learnt. [hindi] so this will do exactly (n-1) recursive calls to merge sort.

(Refer Slide Time: 18.49)

		 

So let?s see the part that we missed out on how do we merge two sequences. I should really say two sorted sequences.  So let?s say these are two sorted sequenences. I have S 1 S 2. 24, 45, 63, 85, 73, 150, 96. These are I believe coming from 24, 45, 64, 85. Are they the same? No they are not the same. So these are the two sorted sequences.  We have to merge them. So what should be clear is that the first element of the resulting sorted sequence is the smaller of these two. So it is clear that the resulting sorted sequence which is S has its first element 17. Then I can strike away 17 from here because I have kind of removed it and I have put it here. I have to merge these two sequences and append them to this sequence that I am constructing. So how do I merge these two sequences now? 

Once again the same idea which is the smallest element which is the next element will be the smallest element in these two sequences put together and that will be one of these two. The next element should be 24 and so I remove 24 out of there and I get two smaller sequences like this. Once again I have to find out the smaller of these 6 elements that has to be the smaller of these two. So I will just compare these two and I will write that down here. So I compared. I found 31 is smaller. I write that down here and I remove 31 from this and then in this manner I can proceed. So 31, 45 would be the next one to go. These are things left. Then next one would be 50. It would be left with 63, 85, 96.


(Refer Slide Time: 20.40)

		 

Then the next one would be 63. We would be left with 85 & 96. Eventually it would be taking that and we would have nothing left. This is what you would get as a result.
 
(Refer Slide Time: 20.56)

		 

So how much time do we take in doing? Order n. Why? Because what are we doing? We have these two sorted sequences. We are comparing two elements; the two right & the two front elements. We are comparing them and taking the smaller of those two and writing them out into the resulting sorted sequence. With every comparison, we are writing out one element.
How many elements do we write out in all? As many elements as there are. so if there are n elements, then I am going to be writing out n elements in all which means how many comparisons am I making? n comparisons. This was assuming whether two sequences were of length n/2 and n/2. Suppose one sequence was of length ?l? and the other sequence was of length ?m?. One sorted sequence of length ?l? elements and the other sorted sequence with ?m? elements in it. S 1 & S 2. What is the total time taken by merge? (l+ m) . These are the four choices. Let?s see minimum of (l, m) max of (l, m) [hindi]. (l + m) [hindi] none of above. [hindi]none of the above [noise]. Which do you think is the right? Let?s look at worst case. How can you tell me what the worst case is? Let?s look at worst case. In the worst case what do we have to do? We have to in the worst case we will have to compare. The worst case should be (l + m). We might have to do every possible comparison. 

What is the best case in your opinion? Best case it?s not really clear. What I am trying to say by best case because what might happen is that if one of the sequences finishes then I can just copy the next one. So even the copying requires time. So if you include the time for the copy, then its always going to be l + m. you cannot do anything more about it. But if you don?t want to include the time for the copying, if you are just counting the number of comparisons that you do. Then if you are just counting the number of comparisons, then you can do less than it will be. So you all understand this. In the worst case, why do you require l + m comparisons? So you might require as many as l + m comparisons. So in fact that was happening in this example.  So you know if you count the number of comparisons that we did in this example. For every element that we had to write out, we have to do a comparison except for the last one. So you really require l + m -1 comparisons. That is the worst case. The number of comparisons if you are counting the number of comparisons. You really require that many and you can create sequences of arbitrary values of l and m so that you require l + m - 1 comparisons.

(Refer Slide Time: 26.19)

		 

So we have understood merge. We have understood the merge set we understood what merge sort is. So let?s try and analyze it. How much time? How many comparisons does merge sort take in all? This is a quick recap of what we said so far. We have this sequence. We spit it into two. Then we spit this into two, this into two and so on and on. When we had these single ones, then we merge this and we got 15. In the previous example I had shown you them going up. I have just kind of created a mirror image. so I have merged them here. Merge these 2 got 2 4merge this to get 3 6 merge these 6 get 2 6. Then I decided to merge these and I get1, 2, 4, 5 here. I merge these two get 2, 3, 6, 6 and then I merge this to get 1, 2, 2, 3, 4, 5, 6, 6. 

So you could also think of pictorially. We could also think of this as what is happening that you did your divisions of algorithmically. It?s the same thing you are doing. Your recursive call this is just a way of representing it. Why am I doing it this way because it will now be easy to compute what the running time is. Can you by looking at this picture say what the running time should be? No time spent in all of this part. No time spent in this split part but when we are doing the merges we spend time. How much time do we spend in doing this? Merge two units essentially. How much in this two? Why am I counting two length of the two sequences? I am merging. I am always going to count that one l + m. so [hindi] each of these merges require two unit of time and how many such merges would I have to do at this level n by two such merges? If these were n elements, then I would do n by two merges of two elements. So total time would be n /2 into 2 n. so that?s what is going to happen. We are going to see another way of doing this in a short while.  But you should understand what?s happening. So each of these merges is taking two units and there are n /2 such merges and after that we get sequences of length two. 

After this step we get basically sequences of length two. As you can see now we are merging these. So each of these merges is now going to take 4 units of time and there would be n /4 search merges. This should correspond to this and the next one would take 8 units of time and that corresponds to n /8 and there are n/8 such merges. n here is 8. So there is only one such merge but in general there would be n /8 and so you would get such a sequence. This is what we have to sum. This is the total time spent by your algorithm. We will see this in a slightly different way in a short while.
 













(Refer Slide Time: 29.45)

		 

So we are going to analyze it once again using recurrence relations. So we had used recurrence relation before I am going to say it once again so that it is refreshed in you mind. So typically for all recursive of algorithms of procedure which are recursive in nature, you can analyze their running time using recurrence relations and is basically an equation or inequality that describes a certain function in terms of its values at pervious points at smaller inputs. So for instance, for a divide and conquer kind of a problem, the time taken to solve T if T of n is denotes the time taken to solve a problem of size n, 
Then it is equal to the time taken. Look at this part. Suppose I divide it into some number of pieces, how many problems did we divide it into two and each of the sub-problems was half of the original thing. 

So the sub problem by sub problem size factor, I mean by what factor have I reduced the size of the problem? So that is the time required to solve each of the sub problems. This part for each sub problem, there is a time required that times the number of sub problems is the total time required to solve the various sub problems plus the time required to do the division plus the time required to do the combining. That would in general denote the recurrence that you would get. Of course this is when n is more; if n =1, then it is basically a trivial problem and whatever the time required to solve the trivial problem would be the value. It would be kind of the base case for this recurrence. The initial condition for instance for the merge sort problem, we divided the problem into two pieces and each of the pieces were of size n by two. So the time required to solve the problem of size n /2 to sort to merge sort n /2 elements will be T n /2. If T n denotes the time required to merge sort n elements, we are using T of n to denote the time required or the number of comparisons required to merge sort n elements. Then this is the kind of a recurrence we get. I have written theta n here but you can just write n here because that?s the upper bound on the number of comparisons that we would do. Let?s say that?s the number of comparisons we are doing. So this is the kind of a recurrence relation. This is clear to every one. We have a problem of size one if n =1. 
This is just a constant because we don?t have to do anything here.

(Refer Slide Time: 32.48)

		 

There are bunches of ways of solving recurrences. We have only looked at what is called repeated substation method. What does that say? That basically says we expand the recurrence by substituting repeatedly and noticing any patters that you get. There is another method which is called the substitution method where you guess a solution to the recurrence and then you verify that your recurrence really satisfies that guess. In the course of this course, we will perhaps see examples of this way of solving recurrence relations. there is a third method called the master method which basically says that you know if a recurrence is of this kind, then this is the solution for this recurrence and you just plug in the values of a and b and you get your solution. I personally don?t like you to remember those formulas. It should always be possible to solve those recurrences by just looking at them. 

But these are three ways. Then there is a fourth method which is a recursion tree that you draw out a tree of the calls that have been made as we drew and you count at each node, how much time is being spent . So we are going to look at lets say, the repeated substation method once again. suppose I am trying to merge sort n elements and n is some power of two, lets say two to the b, this will help us ease the analysis a little bit. So recall the recurrences T on n=1 and T on n equals T of n =1 if n=1 and T on n=2 times T of n by 2+ n. If n is more than one, so we are just going to adopt repeated substitution. I just keep writing over and over again and it?s same what I get. So I get T of n =2 times T of n /2 + n. so I substituted once and now I am replacing this T of n by 2. So this is the substitution step. I am replacing this T of n /2 with 2 times T of n /4 + n /2. Why n /2 because this is T n/2. so this part ??? is coming in here and this part whatever is n written here, it comes again here as plus n and now I just expand this which means I just write it as 22 T n/4 + 2n. Once again I substitute for T n/4. And I expand 2, get this and now I have to observe a pattern basically.
What is that?s happening? I get 22 times T 4 times T n /4 + 2 n, then 8 times t n /8 + 3 n and so on. So in general at any step, I am going to get something like after I have done this thing, I times I get 2 to the I times T of n/2 to the i + i times n. that?s what the 3, the 2 and 1 here and now if I choose i to be log n, what do I get? I get 2 to the log n times T of n by n + n times log n. now T of n by n is 1, 2 to the log n is n T of 1 is 1. so this becomes n in all and  this becomes n log n. so I get a total of n plus n log n which is order n log n. so that?s the total time taken by this procedure. Everyone understands this. So the very simple recurrence you can just solve this using repeated substitution. Now if you are understanding this, let me ask you a question. 

Does this give us the average case time? It is in all cases it will be the same time because we have written that the time to combine the time to combine is n. this is the combining time at the time to combine is always n. so to say because we need to look at our merge step more carefully, when we look at our merge step more carefully and I actually show you the code, we will see that you really need that much time. You can?t do in less than that time as many elements as you trying to merge total number of elements. You are trying to merge. You really need that time. so what you have actually argued is that of n- the time for merge the number of time for merge is theta of n log n. it is not omega. It?s actually theta. It is always you take this kind of time. its bounded by some constant time n log n upper bounded and lower bounded also by some times. Now suppose I had instead of this one here, I had a C, what do you think will be a solution for this? Why am I doing this? [hindi] n into n plus c log n. so this is by just looking at the solution you can see that this is what you would get [hindi] provided C was not that big. the one way of also thinking this entire thing is what we said a recursion tree. Let?s draw out a recursion tree for this. Let me write down the total time taken in the division and the combining at this node. [hindi] 

So I should write down n here. I should write down n/2 and so on and on.  [Hindi] now what?s the total time taken? It?s basically the num sum of numbers written in these nodes which is n here. This level [hindi] and so on. [hindi] I have not said that these c?s are necessary constants. There could also be something will depend upon n. n which case this is how. this is another way to view this thing. You can draw out this tree and you can write the total time being done at that node in the combine at the divide step. So this is what we do in the repeated substation method. Substitute, expand substitute, expand and keep doing that. Observe a pattern. So do it sufficient number of times. You observe a pattern and that on how you would get an expression after the ith substitution and find out a value of i which gives you the value the initial condition. So to say this is what the method is.








(Refer Slide Time: 42.44)

		 

So I can now show you a kind of a java implementation of merge sort. So these things are clear. It is a very simple algorithm. That?s why I am showing it to you. Here we are also taking a comparator object. What is a comparator object? We are not just merge sorting integers. We could use this to sort anything.  So we are looking at a java implementation of a merge sort. So this would require the sort object. So you define an interface called sort object which is implementing a method called sort and this method requires a sequence and a comparator. Why comparator because we said we are not interested in sorting integers. We could be sorting anything. So if I give you tow objects, this comparator is going to tell me whether one is less than the other, greater than the other and so on. Depending upon what you want to do, the sorting on you will design a comparator. So the comparator typically has three functions. ?is less than?,  ?is equal to?, ?is greater than? and you could also have ?is less than? or ?equal to also? or you could just these two do the implementation and all of these three functions are essentially Boolean. They will say true or false.














(Refer Slide Time: 44.31)

		 

So this is basically the merge sort. You have this sort. So you have the sequence comparator.  Let?s say ?n? is the size of the sequence. If n is less than two which means they are just zero or one elements. Nothing to be done. It is already sorted. Otherwise you create a new sequence. S 1 is a new sequence and you are going to insert the first half of s into S 1. So let?s say we have an operation called insert last which will insert an element into the sequence S 1. This is an operation. This is the method on the class sequence or a data type. Similarly we create a new sequence called S 2 and we put the second half of s into S 2. These are how you implemented your methods. You could have done it this way. You could have done it in some other way. But let?s say we have such methods. Then you can break it up in this manner and now you just need to call sort. Sort on S 1 and S 2. This merge sort on S 1 and S 2 and then eventually you need to do this merge. So S1 and S 2 were two sorted sequence. c is your comparator and S is lets say the resulting sequence you want. 

So you will have to do this step and what does this merge step contain? This is basically a two sorted sequence. You have a comparator. This is the resulting sequence. What you?re saying is that while both the sequences are non-empty. You will keep doing something. What is that something? You will compare the first element of S 1 and the first element of S 2. If the first element of S 1 is less than or equal to the first element of S 2, then what is that you should add to S? You should add the smaller which is S1. you should add the first element of S 1 to S. so you are going to remove the first element of S1 from S1 and add it to s. that?s the only thing you are doing here in this statement. Get the first element of S1, remove it and then insert it into S.


 
And you are essentially repeating this. If this is not true, then you will insert, remove the first element of S 2, insert into this and you keep repeating it. Still either of S 1 or S 2 becomes empty when one of them becomes empty. Then depending upon which one it is, if S 1 is empty then that means that you have to just copy S 2 to S. so which means that you will once again keep removing one element at a time from S 2 and inserting it at the end of S. 

That?s what you are doing. Removing the first element of S2 and inserting at the end of S. You keep repeating this. If S1 is empty, if S2 is empty, then that means that S1 is left with some elements. We will keep removing one element at a time from S 1 and insert it into S. With that we will end today?s discussion on sorting. We learnt what merge sort is on which we also saw the divide and conquer paradigm for sorting is.  
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 23 
More Sorting

Today we are going to continue our discussion on sorting. We will begin with radix sort. Then we are going to look at bucket sort. In place sorting is something that we have seen before which are the examples that we know of in place sorting. Heap sort, quick sort, insertion sort, selection sort, bubble sort. Bubble sort we have not done. We are going to see which is in place and which is not in place, not all of these are in place what you just said. We are going to see and then we are going to finally look at how fast we can sort. Can we do better than what we have been discussing so far. What is a radix sort? 

(Refer Slide Time: 1.21)

 

See in radix sort we are going to look at the keys that we are going to sort. So recall that in all other sorting algorithm, we are only comparing the keys. We are not looking at what the actual structure of the keys is. It doesn?t really matter if the keys or people provided you have a way of comparing two persons. If you have a comparator function which is given two persons who can say one is less than the other then you can also sort people. But today or in radix sort, we are going to look at the key itself that we are trying to sort, the collection of keys. We are going to assume that the keys are represented in some base M number system and M is called the radix. If M equals 2 then the keys are essentially in binary, base two.
 



(Refer Slide Time: 02.58)

 

This could be an example 9 is 1 0 0 1 in binary, in base two. We are going to use this representation of 9 to do the sorting first. I can represent 9 in base form or some other base also, base three let us say. Then M would become three and I can still use that to do the sorting and we will see how. And in radix sorting, the sorting is done by comparing bits in the same position. If instead of comparing numbers, instead of comparing 9 with 11, we are not going to compare 9. We are just going to compare the bits in 9 and 11. Which bits? Let us say look at the bit at position three and we are going to compare them. 
We will see all of that in a second and this idea can be extended when the keys are let us say alphanumeric strings also. Not just binary numbers, not just numbers like this but alphanumeric strings so name of people or some such thing. You can also use a same idea to do sorting on that and we will see how. 

I am going to talk of two variants on radix sort. One is called the radix exchange sort so what we are going to do in radix exchange sort is we are going to examine the bit. I am going to assume now that the keys we are trying to sort are some numbers represented in binary first and am going to examine the bits from left to right. Let us assume also that each of the numbers that are given to us have a fixed representation. They are expressed in the same number of bits. That can always be done if the largest number in your collection is let us say some N then you need basically log N bits to represent that largest number. So with log N bits, you can express all the other numbers in log N bits and that would be the number of bits that you would use to represent all the numbers in the collection you have. We are going to sort the array with respect to the leftmost bit first. 

Suppose the numbers are sitting in this array and the left most bit of this numbers. So it really doesn?t matter what these other bits are. I am just looking at the left most bit of each of these numbers. This is the first number, this row the second number is in this row. 
So there are 5 numbers, 5 rows. I am going to look at the left most bits and I am going to sort the numbers according to this bit which means these are two zeros, so they come first. So this number goes at this position, this number goes at this position and the ones come later. This goes here, this would go next and this would be the last (Refer Slide Time: 05:54). This is what I would do. Sort according to the left most bits and since the bit can have only two different values, it is easy. The zeros comes before the ones. Clear to everyone? Now we partition this array, so this is not yet sorted, this set of numbers is not sorted. I will now divide it into two parts. This is the top sub array and bottom sub array. I am going to sort this top sub array independently of the bottom sub array. What do I mean by sort the top sub array? Just look at these numbers, forget this bit because this bit is same for all of these numbers. It will not make any difference in the value of the number, I will just forget this bit.

(Refer Slide Time: 07:54)

 

Similarly when I am sorting this, I will forget this bit. What does it mean to forget this bit, when I am sorting these numbers? I am saying, I would take the number or remove and subtract some number from that. What is that number? If these where k bits number, I am subtracting 2 to the k from that number. If I subtract 2 to the k from each of the numbers and then sort them, then it is same as the sorting the original collection of numbers, it doesn?t makes a difference. I can sort this bottom sub array independently of the top sub array and then I just put them together and I would get a sorted sequence. It is a divide and conquer algorithm once again. 

There is a divide step in which zeros come before the ones. There is a conquer step and the combined step is triviled here so the conquer step, this is the recursion. We will recursively sort the top sub array ignoring the leftmost bit. We will recursively sort the bottom sub array, ignoring the left most bit once again. How are we going to sort these? Using the same idea, we are going to partition this using the second left most bit, this left most bit and so on. How much time does it take to sort these n-bit numbers? I claim it takes order b n time, if I have n numbers and b bits. Why is it?  
Why can you do it in so much time? Pardon. No of bits is log n. Is the number of bits log n? Log of the largest number. The largest number need not be n. It could be much larger than n, it could be much smaller than n. [student: the number of keys can be at most the largest, n is only the number] n is the number of numbers and not the largest number. [Student: exactly sir the number of keys n is at most the largest number.] Number of keys is at most largest number, this statement is not true. You could have duplicates and also the largest number can be much larger than the number of keys [student: 1 to 20,000] okay great.

We will continue this discussion later. So I claim to sort n b-bit numbers you will require order b n time and why is this? Let us try and understand this. Can I write a recurrence for this? Can someone write a recurrence for this? Let?s write a recurrence. Let me stretch and write a recurrence so T (n, b). Why comma b? Number of bits. So T (n, b) is the time to sort n b-bit numbers. Let?s say this is T (n, b). What is this equal to? We are going to partition this n numbers into two pieces. This was the top sub array, let?s say this has i numbers in it and the remaining would be n minus i. Now how much time does it take to sort these i numbers? Because we are going to do it recursively. So it should be T (i, b-1) that?s important the b minus 1. 

Why is it important? Because we are going to ignore the left most bit because the left most bit is going to be the same for all of these numbers. The time required to sort the other n minus i numbers is n minus i, b minus 1 plus something more. How much time? Because I had time to partition this numbers, the numbers in which the left most was 0, come before the numbers in which the leftmost bit was a 1. They had to be rearranged for each and that can be done in order n time. This is the kind of recurrence we would get. 

Now what is the solution for this recurrence? Anyone? What is the solution to this recurrence?  b n.  No, it?s perfectly right if you are saying b n. Let?s substitute, so this is the third method we had talked about. Solving recurrences by guessing a solution and substituting that solution on actually verifying weather its true or not. Let?s see what it should be? Let?s not worry too much about it. Let?s see, suppose T (n, b) was equal to b of n. May be I am wrong. Let?s see whether I am right. What is my right hand side then? The right hand side equals i because n is i times (b minus 1) plus (n minus i) times (b minus 1) plus n which is, this part is (b minus 1) times (i plus n minus i) plus n (b minus 1) times n plus n which is b n which is also our left hand side. So this is correct. This is a solution to this reference. No harm, so this is the time taken and we will say other ways of arguing the same thing.








 
(Refer Slide Time: 13.22)

 

No, we don?t have to do that. This would be the same for all choices of i, we don?t have to any averaging here [student: but when we did quick sorting]. That was because we were doing a randomized quick sort. We were computing the expected time [i can be anything], i can vary but the whole point is no matter what i is. It will always be the same. Their depending upon what i were, the time would be different and so we were computing the average. If we try to compute the average here, you would get still the same. The point is it is always the same no matter what i is. No, even then you don?t have to go. If you do repeated substitution, once again you will see of course it will be more complicated which is why I did not do that method here. You can also solve it by repeated substitution and you will get the same answer. It is just that you will have to keep track of what i?s you use in the various points in the recursion and that will make it a bit more cumbersome. That you will get a same solution. These are all b bit numbers that is what we assume.

The maximum number of bits is b exactly, n has to be less than or equal to two to the power b. No, this is not true. I never said these are distinct numbers, I could have repetitions. [Student: can?t we just say that we have] so that would be one way of arguing it. I just wanted you to show, how to solve recurrence relations also. There could be many ways of arguing the same thing, that?s one argument. Everyone understands what the algorithm is. Yes, how do you combine in this divide and conquer step. We don?t need to combine once we have sorted this top sub array and we have sorted this bottom sub array. All the numbers are b bit exactly. Here that is why this is all uniform. That is why I said, you will take the largest number, see how many bits you need to represent in that and you will use that many bits to represent every number. Otherwise it does not work. 




You just target with zero?s to the left. If 8 is 1 0 0 0, 4 bits then 2 would be 0 0 1 0, 4 bits. 
Great so let?s continue [student: what about negative] we will not worry about negative numbers right now. If you had negative numbers and positive numbers what will you do? 
First split the numbers into negatives and positives, sort them separately and put them together. Why make life more complicated in that? You can always sort them separately. 
No, that will not happen. We will see more examples and it will be clear. In the previous slide I said that you will have your zero?s before your one?s. That was the first step we did let me go back [HINDI] we took these numbers, we changed this so that you had the numbers in which the leftmost bit was a 0 appearing before the numbers in which the leftmost bit was a 1. We did this kind of partition. 

(Refer Slide Time: 17:28)

 

How do you do this quickly? If you recall in my recurrence, I wrote order n for this and you did not arrays occur on that. The point is we can use the partitioning algorithm that we employed in quick sort to do this. What does that mean? We will scan from top to down, finding the first key with the one in the left most bit and from bottom to up finding the first key with a zero in the left most bit and swap. The same kind of technique we use for quick sort and we will exchange these keys and we will keep doing that until you know the scan indices exchanged. So that we know that there are zero?s above and ones below. How much time did it take? At most the size of the array. 

In this manner we can do the partition and then we can just call it recursively. We will get a time of order n b. So that is what we are doing here. We are scanning from the top to bottom so this is the first place we find a 1, this is the first place we find a 0. We swap them, 0 comes here, 1 comes here. Then the next index would be this one and the next here would be this zero. We will also scan them and we would get this and now we are done.

 
(Refer Slide Time: 18.41)

 

What is happening? How is this different or related to a quick sort? I will come to that in a minute but before that see what we are doing at each step. Suppose these are the numbers before we sort them. What does this mean? These are the numbers, the first number in my array is this let?s say this is the value. The y coordinate is the value of the number, this is the number, the second number is this, this is the third, this is the fourth, this is the fifth, this is the sixth. What I am doing at the first step is that I am partitioning the numbers according to numbers which are more than 2 to the b minus 1 and numbers which are less than 2 to the b minus 1. The numbers which are less than 2 to the b minus 1. Why 2 to the b minus 1? Because the first bit, the most significant bit we are saying should be a 1. They will come together and the ones for which it is a 0, they will come together. 

I am partitioning the numbers according to 2 to the b minus 1. Those numbers whose value is more than 2 to the b minus 1, I am moving it to the right part of my array. This is what is happening. These are the numbers with values more than 2 to the b minus 1, so they are in the right part of my array and the numbers which are less than 2 to the b minus 1, they are in the left part of my array. I repeat this on this one. Once again I am going to divide this up and now I am going to consider the numbers here which are larger than 2 to the b minus 1 plus 2 to the b minus 2 essentially. Within this part of course more than 2 to the b minus 2 after I ignore the leftmost bit but otherwise, so we keep doing this eventually we will get something like this which corresponds to a sorted sequence. This is pictorially what is happening. But you understand the algorithm. Now how does this compared to quick sort? Both the algorithms partition the array, both recursively sort the sub arrays. So structure is very similar, the difference is in the way we partition. In the case of radix exchange sort, we are partitioning with respect to not a pivot element but with respect to a fixed quantity. 

We are saying anything more than 2 to the power b minus 1 goes there in the bottom half of the array, anything less than 2 to the b minus 1 goes in the upper part of the array. It is the method of partitioning which makes the difference. In the radix exchange, divide the array based on whether the number is larger than 2 to the b minus 1 or less than. While in quick sort we are partitioning based on a pivot element. The difference is also in the time complexity. For radix exchange we argued just now that the time complexity is order bn.
 
(Refer Slide Time: 21.52)

 

For quick sort we argued that the average case is n log n. Sometimes this might be a better scheme depending upon what the value of b is for you. That was a radix exchange sort where we were exchanging the elements in the array. I am going to look at another one version of radix sort, the principle is the same. This is also another way of implementing radix sort. Quick sort can be better when b is larger than log n. b can be larger than log n. You can have one number, you can have numbers which are 2, 3, 7, 11 and one number which is one million and three. Now you need a huge numbers of bits because there is one very large number so that b would be much larger than log n. Because the number of numbers is very small but the largest number is very large so that the number of bits you require to do your sorting is large. 

We will continue. This is another version as I said of radix sort, straight radix sort. We are going to examine once again the bits from right to left now. Not from left to right but from right to left. The k equal to 0 corresponds to the right most bit now. The least significant bit also called the least significant bit. So k equal to 0 is a least significant bit and b minus 1 is the most significant bit. We are going to sort the array based on this bit, the k th bit in a stable way. Sort the array in table way looking at only k th bit. Let?s see what this means? Do you understand what?s table way means? No, great we will come to that in next slide but let me show you what this is. This is the collection of numbers you have, what the algorithm is. You are first going to look at the rightmost bit and you are going to sort these numbers based on the right most bit. 
As you can see after you sorted them, you first have the numbers which have the rightmost bit as 0 then you have the numbers which have the rightmost bit as a 1. Then you sort these numbers based on the second right most bit. This corresponds to k equals 1, so you are sorting them based on this and then finally you are sorting these numbers based on this. What will happen? At the end of that you will have a sorted sequence.
 
(Refer Slide Time: 24.29)

 

As you can see this is originally non-sorted and what you have here is a sorted sequence. Why is this magic happening? You understand the algorithm. Take the rightmost bit, sort the numbers based on that which means that just restrict your attention to the rightmost bit. Anything that is a 0 comes before everything that is a 1. Now you have done some rearranging of the numbers. Now look at the second rightmost bit. They do the sorting with respect to the second right most bit. Everything which is a 0 comes before everything which is a 1 and so on and on. 

How much time does this take? b n once again, because for each bit you are spending time proportional to n [student: partitioning] partitioning will not be used. We will see how to do it. It is not completely clear why b n but will come to that argument also in a minute. First we need to understand what is sorting in a stable way. What does this mean? 
A table sort is one in which two numbers are the same then after sorting, so if two keys are the same, equal keys then after sorting their relative order remains unchanged. Suppose I have two numbers so I have a collection of numbers. I have 1 3 11 3 5, these are my numbers. I have two threes in there, equal keys. Now if after sorting, of course after sorting this array would become 3 3 5 11. But the two 3?s I have now, suppose with those threes I had one which was colored red and the other was colored blue.
 
The first was colored red and the second was colored blue and after sorting, the red should appear before the blue. Although they are still threes, they should appear in the same relative order as was the case before we did the sorting.
That is called table sort and we will see why it is relevant here and why it is important here. Let?s look at this now. This is what we were sorting and let us say we are sorting with respect to the right most bit.
 
(Refer Slide Time: 27.05)

 

I have 4 keys with 0, they will all come before the 4 keys which are at a one which have the rightmost bit at a one. But I would like that this appears before this, so the first number should better be 0 1 0. The next should be all three zero?s. The next should be 1 0 0, the next should be 1 1 0 which is the case here. I am not permitted to rearrange them. When I am saying I just sort with respect to the last bit, you could also create this array post sorted in which this was the first number but that would be wrong. That would not be a stable sorting and similarly for the one?s. The first should be a 1 0 1, the next should be 0 0 1, the next should be 1 1 1 and the next should be 0 1 1, this is crucial for the correctness of the sorting algorithm. If you don?t do it this way, this will not give you the sorted sequence at the end. So everyone understands what table sorting is. 

Now let?s understand the correctness of the algorithm. We are now going to show that any two keys are in the correct relative order at the end that means if I take two keys, one is smaller than the other. Then in the end, the key which is smaller appears before the key which is larger, very simple proof. So suppose these are the two keys that were given to me. Let me look at the leftmost position at which they differ.







 
(Refer Slide Time: 28.43)

 

Leftmost, so this they don?t differ at this place. They don?t differ at this place even but they differ at this place. Let me call this position k (Refer Slide Time: 28:50). Now when I am sorting this bunch of numbers what is going to happen? When I am sorting remember I am sorting by considering first the right most then the second right most then the third right most and so on. Let?s us understand this. The claim is that the step k, the two keys are put in the corrective relative order. At the first step they may be rearranged I don?t care, they are put in some order. At the second step also they are put in some order, I don?t know. But at the k th step, this key would put before this key because this is a 0 and this is a 1. At the k th step the two keys are put in the correct relative order but all is not done now. 

We want to argue that at the latest step, at the k plus 1 th step and the k plus 2 th step and so on, the relative order is not interchanged [HINDI] because of stability. Now when I am looking at the k plus one th step, at the k plus one th step these are the same. These are the same, because it is a table sort this key which is appearing before this would continue to appear before this and similarly at this next step and so on and on. Beyond the k th step, the relative order would not change anymore. At the k th step you would get the right order between these two keys, the smaller key will appear before the larger key and add sub sequent steps, this relative order would be preserved. The smaller key would be continued to appear before the larger key [HINDI].








(Refer Slide Time: 31.13)

 

Let?s take an example. This is the two keys once again that I am considering. Initially they could be in some arbitrary order. I have in fact that the larger keys appearing before the smaller key. This is the array in which the numbers are? [HINDI]. This is location zero of the array and so on. You would like that the keys be in increasing order but right now bigger number is appearing before the smaller number. When I am looking at the k th step, at this step when I am sorting with respect to this bit, the k th bit I would have put 0 1 0 1 1 before 0 1 1 0 1 clearly because at the k th position, this is a 0 and this is a 1.
 
Now when I am looking at the next more significant bit, I would continue this relative order because at the next more significant bit they are the same. If they are the same then stable sorting ensures that I have to maintain the relative order that was there till that point in which this was appearing before this. So this will continue to appear before this step and in subsequent steps also. So because the sort is stable, the order of the two keys will not be changed when bits more than k are greater than or bits at position larger than k are compared. You can also see now, why I had to start from the right end. If I start from the left end then this technique is not going to work. Take this is an exercise, think of an example if I were to start from the left end, this would not give me a sorted sequence at the end. There is nothing sacrosanct about binary numbers, I can also apply the same technique to decimal numbers. 









(Refer Slide Time: 33.15)

 

What do I do? First I sort with respect to the right most digit which means that the ones would come before the two?s would come before the three?s and so on and on. Again the sorting is stable. After the first step as you can see, the first number would be, so there is a 1 1, there are two 2?s here and so on and on. There is a unique one 0 3 1, so 3 1 becomes the first number. Then there are these two 2?s, 0 3 2 and 2 5 2 which should be the second number. 0 3 2 because it?s stable. So 0 3 2 is the next, 2 5 2 is the third and so on and on. So I sort with respect to this. Next I sort with respect to this, so there is a 1 here, 0 1 5, there is another 1 so I will first put this and then I will put this and so on. As you can see at the end I get a sorted sequence. 

Now we need to figure out the time complexity, how much time have you taken? So how man passes, such passes are we are making? We are making as many passes as the number of digits or the number of bits or whatever it is. But we need to now see what we are going to do in one pass. How are we getting a one?s before the two?s before the three?s and so on in a stable manner. How much time does that take? What kind of a scheme should be employed for that?
 
Exactly, for exchange radix sort we basically wanted to partition the array into two parts only but here because these are digits, not just 0 1 it is not a two way partition anymore. For decimal numbers, we will have to represent it in a binary form to be able to do this.  What is like an insertion? I don?t quite follow what you are saying. We will discuss this later. Let?s figure out what the time complexity is. For k equal to 0 to b minus 1, we are sorting the array in a stable way looking only at k th bit. Suppose this could be performed in order n time then the total time complexity would be order bn. That?s completely clear provided we can do this sorting in order n time which sorting we are talking about. We are looking at a particular digit, one pass. We are looking at a particular digit or a bit and we want to ensure that all the numbers, if I am looking at decimal numbers all the numbers are sorted based on that digit. The one?s before the two?s before the three?s and that the sorting is stable. We want to be able to do this in order n time and the method of choice is what is called the bucket sort algorithm.

(Refer Slide Time: 36.34)

 

That brings us to the second sorting scheme that we talked about. So what is bucket sort? Lots of buckets. We have n numbers, each number is in a certain range. Let?s say one through m, so bucket sort is a stable sorting algorithm and it will take time order n plus m. You understand what we are talking of. This is very useful when you have a large number of numbers with lots of duplicates perhaps and the numbers are coming from the small range. 

Then you don?t need something like n log n time or some such thing. You can then do it in time order n plus the range of the numbers essentially. Let?s see how this works. Suppose this is my collection of numbers 2 1 3 1 2 so m is 3 because you can see the numbers are in the range 1 2 3. The m is 3, there are two two?s and two one?s. So first what we are going to do is you are going to create an M buckets. You can understand what you will do in each bucket. Just take a number and throw it in an appropriate bucket, so we have these 3 buckets.











(Refer Slide Time: 38.04)

 

One corresponding to each possible value in this range and each m element of array is put in one of the m buckets. So these are my buckets, take the first number it goes into bucket two, I put it here.
 
(Refer Slide Time: 38.17)

 

Then I take the next number it goes into bucket one, so I put into here. The third number goes into bucket three, I put it here. The fourth number goes into bucket one, so I append it at the end of this list. End is important to maintain stability and then I take this two and append it at the end.
Now I will just read the numbers, I will take the numbers in the first bucket, basically append all of these lists. So 1 1, 2 2, 3 and so on. I will put the elements from the buckets into an array and just read it off in this manner.
 
(Refer Slide Time: 39.05)

 

This gives us a stable sorting. You understand why it is stable? Because if two keys are the same then they would be in the bucket but we would also have put them in the right order. That?s why we are appending. If we were attaching at the front then we would have to read it the other way around which is the same as that. So with that you should be able to argue that our straight radix sort takes order bn time now. We said for each pass we want to do it in order n time and you can do it order n time using such a scheme, using bucket sort. So you do that in order n time, there are b passes in all so it becomes order bn. Yes, you are going to get one question from this in the exam. Yeah, okay in-place sorting. Yes, you want to know what the question is. 

A sorting algorithm is said to be in-place if it uses no auxiliary data structures. It could use a constant amount of additional space over here and it updates the input sequence only by means of the operations replaceElement and swap. Basically it?s just you have a bunch of numbers, it replaces one of them by some other or it just swaps two numbers. That?s when we call the algorithm to be in-place. So let?s see which algorithms we have seen can be made to work in in-place.








(Refer Slide Time: 40:59)

 

Bubble sort actually you have not seen right or you know what bubble sort is? You don?t know what bubble sort is? I will not go into bubble sort then. Let?s see. Who can tell me heap sort. Is heap sort in-place? [Student: yes sir] We can have one array and basically implement a heap in that array and we are just changing the elements in that array. Merge sort. Is merge sort in-place? [Student: no] Why not? To merge two list, you need additional space. You cannot merge in the same list, so merge sort is not in-place. Quick sort [student: yes sir] Quick sort is in-place because we partition in the same array and then we just did recursively left and right. 

To do the merge, to merge two lists you need additional space because what were we doing in merge? We are taking the first element of the two list, comparing them and putting it out into some other space. You can?t just copy it back there, it would not work. [Student: in an algorithm uses what are one space would increase] Yeah, order one space is okay. It is in-place but not space proportional to the number of elements, it should be independent of the number of elements. You can look at the other algorithms and think off whether they are going to be in-place or not. Radix sort. Is radix sort in-place?  But number of buckets is independent of the number of numbers. [Student: but we are inserting] then nodes that we are creating. Yes, that is additional space. 

So can you modify the scheme to make it in-place? [Student: to keep every bucket just count the number of those elements that are in the array] Yeah, you will have to think about it, think about this. Can you make it in-place? Can you make radix sort in-place? It is a good thing to think about. Let me get to the last topic that we are going to cover as far as sorting is concerned and that is a lower bound for comparison base sorting. What does comparison based sorting mean? It basically means that we are only looking at sorting algorithms in which all you have permitted to do is to compare two numbers. 

Suppose you have a bunch of elements, I am not even saying numbers now and you want to sort those elements. I give you those elements, I have a comparison function which you have to use to do the sorting. So like your comparator, so you give me the two numbers, I will tell which of them is smaller than the other because may be these are not numbers but some objects and I am the only one, who knows how to compare these objects. That is a comparison operation, you give me these two numbers, I decide whether one is less than the other or not, whether the first is less than the second or the second is less than the first and I give you the answer. 

Now the question is how many times will you have to ask me for a comparison? You understand, you wanted to sort this n numbers, how many times will you have to ask me? May be I charge you 1 rupee every time you give me certain comparison to do. The comparison is let us say an expensive operation. Every time you say compare these two numbers for me, I am going to charge you 1 rupee. How much money are you going to spend? n log n, you have seen algorithms which would take no more than n log n time. What we are going to argue now is that there can be no algorithm which takes less than n log n time, n log n comparisons. No one can come up with an algorithm so that algorithm will always take less than n log n comparisons for all inputs. For certain inputs it could take less than n log n comparisons but for all inputs, it would take n log n comparisons. That is not possible at all. We are going to understand this in the following way.
 
(Refer Slide Time: 45:50)

 

Let?s look at the particular algorithm. You have a certain algorithm and let?s say your objects are sitting in some array and you algorithm works on that array. The first step it?s going to ask me to compare two elements of that array. Let say that two elements are at position S1 and S2 [HINDI]. So my very top node here is this node whether S1 is greater than S2 or not. It is going to ask me this and I am free to say whether one is less or whether one is more based on what my comparative functions says. These are so to say, the questions that the algorithm is asking me.
First it asks me to compare S1 and S2. If I said a yes then it would have asked me for a comparison of S1 and S3. Let?s say I have such a thing. If I had said no, maybe it ask me for a comparison of something else, this need not be S1 S3. It could be something else, depends upon what the algorithm is. But at each point it is coming back to me with certain comparisons, with certain numbers to compare.

First time it says something then depending upon whether I say yes or no, then the next comparison it ask me something else. Now depending upon whether I said yes or no, the next comparison it could ask me would be something else and so on and on. The execution of the algorithm is really a path down this stream. Yes. Now at some point the algorithm is going to stop. It doesn?t ask for anymore comparison, it says well I am done, this is your sorted sequence, so which means that this path ends in this external node here, this leaf node here which corresponds to a particular permutation of those numbers. 
This sequence of moves would have been made, if I had certain ordering on the numbers for a certain permutation of the numbers. 

Let?s understand this. [student: there would be some finite criteria of the let me go for a S noise it is not necessary say S1 is less than S2 we can compare any] we can compare anything, yeah. [Student: so those numbers which we are comparing that is randomized over the entire set, any two numbers we pick up randomly] No, so don?t look at it that way. We are saying your algorithm, you have a certain algorithm which has the numbers written in an array, let?s say one through n. At the very first step it is going to come and make certain comparison. Let?s assume it is deterministic algorithm, no randomization for now. So same argument applies for randomization also but for now let?s assume it?s deterministic. 

It will say compare lets say, first time it comes to me it will say look at the number in location 3, look at the number in 7 and tell me which is smaller. Whether the number in location 3 is less than the number in location 7 or whether the number in location 7 is less than the number in location 3. I put a node here saying let?s say this array was S, so whether S3 is less than S7 is the first comparison it asked me for. If I had said a yes, I do not know. If I said a yes, it would go and ask me to compare some other two numbers. It?s your algorithm, I don?t know what it is going to ask me. But whatever it is going to ask me, I am going to put down here. 

Suppose it came and said S2 verses S5, whether S2 is less than S5 and if had said a no, may be it came back and asked me something else. It came and asked me S6 is less than S13, it may be. It is your algorithm [HINDI] so depending upon what option do I have. I have an option of basically saying my yes and no. That?s all I say. I say a yes then I say a no and so on and on, depending upon what I rate as the relative order of these elements in these array. Think of these objects are some complicated objects. You don?t know what the relative order is so that?s why you are asking me about that. I have some way of figuring out what the relative order is. May be today I feel like that the relative order should be based on gpa. Then tomorrow I feel like relative order should be based on height or whatever it is like, I can decide. For that I would make a certain choices of yes and no, which would eventually end up in leaf, in an external node of this. When you reach this say, you well I have sorted it. You will sort because this is all the comparisons you did.
 
The question is how many comparisons did you have to do? The number of comparisons is basically the length of the path that you took, the height of this tree. Yes. So how many comparisons would you have to do? How height does this tree have to be? [Student: n log n] Why n log n? [Student: n factorial permutations of the array so they have to be n factorial leaves] Yes, so that?s right. There have to be n factorial leaves in this tree. Now this is not a straight forward thing to understand [HINDI]. Depending upon what these objects are, I should be able to get all kinds of permutations. Every permutation is a possible solution at the end, every permutation of these n numbers. What is sorting? A sorting you are given the elements like this and eventually what do you generate? You generate a permutation of these elements. Yes or no? [HINDI] what is this? This is just a permutation of this set of elements [HINDI] this is just a permutation of this. Now all possible permutation should exist as leaves. That depends upon what the relative order is, what I have picked as the relative order. 

So given a certain permutation, if that is what I picked as the relative order then your algorithm should end up in that. If I took some other permutation and picked that as the relative order then your algorithm should end up in that and so on and on. Every leaf of this tree corresponds to a permutation and further more every permutation should be representable as a leaf. So which means that this tree has n factorial leaves. It is not a complete binary tree, may be it is. I don?t care; I know it is binary tree. It?s a binary tree with n factorial leaves. So what is its height going to be? At least log of n factorial. So height therefore is at least log of n factorial, at least [HINDI] yeah [student: sir our order to comparator, does it satisfy the symmetric or the transitive property] Yes, it satisfies all of them. 

Even then it can. I take a particular permutation of these elements and I say I am going to answer with respect to this permutation. All queries that are asked for me, I will answer with respect to this particular permutation that I have in my head. [student: is it possible, if suppose I have S1 there are 3 S1 S2 and S3] yeah [student: and at some point at that we have some decisions of S1 S2 and S2 S3] yeah [student: and then other point in the tree the other point in the tree decision based on S1 and S3] yeah [student: that cannot take both the paths yes and no because] Yeah I understand that, but that is not the point. The point all we are trying to say here is that [student: it is not necessary every node] that I could have a certain permutation in my head and I could use that to answer all your questions and it could consistent. 

So there have to be n factorial leaves and if in a binary tree, there are n factorial leaves. Then basically if there are some n leaves in a binary tree then it has to have a height of at least log n. So which means that the tree has to have a height of at least log of n factorial. Height of the tree is at least log of n factorial. This is roughly n log n which means that there is a certain permutation. What is the height? The height is the distance of the longest of the farthest leaf from the root. 
If this is the furthest leaf from the root, then if this were the permutation then your algorithm is going to take a time of at least n log n, on number of comparisons at least n log n it?s going to take [HINDI]. This is the argument, we can go over the slides once again and understand it more carefully.
 
This is the argument for why any comparison based sorting algorithm has to have at least n log n time. So recall you are only permitted to compare two keys. Radix sort is not an example of a comparison based sorting algorithm because you are not comparing keys. You are going into the keys, looking at the bits or the digits and so that?s why radix sort doesn?t have the complexity n log n. It could be less than n log n, if B the number of bits or digits is less than log n. Yeah, so radix sort is the only such. All other algorithms have to have because they are all comparisons based sorting algorithm, they have to have complexity of at least n log n and there are many which achieve that bound. So with that I am going to end today?s class. We looked at radix sort, we looked at bucket sort, we understood what stable sorting is and finally we saw this lower bound on comparison based sorting. 
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
            Lecture ? 24
   Graphs

Today we are going to start talking about graphs. We are going to spend quite a lot of time understanding the basic definition in terminologies associated with graphs, see some examples and then if time permits we are going to do the graph abstract data type or I think we will able to do the graph data type today.

(Refer Slide Time:  01:30)

 

So question is what is a graph? So pictorially this is what a graph is and what are terms we are going to have. So graph is always represented by a two tuple V and E typically, V?s what we will call the set of vertices and E will call the set of edges. So set of vertices and a set of edges together specify a graph. In this picture these red circles are the vertices. I have given each of these vertices a name a b c d e to distinguish them and the blue lines are the edges, so edge really is a pair of vertices. An edge is a pair of vertices or an edge is specified by giving a pair of vertices so this edge is said to connect what is u and v or will not use the term connect but this edge is an edge between u and v; when I say e = u v is an edge then that means it?s an edge between vertices u and vertex v, vertices u and v.






(Refer Slide Time:  2:08)

 

So for instance in this example this graph could be specified either by giving this drawing or giving these this detail. As in v the set of vertices is 5 vertices a b c d e and what are the edges I have? Each edge as you can see is a pair of vertices, an unordered pair of vertices here, a comma b is the same as b comma a. All that specifies is it is an edge between vertices a and b. So a comma b, a comma c is this edge; a comma d is this edge, b comma e is this, c comma d is that, c comma e is this and d comma e is this. So there are 1, 2, 3, 4, 5, 6, 7 edges and there are the 7 pairs mentioned here. So set of vertices and a set of edges. What are they used for? They are for lots and lot of applications, you can model circuits as graphs, each of component of the circuit could be a vertex. 
 
(Refer Slide Time:  04:18)

 
So this could be a vertex, this could be a vertex, this could be a vertex, this could be a vertex, this is a vertex which is your CS201, you are trying to find out the path of these resistance to get CS201, they can be used to model networks. So I can take the map of the city and every intersection could be modeled as a vertex and the roads which are connecting to intersections could be modeled as an edge and then that could be a graph and then start asking various questions on whether how can I go from this place to this place by asking the corresponding question on a graph. So transportation networks, lots of this communication networks all of them are modeled as graphs. 

(Refer Slide Time:  04:53)

 

One more example. So this is typically student day so you wake up, you mediate first 201 then you eat, may be you work then more CS201, play CS201 programming sleep and you dream of CS201 cycles. [Student: so idealistic] There is no room for any other course. This is the day before mine. So this is slightly different from the graph that I had shown in the previous example. Why [student: directed] directed. So this is what we call directed graph because we can?t do any meditation before you wake up. So there is clearly an edge going from wake up to meditation. So every edge has a direction associated with it, we will call such graphs directed graphs. So we also consider directed graphs but in the rest of the lecture I am going to spend most of time with undirected graphs. Whatever things I define will carry over in a straight forward way to directed graphs as well so I will tell you what the difference is. 

So to begin with let me go back to the previous slide. In this example or in this definition where would the difference be when I am talking of a directed graph? So e comma u v is not just a pair, it is an ordered pair let?s say. So the ordering is important, the first vertex typically specify what the start of the edges is or the origin of the edge and the other would specify the destination of the edge where the edge is going from, so what is the start and what is the end. 
So as I said today is a fairly simple lecture, we are going to look at lots of terminologies. So now you have understood what a graph is. So there are two kinds of graphs a directed graph and an undirected graph. So graph which is not directed is called undirected graph and you understand what a vertex is, what vertices are, what edges are. Adjacent vertices, so two vertices so this is all terminologies associated with an undirected graph so two vertices which are connected by an edge are called adjacent. Is this vertex and this vertex, these two vertices are they adjacent? No, they are not connected by an edge while this and this are adjacent and this and this are not adjacent either. So what is it which are connected by an edge are called vertices, the degree of a vertex. 

The degree of the vertex is the number of adjacent vertices it has. So what is the degree of this vertex? 3. So in fact I have written down the degrees of the various vertices on these so this vertex is degree is 2, this vertex is degree 3, this is degree 3, this is degree 3, everyone understands the degree of the vertex. It is the number of adjacent vertices. Sometimes we say that this edge is incident to these two vertices. Should I write down the word? So this edge, let?s say this vertex is vertex a and vertex b and this edge is e, so e equals a b is incident to vertices a and b. So this edge is incident to these two vertices similarly this edge is incident into this vertex as well as this vertex. So degree of a vertex can also be defined as the number of edges which are incident to that vertex. There are three edges which are incident to this vertex, so the degree of this vertex is 3. These are equivalent ways of saying the same thing. So question is what is the sum of the degrees of all the vertices, [Hindi Conversation] twice the number of edges. Because when I am counting, so let?s think of it in the following manner. So the answer is right, twice the number of edges and the argument is actually half a line of an argument. 

So pictorially I would say the following; when I am counting three for this, I am counting three because I am counting this one edge, this edge and this edge. So let me put 3 stones, one on each of these three edges then when I am counting 3 here I am counting this edge, this edge let me put down 3 stones. Then here I am putting down 2 stones, here I am putting down 3 stones, here I am putting down 3 stones. 
















(Refer Slide Time:  10:50)

 

So I have to put as many stones or pebbles, if you want as many peppules as the sum of the degrees of the vertices. Now if I look at any edge, how many pebbles are there on that edge? Exactly 2, so the sum of the degrees of the vertices equals two times the number of edges. So that?s degree and you understand what degree is, you understand what adjacent vertices are. Now let?s define the notion of a path.
 
(Refer Slide Time: 10:57)

 

So a path in a graph is a sequence of vertices let?s say V1, V2, Vk such that consecutive vertices have an edge between them. So if I take vertex Vi and Vi+1 then these two vertices are adjacent there is an edge between this vertices. So there are two examples here. So this is my graph, the same graph as before recall that there is an edge between c and e also. So this is the path a b e d c e. Why is this a path? Because there is an edge between a and b, there is an edge between b and e, there is an edge between e and d, between d and c and c and e, so this is a path. Similarly this is the path b e d c because there is an edge between b and e, between d and e, between d and c. it is easy to construct examples which are not paths. 

Suppose I had written down a b c, a b c is not a path in this graph. Why because while there is an edge from a to b there is no edge from b to c, so everyone understands what a path is. A simple path is a path in which no vertex is repeated so this is an example of a simple path b e c. 

These three vertices are all distinct so it is a simple path. A cycle is a simple path in which the first and the last vertices are the same. So a c d a is a cycle, d a c d is the same cycle, c d a c is also a same cycle. So you can read the cycle anywhere, this is a cycle this is a simple path. In the previous slide we had an example of a path which is not simple. This is not a simple path. Why, this is not a simple path because vertex e is repeated here.

(Refer Slide Time: 13:05)

 

So this is a simple path except that the first and the last vertices are the same. That?s what a cycle is. A graph is said to be connected if there is a path between every pair of vertices in the graph, [Hindi conversation] that the graph is connected. 







(Refer Slide Time: 13:48)

 

Is this graph connected? [Student: yes, the path] path [Hindi Conversation]. So this graph is connected, this is not connected there is no path from here to here, so this is connected this second one is not connected and this is the common mistake connected [Hindi conversation] there should be a path between every pair of vertices. 

 (Refer Slide Time: 15.37)

 

If there is a path then it is connected, if there is no path it?s not connected. So these two vertices so again this is the common mistake when you are writing a minus especially you are going to say these two vertices are not connected because you don?t see an edge between them that?s wrong terminology. These two vertices do not have an edge between them but they are connected because there is a path between these two vertices. So we say two vertices are connected if there is a path between them and a graph is connected if there is a path between every pair of vertices. Is this clear to everyone? Let?s understand the notion of a sub graph, so this is a graph on the left hand side suppose I take a subset of the vertices and of the edges such that the resulting thing is also a graph. 

(Refer Slide Time: 16.23)

 
So I took some vertices from here, this vertex you can see it?s corresponding. I took 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 vertices from here. There are 13 vertices in here I took 11 of them and I took some of the edges between these vertices. I am not taken all the edges, as you can see this edges is not here, this would be called a sub graph of this graph. I cannot takes this edge because the other point of this edge is not there, I have not included here at all. For an edge, the two vertices between which the edge is running are also called the end points of that edge. Each edge has two end points and those are the two end points so this is called the sub graph of this graph. 















(Refer Slide Time: 17:39)

 

Now let?s understand what a connected component is. A connected component is a maximal connected graph. Suppose this is one graph, it is not 3 graphs I have drawn just one graph in. This is not a connected graph. Is this connected? [Hindi conversation] [Student: this is not connected] this is not a connected graph. Why, because there is no path from here to here [Hindi conversation]. This is not connected because there is no path from here to here, there is no path from here to here, so it is not a connected graph. If I look at this sub graph it is connected just this sub graph. These three vertices and these three edges it?s connected. 

These 4 vertices and these 3 edges are also connected, these 5 vertices and the 7 edges on them are also connected. These 3 are the connected components of this graph. Now what?s the definition of connect? It?s a maximal connected sub graph. What does a maximal connected sub graph mean? This needs to be understood more carefully. Suppose I were to take this vertex and this vertex and I were to take this edge and this edge. This is a sub graph, yes or no? This is a sub graph of the original graph but this is not a connected component, I am not going to call this a connected component. Why? Because it is not maximal so what does maximal mean? So when we say maximal in this class, we mean a set is called maximal if we cannot increase the size of the set while retaining the property. So a set is said to be maximal with respect to a certain property. 

If we cannot add more elements to the set and retain the property that?s not true here I can add more elements to this set, I can add more edges or I can add more vertices and both. So I can add this edge and it is still connected I can this vertex and this edge and it is still connected, I can add this vertex and this edge and it is still connected, I can add this edge now it is still connected, I can add this edge now it is still connected. Now if I add any other vertex or any other edge, suppose I decided to add this vertex, I add this but it is not connected anymore. So this is a maximal connected sub graph and so we will call this a connected componenent so this entire thing is the connected component. This is also a connected component and this is also a connected component. [Hindi Conversation] I cannot add any other vertices and still have the property of it being connected. 

So essentially intuitively how do you think of connected component? You just see which are the pieces which are connected among each other, each of them is a connected component as simple as that. So this graph is 3 connected components. More terminologies; what is a forest? Forest is a jungle, jungle is a collection of trees and animals but we will leave out the animals. So we are thinking of forest as a collection of trees so these are trees in the forest now what is a tree here. 

(Refer Slide Time:  23:38)

 

A tree here is a connected graph which does not have any cycles in it. It?s the same as the tree that we till now except the [Hindi conversation] (Refer Slide Time: 23:00). So this is an example of the tree it is a connected sub graph as we can see and it does not have any cycle in it. This is also a tree, this is also a tree, this is also a tree when you have collection of trees it is a forest. So forest is a collection of trees so everyone understands this. What a trees? Tree is a connected sub graph which does not have any cycle in it. So I am typically going to use n to denote the number of vertices and m to denote the number of edges in any graph. So what is the complete graph? A complete graph is one in which there is an edge between every pair of vertices, between every pair of vertices there is an edge. This is an example of a complete graph.








(Refer Slide Time:  24.41)

 

This is a graph on 5 vertices between every pair of vertices there is an edge. So how many edges does a complete graph have? nc2, because there are n22 pairs of vertices and there is an edge between every pair and so you will have so many edges. How many edges does a complete directed graph have? [Student: three by two two] two times nc2 a directed and a complete. So basically there will have to be and edge in both directions right so it will become twice. If a graph is not complete then the number of edges going to be strictly less than n chose two. So in an undirected graph this is the maximum number of edges that a graph can have, n chose two [Hindi conversation]. 

Suppose I give you a graph on n vertices, zero, it might not have any edge at all. So the minimum number of edges in a graph on n vertices is zero and the maximum number of edges is n chose two. So once again we have n number of vertices, m number of edges. [Student: minimum elements connected in graph]. That?s the slide, suppose in a tree so what is a tree? Recall a tree is the connected graph which does not have any cycle in it. How many edges are there in a tree? I have said number of edges in the tree is n ? 1, why? [Student: every pair of] [Student: start from a node and we end and we cannot have a like a cycle so starting] So [student: we can have about one two one two two three after and the number of edges these vertices two] each vertex is degree two. In a tree every vertex is degree two, no. [Hindi conversation][Student: nodes we write a so that will be n minus one you can?t have repetition sir we get we can count the edges by, we will take the direction so the edge coming to a node is one].
  






(Refer Slide Time:  25.54)

 

What do you mean coming to a node?  So is this edge coming into this node or this edge coming into this node? [student: sir we take this as even starting from if you start from any particular node you don?t have whether number of node have to have a ] Let?s prove this. [Hindi conversation] it is a true statement [Hindi] so let?s prove that so what will be the proof? We have to prove that a tree on n vertices has n - 1 edges, induction [Hindi] as simple as that. So proof by induction, so what should be the base case? Let?s say n equals two so suppose I have a connected graph on two vertices [Hindi] statement is true. So number of edges equals n - 1 equals one. So induction hypothesis [Hindi] statement true for all n less than or equal to k let?s say. 
 
So now the induction step. So given a graph on k + 1 vertices. Why should this have k edges? [Hindi] one leaf good. So he is saying something useful, he is saying there is no cycle in the graph. We have to use somewhere the fact there is no cycle in the graph [Hindi]. He says that there has to be one leaf [Hindi] [student: degree one] good. So let?s define a leaf, now as a vertex, a leaf is a vertex of degree one. So his claim is that given a tree on k + 1 vertices. We are given a tree, we are proving this. The tree or every tree has a leaf [Hindi] so maybe we come back to one of the vertices we have already visited [Hindi] so it is not a tree [Hindi] because that was a leaf [Hindi]. [Student: there should be part of the path relating these vertices] exactly this edge cannot be part of any simple path between any two vertices because [hindi] this edge cannot be part of any simple path and so even after I remove this edge and this vertex this there is a path between every pair of vertices. So this is still connected, this is connected and by removing an edge and a vertex I cannot create a cycle [Hindi]. I can apply my induction hypothesis on it so [Hindi] we have removed only one vertex. So this is a tree on k vertices and has k - 1 edges, this is by induction hypothesis [Hindi] and so we prove that [Hindi].


 
(Refer Slide Time:  35.40)

 

You have to use the fact, both the facts are critical that it is a connected graph and it does not have a cycle in it. Otherwise you will not be able to argue that it has k - 1 edges [Hindi]. That?s the proof for this, everyone follows this. Most text books would have this proof also, you can also go back and and look at one of the text. So if the number of edges is less than n - 1 in a graph then the graph cannot be connected at all. Why? This statement, if the number of edges is less than n - 1 then the graph is not connected proof by contradiction. 

Suppose if it is connected then so let?s follow this argument. So suppose it is connected, if it is connected then why is it not a tree? It is not a tree because it has a cycle. So let?s take lets remove an edge from the cycle [Hindi] I should have switched but okay so what what are we trying to argue? If number of edges is less than n - 1 then G is not connected. 
So this is another useful thing to remember that suppose I have a cycle, G is a graph. Suppose I have a graph in which there is a cycle [Hindi] if you have a cycle and if you remove any edge from the cycle you cannot make the graph disconnected by doing that.

So what is the argument that to prove this claim? If suppose I have a graph on less than n - 1 on less than n - 1 which is connected. Why it is not a tree? It is not a tree because there is a cycle in let me remove an edge from the cycle I only reduce the number of edges and it?s still connected. If there is another cycle let me still remove another edge so I will only get less than n - 1 edges and the graph will remain connected eventually I will get a tree after removing all of this. So I am contradicting the earlier claim which says that any tree has to have exactly n - 1 edges in it. 

It cannot have less than n - 1 edges so any graph which has less than n -1 edges cannot be connected [Hindi]. Is there something that is not clear? So couple of examples n =5, m=4 this is a tree on 5 vertices.
 
(Refer Slide Time:  40.46)

 

It has to have four edges, this is a graph on 5 vertices and 3 edges and it cannot be a tree, it cannot be a connected graph at all. Let me ask you a question suppose I have graph on n vertices and it has n - k edges n - k edges. How many connected components do you think it has? I have a graph on n vertices and n - k edges, how many connected components it has? k or more, k when there would be no cycle and if there were cycles then it could have more number of connected components, try to prove this. This is a very simple exercise. So a given a graph on n vertices and n - k edges how many connected components does it have? So more terms; a spanning tree is a sub graph which means you are given a graph so it is a sub graph of a graph and this sub graph has to be a tree and it should include all the vertices of the graph. 


















(Refer Slide Time:  42.13)

 

So spanning tree [Hindi] tree which means the sub graph has to be a tree and [hindi] it should include everything; include everything here means include all the vertices. So as you can see this sub graph includes all the 4 3 7 and 3 10 13 vertices that are there and it is a tree. There is no cycle here so this is the spanning tree of this graph, this is the graph and this is the spanning tree of this graph. G has to be connected if G is not connected then there is no notion of the spanning tree. If G is not connected then no sub graph of the graph of G cannot be a spanning tree [Hindi]. So this is a useful thing to have, quite often your network could be a just spanning tree. 

Suppose these are points I want to connect so these are cities, these are possible roads that I can build but I just want to put the minimum amount of effort, I want to build has few roads as possible so that all these cities are still connected so I could built a spanning tree but this does not provide you any fault tolerance what does that mean [Hindi] you cannot reach from some city to some other city now. As you can see if I cut of this link then these 4 vertices would be disconnected from the other 8 vertices [Hindi] these 6 vertices would be disconnected from the other 7. Spanning tree is a useful but they provide don?t provide much fault tolerance. 











(Refer Slide Time:  44.43)

 

Let?s talk about bridges. Koenigsberg, this is a city in Germany or Austria I don?t remember where. So pragal river okay I don?t remember where this is. This city has this nice thing, there is a river flowing through the city and there is an island in the river and there are bridges in this manner so A is this island and there is a bridge from here to here, here so there are 7 bridges in all. This black bar are the edges so question is can you start from here let?s say or any point. So can one across each bridge exactly once and return to the starting point. Why no, so suppose I start from here I can take this bridge go here [student: it will land up] and you can go on land up [Hindi] so on and see.

Let?s see whether we can solve this problem or not? Suppose this would have been useful if you were a postmen who had to visit the various brides and you did not want to retrace the steps. So this is also known as koenigsberg problem and Euler proved that this is not a problem and we will give a simple proof for that one. So we can model this thing as a graph, there is this island A so these are the going to be the vertices of my graph. This island A this is one piece of land and there is this part B because I can go from anywhere to here. This is one vertex, there is a vertex D and there is a vertex C which is this part. So I will have a graph with 4 vertices in it A B C D and then depending upon so since there is a bridge from B to A.
 
In fact there are two bridges from B to A so I will put two edges between B and A. similarly there are two bridges between A and C so I will put two edges between A and C. There is one bridge from A to D so I will put one edge between A and D, there is an bridge between D and B so I will put one edge between B and D and an edge between C and B so I will get. This is not a graph. Why is this not a graph? Because we did not define a notion of two edges between pair of vertices, we just talked about pair of vertices. The edges don?t form a set, they form a multi set so this is called multi graph. 


(Refer Slide Time:  48.43)

 

What is a multi-graph? In which they put the many edges between a pair of vertices is called a multi graph but this captures that problem in certain sets. So eulerian tour is a path that traverse every edge exactly once and returns to the first vertex and that?s exactly what we want to do. Because these are the bridges so we want to traverse each bridge exactly once and return to the starting vertex. 

Can you do that on this graph? So same problem can now be thought of here, can is start from A and come back to A and and visit each or traverse each exactly once. So the same question a same, can you draw this picture without lifting your pencil or redrawing an edge, you know coming back over a line twice. So [Hindi] Euler theorem says that you can do this if and only if every vertex has even degree [Hindi]. When you come to a vertex, you come by one edge and then you have to go by another edge and if you come again then you will need another edge to or fresh edge to go off by so every vertex has to have an even degree for this to work but here there are all vertices of odd degrees so clearly this cannot be done. 

Now let?s quickly do the uninteresting part, the abstract data type. The graph can be thought of as a container of positions. So you have the regular methods for any positional container like queues and stacks. We always had this methods called size and Is Empty and elements; elements would return all the vertices and the edges that?s in and you can have some methods like swap which can swap two positions replaceElement those kind of thing, these are methods associated with the regular positional container swap is the generic method for any positional container. When you are saying that provide two positions and swap the contents at those two positions that?s the swap method. So here I am not saying it specifically to the graph abstract data type, you will have to think of what it would mean. So you could decide what it means here for this particular data type but I am saying it is a generic methods. These are all generic methods for positional container and I am just saying in that context. So here I have methods which are specific to graphs so numVertices would be a method which returns number of vertices numEdges number of edges vertices would know be an enumeration of all the vertices. 

(Refer Slide Time:  51.05)

 

So it would be a method returns an iterator which will let you iterate through the various vertices of the graph, edges could be a method which returns all the edges. DirectedEdges would be a method if you had a directed graph, it would return all the enumerated all the directed edges in the graph. What does enumerator do and an iterator? It basically returns an object which has two methods associated with it, one method is next and the other method is whether there is anything left, has next whether there is a next method next element at all or not. 

So as you every time you call next it gives you a next object in the enumeration so when you are enumerating edges I call next once it will give me one edge, when I call next again it will give me another edge. What order this edge is come in that you typically do not know. It depends upon how you implemented the iterator. UndirectedEdges could similarly enumerate all the undirected edges incident Edges, if I specify a vertex it would enumerate all the edges incident at that vertex. This is for an undirected graph incident Edges; for a directed graph right there are two kinds of edges either there would be edges which start from this vertex or there would be vertex which end at this vertex. So it could have a notion of any incident edges which are edges entering a vertexV which are ending at vertex V and you could have an out incident edges which are edges which are starting from vertex V going out of vertex, opposite. so I specify an edge e, all of these are objects an edge is also an object and I specify one end point on the edge so this method gives me the other end point of that edge.




(Refer Slide Time:  53.24)

 

Degree gives me the degree of the vertex, inDegree so degree would be for an undirected graph, for a directed graph there would be the notion of in degree and an outdegree. Indegree would be n number of edges coming into the vertex outDegree would be the number of edges leaving the vertex. Similarly I could have adjacent vertices, adjacent vertices would be a method which will turns an iterator over all the vertices which are adjacent to this particular vertex. This would be for the undirected graph, for a directed graph you could similarly have a notion of inAdjacent and an outAdjacentVertices. Then you could have a method areAdjacent whether vertices two vertices v and w are adjacent or not. So this would be return a Boolean value; endVertices given an edge it will return the two end points of that edge. 


















(Refer Slide Time:  53.31)

 

Origin, for a directed edge e it would return where the edge is starting from, destination for a directed edge e it would return where the edge is ending. Given an edge e it will tell whether it is directed or not. This method would be useful when you have, what are called mixed graphs, mixed graphs some edges are directed and some are undirected. Can you give me a setting where it would be useful to have a mixed graph, what kind of a problem setting can you imagine there?

It would be natural to have a [student: roads] roads, traffic network once again where you have some roads are one ways. So you are bi directed edges, roads which are two way could be undirected edges and roads which are only one way could be directed edges. There such a methods could be useful because given an edge you can then determine whether it is a directed edge or an undirected edge. I will just take, I guess this is last slide yes it is. 

Make Undirected e, so you are given edge and you set it to be an undirected edge. You can have a method which reverses the direction so you can have tonnes and tonnes of update method also. You can have methods to create the graph, change remove an edge, remove a vertex do whatever you want. So set direction from, so you can set the direction of an edge suitably we just look through this these slides that I have given. So this is just a subset of method depending upon what application you have, you could design your own set of methods. 







(Refer Slide Time:  55.50)

 

So graph can be thought of as data type, is an abstract datatype on which you can have a bunch of methods which you can use to update and modify the data type. So with that we will end our discussion on graphs we will continue in next class however to see how to actually represent a graph what kind of data structures can you use to represent graphs. 



	
Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 25 
Data Structures for Graphs

Today?s class we are going to be talking about data structures for graphs. If you recall in the last class we discussed various things about graphs. Various terms actually, what undirected graphs are, what directed graphs are, what is a path in a graph, what is a cycle,
what are connected components so on and on. We are going to start using the terminology now. I am going to be discussing three different data structures for representing graphs.

(Refer Slide Time: 01:36)

 

One would be the Edge list data structure, second adjacency list data structure and third would be adjacency matrix data structure. We will see what these are and how they can be augmented, how they can be combined to give better performances, faster running times. 










(Refer Slide Time: 02:43)

 

The simplest data structure is what we call an Edge list data structure. Suppose this is my graph, it is a directed graph. This is just a graph of a various flights. These are airports the blue vertices and the red arcs are flight numbers from a airport to some other airport and suppose we want to represent this. One way is to have two lists, one of the vertices and one of the edges. We call each edge is a pair of vertices. In this case it will be an ordered pair of vertices so we could have two such lists. Let?s see what that corresponds to. That is we called the edge list data structure. The edge list data structure simply stores the vertices and the edges in two unsorted sequences. It?s very easy to implement and this is what it looks like.
 
(Refer Slide Time: 02:59)

 
These are lists of vertices that you had and these were the various edges. Each edge recall corresponds to a certain flight between two airports. This is a flight let?s say NW 35. It goes from airport Boston to JFK and so this particular node has references pointers to these corresponding vertices here. For each edge I will keep two pointers, two references to the vertices between which that edge goes. This is called the edge list data structures. It is very easy to implement so there are many operations which can be done very quickly. For instance suppose there was one operation which was given an edge, find its two end points. So that can be done very quickly. You are given the certain edge and you want to find the two end points of that edge that can be done. Or there was an operation called opposite, given an edge and a vertex, you wanted to find out what was the other end point and so on and on. But there is one operation which is very inefficient and that is finding the adjacent vertices of a given vertex. 

Suppose I give you a certain vertex. I give you vertex DFW and I say which are the vertices which are adjacent to this. How will I do thus? I will have to go through the list of edges and I have to find out all such edges. Suppose I wanted to find out vertices which are adjacent to DFW, I will have to look at this edge. This edge is not an end point of this, so I go to the next one. This is not an end point of this, I go to the next one. This is an end point of this so I will look at what the other end point is, that is LAX. So LAX becomes adjacent to DFW and so on and on. This is what I will have to do to find out the adjacent vertices of a given vertex.
 
(Refer Slide Time: 04:54)

 

If I look at the various operations, I can see what times they take. So let?s see. Size, isEmpty, replaceElement, swap these are all container operations. These are when I am giving you the position, so size for instance is constant time, I can keep track of the number of edges and vertices in the tool list. isEmpty is again constant time, if the size is zero than its empty. replaceElement, if I give you a particular position corresponding to an edge and I say put some other edge at that location, that just takes constant time. 
Similarly swap all of these take constant time. Number of vertices, number of edges takes constant time. What does the method vertices do? It enumerates, it is an iterator over all the vertices. Since I would have to run through all the vertices, that would take time proportional to the number of vertices. Similarly these are iterator?s over the edges, so they will take time proportional to the number of edges. Let?s look at some more interesting thing. Suppose I say insertVertex, I want to insert a vertex. Then how much time should that take? It should take constant or order n. It should take constant time because these are unsorted lists.
 
Similarly insertEdge, insertDirectedEdge all of these can be done in constant time. Let?s look at remove vertex. This last operation you will not able to see it very clearly perhaps because it is getting overlapped here but it is removevertex. So suppose I wanted to remove a vertex. How much time would it take? If I remove a vertex, I also remove the edges which are incident to that vertex clearly. Because otherwise where would the end points of the edges be referring to. I have to essentially get to that vertex and I also have to traverse to the list of edges. The list of edges is, number of edges is order m. I have to traverse through the entire list to find out whether which are the vertices which are adjacent, which are the edges which are incident to this vertex and also remove those edges. Which is why this operation is going to take order m time. [Student:] here when I say removeVertex, I am assuming that you are given the particular vertex you want to remove. Let?s say you are given the position in the list. [Hindi]. In this manner you can look at this slide more carefully and understand the times.
  
(Refer Slide Time: 07:10)

 

This is not the only way of representing a graph. There could be other ways. We are now going to look at what?s called the adjacency list data structure. This is your graph, here I am taking an example of an undirected graph but all these data structures, all the three data structures that I am going talking of today can be used to represent both directed and undirected graphs. 
If so in this undirected graph how do I represent it? I have an array of vertices, an array corresponding to vertices let?s say. This location corresponds to vertex a, this corresponds to the vertex b, this location corresponds to c d and e that?s how it should be. Now what are the vertices which are adjacent to a? They are b c and d, so I will have a link list starting from this location which will have elements b c and d in it. This corresponded to location vertex b. So which are the vertexes adjacent to b? a and e so this links contain only a and e in it. This was corresponding to d, vertices adjacent to d are a c e so that?s why we have a c e in this. These lists are also unsorted lists. So adjacency list of a vertex, so what we are keeping track of is the adjacency list of each vertex.  

How much space does this data structure require? I will have an array of size n and what will be the length of the link list at each location. The degree of that vertex. The total space required in this part is sum of the degrees which we argued is two times m. The total space required is N plus M, so it should be of the order of theta of N plus M. [Student: so having implementing this as a link list [Hindi] we could keep them in an array, that?s the next data structure. We will see what are the pros and cons for that. [Student: array implementation of the linked list] pardon [student: array implementation of the linked list] No, what he is saying is, we will come to what he is saying in the next slide, when I show you the adjacency matrix implementation. 

What?s the advantage of this? How is this better than the previous data structure? [Student: moving a vertex] Adjacent vertices, to find out what are the adjacent vertices of a given vertex that can be done much more quickly here. How much time does it take now? [Student: order] order degree [Student: order degree] If I want to list out all the adjacent vertices. If I give you two vertices and I ask you are these two vertices adjacent? How much time would you take? [Student: order degree] order degree still. Degree of one of the vertices let?s say, the smaller one. [Student: the smaller one] [Student: the larger one] this is an array. This one is an array. This is an array of just references of pointers, nothing else. There is nothing else stored in this array. You can store more information if you want, any information associated with the vertex you can store at this storage location in the array. [Student:]It?s just an array, you can. [Student:] it?s an array typically indexed by? Suppose so you typically number your vertices 1 2 3 4 and so on and then that would correspond to this location.  













(Refer Slide Time: 10:59)

 

We can combine this with the edge list data structure and we will get something more complicated like this. What is this? [Hindi] is just your edge list. Just I showed you and now with each of these vertices, I have the adjacency list associated with each of these vertices. I have both the in adjacency list and the out adjacency list. From each of these elements, there are two pointers. One is pointing to a list of incoming edges and the other is pointing to a list of outgoing edges. We have combined this and that. The adjacency list and the edge list data structure, somehow we have combined them. Let?s see in what regard is this better than this? [Student:] here for instance the operation, suppose I said, given a particular edge. Here we actually have no mechanism of storing edges really. We are not really storing edge information. We are storing information only with regard to vertices, given a vertex what are the adjacent vertices. [Student: so edge information regarding the edges easily obtained what information regarding the edges do we want that we what].  

Suppose I had the same picture as before. I have this graph. I ask you flight UA 120 which airport from, what is the starting airport, what is the ending? Yes, so we have information associated with edges and that is somehow not getting represented in this. [Student: we can store name of] pardon [student: we can store] you can store there is no harm. You can store, so with this whichever was that airport you could do that, but now if you have to retrieve that information. Suppose you have to answer that question, given a particular flight number, what are the starting and the ending airports. You will have to go through this entire data structure to be able to figure that out. While here you could get that information very quickly. [Student: there is also an]. That?s not connected to the top. [Student: as good as having a double arrow for an edge list so this thing is as good as having a double arrow for the edge list] double arrow, what do you mean by double arrow? [Student: doubly length like in the above portion of this slide an arrow points from an edge to its vertex] vertices and from here it?s pointing from the vertex to the edges exactly. Let?s look at what are the time requirements for this particular data structure let?s see. 

Suppose I want to find out the incident edges. Edges incident to a certain vertex so I can get to that vertex, so given a particular vertex I can find both the in edges and the out edges in time proportional to the degree, the in degree and the out degree respectively. So that?s the incident edges. Given two vertices, are those two vertices adjacent? How much time do I need for this? So given one vertex I just need to run through the? it?s given a particular vertex DFW and let?s say MIA. Are this two adjacent? Is there a flight from DFW to MIA, what will I do? I will go in the out list of this and see. No, I will have to do more, I will come to this. I will come to the out list and then I have to go from here to this list here. This is just numbers or whatever, this would be referenced to this information. 
It depends upon how you organize it. This could be organized as out list of edges or it could be organized as out adjacent vertices. If it were organized as out adjacent vertices it would have been easy. But if like here, I am organizing it as edge list, so you will have to now go to the corresponding edge and see what the other end point of that edge is. All of that is constant time.

There are many ways of organizing a graph. I am just giving you a very high level idea and then for your particular application, depending upon what operation you are doing more often, you will have to choose the appropriate organization. [Student: organize the data it will be in minimum of degree of u and common degree of worst case will be maximum] Worst case would be maximum but suppose I also kept degree information associated with each vertices. That?s not very hard to do. Just one integer variable which will keep track of them, then I can make this. It depends upon where you want to optimize. If you are doing this kind of operation very often then it makes sense keep degree and try and reduce running time. If you are not doing this operation then there is no reason why you should keep track of the degrees of the vertices.
  
Third representation is what?s called the adjacency matrix representation and this is very simple representation. Here you just have an n cross n matrix and there would be basically just binary entries bits 1 0, there is a one which is a true, if there is an edge between those two vertices. There is a one here because there is an edge between a and b. What can you say about this matrix? What property does it have? [Student: symmetric] it?s symmetric. If it is an undirected graph it would be symmetric. If its directed graph it need not be symmetric. If in a directed graph you can have it that this would be one, if there is an edge from b to a or you can have it the other way round depending upon what, you can keep any way you like. M [i, j] is true that means there is an edge i, j in the graph. M [i, j] false means there is no edge in the graph and the space requirement is N square. It?s again a very simple implementation, it?s also quite efficient in a certain sense or let?s see.
 




(Refer Slide Time: 17:37)

 

[Student: adding a vertex means] Adding a vertex would mean creating a new row and a new column. It is order n time, order n so that will be order n time. I could have for instance? Again there is a possibility of, instead of having 1?s and 0?s, you could keep track of the edge information here. You could keep? The adjacency matrix structure augments the edge list structure with a matrix, so you could also have the edge list together with the adjacency matrix both of them together. In the edge list recall that for each edge you would have information about what are the two end points. 

(Refer Slide Time: 18:21)

 

There could be referring to the corresponding locations here, instead of pointers they could just be integers not they are telling which row they corresponds to and here instead of 1?s and 0?s, for the ones you could also have the corresponding edge. Augmenting an array, again so this is an operation which is not done very often. Quite often the graphs that you work with are static graphs. That is you don?t add new vertices in the graph, you don?t add, remove new vertices from the graph. If you want to have a data structure which implements that then perhaps this is not the right data structure. [Student: array implementation] you could do that, of course all of those thing could be done. I am not saying it cannot be done here at all but then this would perhaps not be the best data structure to use. 

If this was a frequent operation you were doing, adding vertices. [Student: even that will take order n time only because just when 2n minus 1 basically n square] Let?s look at what are the times required for the various operations here? Given two vertices to determine if they are adjacent or not, it is just a constant time operation. But now given a particular vertex to find out all the vertices which are adjacent to it, how much time does it take? [Student:] Row or column which is order n. It is not order degree now, it is order n this is the difference. 

(Refer Slide Time: 20:21)

 

Why because now you will have to take that particular row or column and look at all the entries and see which are ones and which are zeros. That will give you order n. So incident edges, inIncidentEdges, outIncidentEdges all of them will take order n. insertVertex, remove a vertex I have put down order n squared here because I am assuming that you have to copy them into a new array. It?s not very easy to take a two dimensional array and extend it by one row and one column. You understand why? Because the problem is that two-dimensional arrays are stored as one dimensional arrays. You know after all in a particular row major or column major form, 
If you want to extend it now how does that happen? Because you have to then move all the information. That?s why extending a two dimensional array is not an easy task. You essentially have to copy all the information into a new array. That?s all I will have to say about data structures for representing graphs. There are three different things you have seen adjacency list, adjacency matrix and the simple edge list. You can depending upon what operations are critical, which are the ones that you are doing more often combine them in a suitable manner. If space is not such an issue then you can keep the adjacency matrix data structure because it?s quite simple but it requires a lot of space. It requires n squared space. The standard implementation which is preferred very often is the adjacency list. If you can?t think of anything then just use the adjacency list data structure to implement a graph. Now I am going to go onto graph searching algorithms. This is something that you have done in a certain sense which is why I am going to be taking it up right in this class.
 
(Refer Slide Time: 22:27)

 

What is the graph search algorithm? It is basically a mechanism of visiting all the vertices of the graph in some systematic manner. By systematic I mean, you know in some organized manner so that you don?t miss out on any vertex. A graph could be either a directed or an undirected graphs and we are going to be assuming adjacency list algorithm implementation of the graph for the algorithm that we would be discussing. Graph searching algorithms are the most common algorithm that you typically perform on graphs and it appears on a whole lot of settings.
 






(Refer Slide Time: 23:03)

 

The algorithm that I am going to be discussing now is what is called the breadth first search algorithm or BFS for short. What does BFS do? It will visit all the vertices of a connected component in a graph and it will define for us what we will call a breadth first search tree which will be a spanning tree on this particular connected component. We are going to be discussing breadth first search on undirected graph only today. Breadth first search makes more sense in undirected graphs and the idea is roughly the following. You start from a vertex and this starting vertex let?s call it as s, it is assigned a initial distance of zero. 

We are going to proceed in rounds. In the first round you are going to, so think of yourself as in a maze, in some kind of a maze. You have a string with you, you are going to use this to help you search the maze. You have tide one end of the string at one location in the maze. Now you unroll the string by let?s say just one unit and you see where all can you reach by unrolling this string by just one unit. Those in some sense will be, what we will call the vertices at a distance of one from the starting vertex. After you visited all such vertices then you will unroll the string by one more unit and see which all vertices, new vertices you can visit as a consequence of that and so on. You will understand all of this when we start discussing the algorithm in more detail. We will do this, so we will unroll the string by one more unit find out all that we can visit now.

 







(Refer Slide Time: 24:59)

 

Unroll the string by one more unit, find all the vertices that can be visited now and so on and on. Each vertex we are going to give it a label which will be that when it was first visited what was the length of my string then? If it was visited in the first round then I am going to give it a label of one. If it was visited in the second round I am going to give it a label of two and so on and on. What this label will signify eventually would be the distance of the vertex from the root, from the starting vertex or the root as I call.
  
(Refer Slide Time: 25:34)

 

All of these will be clear with some example. Suppose this was my graph, very simple graph and s was my starting vertex. I give it a label of 0. I am going to have a Q, so this is the only data structure I need to implement in this algorithm a Q. Recall this is very similar to your minor question. You have a Q and on which you have s. At any point what you are going to do is look at the front element in the Q and look at all the neighbours of that front element. The neighbours of this front element are w and r, so you are going to put them into the Q now. I am going to remove an element from the queue, remove the front element from the queue, find its neighbours and put them into the queue, insert them into the queue. When I insert a vertex into the queue I color it gray, after I remove a vertex from the queue I color it black. 

Initially this is the only vertex in the queue so it?s grey. All the vertices which are in the queue will always have a color of grey. In some sense the grey vertices are vertices which have been discovered till now but I have not gone beyond that, grey signifies that. Black means that I have also gone beyond those vertices and white means undiscovered, I have not reached those vertices at all. This is the order in which the thing is. This is the first picture, the second, the third and the fourth. 

Let?s understand this, I had s in the Q, I removed s, colored the vertex black, took its two neighbors and put them into the queue. I assigned them a label of one node then the label of s, so the label of s was 0 so I gave them both of them a label of a 1. Now let?s see what the procedure here would be. I remove the front element of the queue which is w. I will color it black, I will look at its neighbors. How many neighbors does w have? 3 neighbors s, t and x. Amongst these s already is black so I don?t touch it at all. t and x are white so I will put them into the queue and color them grey. From white I color grey, grey color gets colored to black. 

When an element gets inserted in the queue, its gets colored grey. When it gets knocked off from the queue, it gets colored black, as simple as that. t and x get to put into the queue. What label do they get? They get a label of one more than the label of w. Why one more than the label of w? It was because of w, the t and x came into the queue. When I had knocked off w and then looked at its adjacent vertices, I have found t and x so they get label of one more than that. So they get a label of two. 

This is the Q at this point. Now I look at vertex r which is the front of the queue. What will I do? First color it black, look at its adjacent vertices which are white and put them into the queue. Color it black look at its adjacent vertices which are white, this is the only vertex which is white, put that into the queue with the label equal to one more than the label of r. That?s what this is and this get a grey color. Once again you see all the vertices which are grey are sitting in the queue. At any point this is the invariant you have. If a vertex is grey, it is in the queue. If a vertex has not yet been visited it is white, if a vertex has been visited and removed from the queue its black. Everyone understands this. Next vertex we are going to touch is t. We are going to remove t from here, going to look at its adjacent vertices. How many adjacent vertices it has? 3 but the only vertex which is white is u. It is only u which will get entered into the queue and nothing else and u will get colored grey. What will be its label? [Student: three] 3. u will get colored grey, its label is 3 and its get added to the queue. 

Now I knock x out of the queue, it gets colored black. I look at its neighbours which are white. This is the only one which is white, this gets a label of a three, it gets colored grey and it?s added to the queue. So y is colored grey, gets a label three and is added to the queue. This is what the queue looks now like. Now I remove the front element that?s two, look at its right neighbours, it has no white neighbour nothing needs to be done. It gets colored black and we remove it from the queue, so the queue is now u and y only. So I remove u from the queue, I color it black, look at its white neighbours, it does not have any white neighbour so nothing to be done. Then I look finally at y, y is at the front of the queue. I look at its white neighbours, no white neighbour nothing needs to be done. This gets removed from the queue and the queue becomes empty. The procedure stops when the queue become empty, these are the labels on the vertices. 

(Refer Slide Time: 31:29)

 

Now what do these labels signify? One signifies that it was discussed in the first round, two that it was discussed in the second and three that it was discovered in the third round. 
This one is also the length of the shortest path from s. [Hindi] if I look at this vertex u, there are many paths from s to u. I am interested in the path which has the least number of edges on it, the smallest number of edges on it and the path with the smallest number of edges is this path with three edges on it and so this is label three. We will see why this is getting done in this manner shortly.








 
(Refer Slide Time: 33:41)

 

So one more way of thinking of this so that you understand this completely, I started from this. In the first round I am visiting the adjacent vertices of this. These are the vertices which are getting a label of one. These are also called level one vertices, these are the vertices which are getting a label of a one. In the next step, all though I am going one vertex at a time but now the vertices which are going to get a label of two will be vertices which are adjacent to these. Which are these vertices? These are I and c, so these are the two vertices which will get a label of a two now. The vertices which get a label of a three are the ones which are adjacent to the vertices which are at two and they are basically m, j, g and d. This is getting a label of a three. 

(Refer Slide Time: 34:02)

 
The vertices which are getting the label of a four would be the one which are adjacent to the vertices which are at a label three, which are these vertices. So these get a label four and these would finally get a label of a five. You can think of a breadth first search as dividing your vertices or partitioning your set of vertices into levels or sets. There is one vertex at level zero, some vertices at level one, some vertices at level two, some vertices at level three and so on. What will be the number of levels going to be? The number of levels would be the maximum distance of any vertex from s. 

(Refer Slide Time: 34:56)

 

Now this is what the algorithm is, so let?s run through the algorithm and then we will look at the other aspects of the algorithm. Initially every vertex is given a color of a white. So du is the label on that vertex, on a vertex u du is the label so it?s initially infinite which means I have not put any labels on it and pi u, I will come to what pi u is. Pi u signifies the predecessor vertex. The vertex because of which you got its label. What I mean by this is so for instance let?s look at this vertex c here. This got a label of 2 which was the vertex because of which it got the label 2 b, so pi of c would be b. What is the pi of k? You can tell me, this vertex got its label from either this or this. I do not know which, it could be any. We will just pick one of them arbitrarily. This is the initializing all vertices and then how do we begin? 

We color the vertex s which is our starting vertex grey, we give it a label of a 0. Its pi of u is null because it doesn?t get its label from anyone else but from itself and we add it to our Q. We insert it into our queue and this is the entire process. This green should have extended all the way here. What we are doing is while the Q is not empty, we keep repeating something. Let?s say we remove the element from the head of the queue, so u is the element from the head. We are not removing it yet, so u is the element at the head of the queue. We are looking at all the adjacent vertices of u. For all v which are adjacent to u, if the color of v is white only then we do process it. If it is already grey or black, we don?t do anything with it. 
If the color of v is white then what do we do? We add it to the queue, we color it grey and we give it a suitable label. What is the label we give it? d of u plus 1. Whatever was the label of u, we add one to that and we give back to this. Since this vertex v is getting its label from u, pi of v becomes u. We add the vertex v, such vertex into the queue and once you have done it for all the vertices, we do this dequeue operation which is we remove u from the queue. This could have been done here also, it could have been done at the end, it could also have been done here that?s okay and u is colored black to signify that it has been removed from the queue. We keep doing this till the queue has an element in it. 

Does everyone understand what I am saying? [Student: so for initializing also we have to do some operations like breadth first search] What do you mean for initializing? [Student: making the color of every node to be white] Making the color of every node to be a white, what can we do? What is color? Color is something like an array. For every vertex so this is an array of size? [Student: values into it is less than the value we would assign to it] but we can also assign it, you know it is we are just creating an array, so let?s say white is zero. So just assign, put zero to all the entries in the array. We just giving a color to? each of these color d and pi will have to be separate arrays indexed by the vertices, we are just assigning that. 

(Refer Slide Time: 39:09)

 

How much time does the breadth first search procedure take? What are we doing? Basically all the time is being spent in this loop. Yes, because this is just, how much time do I spend here? One for each order n for each vertex, I am spending constant amount of time. How much time do I spent in this part? Constant time and here, all the time is getting spent in this loop. How many time is this loop executed? Order n times. How many times is this part of the loop executed? This is two loops, one within the other. For each vertex v adjacent to u, I am doing. So we are doing as much as the degree times. If I look at these statements they are being, what is the total time I am spending on these statements? Order degree for each vertex and summed over all the degrees of the vertices which is order m, twice the number of edges. Let?s look at each statement and see what is the total time, what is the maximum time that could be spent on each statement. How many times the statement is executed, how many times the statement 10 executed? [Student: order n] Order n. How many times is statement 12 executed? [Student: order m] Order m because 12 is executed degree many times for each of the vertices. So order m, so 12 is executed order m times in the worst case. Similarly if 12 is executed order m times so 13, 14, 15 and 16 could also be executed no more than order m times. 

Actually you can say something about 16, how many times is 16 executed? [Student: order] order n and not order m because you Enqueue a vertex only once. Once you Enqueue it, it becomes grey, once it get removed from the queue it becomes black. You don?t ever touch it again. Once it becomes black you don?t ever put it back into the queue. You only put a white vertex into the queue. In fact this statement 16 here is executed order n times and so this is also executed order n times and this is also executed order n times. It is only this if statement which is really executed order m times. Yes, you understand why if this is executed order n times, this is also executed? Because they are one after the other with no condition in between them. In any case the total time spent on the entire thing is order m plus n. 

(Refer Slide Time: 42:18)

 

Let?s look at the couple of properties of BFS. BFS what it is doing is it starts from a certain vertex, a source vertex s and it is visiting all the vertices which can be reached from s. It will visit all such vertices which can be reached from s. What do I mean by that? All such vertices to which there is a path from s, all those vertices will get visited which means that all those vertices which are in the connected component of s, connected component containing s will get visited. 


If the graph was in more than one connected component, if the graph had more than one connected component then if s is in a certain component, I will only visit those vertices. The vertices in the other connected component I will never be able to reach them at all. The first thing to keep in mind is that it will discover all the vertices which are reachable from a source vertex. If a vertex v is at level I then there is a path between s and v with I edges on it. I have not told you what a BFS tree is? So let?s first understand what a BFS tree is. So my slide order is a bit wrong here. What is a BFS tree that we have generated as a consequence? So recall that for each vertex, I have kept track of one edge which gave that vertex its label. 

(Refer Slide Time: 44:16)

 

So let me consider the following graph of the graph G. What is the sub graph? The set of vertices or all the vertices which are reachable from s, all those vertices which have a pi of v which is not null and s was given a pi of v which was null. So pi is also known as the predecessor. Each of the vertices which have a predecessor is the set of vertices Vpi. Every vertex is given a predecessor, every vertex which was visited given a predecessor. It is basically the set of all vertices which are visited. What are the edges in the sub graph? The edges in the sub graph are the edges from the predecessor vertex to this particular vertex for every vertex. Let me illustrate this. Let?s look at this picture here. This picture if I ignore the dotted edges, if I just keep the dark edges with me, the solid edges they are my? this is the sub graph that I am talking about. Note that each of these vertices has a predecessor except for this starting vertex, this has no predecessor.







(Refer Slide Time: 45:41)

 
 
What is predecessor of this? It was this, the predecessor of this was this and the predecessor of this was this. So these are 3 edges that I am including in my sub graph. The predecessor of this is this, the predecessor of this is this one, this was level 2. At level 3 when I had vertices, this has the predecessor as this, this had a predecessor let?s say this. This has its predecessor this, N has I as its predecessor, M has I as its predecessor. (Refer Slide Time: 46:00) Is this clear to everyone? Because this vertex was discovered because of I. When I took I out from my queue and looked at its adjacent vertices which were not yet visited which are colored white then I found M N and j, so M N and j have as their predecessor vertex I and D and G have as their predecessor vertex c, k could have G or J as its predecessor. 

Let?s say we decided G as its predecessor and H as D as its predecessor and L has G as its predecessor. These are the predecessors and then finally P has L and O has K as its predecessor. [Student: G and J are at the same level] G and J are at the same level, yes. Why is not? [Student: not necessary] It is. They both, why are they at the same level? Because they both get the same label, same level number [student: G is not necessary] they would, it is necessary because they are both adjacent to vertices which have label two, G is adjacent to a vertex which is? [Student: is but I am saying for deciding for an element K which has two predecessors] yeah [Student: you will see which which predecessor has the shortest label] No, both will have the same label then. [Student: then you will not assign] both will have the same label. 

If there was such a, you cannot have a vertex which has two predecessors at different levels. [Student: but this we only call ordered at which order can be different that we can] 
This is the predecessor information and these solid lines now form a spanning tree. Why do they form a spanning tree? How many solid lines are here? How many solid lines do I have? [Student: n - 1] n - 1, yes. Why n - 1 and not n? 
There is no edge entering A because A did not have any predecessor. Every other vertex has a predecessor, for every other edge there is one solid line, exactly one. So exactly n minus 1 edges. Using these solid lines I can go from A to any other vertex. Yes, if there is a solid line entering here that means that I can come to this vertex from that vertex and there is a solid line entering here, so I can come to this vertex, from there is a predecessor of that and there is a solid line entering here, so I can come to this vertex from some other and so on. So eventually I hit the root. Basically starting from s, I can get to every other vertex. This is a tree, it is a connected graph. These solid lines form a connected graph with exactly n minus 1 edges. It has to be a spanning tree. Recall that we said that if a connected graph has no cycles in it then it has n minus 1 edges. If a connected graph has n minus 1 edges then it has no cycles in it and it is a tree. This is what we have. This is called the breadth first search tree, the BFS tree. 

This spanning tree that we have which is the solid lines is the breadth first search tree. I think in both of the previous examples I had this. This is the breadth first search tree here, the blue lines they form the breadth first search tree. Once again for each vertex, I have just darkened the line. I have not drawn the arrows here but I have just darkened the line because of which was corresponded to the predecessor, because of which this vertex got its label. This vertex got its label 2 because of this vertex this got its label 2 because of this vertex and so I have darkened these lines and this forms your spanning tree so this is the breadth first search tree.

Let?s quickly go on. This is the breadth first search tree. I will switch over to this screen to show you something. I did not. So what has really happened is that we started from this vertex s, these were my vertices at level 1, these were my vertices at level 2 and so on. Let?s say the largest level was 7. Now this will answer some of your questions also. When I was at s, all these vertices which are here at level 1 are adjacent to vertices in s. That?s why they are at level 1. The vertices in level 2 are all adjacent to vertices of level 1. That?s why they got the level 2, number label 2. They got a label 2 because they were adjacent to some vertex, may be more than one but they were adjacent to some vertex in level 1. 

Could these vertices have been adjacent to s? They would have been in level 1 because when I looked at all the adjacent vertices of s, I would have discovered this vertex and put it level 1 instead. So such an edge cannot appear. This is the nice thing about structure that you get. I am not showing the tree edges here, I am just showing all the edges of the graph now. All the edges of the graph just go between adjacent levels. They cannot skip a level. I cannot have an edge which skips a level, it cannot go like this. This cannot happen. Why? Because when I was this vertex, would then have been in this level instead. I made a small mistake. I said all the edges go between adjacent levels. They could also go within the same level, yes. Why could they? I could easily have this. This vertex was adjacent, these two vertices were adjacent to s but they were also adjacent to each other, no harm even later. This is what the graph looks like now and this is the important property of breadth first search that you have to keep in mind. Certain edges we would call them tree edges, so this is my BFS tree, this edge. So my BFS tree would look like this now. 
Let?s say these edges and then from each one of them, I have let?s say something like this. I am basically covering all these edges. For each of these vertices is getting its level number because of certain vertex at the previous level and it?s this edge having through did in my BFS tree. This is what my BFS tree [student: from the edges that through from elements of the each other if we have two elements of the same level in the] [Student: we we wont count those] no, if I have an edge between two vertices of the same level, such an edge, this edge was not part of BFS tree. Why? What are the edges which are in the BFS tree? [Student: solid] the predecessor. This vertex did not get its level number because of this vertex. It got its level number because of this vertex. It is this edge which would be part of the BFS tree and not this. 

(Refer Slide Time: 54:24)

 

Let me [student: multiple edges leading to a node we take only one] we take only one. We have that, so just to show you all those things I just said to you, I had organized it like levels but it is the same thing happening here. These are the levels zeroth level, level 1, level 2, level 3, level 4, level 5. As you can see all edges are going either between adjacent levels or within the same level. This vertex could have got its level number from either this vertex or it?s this vertex, so I picked one arbitrarily and this I included in my BFS tree. The BFS tree is not necessarily unique but the level number of each vertex will be unique. Why would it be unique? The level number of a vertex would be the length of the shortest path from s to that vertex. Why shortest path? We have not proved yet. 

If there is a path from A to a certain vertex of length 6 then this certain vertex lets say whatever z, will get a label of at most 6. If the shortest path was of length 4 then this vertex cannot get a label of more than a 4 because on that path. If there is a path of length 4, what does that mean? There is s, there is a first vertex, the second vertex, the third vertex and then this vertex z. 

Then the first vertex would get a label level which means that the first vertex is adjacent from s. It will get a level number of 1, the next vertex will get a level number of 2, the third vertex will get a level number of 3 and this vertex will get a level number of 4. That is why each vertex gets a level number equal to the length of its shortest path from s and that is unique. We said in choosing the predecessor edges, you could choose any one but the level numbers would be unique for each vertex because it corresponds to the length of the shortest path from the route. With that we are going to end today?s discussion on breadth first search. We are going to be using breadth first search for finding the connected components in a graph and we will see that in the next class. 
Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 26 
Two application of Breadth First Search
-Connected components
-Bipartite Graphs

Today we are going to be looking at two applications of breadth first search. In particular I am going to look at applying breadth first search to finding out the connected components in a graph and a second application will be to checking if a graph is bipartite.
 
(Refer Slide Time: 01:20) 

		 

I define what bipartite means and then we will see how to apply breadth first search. Recall that a graph can be in many pieces, so I could have a graph which looks like that. How many connected components does this graph have? [Student: five] five [Hindi] connected components. What we want to do is have a procedure which will label these vertices. It will label every vertex here 1, say that this is in the first connected component. I have just arbitrarily called this as the first connected component. It will label every vertex here 2, it will label every vertex here 3, it will label this vertex 4 and it will label this vertex 5. 

I want a procedure which returns this labeling of the vertices. I will have an array called label let?s say and for each vertex, so this is the index corresponding to vertex v. At this location I should have 1, 2, 3, 4, 5, 6, 7 whatever is a number of connected components that number should appear here. So that by looking at this array, given any two vertices I can just in constant time determine if they are in the same connected component only. If they have the same label then that means that they are in the same connected component, if they have different labels they are in different connected components. Question is how will I do such a thing. This is very useful procedure. First it is also counting the number of connected components your graph has. How will I do this, is the question that we are going to? [Student: do the, for each vertex and if the vertex are maintained one which counts vertexes which already have been traversed].
  
The standard things in any case in the first slide they said it is an application of breadth first search. The first thing you are going to do is to do some breadth first search somewhere [Hindi]. There is a notion of a starting vertex that we take one starting vertex and then we start doing our breadth first search from there. Suppose I take this as my starting vertex and starting doing the breadth first search, so what?s going to happen? I am going to give each vertex a? so there also I was giving each vertex a label, a distance. Let?s call that a distance label and let?s call this component number.

There what I am going to do is all the vertices that I visit, if I do a breadth first search starting from this vertex, which all vertices am I going to visit? Only the vertices in this connected component. Is it clear to everyone? In the very first step, in the first round what I am going to do is I am going to look at all these three adjacent vertices and put them into the queue. These will get visited then when I take this out of the queue then I will put this into the queue and so on. I will end up putting all of these vertices into the queue, removing them and so all of these vertices will get visited. But I would not visit any of these vertices. I would visit only these vertices. As I am visiting these vertices, I can keep giving, assigning them a component number equal to one. I keep assigning whatever these vertices are let?s say these vertices are some a, b, c, d, e and f and a appears here, b appears here and c appears here and d appears here and e appears here and f is here. I give each of these a connected component number of a one.

Now what should I do? [Student: implement the] initially what are the values that I give to each of these connected component number? Let?s say initially everything is zero. Now after I finish this one BFS, some vertices have a connected component number of one and some others have connected component number of zero. Which are the ones which have zero? All of these. And which have a one? These. Now what should I do next? [Student: pick any of the] pick any vertex with zero, easier set then done. How do I pick any vertex with zero? I just start from here and find the first place, first vertex which is a zero. Then what do I do? Then I, let?s say this was the first vertex which was at a zero, very first. 

This is let?s say this vertex here, so I start a BFS from here. [Hindi] there might be a two here, this would get label two, there might be a two here, this might also be two, this might also be two something like that. Now find the next zero. How do I find the next zero? Start from, so this is where the small thought is required. I should not start again from the end, start from where I found the last root vertex and continue from there, continue from the next location. Then once again when I find a zero, I start a breadth first search from there. Once again that will end up labeling certain vertices. [Hindi]

Once again I find the next zero and so on and on. This will at the end do what we wanted it do to. Question is how much time does it take? How much time did this breadth first search take? Number of edges plus the number of vertices. Of course number of edges is more than the number of vertices. Why? It?s at least n minus 1. If a graph is connected then there is a spanning tree in the graph. If there is a spanning tree, the spanning tree itself has n minus 1 edges in it. [Hindi] Is the total turning time equal to order number of edges? [Student: order number is total vertices] Is this the right number? [Student yes] yes [Hindi] number of edges can be much smaller than the number of vertices in this entire graph. 

The graph could have zero edges [student: suppose] so we have not counted the time required for traversing this array to find the next vertex from which to start our breadth first search. How much time does it take to traverse this array? Order n exactly, order n critically because we are not going back from the starting when searching. [Hindi] If we start always from the beginning then it will be order n times the number of components [student: only once] [Hindi] we have just made one scan of this array that?s critical. The time required for that is order n. 

(Refer Slide Time: 07:30)

 

The right time bound should be order m plus n. You understand why am saying m plus n here and not just m? It is not the case that m is more than n, it is not either the case that n is more than m. I can?t say that because this is not a connected graph over which I am doing the breadth first search. So this would be the right thing to say or if want to say maximum of m, n that is also okay. But just saying order m or order n is not correct. [Student: when you know a you started breadth first search then how do you label a without label b] No, I started my breadth first search from here. When I started my breadth first search from here let?s say you are saying I want to start a breadth first search from here suppose I started my breadth first search from here. Then I looked at its adjacent vertices. These b, e and f were its adjacent vertices. These vertices I am going to put them into the queue, at the time I put them into the queue I am also going to go to the location corresponding to the these vertices in this array and mark them one. [Student: sir but without like you are saying we go through the array work] 

We go through the array only once for selecting the root vertex but for each one of these vertices, when I am visiting each of these vertices in my breadth first search, at that point 
I am going to go that particular location in this array and label that vertex. [Student: you are not counting the time] How much is that time? Constant for each of these vertices. So again number of vertices is n so again I would take order n time for that. For each one of these vertices, in any case I am spending a constant amount of time because I am putting this vertex into the queue, I am removing them from the queue. I am actually going to give them a distance label also, all though that distance label is not really required but I am going to access this vertex. I am going to spend a constant amount of time. So that in some sense, in that constant amount of time also accessing this array and updating that information here. [Student: sir is the location where the next element like k where you start way b is it know directly]  

When I start from A, how do I know what b, e and f are? So think of b, e and f as numbers. When I look at the adjacency list representation of a, I have the adjacency list of a. I know what the 3 vertices adjacent to a are, they are let?s say vertices numbered b is 2, 5 and 6. These are vertices number 2, 5 and 6. I will go to locations two, five and six in this array and make them one. You can just have a direct correspondence as numbers, if that?s how you like to think of it. Clear to everyone? We can determine connected components in so much time, order m plus n, linear time. This is also called linear time.
 
(Refer Slide Time: 13:47)

 

We have used this term before. I can remind you such an algorithm is called linear time algorithm. The next application we are going to look at is for what is called bipartite graphs. 
First let me define for you what is a bipartite graph is. We are given a graph G, so recall a graph is given by a set of vertices and set of edges. We are talking of undirected graphs here. G is a bipartite graph if there exist a partition of V into U, W partition. [Hindi] What is partition mean? V equals U union W, U intersection W equals null. I can divide the set of vertices into two pieces, U and W such that every edge has one end point in U and the other in W. 

Let me draw an example. [Hindi] this is an example of a bipartite graph. Every edge has one end in the set U and the other end in the set W, the other end point. That means there is no edge like this or like this. These edges are not there. There are no edges both of whose end points are in U or both of whose end points are in W. [Hindi]

(Refer Slide Time: 16:27)

 

Such graphs actually model a lot of things. One standard setting is these are boys, these are girls and these reflect their, whether they like each other or not. No, we are not saying they are getting married this way. We just trying to find out if they can get married or these could be jobs and these could be applicants to jobs, yes and a certain applicant is suitable for certain subset of jobs and you want to know whether for each job there is an applicant or for every applicant there is a job some such thing. Such graphs find a lot of applications in these kinds of settings. 

Question, given a graph can you determine if it is a bipartite? Everyone understands what the question is. Given G is G bipartite, so suppose you said the answer is yes. Yes, G is bipartite. How will you convince me it is bipartite? [Student: no edge between the same lines levels with] He has a suggestion. What he says is do a breadth first search, after all that?s the topic of this class. Do a breadth first search from which vertex? Any vertex. Let?s do a breadth first search. I said starting from an arbitrary vertex, just to make sure so that you understand that there is nothing sacrosanct about the starting vertex [Hindi].
If you recall, breadth first search will divide the graph up into layers into levels. These will be the vertices at level 1, level 2, level 3, let?s say level 4 and there were 5 levels. There was one other interesting thing that we have talked about as regards breadth first search. All the edges of the graph, we are not talking of the breadth first search tree. We are talking of the original graph. In the original graph there will be edges which go between adjacent levels or within a level. There will be edges of this kind which go between the adjacent levels and there might be some edges which go within a level. There would be no edges which jump levels, there cannot be an edge which goes like that. [Hindi] You should not confuse it with anything else. Everyone follows this. These brown edges are not there. 

We have only the green and the red edges. When is this graph bipartite? Suppose these red edges were not there. Is this graph bipartite? Suppose there were no red edges. Which are the red edges? These are the red edges here. What do I mean by no red edges? There are no edges, suppose all edges go between adjacent levels. [Hindi] i.e. no edge has both end points in the same level. Suppose such was the case, what can you say? This implies the graph is bipartite. Does everyone see why? [Student: componental graph] [Hindi].  
I am assuming that let?s assume that it is a connected graph to simplify matters. Given G is G bipartite so let?s assume G is connected. [Hindi] (Refer Slide Time: 21:50). We will have to check it for each connected component. If each connected component is bipartite then the graph is bipartite. If some one connected components is non-bipartite then the graph is not bipartite. I will come to all of this in a second.
  
For now let?s assume we have given a connected graph and we have to determine if it is bipartite or not [Hindi] (Refer Slide Time: 22:14). Edges which go within the level. The claim is that the graph is bipartite now. Why? Because now you are going to take alternate levels on the u side [Hindi]. This is the bipartition. After all to show you that to convince you that the graph is bipartite what do I have to do? I have to show you a partition of a vertices such that every edge is going between one side, has its end point on the two sides of the partition. 

Now if I do my partition in this manner [Hindi] then every edge is going from a vertex on the u side to a vertex on the w side. Yes, if these red edges were not there. But if these red edges were there then we can?t say because then such an edge is going between a vertex on the w side and another vertex on the w side. If this were the case, all edges go between adjacent levels that is no edges has both end points within the same level then G is bipartite. But we have not solved the problem here. Suppose there is an edge which has both its end points in the same level. What then?
 







(Refer Slide Time: 24:09)

 

Suppose such an edge exits, does that mean that the graph is bipartite? You will not solve the problem. [Hindi] If in the BFS, we find an edge both of whose end points are in the same level, then what? What can we say then? Life cannot be too hard. Then what? Basically we have to say, the graph is not bipartite. Then G is not bipartite but why? [Student: you have partition between u and v then the vertex you started this BFS must be in some partition let?s say u then this would definitely contradict because the next level must be in v, the next level must be in u. Ultimately it will contradict that] Proof is correct. I will give you a different proof though because it will bring out another aspect, another property of the bipartite graphs.  

Suppose we had such an edge. We had an edge [Hindi] both of whose end points are in the same level [Hindi]. Now recall we had a notion of a breadth first search tree [Hindi]. What was the notion of a breadth first search tree? We said there is a tree and basically for each of these vertices, there is a predecessor. For each vertex there is a predecessor except for the starting vertex, the root vertex [Hindi]. Is this [Hindi] if such is the case then G has an odd cycle [Hindi]. Let?s prove that fact now. If G has an odd cycle then G cannot be bipartite. I have not proved this fact, I will prove it. But if I prove it then we are done. [Hindi].









 
(Refer Slide Time: 28:29)

 

So proof by contradiction. Suppose the graph is bipartite what will happen? Suppose this vertex either it?s on the u side or it?s on the w side? Suppose it?s on the u side then this vertex has to be on the w side. Then this has to be on u side, this has to be w, u, w, u, w u, w, u, w, u, u. [Hindi] I started with the u but this could easily have been a w and you would have to adjacent vertices which are both labeled w.
 
(Refer Slide Time: 29:45)

 



Everyone follows this proof. Let me show the previous slide once again. If the graph has an odd cycle then it cannot be bipartite and if we found an edge, both of those end points are in the same level then we have shown it has an odd cycle. This is the odd cycle in the graph [Hindi]. This is the general proof, this is not a proof by example because [Hindi] everyone follows this (Refer Slide Time: 30:20). In this manner you can use breadth first search to check if the graph is bipartite or not which is equivalent to checking, if the graph has an odd cycle or not. 

If the graph has an odd cycle then it cannot be bipartite. If the graph has no odd cycle then does it mean, it is bipartite? If all the cycles in the graph are even in length then does that mean that the graph is bipartite? [Hindi] My next question is if all cycles in a graph G are even. Is G bipartite? Yes or no? [Student: yes] Once again take your graph, do a breadth first search on the graph [Hindi] which is between the same level then there is an odd cycle. [Hindi] By taking alternate levels on one side, you have shown it to be bipartite. Yes? 

How much time did our procedure take? The procedure to check if the graph is bipartite. How much time did it take [Hindi] and then what did we do? We did also something else. We checked if there was an edge both of whose end points were in the same level [Hindi]. Let me write [Hindi]. We will look at all the adjacent vertices then when I come to this vertex, what are the various things we were doing? We looked at all of these and we put them into the queue and we gave them a certain color. What was the color we gave them? Grey, so each of these vertices were colored grey. Then I removed one of the vertices from the queue. Let?s say I removed this vertex from the queue and I looked at its neighbor?s. If any of the neighbor?s is grey then we can stop the procedure [Hindi] (Refer Slide Time: 34:18). If the neighbor is white, grey or black [Hindi]. 

I will repeat. All you need to do is to check if the other end point of the vertex is grey or not. Just grey, if it is grey stop, exit, reboot. The graph is not bipartite. And that?s the only thing we need to check. Why is that? The reason for that is if the other point is black then that does not mean that the graph is not bipartite because you could have such an edge. 
When I am looking at, so let me draw this picture again because this is getting cluttered. When I am looking at this vertex, then it has its neighbor?s one of which is this, which is already colored black. I cannot use black, the other end point being black has a test for non bipartiteness. Because this could be the entire graph for all I care. This is clearly bipartite. But to say that this vertex, when I am looking at this vertex it has a neighbor which is black, use that to say it is non-bipartite, would be wrong. 

Now what we are saying is that suppose there is an edge like this. When there is an edge like this, then when I am considering one of its end points for the first time, then the other end point; since it has not been considered yet, would be colored grey. Yes. Since it would be colored grey, we would be able to identify that edge as an edge which is running within the same level. [Hindi] They will also get into queue. [Student: (Refer Slide Time: 36:55)] here is a valid point. I made a mistake. Does everyone understand his question? No. let?s look at this picture for now. [Hindi] 

(Refer Slide Time: 36:39)

 

What is the point? Depends upon me where the point is. This is the thing that has been. So this should be black clearly. This should be black because it has been expanded out and then since it has been expanded out all of these guys are in the queue. They are all grey. What are the color of these guys? Because I have not touched them yet, they are in the queue. They are grey. Now who can tell me, what I am trying to do? If now suppose I look at this vertex next and this has an edge going here. The other end point of this edge is grey, so I might say it is not bipartite so that is not right. 

(Refer Slide Time: 39:24)

 


Simply level [Hindi]. We are keeping level numbers so that quantity is important. That level number business is important. We are keeping the level numbers of the vertices. If the level number is the same, say the graph is non-bipartite. If the number level is different, continue. Clear? One thing I wanted to do today which I think, I did not do very well in the last class was to argue that the level number of the vertex that your breadth first search procedure is giving you the shortest path. Some of you perhaps understood it. But I think I did not do it very well. 

Let?s do it once since we have some time today. Let me write down the formal statement. what I want to argue is that in a breadth first search, starting from vertex v, the level number of vertex u is the length of the shortest path from v to u. What are we saying? We are saying that I started a breadth first search from vertex v, I got a bunch of levels. Let?s say vertex u is sitting in this level 4; level 1, 2, 3, 4. Then the shortest path? what do you mean by shortest path? Let?s define what shortest path means. You all understand what a path is. So between v and u, there could be many paths in the graph. It could be very complicated. Each of these paths has a certain length. What is the length of the path? It?s just the number of edges on the path, let?s say.

For now we will just keep that as a definition. Later we will modify this definition. So for instance if I were to look at this path, this has 1, 2, 3, 4, 5, 6; 6 edges on it so it has a length of 6. If I had gone like this, it was even longer, it had length 7. If I had gone like this 1, 2, 3, 4, 5; this had length 5. But there is also a path of length 4, this is 1, 2, 3, 4. It is not an important here exactly which it is but you understand that there could be many different paths in a graph between two vertices. The shortest path is just the path which has the least number of edges on it. 

It is relevant clearly because if these are certain roads or some such thing, you would like to travel the minimum possible. Although I have not kept any distances here but let?s say these just reflect the number of hops that you are taking. Say suppose this is some computer network, you want to talk between v and u. Let?s say most of the time is spent in a, when you have to go through one node. This is the link between this computer and this computer, this is link between this and this is the link between this and this. So information travels very fast on link. But at a node, at a computer it has to be processed and forwarded and stuff like that. There is a lot of time wastage here. Clearly you would like to minimize. This is called the number of hops then. This path has 6 hops on it. So you would like to pick a path which has the smallest number of hops because then you are information travels very quick. You would like to find the shortest path. 

The claim is that if this vertex u is at level four then the shortest path from v to u is of length 4. First let?s show that there is a path of length 4. How do I show there is a path of length 4? Very simple. I start from u, I take its predecessor. Its predecessor would be some vertex there. Then I take its predecessor. Its predecessor would be some vertex at the previous level. You understand what predecessor means? Let me recall. Predecessor is the vertex which gave due to which this guy got its label. 

Clearly that is the vertex sitting here. The predecessor of this and I would get to the predecessor of this and clearly the predecessor of all of these vertices at the first level is the root vertex itself. What is the length of this path? It?s just the number of levels and its four. There is a path of length 4. Can there be a path of length 3? [Student:] If there is a path of length 3, then we will have to jump a level which violates the fact that this is a breadth first search. This is a partition it gives by breadth first search. [Hindi]

You understand? If there is a path of length three, then that path cannot visit all these. It has to come here, it has to start from here. But then it cannot visit all these 3 levels. It has to jump over one of these. But if it jumps a level then it?s a path, then it violates our breadth first search property. That shows that this process gives you also the shortest path and the shortest path is just the distance, the level number of each vertices. If I wanted to find the shortest path from a certain vertex, all I have to do is do a breadth first search and the level number would give me the length of the shortest path.
 
(Refer Slide Time: 45:33)

 

Now one last thing I want to do with you is again applications of breadth first search. It?s a very simple application using this fact that we just understood. I give you a graph and I define the diameter of a graph. What do you think the diameter of a graph should be? Maximum distance between two vertices along the shortest path between those two vertices. Maximum distance between 2 vertices in G, let?s just say that. What does distance between 2 vertices means? Distance between two vertices equals length of shortest path. We are not saying that the diameter is the length of the longest path between two vertices. That is a very hard quantity to compute. That?s impossible to compute, more or less. We are not talking of that. Diameter is very specifically defined maximum distance between two vertices and the distance between two vertices is the length of the shortest path. 

How am I going to find the diameter of the graph? [Hindi] What is the answer? BFS, just do a BFS and let?s say the largest level number is 3. The diameter of the graph is 3, that?s not true. We will have to do a BFS on every node and then find the maximum level. You understand why this is wrong? Does everyone understand why this is wrong? Because I could have a graph which looks exactly like this. Suppose this is my graph, so it?s exactly this. If I did a BFS from here, I would get exactly level 3. But the diameter of this graph is not 3. The diameter of this graph is 6. The diameter is 6 because it is the maximum that we are interrupting. So to get the diameter, I would be able to get to the diameter if I started my BFS either from this vertex or from this vertex or from this vertex or from this. You started from one of these, then the maximum level number would have given me the right information. But I did not know which these vertices are, so I will have to try it out for every vertex.  How much time would determining the diameter take? MN time. We are assuming that the graph is connected. 

(Refer Slide Time: 49:22)

 

So it will take, why because each of these breadth first search will take M time and we are doing N. For a disconnected graph it is not defined, let?s say it is infinity. For each connected component you can define the notion. But if I do a depth first search from one vertex and I look at the largest level number, it doesn?t tell me what the diameter is exactly. But it does tell me approximately what the diameter is. It can be at most half the diameter, the maximum level number is at least half the diameter. Yes. The diameter cannot be more than two times the maximum level number for any BFS. You understand? Diameter of the graph is going to be less than or equal to two times the maximum level number in any breadth first search. You understand why? [Hindi] 

The claim is diameter cannot be more than 10. No 2 vertices are more than 10 units of apart. Why? Take any 2 vertices. [Hindi] Since this is true for every pair of vertices, the diameter cannot be less than 10, cannot be more than 10. Diameter is less than 2 times the maximum level number and diameter is greater than? what is the diameter greater than? Maximum level. Diameter we know is at least 5. Why? Because there is this vertex here and one vertex here [Hindi]. So by doing one BFS, you can get an approximation to the diameter. [Hindi] You know the diameter is at least 6 and at most 12 [Hindi].

(Refer Slide Time: 52:46)

 

It?s clear to everyone? So with that we will end today?s discussion on breadth first search. In the next class we are going to look at depth first search. 
Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 27 
Depth First Search (DFS)

Today we are going to be talking about depth first search. This is another way of searching a graph. In the previous class we looked at breadth first search. There are certain applications where depth first search is more meaningful than breadth first search and we are going to look at these application also. One application is the following. You are a mouse, you all know who you are. So you are a mouse and there has to be whatever something, carrot. No, I thought you said carrot, it was cat. There is a piece of cheese at the end of the maze and you have to find your way to this cheese. 

(Refer Slide Time: 02.06)

 

This can be thought of as a graph in the following sense. At each of these squares I put down a vertex. We come to that in a minute. What will a mouse do? The mouse is not going to do a breadth first search unlikely that is going to do. It is not even going to do a depth first search but what it ends up doing is something like a depth first search. What this mouse is going to do as all other mice would is that it is going to go off in one direction. Try to explore that path, that direction fully and if it?s not able to get to the cheese, it is going to try and backtrack. We will understand what all of that means. 





Let?s assume that our mouse has photogenic memory. When it comes back to the certain place, it knows that it has been at this place and it knows what path it took when it was in that place last time. We of course have data structures to keep track of this information but let?s say mouse can also keep track of this. Let?s say it started from here and it went down one step and it did not find its cheese. So it came down another step not yet, so let?s say it decided to go right. Why? I don?t know, it just decided to go right. It is said to go right and then it moved another step and at this point, it can only go down. It went down and again it came down here and lets say whenever it has option it can always goes right. 
It goes right again. 

Let?s say that the mouse tries to take this direction first, east and then it tries to take this direction south and west and north. Now there is no doubt about left and right. It will try to go again east and did not find the cheese, so now it cannot go east, so it goes south and south and now it is stuck. It cannot go anywhere else because all the three sides are blocked but it can move, when it gets stuck like this it backtracks. Backtracks means goes back to the place where it came from. It goes back and at this point, it knows that it has been here before. It went south, so it now tries to go west but it cannot go west. So it has no other possibility but has to go back to where it came from and it came from here, so it goes back. 

Similarly here it goes back and it goes back. At this point when it reaches, it sees that it has another option which it has not explored which is going south. So it goes south now and then it comes here, it comes here. Now it will have no other option but to go west, west, north, north, north and now it comes to this point. It comes to this point but it sees that this is the point which it has been to before. It is already been to this place. It is not going to go to this point any more. If it has been at a particular cell before at a particular location before, it will not go there again because otherwise it will just keep going in a loop. It will not make any progress. 

Since it has been to this point before it will not go here. At this point it has no other option left, it cannot go east, it cannot go west. If it goes north, it goes to a place where it has been before. So it backtracks. At all of these points it will have to backtrack because there is nothing to be done and it comes back at this point. At this point it had explored the two options it had. The third option is not an option, it cannot go west so it will backtrack, backtrack, backtrack come here. 

Now here if it tries to go south, it sees well this is a place I have been to before. Why should I go there? It is after all just trying to find where the cheese is. If it has been to a place before there is no point going there again because it will then repeat the same sequence of steps. So it doesn?t go here. That?s one important thing that the mouse does and so it backtracks and backtrack and comes back to the starting position. What have we learnt? It is not so easy to find cheese. Now it will take the other option comes here, comes here, comes here, comes here, comes here, comes here. He said it will first try to go south, south, struck, backtrack, backtrack. At this point it has another option goes there, goes there, goes there, goes there and now it is a homerun. Time to clap, no.  
It has found the cheese. This is what we will call depth first search and we will formalize this shortly. But this is fairly natural way of exploring. It is not artificial, you don?t have to be a mouse to be doing this. You take a particular direction and you try to follow it till wherever it will reach till whatever you can, using that direction. So direction here now corresponds to in our graph sittings, it will correspond to taking one edge, going out along that edge to the next vertex and continuing and seeing where all you can reach. Then if you cannot reach any other vertex then you start backtracking. While there was a notion of goal here which was the cheese and you would stop here. In our depth first search there is no goal really. So our depth first search would just be a mechanism of exploring all the vertices of the graph. We will keep continuing our depth first search till we do not come back to the starting vertex and there are no other options left from the starting vertex so to say. 

(Refer Slide Time: 7:58)

 

Our search is basically a method of visiting all the vertices with the graph. Now let me do a depth first search on a graph and show you what you get. I will start including some terminology now. This is the graph, we are working with an undirected graph for now. The notion of a depth first search is applicable also for directed graphs. So small simple graph and six vertices and I am starting from this vertex. So as in the case of breadth first search there is always a notion of a starting vertex. I am starting from this vertex. What am I going to do? I am going to take one edge out of this vertex. In the case of the mouse we choose a particular ordering. We said first we will take the option going east then south, then west, then north. Here we can choose whichever we want and typically we are going to take? so recall we are working with the adjacency list implementation. 

For every node, so recall an adjacency list implementation, you have an array and for each vertex you have a linked list and this linked list is a list of adjacent vertices. The first edge I will consider is the edge going to the first vertex and when I have worked with this edge and I am explored everything, all the places I can reach with this end. 
I come back to this vertex then the next vertex I will consider is this one and so on and on. Suppose here this was the first edge I considered. I went along this edge to this vertex. I am now going to just so that you follow the procedure. I am going to do a time stamping. What does it mean? Nothing much. I start at time zero. When I reach a new vertex, I increment my time. I will say I reach this vertex at time 1, so 1, 2, 3 will tell you what is the sequence in which I visited the vertices. I reach this vertex at time one. Now I start from this vertex, I am going to look at the label options I have. I have three different options. 

Let me say I first took this option and come to this vertex. I came to this vertex at time two. At this vertex do I have any other option? There is this edge going out but when I go along this edge, I see I come to a vertex which I have already been to, vertex zero here. There is nothing to be done at this vertex. What do I do now? I backtrack, I leave this vertex. I am done with this vertex. When I am done with the vertex I am again going to increment my time. I came to this vertex at time two and I am going to say I am done with this vertex at time three. I will just increment our time counter. This is not particularly useful this time counter but there will be one application where we will use it. This is just right now to show you how things are progressing. I am done this with this vertex and then where do I go back? Backtrack which means go back to the vertex where we came from. So I come back to this vertex. Now can I backtrack out of this vertex? No, because there are no other adjacent vertices which I have not explored. 

Let?s say this is the next one I go to or let?s say this is the next one I go to. Why not? I take this one and I reach this vertex at time 4. No, our numbering is when we reach a vertex, we increment the counter and we backtrack from a vertex then we increment the counter. That?s the only way we number. I am not saying that I leave from here at time three, I get to here at time four then I could do that also but why keep incrementing unnecessarily. I am going to increment only when I reach a new vertex, so I reached a new vertex I give it a time stamp of a 4. Now at this vertex what are the other options I have? I can go along this edge or I can go along this edge. 

Let?s say I decided to go along this edge. I decided to go along this edge, I came to this vertex at time 5 say. At this point I could go along this edge to this vertex but again this vertex is already visited. I am not going to do that. Since there is no other option left, I will backtrack out of here. I have finished my visit at this vertex at time six. Always backtrack to where we came from. We came from 4 so we go back to that. Now I come back to 4 but there is one other option at four which I have not explored which is going like this. I am going to follow that now and reach this vertex at time 7. Yes? When I reach this vertex at time 7, now I look at the other options I have. This is one option but this leads to me to a vertex which is already visited. 

This is another option I have but this also leads me to a vertex which is already visited. So no other options, I backtrack out of seven. At time 8 I backtrack out of seven and I come back to this vertex. At this vertex I have explored all options. I went along this edge, I went along this edge and had come along this edge, so this is the only edge left. In some sense I go back along this. I have explored this, I have explored this so I am ready to backtrack out of this vertex. I backtrack out of this vertex at time nine. Backtrack out of this vertex at time nine and where do I end up? At this vertex. (Refer Slide Time: 15:48) So now reached this vertex. At this vertex I have explored this possibility, I have explored this possibility. I have not yet explored this possibility but this is meaningless because this vertex is already visited. I have explored all possibilities out of this vertex, so I now backtrack out of this. I backtrack out of this at time 10 and I come back to this vertex. At this vertex I have only explored this edge yet. 

Now I go explore this edge but this is going to a vertex which is already visited, so I cannot do anything. This is going to a vertex which is already visited so I cannot do anything. This is going to a vertex which is already visited, I cannot do anything. I am done with this vertex also. At time 11 I finish at this vertex. Each vertex is been given two numbers. The time at which we came there, the time at which we left, 6 vertices so there should be 12 numbers in all.

(Refer Slide Time: 16.49)

 

That?s what we have 0 through 11. This procedure is called a depth first search. Any questions so far? Once again we will have a notion of black, grey and white vertices just as we had in the case of breadth first search. What should our initial color on the vertices be? White, unvisited same as breadth first search. When should I color a vertex grey? When I reach that node then I color it grey. When should I color it black? When I leave that node eventually then I should color it black. Is it clear to everyone? Same as in the breadth first search. When I had removed the node from the queue, when I inserted the node into the queue in breadth first search I color it grey. When I removed it from the queue I put its neighbors back into the queue. If any of the neighbors is white I put into the queue and I color this node black. So corresponding thing here would be when I backtrack from a node. I have explored all possibilities going out of that node then I color that node black. So if I am at a node, I will look at its neighbors and which neighbor will I go to? 
Suppose this is the first neighbor I consider, in what circumstances, what should be the color of this neighbor so that I would go to this neighbor, white. Yet to be explored. Only if this node is white, will I go to this node. If it is black or grey then I will not go to this node at all. I am at this node, this is what I am saying. If this node is white only then do I explore it so to say. Let?s understand this example a bit more. Let?s mark out the edges along which we traveled in the course of our breadth first search with red. I travel along this edge because I came from here to here along this edge, how did I reach two from this one? I traveled along this edge, so once I went along this edge then I backtracked along this edge. Then I went along this edge then I went along this edge, then I backtracked along this edge. Then I went along this edge, backtracked along this, backtracked along this, backtracked along this. Each one of these red edges, I have went along them twice. 

Once I went along the edge, the other time I backtrack along the edge. How many such edges are there? How many red edges are there? [Student: n minus 1] The number of these red edges would be n minus 1. Why? [Student: forward tree] why it is a tree? Who said it is a tree? We are going to use a different argument for it?s a tree. First why should this have n minus 1 edges? For every vertex there is one edge along which I came to that vertex and the same edge along which I backtrack from that vertex. So for every vertex there is a unique edge except for the first vertex. Since for every vertex there is a unique edge except for one, there should be n minus 1 edges. So number of red edges equals n minus 1. If I just look at the red edges, they form a connected sub graph. Why do they form a connected sub graph? Because by walking along the red edges, I could visit all the vertices. Yes, all I did was walk along the red edges. That?s all I did. So by just by using the red edges, I could visit all the vertices starting from the root, from this start vertex. These red edges form a connected sub graph.
 
(Refer Slide Time: 23:00)

 


So connected sub graphs with n minus 1 edges is a tree. Yes, we have done this before. If I have connected sub graph with only n minus 1 edges in it, it cannot have a cycle we have proved this. So it?s a tree. The red edges form a tree and this tree is also called the DFS tree, the depth first search tree. Is this clear? Just as we had a breadth first search tree, a breadth first search tree was defined in terms of predecessors. Here also we can have a notion of a predecessor. What is the predecessor of this vertex? The vertex which was visited at time one because why should that be the predecessor? I came to here from there so that is an actual notion of predecessor and these edges then? the same thing, same idea is getting repeated. 

Just as we had a breadth first search tree there, we have a depth first search tree here. But the breadth first search and the depth first search tree are completely different. What I am going to do now is to redraw this tree. I am going to keep it like this. Can you all see the picture? I am going to redraw it so that it looks like a rooted tree now. This is this vertex and let me draw it in brown. This is that and the next vertex would be there and from here I have this, so this is this vertex that I have drawn, from here I have an edge to there and I have another edge. These are all the 6 vertices.
 
If you want, I will put down the numbers so that you can also see the correspondences. This is 0 slash 11, this is the 1 slash 10, this is 2 slash 3, this is 4 slash 9, 5 slash 6, 7 slash 8. Let me also draw the other edges of the graph. Right now I have only drawn the tree edges, the DFS tree edges. These are the edges along which we traveled. Let me also draw the other edges. How many other edges do we have? We have 4 other edges. There is one edge from here to here, there is one edge from 1/ 10 to 7/8. This is another edge, this is this edge. There is one edge from 7 8 to 0 11 this edge, there is this edge 0 11 to 5 6. Let me draw it like that. This is an undirected graph so these directions that I have shown are meaningless. It is just to signify that this is how we moved and now I can get rid of this picture. This is our graph. I have just redrawn it so that now it looks like a rooted tree and the predecessor of the node now is just the parent of that node. 

If I define this as the root then there is a natural parent child relationship between the nodes. The parent of this node is this and it is also the predecessor. So quite often we will talk in terms of parent child siblings and all for a DFS tree. When we say that basically means we are thinking of the starting vertex is the root of the tree and we are basically hanging the tree from there and then whatever is the parent child relationship that gets defined that?s what we are working with. 

Let?s look at more properties of depth first search. These green edges, so the brown edges are called tree edges, that?s the brown edges. The green edges are called back edges and we will see why they are called back edges and why not front edges. What?s back about them? Now let me think of depth first search has been done on this graph. This is the entire graph. All the edges and the vertices are here. I started from this vertex, I came down to this, I came down to this and then I looked at, this was one option available to me. But this edge was going back to a vertex which I had already visited. Yes, and so this is called a back edge, going back to a vertex which is already visited. [Hindi] 
Then it is going back to a vertex which was already visited so I don?t go there. There is no other option left here so I backtrack. This is the option here I come here, I come here again this is an edge going back to a vertex which is already visited. I don?t go along this edge. I backtrack from here, I come here. This is an edge going back to a vertex already visited. I don?t go along this, I backtrack, I backtrack. I could now look at this edge again and say well this is going ahead to a vertex already visited. I could potentially have also called it a front edge but since we first considered it as a back edge, we are going to stick to the term back edge. 

This is an edge we going to a vertex already visited, so we don?t consider this and since all option I exhausted here, back track, I come here. These are all edges which we have so to say have been classified as back edges. We don?t or they are going to vertices which are already visited. We don?t do anything and we are done. Who can formally define for me what a back edge is? What is a property of a back edge? Now I think I am getting ahead of myself. Now I have the following question, could there be an edge from this vertex to this vertex. Could this dotted red edge be? So I have drawn a dotted but most likely it should not be there but why? When I came to two, when I came to this vertex at time two then I backtracked out of this vertex only because there was no other option available to me. But if this edge was there then this was an option available to me why because this vertex was not yet visited. This vertex was visited only at time 4. At time 2 this vertex was not visited and so it is still a white vertex and so I would have gone along this edge and if I had gone along this edge then this would not be the picture at all. 

Clearly this edge is not there in the graph. [Student: sir we have the level difference] No, we will understand what these edges are in a second. There is nothing to do with level here unfortunately. This is again two we are saying edges which jump a level in breadth first search, they cannot be because if they were then that would not have been our breadth first search, that would have not been this collection of levels. If this edge was there then this should have not been the picture at all, it would have been something completely different. So this edge is not there. This edge is not there, similarly this edge from here to here is not there or from here to here is not there. 

What are these edges which are not there? What can I say about edges which are there and edges which are not there? Sibling but this and this are not sibling. [Student: there is no ancestor which]. What are the edges which can go from here which can emanate from here? They are only edges which can go up to root or to ancestors? [Student: ancestors] So let?s understand this, this is an imp very important point. I have reached the certain vertex and this is let?s say the sequence of vertices along which I reached here. I am sitting at this vertex at this point and I am ready to backtrack, ready to backtrack means there is no other option available to me. What are the other edges which could have started from here? These are only two vertices which have already been visited. These are clearly vertices which have already been visited. There could be edges from here to here but why can?t there be an edge from here to some other vertex which has already been visited. [Student: there will be no need instead of vertex] No, so why did you say only ancestors? Suppose this was my vertex here, why are we saying that only two ancestor.
Why can?t we have an edge from here to here? [Student: because then it would be a ancestor of this and have been visited earlier it would be] If there was an edge from here to here? [Student: it has to be child of ancestor when do we backtrack before backtracking we would cover that vertex that path would have already been covered]. What is a formal way of saying this? Why should we have only edges from a vertex? Let?s just put down what we have concluded so far. From a vertex we can have edges only to ancestors and such edges are called back edges. What is a back edge? So now we are ready to define what a back edge is. Anyone, what is a back edge? An edge [student: from a node to another] from a node to an ancestor. What is an ancestor? An ancestor with respect to the depth first search with respect to the DFS tree. The notion of an ancestor is coming in only because we have defined DFS tree. 

An edge from a node to an ancestor is called a back edge but not to? [Student: but not to parent] An edge from a node to a parent is a tree edge. So we will distinguish between tree and back edges. An ancestor is not a parent let?s say. Those are the back edges and these edges, the once in the red dotted here are not back edges because they are not going from a node to its ancestor. Neither is this node an ancestor of this nor is this an ancestor of this. So such edges cannot be there at all in our graph. 

Depth first search basically means, so after you do a depth first search you end up dividing now the set of edges into two classes tree edges and back edges and there is no other edge. [Student: we can just say that any edge] Every edge gets classified either as a tree edge or as a back edge. Let me write this down. So DFS classifies every edge as a tree or a back edge. This is similar to breadth first search means breadth first search classifies every edge as an edge going between adjacent levels or going within the same level, let?s see. 

(Refer Slide Time: 36.49)

 

So here depth first search also does this thing for us. Clear to everyone? We are still talking about depth first search in undirected graphs. When we come to directed graphs things will change a bit, keep that in mind. How do we implement depth first search? Looks like a fairly complicated thing. Stacks or recursion let?s see. What are the things we needed? We said we have to keep track of whether a vertex is visited or it?s not visited. Whether what the color of a vertex is. So actually we don?t even need to distinguish between grey and black. 

We just need to distinguish between white and non-white, whether a vertex is visited or it was not visited. That was the only thing we really needed. We are going to keep an array called visited. So what will this have? Basically it will have an entry for every vertex. For a vertex v it will have an entry let?s say 0 or 1, 0 if it is not visited and 1 if it is visited. This is a zero one entry, zero means not visited. Initially all the entries of this array would be a zero. [Hindi] 

Suppose I wanted to do a depth first search starting from a vertex v in my graph. So now my graph is going to become more abstract. This is my graph, this is a vertex v, I want to do a depth first search from this vertex. So what am I going to do? What does depth first search involve, what is the first thing I should do? First I should mark this vertex as visited. Clearly let?s say this is the very first thing I do, visited v equals one. Now I have to look at all the vertices one after the other, adjacent vertices. So let?s say this had 3 adjacent vertices x, y, z. 

Let?s put down a loop for all w adjacent to v, do something. For all vertices w, so w is just a running variable so to say which will take the value x, y or z depending upon which this is. What should I do? [Student: doffs dfs dfs w] I just do visited dfs w right away. No. [Student:] If it is not visited. If visited w [student: equal to one] equal to zero, if not of visited w then [student: dfs] just say DFS (w). [Hindi] else [student: no else how can you define] no else then what? [Student: the else w will backtrack] Backtrack. [Student: else backtrack if for all w] for all w [Hindi] [Student: not of visited of w then we backtrack visited w equal to one] Visited w equal to one [Hindi] (Refer Slide Time: 41:15).

If it has now then [student: sir predecessor] backtrack [student: [Hindi]. if all w is not visited then we backtrack to the predecessor sir after we have done]. Basically [Hindi] but this takes care of everything for us, all our backtracking everything. It is not trivial to understand this part. Why it is taking care of all the things for us? [Hindi] For all [student: adjacent vertices] [Hindi] all adjacent vertices [Hindi] but to convenience yourself this is doing all of that. You know recursion is not magic right it?s after all just a piece of code. [Hindi] It is particularly well suited for depth first search. You can write three line program, four line program for something we spent 30 minutes telling what the procedure is.




(Refer Slide Time: 43.19)

 

What is that is happening? Why is this working out for us? So let?s try and understand that and I will just? I think I will just explain it on this picture. I started a depth first search from v. This is what my depth first search v was. Let?s say the vertices were considered in this order, exactly this order x, y, z. visited v is one, visited x y z are all zero, as are visited of all the other vertices I just started my depth first search here. Then I came to this vertex. Since visited of x is zero, I launched a depth first search here. That?s what we would have done here. 

I launched depth first search on this. What is this depth first search going to do? Let?s say our depth first search visits a certain bunch of vertices which are not already visited and marks them visited one. As a consequence this guy is going to visit a bunch of vertices and set visited one for each one of them, visited at one. And then it is going to terminate, every program has to terminate. It also terminates but what is it that terminates? DFS on x. When DFS on x terminates, this recursive call terminates. Where do we end up? When this terminates we end up in the DFS of v because this is the DFS which called DFS (x). 
The picture is something like the following. You had DFS (v), making a call to DFS (x). This did a lot of recursive calls but at some point it terminated. After it terminated we made a call to DFS (y). When you made a call to DFS (y), why did we make a call to DFS (y)? Because we are looking at all the adjacent vertices and I am assuming for now that y was not visited in this DFS.
 
Suppose it was not visited. So it was still at zero. I made a call to DFS y now. As a consequence, it visited another bunch of vertices. it would not have visited any vertex which was already visited by x. We are ensuring that not going to a vertex whose visited is already set to a one. So it visited another bunch of vertices and then it terminated. 
Now I am going to go to vertex z and try to launch a DFS there but I see that z is already visited. Because z was set to visited, when I did my DFS (y). When that happens, z is already visited so I don?t launch a DFS here. I have taken care of all adjacent vertices and so this terminates now. This terminates means the DFS on v terminates, this whole process terminates. The claim is I would have visited all the vertices that were to be visited. From this vertex which are the vertices I can reach. If I reach a certain vertex, either I can reach it through x or through y or through z. Because after all these are the only three edges incident at this vertex. If I can reach it from x then that means it should have been visited in DFS (x). If I can reach it from y, it should have been reachable from DFS (y). If I can reach it from z, it should also have been reachable from DFS (y). Why? Because z is reachable from y. If z is reachable from y then after I had reached z, I would have continued and visited all vertices which can be reached from z. So anything that is reachable from v therefore is visited and so our DFS from v should terminate and that is exactly what is being done here. 

We are not done with this yet as you can imagine. I want to add my timestamps. How should I modify this procedure? [Student: starts] Suppose with each vertex I want not juts visited but I have two other arrays arrival. Let?s call it a and departure let?s call it d. So a of v so this is also an array and this is also an array. It is not a zero one array sorry, so this zero one was for the previous thing. If I have a vertex u, so a of u will be an integer which will tell me what time I reached vertex u. And d of u would be another integer which will tell me at what time I left vertex u. What modification should I make to this piece of code? [Student: when d of u] [Student: a of u comes when you start DFS v after visited]. So a of u equals [student: plus plus b plus whatever] time plus plus [student: and after DFS not after DFS w after] [Hindi] [student: after the for loop] [Hindi].
 
It?s just saying stamp and increment so that the next guy doesn?t get the same stamp. It could be plus plus time or time plus plus, doesn?t make too much of difference. It will just change the starting. Yes, no? So and time could initially be zero [Hindi]. You can also modify this procedure to identify which edge is a tree edge and which edge is a back edge. Can you do that? [Student: if visited w] [Student: for DFS w we can mark its edge]. Suppose I also wanted this information. Every edge which is a tree edge, I want to mark the tree edge. [Student: with is equal to one then] when I am ready to launch DFS w then that means?. What does that mean, what can I conclude at this point? [Student: noise] which edge? v w [student: v], the edge v w is a tree edge that I can conclude. [Hindi] I can write that statement [Hindi].
 












(Refer Slide Time: 52.48)

 

v w is a tree edge. If you have identified what the tree edges are then its equivalent to identifying what the back edges are, anything which is not a tree edge is a back edge. I am going to stop here today. In the next class we are going to see to analyze the running time of this procedure and we are going to spend a couple of classes in applications of depth first search.   
Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 28 
Applications of DFS

Today we are going to be talking about applications of depth first search. In the last class we looked at the depth first search procedure. We will also be discussing the running time of depth first search today and then looking at an application of depth first search to check if a given graph is two edge connected. Recall what we did in the last class. We wrote a small piece of code depth first search v. Can someone tell me what this was? The first thing we do is we said visited v equals one lets say and then I will not worry about the counters for now because that1?s for all w adjacent to v. He was saying children of v, there is no notion of children of a node in a graph. For all w adjacent to v, if not of visited w then DFS w, that?s it. That?s what we said our DFS procedure is. It first marks the node as visited then it will start the DFS on each of the adjacent nodes provided they have not already been visited. That?s what DFS v corresponds to. How much time does this procedure take? It is a recursive procedure, so you have to do something careful. Some careful analysis of the running time. How much time does it take? Why? Pardon [student: every] compared, what do you mean by compare? [Student: like we have to check the end is visited or not]. 

For every edge we will have to look at the edge twice. The answer is right, you are basically doing a total time of order m. Actually I can just say order m because I am assuming that the graph is connected. In a connected graph m is at least as large as n minus 1 so you can always say order m. So let?s just say order m and we have assumed graph is connected. Now let?s try and see what?s happening here. For this I am going to use my adjacency list representation because that will also give you a better picture of what this is. Recall that in the adjacency list representation of the graph, there will be one entry corresponding to node v. This would be the adjacency list of v, the nodes which are adjacent to v. If a node x is adjacent to v then v is also adjacent to x because it is an undirected graph. 

So x [Hindi] (Refer Slide Time: 04:57). When I do this step for all w adjacent to v, what does this say? How will I actually translate it into code? This is pseudo code, you don?t really have a statement like for all w adjacent to v. What will you do? You will basically have to traverse this list, for that you will have one pointer which points to the first node and then when you have looked at this node then you will update the pointer to point to the next and so and on till the pointer becomes null or reference becomes null. Then you would have reached the end of the list. That?s how you will be actually implementing it. 
Basically every time I go through this loop I advance that reference, I advance that pointer by one. Now if this was the very first node u and it was not visited then what would I do? I would start a DFS on u. As a consequence I will do some other computation and when that computation finishes, I come back to this DFS procedure. The DFS that I was doing on v and I retrieve that pointer.
(Refer Slide Time: 06:01)

 

Essentially I would have that the pointer is still pointing here. That because that was a local variable so to say. You follow what I am saying? I would not start so, suppose let me say I advance through this pointer. This was the longer list and I reached this pointer reached at this point. Let?s say this node is A and A was not visited, so I went and did DFS of A. When I came back from DFS of A then what is the next node I will see in this for loop. I will go beyond this A, I would not continue. I will not start all the way from beginning and how I am retrieving the fact that I was here last, this pointer was pointing to this information. This is coming from the recursion. The fact that in a recursion when I make a recursive call I store the local variables. [Hindi]  

If I were not to write for all w adjacent to v, I would have written something like the following. I would have let?s call this array A. I would have p equals A of V. What is P? P is this pointer of v, so p initially is pointing to here and I would be replacing this by, while p not equal to null, good. Something like this, this might not be completely correct but it will be something like this. While P not equal to null what would I do? I will do something like this and I will do P equals p dot next. You understand what this is. Basically each one of them has a next and this w will p dot data or p dot node or some such thing. What is w? Because we don?t have notion of w, so it will be p dot data let?s say. Basically I am saying each one of these nodes has two members, one is data and one is next and P is referring to this.
 







(Refer Slide Time: 8.45)

 

Why am I going into all of this? If I have to replace this line, I would have to do something like this. This is how you will actually code it up and this while loop, this braces ends here. The while loop will now have these two statements in it. Why am I doing this? Now note that visited is not local variable, visited is some global array. The only local variable that I have here now is my P. When I make this recursive call, this P is stored on the stack. So that when I return from this recursive call, when I finished DFS of A I was so let?s now say P was pointing to here, I am doing DFS of A. When I return from here, I will retrieve this P back and I will increment p or advance p like this, P would now point to the next one. I would come back to this P, I would not come back to this P right at the beginning or some such thing. Why am I saying this? This is crucial for the running time of the procedure. You can understand why, because if every time I was going to start from the beginning then I can?t say order m. I am now using a same fact, I will be traversing this list only once for each node. [Hindi] 

Once I start traversing the adjacency list of a node, I don?t repeat any entry in it. I kind of just keep moving forward in that adjacency list. What does that mean? For each node I am effectively spending time proportional to the degree of that node. Some of that degrees of that node is the number of edges, two times the number of edges in the graph and so the total time required is order m. Yes, she said each edge is visited twice. That?s also true, v x, x v. The edge v x is being looked at here when I look at this node because x is now treated as a adjacent node of v and the same edge v x is looked at here, when I look at this node x. Because v is being treated as an adjacent node of v. Actually every edge is looked twice, exactly twice. This gives you the running time. Is this clear? We are heavily using the fact that this is a recursive program and when this recursive call finishes, we retrieve this particular local variable. This local variable gets back the same value it had before this recursive call was made. Now if I told you that implement DFS without using recursion, you will have to use a stack. 
You will essentially simulate the recursion by using the stack but now it should be clear what variables will you store on the stack. What information would you store on your stack? [Student: current pointer] basically P, the value of P. In the case of recursion two things are stored actually, not just P someone else has to tell me what else is stored. [Student: v also] v local variable [Hindi] they are stored and the parameters to that procedure are also stored. The parameters to this procedure is v, that will also have to be stored in the stack. Of course the stack also stores return address and stuff like that. But those things you don?t need here because you know exactly where it is returning to. This will be your sixth assignment. You will have to implement it. I will give out the details later but you will have to implement DFS without using recursion so that you understand this way.

(Refer Slide Time: 13:16)

 

We looked at DFS, we have classified, the edges has tree edges and back edges and we have looked at what the running time of DFS is. One other thing that perhaps we should do before we proceed further is look at this distinction between tree edges and back edges once more. So tree edges and back edges. Suppose I have an edge u v which is a tree edge and if u v were a back edge let?s see what can we say about the relation between the arrivals time and the departure times for the nodes u and v. Let?s see, let me ask you these questions. Suppose I tell you that the arrival time of u was less than the arrival time of v. That is I reached node u before I reached node v [student: back edge] pardon [student: u v form a back edge] u v form a back edge. 

Suppose I told you, I gave you this information. I reached u before I reached v. Now what is the relation between the departure times of u and v? [Student: departure of v is less than is less than the departure of] departure of v is greater than or less than [student: less than less than] departure of u is less than or departure of v is less than [student: departure of um v is less than] [Hindi] you reached u first, u v is a back edge. 
You reached u first and then you reached v. What does that mean? v is a descendant of u in the tree. u v is a back edge, so there has to be an ancestor descendant relationship between the nodes of u v. One has to be an ancestor of the other. Which is an ancestor of which? Clearly you will reach an ancestor before you reach the descendant because you are coming down from the top of the tree. So u is a ancestor of v. In fact I have shown that in the picture essentially. So u is an ancestor of v. If u is an ancestor of v in this tree then first I would have finished v and only then I would have backtracked all the way and come back to u and finished u. [student: same] No, this is for the back edge. This is not a tree path, sorry. 

There is some tree path between u and v also of course and lets say this is the edge u v. There are other nodes on this tree path, so you would have finished a descendant before you move up because that?s how you backtrack. You finished a node and only then you move back up and then you finish that and you move back and back and so on. The departure time of u is more than the departure time of v. You would have left v, we would have finished v before we finished u. What if u v is a tree edge? Would that make the difference? Suppose once again that arrival of u is before arrival of v. If arrival of u is before arrival of v then u is a parent of v. So u v have a parent child relationship not just an ancestor dissonant but an parent child. u is a parent of v [student: why] because it is a tree edge. u v is a tree edge. So when I depart from v after that only will I depart from u. So departure time of v is less than the departure time of u. Note I cannot say that the departure time of u is one more than the departure time of v [student: because] because it might have more children. Similarly I cannot say that the arrival time of v is one more than the arrival time of u because there could be other children also. 

(Refer Slide Time: 18:05)

 

The relationship is the same but these are the same relationship but they coming from different reasons, roughly the same reason agreed, granted.  
Now let?s look at an application of depth first search to determine if a graph is two edge connected. Let me write down connected. So that?s the term that I am going to be using. Graph G is two edge connected if and only if G minus e is connected for all e. [Hindi] I am saying take any edge, capital E is the set of edges. When I write graph as v, E; v is the set of vertices and capital E is the set of edges, little e is an edge, G minus e means remove the edge from the graph. If it is still connected and this is true for every edge for the graph then we say that the graph is two edge connected. In words a graph is two edge connected if it remains connected even after the removal of any edge. Yes, only one edge. 

A graph is two edge connected if and only if it remains connected after the removal of any one-edge. Let?s see. Is this graph two edge connected? if I remove this edge then it becomes disconnected. Such an edge whose removal disconnects the graph is also called a bridge. This edge we would call it a bridge. This graph is not two edge connected. Is this graph two edge connected? [Student: yes] yes, so graph which is two edge connected will not have any bridge edge. A graph which is not two edge connected will have a bridge edge.  

Why do you think this notion of two edge connectivity would be useful? If this were a computer network and some link fails then you are interested in whether the network is still connected or not. If your network was two edge connected to begin with then no matter which link fails. Your network can still keep functioning because it will still remain connected. But if the network were not two edge connected to begin with then the failure of a link can call the network to break down into disconnected components. So that messages cannot go from one connected component to the other anymore. 

(Refer Slide Time: 21:55)

 

This is basically measuring liability of the network. Now the question is a following. I give you a graph and I ask you is it two edge connected. 
How will you check if it is two edge connected? Anyone? [Student: if we can suppose we have from one end point to the other point of an edge sir each vertex] For each vertex if we can find a cycle. How you will check whether a given graph is two edge connected? We have to do it fast. Pardon [Student: we will look at the departure time] If you look at departure times. [Student: sir between any two node there should be a tree edge also and back edge also] Between any two nodes, there should be a tree edge and a back edge? [Hindi] [Student: so on removal of either of one its still remain connected] [Hindi] Each node should be visited twice? [Student: between any two node] Let?s don?t worry too much about DFS because that is not straight forward but you will see how to do it. 

Let?s see can you check this property of two edge connectivity by some other mechanism? [Student: sir by BFS] BFS yes, what will you do with BFS? You have to check if the graph remains connected even after removal of an edge. So take an edge, remove it check if it is connected then take another edge remove it, check if it is connected. Take another edge remove it, check if it is connected. [Hindi] So that?s more expensive. We can do it in order m square by removing every edge and checking if resulting graph is connected. Yes, but that?s expensive for us. So we want to do something in order m time, linear time. [Student: say that v vertex has a back edge] If every vertex has a back edge only then is the graph two edge connected. [Hindi] Is a cycle two edge connected graph? It is two edge connected. [Hindi] What will be the tree edges? [Hindi] This will be my DFS tree, the one in red and the only back edge will be this last edge.
 
(Refer Slide Time: 27:05)

 

My DFS tree, if I were to draw it differently would look like this. It would be a path, my DFS tree with one back edge only. [Hindi] [Student: when you are back tracking from that vertex if you get a back edge that means there is a cycle connected that particular path of traversal so you can then compute something] Good, we are getting somewhere. 
So let?s develop this, one step at a time. I did a depth first search, so clearly we have to do a depth first search. Suppose this is the depth first search tree I obtain. I have just drawn the tree edges, there are other back edges. I have not drawn them yet. Now when I am backtracking out of this node, backtracking means I am going back up because I have explored this entire thing. What do I require? I have explored this entire thing, I have not yet gone here. So I have come from here, I came like this, I came like this, I explored this entire thing. 

Now I am back tacking. I want to ensure that this edge along which I am backtracking is not a bridge. I want to ensure that it is not a bridge. [Hindi] What will ensure that this edge, I am only interested in this one edge not being a bridge? What we ensure that this one edge is not bridge? [Student: it is connected, it is also back edge] There is a back edge from where? [Student: from this vertex] From this vertex? [Student: yes yes] From this vertex [Hindi] any of these vertices say [Hindi] (Refer Slide Time: 29:25). If you remove this edge, let?s look at the blue tree, the DFS tree. In a tree when I remove one edge, I will get two pieces not more. This will be one piece and the other remaining piece. I cannot create more than two pieces by removing an edge. I create exactly two in a tree. Now these two pieces are connected among themselves by the tree edges, by the blue edges. 

I can go from any node to any other node. How will I go from a node in this piece to the other piece? By going from that node to the end point of this green edge, taking this green edge, going there and then going from here to whichever node I wanted to go to. So which means every pair of node is connected which means that the entire thing is connected, even after I have removed this red edge. This red edge is not a bridge. If such an edge is present then this edge is not a bridge. [Hindi] But it has to go to this node or beyond. [Hindi] I will get two pieces which are disconnected from each other because there will be no edge going from here to anyone here because we said there is no edge of this kind. [Hindi] This is the condition we have to check. Everyone understands? What is the condition we have to check? When I backtrack from a node, I have to check that there is some edge from here which is going to an ancestor of this node. [Hindi] How will I check this? Linear time [Hindi]. There should be some edge from its descendent to one of its ancestors, that?s all. 

Let me write it down in words. When backtracking from a node v, we need to ensure that there is a back edge from some descendent of v to some ancestor of v. [Hindi] I have said descendent without saying proper descendent. So descendent includes the node itself but this ancestor is a proper ancestor which means parent or above. [Hindi] Now we have to somehow ensure this. How will we check this property and we have to do it all fast.






 
(Refer Slide Time: 35:47)

 

[Hindi] We are only permitted order m time. [Student: do we keep track of the back edges then we back track on particular node we believe from our record that particular back edge that node] keep track of all back edges. Do we need to keep track of all back edges? [Student: which] [Hindi] I am interested in a back edge but do I need to keep track of all the back edges starting from? which is the back edge which is of interest. [Hindi] If we keep track of every back edge then we are going to be spending a lot of time. [Student: sir we will delete the] but there is one back edge which is of interest to us. Which is the one back edge which is of interest to us? [Hindi] Clearly if I know [Hindi], you understand what I mean by [Hindi] is going to the node which is closest to the root. How can I figure out which is the deepest back edge? By looking at the arrival time of the other end point [Hindi]. So just by looking at arrival time of this end point, I can figure out what the [Hindi]. 

Now question is how am I going to build this information recursively. I have to find out the deepest back edge from this sub tree. So I am just keeping track of the deepest back edge. Now I have to find out the deepest back edge from this sub tree. How will I find out the deepest back edge from this sub tree? Suppose recursively I have done this information. I have figured this information. From the sub tree I know the deepest back edge because after all we are doing recursively. When I run the DFS from here, I actually end up running DFS?s from let?s say these three. Suppose I figured out the deepest back edge from this sub tree, I figured out the deepest back edge from this sub tree and I figured out the deepest back edge from this sub tree. 

How can I compute the deepest back edge from this sub tree? [Student: compute that compare] Compare all three and take the minimum. [Student: and from u also] the one with the minimum arrival time. [Student: corresponding with the minimum arrival time]. So this will give me an edge whose other end point has a smallest possible arrival time. This will give me an edge whose other point has a smallest possible arrival time, this will give me an edge whose other end point has the smallest arrival time. Now if I take the minimum of these three, it will give me the edge which has the smallest possible arrival time which emanates from this sub tree. Is this true? [Hindi]. Now let?s write our DFS procedure. So this is our, lets give it some other name to distinguish it from DFS. Let?s call it two EC, two edge connectivity. So we are writing a two edge connectivity procedure. Once again it will take as input a particular node. We are going to write it as the DFS thing and I will tell you how we have to call the DFS, this two edge connectivity procedure eventually.

Recall I need the arrival times of the nodes. So I should maintain my arrival counter suitably. So what should I do from my arrival counter? [Student: time is equal to zero] [Hindi] For all w adjacent to v, same thing. For all w adjacent to v do. What should I do? [Hindi] Arrival value of this node itself would be a natural thing to do. Let?s set that at that. Now deepest back edge [Hindi] as I do my DFS calls, I have to kind of keep updating this variable. What should I do for all w adjacent to v do. If not visited w then then what should I do? [Student: then if then arrival v or smaller than dbe] arrival v then dbe equal minimum of dbe, 2 EC of w [Hindi]. [Student: sir even it is even it is visited] if the node is visited then what does that mean, what edge is it? [Student: back edge] it is a back edge [Hindi]. So else what should I write on here? [Student: if you are maintaining that d ec w and some particular array then you can write that minimum of I will say I have to write the same thing again minimum]. We will just write the same thing, dbe equals minimum of dbe comma [student: 2 ec w arrival of] not 2 ec, we are not running a arrival of w. [Hindi] 

Now what do I have to check? [Student: if it is less than or] if dbe is less than arrival v. [student: never be possible] [Hindi] If dbe is less than arrival v then continue. [Student: There we can then we can continue] then we can continue. If dbe equals arrival v then [Hindi] then abort. Basically then you stop your procedure saying you found a bridge. You can do whatever you want, I will just write abort here. You should not write abort, you should end gracefully. But you understand what I am saying. Basically why have we said equal to and not greater than or equal to. Greater [Hindi]. [Student: sir can you explain else part if w is] he wants me to explain the else part. Why do we need the else part, you are wondering. [Student: we need the else part] you need the else part, great. [Student: but why arr w what is the significance of w] what arrival w? The else corresponds to a back edge starting from v. It is going to a node w. w is a node which is adjacent to v, so it is going to a node w. 

How am I keeping track of deepest? I am keeping track of deepest by arrival numbers of the nodes. So that?s why I am comparing it the arrival of that node with this. [Hindi] so I need that and if the deepest is less than this v then its okay, I can continue. If it is here only, deepest is this then this means that this edge is a bridge. [Hindi] No, we cannot say anything about the arrival time of this verses the arrival time of this. I have said this before. We cannot say that this is one less than that. Basically we have to modify this so that I am not considering this edge, this tree edge. This has to be modified so that the tree edge is not considered, this parent edge is not considered [Hindi] (Refer Slide Time: 49:55).  
When we do the DFS from that vertex, we visit all the vertices. All the back edges, if any coming from below cannot go to some smaller number. Clearly they are only going up to this vertex because this vertex has the arrival number zero. There is now vertex with arrival number less than zero. [Student: dbe should not be zero] [Hindi] basically there are many ways of doing it. You could perhaps have marked this edge as a tree edge. You will have to think of ways of doing this. I will leave that as an exercise. These are two minor things but they are important. Your procedure would not run at all, if you were to ignore them. What is a running time? [Hindi] So essentially as same as before. For every edge we are spending a constant amount of time. The total running time is still order m. this is clear? So actually very sophisticated procedures can be built on top of DFS. 

(Refer Slide Time: 52:46)

 

There are many other graph problems which can be solved in liner time. They might seem very complicated problem but you can essentially solve them in linear time using depth first search. I will mention one other problem because I have a couple of minutes. We will not of course discuss it. The other problem is, is G a planar graph? Do you know what a planar graph is? You are not done a discrete math?s courses. No, you are doing it next semester. [Hindi] that is not sufficient. What is a planar graph? A planar graph is a graph which can be drawn in the plane such that its edges do not intersect. [Hindi] This is a planar graph. I can draw it whichever way I want but the edges should not intersect. 

Now suppose what is this graph? This graph is the complete graph or almost the complete graph on five vertices. I told you what the definition of a complete graph is. Complete graph or a cleak (Refer Slide Time: 54.55) Is this the complete graph, compete graph on five vertices. No, why not? The edge 2 5 is missing, [Hindi] 2 5 is the edge which is missing. [Hindi] There is no way I can draw 2 5 here without crossing and that has nothing to do with the way I drew the initial thing [Hindi]. There is no way I can draw 2 5. Actually this complete graph on 5 vertices is not a planar graph [Hindi]. 
If I were to draw it this way, it would cross with this. If I were to draw it like this, it would still crosses with this edge. If I were to draw it like this, it would cross with this edge and so on and on. There is no over drawing this. So that?s the question, is a given graph a planar graph. This problem can be solved using depth first search. So that you can get an algorithm which runs in linear time, order m time, very sophisticated algorithm to check if the graph is planar or not.
 
(Refer Slide Time: 56:07)

 

This is an example of a non planar graph that I had shown. You learn more about this in your discrete math?s course. What are non-planar graphs, what can you say about non planar graphs? [Hindi] This is another example of depth first search but we are not going to be taking up this in this course. There is one third example which I will do in two minutes. So just as I defined two edge connectivity, I can define two vertex connectivity. 
Just replace the edge by a vertex. A graph is two vertex connected, if removing any vertex still keeps the graph connected. This corresponds to computer failures. 

Now instead of link failures earlier [Hindi] then you would call it a two vertex connected. So no matter which computer breaks down. If the network is still functioning, it is still connected. Then you would call it a two vertex connected graph. For instance this would be an example of a graph. Is this two vertex connected? No, why because if I remove this vertex, it becomes disconnected. When I remove a vertex, I also remove the edges incident to that vertex. Clearly it becomes disconnected but this is a two edge connected graph. This graph is two edge connected. Once again the same question given a graph, is it two vertex connected that can be checked by depth first search in linear time. Such a vertex is called a cut vertex, bridge [Hindi] corresponding notion is cut vertex here.
  



(Refer Slide Time: 58:31)

 

In today?s class we have done example of depth first search which is checking if a given graph is two edge connected. There are many other application that depth first search can be put to. I have shown you, I mentioned briefly two examples, checking if a given graph is a planar graph and checking if a given graph is two vertex connected. So next class we are going to look at depth first search in directed graphs and see how it is going to be different from undirected graphs. 
Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 29 
DFS in Directed Graphs

Today we are going to be talking about depth first search in directed graphs. In the last two classes we have looked at depth first search in undirected graphs and seen applications for the same. How does depth first search in directed graphs differ from depth first search in undirected graphs? Let?s take an example of a directed graph. Let me make sure this has something interesting happening. Let?s put this edge back here. Suppose this is the directed graph that I started and this is my start vertex. The process is going to be exactly the same. As in the depth first search, so the code for depth first search would remain the same. Let me write it down once again for your benefit. So depth first search from a vertex v, what did we do. We set visited v to 1 and then for all w adjacent to v. What do you do? If not visited w then DFS (w) and this was the quote for depth first search in undirected graphs. 

Now this is the same quote for depth first search in directed graphs. I just need to redefine what the adjacent means now. What do you thing adjacent means? So a vertex w will be called adjacent to vertex v, if I can go from v to w. I am at vertex v, I want to look at all the vertices to which I can reach from v. So these three vertices would be adjacent to v. So sometimes the term out adjacent is used because you might also want to call this vertex adjacent. So x will be called in adjacent but here we talk of adjacent we mean out adjacent, vertices to which you can go from vertex v and the rest are the same. Let?s see what will happen now. Let?s say I start from this vertex S and I took, this has the first edge to go out. I came to this vertex, now I took this as the first edge to go out. So I came to this vertex. Now I will consider the edges going out of this vertex. 

How many edges are going out of this vertex? Only one which is going to a vertex which is already visited. Yes. So there is nothing more to be done at this vertex, I will backtrack from this vertex, go back to where I came from. I come back to there. Now from here I am going to look at the other edges which are going out of this vertex, there is one more edge so that will bring me to this vertex. From here I am going to look at the edges going out of this vertex. There is only one edge which is going out of this vertex that's going to bring me to this vertex. 

From here I am going to look at the edges going out of this vertex. There is only one edge going out of this vertex which is going to a vertex which is already visited. So I am done here, I backtrack come back here. Yes, backtrack. Why do I backtrack from here? Because there is no other edge going out of this vertex. So I backtrack, I come back here. There is no other edge going out of this vertex, so I backtrack, I come here. From here now what will I do? I will look at the other edge going out. This is going to this vertex, so I come to this vertex along this edge and from this vertex, I look at the other edge going out, any edges going out, so this edge is going out but it is going to a vertex which is already visited, yes. So I backtrack from this vertex and I come back to here. There is no other edge going out of this vertex so I backtrack from S which effectively means I have finished the procedure. [Hindi] This is what DFS here would look like. 

Let me give the arrival and departure numbers to these vertices so that it?s completely clear what we are doing. So this zero, can some quickly tell me what these numbers would be? 1 here, 2, 3, 4, 5, 6, 7 that is very easy. We are just saying 1, 2, 3, 4, you are not putting it down on the paper, it is harder for me; 7, 8, 9 here, 10 also here, 11 here. [Hindi] It?s clear? This is how our depth first search happens. Now suppose let me modify this graph a bit. Let me change the direction of this edge. Let me make it this way. What will happen now? [Student: vertex will not] This vertex will not get visited. 

What will be the departure time of this vertex now? I will come back to this vertex, so I departed from here at time 8, I came back here. Come to here and now there is no other edge going out so I will finish at time 9. Now what happens? What about this vertex, when will this get visited? [Student: this will be only be visit start from] Now we are going to? so depth first search when it starts from a vertex, it might not visit all the vertices. This is true even in the case of an undirected graphs. It will not visit all vertices if the graph is not connected for instance as was the case of breadth first search. You started the breadth first search from a vertex, it would visit only the vertices and that connected components.
 
Similarly depth first search in an undirected graph would visit only the vertices in that connected component. In a directed graph, the depth first search would visit only such vertices which can be reached from this vertex such that there is a path from this vertex to that vertex. [Hindi] There is no path from this vertex to this vertex. Why, because this vertex is no edge coming into it now. There are only two edges going out of this vertex and no other edge coming in. There is no way of going from here to here. In fact there is no way from any vertex to this vertex. This vertex does not get visited when we do our first depth first search but just as in the case of breadth first search what we did was, if some vertices were not visited we took a vertex which is not visited and started out breadth first search from that vertex. 

Similarly we are going to do that here. If some vertex is not visited after we finish our depth first search from this vertex then we are going to pick that vertex which is not visited and continue our depth first search from there so which means that I am going to now pick this vertex, give it an arrival time of 10 now, look at the edges which are going out of this vertex. They are all going to vertices which are already visited. So there is nothing to be done so which means that I also finish this depth first search. In this manner every vertex is going to get both an arrival and a departure time. I will keep this picture and I will again redraw the tree, the depth first search tree.





(Refer Slide Time: 09:31)

 

We did a similar thing in the last time. [Student: sir we are all vertices] we have to visit all the vertices, so both depth first breadth and depth first search require that you go visit every vertex. So while I did not elaborate on this when we were discussing depth first search in undirected graphs but there I was assuming that the graph was connected. If the graph is not connected, you did a depth first search from a vertex, you visited a bunch of vertices. If you have not visited all vertices then you will take another vertex which is not visited and start a depth first search from here. [Student: sir to see that whether this vertex is not been visited, we also have to go to once again to all the vertices]. We had looked at a procedure for doing this in the clever manner, when we were looking at connected components. All we have to do is traverse the visited array to find the first vertex which is still at a zero and we will never have to retrace, we will just have to make one scan of that array. We can adopt the same procedure here. 

Let me now draw. These are the tree edges that you have drawn. Now I am going to draw the back edges. Now we will understand what kinds of edges can there be, just one second. I have drawn all the edges as you can see, so the tree edges are in blue. [Student: we can't] You can't see the colors. Wow, I didn't realize that. That?s tragic, let me try to make them darker. Can you see the color now? [Student: yes] It looks a deeper black [student: it is easy bold and fine] fine [student: find in dash dash dotted] so good that you pointed out. Is that today that you can't? [Student: no noise every day you pen moves on that we assume that it is the same color] is it better? [Student: slightly] slightly [student: but because if you use a pen now we realizing its blue] so this blue edge, no it?s not blue. Does it look blue now? [Student: its red light red] its red [student: sir sketch pen. Sir we have to make that out by using the sketch that you are using] [Hindi]. Hopefully the others were watching this program, we will be as smart as you are; (1, 8), (2, 3), (4, 7), (5, 6) and (10, 11).  

Now we don't have the nice picture we had in the case of undirected graphs that all edges are in two categories either tree edges or a back edges. As you can see there are three different color already and I have not yet used a forth color. There is also a possibility of a forth color and let me show, you could also have an edge which goes from here to here. This is green clearly. This edge could have been there, why? Suppose this edge was there, I am continuing to follow this path. When I come back here to this vertex, I look at the other edges going out, this is another edge going out but its other end point is visited. Now we have to classify these edges, we have to give them names. Our green edge, this edge is a forward edge. Why forward? Because it is going forward in the tree, down. We will call that forward. The brown edge is a back edge, it is going back up in the tree. We don't use the term backward edge, we just use back edge. It is nothing backward about the edge. The red edge is called a cross edge. [Student: which has which one which one].

This is a red edge and this is also a red edge and this is also a red edge, these three are red edges. The brown also looks like black. Is it? The brown edge is that edge, it is a back edge. So it should be clear. Tree edge is completely clear, an edge along which we traverse is called a tree edge. Now something that is going forward in the tree is called the forward edge, something that is going back in the tree is called a back edge. Something that is not going either forward or back is called a cross edge. What is a property of a cross edge? [Student: not going to ancestor] not going to ancestors or to a descendant. [Student: descendant]. The two end points of the edge don't have an ancestor descendant relationship, not parent child, ancestor descendant relationship. The two end points of this edge or none of them is a ancestor of the other. 

Similarly for this edge, none of them is an ancestor of the other. This is one tree, this is another tree so to say, just a single ten vertex. This is not an ancestor of any of these vertices. These three are cross edges. Could I have a cross edge which goes in this direction? It would have become a tree edge. So cross edge would go in one direction only. What is that direction? We will translate all of these into numbers very soon but essentially if you draw picture in which you are first visiting the left side of your tree and then going right. Then they are going from right to left. The cross edge only go from right to left so to say not higher to lower but more like right to left. This cross edge is also going from right to left, this is also going from right to left. We know this is bit subjective but I will tell you what the actual thing is. 
 
What?s the property of a forward edge, who can tell me? So what is it that makes an edge of forward edge? So a forward edge how can I relate its arrival and departure times. If I have an edge u v which is a forward edge, what can I say about the arrival of u verses the arrival of v. [Student: arrival of u arrival of v] pardon, arrival of u. [student: is less than other] it?s an forward edge such an edge. This is u, this is v. I would have first reached u and then I would have reached v. What can I say about the departure of u verses the departure of v? [Student: departure of v] Clearly I will leave this before I leave that. So departure of v is less than the departure of u. That's for a forward edge. I can play the same game. Let me now say it for a back edge. Once again I have a u v back edge. So arrival of u verses of arrival of v which is smaller? This is a back edge, this is u, this is v. 
V will be smaller because when I am saying the edge is u v, in a directed edge I always specify the tail first. This is u, that's v. This is v then I reached this before I reached this. The arrival of v is less than arrival of u. What about departure? Departure of u verses departure of v. Departure of u is a u v edge, I will leave u before I leave v. Now let?s come finally to our cross edges and since this is more important and you have not seen this before. Once again u v is a cross edge. If this is a cross edge, this is u, this is v. What about arrival of u verses departure of u? [Student: arrival of u] arrival of u verses arrival of v, this is u, this is v. [Student: every] arrival of u is greater. This is u, this is v. I will reach here, after I have reached here. Arrival of u is greater than the arrival of v. I will first arrive at u actually then I will depart form u. Yes, so I should actually write it in a more sophisticated way as departure of u. Sorry arrival of u is not greater, I would first less. I first arrive at u then I depart from u then I arrive oops. What am I saying, sorry. 

(Refer Slide Time: 21:25)

 

I have managed to create a mess there. Let me do it again. So we are considering a cross edge, can you all see this picture? Let me consider a cross edge again. I am looking at an edge u v, so in my picture this is u, this is v. Now it should be clear. So which vertex will I reach first? [Student: v] v, so first I reach v then I leave v. That's important then I reach u, then I depart from u. Yes.  










(Refer Slide Time: 22:49)

 

The arrival and departures are arranged in this way for a cross edge. Great. So in a depth first search the edges get classified into four categories and based on the arrival and departures, you can figure out what those categories are or based on the fact that whether they are a tree edge or not. So first you will create the tree edges, first you will identify what the tree edges are. Using the tree edges you can identify the parent child relationships also, ancestors descendant relationships and using that you can figure out whether it is a forward edge or a back edge or a cross edge. If the two end points have an ancestor descendant relationship then it?s either a forward edge or a cross edge or a tree edge. 

If they don?t have then it has to be a cross edge. Sorry in the first case it?s either a forward or a back or a tree and in the second case it is a cross edge. When you have to distinguish whether it is a forward or a tree or a cross, then again you know it?s based on whether the tail is an ancestor of the head or whether the tail is an descendant of the head. 
You can use that to figure out these things. So as an application we are going to look at how to check if a given graph has a cycle or not. So that's the application we are going to look at. I am giving you a graph G, so given a directed graph G check if G has a cycle. What is a cycle in a directed graph? It?s basically a path which closes on itself, the starting and the ending of the path are the same. 

How will you check if a given directed graph has a cycle? How will you check if a given undirected graph has a cycle? [Student: starting node if we have any back edge if we access the starting node] [Hindi] so then there is a cycle. If there is a back edge there is a cycle. If there is no back edge, is there no cycle in the graph? [Student: yes] Because then the remaining edges are just tree edges, they form a tree. A tree does not have a cycle in it and a back edge, why does it form a cycle? Because the two end points of the back edge are connected in the tree. There is a path between those two end points. 
So for an undirected graphs, it is very simple. For a directed graph it is not so simple. When does a directed graph have a cycle? Once again you want to do a depth first search. In a depth first search in what?s [student: the back edges are definitely] If there is a back edge then the graph has a cycle. If we encounter a back edge then G has a cycle. Great, why? Well, let?s just look at an example. This was the tree, this was the back edge, what is a cycle? Well, the cycle is exactly this. 

(Refer Slide Time: 26:32)

 

Since it is a tree, this node is an ancestor. This is a back edge so this is an ancestor of this. If this is an ancestor of this, there is a path in the tree from here to here. I will go down to one of its child and so and on and reach here and this together with this forms the cycle for us. If there is a back edge, there is a cycle. If there is a cross edge, does that mean that there is a cycle? So question is if there is no back edge, does that mean G is acyclic? I am using a term here, what does acyclic mean? No cycle. [Student: there is a cycle consider a graph it is only a cycle when definitely you will not have a back edge] He is saying consider a graph which is only a cycle. We will not have a back edge. It will have a back edge. Is it clear? So he is saying consider a graph which is only a cycle [Hindi]. When we do a depth first search of this graph, what do we get? No matter of what vertex I start from. 

I will let?s say go down here then I go down here then I go down here, here, here and at this point this is will become a back edge because I will just retrace my path here and reach here. So this will become a back edge. So back edge, if you find a back edge there is a cycle. If there is a cycle, it seems that you will get a back edge. Can you prove this? If there is no back edge does that mean that there is no cycle in the graph. No. [Student: all forward] If there is no back edge, does that mean G is acyclic? [Student: due to a cross edge also because it can happen that in a particular cycle the child is traversed by some other path like the origin of cross edges] [Hindi]. 
So let me do a very interesting proof of this statement or what is a statement? This is not a statement, this is a question. So statement is no back means no cycle. No back edge, no cycle. [Student: for a cycle does it mean a arrow is going in one direction] Of course, this is a directed graph. A cycle means a path. I was very clearly specified in a directed graph, a path means you can go from one vertex to the next. Think of them as one way streets. 
A cycle would mean only if you can traverse in such a manner such that you can reach the starting point. Great. 

So no back edge means no cycle and how am I going to prove this? Let me do the following. I will do a depth first search and now I will order the vertices according to their departure times. So do DFS, order vertices by their departure times. What does that mean? Let?s say I will first put down the vertex which has the largest departure times say [Hindi]. This has the largest and this has the smallest and the departure times are decreasing like this. This is the largest side [Hindi]. So this is the vertex which has the largest departure time, no two vertices at the same departure time. Yes because every time we depart from the vertex, we change the time. So this is the vertex with the next smallest, the next smallest, the next smallest and so on and on. 

(Refer Slide Time: 31.40)

 

Now let?s go back to all that we have done so far. Let?s look at this picture. For a forward edge, if u v is a forward edge then the departure time of u is more than the departure time of v, [Hindi] yes. So the departure time if u v is a forward edge then the departure time of the u, u is the tail, v is the head of the edge. The edge is going from u to v. The departure time of u will be more than the departure time of v. So if I have a forward edge then will it go from left to right or will it go from right to left? If I have a forward edge its tail would have a higher departure time, so it could go from left to right. A forward edge would like this. 

Let?s look at a cross edge now. A cross edge, u v cross edge the departure time of u is again more than the departure time of v. So the edge will also go, a cross edge would also go from left to right. [Hindi] there is a cycle in the graph. So what edge remains? [Student: tree edge] tree edge. What is the property of the departure time if u v is a tree edge then departure of u v [Hindi], so first I will leave from v and then I will leave from u. So departure of u would be more than the departure of v. great. So a tree edge also goes like this. All edges are going from left to right. How can they be a cycle? Can you create a cycle by just going from left to right? No. If you have to come back to the starting vertex [Hindi] and you have to come back to this vertex, [Hindi] you can go forward but at some point you have to come back but there is no edge which is coming from right to left. So there is no cycle in this graph. [Hindi] yeah, everyone follows this proof. 

Now you know why we were worrying about departure times. So you have to do this with departure times. You can?t do this with arrival times, unfortunately. So try that as an exercise. If I were to order them by arrival time this is not going to work. [Hindi] So this is actually a very simple proof theorem. If there is no back edge in the graph then there is no cycle. If there is a back edge then there is a cycle. So all you have to do is do a DFS, if at any point you encounter a back edge, you declare that the graph has a cycle. If you are able to finish your depth first search without encountering a single back edge then you declare the graph is acyclic. [Hindi] What is an acyclic graph? A acyclic graph is a graph which does not have a cycle. 

(Refer Slide Time: 35:45)

 

This ordering so? what we have? Another thing we have shown and that is the very nice thing is that given an acyclic graph, let?s say G is an acyclic graph. We can order the vertices of G so that every edge, so that every edge goes from left to right, yes. That?s what we did here. We said we started with an acyclic graph. We started with a graph, we did a depth first search in a graph. 
We did not encounter any back edge. So we said the graph does not have a cycle. It is an acyclic graph and then we said lets order the vertices of the graph according to decreasing departure times. Then what we see? If we order the vertices in the manner then every edge goes from left to right. This ordering is also called a topological sort [Hindi]. So in a acyclic graph you can order the vertices, linearly order the vertices so that the edges are going only from left to right. This ordering is called the topological sort. So how much time do you take to find a topological sort? You just have to do a DFS and then, then sorting. [Student: this is] check what? [Student: is there exists some] [Hindi] but you have to produce an ordering of the vertices right. [Student: Hindi] Right, so we don?t need to sort because we know what the departure times are. What is a maximum departure time we can have? [Student: 2 n] 2 n, yes 2 n actually 2 n minus 1 because each vertex is getting two numbers, right. 

So the total set of numbers that we are going to be assigning arrival and departure times will be in the range 0 through 2 n minus 1. So the departure times is at most 2 n minus 1 that is at least to 1. So I can just have an array with 2 n entries in it and as I depart from a vertex, as I assign it with departure time at the position in the array I put down the vertices. I just need to make one scan of this array to get the vertices in the right order. [Hindi] So you don?t need to sort the n departure time because if you had to sort, you would take n log n time. We have not seen a sorting algorithm which performs better than n log n. So this ordering is called the topological sort and it can be computed in order n time, order n plus m time. So I have used, so I should also tell you about another term that is used for acyclic graph. So we are talking of directed graphs here. A directed acyclic graph, directed graph which is acyclic is also called a DAG, D A G [Hindi]. 

(Refer Slide Time: 40:29)

 

These graphs arise quite a bit in circuits, in combinational circuits where you know your pulses are essentially traveling from one side to the other. There is no notion of a cycle, there is no loops and so these graphs model that and we can do a lot of things on such kind of graphs which we cannot do on a regular directed graph. The graph which has cycles in it and we will see some of that in this course. Questions so far? [Hindi] So let me introduce, I will like to take one more application of DFS in directed graphs and since that is a longer application, we will not be able to finish it in this class but I will develop the terminology for it. So let?s go back to our notion of? so for undirected graphs, so if I gave you an undirected graphs there was a notion of whether the graph is connected. Yeah, when is an undirected graph connected? When it all vertices [student: there is a path between every two vertices] there is a path between very two vertices then we say that the graph is connected. So connected means there is a path between every pair of vertices. In a directed graph the corresponding term is what is called strongly connected.  
A directed graph is called strongly connected, if there is a path between every ordered pair of vertices. 

Why am I using this term ordered pair of vertices? Between which, so I said pair of vertices but you know it is a directed graph. May be there is a path from u to v but there is no path from v to u. Let?s take an example. Is this graph strongly connected? Let me label. There is a path between a and b but there is no path from b to a. There is a path from b to c but there is no path from c to b. there is a path from d to c but there is no path from c to d. [Hindi] It is still not strongly connected. There is a path from a to b [student: but not from] but from b to a there is no path still. Pardon yeah, sorry. [Student: what do you mean by the] no, no when I say there is a path from a to b it basically means that I can go from a to b like this or like this right but I cannot go b to a there is no path from b to a. Is there a pair pair of vertices such that there is a path between these two vertices in both directions. [Student: no] no cannot happen right because not in this graph.
  
(Refer Slide Time: 44:55)

 

Let?s see more examples which will perhaps be better. So a b c d, is this strongly connected? No, yeah between any two vertices. So from d to b there is a path, from b to d also there is a path because everything is on a cycle. [Student: cycle] yes, so this graph is strongly connected. Is this graph strongly connected? [Student: yes sir] [Hindi] is there a path sorry, is there a path from a to f? [student: yes yes] yes [student: a to b] yes a and f there is also path from f to a going like this. So this graph is also strongly connected. No that is not sufficient right, I can?t just take two pairs and check them and say it is strongly connected. We have to look at every pair right but you can check. [hindi] all of these form one strongly connected component. For all these 4 vertices any pair I choose is connected. Similarly for these four vertices any pair I choose is connected yeah and that should somehow tell you how to do things. [Student: both of them must be part of cycle] right something like that. 

If there is a notion of strongly connected there should be a notion of weakly connected, not daily weakly connected, sorry. It?s weakly connected. What do you think is a weakly connected graph? [Student: between u and v all v to u] yes [student: as undirected graph] no no, not as undirected they are connected. The following is [student: there is a path from u to v or] there is a path from [student: u to v or v to u] or v to u and or v to u. The following graph is weakly connected. [Student: yes] yes [Hindi] Is there a path from b to d? No. Is there a path from d to b? No. Is it weakly connected? [Student: no] no, this is not a weakly connected graph. 

So why am I showing you this example, it?s not sufficient to say... [Hindi] So this is a graph such that if I were to ignore the directions, it is a connected graph but it is not a weakly connected graph. So what is the right definition of weakly connected? A graph is weakly connected if for every pair of vertices u, v there is a path from u to v or a path from v to u or both. If both the paths are there its okay and this should be true for very pair of vertices right. [Hindi] right, good. So I have told you what is strongly connected graph is and what a weakly connected graph is.
 
(Refer Slide Time: 49:36)

 

So natural question is given a graph, is it strongly connected? [Hindi] given a graph G, is G strongly connected? So what are you saying? We are doing a DFS starting from here. 
So we have to check, so strongly connected means we have to check that from between every pair of vertices, every ordered pair of vertices there is a path. So from u to v there should be a path and from v to u there should be a path. So one solution that is being suggested is that you take one vertex, do a DFS from here. It should visit all vertices and then take another vertex and do a DFS from there. It should also visit all vertices. Take a third vertex and do a DFS from there, it should also visit all vertices. So do this DFS. How many times? n times from every vertex. If each of those DFS?s visit all the vertices then the graph is strongly connected. Yes, is that clear. Is this argument clear? If all of these DFS visit each and every vertex then the graph is strongly connected. Perfectly okay, except how much time does it take? Order m n time. That?s too much for us. We want do it in linear time. So we want to do it in order m plus n time. 

(Refer Slide Time: 51:45)

 

So question is how did you do that? [Hindi] So try to think about this and think in terms of the procedure we used for two edge connectivity. We will borrow ideas from there. In two edge connectivity what did we require? We required that for every sub tree there should be an edge, back edge going out of the sub tree so to say, going to a ancestor of the root. Here also we require something similar that?s the hint. So you will have to see what that is and we will discuss it in more detail in the next class.  



Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering
IIT Delhi
Lecture ? 30
Applications of DFS in Directed Graphs

Today we are going to look at more applications of depth first search in directed graphs. Recall that in the last class I had mentioned that we will talk about strong connectivity. We will see how to figure out, if a given directed graph is strongly connected or not and I have defined what is strong connectivity means. It essentially means that between every ordered pair of vertices there is a path in the graph so which means I take two vertices lets say u and v, there should be path from u to v and also a path from v to u. Then we would call the graph strongly connected. So in the last class there was a very simple algorithm that was suggested which was that take a vertex, do a DFS from there. Take another vertex, do a DFS from there and so and on which means do a DFS from every vertex in the graph. 

If in each one of these DFS?s, we include all the vertices of the graph only then is the graph strongly connected. That should be easy to see. Let?s see if we can reduce the number of DFS calls that we make, instead of making n DFS?s, can we reduce the number of DFS from n to lets say some small number. Let?s say I take one vertex v and I do a DFS from here and when I do a DFS from here, I visit all the vertices in the graph. This is let?s say the DFS tree I obtain and if these were the only vertices in the graph then I have visited all the vertices in the graph. So what do I know? I know that there is a path from v to every vertex in the graph. If DFS v visits all vertices in graph G then there exists a path from v to every vertex in G. 

Suppose we could somehow figure out, we could also do the following. We could somehow figure out that there exists or I claim there exists a path from every vertex in G to v. Let?s assume this is the case, suppose this is true that is [Hindi Conversation] then does that imply that the graph is strongly connected. Basically we are saying 1+2 implies G is strongly connected. So if we have to find a path between some two vertices like say x and y, so what will I do? go from x to v, by the statement two there is such a path from every vertex to v and then I go from v to y by statement one. So all I need to somehow ensure is that there exist a path from every vertex in the graph to v. 










(Refer Slide Time: 05:25)

 

How will I ensure this? This says that there is a path from v to every vertex in the graph. If I can ensure that there is a path from every vertex in the graph to v, then I am done. How will I ensure that from every vertex in the graph there is a path to v? [Student: from the lower most vertex, we are ending first starting from the path deepest back edge have as a back edge]. So the question is how do I ensure that there is a path from every vertex to v? [Student: sir we can back edges] Think of something new [Student: also we need the cross edges to if there are some cross edges so we can go along from them to]. Suppose I were to do the following. 

(Refer Slide Time: 08:14) 

 

I took my graph G and I reversed [Hindi Conversation], reverse edges to get a graph G sup R [Hindi Conversation] [Student: do DFS from that vertex again now it look at a path] so do DFS [Student: that gives now] do DFS v on this graph G of sup R [Hindi Conversation]. [Student: if you get all the vertices] [Hindi Conversation] if all vertices are visited then this implies, in G there is a path from every vertex to v. Yes or no? [Hindi Conversation] [Student: yes sir] if every vertex gets visited in this DFS then I can say that the graph is strongly connected [Hindi Conversation]. 

In this graph there is no path from v to this vertex which means that in the original graph there is no path from this vertex to this vertex which means that the original graph is not strongly connected (Refer Slide Time: 09:01)[Hindi Conversation]. [Student: in adjacency list we can fetch change the function go along] we can also change the function DFS instead of looking at out adjacency edges, we can look at inadjacency edges and we don?t even have to reverse the graph then [Hindi Conversation]. 

So essentially by using two DFS?s, you can figure out if the graph is strongly connected or not. So everyone understands the procedure. Let me write down what the procedure is. Pick an arbitrary vertex v, do DFS v, reverse G, do DFS v on G reverse, if all vertices are visited in both DFS?s then G is strongly connected else G is not strongly connected [Hindi Conversation].
 
(Refer Slide Time: 11:19)

 

We will now try to do it with one DFS. It?s an academic exercise for this problem but at least you will learn the properties of DFS and because this is perfectly fine algorithm and it just requires two DFS?s to do it. So let?s look at another way of checking if a graph is strongly connected or not. Now we will use our whole gammon of definitions and terminologies. So what are we going to do now? We are going to do a DFS, we start from a vertex and we do a DFS. So let?s say these are the red edges which form the DFS tree. [Hindi Conversation]
Suppose this is what I get my DFS tree as. Now I remarked in the last class that we are going to use ideas similar to what we developed for two edge connectivity. So two edge connectivity [Hindi Conversation], we were saying that when we are back tracking, lets say backtracking from this vertex I am going back to the parent because I have finished everything. I ensure that there is some edge which goes from this sub tree to it can only go to an ancestor or above because we said there are only back edges in the case of undirected DFS. So if an edge goes out of here, it cannot go to one of these nodes. It can go only to an ancestor. So we said we would like to have one such edge.
 
Now we need similar such thing in the case of strong connectivity. From here we would like that there is some edge going out [Hindi Conversation]. What kind of edges will be going out of this sub tree? Only back edges but if this were the sub tree I was considering then there could also be either there is back edge out of this sub tree or there is a cross edge out of this sub tree. So I could also have an edge going like this. It can happen. So this edge is also an edge going out of this sub tree, if there is an edge going out of the sub tree I am happy. This is the only thing we will require. Clearly this is necessary, if from this sub tree there is no edge going out then this graph is not strongly connected. Why? Because then you can only enter the sub tree, we can only come to these set of vertices, we can go out of this set of vertices. What I mean by that is if there were no edge going out of this sub tree then I cannot go from this vertex to the root for instance to vertex v because there is no edge going out of this 4 edges.
 
This is like an island in itself, you can only come into here but you cannot go out of here. 
[Student: but if an edge is going out what do you do?] So all I am saying is that we require that it is necessary that an edge go out whether that is sufficient, we have to figure out. It is clearly necessary that an edge goes out of every sub tree. You understand the difference between necessary and sufficiency. So it is necessary that an edge go out of every sub tree. You understand what I mean by out of every sub tree, so you are looking at the sub tree. What is a sub tree here? It is the descendance of any one vertex, take any one vertex, look at all its descendant that?s a sub tree we are interested in and there has to be at least one edge going out of that. This is clearly necessary [Hindi Conversation] no edges is going out of that sub tree then we can stop the process at that point, stop our DFS and say that this graph is not strongly connected. 
 
Now we will see how we can check this thing. We will look at the procedure, we will look at what modification to make to the DFS so that we can check that there is an edge going out of every sub tree. The check would be very similar to what we did for the case of two edge connectivity but now let?s see that this condition is also sufficient. If from every sub tree there is an edge going out then the graph is strongly connected [Hindi Conversation]. Why? [Student:] why [student: arrival time is the difficulty] of what [student: all the sub trees if there is an edge from here to some other sub tree]. [Student: the vertex that is feature will have a lower arrival time] will have a lower arrival time than whom [Student: then the vertex in all the vertices of the sub tree] close, almost there. 
So what am I going to do? [Hindi Conversation] from the root I can reach every vertex. Now all I have to show you is that from every vertex I can reach the root. 
If I can reach the root from every vertex I am done. Yes, because then how do I find a path between x and y? I go from x to the root and from the root I go to y. So all I have to show you is a path from every vertex to the root. So we want to find a path from every vertex to the root [Hindi Conversation]. We take some vertex, let?s say this is the vertex x I want to go from x to the root v. how will I go? I am going to look at the sub tree rooted at x [Hindi Conversation], x is some vertex somewhere in my DFS [Hindi Conversation]. 
Will it be smaller than x or larger than x? It is smaller than x and [Hindi Conversation] smaller because this is a cross edge. 

You would have first come here in fact [Hindi Conversation]. So let?s give this name, suppose this node is w so I can reach a node w, so what have we said? We have said that from x, can I go from x to w? Yes, why? Because this is a sub tree, these are all descendant this node is also a descendant of x which means that I can come to this node and then I can take this edge and get to w. So what?s the big deal about to getting to w? From x I can get to a node which has a strictly smaller arrival time than x. Now from w I can repeat the same process from w, I can get to a node which has a strictly smaller arrival time than w which means that may be [Hindi Conversation]. 

Now what do you know? You know that the arrival time of x is strictly larger than the arrival time of w which is strictly larger than the arrival time of a which is strictly larger than the arrival time of c [Hindi Conversation]. I can get to a node with the smaller arrival time [Hindi Conversation] [student: when you reach] when I reach the root. So that will give me a path from this node to the root [Hindi Conversation]. Just this requirement that an edge go out of every sub tree is both necessary and sufficient. Just this requirement and this is an easy requirement to check and we are going to do that next. 

(Refer Slide Time: 21:44)

 

So how am I going to check this requirement? How should we modify DFS so that we can check if there is an edge going out of every sub tree? I am using the sub word sub tree a bit loosely, here by sub tree I mean the part of that tree which is composed of the descendants of any vertex. So take any vertex, look at all its descendants, that part of the tree is what I am calling a sub tree. The generic definition of a sub tree is slightly different. How will you modify DFS to be able to this? In the case of two edge connectivity we had modified DFS so that it returns to us the deepest back edge. The arrival time of the node to which there is a back edge from the sub tree and the smallest such arrival time [Hindi Conversation]. In the case of two edge connectivity what we have done was that we would return from every sub tree, would return the deepest back edge by that we mean the arrival time of this node. 

Now we will do the same thing here. This DFS would return to us, not the deepest back edge anymore, the node with the smallest arrival time to which there is an edge from this sub tree. so let me write that down, DFS returns the smallest arrival time to which there is an edge from this sub tree rooted at let?s say v. so DFS v returns this (Refer Slide Time: 25:29). What I am trying to say now is the following. When I do a DFS from v, there could be many edges going out of this sub tree. Let?s say there are four edges going out of this sub tree and edge which is going out of the sub tree will either be a back edge or a cross edge. Forward edge cannot be going out of the sub tree, it can only be coming in to the sub tree or if it starts from here in the sub tree it will go within the sub tree only. So now essentially I am going to look at these four arrival times and take the smallest amongst them and that is the quantity the DFS we will return to v. If this arrival time was a, this was b, this was c, this was d, DFS v returns min of a b c d. Clearly b is smaller than a and d is also smaller than c. 

(Refer Slide Time: 26.45)

 


This is what we have to return. Now we have to figure out how we will return this thing. Note that DFS is a recursive procedure, when I do a DFS from vertex v, I end up doing a DFS from its neighboring vertices. So suppose its three neighboring vertices were x, y and z so I will do a DFS from here I will get this tree, let?s say I do a DFS from here I get this tree, I do a DFS from there I get that tree (Refer Slide Time: 27:21). Now this DFS x will return something to me. So it will look at all the edges which are going out of this sub tree and look at the node with the smallest arrival time to which there is an edge from here. 

Similarly DFS y would return something to me and DFS z would return something to me. So what is the value of DFS v going to be? DFS v would have a value, we have to find out the edge going out of this sub tree. so an edge which goes out of this sub tree would be an edge going out of here or out of here or out of here but an edge going out of here could end up here itself. So it is not really going out of this sub tree [student: we have to check it with arrival time of the.. then it will not] and we will also look at v of course. So from v we will look at the back edges out of v and cross edges out of v. 

So we will basically look at all the edges going out of here, all the edges going out of here all the edges going out of here and all the edges going out of v and take the minimum of their arrival times. When will the edge be going out of this sub tree? When that arrival time is less than the arrival time of v [Hindi Conversation] there is an edge going out but [Hindi Conversation] there is no edge going out [Hindi Conversation]. So you have to check that [Hindi Conversation] that is less than the arrival time of v. If it is less than the arrival time of v then that means that the edges going out of the sub tree and we are okay if it is not less than arrival time of v then we can stop the procedure and say that the graph is not strongly connected. 

(Refer Slide Time: 29.36)

 

So now I just have to write the code for this. So what should I do? Let?s call this a strongly connected. So what is strongly connected going to do? So first it is going to be setting up the arrival time of the vertex v. Now what should I do next? I need to keep track of this minimum so I need a variable for that so let me declare some variable. What should we call it? Min, no min is for the function so what shall I call it? You can?t think of a name of a variable? xyz that?s how you name a variable. Let?s call it xyz, so what should be my initial value of xyz be? Arrival of v, no harm in setting it to this value and now what am I doing, what should I write next? For all w adjacent to v, out adjacent is the same as adjacent we are saying adjacent means out adjacent here; adjacent to v do, if not visited. Somewhere I have to set visited v equals one. As soon as I started DFS I set the visited variable to one. 

If not visited v then what do I do? Then I do a DFS from there or I run this procedure from that vertex w and it will return something to me and I have to update what it returns. So xyz equals minimum of xyz, SC [w], else if visited w is true; if the vertex is w is already visited so then that means that is either a cross edge or a back edge starting from v else xyz equal min of xyz, arrival of w. What are we doing here? We are looking at the cross edges and the back edges starting from the vertex v and also including that into the min computation. Now what are mins? This is the end of this for loop. If xyz equals arrival v then stop and we will stop with saying that graph is not strongly connected else we just return. 

(Refer Slide Time: 33.46)

 

This will again have a small problem for the root vertex because for the root vertex this quantity will turn out to be zero, cannot be less than a zero. So that has to be checked we can?t abort always, we have to just ensure that it is not the root vertex [Hindi Conversation]. So almost exactly like we did for two edge connectivity.

So how much time does this take? Almost the same time as depth first search. We have one additional variable which we are modifying and that?s all the time, the additional time that we require. So as far as applications of DFS are concerned, you will see quite a few applications. Can someone tell me what all you have seen? Application of DFS in directed graphs, what are the applications we have seen so far? Two edge connected is not for directed graphs. Checking if a graph is strongly connected, checking if a graph is acyclic and of course also topological sort all though it is the same as that. There are a lot of other applications, I am not going to be taking them up in this class. Any questions so far? 

So as a recap since we are now going to be switching topics, let me also look at what we have done for undirected graphs. We actually have looked at only one application that was for two edge connectivity. I did mention that you can use similar procedures, you can use DFS to also check if a graph is vertex connected and if it is plain but we did not do those two applications in detail in this class. What application did we see for breadth first search? Finding the connected components of the graph and checking if a graph is bipartite. 

(Refer Slide Time: 37.42)

 

Also the shortest distance that?s just breadth first search, the label that it returns are the shortest distance of the vertices from the root, the starting vertex and all of these are linear time procedures.

 	Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering

Indian Institute of Technology, Delhi
Lecture ? 31
  
Today we are going to be talking about minimum spanning trees. We are going to define what a minimum spanning tree is. We are also going to look at algorithms of minimum spanning trees. So you all know what a spanning tree is. Does everyone know what a spanning tree in a graph is? Did we define it? Yes we did. So I gave you an undirected graph, so what is a spanning tree in G? 

So spanning tree, the term is composed of two things. Tree and panning, tree means it should be a tree. What is a tree? Tree is a connected sub graph without any cycles. Connected sub graph without cycles that is a tree. And what does spanning mean? Spanning means that it should include all vertices. This basically means that it should include all vertices that would be a spanning tree in the graph.  

So for an example if I drew a graph something like that, say this were my graph. Then I could draw a spanning tree in this graph by let say, I am going to use a very,  let us say, I pick this edge in the tree, pick this, pick that, pick that then I pick this and I pick this. How many edges do I need to pick? Is this a spanning tree? No. This vertex is not included here. I could pick one of the edges incident at this to include this. This is a spanning tree. We know that our spanning tree, if the graph has n vertices, so spanning tree has how many edges? number of vertices minus 1 edge. Yes: great. 

(Refer Slide Time: 03:43)   

 

What is a minimum spanning tree? Minimum spanning tree is a spanning tree of minimum weight. Sometimes I will use the term weight, sometimes I will use the terms length, one at the same thing. (Hindi) you are not just given the graph but we are also given a length function on the edges of the graph. Let me assume the lengths are non-negative reals. So, every edge has a length now. So, think of that in the following way. These are certain cities and I can either draw or I can either connect these 2 cities by a wire or I can connect these 2 cities by a wire or I can connect these 2 by a wire, I can connect these, these or these.  

These are the possible options I have of connecting them with wires. Some options are not there. Why? (hindi) could be some reason (hindi). So, this is a graph. Now, to connect 2 particular cities, I am also told, how much length of the wire I need, that is my length function. Suppose I told you that you need 3 here and you need 5 here and you need 2 here, what are the units? It could be anything, 1000 kilo meters, meters, I do not care. This could be 1, this could be 7, this could be 4, 6, 9.

I just put in some numbers in between. So, we need to connect these cities which means we need to create a spanning tree and I am not interested in any spanning tree. I am interested in spanning tree for which the length of wire is spent as small as possible. So, that is what I mean by a minimum spanning tree, a spanning tree of a minimum length. What is the length of the spanning tree now? So, sum of the lengths of the edges in the tree. So, let us formally define that the length of the spanning tree equals sum of the lengths of the edges in the tree. 

So, I might now decide to pick some edges. Suppose I pick, I did not give this length, I give this the length 4. Suppose, I pick this edge and I pick this edge and I pick this edge and I pick this edge and I pick this edge. (hindi) So, I cannot pick this edge, it is not a tree at all, I cannot pick. So, let me pick something else. So, let me pick this 6. (hindi) What is the length of this tree? 6 plus 4 plus 4 plus 3 plus 5, 22 right? There might be other trees in this, other spanning trees which are smaller than 22. I am interested in the finding the one which is the smallest.
 














(Refer Slide Time: 07:46)

 

There is no notion of the root. In the case of spanning tree, when we defined, spanning tree is trees in graph, there was no notion of the root over. We call that also, we use the term free tree for that. There is no notion. Since, the vertex, since all the vertices are going to be included (hindi) 3, 4, 1, 2, 5, 9, 7, 4, 6 and the edges we included were, this edge, (hindi) one way you can convince yourself that this is not the smallest possible tree 
is, by seeing that for instance.

 If I include this edge of the length 1 then what is going to happen? Will it be a tree or ? We will be have a cycle. So, that is one property of a spanning tree always (hindi). It is a connected sub graph which means (hindi). So, I can go from, let say, this is vertex U and this is vertex V, I can go from U to V by following the edges of the tree and then I can take this edge that is added just now, back to go to U which means it form a cycle.

If it is a cycle, it is not a tree any more. Now, in this cycle, suppose the edges have a certain length (hindi). Now, what can I do? I can include this edge and drop the edge of length 4. Will that remain? Will that continue to be a tree? If I were to drop this edge, will it remain a tree? Why? Why would it remain connected? No notion of a descendant. There is no notion of a descendant. Why? That is the question. We have discussed this before. 

The point is that, if there is a path which gets it straight then that path must be using this edge. (hindi) I can now create alternate path between those 2 vertices. By instead now, going like this between this 2 edges. So, that means they are still, is the path between those 2 vertices (hindi). All paths are still there or not paths are still there, all face of vertices are still connected. May be, by different path but they are still connected. 

So, that is an operation we can do always. I can try and add an edge, try to add an edge in to this tree and see if as a result of that operation, so, as a result of that operation, I will clearly form a cycle. If from that cycle, I can drop and edge of longer length than the edge I added, then I would have reduced the cost of this tree, the cost of the length or weight, I will use these terms (hindi) 

So, I form of this cycle. I can drop any edge of this cycle and it will still remain a tree. Which is the edge I would like to drop? 4, I will drop this edge, 4 and now, my new tree has length what ? (hindi), 22 minus (hindi) So, this new tree length 22 minus 3 (hindi) See, I can repeat the process, I can see, there is something else that can be done. Can you say something else that can be done? 2, so suppose, I include 2.

So, now I form the cycle which has edges of length 3 and 5 in it. So, it make sense to drop 5 and the reduction now is 3 units again. Now, let me draw the tree, we have the (hindi) So, the tree that I have at this point is, this is the tree I have now. This is the edge, length 1, this is 3, this is 2, this is 4, this is 6. 

Would it make sense to include some other edge and try and repeat the process? No, because, if I include this edge which is of length 4, then this edge is the longest edge on the cycle that gets ? (hindi) and the edge I am including is the longest edge on the cycle. It does not make sense to include it, therefore. If I include this edge, it is of length 5. The cycle found may, this is again the longest edge of the cycle. Does not make sense to include, If I include this edge 7, this is the cycle that get formed and 7 is again the longest edge of the cycle. 

If I include this edge 9 then there is the longer cycle that gets formed. But once again 9 is the longest edge of the cycle. So, it does not make sense to include 9. So, it does not make sense to include any other edge. And so, actually, I should not say: and so, the reason is that, is not the reason why, I cannot claim from this that this is the minimum spanning tree. But, it is the minimum spanning tree. We will use different arguments proving that. So, this is a minimum spanning tree in this graph. So, MST has length, what is the length of this? 16.
















(Refer Slide Time: 16:36)

 

This could be treated as an algorithm, so computing the minimum spanning tree. How did it work? You start with some tree then try to include an edge, look at the cycle that gets formed, see, if you can drop an edge of longer length from that cycle or if there is a longer edge on that cycle. If there is, then you can drop that edge and therefore reduce the cost of the tree.

So, you keep doing procedure till you reduce the cost of the tree and you stop when you cannot reduce it anymore. But, we will not look at this algorithm or we will not analyse this algorithm because it will be fairly expensive, in terms of running time. We are going to look at the different algorithm for computing the minimum spanning tree and algorithm we are going to look at, is called Kruskal?s algorithm.

So, we are looking at Kruskal?s algorithm in minimum spanning tree. It is actually very simply algorithm. Let me illustrate it on the same graph that we had before. I am going to draw this graph out. 













(Refer Slide Time: 18:38)

 

The algorithm is the following, it says, take the minimum edge in the graph. (hindi) This is an example of what is called the greedy algorithm also. You will be greedy, you are all greed, but, we will now be greedy to compute the minimum spanning tree. So, what you think greedy means? Try to get the larger. Go for less? That is not greedy. Make the best possible choice at each time without thinking about the future. That is the key thing. Make the best choice available, I will not write without thinking of the future. But that is reason why it is called greedy. We just make the best available choice at each time. Here, of course, our objective is to minimize the length of a tree.

 What is the best thing to do at the first step? Take the edge with the smallest length. So, let us do that. We pick the edge with the smallest length, that is 1 and this gets included into a minimum spanning tree.  So, I am now building the minimum spanning tree, edge by edge. I included the edge 1, edge of length 1. Which is the next smallest edge? Which is the next best choice to make it? 2? 2 or 3?  When you are saying, we need to connect it your thinking of the future. Do not do that. Think of the, be greedy, as greedy as you can. Just take the best choice. 

So, the next best is, just go for the length 2 and now what is greedy? Say next, which is the next I should take? 3, 4 (hindi) So, it will form a cycle. Since it forms the cycle, we are not getting what we wanted. It is not a tree any more. (hindi) So, you understand the only modification that I am making? If by including an edge, I form a cycle, I do not include that edge. So, I am not going to include edge 4, sorry, this, this particular edge. Which is the next edge that I would like to include? 5, I try to include 5 but 5 also forms the cycle. So, I do not include 5. 

Which is the next edge I would like to include? 6, this edge, 6. Actually, I can stop at this point because now I have a spanning tree. Any other edge that I try to include will always form a cycle because this is a spanning tree and so I can stop this. As you can see, this we have already argued in, we have not argue, this is the best possible. Actually, till now, we have not seen an argument for why this is a best possible. But, this is the same solution, we obtained earlier. Soon, we are going to argue that this is the very best possible. Does every one understand? Yes? Can we write down what the algorithm is? 

(Refer Slide Time: 22:42)

 

Let us quickly write down what the algorithm is, because it is very easy to write down. So, Kruskal?s algorithm (hindi). So, let us say the very first step is sort edges in increasing order of length. Let us say, this is order is e1 e2 e3 em. What is this mean?  This means e1 is the edge with the smallest length. So, to be more specific, l of ei, length of edge ei is less than the length of the edge ei plus 1, less than or equal to. Now, what should be the next step?  For i going from 1 to m do, what should I do? Take the first element? Take ei to that spanning tree. 

So, we need to define some spanning tree. Let say, my spanning tree is T which is initially, null. T is the set of edges. So, what should I do now? If ei union T has a cycle, or is a tree then T is T union ei. (hindi) return T, i plus plus for loop, (hindi) You could do that, may be. (hindi) How do we check? We can later array and we can given it a on every edges in type greater we can increases when it is greater than two that we can Wrong. (hindi) 

How do you check, if that edge that you are trying to include, form the cycle or not? (hindi) What is visited means? Before including a vertex in the tree? No, we do not include the vertex in the tree. We are only including edges into the tree.





(Refer Slide Time: 26:51)

 

As we included edges in the tree, what is visited array of that edges means? (hindi) what does visited array of vertices mean? (hindi). 


(Refer Slide Time: 28:05)

 

Sir, we can do that we can take that both vertices the n point of that edge and we can go for dfso and pfso DFSO, BSFO in what?  We will have to develop this later. (hindi) We will see how to do it and important thing is also to do it efficiently. (hindi) because that will dictate the running time, because expect for this step, there is nothing else this algorithm. (hindi) How much time will that take? m log m (hindi) So, this will, if this step takes less then m log m time, over all iteration put together then this will be an order, m log m algorithm. (hindi) So, we have to somehow achieve that, but we will come to that point later. Before that, we have to argue, why this is a minimum spanning tree? (hindi) That means all you are saying, you know, you cannot greedily impure the tree. May be, but there is some other sequence of, (hindi) No, we are not taking always the minimum. 

We are not taking always the minimum, because, we are at sometime, when the minimum forms a cycle, we through that minimum away. (conversation) So, we need to formalize this equation. All I will say is, this is not a proof. This is an intrusion for the proof. Let us see what a proof looks like. (hindi) So now, what we are going to do? Look at this carefully because this is how you will have to write your proofs in the exams. I am going to look at the edges of Kruskal?s algorithm, the edges picked by Kruskal?s algorithm. So, Kruskal?s algorithm picks up certain set of edges. Let us call those edges, let us give those edges name.

Let us call them g1, g1 is like say, the first edge in Kruskal?s algorithm. So, clearly it is the edges of the, so let us g1 g2 g3. How many edges of does Kruskal?s algorithm take? n minus 1,less than or equal to (hindi) all edge lengths are distinct (hindi) the same proof can be extended to a non distinct case also. That will be a question in the majors. 

So, but for now, you will, we will just assume that ...  The other set of edges is the set of edges that, what should be the other? So, this is the edges that Kruskal?s pick. Suppose, you have figured out, what is the best possible tree is? (hindi) f1  f2  f3(hindi) and again this ordering is such that f1 is less than f2 which is less than f3 and so on. (hindi) This is the optimum tree. By optimum I mean, the best tree, the minimum spanning tree. Now, these 2 sets of edges need not be the same. 

(Refer Slide Time: 33:37)

 

What we will argue, is that they are indeed the same. If they are the same then what we have found from Kruskal?s algorithm is the best tree. So, if these 2 set of edges are not the same, so, the proof is by contradiction. Suppose, these set differ and the first place they differ is i. What you mean by that? (hindi).Because, that will mean that gi and fi are also the same. 

So, let us say, this is the first place where they differ, first point of difference. (hindi) Case 1, gi, let say less than fi. (hindi) every, all edge lengths are distinct. (hindi) g i cannot be 1 of these because it is of length, strictly less than, (hindi) I actually mean, length of gi less then length of fi, just to be more particular. I am not going to write it again but that is what I, it really means.   

So, gi less than fi, then that means, the g i cannot be 1 of these and it cannot be 1of these. Can it one of these? No, because these are the same as these and they are all distinct. So, these cannot be the same, so that (hindi) So, g i less than fi, add gi to opt tree. (hindi) gi is the longest edge in the cycle, can gi be the longest edge on the cycle?  Add gi to optimum tree. Let C be the cycle formed. This cycle, I am calling it C. 

(Refer Slide Time: 39:54)

 

You are saying (hindi) all of these edges had length less than gi, what are these edges? These edges are some edges are from this side. (hindi) They are all from this set, f1 to fi, n minus 1. Now, if every edge has length less than gi, then these edges have to be f1 through fi minus 1, yes.  

These edges have to be from the set f1 to fi minus 1. (hindi) Then these edges have to be from the set, f1 to fi minus 1. But, f1 through fi minus 1 is identical to g1 to gi minus 1. (hindi) Contradiction, should I repeat? No? 


(Refer Slide Time: 43:30)

 

Quick repetition, gi is less than fi, I add gi to the optimum tree, to this tree. (hindi) But, this set is identical to this set. Great, that is why this cannot happen.

 Let us look at the other cases. Do you want me to repeat? Does everyone following the line of argument?  There are 2 or 3, 2 or 3 fine point there. But, let us look at the other case. Let me 1 second, write down this, because it is useful to have this.
 
We are not talking of case 2 and this case is when fi is less than gi. Recall that these are identical and now f i is less than gi, that is what we are going to do now. (hindi) fi is distinct from g1to gi minus 1. (hindi) Why did Kruskal not pick fi? Ask him, why ask us?  Why did Kruskal not pick fi? Because they form the cycle, that was the only reason why he did not pick fi (noise). 

If it forms the cycle, so because, fi union g1 through gi minus 1 contains a cycle, these set of edges (hindi) but these set of edges is identical to fi union f1 to fi minus 1(hindi) that implies optimum tree also has a cycle but that is not possible. Because it is a tree, so that is the contradiction. (hindi) There is no place where these 2 sets differ. Because, you said, if these 2 sets differ, let us look at the first place where they differ. If they differ, then let us look at the first place where they differ and apply this (hindi) that means they do not differ at all, if they do not differ, these to case are the same. So, Kruskal?s algorithm finds the best tree. (hindi) 

 





(Refer Slide Time: 47:47)

 

Now, note that there is no notion of the term the best tree. Should I use the term the best tree or a best tree. (hindi) So, is the minimum spanning tree unique? (hindi) Yes, listen to me carefully (hindi) edge lengths are distinct. Is the MST unique? Yes, if edge lengths are distinct. No, otherwise. 

Why no, otherwise? (hindi) very simple. So, if edge lengths are not distinct, then you could have many different minimum spanning trees. But, if the edge lengths are unique or distinct, then you have a unique minimum spanning tree. (hindi) That follows from the proof that we have seen, they will be a unique minimum spanning tree. (hindi) Great, (hindi) we have proved the correctness of Kruskal?s algorithm. (hindi)


















(Refer Slide Time: 50:12)

 

Now, we have to figure out (hindi) when I include an edge, how do I ensure that no cycle in formed. Basically, that is the question, how do we check if a cycle is formed when an edge e, let say, an edge e, u comma v is included and we have to do this quickly. (hindi) 

When will be a, when will a cycle be formed?  I am trying to include this edge, u, v. (hindi) When u and v are already connected (hindi) Cycle is formed if and only if u and v are already connected. Yes, already connected, (hindi) u and v are in the same connected component.  Connected component or component ? (hindi) 

So we are, what we are going to do is, we are going to maintain the collection of components. So note that, so, what is happening? In Kruskal?s algorithm, suppose, these were my vertices (hindi) Does not matter. This is or no matter, what 3 edges I pick, it will always have n minus 3 connected component. (hindi) 3 edges which form of a forest, then the number of trees in the forest. See, if I have a forest containing some k edges, then how many trees are there in my forest? n minus k, this you can prove very easily. 

Basically, (hindi) I combine, 2 connected components into 1 connected component. That is the way you have to think of, I combine 2 connected components into 1 connected component (hindi) So, the number of connected components reduces by 1 every time you take an edge. (hindi) So, (hindi) at each step (hindi) with every edge we include. So, what we are going to do is, to maintain the connected component we have at this stage, how many connected component have at this stage? 4. 

I am going to maintain them as 4 sets of vertices. What do I mean by that? Let me give these vertices names, a, b, c, d, e, f, g, h. So, I am going to have 4 set, abc is 1 set, fgh is another, d is the third and e is the fourth. These are the 4 sets that I have at this point.

I am just interested in the sets of vertices. The sets of vertices in 1 connected component. They form 1 set, as you can see, these sets are all disjoint and there union is the universe, they form a partition of the vertices sets. Now, what is it that happens at each step? When I try to include an edge (hindi) what do I have to check? (hindi) So, this is the data structure we have to maintain. 

(Refer Slide Time: 57:39)

 

We are going to discuss it in more detail in next class. You just keep this in mind and we will see how do to this data structure and what time does it give us for Kruskal?s algorithm. So, with that we are going to end today?s discussion. We are going to continue the discussion on this data structure in next class. 
                  Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering

Indian Institute of Technology, Delhi
Lecture ? 32
First class we looked at the Kruskal?s algorithm for computing minimum spanning trees. We were, we saw what the algorithm was and what we were doing was, to complete algorithm, we have to figure out, how to detect if there is a cycle that gets formed when we add an edge into the current set of edges that we have already went by. 

So, if you recall what we have said was that we would try and maintain the collection of connected components. So, I am going to revise part of that but we are going to today, look at the data structure, for being to do that, that is called the union fine data structure. So, let see, where we were as far as Kruskal?s algorithm was considered. We said, I have already picked a set of edges. 

(Refer Slide Time: 3:27)

 

Recall that the set of edges would always form a forest, it would be a collection of trees. There could be no cycle that is already existing. So, these are the set of edges that have already been picked in Krukal?s algorithm. Now, when I am trying to add a new edge, I have to check if it forms a cycle or not.

Suppose, this is the new edge being added, this forms a cycle and 1 way of detecting whether a certain edge would form of a cycle or not, is to check, if its 2 end points lie in the same tree of this forest. The tree is the same as the connected component and so, it suffices to check whether the 2 end points of an edge, line the same connected component. This is where we were, at the last class.
So, we have to somehow maintain our collection of connected components. So, as the algorithm proceeds - we have discussed this in the last class - in number of connected component reduces by 1 with every step.

Initially, we started off with n connected components. Initially, we had n connected component and eventually, finally, we have only, how many connected component will we have? 1. This is how the algorithm proceeds. So, I am going to abstract this problem out and capture it as a problem on maintaining a collection of disjoint sets.
 
So, what is the setting now? I have a universe of elements. Let say, I have e1 e2 en. Let us say, these are n elements in my universe. Initially, each of these elements is a set in itself. Now, the following operation, so, this is a collection of disjoint sets that I have. Initially, I have the collection of disjoint sets, at each stage I am going to have a collection of disjoints.

So, what we are trying to do is to maintain a collection of disjoint sets under the operations of, let us see, what are the operations, we are doing, going to be doing on the disjoint sets. So, what do these elements correspond to, in the case of Kruskal?s algorithm? But, what are they initially? Initially, these e1 e2, what are e1 e2? These would be the vertices, the initial vertices. 

Now, what are the operations that we have to do on this collection of disjoint sets? 1 operation is union, the operation of union has to be down when I have 1 connected component, I have another connected component and the edge that I add, runs between these 2 connected components. Then, the resulting thing would be, this entire thing could be, 1 connected component and it should get reflected here, by taking the union of the corresponding sets.

So, I have to do an operation of union that is 1 operation under which I have to maintain this collection of disjoint sets. So, what do I mean by that? Suppose, this collection have e1 e3 e7 in it and this, sorry, this set has e1 e3 e7 in it, this set has e2 e5 and e9, then after the union, I should not have these 2 sets in my collection. But these 2 sets of the collection should get replaced by 1 set which is e1 e2 e3 e5 e7 and e9.

The other operation that I have to do is, so, given an edge, I have to look at the 2 end points of the edge and determine if they belong to the same connected component or not.  So, the end points of the edge would be the 2 elements and I have to check given to 2 elements, whether the line same set or not. So, we will call that an operation of find. 

So, what does find do? Let say, find takes as parameters 2 elements, x and y and returns true, if x comma y are in the same set. What should it return? Well, this is not a completed description of find. If x and y are not in the same set, what should find return? It should return the 2 sets in which those 2 elements lie. Why? Why should it return the 2 sets? Because, then we need to do the union on those 2 sets, exactly. So, union should take as parameters, 2 sets, let say s1 and s2 and should take the union. This is what we require of our data structure. 
(Refer Slide Time: 7:39)

 

So, this is not an accurate description of find, I will have to modify it. But you understand the need for returning the sets in which the 2 elements lie. So, instead of find x comma y, let me have just an operation, called find x, returns the set in which x lies. There is a unique set in which x lies, because our collection of sets is always a partition of universe. So, there will be a unique set and you want to return that set.

Then how will we implement this operation? We will do find x, we will do find y, if the 2 sets are the same then we will conclude that they are forming a cycle, if those 2 sets are different, then you would take the union of those 2 sets. Yes, everyone following? So, if I were to write down Kruskal?s algorithm now, it would look like this. So, with this, assuming that these 2 operations exist, it would look something like this.  

















(Refer Slide Time: 8:22)

 


So, what were the steps of Kruskal?s algorithm, first step? Sort the edges. Sort the edges in increasing order of length. Let say, this ordering is e1 e2. Please do not confuse between this e and the element e that I had in the previous slide. This e corresponds to edges and the previous e is element. So, keep these separate. Now, what should I do? For i equal to 1 to m do. 

I pick an edge, what is the edge? ei, I am considering an edge ei. So, let ei equal to u comma v which means u and v are 2 end points of the edge ei. What is it that I have to do? If find u equals find v, then it forms a cycle. Then, we do not have to do. So, I should really do, not equal. If find u is not equal to find v, then, T is T union ei.. So, I should have some T and initialised to null and what else should I do? Union: find u, find v. 

Of course, I need to initialize this collection of disjoint sets. So, I would have, when I create this collection of disjoint sets, it will get initialized to, what will it get initialized to? Singleton, so, each element in the collection would be singleton vertex. So, this would be what the procedure will look like now. We need to understand, what kind of data structure to keep for, to find and what kind of a data structure to keep for maintaining the collection of disjoint sets, so that these operations can be done very quickly.

How many times do we do the union operation? How many times do we do the find operation? Number of unions, because, every time I do an union, I include an edge into my tree. How many edges can be there in my tree? Exactly: n minus 1. So, I will have exactly, n minus 1.  How many finds will I have? For every edge I consider, I have to do 2 finds. In the worst case, how many edges will I consider? All the m edges, no more. So, number of finds is less than or equal to m. Let us make it. Not equal to 2 m.


(Refer Slide Time: 12:17)

 

I have said, for i equals 1 to m. But, you know, you can always break out of this procedure, the moment you form a tree. So, if you form a tree before, you can break out of this procedure, of this for loop. So, you will keep it less than or equal to 2 m. So, what will be the total running time of this procedure then? If this operation; let say takes u times and this takes f time, then what is the total running time? Anyone? This step will take m log, m time, plus u times n, plus, m times f. So now, we have to find out a good data structure. By good view, mean, which would do the u and which will have the small u and small f.  Is everyone comfortable with this? 


(Refer Slide Time: 13:17)

 
Are there any questions to this point? Can someone suggest a data structure to me? How will we maintain this collection of disjoint sets? Linked list, let say, liked list. You are at the end of the course, but, you cannot think beyond liked list. So, how will we have? We will have as many linked list as the number of sets, is the qutei. So, 1 linked list for each set.   

No, let us complete this first. How much time union take and how much time did find take? Union can be done in constant time. (hindi) So, let say, we keep track of the front and the end of each liked list. If we do that, then I can combine 2 linked lists in constant time. Union will not take too much time. But, how much time did find take? Order size of the liked list, which in the worst case could be end. Now, is that good? We were to do this, we will take m, n time which is too large.

We are looking for the time complexity or something like, n log m. If we looking for something like, n log m, this quantity should be no more than log m and this quantity should also be no more than log m. We can permit it to be as large as log m. So, this is not too good at data structure. Someone had another idea. What was your idea? A tree? What will you do with the tree? How will you use a tree? Heap? What will you do with a heap? 

How much time does it take to merge 2 heaps? Order n? Height of the heap, why? Why does it take, if I have 2 heaps, why does it take order H time to merge them? Order smaller, no. Number of elements in the smaller heap, but that could be as large as n by 2.  What else? So, we will have a new data structure ? and what will a new data structure be? The sets, I will just show this thing to you and then you will understand what is happening. 

(Refer Slide Time: 16:32)

 

So suppose, my universe was a, b, c, d, e, f, 6 elements, simple. So initially, recall that my, what are the sets in my collection? The singletons: a, b, c, d, e, f. So, I have a, I have b, I have 1 node for each of these 6 sets. Now, suppose you say, union, the set containing a, so, you will say something like, union find a comma, find b. The set containing a and the set containing b. So, what I am going to do is to make 1 of them, so, each of these nodes has only 1 point ? or a reference. So, it has a data field and 1 reference field. 

So, I will make 1 of these guys point to the other. So, at the next step I will have, when after I do this, this is what my collection looks like. These of course, remain like as they are. (hindi) Suppose, you were to say the same thing, union, find a comma find c. When I say find a, I will start from a and keep going up, till I head the root. So, this is now the root. 

How do I know it is the root? Because, its pointer points to itself or it is null or whatever. Yes, when we do the union, you will understand this, you know as, as we proceed. So, this is what the trees look like. So, each one of them is a tree. Right now, this tree has only 1 node in it. But, this tree has 2 nodes in it and now the pointers are going up. You just have parent pointer ? When I say find a, I will start from a and keep going up the tree till I reach a node, let say, (hindi) parent point is null or it is pointing back to itself. I think at that point, I know it is a root. 

And so, this says that find a, that the element a, is in the set whose root or in which b is.  When I come to b, this is the element, this is the set in which, what we are doing is that for each set, how do we represent a set? So, each set is represented by 1 of the elements in a set which is in this representation, it will be the root of the set.

So in some sense, all the element of the set, elect a leader and this is the leader of that set. So, a and b are the only 2 elements of the set and the leader is b. So, when I say find a, it return to me, reference to this node which says that this is the root of the set to which, in which element a lies. When I say find b, what will it return? The same thing and then I can compare those 2 and can determine that they are in the same set or not. So, what find a, find b returns are the roots of the corresponding trees. You will understand this as you proceed. 













(Refer Slide Time: 21: 06)

 

What union does is, it takes the root of these 2 trees and links them up and makes 1 point to be other. For instance, here, I might decide to make b point to c. In which case, my new representations would look like this. Now, if I were to do a find a, what will be returned when I do the find a? A reference to c, a reference to this node and when I do the find b, what is returned? A reference to the same node. So, I can compare these two, I can return c or I can return a reference. Actually, it is best to return a reference because then that can be used by the union operation. 

What do you mean? c was not alone? If c was not alone, I understand what you mean, if c was not alone. We will come to when c was not alone. So, all you are doing is, taking the roots and merging them. So suppose, at this point I did another operation which was union, find d find e. So, what will I do? I will link up the roots for d and e. 


















(Refer Slide Time: 21: 42)

 

So, let say, I decide to make d point to e. Now as you can see, d and e are not alone. If I decide to do an operation like, let me keep this picture there and if I do, let say, union or let me write it down here. Union, find a comma find d then, what does find a return? It returns a pointer to c, d returns a pointer to e. I need to link up these 2 nodes. So, I can make c point to e or make e point to c, whichever I please. Let say, I decide to make c point to e. This is what I would get there. And of course, f would be sitting on its own. Is everyone understands, what the procedure is? 

You understand what the find operation is? What does find operation do? It starts from the element and keeps tracing the pointers up, till it hits the roots. Exactly, so, you have a list of vertices. When I have an edge, I have its 2 end points, I have the vertices and from that vertex list, I must have the reference to this node. Recall, you have a data structure for your graph in which you have an array. 

Suppose, I had an adjacency list of presentation, so this array would contain my list of vertices. I could have another reference from here to this node here, for every, so, this was node vertex b, try to have another reference from here to here, so that I can access the state. This is just referring to this particular node and so this will always remain the same. So, what is the problem with this implementation? How much time does union take?   

So, union takes now as input, references of root nodes. And so, all it as to do is to modify 1 pointer, 1 reference to point to the other, to refer to the other. So, union takes order 1 time but, how much time does find takes? Find could take a lot of time because it might go through a very long ways to reach the root, in the worst case. Can you construct a sequence of unions in which this would happen? Write at the side first to merge a b, then a c, then a d, then a e, and a f and if you were to doing the union in this order then, things would go back. 

(Refer Slide Time: 25:38)

 

We have to do the unions in a more clever manner. Find? No, we will do the union in a clever manner. So recall that when we were linking elements, we had that option, either link make 1 point to the other or point, make the other point to the first.  So now, we will exploit that.


So, I am going to use a rule called union by rank in which if I have 2 trees and suppose, this has n1 nodes in it and this has n2 nodes in it. Then, I will make the lighter tree point to the heavier one. So this, without loss of generality, let say, n1 less than n2. Then, I will make this point to that. We will make the lighter point to the heavier. Now, you will not have a this kind of a scenario in which you know, if you have this, let say, 6 elements; first you made this point to this and then, when you are trying to combine this and this, you will not make this point to this anymore. 

What will happen now? You will make this guy point to this and now, this tree has 3 nodes in it. So, if this combines this, you will make this point to this and this point to this and this point to this. So that now, what is the height of this tree that I get? 1 only, and so, find will take very little time. (hindi) So, we have to see that if we use this root, what can be the height of the tree in the worst case. How high the tree become? How high can the tree become if we use this root?





(Refer Slide Time: 28:24)

 

Anyone? Login, why? You are trying to construct the worst case. But that need not be the way we do things. That might not to be worst case. How will you argue that this rule of union by rank will lead to trees which have height? 

So, what is the claim we want to make? No, not the height is minimum. A tree with n1 or n, n1 nodes let say, has height less than or equal to log of n1. Suppose, I have to make, prove this claim, at tree with n1 nodes, set any point if I have a tree with n1 nodes in it, it has height at most log of n1 (hindi) by induction.

Good. So, let us use induction. I am not going to write down the proof formally, but I will tell you what the procedure is. So, I am combining 2 trees: one, n1 nodes, the other, n2 nodes. Without loss of generality, let us say, n1 is less than or equal to n2. Let us assume that the induction hypothesis is true till this stage of my procedure. That means (hindi) height is less than or equal to log of n1 and (hindi) is less than or equal to log of n2 (hindi) Everyone with me? 

Now, we have to show, as a consequence of this I will get a new tree with how many nodes in it? n1 plus n2 could be the number of nodes in the new tree. So, I have to argue that its height is no more than log of n1 plus n2. Let see whether that is true.

So, what are the 2 cases? n1 strictly less than n2. What will be the height? No, what will be the height? The height could be, so, height of resulting tree is either the height of this tree, so we have done this. It is either the height of this tree or it is the height of this tree plus 1. 

So, height of resulting tree (hindi) height of resulting tree is less than or equal to max of h2 comma h1 plus 1. It can take a value of h1 plus 1 also. (hindi) the height of resulting tree fine, equals max of h2 fine (hindi) Now let see, this is the height of the resulting tree. If this value equals h2, there are 2 possibilities: either this value equal to h2, but h2 less than log of n2 which is less than log of n1 plus n2. The other possibility, as this quantity equals h1 plus 1 which is less than or equal to log of n1 plus 1 which is equal to log of 2 times n1 which is less than or equal to log of n1 plus n2, because n2 is greater than or equal to n1. Actually, we have not used the fact that n1 is strictly less than n2.  Have we used that fact? So, it will become equal, but that is okay. So, I do not really need this. 

What I was said? One of the trees is, has lesser or equal to number of nodes than the other, if they are equal, actually you can connect it in any way. So, I made 1 point to the other. This, by induction hypothesis this height is at most log of n1, this is height at most log of n2. 

What is the height of the resulting tree? It is either the height of this tree or it is height of this tree plus the 1. If it is the height of this tree then, it is log of n2 which is less than or equal to log of n1 plus n2. If it is this tree plus 1, height of this tree plus 1, then it is log of n1 plus 1 which is log of, the same as log of 2 times n1 which is less than or equal to log of n1 plus n2, because n2 is larger than n1. 

This is the base case 2. When n equals 1, the height becomes 0. Let us define the tree of 0 with the only 1 node, as having a height zero. (hindi) If n equals 2, this becomes 1 which is okay. When we have 2 nodes in the tree, it has height 1 then, by definition. (hindi)  So now, what are we saying? If this is, if the tree has only 1 node in it, this is height 0. If this is the case, that is height 1. So, I am counting the number of edges on the longest path from the, from one of the leaves to the root: counting the number of edges and not the number of nodes.

Everyone with me, so, what does this show? Is this the complete proof? What am I doing in induction on? Number of nodes in the tree, so, I am assuming that the statement is true for all nodes of a certain number, less than a certain number, I can say, it is true for this when I link this and I get a tree with larger number of nodes, it will continue to be true.
















(Refer Slide Time: 36: 14)

 

Great, this is called union by rank. Rank meaning, the rank is the number of nodes in the tree. You can also do a union by height. As in, you can keep the, make the shallow tree point to the tree with larger height. That could also work. Let see why?  

What am I doing now? This is a tree of height h1, this is tree of height h2. h1 less than or equal to h2. I do this, I make root of h1 point to the root of h2. I am just showing you 2 alternative ways to do the same thing. Now, what should my induction statement will be? How will I proof, what is the claim I should try to make? What holds true?

So, a tree of height h; what should I try and proof? A tree of height h has at least, 2 to the h nodes. This is converse of that claim. There we were talking of a tree with so many nodes of height at least log of h. 

Here, if the height is h, then it has at least 2 to the h. At most, it has height at most log of n1, here we would write that the tree of height h has a large number of nodes in it, at least, 2 to 3 h. So, that means that you can never have a tree of height more than log of n, because if the height of the tree was more than log of n, then it will have more than n nodes in it. It is not possible because, there is only n node in the graph to be ? in a collection to be ? So, that will place a log in bound on the height of any tree. 

How will we prove this? Once again by induction, suppose, claim is true for all trees of height up to a cretin number and then when I do this linking, the resulting tree has height h2 max of h2 comma h1 plus 1 once again. h1 plus 1 will occur  only if h1 equals h2.

Now, this is the height of the new tree. Let say, h is the height of the new tree, this is this. Why is this h, why is the number of nodes, now, what is the number of nodes in the new tree? Equals n1 plus n2; number of nodes in this tree plus the number of nodes in this tree.

Now, we know that n1 is at least 2 to the h1, n2 is 2 to the h2. So, this quantity is greater than or equal to the 2 to the h2. That is one way of thinking of it and since, h2 is more than h1, this is also greater than or equal to 2 to the h1 plus 2 to the h1 which is equal to 2 to the h1 plus 1. So, the number of nodes is grater than or equal to max of 2 to the h2 comma 2 to the h1 plus 1. I can also write it as, max of this is 2 to the max of h2 comma h1 plus 1 which is 2 to the h. (hindi) Proofs are very similar, if you look at it carefully. You are just turning them around, both of these schemes can be used to do the union. 

What do they both ensure? Why does this ensure that the height is more than, no more than log n? because, if a tree has a height more than log n (hindi) 2 to the, a tree of height log n will have 2 to the log  n which has n node in it already. If it has height more than login n, if it will have more than n nodes, which is not possible.

(Refer Slide Time: 41:47)

 

So, from this argument, no tree will have height more than log n and this argument, we have already said directly that no tree has height more than log n, log of the number of nodes in the tree. Since the maximum number of nodes in any tree is at most n, no tree has height node the log n. So now, given this, how much time does union takes? 

Constant time? Yes, so something has to be done more and what is it that has to be done? In the root node, we have to keep the track of, either the height or the number of nodes, whichever it is. And, this is an information which is not difficult to maintain because, when I do an union, this value is updated to either the height, if you maintain the height it becomes the max or if you are maintaining the number of nodes, you just add this quantity that.

So, how much time does union take now? Constant time because, you just have to update this variable and do this reference update. How much time does find take now? I start from a node and keep moving up the tree. Since the height of the tree is never more than log n, find takes no more than log n time. 

Total time taken by Kruskal?s algorithm then becomes, time for sorting, m log n plus, how many unions did we say we need? At most n unions, every time I include an edge, I need 1 union. Since, each one of them is taking constant time, this is just order replay. How many finds do I need? For every edge, I may need 2 finds. 

So, m times log n. What is this? m log n (hindi) What is log m? (hindi)  n square? (hindi) value is 2 log n and minimum value (hindi) in the connected graph? log of n. So, log m and log n are the same things. log m is theta log n. They are in the constant. So, whether you write log n here or log m here, it is immaterial. They are the same quantities.  

(Refer Slide Time: 45:26)

 

So, I will, the next thing I want to do is to show you 1 way of improving the union find data structure. We saw the method, one way so, this data structure we said if you do union by rank then the time required for find improves. It becomes order log n. You can further improve the time required for find by using the technique called path compression whose analysis, we are not going to do in this class. But, you will do learn in your algorithms course.

What is the technique? So, it is just the following. You have your tree in which you are doing a find. (hindi) So, you went up the tree like this to do this find. Now the question is, why do not we do something at this point, so as to improve the performance of future finds?

You might have to find this node again? Yes, we will modify these things. We will make this parent pointer, point straight to the root and not just this guy, everyone on this path. Why are we doing this? Anyone? because now, this guys become closer to the roots. What is, I am not drawing rest of the tree but now, you can see that this node is this, this, let me put a dot here. This guy, connected to just 1 link to the root. Let me put a cross here and this last 1, without anything is this. Of course, there are sub trees hanging down here. They will continue to hang down here, because there could be other nodes pointing towards this so they will continue to. But when I later, so, what are we doing as a consequence of this? We are bringing the nodes closer to the root, thereby, reducing the height of the tree and that reduces the number of, the time required to a find. No, the root is not changed to this tree. The root of this tree remains as before. Union by height, you are worried about if the union procedure requires height.


The root of this tree will point to the root of other tree. No, these will not change then, these will not change then, we are not going to change these now. If I take the union of this with someone else that is okay. I did that, I am not going to change this pointer. So, if you are opting this procedure, then union for the union operation, you should not work with height. But, with number of nodes: you understand why? because we are changing the height of the tree by doing this.

We might be changing it in a manner which might be hard to recompute. After I do this compression, may be this was a node, there was leaf here which was the farthest from the root. When I change this, the height of the changes, but how do I now compute the new height of the tree in constant time? 

Very difficult, so we use the union by number of nodes. That is the procedure we are going to use. So, the metric will be which tree has lesser number of nodes. The tree with lesser number of nodes will be made to point to the tree with larger number of nodes or the root of the tree with less than number of nodes will be made to point to the tree, the root of the tree with larger number of nodes. Everyone follows this? questions?  

What we doing in? What are we doing in path compression? What we are doing in path compression is that, we say, when I am searching for this node, when I doing a find operation on this node, in any case I am going to travel all the way up. I am going to traverse this link and I am going to traverse this link and this link and this link. 

Why do not I do something now, which will make future easier for me? Future finds easier, lesser time, so, what I do is, that once I do this, I will may be make another pass of this and now change the pointers to directly point to there. We do not know the root, we do not know when to make the, no, but when I am doing the find, I need to know where the root is that is why I am doing a find in the first place. How do I know, the root point there. Only when I reach the root, I could know where the, what the root reference is. So, I need to make a second path to update.

Every time we do a find, we will update the pointer. Then there is no need. So, for here instance I did not change this. This is no point, what does it mean to change the pointed this. This we will change because, that gives us, the next time the picture is this, we will look at this picture and decide what to change and what not to. So, again when I go throw here, what is the point in changing it, because it is already pointing the ? This becomes the picture, I do not have this picture any more in the, the next type. How do we directly go to the nodes in this tree? We keep a reference. 
For every node, when I am keeping the node in this union find data structure, the node corresponds to a vertex of my graph. I do not just create the union find data structure as the standalone entity. I have to link it up with my graph somehow. For every vertex, I have 1 node here, I keep a kind of cross reference from the vertex in my linked list data structure or the adjutancy list data structure for the, in which I have the vertices, I would have the reference to the corresponding node in the union find data structure. So that when I have to reach a particular vertex, I can follow that reference and find out which connected component that vertex belongs to (hindi) 

So, with that we are going to end today?s class on the union find data structure and we are going to discuss another algorithm for computing minimum spanning trees in the next class. 



              Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering

Indian Institute of Technology, Delhi
Lecture ? 33

In the last class we looked at the union find data structure. That was a data structure to maintain a collection of disjoint sets, under the operations of union. That was the operations we were doing in the sets, that was the operation which were modifying the sets or modifying the collection and the find operation was just to identify, given an element, which set it belongs to. We used this, we needed this data structure to implement the Kruskal?s algorithm. Kruskal?s algorithm was the first algorithm we looked at for computing a minimum spanning tree in a graph. It was an example of the greedy algorithm. 

Today, we are going to look at another algorithm for computing the minimum spanning tree in a graph. This one is due to Prim and that is what we are going to discuss today.  So, let me define the notion of a cut in a graph first, for you. So, recall that we are talking of undirected graph. A spanning tree, the notion of a spanning tree is defined only for an undirected graph. For a directed graph, there are different notions. We do not say spanning tree in a directed graph. It is only an undirected graph we are talking about here. 

So, give you an undirected graph, a cut: let say, this is graph G and a cut in graph G is a partition of the vertex set into 2 parts. What does it mean? Let me partition, break vertex into 2 pieces. So let say, 1 piece is this and the other is remaining because, it is the partition. This way of splitting would define a cut. 

So, if for an instance, I had vertices, a, b, c, d, e. So, then the first cut I am considering is a, b one side and c, d, e on the other side. There could be many other cuts possible, I could have a, c and d on one side and b and e on the other side or I could have only 1 vertex let say, c an one side and the other 4 on the, the remaining 4 on the other side. These are all examples of cuts in the graph. How many cuts can there be in the graph? 2 to the power n? 2 to the n or something? 2 to the power n minus 1. because, this partition is the same as saying, c d e and a b. 

How did you come out with the number 2 to the n? Every element, for every element, there are 2 choices, either left or right. So, there are 2 to the n possibilities but then, but then, we are repeating. Each possibility is repeated twice. Once  we will say, a b go on the left side, c d e go on the right side and the other time we will say, c d e go on the left side, a b goes the right side. But the partition is the same. So, that is why number of different cuts, 2 to the n minus 1, minus 1 also. Okay, then null? You are talking of the null (hindi) Sometimes, what we do is when we say a cut, so cut is really a partition of vertex a. But I sometimes would also say, these edges which are going from 1 side to the other side are called the edges in the cut. So, these edges could be called the edges in the cut, edges in the cut or edges of the cut, whatever. You understand which edges I am talking about? Edges which have 1 end point in 1 side of the partition and the other end point in the other side of the partition. So, you understand what a cut is. You need this notion and how to we use this notion?

(Refer Slide Time: 5:53)

 

Suppose, I take a cut of my graph, some cut in the graph, S and I will denote other point, other side by S complement or V minus S where V is the vertex set. So, I have a graph G S V, the vertex set and e is the set of edges. So, S is the set on one side and the remaining is the set on other side. Now, let me look at all the edges which are in the cut. 

Recall, we are trying to compute the minimum spanning tree. So, I am now going to be taking of a particular property of the minimum spanning tree. So let me take this cut and let me look at the edges in the cut and let us look at their lengths of these edges. So, may be, this edge is length 2, this edge is 4, this is length 3, this is length 7. 

I am going to be assuming that all the edge lengths are distinct. This is just to ease all my arguments, to simplify my arguments. All the algorithms, everything works even when edge lengths are not necessarily distinct. So, just to simplify my presentation here that I am going to assume edge lengths are distinct. 

Now, the claim is that, so recall, when the edge lengths are distinct, there is a unique minimum spanning tree. We have discussed this before: there is 1 and only 1 minimum spanning tree. Now, the claim is that this edge which has length 2 will be a part of that minimum spanning tree.

Let me write down the claim more formally, for any cut, any cut S, S complement; the minimum edge in the cut belongs to the MST (hindi) 2 to the n minus 1 different cuts. 


(Refer Slide Time: 8:20)

 

(hindi) 1? for instance (hindi) let us make a tree such that there will be both of these edges will be part of the minimum spanning tree or part of the minimum spanning tree. (hindi) both the edges are part of the minimum spanning tree. I could have my edge length so that this was my minimum spanning tree. I could choose my edge length so that this is the minimum spanning tree. So, it is not necessary that only, for any cut, there is only 1 edge which is the part of minimum spanning tree, there could be more than 1 edge which could be a part of minimum spanning tree.

(Refer Slide Time: 9:26)

 

But, all we are claiming is that the minimum edge in the cut will belong to the minimum spanning tree (hindi) why? What is the proof? So, proof is by contradiction (hindi) you are saying that this is the minimum spanning tree but, that tree does not contain this edge. So, let T be a MST. MST is short for minimum spanning tree. T is an MST which does not contain edge (hindi) does not contain edge (hindi) which does not contain edge e. 

So, what now? What will I do? I will add e to the tree, add e to T. What will happen? Cycle will be formed. Why will a cycle be formed? Because, these 2 vertices are already connected, these 2 vertices are already connected in the tree. So, there is some path going from this vertex to this vertex. Any path that goes from this vertex to this vertex, from u to v which connects u and v has to use 1 of the edges of the cut.

Think of this as, this is the river in between, this is 1 end, this is the other end. If you have to go from this end to other end, you have to use 1 of these bridges, no other alternative. So, it has to use 1 of these edges which means that the cycle that gets formed has to use 1 of these edges at least.

But, all these edges has length more than 2. So, what will I do? Same thing, I will remove this edge from the cycle and at that, this will reduce the length of the tree. So, let me write that down. So, we add e to T, let me continue.

(Refer Slide Time: 11:59) 

 

So, after you add e to T, so let me addition of e to T forms a cycle. Say, capital C. C contains at least 1 edge of the cut. Yes or no? At least, 1 edge of the cut, this implies C contains at least 2? It will contain an odd number of edges. Added C, at least 1 edge, let me write down other than e, good. (hindi) e will have to be part of the cycle. It is because of the addition of e that cycle got formed. There was no cycle earlier, e has to be part of the cycle.
So, C contains at least 1 edge of the cut other than edge e. So, this implies C contains on edge of length more than the length of e. C contains an edge of length more than the length of e. They can be greater than ? I just have to show that there is some edge in the cycle which has length more than e. No, we created the cycle, we brought in e, now how do we want to reduce the cost of the tree? By removing some other edge whose length is larger than e. By removing this edge from T union e, we get a smaller tree.

That is a contradiction, because we assumed that T was a minimum spanning tree. (hindi) This we have discussed before that, from a cycle I can remove the any edge and it will remain a spanning tree. 

We can also remove that, I just have to show you that there is some edge in the cycle which can be removed. To just, I have to just eye with a contradiction. I am just arriving at a contradiction here, this is a mind game. We are not actually removing any thing, I am just proving a structural property. I am saying, in any cut the minimum edge has to be a part of minimum spanning tree.

I do not ever sit down and remove any thing. There is no algorithm does that. This is just a proof, proving this statement. (hindi) How do know that 1 of the edges of the cycle has length more than e? Because, we can only argue for the edges of the cut, for then we know for sure that they are all more than e. That is all, nothing more. Now, how are we going to use this claim?

(Refer Slide Time: 16:57) 
 
 

So, Prim?s algorithm exploits this simple fact: that, if you take any cut, the minimum edge in the cut will be part of the minimum spanning tree, always. Prim?s algorithm is an algorithm which essentially is built around this simple fact. (hindi) So, let us understand what Prim?s algorithm is. 
Let say this were my, this were my, what? Graph and let us give every edge length. So, I will just rapidly put down edge lengths, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, say, I missed out one, 13. Now, how does Prim?s algorithm work? First, we start from some vertex and we call this the root vertex. Now, the first partition that I am considering, the first cut I am considering is the root verses everyone else. Root of 1 side and everyone else on the other side. 

So, which edge has to be part of the cut? The edge of length 1, so, I will include this into my cut, into my which edge has to be part of the minimum spanning tree, edge of length of length1 and I will include it into my 3. Now, what is the cut I am going to consider? This and this, so I am going to, this is going to 1 side of the cut, this is going to be one side of the cut and the other side of the cut is going to be all the other vertices. 

So, which are the edges which are part of the cut? 9, 8, 2, 3, 11, so which is the smallest? 2, so we know for a fact that this has to be there in the minimum spanning tree, this included (hindi) What is the cut I am going to consider? These 3 vertices on 1 side and all the other on other side, so side let me extend it like that: can everyone see this? This is 1 side of the cut and all the other vertices are on the other side of the cut. 

So now, which are the edges in the cut? 9, 8, 7, 5, 13, 11: 3 is not in the cut, because both end points of 3 are on the same side. So, the smallest of these is 5. So, this gets included. Now, I am going to, you can now understand how I am going to extend it. This becomes my, 1 side of the cut and the remaining becomes the other side of the cut, 4 is in edge, 3 is some kind of a gone, so 9, 8, 6, 4 - no, not 10 ? 13 and 11 (hindi) 4 (hindi)

(Refer Slide Time: 21:20)

 

Which are the edges in the cut? 11, 13, 10, 6, 7, 8, 9; so, six is the smallest, 8 gets included and now, my set becomes this. It is time, we finish, because it is getting very messy. So, which are the edges in the cut now? Is 8 in the cut? No, it  looks like it is in the cut, but it is not because, both its end points are in the same side. So, it is 9, 10, 11, 13 also 12, of course. So, 9, 10, 11, 12, 13.

So, 9; 12 is not in the cut any more. So, it is only 10, 11, and 13 are the options, 10 is the smallest and we that we are done. Because, all the vertices are now included. Everyone understands the procedure, it is very simple. We have done the proof effectively. Not effectively, we have done the proof. Because, what did we say? We proved this claim that the minimum edge across the cut has to be part of the minimum spanning tree and that is the edge we are picking at every point. So, this is the minimum spanning tree. 

(Refer Slide Time: 22:33)

 

You will get the same tree, since this is unique, I assumed edge lengths are distinct, you will get the same tree when if you were to run the Kruskal?s algorithm. Everyone follows this? So, I am not going to write down the pseudo code for this, but you understand how the algorithm works. Very simple, so the key idea is that we have, so let us try and see how you would implement this algorithm? 

So, what is it that you have to maintain?  Of course, you have a data structure for the graph. Now, what it, what is the operation you have to do at each step? Add a vertex to a set? So at some, at any point you have the following; this is 1 side of the cut, your root is here. Let me call this set as S. So, the set is, this is going to be the set on 1 side of the cut, as in the vertices we have already reached, so to say, from the root.

So, the root is always the part of the set S and the remaining vertices, S complement of V minus S. I have to maintain this collection of vertices which are on 1 side. This can be done very easily by keeping 1 bit with every vertex. If the bit is 1 then that means, let say it is on the S side, if it is 0, it is on the S complement side. That is very simple. Now, what is it that I have to do at each step? I have to find out, I have to look at all of these edges and to find the minimum. How and ? I am going to do this? (hindi) you look at all of this vertices, you look at their adjacent edges, for each edge you see whether the other end point is in S or not. If it is not in S then you look at its length and you look at all these edges and find the minimum. So, how much does the time take? How much time does it take? Order, you are going to each vertex, looking at all its adjacent edges; looking at all its adjacent edges, going to each vertex in S, looking at all its adjacent edges. What is the maximum time it could take? Order m at each step and you have n such steps. So, order m in time (hindi) I am not even writing it down because it is a very ?. 

(Refer Slide Time: 24:56)
 
 

So let us do, look at something clever. What could be a clever way? Maintain a minimum? Let see, this is some idea: so, you are saying I have this set s, I have s complement and there is this vertex V, let us call it V.

I have a single this vertex out because this is the vertex which is now going to go from right to left. (hndi) You have figure out minimum of all of these edges, you know the minimum of these edges in the cut. Now, what is going to happen? V, let me draw it this way now. 











(Refer Slide Time: 27:53)

 

Some edges are going to go from v to S, some edges go from V to S complement, there are of course, some edges which go from S to S complement. Now, right now, I know the minimum of these edges because V is the same. This picture is the same as knowing the minimum of the edges. When I move V from here to here, I want to find out the minimum of these edges. What will I do? Compare the? So I just, suppose I have kept track of the minimum of these, noting else. Let say that the minimum was 1, the minimum has to be an edge incident to this one: that you understand? So, that is why we pick this one. This is 1, this is 2, this is 5, this is 6 and this is 7 and then these are 8, 9, 3 let say. 

Now, the new minimum, earlier the minimum was 1 and now the new minimum is 2. If I just know the minimum, it is not going to useful. So, you want me to keep the track of the second minimum also? Which does not go to? But, what do I know about V as in, V something that I find out. So, we know the minimum that is coming to this vertex. So suppose, we keep the track of the minimum to each of the vertices in s complement. Insert V? So, what he is saying is, but how to, what data structure should I use to keep these edges? 

A heap? Min heap? So, that could be 1 possibility. I have a heap, does everyone know what a heap is? So, heap is the data structure which will and which I can put some elements, each one of them has certain priority or certain key and it will give me, I can use an operation called delete min which will remove the minimum element, I can find out what the minimum element is in the heap in constant time, I can insert element into the heap, I can also remove element from the heap. I can do all of these operations and except for find min, all operation take in log n time. Find min takes constant time. Everyone remembers at least this much.

So suppose, I were to keep a heap here, a heap which contains these edges, 2, 1, 6, 5, 7. Then, using find min I can find out what is the minimum edge. That will, once I know the edge, I know the other end point of vertex, I know both the end points of the edge, I know which vertex I have to bring in. When I have to bring in this vertex, I look at all the edges incident at this vertex. 

The edges which are going in S, I have to remove them from this heap and the edges which are going from this vertex to vertices not in S, I have to add them into the heap. But, let us keep it clean. Like, when we say that our heap is going to contain the edges of the cut, let it contain only the edges of the cut, because you are going to spend the same amount of time in any case. We do not search in a heap, please remember, heap is a very bad data structure for searching. 

So, how do you find out where the edge is in the heap? Once again the same thing, when you put some information related to the edge into the heap, you do not just put it and forget it, you keep a track of where the information is, where the particular node is. We do not need to delete, but there is no harm in deleting also. No, you can also delete from a heap. Why cannot you delete from a heap? 

So, there is some confusion here. Let me back up a little bit. We need to, I think I need to show this delete operation to you again. When we do or the other class we will do delete operation once again but take it from me, you can also delete from the heap in the same log n time. Of course, to delete you need to know where the element is, you are not searching for the element, you know where the element is and then you can delete from the heap in log n time. 

So, that is what we are going to do. When this element comes in, I am going to insert 3, 9, and 8 and I am going to delete 1, 6 and 5 and I know exactly where 1, 6 and 5 are and this is how I will do the implementation. So at any point, what does the heap contain? Exactly the edges of the cut: exactly those edges and noting else. It is best to keep it clean so that you know what is happening. Now, how many operation are required on the heap? what kind how many?   














(Refer Slide Time: 32:29)

 

So, let us look at the heap operations. Let me draw the picture again for you. This is vertex V, it is coming in on this side. So, these edges have to be removed. These edges have to be added. Each remove and add takes log n time. How much time am I spending? Degree times log n for ever vertex that I process. 

So, degree of V times log n and then this has to be summed over all the vertices, because I will processes each vertex exactly once. Log n, number of elements in the heap, which could be m. So, I should write log m. Number of elements in the heap which is the number of edges in the cut, could be as much as the number of edges. All that it will ever may be that but it could be as large as then in the worst case. 

We are decreasing, but you know those are so small, that it will not going ? even if you do a careful analysis, you will still get the some order. This is what the total running time will be. These are not the only operations; I also what have to do 1 find min. How many times do I have to do find min? Number of vertices time; n times. So, (hindi) n into order 1, constant time, that is a small order time in this one.  

What else we have to do? What are the other operations you might have to do in the heap? Just insert and delete: and this operation, find mine. So, everyone understands what the procedure could be now? You start with your initial vertex, root and all the edges   incident to the root will be your initial heap, will be the elements of  initial heap. Then, you will do a find min that will tell you what vertex to include on that side. Then you are going to look at all the vertices adjacent to this vertex, all the edges that are adjacent to this vertex, beside, if you are to remove an edge or to add an edge. (hindi) Let say, I have an array S. S in an array, S of V equals 1 if V belongs to S, 0 otherwise.



(Refer Slide Time: 37:42) 

 

What is this S? This S is the S side of the cut. Not visited, but reached; all those vertices that have been reached. So, this is 1 data structure we are going to have. Then, we are going to have a heap and let see what, so initially, I pick a vertex as the root and S of the that vertex root equals 1 and for all V, S of V equals 0. Initially, everything this is my initialization. Only the root is on, so, initially everything is 0 and S of r is set 1. This is my root vertex. 
 
Now, what should I do? I should insert the edges incident at the root into the heap. So, what should I do? For all e incident to r, do H that is my heap, dot insert e. Some such thing, I have to insert the edge into the heap. So, everyone follows ? (hindi) so (hindi) while not, I would say not H dot empty. (hindi) do. Now, what should I do? I should find the minimum edge. H dot find min equals let say, this is equal to, what will this return? This will return in edge, (hindi) and now what should do I with f now? (hindi) 

I have to find the end point of f which is not in S. Let V be the end point of f such that S of V equals 0. (hindi) For all e, e adjacent to v do. What do I do? For all e equals V comma W (hindi) If S W equals 0 (hindi) then edge dot insert e, not w; (hindi) else H dot (hindi) this is the spanning tree, minimum spanning tree. (hindi) this entire thing is the part of the this loop. Clear to everyone? (hindi) As you can see, all the effort is being spent in this step or not too much actually in this step. How many times is this loop going to be executed? n times? 

How many times is this loop, the while loop is going to be executed? n minus 1 or n times? n minus 1 times. Why, because every time I execute this loop 1 vertex comes into the set as n vertices (hindi) So, this is getting executed n minus 1 time. So, this statement is executed n minus 1 time but, this is again just a constant time operation. So, this is not too much time. (hindi)

It is this way the most of the work is getting done. For every if you are looking at all the vertices adjacent to the vertex V, degree and for each one of those vertices or edges, you are spending, doing either an insert or a delete. Log n, degree times log n, you have seen that before, summed over all vertices will be m log n. There is a modification to this scheme that can also be done. 

(Refer Slide Time: 45:58)

 

In this modification, what you do is you have your set S, you have your set S complement. Each vertex in S complement has a number on it. What is this number going to be? Let us look at all the edges going from this vertex to S. Suppose, these were the 3 edges going from vertex V to S, suppose these edge lengths are 3, 4, and 7 then this vertex is going to have a number of 3 return ? (hindi ) 

What are we going to do with these numbers? We are going to create a heap. Now, my heap is a heap of vertices and not of edges. So, this heap has 1 element for each vertex in S complement. (hindi) What will be the minimum element in the heap? It will be the vertex which has the minimum edge incident at it among all the edges in the cut. So, it will be the vertex which has to go across. (hindi) 

If I find the vertex the vertex which is sitting here at the top, the vertex with the minimum value, minimum label, let us call this labels: the vertex with the minimum label let say, it is this vertex (hindi) so, that means that the minimum edge going across this cut is 1. We are not keeping track of edges which are here, within here. This vertex will only contain a label which is equal to the minimum of the edges which are going across the cut. If there is a vertex which has no edge going across the cut, incident at it (hindi) so the minimum will tell me which is the minimum edge going across and that is the vertex that I have to move across and when I have to move across the vertex, what do I do? How do I update information? Very simple, this is vertex V (hindi) all those in S bar which are adjacent to this vertex. (hindi) 

The minimum edge that is coming to this vertex, so far is 4. But now, this edge is also coming to this vertex, because this vertex going on that side (hindi) So, that is the only things we have to do now. So, what is the operation we have to do on the heap now? Update what? Update or decrease? We will only decrease the label, decrease priority. Decrease priority is the operation we have to do. Decrease priority (hindi) no insert, no delete. Find min, of course and decrease priority (hindi) since, I have 4 or 5 minutes, let me write down the code for even this 1. (hindi) 

(Refer Slide Time: 52:23) 

 


So, once again I will, as in the previous case I will have my array S, I will do the same initialization; for all vertex V, S V is initially 0 and S of r is 1. Now remember, the heap is going to contain vertices in S complement. H is going to contain vertices in S complement. What should I put initially in my heap? Everything? Except r? So for all V, S V is equal to 0 and let say, I do H dot insert V comma (hindi) distance from r, what does that mean? What does that mean? Length of the edge from r? Infinity (hindi) 
       
H dot insert (hindi) No, no, everything is infinite only r is 0 (hindi) while H is not empty, we will do something. What will we do? We will take the minimum let say, that is the vertex V. I will take the vertex (hindi) for all W adjacent to V do. If W is adjacent to V what should I do? If S W equals 0, what does it mean? W is on the S complement  side. Then what should I do? Then I have to update its priority. Then if what should be the new, what is the option here? 

If, we will need a label array somewhere. If label of W (hindi) which is keeping track of all the labels, so if label W (hindi) is greater than length of V comma W then that means that this edge now, so what is it, what are they saying? (hindi) then label W equals 6, sorry, equals length V W and H dot decease priority W comma label W. (hindi) 

So, this is an alternative way of doing. So, the same running time complexity. You will check that yourself. Why is that? because, once again we are looking at degree, for spending degree time at each vertex and degree times log n, because decrease priority also takes log n time.

(Refer Slide Time: 10:04)

 

Some over all vertices will come m log n and that is basically where most of the work is getting  down (hindi) so with that I am going to end today?s lecture. So, we looked at Prim?s algorithm for computing minimum spanning trees and we saw 2 ways of implementing it.



               Data Structures and Algorithms 
Dr. Naveen Garg
Department of Computer Science and Engineering

Indian Institute of Technology, Delhi
Lecture ? 34
Today we are going to be talking about the single source shortest path. In the last class, we looked at the minimum spanning trees, Prim?s algorithm for computing minimum spanning trees. This is a different problem and I will soon tell you what the problem is? What we will, how you will find is that the algorithm is strikingly similar to Prim?s algorithm. So, let see what is the problem is and what the algorithm in this problem does. 

So, once again you are given a graph and today we can assume that the graph is directed. So, this algorithm or this problem is well defined even for directed graph. I am going to assume that the graph is directed. Let us take a graph, each edge has a length l, is a function from the edges to positive ? non negative, we can have the edges of length 0 also and let us give ? Let say, this edge has length 2, this is length 3, this is length 3.1, this is length 7, this is length 0.5, this is length 1.1, this is length 5.

These this could be a network of let say, the roads in a city. So, each length so the length corresponds to the distance from this end point to this end point and let see these are 1 way roads. I will, at the end of this class I will tell you other applications of this problem, the ton and tons of that. What is the problem? The problem is to find the shortest path. 

Let say, I want to go from this place S, S is also the, also called the source; I want to go from S to t, t stands for termination, no destination. Why should t stand for destination? You will figure out yourself. 

(Refer Slide Time: 4:03)

 

I want to go from S to t and clearly I would be interested in taking the shortest path. How many different paths are there in this graph between S and t? 4 only? So, there is 1 path I go like this, then I go like this, then I come back here, that is 1 path. The other path is I go like this, I come down, then I go like this and then I come down, this is a second option. The third option is that I go like this, come down, I go straight, this is third option. Fourth option is that I go 2 and then 7 and then 1 other option is this, this and this. 

Is there 1 more? 0.53? 3, I cannot come back, I cannot come back on this. This is not a path because that this edge is directed in that manner. It would have been a path if this was an undirected graph but not in this graph. So actually, this graph has only 5 different paths. So, it is a simple problem to compute the length of each path and take the minimum. So, what is the length of this path? 8.6. Length of this? 7.7. What is the length of this? 2,3 8.1. This is 9, this is 8.6. If these were your GPA?s, you would have preferred this. But clearly, we are interested in the shortest path. So, we would like to take this path, 7.7 because it is the shortest path, shortest way to get to a destination. 

(Refer Slide Time: 5:49)

 

So, can we always do this thing, as in list out all the paths in the graph and just take whichever is the shortest, compute the length of each part and then compute the shortest? Not feasible? Why is not feasible? Not practical, why is not practical? What is impractical about it? High time complexity, why high time complexity? Why should there be, may be a graph on n vertices never has more than n paths. Then it is simple matter. Just list out all your n paths and just compute the length of ? If it has a cycle? Will you ever go along a cycle? You are looking for a shortest path. Why would you want to cycle? 

So, you can ignore ? not when you are listing out your paths you will try ignore cycles. So, it is not feasible but that is because, the number of paths can be very large. What do you mean by very large? Let us look at an example. Very large means exponentially varying. This is my graph, edges are all directed left to right. You get the idea, this is S and this is t. Sorry, there are more such diamonds, this is t. 

If there are n vertices, how many such diamonds are there? How many such diamonds are there if there are n vertices? Or let us do it the other way round. If there are k such diamonds, then there are, how many vertices are there? k diamonds would count as 3 k plus 1. Just do your count correctly. With each diamond I associate this vertex, this vertex and let say, the left end. I cannot associate both because ?   So, it will be something like, 3 k plus 1 vertex. How many path are there between S and t? 

2 to the k, why? because for each diamond, I can either take the top part or the less bottom part. So, there are 2 options in each diamond. So, I can have 1 path which just takes the top part always, 1 path that takes (hindi) 2 to the k different paths between S and t. Everyone follows? (hindi) so, k is roughly n by 3. So, this is 2 to the n by 3 paths roughly. 

How large is this number (hindi) 2 to the 30 is the number of particles in this universe. (hindi) that is the number of particles, roughly. Of course, no one has counted it, this is an estimate and 30 would be achieved for a graph of size 90, n equals 90 which is not a, which is a very small number, actually. These graphs are typically huge when you are trying to compute shortest path system. So, it does not make any sense to do this caliber, to follow such an approach to compute shortest path. 

What should we do? How can we solve this particular example? Can you think of a way of solving this example? Of course, there are lengths on all of these edges which I have not written down but you can imagine; 0.5, 1, 1.1, 0.2, 0.3, 0.4, 0.8, 0.5, I have just taken some arbitrary numbers. What is it that you can do in this example to figure out shortest path between S and t? 

Compare, so what you are saying is here, I am going to see, 1.5, if I take this term, thing and 1.3, if I take this one, so I will go bottom and so I would have reached this point now. And then from here, I have an option of going 0.7 up or 1.3 down. So, I will take this one, now I would have reached this point. 

So, what you were essential saying is that to reach this destination, I will first have to get to this, then I will have to get to this, then I will have get to this and so on and on. So, first I will have to go from here to here. So, I can choose the, there are 2 options and let I can choose the better option of going from here to here. And then, I have to go from here to here, there is no other possibility in this graph. So, I can choose to better option and so on and on. 






(Refer Slide Time: 12:12)

 
                      
Now, this works in this example, not to always. I have to get to this vertex and I am staring from here. So, but I do not know which is the first vertex I should get to from here. There is no such clear demarcation. As in saying that if I have to go from here to here, I have to clearly go from this vertex to this vertex. We cannot say such a thing in ? So, if I have to go from here to here, I cannot say, which is the first vertex have to get through. May be, I have to go through this vertex, may be not: cannot say. Everyone with me? So, that is the part of the problem. So any ideas, how will we do this?

(a to be must contain all shortest path but never part of the part if x is the part of the part from a to b then a to x is the part of the part form the  to b then a to x also be a shortest path we can you some shortest  heap improving the path)

That is an important property of a shortest path. So, this is 1 property we are going to be using. So, what he saying is, suppose this is the shortest path from S to t, this is some path in the graph. There are some intermediate vertices, I have not drawn. Let say, x is one of these vertices. Then the part of this path from S to x, that means this part of the path. What I am referring to is, this part of the path, is the shortest path from S to x. This is also the shortest path from S to x.










(Refer Slide Time: 16:22)

 

Why? This is, we are saying, this is the shortest path from S to t. Why should it be also the shortest path from S to x? x is any arbitrary vertex. We are trying to arrive at the contradiction, you are saying. Suppose, there were a better path between S and x, (hindi) So, then the claim is that the shortest path between S and t is this blue path from S to x followed by this green path from S to t and there was nothing particular about x. Any vertex any vertex on this path, the shortest path from S to that vertex is this path. (hindi) By the way, the blue path (hindi) so, this is also a shortest path from S to x. So, that is all you are saying, this is also (hindi) this part of the path is also a shortest path from S to x. (hindi) 

(Refer Slide Time: 17:44)

 

So, what does this suggest? This you know, this is giving you the following idea (hindi) we will end up having to compute the shortest path from S to many other vertices, all the vertices on the path. So, what the algorithm we are going to discuss today is actually going to compute the shortest path from S to every vertex in the graph. Not just t, in fact, what we are going to do is, forget t. First step of the algorithm, forget t and just compute the shortest path from S to every vertex of the graph. 

Once you computed the shortest path from S to every vertex the graph, you also got the shortest path S to t. (hindi) So, we will compute the shortest path from S to every vertex in G. (hindi) So, this problem is also called as single source shortest path problem. Why? Because, we are starting from 1 particular source and from that source we are computing the shortest path to every other vertex in the graph. Sometimes this would be abbreviated as triple s p. (SSSP) single source shortest path. 

(Refer Slide Time: 19:43)

 

Now, given that we are not just interested in the shortest path from S to t but, the shortest path from S to every vertex in the graph. Let see, what we can say about this example. We are interested in the shortest path from S to every vertex in the graph. (hindi) and without using your head too much, which vertex, you know which vertex or the shortest path from S to which vertex is immediate? Adjacent ones? minimum of the adjacent ones. 

We know by looking at this, we know that the shortest path from S to this vertex is 0.5. But this, the shortest path from S to this vertex is not 2. So, we cannot say that I know the shortest path from S to every adjacent vertex. But we know, for a fact that the shortest path from S, so if these are adjacent edges or the 3 adjacent vertices and suppose these lengths were l1 l2 l3. Let us assume l1 less than l2 less than l3, then I know for a fact that shortest path from S to V1 is l1. Why? Why do I know this for a fact? 

Any other path, since it has to start from S, it has to take some edge out of S, either it takes this edge or this edge or this edge. If it takes this edge then the length of the path will be at least l2. Why? because edge lengths are all positive. Why negative (hindi) has to be at least l2 because this part of the length, path is only non negative. It cannot be negative. Similarly, (hindi) so the length of the path has to be at least l3. 

So, what does that mean? The shortest path from S to V1 is l1 (hindi) so just by looking at this I can say that, so I am going to use d to denote distance. So, distance of vertex V1 I know is l1. What can I say about the distance of vertex V2? d of V2, what can I say about that? greater than? greater than l1? Greater than l1 and less than or equal to l2. Let us that put down and similarly distance of V3 is less than or equal to l3 (hindi) we do not know the correct distance. But, we have an upper bound on the actual distance. This is an upper bound that this actual distance is only less than this value. 

(Refer Slide Time: 23:44)

 


So, what are you going to do now? We know that actual distance of V1. Now, how can we extend it? How can we find out the, we know I have to find out the distance from S to every vertex. (hindi)

(initially the directed length we have each vertex have vertices the shortest path have length of  the shortest path found nay point if you at vertex found with visited vertex then compare to the are in the length already stored in the vertex to the some of the path) (hindi) 
  
I am just taking an example and soon we will concretize this algorithm. So, do not worry too much about it. Let say, V1 V2 V3 (hindi) l1 l2 l3 edge lengths (hindi) Let me now replace them by numbers because ? otherwise, we will keep having lots of ls. So, let me put down numbers, 0.7 and say this is 1.1 and this is 2.3. So, l1 is the smallest, let us put down more numbers. (hindi) 

Let us try and understand what I have done here? Just to separate this out. So, we know this, let me circle it. We know the correct distance of vertex V and the correct distance of vertex V1 is 0.7. Now, looking at the picture can I say, so this is the only, you know there might be other edges and so on between theses vertices, I have not shown those edges. I have just looked at the edges going out of V1 now and there are 5 edges, 4 edges going out of V1.

Now, looking at these edges can I say for certain, what the shortest path to some other vertex? I am not saying to this specific vertex. What is the shortest path, I just want to know the correct shortest path to some 1 vertex. No, you are going into details. Let us understand what the idea is. Let us look at vertex V2 or let us start by looking at vertex V3 (hindi) we had seen that (hindi) we had seen only this edge and we had said that the distance of V3 from S is less than 2.3. That is what we had so far, it less than 2.3. Now, can I say something better? Because, now I see this edge from V1 to V3, I know the shortest path from S to V1, what is it? 0.7. I see that the length of this edge is 0.3. So, 0.7 plus 0.3 means 1. So, I have found another path from S to V3 of length 1. I can say that the distance of V3 from S is less than or equal to 1. 

Now, let us look at vertex V2. 1, we will come to why it is exactly, 1 step at a time. All I will say is V3 has a, the distance of V3 from S is less than or equal to 1. That is all I can say at this point, till I bring in some additional argument. So, the distance of V3 from S is less than equal to 1. That is all I can say. What can I say about the distance of V2 from S? Well, I have found another path to V2. That is the path which goes from S to V1 and then follows the edge V1, V2. But this path, any such path is going to have a length of at least 1.3. In particular there is only 1 path at this point but this has a length of 1.2 and since earlier I had a length of 1.1, well, I have not found a better path to V2. 

What can I say about V4 now? Well, V4 (hindi) there was no path from S to V4. Now, I am seeing a path from S to V4 whose length is; well, this is an edge from V1 to V4, so first I get to V1. I know the shortest path is from S to V1 that is of length 0.7. So, this is 1.1 and distance to V5 is 1.3, 0.7 plus 0.6.

So, these are my new upper bounds in the distance. I call them tentative distances. This of course is fixed, final. This will not change. This was the shortest distance, this can never reduce but the other distance is might reduce as the algorithm proceeds. For instance, this distance, this tentative distance which was earlier 2.3 has now reduced to 1.1, 1 sorry. 

This did not reduce. This earlier was not reachable at all. Let say, it was infinite earlier. Now, it has 1.1 and this also became, came down from infinite to 1.1. Now, what I am claiming is that let me look at the smallest of these which is 1 and this is the correct distance of V3 from S. (hindi) At least 3 edges? No, we cannot say such a thing. 


(Refer Slide Time: 32:48)

 


Conversation (hindi) This is what, so let us first say what the algorithm is and we will have to prove its correctness and we will have to look at its implementation and we have 3 lectures do it. So do not, let us not rush ourselves. This is not a straight forward algorithm but let us understand what the algorithm is first. So, once the algorithm in grained then we will see what the, why it is correct. So, what is my algorithm now? So, my algorithm is going to have a set S of vertices (hindi)

What does this set denote? This set is the set of vertices to which I have found the shortest path from little s. (hindi) so this, what is going to happen in this algorithm is with every step of algorithm, I would have found the shortest path to 1 more vertex. When I say I found the shortest path; I mean these shortest path, not an upper bound or any such things. I have found the correct value of the shortest path from S to that vertex. (hindi)

I would include 1 vertex into the set. At every step I will include 1 vertex into the step. So at any point, this is the set of vertices which I have found the shortest path to and this is the set of vertices for which I have not yet found the shortest path. I just have an upper bound on the value of the shortest path. I know that the shortest path less than this number. But, I do not really know, what is the correct value of shortest path? 

So, each of these vertices has an upper bound. This is the vertex V, let d of V, so, let me write down; for all V in S complement, d of V is an upper bound on the length of the shortest path from S to V. You understand what I mean by upper bound? It is a quantity which is only larger than the length of the shortest path. It can never be smaller than the the shortest path and for all V in S, d of V is the length of the shortest path. (hindi) 

I am just telling you, what are the various invariance the algorithm is going to ? (hindi) These quantities would be upper bound and this could be the actual values. Now, I will tell you how do I figure out, how do I move 1 vertex from here to there? Which is the vertex I move from here to there and what happens when I move that vertex? So first, which is the vertex I will move from here to here? 

(Refer Slide Time: 37:27)

 


So, I find the vertex in S complement for which d is minimum is moved to S (hindi) I have my S complement, I have my S, I have these tentative distance on all of these vertices. (hindi) I will move that vertex here (hindi) this is the vertex which will move from S complement to S (hindi) So understand, what moves from S complements. Now, what happens when this move? When this move is down? How do these numbers change? So, how do we update? That is what we have to determine now. (hindi) Let say, I am looking at this edge, (hindi) from what we have said, the right distance from little s to this vertex W will now become 1.1, will be 1.1. 

So, I know that from S, I can reach W using the path of length 1.1 which means that I can reach this vertex by using the path of length 1.1 ? 3, 4.1. So, I can change this 9 to 4.1 now and this is my new upper bound or the length of the shortest path from S to this vertex. So, this is how I am going to update these labels. These ds are also called distance labels. (hindi) I have not told you, you have a slide ? of why we are doing? What we are doing? But, we have not discussed why it is not correct. We will do the correctness for this. So, do not lose heart. I first want to make sure that you understand what the algorithm is.(hindi) 






(Refer Slide Time: 41:34)

 

Understanding will be complete after we write down the pseudo code. So, let us do that. So this is going to be almost exactly like Prim?s algorithm with a very small modification. So, you can now start telling me what I should do? What should I do? What should my initial values of distance labels be? Infinite, distance label recall, we said is an upper bound on the length of the shortest path from S to that vertex. Since, initially I have no clue what those things are, I put every vertex, I give every vertex distance labels of  d of V equals infinite and d of S is 0, distance of S is 0 because it is ?

Now, what should I do? I should take vertex with the, at any step what do I do? I take the vertex to the smallest distance labels. So, all of these guys should be put into a heap. So, for all V belonging to V, dV equals infinity and H dot insert V comma dV. I have inserted the node V with priority, dV. This guy also I am going to insert. H dot insert S comma 0. So, the minimum (hindi) we have already inserted, you are saying there. Let say decrease, let us call it decrease priority. (hindi) 

In my heap the minimum, the top element will be S. I should now remove this element. What should I do? I should remove the minimum. Did I also remove the minimum in the case of Prim?s algorithm yesterday? or we just did the find min? That was a mistake. Correct that in your notes. We do not just have to do a find min, we also have to remove it from there. because recall, the only vertices that we are keeping in the heap all the vertices on the S complement side. So, let us look at this and you can then go back and look at your Prim?s notes again. 

So, what should I do first? I should remove the minimum element. So, H dot delete min and let say, this is vertex V. So, I get the minimum element, the vertex with the minimum priority and that say let say that vertex V. What should I do now? This I got as the minimum vertex, I should look at the edges, I should look at the edges going out of this vertex or the same as saying, let me look at the adjacent vertices. 

So, for all w, let me just say, out adjacent so it is complete clear what I am saying, out adjacent to V do. So, I am looking at this vertex W which is out adjacent to V. Now, I have to update its label. When should I update the label of W? W already has a label. What is that label? That label is an upper bound on the shortest path. Basically, that label is saying (hindi) now we are seeing a shorter path. But, what she is saying is that will we, we should update W only if it is in S path. 

(hindi) even if I were to not to worry about that, the point is I have that I have already found the shortest path to W. If W were in S that means I have already found the shortest path to w. Then there is no way it is getting updated now. Because, if it is getting updated; there is something seriously wrong with what I am saying. Think about this. We do not really need to maintain this information in this particular case. 

So, when should I update W or distance label of w? If I find a better path to W and what would be the better path? Through V, so d of W equals minimum, d of W comma, d of V plus l of V W. This is the other path I am saying: to W and if this is shorter then let us put this as the new level. Everyone understands (hindi) because S complement (hindi) the only vertices in the heap are the vertices in S complement (hindi) H dot decrease priority, W comma, this is together (hindi) 

This statement would not be executed (hindi) you can do it more carefully this implementation, only if d W decreases, should you call this, but waste of time. You will have to follow a few pointers to get to be heap, make a comparison all of ? (hindi) So, we have done. At the end, where are the shortest path? Sorry, what have we found out? We have only found out the length of the shortest path and they are all setting in d. 

(Refer Slide Time: 49:54)

 

We will come to that, we will come to that (hindi). It is not very different from Prim?s. (hindi) We do not have to maintain S and basically (hindi) this is similar to the version of the Prim?s, the second version of Prim?s that I talked about. (hindi) You recall the second version of Prim?s. What was that? 

We had maintained S, S complement and for every vertex in S complement, I had 1 label. What was that label saying? Look at all the edges going from this vertex to S, the minimum of these edges would be the label here. Now the label is saying, completely something completely different; in the case of Dijkstra?s algorithm. This was Prim?s algorithm. 

This label, label V equals minimum of (hindi) l1, l2, l3, minimum of these 3 area. Now again, we have a similar thing. We have, each of these vertices has a label. We are calling that label d now. But d is not the minimum of these 3. It is not that, d is not the minimum of l1, l2, l3 (hindi) d has some other semantic which we will see later. (hindi) 

Can someone hazard a guess here? What did you think is d? Minimum of, min of S, the value of d V is min of d W1 plus l1 comma d W2 plus l2 comma d W3 plus l3 (hindi) All these vertices are part of a heap. Priority is this label here and the priority is this d value here (hindi) We will see why this is true. This is we will see when we are doing the correctness of the procedure.

(Refer Slide Time: 54:22)

 

Who can tell me what the running time is? V square log V? Why V square? I understand, but why n square log n? Second summation degree log n (hindi) we are spending time proportional to the degree of this vertex or the number of times we are calling this loop. Time will be log n times that because of the decrease priority operation. Because, each decrease priority operation takes time at most order log n and (hindi) because up this operation corresponds to removing the vertex from S complement (hindi) each time it takes log n time, so it will be n minus 1 time log n.

This is the now realize, this is the mistake I made in the Prim?s algorithm. (hindi) because the semantic is the same. The vertices in S complement are the ones in the heap and so when I am moving a vertex from S complement to S, I have to remove it from the heap. So, you have to do a delete min operation. We just said, find min. If you do not do, it will be an infinite because the same thing will coming as min, min, min, min. (hindi) 

So, this total running time, let me put it down. No, not n, but m. Not n, but m why because, we have the total time, the total number of times this operation is being executed is order to m (hindi) With that we will end today?s discussion on shortest path, but we will continue this because, we have to prove the correctness of this algorithm. We also have to see, how to find the shortest path because, here we have only computed length of the shortest path from S to the root. 
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 35
Correctness of Dijkstra?s Algorithm

In the last class we looked at dijkstra?s algorithm for computing the single source shortest path. The algorithm I do not recall if I did tell you the name of the algorithm. It is called dijkstra?s algorithm. What remains to be done? We have seen what the algorithm is. Hopefully you all understood, I am going to be revising it any case today. We had also argued how to implement the algorithm and what its running times would be. The implementation was very similar to prim?s algorithm computing minimum spanning tree with some small modifications. But we have not argued why the algorithm is correct? Why the dijkstra?s algorithm is correct?

As a quick recap of what is being done in dijkstra?s algorithm. We are given a source vertex, we are given a graph. You are given a length function on the edges and the edges are non negative. You can have zero edge lengths but you cannot have a negative edge length. That is important and we will see why. What is happening at each point? With each vertex we have a distance label associated with that vertex d[v]. Today we have to first understand the semantics. What do these numbers represent? Initially d[v] recall is infinite for every vertex. d[v] is an upper bound on the distance of v from s. And this quantity with every step only decreases and it never increases. 

At a particular step what does the algorithm do? At any point we have two sets, the capital s and s complement. The vertices which are in s, for them we have computed there actual shortest path distances from s. We have already computed that. And the vertices which are in s complement, for them we do not know the correct distance. We only have an upper bound on the correct distance. At a particular iteration what do we do? We take the vertex for which the d[v] is minimum. 

Suppose this is the vertex u, so d[u] is less than or equal to d[v] for all v in s complement. (Hindi Conversation) signifying the fact (Hindi Conversation). And then we are going to move u across. You will go from s complement to s. We will also have to make this claim to argue correctness. That is at this point (Hindi Conversation) that is the correct shortest path distance of u from s. When we move across how do we update the other labels? If there are 3 edges going out of u to other vertices in s complement (Hindi Conversation) What is the new distance label of w? min d[w], d[u] + length of the edge [u, w].  







(Refer Slide Time: 06:38)

 

This is how we update the distances (Hindi Conversation). As you can see this operation shows that d[w] value will only decrease, it cannot increase. Because it is the minimum of two quantities one of which is d[w]. d[w] value will only decrease. So if these distance label were sitting in a heap then all you have to do is to do a decrease priority operation on the heap. When you are removing the minimum that correspond to the delete min operation. This is just a recap of what we have done in the last class. We have seen all of this before. (Hindi Conversation) that is the algorithm we keep doing this, till every things moves from here to here and then we are done. What is the initial composition of s and s complement? Initially the only vertex in capital s is little s and every thing else is in s complement. 

Let us first understand given a vertex u, what does d[u] means. Given a vertex u in s complement, this is s and little s is here (Refer Slide Time: 06:50). What is the meaning of d[u]? What connotation or what semantic can I associate with d[u]? Shortest length of a path including vertices only from? yes. So claim is d[u] is the shortest path from s. Let us look at all path from s to u which use only vertices of s and at one point that take one edge across and reach u. (Hindi Conversation) 

There can be many such paths. This is one path may be there is a second path which goes like this. There may be a third path which goes like that and so on. There could be again a whole lot of such paths. Let us compute the length of all those paths and let us take smallest amongst those. That is d[u], so formally I could try d[u] = length of shortest path from s to u that includes only vertices from the set s. (Hindi Conversation) except for u which is the last vertex, the other intermediate vertices will only be vertices from s. That is the meaning of d[u], that I clime is the meaning of d[u] and let see whether this is really true. 


How will we prove that this is true? The best way to do this is to prove it by induction. Make sense to try and prove it by induction. We will assume that at this point this is true. The numbers or that d on these vertices do reflect this quantity. And then we will say when I move one vertex across then the new d?s on the vertices continue to reflect the same thing. And then we will show that the base case is also true and that will establish this claim. If it is true now I move a certain vertex across. Let say vertex w which is here, moves across. As a consequence I might end up updating the distance label of u. (Refer Slide Time: 10:14) so this edge is not there, this is just signifying that this vertex moves across. Let say u1, u2, u3 are the 3 neighbors of w that is out neighbors.  

So we are going to update u1 to d [u1 ] = min [d [u1], d[w] + l [w,u1 ]]. This is how we are going to update. Why is this correct? Why is it that after this the new value of d [u1]? May it is the new value, may be it is the old value, still satisfies this thing. If d [u1] remains unchanged then that means the earlier value was the smaller one. So what is that happening? The w moved across, let me draw the picture s complement minus w and s+w, w is here, and u1 is here. Not another path, but I should say let me look at all the paths from s to u1. Now those paths, what could they be doing? So I have to look at all path form s to u1 which only includes vertices from this side. (Hindi Conversation) Either the paths do not include vertex w at all and they go directly like this but then the length of such a path is already captured in the earlier value of d [u1 ]. The d [u1] is smaller than the length of all such paths or such a path includes w. One possibility is that it includes w, that it visits w and then goes to u1. 

Third possibility, it goes to w goes to some other vertex and then goes to u1. Let us look at these 3 things. We are saying that either it does not visit w at all which is the first case. Then what remains is, it visits w. It touches w. What touches w? We are looking at the shortest path from s to u1 which includes only these vertices. 

(Refer Slide Time: 13:51) 

 

So the other case is that it touches w. And the moment it hits w after that it goes directly into u1. This is the second case. Third is it comes to w then it goes of some where else that it goes to u1. You are claiming that the third case cannot happen. What do you mean by cannot happen? The claim is that this path has length at least as large as d [u1]. 

We have to look at these three cases one after the other. The case one is covered. Case two is also covered. Why because this path comes up to here (Refer Slide Time: 15:06) and then I take this edge. If this is the shortest path from s to u1 and it goes via w like this then this part has to be the shortest path from s to w (Refer Slide Time: 15:26). That is equal to of length d[w] by induction hypothesis. And then this plus this length would be the length of this path from s to u1. That is getting captured here. We are accounting for the length of such a path. What remains is this case three. Why cannot we have this? Let say this vertex is x. You are wrongly concentrating on w, it has to be x. We have to look at x. The x was already there. We already knew the shortest path from s to x and that was equal to d[x], the label on x. It does not involve w, we already have that information. 

If this is the shortest path from s to u1 then s to x is also the shortest path from s to u1. What is the blue line that I have drawn? This is this the shortest path from s to x. The x was already a path of s which means we already had the shortest path from s to x. And the shortest path from s to x is using only vertices of capital s, not using w at all and that path sits here. If this path which goes from s to w then from w to x and then from x to u1 is the shortest path from s to u1 using vertices only of s + w. Then this path from s to x followed by x to u1 is also the shortest path. If it is also the shortest path then we have already accounted for it in d [u1]. d [u1] is only less than this quantity, because what is d [u1]? d [u1] till now is the length of the shortest path from s to u1 using vertices only of capital s. So this path that is now I have created does not use vertex w at all. So it becomes something like case one, except that you have to make this argument.  

(Refer Slide Time: 18:52)

 

So may be all of this is not very clear. So let us try and do it more completely. We are looking at s, u1. w is the vertex which has just come in. And we are looking at third case, when shortest path from s to u1 goes via w to a vertex x and then it jumps across to u1. Now the length of this path, which is s to w plus w to x plus x to u1. The claim is that the length of this path is at least as large as the length of this path from s to x, followed by x to u1. Let me write it down, length of path s to w to x to u1 is no less than that of the path s to x to u1. 

What is the path from s to x, I am taking? The shortest path from s to x, that does not include w. Shortest path from s to x does not include w. Why, because w just came in and x was already there. By our induction hypothesis we had already found out the shortest path from s to x. That is the path which is lying completely in s. Let me write that part also. The shortest path from s to x does not include w and only includes vertices of capital S. This is capital S + w now, this is capital S complement minus w. Capital s was the part before we moved this thing. This path which is going like this (Refer Slide Time: 21:28), has length at least as large as the length of this. But what do I know about this? About this I know that d [u1] is less than or equal to this u1, length of path s to x to u1. Already I know that, because d [u1] was the smallest possible path which used only vertices of s. This is only less than that and since this is less than this, d [u1] is less than or equal to this longer path. So I can effectively ignore this thing, I do not have to consider it, because it is already included in my d [u1]. d [u1] is already a smaller quantity than that. So all such paths are captured in d [u1]. d [u1] is the quantity only smaller than such paths. The one path which is not captured is such a path which goes to w and then takes an edge out that is case 2. For that we have included here. And this smaller of these 2, will give me the smallest path from s to u1 which only includes vertices of s + w (Hindi Conversation). 

(Refer Slide Time: 23:24) 

 


That is a semantic and once you have the semantic in mind then it is easy to prove, most of the other things. What is the meaning of those distance label? Once you have that in mind every thing else are very simple to follow. Are every one convinced that d distance label reflect this kind of length. Why are we justified in moving the smallest label vertex from here? That is what we are doing at each step. At each step I am taking this vertex which has the smallest label and moving it across and claming that it has the right number on it. Why am I justified in doing this? What we are claming is d [w] is the length of the shortest path. What is w? w is the vertex which has the smallest label. 

The w is the vertex with the smallest d value in s complement. The claim is d [w] is the length of the shortest path from s to w. Why is this claim true? Proof by contradiction, suppose this is not true. If this is not true what does that mean? That there is a shorter path form s to w. Shorter than what? Shorter than d [w]. What will that path do? That path will visit some vertices here (Refer Slide Time: 25:47) and at some point it has to jump across. And then it has to come to w, may be it jumps back and then comes again. It can do a whole lot of crazy things, but it has to at least jump across once.  

If it jumps across once, let say it jumps across and reaches vertex x. What is the length of this portion of the path? (Refer Slide Time: 26:16) greater than or equal to d [x]. That is our semantic that we have associated with d [x]. So this part of the path is at least d [x]. (Hindi Conversation) Why because we said, the shortest path from s to x which uses only vertices of s, has length d [x]. This is the path which uses vertices only of s. So it has length at least d [x] or more. That is why I put greater than or equal to. This part of the path whose length is at least d [x] which means the entire path has length at least d [x]. Here I am using the fact that edge lengths are non negative. So this entire path that we have drawn has length at least d [x]. So path drawn has length greater than or equal to d [x], which is greater than d [w]. It is greater than or equal to, may be they were at the same distance.  

(Refer Slide Time: 26:43)

 
So this path has length at least as large as d [w] (Refer Slide Time: 27:50). Which means it is not a shorter path. We started of by saying that there is a shorter path than d [w]. There is a path whose length is strictly less than d [w], so that is not the case. If there was such a path it should have a length, greater than or equal to d [x] it cannot be strictly less than do u. You said this is the best path. All I am saying is this part of the path has to have length at least d [x]. If it were the shortest path then it would have exactly d [x]. But may be, you did some thing I do not know, it is at least d [x]. (Hindi Conversation) there is no path which is smaller than length d [w]. There is no possible path at all. There is no path from s to w of length strictly less than d [w], which means d [w] is the length of the shortest path from s to w. 

(Refer Slide Time: 30:37)

 

And so we are justified in moving this vertex across because we have found the length of the shortest path form s to w. What is that path? That path would include some vertices from here and then jumps across. We are justified in moving this vertex. Let us recap the argument. This clam is okay (Hindi Conversation) we were proving this (Refer Slide Time: 30:07) length of shortest path from s to u that includes only vertices from s equals d[u]. This we were proving using induction and when proving this induction statement we required that (Hindi Conversation) we have found a shortest path from s to x already. And that we have proved now which is shown in the slide that is (Refer Slide Time: 26:43) (Hindi Conversation) Induction is being applied on the number of elements of s. So what is the induction statement? What should be the induction statement? I have told you everything, now you need to just turn it in your head and figure out what is right induction statement? 

This is s and this is s complement (Refer Slide Time: 31:21). So the induction statement should be for all x in s. What should I write after this? d [x] is the length of the shortest path from s to x. For all x in s complement, d [x] is the length of the shortest path from s to x which includes only vertices of capital S. (Hindi Conversation) induction hypothesis. How will this work? We will do the base case later.  

We say there is a vertex w, which we move from here to here. Since it has moved from here to here, we have to argue for vertex w that d [w] is the length of the shortest path from s to w. That we did here (Refer Slide Time: 33:26), d [w] is the length of the shortest path from s to w. For the other vertices s complement we have to argue this statement that (Hindi Conversation) and that we have done in the slide which is given above that is d [u] is the length of the shortest path from s to u that includes only vertices from s. Basically this was vertex u1, we looked at these three cases (Hindi Conversation) from s to u1 which only include these vertices from s. That either includes the vertex w, if it does not include the vertex w then we know that the length of such a path cannot be no more than d [u1]. If it includes the vertex w then if it were such a path then its length is no more than d [u1] but if it were such a path case two then its length is exactly this (Refer Slide Time: 34:36). So the minimum of these 2 quantities is the length of shortest path from s to u1 which only includes vertices of s+w. (Hindi Conversation) for the other vertices basically sub vertices (Hindi Conversation) statement continues. 

It will continue to remain true because we have not changed the distance labels at all. What have we changed as the result of this? (Refer Slide Time: 35:02-35:26) We have moved one vertex from here to here and changed the distance label of these vertices. Since we have changed the distance label of these vertices, we need to argue this statement of those vertices and since we move this vertex from here to here we need to argue this statement for these vertices and we have done that. (Hindi Conversation) which only includes vertices of s. No, you do not want to say (Hindi Conversation) if you just say that for all x, d [x] is the length of the shortest path which includes only vertices of s. When w comes in here, you will have to look at all the vertices their and also argue that why w coming in, the length of their shortest path is not reduced. 

(Refer Slide Time: 36:13)

 
Because w only comes in now (Hindi Conversation) complicated. This is what exactly the thing is. So vertices (Hindi Conversation) number, right number and they will not change. That is the length of the shortest path from s to those vertices, period. And those vertices s complement (Hindi Conversation) that is qualified in a certain way. That is the length of the shortest path from s to that vertex, provided the path uses only the vertices of capital S. That is the qualification to the path (Hindi Conversation). This is the property that you have to keep in mind about these things. No, for the vertices which are already here, it is the same. We have not changed the distance labels for those vertices at all. If earlier it was true that for a vertex which was here, the d was the length of the shortest path from s to that vertex. Then it remains, it is the shortest path unqualified. This is not qualified (Hindi Conversation) it is the shortest path from s to that vertex. 

Let us do the base case. What is the base case? But base case is very simple. The s has only little s in it and s complement is every thing but little s in it. (Hindi Conversation) basically length of this edge only. We start of by putting these as infinite but then the moment we move this across (Refer Slide Time: 39:00), this will get the length equal to this. So if this was 7, this was 12 and this was 3, then these would get distance labels 3, 12 and 7. Is this correct? Is this the length of the shortest path from s to this vertex which uses vertices of capital S only? Yes, there is no other vertex in capital S. 

(Refer Slide Time: 41:30)

 

So there is only this one edge and it is the length of the shortest path. It is similarly for 7 and 12, so it is correct. The base case is correct and at each step we are maintaining these 2 properties. This is what you have to keep in mind. You just have to say this is length l1, this is length l2 and this is length l3 and this will have l3 written on it, this have l2 written on it and this will have length l1 on it. And this need not be 3 edges and there can be any number of edges. This is a proof for any graph. (Hindi Conversation) that argues the correctness of this algorithm. This algorithm is due to Dijkstra where j is pronounced as y, so it becomes Dijkstra. What did we argue is the running time of this algorithm? This algorithm would not work if the graph had edges of negative length. In fact if a graph has edges of negative length then actually there is no notion of a shortest path some times. Do you understand what I am trying to say? Suppose this was my graph, one comes repeatedly. So for instance the shortest path from s to t has length minus infinity. Why, because I start from here, go down, I come back and keep going (Refer Slide Time: 42:33). Every time I go around this cycle, I get minus one and I just keep doing this. So the problem is because you have a negative cycle. This is called a negative cycle. If the graph has a negative cycle then the shortest path is not defined. There is no notion of a shortest path any more, because the length of the shortest path could be minus infinity. But this graph need not have a negative cycle. For instance if this edge had a length of minus one, now what is the shortest path from s to t? It is length one, I would go like this, come down and then go like this. What will be the length of this path? It is one. When do you think negative length make sense? Is this very artificial, negative lengths, can you think of a setting? 

(Refer Slide Time: 47:25) 

 

What other settings can you think where you can have negative edge lengths? So one other setting that I saw somewhere was, this is a graph which represents currency tradings. You can think of it in a different setting also. So each node is a currency, you are a global currency trader. And then an edge reflects that if I change from, if this is let say rupees, this is Indonesian path then this is the profit I incur in doing that. And negative would then correspond to a loss because of what ever I exchanged at that point of time. 

And then finally you want to change your rupees into, let say back into rupees. What you are seeing if there is a positive cycle in this graph. Then you will just keep going around the cycle and keep making when see (Hindi Conversation) I do not understand your example but there are many such setting where negative would make sense. How can I solve this or how can I find out the shortest path in graph which has negative edge lengths? Does Dijkstra?s algorithm work here? Keep the minimum as zero and shift increase everything by plus one that is by the minimum (Hindi Conversation). Add sufficiently a larger number so that there is no negative length. Brilliant idea except that it does not work (Hindi Conversation). You add a delta x so that everything becomes positive. Why does this not work? Number of edges on one path could be different from number of edges on another path. Suppose I have a very simple graph (Hindi Conversation). What am I trying to show you? What will I do? Let me finish this example first. So this path is the shorter path but its length increases because there are a lot of edges on it. Let say all edges had length one on it and one edge had a length of minus one. So what is the length of this path? 

The length of this path is 3+4 =7-1=6. Let us make this path of length longer than 6, so let make it 3, 3, 3. This becomes 9. If I increase everything or let me make it 3, 2, 2 just to be on the safe side. So it is 7, longer than this. So this is the shortest path but if I now increase every edge by one, then what is the length of this path? (Hindi Conversation). So (Hindi Conversation) what do you mean reconvert back? Keep track of number of edges. How will you keep track of number of edges on the path? (Hindi Conversation) There is no notion of a shortest path if there is a negative cycle. But the graph need not have a negative cycle, let say this graph there is no negative cycle in the graph. Negative cycle (Hindi Conversation) is well defined. 

(Refer Slide Time: 49:55)

 

So first tell me what goes wrong in the dijkstra?s algorithm? Where does it break down? Did this argument break down? Here we argued the d[u] is the length of the shortest path from s to u that includes only vertices from s. Does this break down? (Hindi Conversation) This part of argument is fine I think (Refer Slide Time: 51:10). But this part of the argument, when we said the d[w] is the length of the shortest path from s to w, is not correct any more. Because I could have a path which goes like this. But this is shorter than d[w] because there are plenty of negative edges on this part. 
(Refer Slide Time: 51:43)

 

So this inequality is not true. (Hindi Conversation) Now I cannot say anymore that d[w] is the length of the shortest path from s to w, this thing breaks down. So dijkstra?s algorithm does not work, you cannot use dijkstra?s algorithm to compute shortest path if you have negative edge length. (Hindi Conversation) How are you going to make your millions? How will you do that tell me?  

So this is a valid question here. We will basically not discuss it today, because we do not have the time do it. So that is one thing remains to be done. (Hindi Conversation) Every vertex has linked list of edges. What does it do with the linked list of edges? What way does it maintain a pointer to? (Hindi Conversation) The shortest path from s to w and then the edge w u1. (Hindi Conversation) and so on (Hindi Conversation).  

We are going to look at it in more detail in the next class. How exactly you are going to do it, what is the modification to the code that will be required do such a thing. That is the one thing I would like to cover. The other thing I would like to cover is also to show you an algorithm to compute shortest paths, when you have negative edge lengths. But it is going to be very different from this algorithm. These are the two things that remain, we will take this up in the next class.    
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 36
Single Source Shortest Paths 

We will continue our discussion on single source shortest path. Today we are going to see how to actually compute the path. In the last class we have seen how to compute the distance of any given vertex from the source, when all edges had non negative lengths. Today we are going to see how the path can be found and in second part we are going to see how to compute single source shortest path when edges might have negative lengths. 

I am going to start from the picture. You have s,   and there was a vertex w. So the   should be . The vertex w is going to move from right to left and when this vertex is moving it causes the distance label of the adjacent vertices to be updated. If this were the vertex u, we said that d[u] is going to be min [d[u], d[w] +l [w, u]]. I have repeated this many times. Essentially what this means is that if d[u] is getting this (d[w] + l [w, u]) value then that means the best path that I am finding from s to u which includes only vertices of s and w. And it is going through w or w is the vertex preceding u on the best path from s to u. 

(Refer Slide Time: 04:50)

 
 
Let me write it down, if d[u] = d[w] + l [w, u] then the best path or the shortest path from s to u (using vertices of s + w) has w preceding u. Basically we are saying that the best path from s to u and then the vertex preceding u on that path is w. This is the best path from s to u, if this is how the updation is happening that is if d[u] was getting this (d[w] + l [w, u]) value. And this is not the best shortest path from s to u because this is under this qualification, using vertices only of s and w. Because w is going to left hand side and using only these vertices the best path from s to u will have w preceding u. This is the information we will maintain with vertex u. What is the vertex preceding it on the best path that I have found so far? The best path means the shortest path. With each vertex we maintain predecessor information. What do I mean by that? If w = pred [u] then w is the vertex preceding u on the best path from s to u. How am I going to use this information to compute the shortest path?
 
Suppose I have this predecessor information for each vertex, this is vertex u and I know that the vertex which precedes u and the best path from s to u is pred [u] and I have maintained this information. Let say this is a vertex , I am going to look at predecessor of  because this part of the path from s to  has to be the shortest path from s to . If this is the shortest path from s to u then this has to be the shortest path from s to . The predecessor of  would be the vertex preceding it, on the shortest path form s to . So this vertex  would be predecessor of . This part of the path has to be the best path from s to . I have the predecessor information for   that tells me what is the vertex preceding  on the best path from s to .  This I have information that is pred [ ] and so on. I can keep doing this till I reach the source vertex. 

(Refer Slide Time: 07:28)

 

Once I have this information I can just keep tracing it back till I reach the source vertex and that will give me the path. How to we maintain this information? When ever we update the distance label, we have to keep track of this. We update the predecessor. 


When do we update or how do we update? When we transfer a vertex from the side   to the side s, that is w is the vertex which is getting transferred. Then this is the vertex u and I am updating its information. How should I write it now? If d[u] is greater than d [w] + l [u, w] then d [u] is d[w] + l [u, w]. What is this saying? The d[w] + l [u, w] is the length of a better path. If the path you found now is better then what you already had then you update the information and pred [u] = w. That is the only modification we are doing. Then basically we are storing with each vertex, 2 pieces of information d and this predecessor information. And now using this predecessor information you can find the path and keep moving back up. 

How? Suppose I wanted to find the shortest path from s to a vertex x. while! pred [x] equals null. I will set the predecessor of my source to null. Because the source vertex has no predecessor. Initially let us say x = v, v is the vertex for which I am finding. Find the shortest path from s to v? Initially x = v, now I will go through this loop, while pred [x]! = null. Print it directly that is print [x]. (Hindi Conversation) Initially v, (Hindi Conversation). So in this manner you can also print out the path, I have printed out the vertices of the path once you have the vertices in the path you can also figure out the edges of the path if you want to print out the edges. One suggestion is instead of having this condition I can also right x! = null. (Hindi Conversation) All of you can figure this out.

(Refer Slide Time: 12:36)

 

What we have done using this procedure? We have computed the shortest path from s to every vertex in the graph not just one vertex but every vertex in the graph. Let us draw all the shortest path and let see what is the sub graph that we get. (Hindi Conversation)




Let say this is the shortest path from s to vertex . (Hindi Conversation) Except for root vertex that is the source vertex. (Hindi Conversation) n-1. Do you understand what edges I am taking about? I can also call them as the predecessor edges. (Hindi Conversation) This is the predecessor vertex. If I am looking at the vertex u and v is the predecessor vertex, we have kept track of the predecessor vertex information. Bur we could also have thought of this (u v) path as the predecessor edge. The edge connecting the predecessor vertex to this vertex is the predecessor edge. (Hindi Conversation) 

How many predecessor edges we have? The n-1 predecessor edges. (Hindi Conversation) I can reach every vertex because if I take any vertex I can take its predecessor edge go to the previous vertex, go to the predecessor vertex and I said I will eventually reach s. Is it clear that I will reach s in this process. Because the whole point is that this guy got its length from here, which means we must have been able to reach this vertex in the first place. And this guy got its length from here and so on. (Refer Slide Time: 16:20-16:30)  We will eventually hit the root. So what can we say about this sub graph? If I ignore or forget the directions of the edges then it is a connected sub graph with n-1 edges. It will be a tree that is a spanning tree. But if I bring back the direction, note that in a directed graph there is really no notion of the spanning tree.  

(Refer Slide Time: 18:57)

 

There is another name give to it. It is called a branching, the same thing. It is basically a sub graph with n-1 edges such that I can reach from a specific root vertex to every vertex. Such a thing is called a branching. It is not very important, you do not need to remember this step, but you understand what the idea is. With these edges I can go from the source vertex to every other vertex. In the context of shortest path this is called the shortest path tree. Because if I just look at this tree, these set of edges then it is very nice. If I want to find out the shortest path from s to this vertex which is next to , what do I have to do?
I just follow the unique path that is there in this tree from s to this vertex, or conversely I go to the predecessors and then to the root. That is the path, which is the shortest path from s to this vertex which is next to . Similarly the shortest path from s to this vertex is this path and so on. All the shortest paths are getting captured here by just these n-1 edges. These n-1 edges capture for us in a second manner, all the shortest path from the source to all the various vertices. (Hindi Conversation) This is called the shortest path and we can compute this tree in the same time as required by Dijkstras algorithm. We have not spent any additional time.  

You also know how to find out the shortest path. What remains is what we had mentioned in the last class if edges have negative lengths. How can you then find out the shortest path? Once again we are talking of single source shortest path. When edges have negative lengths. What I mean by this is, not all edges have negative lengths, some have negative some have positive. But we are permitting edges to have all kind of negatives and positives.  Once again we have a source and we are trying to find out the shortest path from this source to all the vertices of the graph. I am going to assume that there is no negative cycle because recall if there is a negative cycle then this thing is meaningless.

There is no negative cycle in the graph. What can you say about the shortest path from s to v? Can it be a non simple path? What is a simple path? It does not repeat any vertex, no cycle on the path. Can this be a path like this? Why not? This cycle cannot have negative length which means if I were to go in the straight line above the circle, it would be shorter. Because this cycle has positive length. It will only be shorter to go directly like this. The cycle could have the also zero length in which case it will not be shorter but this will be a path of same length. 

(Refer Slide Time: 25:02)

 

There always exists a shortest path which is the simple path. This could also be the shortest path if this cycle had zero length. But the point is there will always be a simple path also we can just restrict our attention in finding a simple path. We will only be interested in finding simple path. The algorithm is very simple actually, it is a gossiping kind of algorithm. What is that mean? Think of each of this vertex as a person and this person talks to 3 people. (Hindi Conversation) Whatever information this guys gets, he promptly communicates it to his 3 mates. What they do is, they intern communicate this information to their mates and so on. (Hindi Conversation) What is the information that is being gossiped about? Length of the shortest path from the source.  

What this guy is going to tell these people is that the shortest path that I have seen so for from the source has length 10. So he will communicate this to all this 3. And what will this guy do? (Hindi Conversation) Now I have found a path of length 8. (Hindi Conversation) What is the shortest path it had found so for from the source? May be that information was 9, so now it will quickly change its thing to 8 because it had found a path of length 8 now. And may be (Hindi Conversation) that will call one round (Hindi Conversation) from its in neighbor (Hindi Conversation)

(Refer Slide Time: 29:39)

 

I have found a path of length 9 and this is 4th and here you have a path of length 13. (Hindi Conversation) I have found a path of length 12 and this is 1 and you have the path of length 13. (Hindi Conversation) I have found a path of length 13 this is minus one so you have a path of length 12. What is this guy going to do? It is going to take the minimum of these 3 and if it had some original, earlier information. It is going to take the minimum all of them and put that down as the new path length. Let say original information was more than 12 then that will become path length and this is the information that it would communicate in the next round. (Hindi Conversation) 

One round is inner round (Hindi Conversation) ever one talks to their neighbors and sends the information. And then each node also gathers the information from its neighbors and updates its value. That is the end of the round after it has updated its value. In inner round, each guy talks to its neighbors. (Hindi Conversation) First transfer then updation. Let us say the round starts with the transfer then when you have got information from all your neighbors, you update. All the nodes do the same thing simultaneously. A node will get information only from its inmates. (Hindi Conversation) You have these vertices, you have to implement this thing. What is the information we are maintaining with each node, once again we call it a distance label. With the node v, I am maintaining this distance label. 

How will you implement one round? I take a vertex, there is no notion of actual transmitting any more. This was just to show you the idea. What does this vertex do? It look at its in adjacent vertices. (Hindi Conversation) propositional to its degree. The total time (Hindi Conversation) number of edges. This is how you will implement one round. (Hindi Conversation) zero on the source and infinity for all others (Hindi Conversation) If in a round nothing changes then we can stop. 

(Refer Slide Time: 35:44)

 
 
(Hindi Conversation) from the source we will have to prove that (Hindi Conversation) let see (Hindi Conversation) There is a difference between length and number of edges on the shortest path. This is the shortest path using edges length (Hindi Conversation) n-1 shortest path (Hindi Conversation) A shortest path from s to v then what is the shortest path from s to this vertex which is between . That property continues to hold true. (Hindi Conversation) 


That was independent of whether the edge lengths were positive or not. Shortest path from s to this vertex will be this they can not be a shorter path because if there were the shorter path then this could not have been the shorter path from s to v. Similarly shortest path from s to this would have been and so on. 

So the claim we are going to make is, if the shortest path from s to vertex x has l edges on it then d[x] gets the right value which means the value of the shortest path after l rounds. (Hindi Conversation) After 5 rounds this vertex v will have the correct length of the shortest path. Why? This claim is true we will prove the claim by induction on l. l equals one (Hindi Conversation) is this true? (Hindi Conversation) 

When we find a better path because if there is no better path, we will never be changing the value later (Hindi Conversation) value  (Hindi Conversation) after five steps then it will not change beyond that. (Hindi Conversation) You take the value of the in adjacent vertices which has not been updated in that round. There is a vertex whose value has been updated right now and it has vertex you reach the vertex after that you will consider new value or old value for confusion a new value or old value. Let us forget about new values or the old values.  

Let me write down the code (Hindi Conversation) for all v do (Hindi Conversation) for all w in adjacent to v do. The d[v] = min [d[v], d[w] +l [v, w]] (Hindi Conversation) for i =1 to (Hindi Conversation) n-1 do (Hindi Conversation) you will try to reduce the running time a bit. The worst case will remain the same. (Hindi Conversation) d[s] equals zero, for all v in v-s, d[v] equals infinity. (Hindi Conversation)

For i=1 to n-1 do is just to count the number of time we have to repeat the process.(Hindi Conversation) in adjacent vertices (Hindi Conversation) Why did you put down only n-1 and no more because I argued in this claim that (Hindi Conversation) What is the maximum number of edges that there can be on any shortest path. We have argued that on any shortest path there can be no more than n-1 edges because there will always be a shortest path which is simple. Does not repeat, so at most n-1 edges on it and so I need to repeat it only n-1 edges and no more. (Hindi Conversation) Should we do the correctness for this? Yes let us do the correctness, why is it correct? Why have we computed the shortest path? Have you computed the shortest path?

Is it clear that after round 5 this vertex v is going to get a distance label of . It is going to get a distance label of this or smaller (Hindi Conversation) I am assuming this is the shortest path. After one round this guy has to get a label of  that is completely clear, after 2nd round this guy is going to get a label of  or less. This guy is going to get a label of  after 3 rounds or something even lesser.




(Refer Slide Time: 41:49)     

 

After 4 rounds this is going to get a label of  or some things lesser. This guy is going to get a label of  after 5 rounds or something lesser. So for correctness we need to argue that this guy is not going to anything lesser than . If you have argued those then we have that it gets exactly the distance label (Refer Slide Time: 42:00-43:17) 

We have to argue that a vertex cannot get a distance label which is lesser than the length of the path from the source to that vertex. I give a vertex of a certain distance label, it is because I have found a path of that length from the source to that vertex that I have to convince you. And that follow from the way we are doing things. That will be a proof by induction. What should we induct on? 

What is the claim we are trying to make? Let us write that down. If a vertex v gets a certain distance label d in round r, then we have found a path of length d from s to v. This is what we are trying to argue. Basically what I am saying is that if a vertex is getting some distance label it is not that (Hindi Conversation) Why is this claim useful?(Hindi Conversation)  l(Hindi Conversation) This is not a shortest path. Contradiction Why is it true? (Hindi Conversation) Distance label d need not be an integer it is just the length of the path. (Hindi Conversation) Suppose this statement is true till round r-1. Then in round r, if this vertex v is getting as a certain distance label d because either that was the distance label on it or because one of its neighbors, so this vertex getting a distance label d (Hindi Conversation) It could have been updated in the same round and you are worried about that aspect. Then we will have do a double induction kind of a thing. 

Let us not worry about that, I will modified the code so that every thing gets updated simultaneously. (Hindi Conversation) But let us just assume this vertex u, this d[u] was the label in round r-1. So now we can apply induction hypothesis which means that we found a path of length d[u] from s to u. That was the induction hypothesis saying. (Hindi Conversation) And this plus this means that I have found a path from s to v of length exactly d[u] +l [u, v] which is all d (Hindi Conversation) that is complete (Hindi Conversation) 

There is a valid doubt, you guys are worried a little bit (Hindi Conversation) look at the algorithm but let us modify the thing so that we do not do that at all (Hindi Conversation) for all v do (Hindi Conversation) x [v] equals d[v], x[v] equal to min[x[v] , d[w]+ l[v,w]](Hindi Conversation) In fact that will be faster than this one because you are kind of doing a little bit more work in each round. For this perhaps the correctness is more apparent so it is okay if you do it this way. But even that it is correct. 

(Refer Slide Time: 52:12)

 

The last thing I want to say very briefly is APSP, this is All Pair Shortest Path. (Hindi Conversation) all pair, every pair of vertices (Hindi Conversation) n (mlog n) time (Hindi Conversation) n (m, n) (Hindi Conversation) these are not the best possible, there are algorithm which can do better than this. You will learn about these algorithms in the algorithms scores. But let me just tell you what are the better bound nodes. 










(Refer Slide Time: 56:29) 

 

So for single source shortest paths with positive edges length (Hindi Conversation) m logn, the best bound known is m+n log n (Hindi Conversation) m could be as larger  in which case this will be  +nlog n could be  and this will be  log n. So factor log n more. This is single source shortest path when edge lengths are positive. When edge lengths can be negative, when I write negative it does not mean that every thing is negative. Please remember this, it just means that in fact every thing is negative (Hindi Conversation) best possible be mn. You can not (Hindi Conversation) All Pair Shortest Path (Hindi Conversation) mn logn. Best possible (Hindi Conversation) n (m+ n logn) (Hindi Conversation) best possible is mn+  log n, basically same as this one. Best known, no these are not the lower bounds these are only the best known. (Hindi Conversation) So with that we will end today lecture on shortest path. In fact that also brings us to the end of this course. We are not going to take up any other topic, this is all we have to discuss. 
 
Data Structures and Algorithms
Dr. Naveen Garg
Department of Computer Science and Engineering
Indian Institute of Technology, Delhi
Lecture ? 1
Introduction to Data Structures and Algorithms

Welcome to data structures and algorithms. We are going to learn about some basic terminologies regarding data structures and the notations that you would be following in the rest of this course. We will begin with some simple definitions. An algorithm is an outline of the steps that a program or any computational procedure has to take. A program on the other hand is an implementation of an algorithm and it could be in any programming language. Data structure is the way we need to organize the data, so that it can be used effectively by the program.

(Refer Slide Time: 1:27)

 

Hope you are all familiar with certain data structures, an array or a list. In this course you will be seeing a lot of data structures and you will see how to use them in various algorithms. We will take a particular problem, try to solve it and in the process develop data structures. The best way of organizing the data, associated with that problem. What is an algorithmic problem? An algorithmic problem is essentially, that you have a certain specifications of an input and specify what the output should be like. Here is one specification. A sorted, non decreasing sequence of natural numbers of non-zero, finite length. For example:
?	1,20,908,909,100000,1000000000
?	3.
This is a completely specified input. Above are the two examples of input, which meets the specification and I have not given any output specification.

What is an instance? A sorted, non-decreasing sequence of natural numbers of non-zero, finite length forms an instance. Those two examples are the instances of the input. You can have any possible number of instances that may take sequence of sorted, non-decreasing numbers as input. 

(Refer Slide Time: 2:29)

 

An algorithm is essentially, describing the actions that one should take on the input instance to get the specified output. Also there can be infinitely many input instances and algorithms for solving certain problem. Each one of you could do it in a different way.

(Refer Slide Time: 3:33)

 

That brings the notion of good algorithm. There are so many different algorithms for solving a certain problem. What is a good algorithm? Good algorithm is an efficient algorithm. What is efficient? Efficient is something, which has small running time and takes less memory. These will be the two measures of efficiency we will be working with. There could also be other measures of efficiency. 

(Refer Slide Time: 3:53)

 

But these are the only two things we will be considering in this course. We would be spending more time on analyzing the running time of an algorithm and we will also spend some time on analyzing the space. We would be interested in the efficiency of algorithms, as a function of input size. 

Clearly you can imagine that, if I have a small input and my algorithm or a program running on that input will take less amount of time. If the input becomes 10 times larger, then the time taken by the program may also increase. It may become 10, 20 or 100 times. It is this behavior of increase in the running time, with the increase in the size of input would be of our interest. 
Let us see the slide.











(Refer Slide Time: 5:20)

 

How does one measure the running time of an algorithm? Let us look at the experimental study. You have a certain algorithm and you have to implement the algorithm, which means you have to write a program in a certain programming language. 

You run the program with varying data sets in which some are smaller, some are of larger data sets, some would be of some kinds and some would be of different kinds of varying composition. Then you clock the time the program takes and clock does not mean that you should sit down near stopwatch. Perhaps you can use the system utility like System. Current Time Millis (), to clock the time that program takes and then from that you try to figure out, how good your algorithms is. That is what one would call as the experimental study of the algorithm.

This has certain limitations, let us see them in detail. First you have to implement the algorithm in which we will be able to determine how good your algorithm is. Implementing it is a huge overhead, where you have to spend considerable amount of time. Experiments can be done only on a limited set of inputs. You can run your experiment on a small set of instances and that might not really indicate the time that your algorithm is taking for other inputs, which you have not considered in your experiment. 









(Refer Slide Time: 6:23)

 

If you have two algorithms and you have to decide, which one is better. You have to use exactly the same platforms to do the comparison. Platform means both the hardware and software environment. Because as you can imagine, different machines would make a difference, in fact even the users who are working on that system at that particular point would make a difference on the running time of an algorithm. It becomes very messy, if you have to do it this way. Hence same hardware and software environments should be used.

What we are going to do in the part of this course? In this very first lecture, we have to develop the general methodology, which will help us to analyze running time of algorithms. We are going to do it as follows: First we are going to develop a high level description of an algorithm. The way of describing an algorithm and we are going to use this description to figure out the running time and not to implement it to any system. 

A methodology would help us to take into account of all possible input instances and also it will allow us to evaluate the efficiency of the algorithm in a way that it is independent of the platform we are using. 











(Refer Slide Time: 7:38)

 

Pseudo-code is the high level description of an algorithm and this is how we would be specifying all our algorithms for the purpose of this course.

(Refer Slide Time: 8:23)

 

Here is an example of pseudo code and you might have seen this in earlier courses also. What is this algorithm doing? This algorithm takes an array A, which stores an integer in it and it is trying to find the maximum element in this array. Algorithm array Max (A, n) The above mentioned example is not a program, because the syntax is wrong. But it is a pseudo code which is a mixture of natural language and some high-level programming concepts.
I am going to use a for loop, do loop, if-then-else statement and a while loop. But I will not bother about whether there should be a semicolon or a colon, because they are required for the compiler. But for our understanding, what the program is doing is clear. In the beginning it keeps track of the maximum variable in a variable called current max which is initialized to the first element of the array. Current Max ? A [0] Then it is going to run through the remaining element of the array, compare them with the current maximum element. If the current maximum element is less than the current element, then it would update the current max. A[i] becomes the new max and then when the loop terminates we would just return current max.
        If current Max < A[i] then current Max ? A[i]
        return current Max
It is a very simple algorithm but just with this pseudo-code, you are able to understand what it is doing. This will not run on any computer since it is the pseudo-code, but it conveys the idea or the concepts. 

(Refer Slide Time: 8:48)

 

Thus pseudo-code is more structured than usual prose, but it is less formal than a programming language. How pseudo-code will look like? We will use standard numeric and boolean expressions in it. 










(Refer Slide Time: 10:33)

 

Instead of the assignment operator which is ?=? in java, I will use ? and instead of the equality operator, an equality relationship in java which is ?= =? the same in C, I will just use ?=?. I will declare methods with the algorithmic name and the parameter it takes. Algorithm name (param 1, param2)

I will use all kinds of programming construct like if ?then statement, if ?then? [else] statement, while ? do, repeat ?until, for ? do and to index array I will say A[i], A [i, j]. It should be clear in what it is doing.

I will use return when the procedure terminates and return value will tell about the value returned by the particular procedure or a function. returns: return value When I have to call a method, I will specify that with the name of the method and the argument and the object used. calls: object method (args)















(Refer Slide Time: 11:22)

 

Object specifies the type of the value returned by the particular method. You will see more of this, when we come across more pseudo-code. How do we analyze algorithms? First we identify what are the primitive operations in our pseudo-code. What is a primitive operation? It is a low level operation. Example is a data movement in which I do an assignment from one to another, I do a control statement which is a branch (if? then ?else) subroutine call or return. I do arithmetic operations or logical operations and these are called as a primitive operation.
?	Data movement (assign)
?	Control (branch, subroutine call, return)
?	Arithmetic an logical operations (e.g. addition, comparison)
In my pseudo code, I just inspect the pseudo code and count the number of primitive operations that are executed by an algorithm. Let us see an example of sorting. The input is some sequence of numbers and output is a permutation of the sequence which is in non decreasing order. What are the requirements for the output? It should be in non-decreasing order and it should be the permutation of the input. 













(Refer Slide Time: 12:37)

 

Any set of numbers which are in non-decreasing order does not make an output. Algorithm should sort the numbers that were given to it and not just produce the sequence of numbers as an increasing order. Clearly the running time depends upon, number of elements (n) and often it depends upon how sorted these numbers are. If they are already in sorted order then the algorithm will not take a long time. It also depends upon the particular algorithm we use. The running time would depend upon all these things. The first sorting technique we use is the one that you have used very often.
Let us say when you are playing game of cards. 

(Refer Slide Time: 13:11)

 
What is the strategy you follow, when you are picking up a set of cards that have been dealt out to you? You like to keep them in a sorted order in your hand. You start with the empty hand and you pick up the first card, then you take the next card and insert it at the appropriate place.

(Refer Slide Time: 14:45)

 

Suppose if I have some five cards in your hand already, let us say 2, 7, 9, jack and queen. Then I am getting 8, so I am going to put it between 7 and 9. That is the right place it has to be placed in. I am inserting it at the appropriate place and that is why this technique is called insertion sort. I keep on doing this, till I have picked up all the cards and inserted in the appropriate place.

Let us see the pseudo-code for insertion sort. I will give an array of integers as input and output is a permutation of the original numbers, such that it is sorted. The output is also going to be in the same array.
      A [1]  A [2]   _  A[n]
This is the input, output specification. I am going to have 2 variables or indices i and j. The array is going to be sorted from a [1] through a [j-1]. The element should be inserted at the   Location, which is the right place to insert. Clearly j has to vary from 2-n. 
   For j? 2 to n do








(Refer Slide Time: 15:43)

 

I am going to look at   element and I put that in key. Key ? A[j] I have to insert A [j] or the key in to the sorted sequence which is A [1] through A [j-1]. i.e. A [1_j-1] I am going to use the index i to do this. What is index i going to do? Index i is going to run down from j-1 down to 1. We have to decrease index i, which we are doing in the while? do loop. 

It starts with the value j-1. I have to insert 7 and i am going to move 9 to   location, because 9 is greater than 7. Then I compare 7 with 8 and 8 is still greater than 7, so I will move it right. Then I compare 7 with 6. As 6 is smaller than 7, I would put 7 in the appropriate place. 

I run through this loop, till I find an element which is less than a key. Key is the element which I am trying to insert. This loop will continue while the element, which I consider is more than key and this loop will terminate, when I see an element which is less than key or the loop will terminate when I reach i=0. While i >0 and A[i] > key do A [i+1] ? A[i]
That means I have moved everything to the right and I should insert the element at the very first place and I am just shifting the element one step to the right. Do A [i+1] ? A[i]

Note that I have to insert 7 at the right place, so I shift 9 right to 1 step.   location becomes empty, then I shift 8 to 1 step, so this   location becomes empty and now I put 7 there. i + 1 is the index, which would be the empty location eventually and i put the key there. A [i+1] ? key All of you can implement it. May be you would have implemented it in a slightly different way, that would give you a different program, but the algorithm is essentially the same. You are going to find the right place for the element and insert it. Let us analyze this algorithm.


(Refer Slide Time: 19:34)

 

I have put down the algorithm on the left. There is a small mistake in the last line of the slide, where there should be a left arrow. Please make a correction on that.
    A [i+1] ?  A key
Let us count.
   Key ? A[j]
   I ? j-1
These are all my primitive operations. I am comparing i with 0 and I am comparing A[i] with key, also I take and, so there are three primitive operations.
 while i >0 and A[i] > key
 
 Each of the operation takes a certain amount of time, depending upon the computer system you have.  , , , , ,  just represent the amount of time taken for these operations and they can be in any units. I am counting the number of times, each of these operations is executed in this entire program.

Why this operation is done n times? I start by assigning j =2 then assign 3, 4,5,6,7 and go up to n. Then when I increment it once and check that there is one more, so I have counted it as n times. There might be small errors in n and n + 1, but that is not very important. Roughly n times we need to do this operation. 








(Refer Slide Time: 19:34)

 

How about this operation? Key ? A[j] I am going to do exactly n-1 times once for 2, once for3, once for 4 up to n. That is why this operation is being done up to n-1 times. Just leave the comment statement. Again the operation will be done exactly n-1 times.

We have to look at how many times I come to this statement. While I >0 and A[i] > key  - Counts the number of times I have to shift an element to the right, when I am inserting the   card in to my hand. In the previous example when I am inserting 7, I had to shift 2 elements 8 and 9.    is going to count that quantity and that is the number of times I am going to reach A[i] part of my while loop. While I >0 and A[i] >key 

I will be checking this condition for many times. For one iteration or for the   iteration of this for loop, I am going to reach this condition for   times. The total number of times I am saying that condition is the sum of   as j goes from 2 to n.
        
     while I >0 and A[i] > key
     do A[i+1] ? A[i]

Every time I see (A[i] >key) condition I also come to A[i], because the last time I see the statement I would exit out of this condition. That is why this is  -1 where j going from 2 to n.
        
A [i+1] ? A key. This statement here is not a part of the while loop rather it is a part of the for loop as it is done exactly n-1 times as the other statement. If you knew about the constants then the total time taken by the procedure can be computed. You do not know what   is.   is quantity which depends upon your instance and not problem. Problem is in the sorting. The instance is a set or a sequence of numbers that have given to you. Thus   depends upon the instance.

Let us see the difference that   makes. If the input was already sorted, then   is always 1( =1). I just have to compare the element with the last element and if it is larger than the last element, I would not have to do anything.  is always a 1 if the input is already in increasing order. 

What happens when the input is in decreasing order? If the input is in decreasing order, then the number that I am trying to insert is going to be smaller than all the numbers that I have sorted in my array. What am I going to do? I am going to compare with the  element,  element,  element,  element and all the way up to the element. When I am trying to insert the   element, I am going to end up in comparing with all the other j elements in the array. In that case when   is equal to j, note that the quantity becomes its summation of j, where j goes from 2 to n. It is of the kind  and the running time of this algorithm would be some constant time  plus some other constant times n minus some other constant.
      

Thus the behavior of this running time is more like . We will come to this point later, when we talk about asymptotic analysis but this is what I meant by . On the other hand in the best case when  =1, the sum is just n or n-1 and in that case the total time is n times some constant plus n-1 times some constant minus some constant which is roughly n times some constant. Hence this is called as linear time algorithm.
















(Refer Slide Time: 24:36) 

 

On an average what would you expect? In the best case you have to compare only against one element and in the worst case you have to compare about j elements. In the average case it would compare against half of those elements. Thus it will compare with  , even when the summation of   where j goes from 2 to n, this will be roughly by  and it behaves like . This is what I mean by the best, worst and average case. I take the size of input, suppose if I am interested in sorting n numbers and I look at all possible instances of these n numbers.


















(Refer Slide Time: 26:47)

 

 (Refer Slide Time: 27:08)

 

It may be infinitely many, again it is not clear about how to do that. What is worst case?
The worst case is defined as the maximum possible time that your algorithm would take for any instance of that size. In the slide 27:08, all the instances are of the same size. The best case would be the smallest time that your algorithm takes and the average would be the average of all infinite bars. That was for the input for 1size of size n, that would give the values, from that we can compute worst case, best case and the average case. If I would consider inputs of all sizes then I can create a plot for each inputs size and I could figure out the worst case, best case and an average case. Then I would get such a monotonically increasing plots. It is clear that as the size of the input increases, the time taken by your algorithm will increase. Thus when the input size becomes larger, it will not take lesser time. 

(Refer Slide Time: 28:18)

 

Which of this is the easiest to work with? Worst case is the one we will use the most. For the purpose of this course this is the only measure we will be working with. Why is the worst case used often? First it provides an upper bound and it tells you how long your algorithm is going to take in the worst case. 

(Refer Slide Time: 28:50)

 

For some algorithms worst case occurs fairly often. For many instances the time taken by the algorithm is close to the worst case. Average case essentially becomes as bad as the worst case. In the previous example that we saw, the average case and the worst case were . There were differences in the constant but it was roughly the same. The average case might be very difficult to compute, because you should look at all possible instances and then take some kind of an average. Or you have to say like, when my input instance is drawn from a certain distribution and the expected time my algorithm will take is typically a much harder quantity to work and to compute with. 

(Refer Slide Time: 30:36)

 

The worst case is the measure of interest in which we will be working with. Asymptotic analysis is the kind of thing that we have been doing so far as n and  and the goal of this is to analyze the running time while getting rid of superficial details. 

We would like to say that an algorithm, which has the running time of some constant times  squared is the same as an algorithm which has a running time of some other constant times  ,because this constant is typically something which would be dependent upon the hardware that your using. 
       = 

In the previous example ,  and   would depend upon the computer system, the hardware, the compiler and many factors. We are not interested to distinguish between such algorithms. Both of these algorithms, one which has the running time of  and another with running time  have a quadratic behavior. When the input size doubles the running time of both of the algorithm increases four fold. 

That is the thing which is of interest to us. We are interested in capturing how the running time of algorithm increases, with the size of the input in the limit. This is the crucial point here and the asymptotic analysis clearly explains about how the running time of this algorithm increases with increase in input size within the limit.

(Refer Slide Time: 32:20)

 

Let us see about the ?big-oh? O-notation. If I have functions , g (n) and n represents the input size. f (n)  measures the time taken by that algorithm. f (n) and g (n) are non-negative functions and also non-decreasing, because as the input size increases, the running time taken by the algorithm would also increase. Both of these are non-decreasing functions of n and we say that f (n) is O (g (n)), if there exist constants c and , such that f (n)   c times of g (n)   .
  f (n) =O(g(n)
  f (n)   c g(n) for n    

What does it mean? I have drawn two functions. The function in red is f (n) and g (n) is some other function. The function in green is some constant times of g (n). As you can see beyond the point , c (g (n)) is always larger than that of f (n). This is the way it continues even beyond. Then we would say that f (n) is O (g (n) or f (n) is order (g (n)).
 
   f (n) = O (g(n)) 







(Refer Slide Time: 33:56)

 

Few examples would clarify this and we will see those examples. The function f (n) =2n+6 and g (n) =n. If you look at these two functions 2n+6 is always larger than n and you might be wondering why this 2n+6 is a non-linear function. That is because the scale here is an exponential scale. The scale increases by 2 on y-axis and similarly on x-axis. The red colored line is n and the blue line is 2n and the above next line is 4n. As you can see beyond the dotted line f (n) is less than 4 times of n. Hence the constant c is 4 and   would be this point of crossing beyond which 4n becomes larger than 2n+6.

At what point does 4n becomes larger than 2n+6. It is three. So   becomes three. Then we say that f (n) which is 2n+6 is O (n). 
     2n+6 = O (n)

Let us look at another example. The function in red is g (n) which is n and any constant time g (n) which is as same scale as in the previous slide. Any constant time g (n) will be just the same straight line displaced by suitable amount. The green line will be 4 times n and it depends upon the intercept, but you?re  would be like the line which is blue in color. So there is no constant c such that  < c (n). 

Can you find out a constant c so that  < c (n) for n more than . We cannot find it. 
Any constant that you choose, I can pick a larger n such that this is violated and so it is not the case that  is O (n). 





(Refer Slide Time: 35:55)

 

How does one figure out these things? This is the very simple rule. Suppose this is my function 50 n log n, I just drop all constants and the lower order terms. Forget the constant 50 and I get n log n. This function 50 n log n is O (n log n). In the function 7n-3, I drop the constant and lower order terms, I get 7n-3 as O (n).
 
(Refer Slide Time: 37:01)

 

I have some complicated function like 8  log n+ 5  +n in which I just drop all lower order terms. This is the fastest growing term because this has   as well as log n in it. I just drop , n term and also I drop my constant and get  log n. This function is O ( log n). In the limit this quantity (8  log n+5 +n) will be less than some constant times this quantity (O (  log n)). You can figure out what should be the value of c and , for that to happen.

This is a common error. The function 50 n log n is also O ( ). Whether it is yes or no. It is yes, because this quantity (50 n log n) in fact is   50 times  always, for all n and that is just a constant so this is O( ). But when we use the O-notation we try and provide as strong amount as possible instead of saying this statement is true we will rather call this as O (n log n)). We will see more of this in subsequent slides.

How are we going to use the O-notation? We are going to express the number of primitive operations that are executed during run of the program as a function of the input size. We are going to use O-notation for that. If I have an algorithm which takes the number of primitive operations as O (n) and some other algorithm for which the number of primitive operations is O ( ). Then clearly the first algorithm is better than the second. Why because as the input size doubles then the running time of the algorithm is also going to double, while the running time of O ( ) algorithm will increase four fold.

(Refer Slide Time: 39:10)

 

Similarly our algorithm which has the running time of O (log n) is better than the one which has running time of O (n). Thus we have a hierarchy of functions in the order of log n, n, , , .

There is a word of caution here. You might have an algorithm whose running time is 1,000,000 n, because you may be doing some other operations. I cannot see how you would create such an algorithm, but you might have an algorithm of this running time. 1,000,000n is O (n), because this is   some constant time n and you might have some other algorithm with the running time of 2 .
Hence from what I said before, you would say that 1,000,000 n algorithm is better than 2 . The one with the linear running time which is O (n) running time is better than O ( ). It is true but in the limit and the limit is achieved very late when n is really large. For small instances this 2  might actually take less amount of time than your 1,000,000 n. You have to be careful about the constants also. 

We will do some examples of asymptotic analysis. I have a pseudo code and I have an array of n numbers sitting in an array called x and I have to output an array A, in which the element A[i] is the average of the numbers X [0] through X[i]. One way of doing it is, I basically have a for loop in which I compute each element of the array A. To compute A [10], I just have to sum up X [0] through X [10], which I am doing here.
   For j ? 0 to I do
   A ? a + X[j]
   A[i] ? a/ (i+1)
To compute A [10], i is taking the value 10 and I am running the index j from 0-10. I am summing up the value of X from X [0] - X [10] in this accumulator a and then I am eventually dividing the value of this accumulator with 11, because it is from X [0] to X [10]. That gives me the number I should have in A [10]. I am going to repeat this for 11,12,13,14 and for all the elements.

(Refer Slide Time: 41.34)

 

It is an algorithm and let us compute the running time. This is one step. It is executed for i number of times and initially i take a value from 0,1,2,3 and all the way up to n-1. This entire thing is done n times. This gives you the total running time of roughly .
    a ? a+ X[j] 
This one step is getting executed  times and this is the dominant thing. How many times the steps given below are executed?
 A[i] ? a/ (j+1)
 a ? 0 These steps are executed for n times.  a ? a + X[j] But the step mentioned above is getting executed roughly for some constant   times. Thus the running time of the algorithm is O ( ). It is a very simple problem but you can have a better solution. 

(Refer Slide Time: 43:19)

 

What is a better solution? We will have a variable S in which we would keep accumulating the X[i]. Initially S=0. When I compute A[i], which I already have in S, X [0] through X [i-1] because they used that at the last step. That is the problem here.
    a ? a +X[j]

Every time we are computing X. First we are computing X [0] + X [1], then we are computing X [0] + X [1] +X [2] and goes on. It is a kind of repeating computations. Why should we do that? We will have a single variable which will keep track of the sum of the prefixes. S at this point (s? s+x[i]), when I am in the  run of this loop has some of X [0] through X [i-1] and then some X[i] in it. To compute  element, I just need to divide this sum by i +1.
   S ? S +X[i] 
   A[i] ? S/ (i+1)
I keep this accumulator(S) around with me. When I finish the  iteration of this loop, I have an S, the sum X [0] through X[i]. I can reuse it for the next step. 








(Refer Slide Time: 44:15)

 

How much time does this take? In each run of this loop I am just doing two primitive operations that makes an order n times, because this loop is executed n times. I have been using this freely linear and quadratic, but the slide given below just tells you the other terms I might be using.

(Refer Slide Time: 46:01)

 

Linear is when an algorithm has an asymptotic running time of O (n), then we call it as a linear algorithm. If it has asymptotic running time of , we called it as a quadratic and logarithmic if it is log n. It is polynomial if it is  for some constant k. 

Algorithm is called exponential if it has running time of , where a is some number more than 1. Till now I have introduced only the big-oh notation, we also have the big-omega notation and big-theta notation. The ?big-Omega? notation provides a lower bound. The function f (n) is omega of g (n), 
 
  f (n) = (g(n)) 

If constant time g (n) is always less than f(n), earlier that was more than f(n) but now it is  less than f(n) in the limit, beyond a certain   as the picture given below illustrates. 
 c g (n)   f (n) for n   
f (n) is more than c (g(n)) beyond the point  . That case we will say that f (n) is omega of g (n).
f (n) =  (g (n))

(Refer Slide Time: 47:51)

 

In   notation f (n) is  (g (n), if there exist constant   and    such that f (n) is sandwiched between  g (n) and  g (n). Beyond a certain point, f (n) lies between 1 constant time g (n) and another constant time of g (n). Then f (n) is   (g (n)) where f (n) grows like g (n) in the limit. Another way of thinking of it is, f (n) is  (g (n)).  If f (n) is O (g (n)) and it also  (g (n). There are two more related asymptotic notations, one is called ?Little-oh? notation and the other is called ?Little-omega? notation. They are the non-tight analogs of Big-oh and Big-omega. It is best to understand this through the analogy of real numbers. 




(Refer Slide Time: 48:48)

 

When f (n) is O (g (n)) and the function f is less than or equal to g or f (n) is less than c (g (n). The analogy with the real numbers is when the number is less than or equal to another number.  is for   and   is for =.  (g (n) is function and f=g are real numbers. 

If these are real numbers, you can talk of equality but you cannot talk of equality for a function unless they are equal. Little-oh corresponds to strictly less than g and Little-omega corresponds to strictly more. We are not going to use these, infact we will use Big-oh. You should be very clear with that part. 

The formal definition for Little-oh is that, for every constant c there should exist some   such that f (n) is < c (g(n) for n >  . f (n)   c (g(n)) for n     How it is different from Big- oh? In that case I said, there exist c and   such that this is true. Here we will say for every c there should exist an . 














(Refer Slide Time: 49:12)

 

The slide which is below defines the difference between the functions. I have an algorithm whose running times are like 400n, 20n log n, 2 ,    and . Also I have listed out, the largest problem size that you can solve in 1 second or 1 minute or 1 hour. The largest problem size that you can solve is roughly 2500. 

Let us say if you have 20n log n as running time then the problem size would be like 4096. Why did you see that 4096 is larger than 2500, although 20n log n is the worst running time than 400n, because of the constant. You can see the differences happening. If it is 2 then the problem size is 707 and when it is  the problem size is 19. 

See the behavior as the time increases. An hour is 3600seconds and there is a huge increase in the size of the problem you solve, if it is linear time algorithm. Still there is a large increase, when it is n log n algorithm and not so large increase when it is an  algorithm and almost no increase when it is  algorithm. If you have an algorithm whose running time is something like , you cannot solve for problem of more than size 100. It will take millions of years to solve it. 
 










(Refer Slide Time: 50:52)

 

This is the behavior we are interested in our course. Hence we consider asymptotic analysis for this.
