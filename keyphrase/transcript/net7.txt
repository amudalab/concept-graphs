COMPUTER NETWORKS
Prof. S. Ghosh
Dept. of Computer Science & Engineering
IIT Kharagpur
                                                      Lecture -31
                                                          TCP
                                                (Start time 00.42)
Good day, our topic today is TCP. TCP is the second most important transport protocol.

(slide time 01:13   - 01:44)
TCP is very widely used by many applications. We have already discussed UDP. Actually this is a little more complex then UDP but it also has some advantages. We will see what they are.  So the Transport layer responsibilities are:

( slide time 01:13   - 01:44)

Transport layer creates packets from byte steam received from the application layer. And in order to multiplex and de-multiplex amongst various applications it uses port numbers to create process-to-process communication. It uses a sliding window protocol to achieve flow control and uses acknowledgement packets, time out and retransmission to achieve error control. So, unlike UDP which is unreliable this seeks to provide a reliable communication. That means it is error free, it is a connection oriented protocol and it also has some kind of congestion control mechanism. And it does the basic thing of connecting between processors amongst two distant nodes possible.

( slide time: 02:54-03:02)

TCP is called connection-oriented reliable transport protocol. It adds connection oriented and reliability features to the services of IP. IP as such is best effort kind of service so it does not give you any reliability. Secondly IP is connectionless whereas TCP tries to give some kind of connection-oriented flavor to the connection. And proposal is implemented entirely at the two end nodes. That means the intermediate routers etc do no have any role to play.

(slide time: 03:35-04:07)

The communication abstraction is that, it is reliable and ordered. Ordered means the packets are received in order. In IP it is not guaranteed that the packets will arrive in order because what might happen is that one packet may be routed in one direction and the other packet may be routed in other direction so the packet which was sent first may land up at the destination earlier and vice versa. But TCP makes them ordered. TCP brings an order amongst them.

( slide time: 04:08 ? 04:22)

TCP is point-to-point and unicast. All these features namely it is ordered, point-to-point, reliable and unicast is what gives the connection-oriented flavor to TCP. It takes the byte-stream as an input and gives that as the output. It is a full loop-less connection. That means both the nodes A and B are connected by TCP then A can communicate to B and B can communicate to A and it has flow and congestion control.

(slide time: 04:22-04:32)

TCP (Transmission Control Protocol) is a connection-oriented protocol, reliable, unicast, end-to-end, byte-stream over an unreliable network.

( slide time: 04:58-05:41)

So, before any data transfer TCP establishes a connection. Just like in a connection-oriented network like classical telephones, first there is a connection which is set up by all kinds of control signals. Similarly, like in ATM, first a connection is set up and then the actual data transmission begins. Similarly, in TCP a connection is set up between the two end points before any transmission takes place, so it sets up a connection.

(slide time: 05:42 ? 06:09)

One TCP entity is waiting for a connection that is the server. The other TCP entity-client contacts the server. The actual procedure for setting up connection is more complex. The client first makes a request for a connection and then the server acknowledges the request and accepts it, then the data transfer begins. So there is a connection set up before actual data transfer can take place and at the end of the data transfer there is a disconnect phase also.

( slide time: 07:08 ? 07:34 )

It is reliable. The byte-stream is broken up into chunks which are called segments. Therefore at the TCP level they are called segments. The receiver sends acknowledgements for segments so this is the basic mechanism by which reliability is brought in that each and every segment is acknowledged. TCP maintains a timer. If an ACK is not received in time the segment is retransmitted. So this is the basic mechanism for reliability. The sender sends a packet and it waits for some acknowledgement. And if the acknowledgment does not come through from receiver then the sender assumes that the packet has been lost and sends the packet again hoping that this packet will eventually reach the receiver and the receiver will send the acknowledgement. This might also lead to duplicate packets at the destination. For example, the sender has sent the packet and the receiver has received it and it has sent an acknowledgement but then the acknowledgement got lost. So naturally the original sender did not receive any acknowledgement and after sometime it will send it again so a duplicate segment would be a received by the receiver but then it knows that it is a duplicate and will eliminate that. So, by this way it achieves reliability. 


(slide time: 07:34 ? 07:59)

TCP can detect errors. TCP has checksum for header and data like UDP. Segment with invalid checksums are discarded. Each byte that is transmitted also has a sequence number. So, if some intermediate sequence number is missing the receiver knows that something has been lost.  

( slide time: 07:59 ? 08:32)

To the lower layers TCP handles data in blocks, the segments. This is where the packets actually originate. But to the higher layer TCP handles data as a sequence of bytes and does not identify boundaries between bytes. So, the higher layers do not know about the beginning and end of segments. Hence to the higher layer it is just a stream of bytes. 


( slide time: 08:33 ? 09:02)

For example, the application writes 100 bytes then it writes 20 bytes so all these go into the queue. So the queues are bytes to be transmitted and then TCP transmits them. At the other end also there is a queue of bytes and at the other end also it reads 40 bytes at each go. So, to this application this is the byte-stream and for this application also this is just a stream of bytes coming in such units.  

(slide time: 09:03 ? 09:37)

The unit of data transfer between devices using TCP is a segment. It is 20 to 60 bytes header followed by data. It is a 20 byte header if there are no options and up to 60 bytes if it contains some options. So there can be up to 40 bytes of option. So the header naturally contains some field which allows the TCP protocol to run. Now let us look into the details of this header.

(slide time: 09:37 ? 09:02)
The Header has 16 bit source port address and 16 bit destination port address. It also has a sequence number, acknowledgement number, Header length (HLEN), reserved bits, some flags and then there are some options like window size, checksum, urgent pointer, options and padding. 

(slide time: 10:21- 10:30)
So there is a source port address. In the client-server processes actually communicate through ports. And the port and the IP address together form the socket which uniquely identifies every session. When a TCP session is going on, on both the sides two port numbers are assigned. For standard applications the first communication will start on well-known port number then they will switch to ephemeral port numbers. 

(slide time: 10:31 ? 10:51)

Then there is a source port address which is a 16 bit address that defines the port number of the application program on the host that is sending the segment. 

( slide time: 10:51 ? 11:15)

Similarly there is destination port address which is again 16 bits. The port number is from 1000 to 65000 and those are the ephemeral port numbers. Destination port address defines the port number of the application program on the host that is receiving the segment.

(slide time: 12:50 -12:55 )
There is the sequence number which is a 32 bit number. It defines the number assigned to the first byte of data contained in the segment. During connection establishment a random number generator is used to create an Initial Sequence Number (ISN). The field for the sequence number has been kept quite big, it is a 32 bit and is kept with a reason. If the number of bits for the sequence number was small then what could happen is that, when a particular session starts it sends the number and then it would go back to the beginning. For any finite length sequence number after some point it is going to go back. Now, if it goes back and starts these numbers once again for a very small segment size what might happen is that first of all the two packets may get the same segment number which are on the network at the same time and similarly other kinds of confusion might arise. So what is done is that, actually a large sequence number is generated so that even if it comes back to a low number we can know that which one came earlier and which one came later. If after receiving very high numbers if you start getting some low sequence numbers you know that it has looped back. Actually it may not be a strict loop.

(slide time: 13:27 -14:28)

Therefore this is the range of sequence numbers. So 232 is about 4 {?lian}  which is a large number and random number is used to generate the Initial sequence number. This initial sequence is expected in a large range so it is not going to clash and this ISN is exchanged between the two nodes. 

( slide time: 14:29 ? 14:44 )

There is an acknowledgment number which is 32 bit. Just as the segments which have been sent they have a sequence number. Similarly these segments as they arrive on the other side will get an acknowledgement. So it is one acknowledgment after the other so a stream of acknowledgements will come. Actually in the best of circumstances there will be as many acknowledged numbers as there are segments which are sent. So this acknowledged number will start somewhere and that would be communicated by the receiver to the sender in the connection set up phase which is also a 32 bit number that is large. It defines the byte that the receiver of the segment is expecting to receive form the other party. This is standard norm for the sliding window protocol. 

(slide time: 14:53-14:57)
The next field in the header is the header length which is of 4 bit. This indicated the number of 4 byte word in the TCP header. This is required because the header is of variable length. The header could be anything from 20 bytes to 60 bytes depending on the options. So the Header length is to be specified and then there are some reserved bits for future use

( slide time: 14:58 -15:51)


( slide time: 15:51 ? 16:17)

followed by some flags. It defines 6 controls flags. URG defines the urgent pointer which determines whether the urgent pointer is valid or not. If the urgent pointer is not valid the urgent pointer field itself may contain some garbage. ACK tells whether acknowledgement is valid or not. PSH is for request for push. RST is for resetting the connection. SYN is to synchronize sequence number and FIN is to terminate these connections. These flags are used for setting up and termination of connections.

( slide time: 16:17 -16:35)

There is a window size of 16 bit which defines the size of the window in bytes that the other party must maintain. It is sort of controlled by a receiver. The receiver gives the window size which is of the sliding window protocol. And then there is a Checksum which is of 16 bits. The checksum is like UBP.

( slide time :16:36 ? 16:58)

The urgent pointer is 16 bits and is valid only if the urgent flag is set. It defines the number to be added to the sequence number to determine the last urgent byte in the data section of the segment.

( slide time :16:55- 17:02)

Options are up to 40 bytes. Options could be a single-byte or multiple-byte. Multiple-byte in options may contain maximum segment size, windows scale factor and timestamp etc. In single-byte there are end of options and no operation for padding purpose.  

( slide time :17:03 -17:24 )

End of option is used for padding at the end of the option field and no operation is used as filler between the options. 

( slide time :17:24- 17:51 )

If there are options on the other hand then one option is the maximum segment size which defines the size of the biggest chunk of data that can be received by the destination of the TCP segment. This is not the segment but the size of the data which is taking the header field out. 

( slide time :18:53 -19:00)

The window scale factor defines the size of the slicing window and how it is changed. Timestamp is filled by source when segment leaves and destination returns it in the echo reply field. This allows the source to determine the round trip time. Some estimation about the round trip time is very essential because suppose a particular segment has been sent then the sender is expecting an acknowledgement. If the acknowledgement does not arrive on time, then how long would the sender wait for the acknowledgement? So you have to make an estimate and that estimate is based on the round trip delay, the maximum segment life. So it has to wait that much or may be some more and only then the sender would sort of come to the conclusion that may be the original packet is lost or may be the acknowledgement is lost. But in either case the packet or the segment that was send it has to be retransmitted.


( slide time :19:01 ? 19:24)

There is a Checksum, it has the same calculation as UDP and inclusion is mandatory with TCP.

( slide time :20:21 ? 21:10 )
Setting up connections and resetting of connections: TCP is a connection-oriented protocol. So, a virtual path between the source and the destination has to be established. This is only a virtual path because remember that TCP is working on IP that means an IP is a connectionless service. So the underlying actual service is actually connectionless. But TCP gives a feeling to the upper layer, this application layer that, as if it is a connection-oriented thing. That means all good things about connection orientation like segments, arriving in order and reliability etc is present. But TCP has to work on a connectionless IP network. So this path established from source to the destination is only a virtual path unlike traditional strict connection-oriented service where the connection may be physical.  

( slide time: 21:10 -21:21 )
For connection establishment four actions are required before sending data. And here two of them may be combined so it can even be three actions. First host A sends a segment, if host A is the client and it wants to establish a connection to the server may be a host B, so host A sends a segment to announce its wish for connection and includes its initialization information. And then host B sends a segment to acknowledge the request of A. Here host B sends back an acknowledgement.

( slide time: 21:21-21:24)


Then host B sends a segment that includes the initialization information. So here the second and third steps can be combined.

( slide time: 21:24-  21:47)
That means the host B can acknowledge the request of host A as well as in the same acknowledgement it might send the initialization information meaning the sequence number etc have to be exchanged.

( slide time: 21:47- 24:09)
Then host A sends a segment to acknowledge the request from B. Second and third can be combined which is called as the Three-Way Handshaking.
 

( slide time: 24:09-24:30)
For the first SYN, that is, for synchronization the sender sends a synchronization which is a request to set up a connection to the host. And it also gives initial sequence number. The receiver then sends a SYN, its own SYN the sequence number which is something like 4800 and an acknowledgement of 1200. This segment which is the segment 1 already consumes the first sequence number 1200. So, while acknowledging it sends the value 1201 meaning that 1201 is the next segment or next segment number that the receiver is expecting. What might happen is that, this packet may get lost because once again the TCP is sitting on a best effort kind of IP service so it may get lost. Therefore what would happen to the sender is that, the sender will not get any acknowledgement so after some time it will send the SYN pack again if it is persistent and then finally it will get the SYN and the acknowledgement. Similarly, if this is lost once again this is sent after sometime and finally this is achieved and then the sender sends the sequence number 1201 and acknowledgement 4801. So this sequence number is always the next number that is expected from the other side. When it sends sequence number1200 it replies back saying that next it is expecting 1201 and its starting number is 4800. So he replies in his acknowledgement that the next acknowledgment he is expecting is 4801. So this is the Three-Way Handshake where the SYN and the ACK has been combined and then the data transmission can start. 


( slide time: 24:53-24:58)  

On the other side we have to think about the connection termination. To terminate the connection either party can close the connection. If connection is terminated in one direction data can continue to be sent in the other direction. Remember that this is the full loop-less communication which means A is communicating to B and B is communicating to A at the same time. Even if only one side is sending data to the other side the acknowledgement is coming from the other side anyway. Now, the connection can be terminated from the both sides but then somebody has to initialize the termination.


( slide time: 24:58 ?25:22)

So, if connection is terminated in one direction data can be continued to be sent in the other direction.  

( slide time: 25:27 ? 25:34)
 Four actions are required to close the connection in both directions. First, host A sends a segment announcing connection termination. This means it sends the segments containing FIN. 

( slide time: 25:34-25:44)
Host B sends a segment acknowledging the request from A and after this the connection is closed in one direction.


( slide time: 25:44-25:50)
When host B has finished sending data it sends a segment indicating connection closure.

( slide time: 25:51-25:55)
Here, the second step and the third step cannot be combined together. 


( slide time: 25.55 ? 26:10)

(Slide time: 26:11-26:24)
Although we are sort of allowing the termination of connection for one side but on the other side it may have acknowledgements or other things to send to this side so it will not terminate the connection. Therefore these two cannot be combined together.


(Slide time: 26:24- 29:32)
The third step can be taken only when host B has finished sending data from its side and it sends a segment indicating a connection closure. Host A acknowledges the request from B. So this is called a Four-Way Handshaking.  

( slide time: 29:32-29:36)

This is the diagram showing Four-Way Handshaking. For example, assume that the sender has sent a FIN in a segment 2500 and when it receives the FIN if everything is all right he will send back an ACK and he say he will say that the next one he is expecting from the other side is sequence 2501 and sequence 7000 which is the acknowledgement for this FIN. After sometime when B has finished sending all its acknowledgements and other things it might want to send to the sender then it is sends a FIN from this side. So, this is for closing the connection from the other side. The acknowledgement that is expected once again is 2501 and this is still 2501 because nothing else has arrived from the other side and then it takes the next segment number 7001 from this side. And he acknowledges this second FIN when the acknowledgement reaches here so everything is closed gracefully. Of course things may not run so well because one or more of these packets may get lost on the way because we know that the underlying network is unreliable. If the first FIN is missed he does not get any acknowledgement and after sometime he will send the FIN again. Similarly, if this ACK is not received then he will send this FIN once again anyway. So this FIN may be lost. Since he does not get any acknowledgement he will send the FIN after sometime. The trouble is over here because at this point of time after sending this acknowledgement the sender will assume that everything is fine so he will close. But this last acknowledgement may get lost. As all the FINs have been sent and received and the acknowledgement has been received and sent he will say that it is the end of the story but for him it is not end of the story because this ACK is now lost but if this fellow has already completed he will not send any ACK any longer. So what he will have to do is that after sometime he will have to close the connection.

( slide time :29:36-29:42)
Actually the TCP goes through a state diagram.

( slide time: 29:42-31:11)
This is the state diagram and these are the different states.

( slide time: 31:11-32:21)
One is of course closed that means no connection is active or pending. The other is listening. May be the server is waiting for an incoming call. SYN RCVD means somebody has made a request, a connection request has arrived and is waiting for acknowledgement. SYN SENT: The client has started to open a connection. ESTABLISHED means a connection has been established and that is the normal data transfer state. FIN WAIT 1 means client said it is finished. FIN WAIT 2 means server has agreed to release. TIMED WAIT means wait for pending packets. This is the 2 MSL wait state so for the last pending packet to come in you have to wait this much. This is the maximum segment and this is sort of estimated from the round trip time. CLOSING means both sides have tried to close simultaneously. CLOSE WAIT means server has initiated a release and a LAST ACK is waiting for the pending packets.

( slide time: 32:21-33:05)
When it is closed nothing is happening. Now some SYN or SYN plus ACK it receives and once it receives that then it goes to the SYN RCVD state. As explained earlier this is the connection set up phase and once the connection is set up then it reaches the Established phase and then the two way communication is going on. Then, after this, it goes through a closure either through the closing and then FIN WAIT 1 and FIN WAIT 2, TIME WAIT etc and here it is the close width and the last ACK.

( slide time: 33:05-33:17)
The flow control is implemented by TCP. This defines the amount of data a source can send before receiving an acknowledgment from the destination. TCP uses a window imposed on the buffer of data to limit the amount of data sent before an acknowledgement must be received. This is the traditional sliding window protocol.

( slide time: 33:18 -34:21)
In the Sliding Window Protocol a sliding window is used to make transmission more efficient as well as to control the flow of data so that the destination does not become overwhelmed with data.


( slide time: 34:23-35:01)
In sender buffer there is the occupied part of the buffer and out of this some number or packets or segments have been sent but they have not been acknowledged and these are the next byte to be sent which is available in the buffer. Now, depending on the windows size it can go on sending these bytes. Or if the window size is already over then it has to wait here. Let us assume that the size of the window is three units, in that case the sender has to wait here till some acknowledgment comes. And if the acknowledgment comes suppose 200 is acknowledged then the window will automatically slide and this now can be sent.


( slide time: 35:02-35:22)
At the receiver side it has received 194 to 199. This is the occupied part of the buffer and from this buffer the destination process will keep on consuming the data from these segments as q stream of bytes going out of this segment. Here, this is the empty part of the buffer where new segments can come in. 

(Slide time: 37:48-39:55)
Suppose the size of the receiver window is 7 then after having sent this the sender can go on sending up to the receiver window size. The point is that, if a window size is not fixed what the sender might do is that, the sender may send in a lot of packets. Or may be the channel is absolutely bad, in that case none of the packets have gone to the other side so you will have to send all these packets once again, and that is one point. And secondly the sender may be very fast but the network may be congested. So the sender will push in a lot of data but that will only make the congestion worse. So whatever would have been received on the other side does not reach that side because the intervening network has been congested, or may be some buffer has overflowed or some router has dropped some packets etc so there is a limit on the window size. The window size also has a bearing on the speed at which effective data transfer is taking place. If the window size is very small, suppose 2, then after sending two segments the sender has to wait till the acknowledgment comes back. So the efficiency of the channel goes down because for each acknowledgement the packet will reach the other side and then the acknowledgment will travel all the way back so it is a round trip delay. Now suppose the window size is only 1 so after sending each segment or each byte you wait for the round trip time and then you send the next byte. So now the overhead has become very high and now this is the minimum possible rate at witch the sender is sending. On the other hand if the window is very large a large amount can be sent even without waiting for acknowledgement. 

( slide time: 39:55 ? 39:58)
Now when everything is fine the stream of acknowledgements will also start coming in after this round trip delay and the overall efficiency would be high. Therefore between congestion and the efficiency at which the data can be transferred that is the effective data transfer speed there is a trade-off. 
Suppose if these are the sequence numbers and now the receiver side may be 205 has dropped somewhere and 206, 207, 208, 209 etc has come. So after acknowledging 204 where the acknowledgement packet will tell that the next byte its waiting for is 205 although 206, 207, 208, 209 etc have been received but he will not send any acknowledgement. So after sometime the sender will realize that this acknowledgment for 205 has not come. So he will start pumping 205, 206, 207, 208 and 209 again. Now as soon as 205 arrive what happens is that this gap is closed and the window will slide all the way here and he will say next byte that is expected is 210. Then what will happen is, 206, 207, 208, 209 now becomes duplicated segments on the receiver side so they are simply dropped. Using the segment numbers not only we can put them in order, suppose 206 comes earlier and 205 has come but has come later but then they can be put in proper sequence because sequence numbers are there.  And secondly even this 205 was lost it can still be recovered by this retransmission etc so that is how this protocol is reliable. And when this is acknowledged the window slides.

( slide time: 39:59-40:11)
So, this is the size of the receiving window.


( slide time: 41:17-41:31)
Sometimes what happens is that, if there is actual congestion in the network, you will find that only after sending some packets and seeing that some acknowledgements have not come and the packets have got lost. So, the sender thinks, one thing is, this packet may have got lost and secondly, may be the network is congested. So what will happen is that, there is a mechanism for automatically reducing the window size so that now you are going to transfer data at a much lower rate but then you are sort of trying to control the congestion. So this is the congestion control that is in built into TCP. There are some variations as to how this window size is going to be changed over time etc. So we will come to later on we when will talk in more details about congestion control.


( slide time: 41:32-41:51)
In TCP the sender window size is totally controlled by the receiver window value, the number of empty locations in the receiver buffer. However, the actual window size can be smaller if there is congestion in the network.


( slide time: 41:51- 42:15)

The source does not have to send a full window?s worth of data. When you have limited data then you will send only that data which is available. The size of the window can be increased or decreased by the destination. The destination can send an acknowledgment at any time. 


( slide time: 42:18 -43:50)
Error Control:
The TCP is a reliable transport protocol. TCP delivers the entire stream to the application program on the other end in order without error and without any part lost or duplicated. So TCP provides reliability using error control.

( slide time: 43:51 ? 44:01)
For error control there are the different things that TCP would do. It detects corrupt segments, by detecting lost segments, by detecting out of order segments, by detecting duplicated segments and by correcting errors after they are detected. Detecting corrupt segments is, when checksum would be wrong in that case you can drop it. Or if you can correct the error using the checksum Then you may correct it also. Detecting lost segments is achieved by getting the help of the sequence number. When you have the sequence numbers and when one particular segment in between is lost you know that the segment number is lost because that particular segment number would be missing. Detecting segments that are out of order is, when the segments are in out of order we have got the sequence number and so we put them in order. You can detect and correct it at the same time. Detecting duplicated segments: If a particular segment has come twice, may be its acknowledgement is lost or delayed, in that case once again by the serial number we can see that it is duplicated and we can drop it. 


( slide time: 44:22 ? 44:39)  
Error detection is achieved by three simple tools: Checksum, acknowledgement and time-out. Suppose you had sent the first segment and the acknowledgement never came then after sometime you have to give a time-out and that is how the sender detects that this segment has been lost and it has to be re-transmitted. 

( slide time: 44:38 -44:52)
Checksum, acknowledgement and time-out are the three things together offer reliability and error detection and correction capability of TCP.

( slide time: 44:52-48:05)
For error correction when time-out counter expires segment is considered to be corrupted or lost and the segment is retransmitted.

( slide time: 48:05 ? 49:35)
TCP Timers: TCP has to maintain a number of timers inside. TCP has to do a lot of work to give that reliability which is necessary on this unreliable IP service. That is why this protocol is a little more complex than others. But then at the same time this is necessary. For example, when you are sending a File Transfer Protocol for example, you are transferring a file from one machine to another then even the misplacing of few bits will make the whole file useless. So there are applications and specifically protocols like FTP and ACTP etc use the TCP protocol because there is reliability of the connection and absolute error control, it is error free, and the nature of communication is very high. Then there may be applications where a loss of few bits or bytes here and there does not really matter. For example, suppose we are carrying a voice communication, now if you are carrying voice communication what would happen is that even if a few bits here and there are lost the quality is not impaired that much. Of course voice or other multimedia kind of communication is very sensitive to other kinds of network parameters. But the point is that, even if few bits are lost here and there then the other person will also be able to make out. So in this case, on the other hand if there is a delay and differing rate of transmission because if the window size is large the rate at which you can transmit is high and if the window size is small the rate at which you can transmit is small. So, if the rate at which transmission is taking place keeps on varying or takes a lot of time etc is not acceptable for voice communication. So those are the cases where we will not use TCP. There are cases where we definitely want to use TCP because of its reliability and then there are cases where we do not want to use TCP. So FTP and HTTP etc are examples where TCP is necessary and we use them. TCP uses four timers: Retransmission timer, persistence timer, keep-alive timer and time-waited timer. 

( slide time: 49:35 -51:56)
Retransmission timer: Used to control a lost or discarded segment. Suppose a segment is lost, now, after what time will it be retransmitted? So in this retransmission timer using the timer stamp you can get an idea of the round trip delay and this you will use to set your retransmission timer. And there will be a buffer because it is not constant. By this way you can set up your retransmission timer. As soon as the segment is sent immediately the retransmission timer starts. When the next segment is also sent because your window size happened to be more than 1 then you will need another retransmission timer for this segment. So, for each segment we are maintaining this retransmission timer and as soon as one of these timers is timed-out it means that the packet was sent and the acknowledgement was not received. Therefore, immediately you have to retransmit that packet. So this is the retransmission timer. TCP creates a retransmission timer when it sends a segment. If an acknowledgement is received before the timer goes off the timer is destroyed because this timer is not required.  For each new segment you start a new retransmission timer. 

( slide time :51:57-52:03)
If the timer goes off before the acknowledgement arrives, that means the acknowledgement has not been received before the estimated time then the segment is retransmitted and the timer is reset. Now this timer will be reset because even though you retransmit the retransmitted segment may also get lost so you have to start the retransmission timer once again. And so TCP uses a dynamic retransmission time which is different for each connection and may be changed during the connection. TCP is a protocol and there may be a lot of connections going through this TCP protocol. Just think a simple case of a web server. Now there may be a number of people for very active websites and a number of people may be hitting it at almost the same time. So if you have a concurrent server and you have spawn so many processes on the application side and each of the server is sort of serving one particular client and a number of them. So a number of TCP services are going on at the same time but of course they are using different port numbers. So whenever any acknowledgment or anything comes you will see the port number to determine to which process this belongs to. Secondly, now assume that two clients using the same server at the same time. Now the two clients may be connected in two different networks and the round trip time for each of these clients may be different. Since the roundtrip time is different you cannot use the same kind of retransmission time for each of these connections. This has to be dynamically assessed from the time stamp so it may be different for each connection and may be changed during the connection.

( slide time: 52:03-52:26)


( slide time: 52:40-52:50)
There is a Persistence timer. When TCP receives an acknowledgment with the window size of 0 it starts a persistence timer. Window size of 0 means, it does not accept anything at the moment because either it is overwhelmed or something is sort of wrong in between. So anyway it is very congested and is not willing to receive anything. But now what would the sender do?


( slide time: 53:15- 53:23)
He will start a persistence timer and when the persistence timer goes off the TCP sends a special segment called a probe. What would have happened is that the buffer may have become full. So now, before sending a packet you have to find out whether it has recovered so you have to send a probe segment. 

( slide time: 53:23-53:34)
This prevents a deadlock from occurring in case an acknowledgment is lost.


( slide time: 54:29-54:51)
There is a keep-alive timer that prevents long idle connections between two TCP?s. Suppose two sides the connection has been setup and nobody has closed the connection but both sides are idle for a long time. Now, as soon as the TCP connection is setup and after the last communication this keep-alive timer is automatically started. When the keep-alive timer goes off, that means there has not been any communication between the two for a long time. What might have happened is that the other side might have got switched off. Therefore at times there can be disruption in connection. Hence it is not a graceful connection.

( slide time: 55:22- 55:38)
Each time the server hears from a client it resets the timer and starts the countdown from the beginning. If it does not hear from the client after the timer times out it once again sends a probe segment to see where the other side is alive or not. If the other side is indeed alive and wants to keep the connection on it will reply to the probe segment and then it will know that it still wants to do something but then it is much occupied at the moment and that is why it is not sending. So this is the keep-alive timer. If on the other hand it has been switched off then this probe will go unanswered. So after sometime this side will take some action to close everything. 

( slide time: 55:22- 55:38)
Time-Waited Timer is used during connection termination. When TCP closes a connection it is not really closed until this timer times out. It allows for FIN segments to be received. In the previous case the last acknowledgment has not been received because it was lost. So everything was fine but the other side has actually gracefully terminated the condition and stopped it. So it has to wait for sometime which is more than twice the round trip delay. Therefore it waits for the time-waited timer and once that is over then the connection can be terminated. With this we close our discussion on TCP and there is some more variance of TCP regarding how you control the window sizes etc. We will discuss that when we discuss congestion control.









        Lecture No # 32
              IP Multicasting

( slide time: 56:51 ? 57:01 )
Good day, today?s topic is IP Multicasting. There are three modes of operation. One is Unicast that means one sender is sending something to one receiver. The other is broadcast which means one sender is sending it to all the receivers that means to all the nodes in the networks. Multicasting is, when you want to send it to just a group of hosts and not all the hosts. 

( slide time: 57:26 ? 57:46)
IP Multicasting: Multicast communication refers to one-to-many or many-to-many communication. Unicast refers to the source is one and the destination is one. Broadcast is, when source is one and destination is to all and in multicast destinations are a few.
  
( slide time: 57:46 ? 58:08)
IP multicasting refers to the implementation of multicast communication in the internet. Individual hosts are configured as members of different multicast groups. Multicasting is not connection-oriented. An IP multicast group is identified by a class D address. So these are the general parameters. One particular user may be a member of different multicast groups. For a one particular multicast group there will be a few members in the network and it has to reach those and not others. And the other thing to be understood is that, multicasting is not connection-oriented that means all the packets are sent. That means it is packet by packet and it is not from the source, multiple channels or a prairie set up like that.


 
  
COMPUTER NETWORKS
PROF. S. Ghosh
Dept. of Computer Science & Engineering
IIT Kharagpur
Lecture No # 32 
IP Multicasting
Good day. Today?s topic is IP Multicasting. (Refer slide time: 00:56- 01:08)

Slide time: 00:56- 01:08
Till now, we have seen the three modes of operation: One is unicast where one sender is sending to one receiver. One is broadcast where one sender is sending to all the receivers that means all the nodes in the network and multicasting is when you want to send it to a group of hosts but not all the hosts. So, IP multicasting is the topic for today. (Refer slide time: 01:31- 01:53)


Slide time: 01:31- 01:53
Multicast communications refers to One-to-many and many-to-many communications. For example, this is unicast when the source is one and the destination is one, broadcast is when source is one and destination is all and multicast is when destination are a few. (Refer slide time: 01:53- 02:14) 

Slide time: 01:53- 02:14
- IP multicasting refers to the implementation of multicast communication in the internet. 
- Individual hosts are configured as members of different multicast groups. 
- Multicasting is not connection-oriented. 
- An IP multicast group is identified by class D address. 
 These are the general parameters. One particular user may be member of different multicast groups but for one particular multicast group there will be few members in the network and it has to reach those and not others. Other thing to understand is that multicasting is not connection-oriented that means all the packets are sent. It is packet by packet it is not that from the source multiple channels are prairie setup or anything like that. (Refer slide time: 02:46- 03:15) 

Slide time: 02:46- 03:15
They are identified by class D address which is for multicast. The class D address is seen in the fashion that the beginning four bits are 1110. So the initial value is 224. something and these 28 bits. This is how by looking at an address you can see that this is the multicast address. (Refer slide time: 03:15- 03:26) 

Slide time: 03:15- 03:26
There are many applications like news, sports, stock and weather updates. Let us take the example of stock updates. Now, not everybody would be interested in stock updates, only some group of people would be interested in stock updates. Again, it may be such that one group of stock is of interest to one group of people, another group of stock to another group of people and so these would be different multicast groups and the news feed should reach these people. Then multicasting may be applied in distance learning. (Refer slide time: 03:49 - 04:17)

Slide time: 03:49 - 04:17
When some learning material is distributed to distance learners and just specific group of learners, configuration, routing updates, service location may be the areas where multicast may be applied. PointCast-type push applications where push means when the actual source of information finally on its own sends it to the group of people. For example, stock quotes may be (Refer slide time: 04:28 - 04:52)

Slide time: 04:28 - 04:52
pushed. Teleconferencing, audio, video, shared whiteboard, text editor etc is an interesting and important application of multicasting. That means you may like to have a video conferencing amongst a group of people so this same video stream should reach the entire group of people. (Refer slide time: 04:52 ? 05:18)

Slide time: 04:52 ? 05:18
- Distributed interactive gaming or simulations, some people are participating in some game in a distributed fashion. 
- Email distribution lists. 
- Content distribution; software distribution. 
- Web cache updates, Database replication. 
Multicasting has very large number of applications. But the trouble is, multicasting is a little complex. The technology is not so simple and the fact is there are most of the routers which are in operation today are not configured for multicasting in a proper manner because it takes a toll on the routers capabilities. So we will come to see what a multicast routing is all about. (Refer slide time: 05:42 - 05:59)

Slide time: 05:42 - 05:59
There are three essential components of IP multicast service: 
- IP multicast addressing is, how you address. 
- IP group management. 
- Multicast routing. (Refer slide time ? 06:00- 07:13 )

Slide time ? 06:00- 07:13
If you look at this diagram, suppose you have these three routers and three networks connected to these three routers respectively. Suppose, you take any of these routers and that is connected to its own group of machines and some of these machines would be the members of a multicast group. Of course, new machines can come in and new machine can actually decide to join the multicast group and some of the old group members may choose to leave a particular multicast group. So there is a group membership protocol which goes on between the router and the different machines connected to the network. Then, amongst the routers there is question of multicast routing. So this Internet Group Management Protocol (IGMP) runs between host and their immediately neighboring multicast routers. And within the routers we have the multicast routing protocol running. (Refer slide time: 07:13 - 07:41) 



Slide time: 07:13 - 07:41
This is another picture showing the same thing. This is the service model, suppose, these are the hosts and host to router protocol which is known as the Internet Group Management Protocol (IGMP). And then amongst the routers there is multicast routing protocol and there are various types of multicast routing protocol, we will just discuss a few. (Refer slide time: 07:41 ? 08:07)

Slide time: 07:41 ? 08:07
- IP multicasting only supports UDP as higher layer. 
- There is no multicast TCP. UDP is a connectionless datagram oriented protocol and TCP is connection-oriented. Since, IP multicasting is essentially connectionless that is why it chooses UDP as the transport layer protocol. (Refer slide time: 08:07 - 09:13)

Slide time: 08:07 - 09:13
If you look at the details of this part of the protocol stack here we have the network interface, IP and IP multicast layers. The IP part of this network layer takes part in the normal routing protocol whereas IP multicast part takes care of the multicasting routing protocol. Then above this in the TCP IP stack we have the TCP protocol and the UDP protocol. So, for TCP we use the stream sockets whereas for UDP we use the datagram sockets as well as multicast sockets. So, multicast sockets also use UDP. This is the socket layer and above this we have the user layer. So that is the application layer which uses these multicast sockets and uses UDP to send multicast messages which are routed by the multicast supporting routers. (Refer slide time: 09:13 ? 09:51)

Slide time: 09:13 ? 09:51
IP multicast works as follows: 
- Multicast groups are identified by IP addresses in the range 224.0.0.0 to 239.255.255.255. These are the class D addresses. 
- Every host (more precisely every network interface card) can join or leave multicast group dynamically. 
- At present the way it has been done has no access control. So, if there is a multicast group which requires access control then this has to be implemented in the application layer. As the protocol stands today this has not been included. One of the reasons is that IP multicasting in actual practice constitutes a very small amount of traffic compared to its potential true multicasting. Later on let us see what is true multicasting and simulated multicasting. But actual true multicasting traffic is really small compared to its potential mainly because most of the routers may not support, they are not more precisely configured to support multicasting because of the cost involved. (Refer slide time: 10:46 ? 11:06 )

Slide time: 10:46 ? 11:06
Since there is no access control every IP datagram sent to a multicast group is transmitted to all members of the group. 
There is no security, no floor control. 
Moreover since it uses UDP, IP multicast service is essentially unreliable. (Refer slide time: 11:06 ? 11:46)

Slide time: 11:06 ? 11:46
More detail about the multicast addresses: 
- The range of addresses between 224.0.0.0 and 224.0.0.255 means the last byte for the first range inclusive is reserved for the use of routing protocols and other low level topology discovery or maintenance protocols. 
- Multicast routers should not forward any multicast datagram with destination address in this range. So they are reserved addresses and other addresses can be distributed to different multicast routes. (Refer slide time: 11:46 - 12:21)

Slide time: 11:46 - 12:21
Examples of special reserved class D address: 
- 224.0.0.1 really means all systems on this subnet. 
- 224.0.0.2 means all routers on this subnet. 
- 224.0.1.1 is for NTP (Network Time Protocol) used for synchronizing machines. 
- 224.0.0.9 is for RIP-2 (a routing protocol). 
So these are some special addresses and there are others. (Refer slide time: 12:21 - 12:28)

Slide time: 12:21 - 12:28
Now there is a question of multicast address translation. You remember what happens in the case of unicast is, from the IP address, for transmitting packets in the local network we need to go down to the data link layer and we need to find out the hardware address. Let us say if we are using Ethernet then we need to find out the Ethernet or MAC address and then data is actually sent as an Ethernet frame so the Ethernet Address is put over there. Now, that is the case when we are handling unicast. Now, what happens in multicast? For this particular IP address it is actually representing a particular multicast group and there will be a number of machines in that group. How do you handle it in the Ethernet level? (Refer slide time: 13:23 - 14:19) 

Slide time: 13:23 - 14:19
- In Ethernet Mac addresses a multicast address is identified by setting the lowest bit of the most left byte. That is this byte. Suppose you want 1, 2, 3, 4, 5, 6, if you remember, Ethernet address is 6 bytes long, now, of the first byte the most significant byte if you want and the last bit of that is set to 1 in Ethernet Mac addresses to indicate that this is the multicast address. 
- Unfortunately not all Ethernet cards can filter multicast addresses in hardware. So, if it cannot be done in hardware then filtering is to be done in software by the device driver. So, you accept the packet and then do the filtering if it is multicast and if you are a member of the group and then accept it. (Refer slide time: 14:19 ? 15:49 )


Slide time: 14:19 ? 15:49
This is how the mapping is done. Suppose in this 1110, the first 4 bits and suppose this is the class D address and we are looking at the first byte of that address and the first four bits 1110 identifies that this is a class D address. Then this bit is actually ignored and then we have a 23 bit address. This 23 bit address comes straight to the Ethernet address. So these 7 bits, these 8 bits and these 8 bits are matched straight to the last 3 bytes of the Ethernet address. For the first three bytes of the Ethernet address we have a one here showing that this is multicast. Actually the Ethernet address with 01, 00, 5e in the first 3 bytes are reserved for IP multicast. So 01, 00 and this is 101 is 5e and 1110 is e. So, this is 01, 00, 5e and this is first 3 bytes, this is reserved for multicast and this part comes straight away. (Refer slide time: 15:49 ? 16:12)



Slide time: 15:49 ? 16:12
- Now let us move on to IGMP which is the Internet Group Management Protocol. This is a very simple protocol for the support of IP multicast. 
- IGMP is defined in RFC 1112. 
- IGMP operates on a physical network that is the single Ethernet segment. (Refer slide time: 16:12 ? 17:08)

Slide time: 16:12 ? 17:08
If you remember, in the previous diagram we saw that one particular router is connected to one Ethernet segment. So IGMP is between this router and the host which are there. So IGMP is used by multicast router to keep track of the membership. Now, who all amongst these members, who has ceased to be a member, who is the new person who wants to join as a member, so this has to be kept track by the local multicasting router and that is what IGMP supports. So, it supports joining a multicast group, query membership and send membership reports. So the multicasting router will send queries from time to time and the host will respond or not respond depending on whether or not they are members of the group. (Refer slide time: 17:08 ? 17:27) 

Slide time: 17:08 ? 17:27
So, we have this multicasting router over here and one single Ethernet segment and number of machines connected there. So IGMP query comes from this multicasting router and IGMP report goes to the router from the host. (Refer slide time: 17:27 - 18:30)

Slide time: 17:27 - 18:30
There may be an IGMP general query that IGMP group address is set to be equal to 0. It means that this query is for all the hosts, for all the groups and destination IP address is broadcast in this subnet. You remember that 224.0.0.1 is broadcast in this subnet and source IP address is the routers IP address. There may also be groups specific query in which case the IGMP group address is the group address, destination IP address is again the group address because now I want to give this query only to the members of one particular group and source address is the routers IP address. And this individual host sends the reports so it is the IGMP membership report. Therefore IGMP group address is the group address, destination IP address is also the group address and source IP address is equal to host?s IP address. (Refer slide time: 18:30 - 19:02)

Slide time: 18:30 - 19:02
In the IGMP message format there is a version and type. Type may be 1 or 2, version is usually 1 and then a 16-bit checksum and 32-bit group address. Type: 1 for the query sent by multicast router and 2 is a response sent by a host. Group address is a class D IP address. On query it is 0 and on response it is the group address being reported. (Refer slide time: 19:02 - 20:27)

Slide time: 19:02 - 20:27
- A host sends an IGMP report when it joins a multicast group. (Note: Multiple processes on a host can join. A report is sent only for the first process). This means that when a host wants to join one particular multicast group then it sends an IGMP report to the router that it wants to join. 
- On the other side when a particular host wants to leave a group, it does not want this multicast traffic any longer so when it wants to leave that group it does not do anything at all. Only thing is that when the next query comes for this particular group, for which it was a member then it will not respond. So this means that there is some kind of aging in the group membership list that the router would maintain. So, no report is sent when a process leaves a group. 
- A multicast router regularly multicasts an IGMP query to all the hosts (group address is set to 0). 
- A host responds to an IGMP query with an IGMP report. If somebody fails to respond then it is taken that he has left the group. (Refer slide time: 20:27 - 20:51)

Slide time: 20:27 - 20:51
What does the IGMP host reports look like? 
- Host sends a report when it joins a group. 
- It does not report when it leaves the group but does not respond to the next query. So this is the IGMP report, the time to leave is 1, the IGMP group address is group address, destination IP address is group address and source IP address is host IP address. (Refer slide time: 20:51 ? 21:17)

Slide time: 20:51 ? 21:17
For general query, group address is 0, destination IP address is naturally broadcast and source is the routers IP address. 
- So routers sends query at regular intervals to see if anyone still belongs to any group. Queries send out each interface. 
- Host responds by sending one response for each group to which it belongs. (Refer slide time: 21:17 - 21:36)

Slide time: 21:17 - 21:36
IGMP messages are only 8 bytes long. We have Ethernet header, IP header and the IGMP message which is version, type and some part is unused and the checksum and a 32-bit class D address. (Refer slide time: 21:36 ? 22:04)

Slide time: 21:36 ? 22:04
Suppose you have a network with multiple multicast routers. That means the same network but it has got multiple multicast routers. Only one router responds to IGMP queries so this is the Query. So the router with the smallest IP address becomes the Querier on a network. One router forwards multicast packets to the network, so it is the forwarder. If a network happens to be so constituted that there are two routers connected to it and both of them support multicasting in that case, only one router will actually do the querying so out of these two routers whichever has the smaller IP address will be the one which does the querying. (Refer slide time: 22:27 ? 22:38)

Slide time: 22:27 ? 22:38
Now we come to the topic of Multicast Routing. So, what is special in Multicast Routing? (Refer slide time: 22:38 ? 22:52)

Slide time: 22:38 ? 22:52
Let us see this diagram. Suppose there is no support for multicast at the network layer which is the case in many practical situations, even then you could sort of simulate multicasting by doing repeated unicast. So your original source has the list of all the members meaning all their IP addresses so it sends the message to all of them one by one, one by one, one after the other. So this is just a successive unicast done. Therefore this is as if a multicast. Obviously you are more packets are packed into hops if you take some measure like that and of course you are doing much more work than what is strictly necessary but this is all you can do. This has an advantage that the source can closely control that who could be a member of the group and who would not be a member of the group. Therefore you can impose some kind of access control by having an access list. (Refer slide time: 24:05 ? 26:37)



Slide time: 24:05 ? 26:37
Now, if there is support for multicasting then what would happen is that, let us say there is one packet being sent to all three members of the group and say this is the source therefore the source will send only one packet to the next router. This router is going to duplicate the packet as to one on this link and the other on this link. This packet is ultimately destined for this host whereas this packet again gets multiplied and now this packet goes to this host and this packet goes to this host. So, the number of packets traveling down is minimized a lot, of course the final number of packets is the number of users but if you just consider how much each packet travels, that means if you take the number of packets into the hop count kind of a weighted measure then that could be much lower overhead in some sense for packets that are traveling. So it is much lower overhead on the links but may be more overhead on the routers. Now, if you have to have this multicasting capability for the routers this specifically requires two things. One is, packet forwarding that can send multiple copies of the same packet. For example, consider this router, this router is receiving only one packet but it has in its list that there are two sort of users for which the packet is supposed to go through this router. So it has to duplicate the packet one for this link and other for this link. So it forwards multiple copies of the packet. This capability has to be there in the router. Secondly, multicast routing algorithm builds a spanning tree dynamically. But how you found this tree? This looks somewhat similar to forming a routing tree for unicast cases but there are some differences. This tree has to be built up dynamically and in a distributed fashion by the routers. For that they have to run IP multicasting protocol between themselves. That is what we mean when we say routers are supporting multicasting. (Refer slide time: 26:37 - 26:52)
  
Slide time: 26:37 - 26:52
Goal: The main goal of multicast routing protocol is to build a spanning tree between all members of a multicast group. So these are the members of the multicast group and we have to somehow get this tree. (Refer slide time: 26:52 - 27:57)
 
Slide time: 26:52 - 27:57
This can be looked upon as a graph theoretical problem. In whatever graph you have you have to embed the tree such that all multicast group members are connected by the tree. Suppose these are the three members of the multicast group seen in the previous graph and you want to form a tree like this then the only solution is to have a shortest path tree or source-based tree. That is, build a tree that minimizes the path cost from the source to each receiver. So this is the so called source-based tree and you can form this. Hence this is one kind of a solution. (Refer slide time: 27:57 ? 30:24)

Slide time: 27:57 ? 30:24
This is called a Source-Based Tree. 
- This is a good tree if there is single sender. So sometimes multicast group is such that there is a single sender. Take the previous example we were talking about, a central news service or some kind of financial advisory service has some members and these are the members of this multicast group and there are some stock codes. So this particular group may be interested in the quotation of a particular group of stocks and for this particular group of stocks this company collects all the information, the stock value, etc in a regular fashion and keeps on pushing it to the members of this group. Now, this is multicasting where there is a single sender. And in such cases making a Source-Based Tree makes quite lot of sense. But of course, if there are multiple senders you need one tree per sender. Now that becomes really difficult where it becomes more democratic where all the group members are interacting and any of them can send messages to any member in the group. Therefore you have to have a tree for each sender. Having a single sender is easy to compute. For this multicasting router we will always assume that whatever unicast routing is happening through OSP etc is always present here but this multicast is sitting as an additional service by the router which essentially means that the unicast routing table is available for building up your Source-Based Tree. So in such a case this is easy to compute. The tree is built from receiver to the sender. This is called reverse shortest path or Reverse Path Forwarding (RPF). (Refer slide time: 30:24 ? 31:20)

Slide time: 30:24 ? 31:20
A second solution to the same problem is that, if you remember the other graph, this graph looks different because this is the graph where it minimizes the total cost of the edges. If you do not know who your sender is going to be, that means if any member of the group can be a sender then it makes sense to make the tree in such a manner, suppose, if you assume that all of them send packets frequently then in that case having the tree with the minimum total cost of edges would be the optimum solution. So this is the second kind of solution. (Refer slide time: 31:20 ? 32:18)

Slide time: 31:20 ? 32:18
This is very difficult to compute, this is called the Core-Based Tree. 
- This is a good solution if there are multiple senders. Instead of keeping one Source-Based Tree for each potential source we keep one Core-Based Tree. 
- Very expensive to compute, not practical for more than 30 nodes for a very good solution.
- Selects one router as core (also called ?rendezvous point?). 
- All receivers build a shortest path to the core using the reverse shortest path or reverse path forwarding. 
But who would be the core of the rendezvous point depends on how good your core based tree is thereby depending on how you choose the core. If you have chosen the core towards the center of the potential graph then that is good. (Refer slide time: 32:19-34:26)


Slide time: 32:19-34:26
 Let us see the details of Reverse Path Forwarding (RPF). This is the way to build the tree.
- RPF builds a shortest path tree in a distributed fashion by taking advantage of unicast routing tables. 
- Main idea: Given the address of the root of the tree, you know the source. This tree is being formed from the sources. Each of the destinations, that means each of these potential recipients are trying to reach to the source and for that they use the unicast routing table which is already there in the router. So given the address of the root of the tree, a router selects its upstream neighbor in the tree, the router which is the next-hop neighbor for forwarding unicast packets to the root. So, what you do is, for each of the potential recipient you are trying to minimize the path cost from this recipient to the one single source. Right now for the Source-Based Tree you have one single source. Whatever you do for unicasting, while sending a message from this node to that node is what you have to follow. What the routers have to do is that, on the way suppose two different potential routes from two different recipients go through the same router then from this point onwards it is expected that this router to the final destination is actually the source of multicast communication and there is only one path and the tree would be automatically formed. This is the basic idea of Reverse Path Forwarding. (Refer Slide time: 34:27-34:48).

Slide time: 34:27-34:48
- How can this be used to build a tree? 
- RPF Forwarding: Forward a packet only if it is received from an RPF neighbor. 
- Set up multicast routing table in accordance from receiver to sender along the reverse shortest path tree. (Refer slide time:34:49-35:51)

Slide time: 34:49 - 35:51
This is an example. Suppose H1 is the source and RPF neighbor of R3, this is R3 and this is R1, R4 and R5. From these when they try to reach H1 they go from R3 to R2 to some path to H1. So R2 is the RPF neighbor of R3. The destination is H1 and the next hop is R2. This is the unicast routing table. So R3 knows that R2 is the RPF neighbor of itself. (Refer slide time: 35:52 - 36:09)

Slide time: 35:52 - 36:09
- Routing table entries for Source-Based Trees and for Core-Based Trees are different. 
- Source-Based Tree: For Source-Based Tree it is (Source, Group) or (S, G) entry. 
- Core-Based Tree: Naturally anybody can be communicating. So it is (*, G) entry. (Refer slide time: 36:10 - 36:49)

Slide time: 36:10 - 36:49
The Source IP address, Multicast group, Incoming interface (RPF interface) and Outgoing interface are the L2, L3 etc, this is a list. And finally when a packet arrives the router has to forward one copy of the packet along each of these outgoing links which eventually reach some members of this particular multicast group. (Refer slide time: 36:50 - 37:08)

Slide time: 36:50 - 37:08
For building a Source-Based Tree in a network like this set routing tables according to RPF forwarding and then use Flood-and-Prune. (Refer slide time: 37:09 - 39:43)

Slide time: 37:09 - 39:43
- Set routing tables according to RPF as we have already discussed. 
- Flood-and-Prune. 
What is flood? Forward packets that arrive on RPF interface on all non-RPF interfaces. Receiver drops packets not received on RPF interfaces. These routers require the capability of forwarding multiple copies of the same packet. So, if a packet has come from its RPF neighbor, (RPF neighbor is with respect to a particular group) bearing this address means that it is actually coming from the source. Now it has to be forwarded to each of the outgoing interfaces. Of course it is a non-RPF. This means, when I say RPF interface it means that the RPF neighbor is coming from the source side to all others which lead to different members of this Multicast group. What happens if you happened to get a packet from a link for this particular group who is not your RPF member? First of all, how did it happen? You must remember that we are doing this in a distributed and dynamic system so things can come up and go down. So that way a packet can come in. But obviously so far as this particular router is concerned if a packet comes from non-RPF link for this group then this is not coming from the source so that packet is dropped. And naturally it also does pruning. Pruning is sending a prune message when a packet is received on a non RPF interface. This is one case when you prune. Or when there are no group members in its local network and no connection to other routers. Suppose, it so happens that this particular router, you remember that this router is also connected to its local network and with the host in the local network it is running IGMP always finding out who are the members of group etc. It could happen that this local member has retired. It no longer wants to remain in the group that means it is no longer sending your IGMP reports. So it has nobody to send it to nor is it connected to neither any router nor a part of a link from a distant source to a distant destination, nor a transit link like that. So it is not connected in that case also. Now as the local contributor member of the prune has retired then this of course may not be known to others. So it may still get a packet but then what it will do is that, it will send a prune message stating not to send anymore packets to it any longer. So it will send the prune message along the route because the neighboring router has sent in a packet and it has nobody to distribute it to nor is it a transit router. So, naturally it drops this packet because it has no use for this packet. And along this link whoever has sent this packet to it the local member sends a prune message. It means do not send the local member any further packets because it has nothing to do with this group anymore. So that is one case, (Refer slide time: 41:23 - 42:24)

Slide time: 41:23 - 42:24
or, has received a prune message on all non RPF interfaces. That means, it was a member earlier with all the non RPF interfaces and just like the group member may have retired similarly this may also have been a transit router on a link from some distant source to distant destination this may be an intermediate router on the way. But then it has got a prune message on all its non RPF interfaces. That means it was a member of a transit link earlier but now it is no longer a member. So once again whoever had sent it a packet, it will send back a prune message to that destination that prunes this link as well. So this is no longer interested. (Refer slide time: 42:25 - 42:51)

Slide time: 42:25 - 42:51
- Prune message temporarily disables a routing table entry. 
- Effect: Removes a link from the multicast tree. 
- No multicast messages are sent on a pruned link. 
- Prune messages is sent in response to a multicast packet, which has come and which satisfies any of these conditions. (Refer slide time: 42:52 - 43:07)

Slide time: 42:52 - 43:07
Once again that it has received on the non-RPF interface or when there are no group members in its local network and no connection to other routers or it has received a prune message on all non RPF interfaces. So in such cases the prune message is sent. (Refer slide time: 43:08-43:19)

Slide time: 43:08-43:19

The prune message has the effect of temporarily disabling a routing table entry. The question is, why temporary? (Refer slide time: 43:20 - 44:16)


Slide time: 43:20 - 44:16
- Why the routing table is only temporarily disabled? 
- What happens is that, you may have a receiver who may again like to join. So one needs to reactivate a pruned routing table entry in that case. So what happens is that, this group member may have gone away somewhere and now has come back and wants to be a member of the group once again. So, it gets the IGMP report saying that it is a member of this group. Now, it is aware of who knows the source, this multicasting router. So what it will do is that it will try to reactivate this link and then the rest of it will work, so this is called Grafting. 
- Sending a Graft message disables prune and reactivates the routing table entry. So this pruning and grafting are complimentary to each other. You prune to disable and you graft to enable again. (Refer slide time: 44:17 - 45:50)

Slide time: 44:17 - 45:50
Next is the Core-Based Tree. This was a Source-Based Tree when you have one source and many receivers. Now you have many to many kind of situation. That was one to many communications, not one to all (not broadcast). But now we have many to many communications. That means there are many members of the group who might like to communicate with other members of that group. In this case we would like to have what is known as a Core-Based Tree. 
- One router is the core. 
- Receiver sends a join message to RPF neighbor with respect to core. Now every receiver actually wants to join to the core. 
- Join messages creates a (*, G) routing table entry. 
- Source sends data to the core. 
- Core forwards data according to routing table entry. 
Now, since there is no source or that anybody could be a source we put a star in place of s. For a particular router all the links get messages like this so a message may come in through any such link and it has to be forwarded to the other links. (Refer slide time: 45:59 - 46:47)

Slide time: 45:59 - 46:47
We just mentioned about multicast routing protocols which is actually implemented in most of the routers and many of the level 3 switches. This is called DVMRP. So you find them actually but unfortunately I have seen it very rarely being used. But this is there in most of the routers of today as well as in many of the Level 3 switches DVMRP is there 
- DVMRP is a Distance Vector Multicast Routing Protocol. 
- This is the first multicasting routing protocol. 
- It implements flood and prune. 
Distance vector routing, if you remember, the distance vector routing uses the distributed Bellman Ford algorithm which is implemented by RIP. The centralized diesters algorithm, the link state algorithm is implemented in the routing protocol called OSPF. (Refer slide time: 46:48 - 47:58)

Slide time: 46:48 - 47:58
Open Shortest Path First: Recall our discussion about routing protocols and this OSPF is currently the most acceptable routing protocol. There is a multicast extension of OSPF which is known as MOSPF (Multicast Open Shortest Path First). 
- Multicast extensions to OSPF: Each router calculates a shortest path tree based on link state database. 
- It is not very widely used. 
- PIM-SM builds Core-Based Trees but they are not widely used. (Refer slide time: 47:59 - 48:18)

Slide time: 47:59 - 48:18
- Distance-Vector Multicasting Routing (DVMRP) consists of two major components: 
- A conventional distance-vector routing protocol (like RIP). 
- A protocol for determining how to forward multicast packets based on the unicast routing table. (Refer slide time: 48:20 - 49:27)

Slide time: 48:20 - 49:27
- DVMRP (Distance-Vector Multicasting Routing) routers forward a packet if: 
- The packet arrived from the link used to reach the source packet. This is the reverse path forwarding. That means if from this router I want to reach the source then I have to go to that next hop and that is my RPF neighbor. So, if the packet arrives from this link from my RPF neighbor link, arrived from the link used to reach the source of the packet then this is the RPF chain. 
- Packet forwarded only to the child links not in the direction from which it came but other child links. 
- If provided the downstream links have not sent a prune message. (Refer slide time: 49:28 - 52:45)

Slide time: 49:28 - 52:45
But DVMRP has limitations: 
- Like distance vector protocols, affected by count-to-infinity and transient looping. In the count-to-infinity problem some link has failed but nobody could make out that the link has actually failed so it is going round and round known as the count-to-infinity problem where exactly the counting of potential distance to that link comes to infinity. Since we are using the same distance vector routing. Transient looping means sometimes a routing loop may form and the packet goes round and round. 
- Multicast trees are more vulnerable than unicast for these problems. 
- This shares the scaling limitations of RIP. And this scaling limitation essentially comes from what I have written in the last.
- No hierarchy: Flat routing domain. One of the advantages of OSPF over RIP was that in OSPF we break up the network into a hierarchy. There are these autonomous regions or autonomous domains and then further down it can be broken up so that the routing problem remains simpler and you can scale to bigger and bigger networks. Since DVMRP is based on RIP or essentially on the ideas of RIP it is again a problem in DVMRP also. Here you cannot scale and then you have further problem because of multicasting. 
- You may have (S, G) state in routers: even in pruned parts. 
- Broadcast-and-prune has an initial broadcast. When I say flood-and-prune, actually you are flooding the network so there is some kind of broadcast going on. If the network size is small, this broadcast may be acceptable but when the network grows bigger and bigger broadcast becomes unacceptable. So that is again another problem in scaling. 
- This is limited to few senders. Many small groups also undesired. Since this essentially forms a Source-Based Tree you can have only a few of them, just a few senders. Many small groups are also undesired. If you have large number of groups, once again you have the same problem of maintaining so many trees and that also becomes a limitation for scaling. (Refer slide time: 52:46 - 53:29)

Slide time: 52:46 - 53:29
Let us discuss about an effort to implement multicasting. As I told you, most of the routers are not configured to use the multicast in every manner but still some people want to use multicasting. So they built up this Multicast backbone (MBone) which is essentially an overlay network of IP multicast-capable routers using DVMRP. So it uses DVMRP and it is an overlay network of IP multicast-capable routers. What does that mean? (Refer slide time: 53:30 ? 55:05)

Slide time: 53:30 ? 55:05
That means, some of the routers in the network in some places are multicast-capable. And what happens is that they are going to support multicasting in its own locality. That means it will support multicasting amongst the network to which they are directly connected. These routers are going to run a multicasting protocol between themselves. But then in between there are whole lot of other routers. In between there is a cloud of routers which are not supporting multicasting so what it will do is that it will tunnel through this cloud to the next multicast supporting router. So this is the picture of the MBone. You have R which is the host or the router and this R and this H are the MBone routers. They support and the part of the MBone is shown in light blue where the multicasting is directly supported whereas when they try to communicate to another multicast supporting node over a cloud which does not support multicasting they tunnel through it. (Refer slide time: 55:08 - 55:18)

Slide time: 55:08 - 55:18
- MBone tunnel is a method for sending multicast packets through multicast-ignorant routers. 
- IP multicast packet is encapsulated in a unicast IP packet (IP-in-IP) addressed to far end of the tunnel. (Refer slide time: 55:21 - 56:07)

Slide time: 55:21 - 56:07
You have the IP header destination which is unicast and then you have another IP header destination that is multicast and then the transport header. What happens is that, the intervening routers which are not multicast-enabled are going to see this destination and this destination would then actually the next multicast router. And here this part will be the pay load so the network nodes would not look into this. When it reaches the next multicast supporting router it will get this and then discard this and then look at this multicasting header. So this is the IP-in-IP encapsulation and tunneling. (Refer slide time: 56:07 - 57:23)

Slide time: 56:07 - 57:23
- Tunnels act like virtual point-to-point link. 
- Intermediate routers see only router header. That means the unicast routing header. 
- Tunnel endpoint recognizes IP-in-IP (protocol type = 4) and de-capsulate the datagram for processing. 
- Each end of the tunnel is manually configured with unicast address of the other end. So, this is what you have to do. This was done to implement multicasting in an environment and try it out. If there are problems about the one which is actually implemented in most of the routers namely DMRP that does not scale well and if there are many groups, in today?s world when everybody in sort of networked and people have all their special interest etc it is quite considerable that the number of groups will explode if it could really do multicasting in a very easy fashion and that is very difficult for routers to handle. That is why most of them do not use it at the moment. But potentially this is a very useful kind of technology. Thank You. (Refer slide time: 57:27 ? 57:28 )

Slide time: 57:27 ? 57:28
Good day. Today we will talk about some protocols which are useful for controlling the network and making the machines connected to the network. Specifically, under the broadcast we will talk about DHCP and ICMP. There are some protocols associated with this and we will talk about this. (Refer slide time: 57:59 - 58:02)

Slide time: 57:59 - 58:02
(Refer slide time: 58:03 - 58:20)


Slide time: 58:03 - 58:20
DHCP is the Dynamic Host Configuration Protocol. It is about configuring a host, configuring a machine, configuring a may be a PC or some computer which is connected to the network. (Refer slide time: 58:20 - 58:44)

Slide time: 58:20 - 58:44
Its chief utility: There are other utility are DHCP we will be discussing later. The chief motivation came from dynamic assignment of IP addresses. Now, dynamic assignment of IP addresses is desirable for several reasons.

COMPUTER NETWORKS
Prof. S.Ghosh
Dept of Computer Science & Engineering 
IIT Kharagpur 
Lecture 33
DHCP AND ICMP
(Refer slide time: 00:47)

Good day. Today we shall talk about some protocols which are useful for controlling the network and also in keeping machines connected to the network.  Specifically under the broadcast we will talk about DHCP and ICMP. There are some protocols associated with this so we shall discuss about it.   

(Refer slide time: 1:14-1:16)



(Refer slide time: 1:18-1:38)

 DHCP is Dynamic Host Configuration Protocol. It is about configuring a host, configuring a machine may be a PC or some computers connected to the network. 
 

(Refer slide time: 1:35-3:29)
The chief motivation came from the dynamic assignment of IP addresses.  The dynamic assignment of IP addresses is desirable for several reasons:
IP addresses can be assigned on demand.  
For example, when you have a scarcity for real IP addresses then you keep a central pool of IP addresses and then as some computer comes on line it assigns an IP address from the pool and when it goes out those IP addresses are withdrawn and are given to some other machines.  
Another place where it may be required is, suppose you have some kind of a RAS or Remote Access Server to which a number of machines should be connected via dial up connections then in that case you give them a temporary IP address for the connection. Now if somebody wants to visit some network with a laptop then they have to get a network address of that particular network therefore that network address has to be dynamically assigned. 

IP addresses are assigned on demand.
It avoids manual IP configuration which is prone to errors. 
It also supports mobility of laptops. 


(Refer slide time: 3:30-3:59)
Dynamic assignment of IP addresses is done using three different protocols.  
1.   RARP: It was widely used up to 1985 and even beyond this period people kept using it. 
2.   BOOTP: Bootstrap protocol was used until 1993
3.   DHCP: Is used after 1993 and currently this is in wide usage.  
A  Bootp client can also use DHCP server.   
 

(Refer slide time: 4:08-5:33) 

RARP is actually the reverse ARP Address Resolution Protocol. The problem is, given an IP address what is the MAC address. This is for mapping between the IP addresses and the MAC addresses. Finding the MAC address for the IP address is useful when you want to communicate over a LAN. RARP is the reverse of this. Given a MAC address, RARP finds IP address. This would be necessary in case you have something like disclosed work station which boosts the signal over the network. A disclosed work station has its own MAC address and it wants to get an IP address assigned. This is where RARP is typically used. RARP is used to broadcast a request for IP address associated with a given MAC address. RAP server responds with an IP address. It only assigns IP address and not the default router, subnet masks etc that are required and they are not a part of this server.  
Refer slide time: 5:34-5:46)

(Refer slide time: 5:34-5:46)
So IP address to MAC address is the ARP and Ethernet MAC address to IP address is the RRAP.
 

(Refer slide time: 5:45-6:39)
Let us see the improved version of RARP i.e. BOOTP.
BOOTP not only assigns IP addresses dynamically but also has some more functions. Host can configure IP parameters at boot time. Basically there are three services: 
 IP address assignment 
Detection of the IP address for a serving machine
The name of the file to be loaded and executed by the client machine i.e. the boot file name  
This is the source from which it gets the name bootstrap protocol i.e. when the machine is booting up it not only gets an IP address but also gets the name of the file which can be loaded and executed. This is the bootstrap protocol or BOOTP.

(Refer slide time: 6:40-7:37)

 BOOTP not only assigns IP address but also default router, network mask, etc.   Therefore whatever that particular machine requires for communication namely the addresses such as network, subnet mask, gateway etc are given by the BOOTP protocol. This is sent as an UDP message. So UDP port 67 is for server and UDP port 68 is for the host. Port 68 for host is required when you may want to find out a machine from bootstrap protocol which is already available on the network. 
And use limited broadcast address that is 255. 255. 255. 255. If you recall from our discussion about addresses this is a broadcast address where the broadcast is limited to this particular subnet or network.

 
 
(Refer slide time: 7:51-8:14)
BOOTP can be used for downloading memory image for diskless workstations. So whatever was the motivation for RARP the same thing can be done through BOOTP also. But assignment of IP address to hosts is static. This is one sort of drawback of BOOTP 


(Refer slide time: 8:16-8:52)

To make it dynamic we go to the dynamic host configuration protocol which is standard now and more versatile than RARP and BOOTP. It can do a lot of things apart from just giving the IP address. This was designed in 1993 as an extension of BOOTP with many similarities to BOOTP and same port numbers as BOOTP.  That is why DHCP server can handle a few BOOTP clients.  

(Refer slide time: 8:52-10:05)

Extensions: There are lots of extensions especially with options. But these extensions support temporary allocation or leases of IP addresses. Leasing of IP address, suppose when we have a remote access server and when people are dialing what would happen is that, it would be given a particular IP address for a fixed amount of time.  When its lease expires then that IP address may be withdrawn. And half way down the lease period if there is no great demand for IP address then the lease may be automatically extended or if there is a great demand the lease may be withdrawn also. This is for a temporary period of time and that is how it is dynamic. DHCP client can acquire all IP configuration parameters. Not only subnet mask and gateway addresses which are there in BOOTP but also other kinds of parameters can be downloaded from a DHCP server.
  

(Refer slide time: 10:16-10:23)


So DHCP is the preferred mechanism for dynamic assignment of IP addresses and DHCP can interoperate with BOOTP clients.

(Refer slide time: 10:24-12:17)

DHCP has a number of options. It is not possible to mention all the available options here. Other DHCP information is sent as an option so the number of options is actually greater than 100 which include things like subnet mask, name server, host name, domain name, forward on/off, default IP time to leave, broadcast address, static route, Ethernet encapsulation, x window manager, x window font, DHCP message type, DHCP renewal time, DHCP Rebinding, time server SMTP server, client FQDN, printer name etc. As the number of services given over a network grew it became important to give more information to the machines. Originally the machine was used just for communicating between two computers. Suppose there may be a centralized print service in the network and whenever you want to print something it can be done in the network. Similarly all other kinds of services became available in the local network as well as over wider networks. So all these would require some kind of configuration on the host end therefore such information can be transferred via this DHCP.   
 (Refer slide time: 12:18-12:50)

(slide time: 12:18-12:50)

There are a number of DHCP operations. Let us discuss a few of them.  
 DHCP DISCOVER: 
At this time the DHCP client can start to use the IP address.
Renewing a lease: 
It is sent when 50% of the lease has expired. If DHCP server sends DHCPNACK then the address is released. Then you know your lease is not going to be renewed.


                                     ( slide time: 12:49-12:55)

DHCP RELEASE: At this time the DHCP client has released the IP address, so the client has given it up.
 (Refer slide time: 12:56-13:40)

                                       (slide time: 12:56-13:40)

DHCP message header fields:  In some fields there is an opcode. It may be a DHCP request from the client or it may be DHCP reply from the server. The DHCP message type is sent as an option. The hardware type of message is 1 for Ethernet and hardware address length is 6 for Ethernet. Hop count is set to 0 by client and transaction ID is an integer used to match reply to response if there is more then one request.
(Refer slide time: 13:38-14:30) 

(Refer slide time: 13:38-14:30)

Seconds: It is the number of seconds since the client started to boot.  
Client IP address, your IP address, server IP address, gateway IP address, client hardware address, server host name, boot file name, etc. All these fields are available so when the client sends the request it would fill in whatever is known to it, maybe the MAC address is known to it. So it puts in the MAC address and all other fields are left blank.  
 DHCP server will pick up the message that we broadcast and then fill up all the other necessary fields and then broadcast it back.
 

 (Refer slide time: 14:29-14:50) 

The following are the DHCP message types sent as an option: 
DHCPDISCOVER
DHCPOFFER
DHCPREQUEST
DHCPDECLINE
DHCP acknowledge
DHCP not acknowledge
DHCPRELEASE
DHCPINFORM and so on  
 

(Refer slide time: 14:48-14:58

Our next topic is ICMP Internet Control Message Protocol.   
(Refer slide time: 15:05-15:30)
Let us see IP protocol and its deficiencies before that.  



(slide time: 15:05-15:30)

The internet is of course based on the Internet Protocol. IP protocol has some drawbacks. Though it is a best effort delivery service it lacks error control and lack of assistance mechanisms. Since IP is a best effort delivery at some point of time the effort may not be enough and routers ore other nodes on the network may have to drop packets and packets may not reach its destination on time and in proper order.. First of all there is no error control and secondly if such errors do occur there is no message to the sources. Secondly, if you want to control the network for some reason, for example, may be the network is getting congested and so you want to do something about it, but IP does not have the mechanism. So, for all these purposes ICMP was brought in.
 

                                     ( slide time: 16:36-16:55)

Therefore what happens if a router must discard a datagram because it cannot find a route to the final destination? What if the time to live field has zero value? What if it has to discard all the fragments because not all were received in a predetermined time limit? In all these cases IP has to discard a packet.  
 

                                       (slide time: 17:05-17:24)

And similarly there are other situations. For example, may be it has reached the destination but the port is not available. So IP protocol also lacks a mechanism for host and management queries. So ICMP was designed to compensate for these deficiencies. 

                                          (slide time: 17:25-18:01)

 ICMP is a type field that indicates the type of ICMP message being sent and the type may be queries or errors. Code field gives further information specific to the ICMP message. For example, when an error occurs it tells what kind of error it is.   Checksum field is used to verify the integrity of the ICMP data. So once again the checksum is included to control the error.  


(Refer slide time: 18:00-18:24)
There are two types of ICMP messages. One is error reporting and the other is query response. If there is some error then the error reporting type of ICMP message would be generated and if there is a query another type of ICMP message would be generated.  
 

(Refer slide time: 18:23-18:29)
There is no effort in ICMP to correct the errors. This is the job of some other layer. So it does not really try to correct the errors but nearly reports the errors. The error messages are sent to the source. Suppose the datagram has been sent and something has happened to it and due to that there is an error, and now whoever drops that packet send an ICMP message back to the source. It may be a router on the way or may even be the final destination.  
 

(Refer slide time: 19:23-19:50)
These are the various types of errors in error reporting. There may be a destination unreachable, there may be a source quench sending to first. Some of the important ones may be time exceeding, some may be parameters problem or redirection, etc.   


(Refer slide time: 19:51-21:29)
Please note that, no ICMP error message would be generated in response to a datagram carrying an ICMP error message. That means, somebody has generated an ICMP error message and it is traveling back to the source, and that error message itself gets an error and may have to be dropped on the way, then in such cases we do not generate another ICMP message. A little bit of problem happens due to congestion of networks. So if the network is very congested many packets may get dropped. And then if in response to dropping many packets you generate more packets then the congestion is not going to go away. So, ICMP error messages do not trigger other ICMP error messages for a fragment datagram that is not the first set of fragment. For example, the datagram may have been fragmented into a number of parts, may be fifty parts, now for each of them you generate an ICMP message. Then the ICMP message would be too many so it is only generated for the first fragment. For a datagram having a multicast address, once again we cannot send an ICMP messages to all members of the group and for a datagram with a special address such as 127. 0000 or with some address like 0.0.0.0.0 also no ICMP error messages are generated for these.  
 (Refer slide time: 21:30-23:15)

All error messages contain a data section that includes the IP header of the original datagram plus the first 8 bytes of data in that datagram. This information is required so that the source can inform the protocols about the error. From the original packet that was dropped the IP header of that original packet is sent back. First of all you need to know the source and know where you want to send back this ICMP message.  Secondly, even after this ICMP message gets back to the specific machine from which the original packet was generated. At this point it may have some error messages due to network intervening or this may have to do something with some process or application which is running on the source machine. So, after getting the message the host must know to which process it relates to. After the IP header what comes is the transport layer header so, a part of the transport layer header also goes back along with the ICMP message .This information is required so that the source can inform the protocols about the error.  
  

(Refer slide time: 23:16-23:50) 

Destination Unreachable: This is one type of an error message. When a router cannot route a datagram or a host cannot deliver a datagram, then in that case the destination is unreachable. A router cannot detect all problems that are preventing the delivery of a packet. So it is not always possible to exactly know why the destination is unreachable. But at least this information that the destination is unreachable, goes back to the source.  
 

(Refer slide time: 23:51-25:30) 
Source Quench: This is a crude attempt to implement some kind of flow control. IP protocol has got no flow control. Routers and hosts have limited size queues. So what happens is that, may be in an intermediate router and a number of packets have come up and certainly there is a flood of packets into one intermediate router from various directions. So what would happen is that its buffer is going to overflow and it will not be able to process because there is a limit depending on the speed of the router etc, there is a limit as to how fast packets can be processed and forwarded by an intermediate router and if other packets keep coming in, within that time they are going to be stored in the buffer where in the buffer might overflow. In that case the router cannot do anything else but to drop those packets. This router desperately wants to tell other people in the network to slow down on sending packets and that it cannot handle it because of overload. Basically it tries to slow down the flow of packets into itself. So it sends the source quench ICMP message towards the sources. If datagram is received faster than they can be processed the queue may overflow and in that case it asks the network to slow down.  

(Refer slide time: 25:32-25:43) 

If a router or host discards a datagram due to congestion it sends a source quench message to the sender. The source must slow down the sending of datagram until the congestion is relieved.  
 

(Refer slide time: 25:44-26:03) 
This may be used when bottlenecks occur. For example, on a WAN link with too much congestion it is used to reduce the amount of data lost. But a warning is, source quench message will in turn generate network congestion. There were already too many packets in the network but you have sent a source quench packet towards the source which is just one hop towards the source was already getting packets from the source but will also get an ICMP message from the router just one hop down so it is having more packets now. So by this way congestion might travel towards the source but anyway finally it reaches the source and the source will hopefully slow down and all these will die.  

(Refer slide time: 26:48-27:18) 

Time Exceeded:  Whenever a router receives a datagram with a time-to-live value of zero that means it has been going round the network it discards the datagram and sends a time exceeded message to the source. When the final destination does not receive all of the fragments in a set time it discards the received fragments and sends a time exceeded message to the source. These are two different cases: One is that, in the destination all the fragments did not reach so there was a specified time after which it has to drop all the fragments and send a time exceeded message to the original source.  
The other thing is that, when a router receives a datagram with the time-to-leave field which is zero. If you remember, keeping a time-to-leave field and decrementing it at every hop is quite important because suppose there were some packets which were floating around in the network and due to some trouble with the routing tables a loop has been formed, so, if you do not have this time-to-leave field it will go round and round at infinite term where they slowly burden the network. So, the solution to that was, after a certain number of hops the packet is dropped and when a packet is dropped a time exceeded message is sent to the source.  
There may be parameter problems.
 

(Refer slide time: 28:35-28:48) 
If an ambiguity is found in the header of a datagram the datagram is discarded and a parameter problem message is sent back to the source.  


(Refer slide time: 28:47-28:59) 

Redirection: A host usually starts with a small routing table that is gradually augmented and updated. One of the tools to accomplish this is the redirection message, so, actually this helps in routing.  
 

(Refer slide time: 29:00-29:18) 

Now let us come to queries. ICMP can also diagnose some network problems. For example, echo request and reply, time stamp, address mask, router solicitation and advertisements, these are example of queries. We will just see a few of these also. 


(Refer slide time: 29:22-30:01) 
Echo request and reply: Is used very often when you want to find out whether the network is up and running or not. An echo request message can be sent by a host or router. An echo reply message is sent by the host or router which receives an echo request message. The echo request and echo reply message can be used by network managers to check the operation of the IP protocol.  
   Echo request and echo reply message can test the reachability of a host. This is usually done by invoking the ping command. Later on we will get into more details of ping because that is one kind of command which even users quite often require. For example, if you are logged on and you find that you cannot reach your destination anywhere in the network then you have to find where the problem lies, is it in your local network or in the local subnet. Therefore in the local subnet you might ping that gateway to see whether you can reach up to the gateway. If your ping message goes up to the gateway and you get an echo reply then you know that up to that much the network is ok. And if you are ok up to the router you may want to ping the router in the entire network. Now the problem may be somewhere in the link outside. The problem may even be in the destination which you are trying to reach. So one way is to go probing the network, even by users is to use ping.    
 (Refer slide time: 31:16-31:47)

(Refer slide time: 31:16-31:47)
Timestamp Request and Reply: Timestamp request and timestamp reply messages can be used to calculate the round-trip time between a source and a destination machine even if their clocks are not synchronized.  So, sending time is equal to the value of receiving time stamp minus value of original time stamp.  So this way you can get some idea about the round-trip time. There are other ways also.  


(Refer slide time: 31:49-32:12)

So receiving time = time the packet returned ? value of transmit timestamp. Round-trip time = sending time + receiving time.  So the timestamp request and timestamp reply message can be used to synchronize two clocks in two machines if the exact one-way time duration is known.  
 

(Refer slide time: 32:14-32:21)

Address-Mask Request and Reply: Enables a host to request and receive the network or subnetwork mask. It is useful for diskless stations at start up. But we have seen the DHCP is another way of handling this.  

(Refer slide time: 32:23-32:43)

Router Solicitation and Advertisement: Allows request of routing information and the reply of this information. Routers can periodically send router advertisements without being solicited. Suppose a router has just been connected to the network, anyway the routers have to run the routing protocol like the RIP or BGP, OSP etc, this means it needs to communicate to the neighboring routers, but how do the other routers know there is a new router in the group. So, one way is, as soon as the router gets connected it does some router solicitation and it advertises itself so that other routers get to know that and slowly the entire network becomes aware of this new router which is connected. Similarly, a link may go down and all kinds of other things may happen. So, the exchange of router information has to happen through some mechanism.  

(Refer slide time: 33:44-34:15)
Router Discovery Message: Host can learn about available gateways to other networks. Host send the router solicitation message to begin the process using the multicast address of 224.0.0.2 as the destination. It can also be a broadcast message in case a router does not accept multicast messages. When a router receives the message it will advertise its available gateway.  
 (Refer slide time: 34:16-34:28)

(Refer slide time: 34:16-34:28)
The checksum of the ICMP message: In ICMP the checksum is calculated over the entire message, that is the header and data combined. This is just to keep some control over errors.   

(Refer slide time: 34:26-34:48)
Clock Synchronization: Software may require time synchronization. So ICMP time stamp message combats this problem. It allows local host to ask for the current time from a remote host using ICMP timestamp request. So it is type 13.  
 

(Refer slide time: 34:47-35:04)
Remote host uses ICMP timestamp reply which is type 14. So, the better way of synchronizing the clocks is to use the network time protocol. The time is the UT Universal Time.  


(Refer slide time: 35:05-35:08)

Ping and Traceroute:    

(Refer side time: 35:15-35:55)
This is an overview. This is part of the ICMP messages. Ping sends an ICMP message to a remote host and lets you determine if that host is responding.  Actually ping uses echo and echo reply for the ICMP message. Traceroute uses TTL fields to query all hosts enroute to a specific destination. you can use traceroute to map a network. That means, if you want to know which is the route you are tracing then this helps you.   

(Refer slide time: 35:58-36:46)
Ping is named after sonar. In sonar if you want to probe some place you send an ultra sound signal just like you do in radar and if it bounces of something you get a ping, so that is where the name comes from. If you want to send an echo request you expect an echo reply and that is your ping. So server normally implemented in kernel uses ICMP echo and echo reply messages. On UNIX the identifier field is set to UNIX PID or sending process. Sequence numbers starts at 0 incremented every time a new echo message is sent. Actually, when you ping a machine not just one request is sent. The machine you are trying to ping or the channel may be noisy and if that happen then your echo request or the reply may get dropped in between. So, sending one request is not sufficient and may be three times or five times etc you can configure it, it sends echo request and it expects all the three or all the five replies. And if it receives none of them, then in that case it will say that hundred percent of packet loss or it may get two out of five so it will say sixty percent of packet loss or forty percent.  
 (Refer slide time: 36:37-39:46)


(Refer slide time: 36:37-39:46)
Let us see one example of ping. Suppose we ping a machine
144. 16.182.1, we have pinged this machine and then give the IP address over here. By the way if you have a name server on the network you could also put the name over there. Ping 144. 16.182.1, 56 data bytes is your data plus it will have some thing. So you may get a result like this: 64 bytes, this is what you are getting from the echo reply, 64 bytes from 144 16 182 1. ICMP sequence = 0 time-to-leave is 240 and time = 37 milliseconds. So it gives you some idea about how much time it takes. Then another packet has came back as an echo reply 64 bytes from the same machine, sequence number 1 and time is so much. For each packet it receives back as reply it is going to print a line like this and then finally it will give you a statistics as something like this: 13 packets transmitted, 11 packets received which means that it had originally sent 13 packets and it got only 11 packets back so 2 packets must have got lost. Therefore it is a 15% packet loss. And the round-trip time you may calculate the mean, average, max.  So, from the ping you can get an idea about the round-trip time.   


(Refer slide time: 39:51-40:12)

Some details on the output sequence number are shown for each message. In our example message returned in order but we lost some packets. They may be returned due to out of order. Also TTL field of return message is displayed and round-trip time is calculated at the host based on the sequence number.  
 (Refer slide time: 40:14-41:45)

We can estimate not only the round-trip time but also the bandwidth using ping.  But this works only for few hops. If it is beyond a number of hops your ping will not work. The ping packet can estimate the bandwidth in this way: 20 byte IP header, 8 byte ICMP header, 56 byte message this can be set by the user, so the total datagram size is that + 76 +8 = 84 bytes so 84 bytes were sent. Now, if it was sent through PPP it will add about 8 bytes so the total size will be 92 bytes.   So this connection looks like 92/.180/2 that is about 1069 bytes per second.  What is this .180? It is 92 bytes so this is time. This gives you some idea about what kind of bandwidth you have. In this particular case the bandwidth is not that much but it is only about 1069 bytes per second. This is a very crude estimate but you can get some kind of feel about your immediate locality.  

(Refer slide time: 41:56-42:23)
Record Route Option: Most ping implementations provide record route which is ? R option on linux, ? r option on windows.  Each router stores its address in the IP options field, only 9 addresses are possible. Thus round-trip is only possible for 4 routing hops. So you can take only 4 hops and within those 4 hops you can find out that how your message went and how it came back. That is, may be it came back through different paths or it could have returned in the same path etc.  You can actually trace the route and because of the limitation on the size that is on the number of addresses you can store you can only route or map the network in your immediate locality. But if you want to go beyond this then you have to use something else called Traceroute.  

(Refer slide time: 43:12-46:16)

Traceroute uses a sequence of ICMP messages to determine the current route to a particular destination. This is actually done in an iterative fashion. Suppose I want to traceroute to a distant machine whose IP address is known. Then I will send a message to that machine but with a very small number for the time-to?live. Therefore what will happen is that my message will take so many hops but then it has not reached the destination so may be it must have just started and it will be somewhere in the beginning so its time-to-leave is going to become 0.  As soon as the time-to-live becomes 0 the intermediate node may be that router will have to drop the packet and it sends an ICMP message back to the source. Now my program gets this ICMP message and now it sends the same dummy message to the destination after increasing the time-to-live by one unit. Now it is going to pass that router that had dropped the packet in the previous instant and so it will go one more hop and then the packet will get dropped so that router is now going to send an ICMP message back to the source. Now we will know which router is on the way. Therefore by this way iteratively you keep on increasing the time-to-leave one by one and you trace the entire route, that is, you map it out.  
      But let us see what happens when it reaches the destination? When it reaches the destination what happens is that this message is sent to a very unlikely port, a randomly selected port. Most probably the destination machine will not know about this port so it will say that the port is unreachable and then that ICMP message will come back. Now we know that we have reached the destination. Hence this way we have traced the entire route one by one from the source to the destination.  Traceroute uses a sequence of ICMP messages to determine the current route to a particular destination. The TTL specifies the number of hops a message can travel. Trace route sends UDP datagrams while varying the TTL. The router that drops the UDP packet now replies with a time exceeded ICMP message.  
 

(Refer slide time: 46:20-46:40)

The end point will not reply with that ICMP message because it has already reached there. So traceroute sends to an unlikely UDP port. Eventually get a no such port ICMP message. It knows that it has reached the end.  

(Refer slide time: 46:41-46:46)

So this is the reference about ICMP messages. Actually these are not the only internet control message protocols but there are a number of others which we did not discuss. We just discussed a few of them. There are other protocols like DHCP, BOOTP, RARP, ARP.  For example, they help in running the network in a better fashion. ARP protocol is a low level protocol. Then we have this RARP, BOOTP and DHCP for assigning a network. This ICMP helps in controlling the network operation and giving error messages. Then there is another side protocol which we will discuss in the next class namely IGMP which is internet group management protocol. So, that is another part of routing that we have not discussed as yet.    
Preview of the next lecture
Lecture ? 34
DNS & Directory

Good day, so today we will take up two topics and they are DNS and directory.  First let us talk about DNS.

(Refer slide time: 48:29-48:33)



(Refer slide time: 48:35-50:00)

The DNS is a short form for Domain Name System.  Until now we have seen two kinds of addresses. One is MAC address or the so called hardware address and in case of Ethernet they are also called Ethernet addresses which is used in the data link layer for direct communication.  
             Then we have seen IP addresses which are used for communication  between two end points and these two end points may be anywhere in the network. So IP address includes information used for routing and there is a network part and post part etc.  
             But unfortunately these IP addresses are tough for humans to remember. You can remember only a small part of address which you basically require for your own configuration like your own IP address, address of your gateway, address of your mail server etc. But beyond that if you have to remember IP addresses of other people it becomes very difficult for human beings to remember.  
          And of course they are impossible to guess. Humans find it much easier to remember and use the domain names. Domain names are what people use in surfing the web and for a www site sometimes you do not know the name exactly but you make out some guesses by some combination of .com or .net etc. We not only can guess but it is much easier to remember. We remember so many site names.  But now, what is the name? It is used to map some particular machine or site.  Therefore this is also some kind of an address.
          So we have a third layer which are the domain names. Today we will see how we use these domain names. And of course just as in the local area network you require mapping from IP addresses to MAC addresses which is done by the ARP protocol and in the reverse MAC address to IP address done by RARP. Here you need a mechanism for mapping the domain names to IP address.  

(Refer slide time: 51:51-52:36)

We have discussed this, why not centralized DNS>? Single point of failure, traffic volume, distant centralized database would not work, maintenance would be a problem, does not scale so it is distributed. So, no server has the entire name to IP address mapping. Local name servers: Each ISP Company has local name or the default name server and host query first goes to the local name server. Authoritative name server:  For a host, towards that hosts IP address names can perform name address translation for that host name.    


(Refer side time: 52:37-53:10) 
   
There are some root name servers which are [[?.centric]] and some of the biggest name servers are in USA but of course it depends on what root it is and they could be distributed also.  
 

(Refer slide time: 53:10-53:36)

NSLOOKUP is an interactive resolver that allows the user to communicate directly with a DNS server.  So, from the OS you can use this NSLOOKUP and give a name query.  This is actually a name server look up and that is how the term NSLOOKUP comes. NSLOOKUP is usually available on UNIX workstations.   

.   (Refer slide time: 53:38-53:55)

Servers handle requests for their domain directly. Servers handle request for other domains by contacting remote DNS servers. Servers cache external mappings.
 

(Refer slide time: 53:56-54:21)
If a server has no clue about where to find the address for a hostname it asks the root server. The root server will tell you what name server to contact. A request may get forwarded a few times.  For example, if the IITKGP has a name server.  Now the IITKGP has a name server and request for a particular domain name translation has come to it and it does not know to whom to connect the name server so it can always transfer it to the next higher level namely ERNET and if the ERNET also does not know where this ERNET is there then it can contact the IN. IN will definitely know all the sub domains under it. IN has to know because it is administering that domain.  
      Similarly if it is for some address which is from outside you can send it directly to the root of that particular domain, suppose from India you are trying to contact something for Japan then you can send it to the JP root name server and then JP root name server would know which name sever to contact so that will come back.  So this way the DNS queries will go back and forth few times and finally the name will be resolved.  

(Refer slide time: 55:30-56:39)

Now we come to LDAP which is the Lightweight Directory Access Protocol.  Since this was designed by the same people who has designed OSI this X.500 actually tends to be a little complex. It is heavy and it uses the all the seven OSI layer for the internet purpose. Actually it uses the TCP IP stack rather than the seven layer OSI layer stack. There was a lightweight directory access protocol which can interoperate at least on one side. So the LDAP can use that X.500 directory service but this is much simpler than X.500 and LDAP is used in many places. These are Lightweight Directory Access Protocol. It supports X.500 interface, it does not require the OSI protocol. It uses the TCP IP protocol so this is X.500 for the internet crowd. It is useful as a generic addressing interface like Netscape, address book and so on.  

(Refer slide time: 56:41-56:57)

The LDAP or Lightweight Directory Access Protocol is a networking protocol for querying and modifying directory services running over TCP IP. An LDAP directory usually follows the same X.500 model which we have discussed.  


(Refer slide time: 56:58-57:24)
Now it is a tree of entries, each of which consists of a set of named attributes with values. An LDAP directory often reflects various political geographic and or organizational boundaries depending on the model chosen. When you do that you can also define your security policies based on this directory and based on these boundaries. These are especially authentication services.  
 

(Refer slide time: 57:33-57:40)
So directory is a tree of directory entries.  An entry consists of a set of attributes an attribute values.  The attributes are defined in the schema.  

(Refer slide time: 57:43-59:00)

So this is the protocol stack for LDAP. Suppose you have a directory based application or some authorization service or may be access to some information which may be there for the organization which uses LDAP may use TLS. This TLS is Transport Level Security. You can also use SSL.  Here we are talking a lot about security. In future we will give one lecture to security because this has become so important. Now, in an organizational context a directory may be an important component of the entire security arrangement. Security is a complex issue. But anyway for this LDAP we need to communicate securely in many cases and many LDAP implementations support this TLS the Transport Level Security or you can use SSL or SASL also.  
	






COMPUTERS NETWORKS
Prof. Sujoy Ghosh
INDTAN INSTITUTE OF TECHNOLOGY
IIT KHARAGPUR
Lecture - 34
DNS & Directory 
(Refer slide time: 00:42)

Good day. Today we will take up two topics, DNS and directory. First let us talk about DNS. (Refer slide time: 00:54-00:56)

Slide time: 00:54-00:56
DNS is the short form for Domain Name System. (Refer slide time :00:57-4:19)


Slide time: 00:57-4:19
 We have seen two kinds of addresses. One kind is MAC address or the so called hardware address. In the case of Ethernet it is also called as Ethernet addressed which is used in the data link layer for direct communication. Then we have seen IP addresses used for communication between two end points. The end points can be anywhere in the network. So, IP address includes information used for routing. IP is used for routing where there is a network part and post part etc. But unfortunately, this IP address is tough for humans to remember. We can remember only a few addresses that we really require for our own configuration, like our own IP address, address of gateway, address of mail server etc. Beyond that it is very difficult for human beings to remember IP addresses of other people. They are impossible to guess. What human beings find is, it much easier to remember and use the domain names. Domain names are used most of time by the people for surfing the web (www site name used for surfing). While surfing the web for a www site, sometimes you may not know the name exactly but you can make it out in 3 or 4 guesses, may be make some combination of .com, .net etc. The most important thing here is that it is much easier for humans to remember site names. The name is used to some particular machine, site etc. This is also some kind of an address and we have the third layer namely the domain names. We will see how we use these domain names. Just as in the local area network, you require a mapping from IP addresses to MAC addresses done by the ARP protocol and the reverse MAC addresses to IP addresses using RARP. You need a mechanism for mapping the domain names to IP addresses. That is what this whole scheme of DNS is all about. May be sometimes we will look into the reverse query also. (Refer slide time: 4:20-5:58)


Slide time: 4:20-5:58
- The domain name system is usually used to translate a host name into an IP address. 
- Domain names comprise a hierarchy so that names are unique, yet easy to remember. 
It is important to remember that, if it is an address it needs to be unique. In IP v4 we ran out of IP addresses because we had limited length. Here, we can go easily with the length because these are not to be used in high speed computations but may be used just once or twice in a session. This means you can allow longer names, that is, rather than 4 bytes you can use many bytes but then this also has to be unique. What people thought was, if we make a hierarchy of names which is a logical hierarchy which corresponds with the external world then not only they will be remembered easily but can be easily administered and we can make them unique. So, domain names comprise a hierarchy so that names are unique yet easy to remember. (Refer slide time: 5:59-6:48)
  

Slide time: 5:59-6:48
This is what the hierarchy looks like. There is a root and from the root we have the top level domain. Here we have edu, com, org and so on. And finally these are for some nations. Under edu, we have mit, Albany and all other kinds of organizations. Under in, there may be ernet and all that. Under mit, also there may be cs or something. So there is a tree and a hierarchy. This is sort of a global hierarchy and is the DNS hierarchy. (Refer slide time: 6:49-8:34)


Slide time: 6:49-8:34
- Each host name is made up of a sequence of labels separated by periods. 
o Each label can be up to 63 characters long
o The total name can be at most 255 characters. So, you have up to 255 bytes to code these names so you can go easy on the length 
- Examples: Whitehouse.gov, .gov is for government and since it started in USA, it is US government and whitehouse.gov is a domain. Similarly let us say csc.iitkgp.ernet.in, .in is a top level domain standing for India and ernet is an organization which comes in the next level in in. Under ernet, there is iitkgp which comes in the next level and under iitkgp there is csc which again comes in the next layer. So, starting from here you can go from the leaf of the tree right up to the root of the tree that is the DNS hierarchy tree. And, after giving a label at each level we put a dot and then go up to the next level. This is how we name hosts. (Refer slide time:8:35-9:04)


Slide time: 8:35-9:04
- The domain name for a host is the sequence of labels that lead from the host to the top of the worldwide naming tree.
- A domain is a subtree of the worldwide naming tree 
So, mit is a domain, ernet is a domain and so on. There is a subtree under ernet and there will be many sub domains. So any domain is a subtree of the worldwide naming tree. (Refer slide time: 9:05-9:25)


Slide time: 9:05-9:25
- A host has a domain name specified using a sequence of names, each of which may be up to 63 characters long, separated by periods. 
- Names are case sensitive.
- A domain is an absolute domain name or a fully qualified domain name (FQDN), if it ends with a period. (Refer slide time:9:26-10:41)


Slide time: 9:26-10:41
- Most generic domains (.com, .edu, etc) are international, but there are some like .gov for government and .mil for military are US-specifications. So when you say .gov, it actually means US government and .mil is for US military. If you remember, the history of internet started in USA from the ARPA net and it evolved. At that time, it was very US centric but now the whole world has embraced it. 
- New top level domains recently been proposed, though they are not very popular yet.
- Countries each have a top level domain (2 letter domain name). For example: in for India, jp for Japan, uk for UK and so on.
- A system is required to map the domain names to IP addresses. Just like we have ARP for IP address to MAC or RARP for MAC to IP address, we need a system to map domain name to IP address that is the chief one and may be the reverse also. (Refer slide time:10:42- 12:45)
 

Slide time: 10:42- 12:45
Implementing DNS: 
- Distributed database implemented in hierarchy of many name servers. This is distributed because this is widely used and you cannot have a centralized database. Centralized database will be much more difficult to administer, Contribute to a single point of failure and the network traffic at that node will be tremendous. You will not be able to handle the network traffic if everybody tries to login to the same central name server. That will not scale and it needs to be distributed. Later on we will study the way it is distributed and the way it is administered. What are distributed are the name servers (servers with gives you the mapping from the domain name system to the IP address). 
- There is an application-layer protocol host, routers, name servers, which sort of all combine and communicate to resolve names (provide this address/name translation). 
o Note: Core internet function is implemented as application-layer protocol
o Complexity at networks edge so that at the networks core where the traffic is very high and very heavy this is not present. If you had to do the domain name servers at the core routers then it will be of much strain on the routers so they have been put to the edge of the network (Refer slide time:12:46-14:29)


Slide time: 12:46-14:29
Full Resolver: For resolving we use resolvers, and there are full resolvers and stub resolvers.
- The client for this naming system is called resolver. This is transparent to the user and is called by an application to resolve names into real IP-addresses or vice versa. When you type a domain name in your browser, the browser is a client application program for http and so its calls the resolver to translate these domain names to corresponding IP addresses. The browser will send the http request to the web server using that IP address. So it gets it from the resolver.
- A full resolver is a program distinct from the user program, which forwards all queries to a name server for processing. It knows about the name server. 
- Responses are cached by the name server for future use, and often by the name server. The local full resolver in your host may connect to the local name server. If the local names server does not have the resolution, it is going to contact other name servers and finally will get the resolution. We will look into the details later. (Refer slide time:14:30-15:19)


Slide time: 14:30-15:19
In this diagram, the user program gives a user query to the full resolver. The full resolver gives this query to the name server. This name server has its own database, so it may look up in its own database. If it is not there then it may send the request to foreign name servers and finally it will give a response. The full resolver also maintains a cache and it will cache this response so that if it gets another request to resolve the same name then it can find from the cache and give the response to the User. Name server also maintains its own cache. (Refer slide time: 15:20-16:16)


Slide time: 15:20-16:16
- A stub resolver is a routine linked with the user program which forwards the queries to a name server for processing. Responses are cached by the name server but not usually by the resolver. There are two differences between a stub resolver and a full resolver. One is that, full resolver has a cache but stub resolver usually does not, and secondly, stub resolver has to be linked with the user program where as the full resolver runs by itself. 
- On UNIX, the stub resolver is implemented by two library routines: gethostbyname() and gethostbyaddr() for converting host names to IP addresses and vice versa. (Refer slide time:16:17-16:34)


Slide time: 16:17-16:34
Stub resolver is a part of the user program which gets linked and this routine sends the query to the local name server and gets the response and passes it on. Naturally the user program gets it. (Refer slide time: 16:35-20:04)


Slide time: 16:35-20:04
DNS Organization: Is a distributed organization.
- Distributed Database 
o The organization that owns a domain name is responsible for running DNS server that can provide the mapping between hostnames within the domain to IP addresses. 
o So some machine run by say an organization like RPI is responsible for everything below within the rpi.edu domain. 
o There is one primary server for a domain, and typically a number of secondary severs containing replicated databases. 
So, it is important to understand how this is organized. This is a global system, which means this is very big. So any effort to control this centrally would become quite difficult. Let us say csc.iitkgp.ernet.in is a domain. Now .in is a domain, which is for whole of India. There is one specific organization (in India) which looks after this domain. So whatever sub domains are there under this domain, it is the responsibility of that organization to keep track of who all can be given sub domains. That is, who all can be given names within these sub domains.in. Also, any name resolution query coming from anywhere else in the world will direct their query first to this organization who is maintaining the domain .in. 
Also there are so many organizations maintaining their own sub domains under the domain .in. Either that organization will have it in its cache or it will forward the query to the particular domain. Suppose if it gives sub domain ernet, ernet again gives sub domains to iitkgp And again under iitkgp there may be many sub domains but ernet does not really bother about whatever sub domains are there under iitkgp. It only has to know that iitkgp is an organization under ernet which has got a name and a separate domain. It is for iitkgp to decide how it is going to break this domain into further sub domains or it may not break at all. This way the entire domain administration is decentralized and it is easy, and also these names are some kind of addresses that has been made unique. Since the names are broken into names which stand for actual organization at any level, naturally any particular organization administering a domain will not have two organizations having the same sub domain name under it. This way all the names automatically become distinct which is another good advantage. (Refer slide time: 20:05-20:54)


Slide time: 20:05-20:54
Why not centralize DNS? 
- Single point of failure. 
- Traffic volume. 
- Distant centralized database should not work 
- Maintenance should be a problem 
- Does not scale so new server so its distributed server
- No server has all names to IP address mappings 
Local Name Servers: 
- Each ISP Company has Local Name Server (Default Name server) 
- Host DNS query first goes to the local name server 
Authoritative name server: 
- Sometimes we may come across something that has been given by authoritative server 
- For a host stores that hosts IP address, Name 
- Can perform name/address translation for that hosts name
(Refer slide time: 20:55-21:27)


Slide time: 20:55-21:27
There are some root name servers. The root name servers are actually still mostly US centric and have some of the biggest name servers. So, root name servers are all in USA but of course that depends on what root it is and therefore they could also be distributed.   (Refer slide time: 21:28-21:54)


Slide time: 21:28-21:54
nslookup: 
- nslookup is an interactive resolver that allows the user to communicate directly with a DNS server.
- From the OS, you can use this nslookup and give a name query so this is actually name/server look up. This is why it is called nslookup
- nslookup is usually available on UNIX workstations.  (Refer slide time:21:55-22:12)


Slide time: 21:55-22:12
- Servers handle request for their domain directly 
- Servers handle request for other domains by contacting remote DNS server(s) 
- Servers cache external mappings (Refer slide time:21:13-23:43)


Slide time: 22:13-23:43
Server Operation:
- If a server has no clue about where to find the address for a hostname, it asks the root server 
- The root server will tell you what name server to contact 
- A request may get forwarded a few times 
Let us say the iitkgp has a name server. A request for a particular domain name translation has come to it. It does not know to whom to connect the name server.. So it can always transfer it to the next higher level, namely, ernet. If the ernet also does not know where in .in it is there, then it can contact in. in will definitely know all the sub domains under it. It has to know because it is administering that domain. If it is for some address which is from outside you can send it directly to the root of that particular domain. Suppose from India you trying to contact something for Japan, you can send it the jp root name server and then jp root name server would know which name server to contact. DNS queries will go back and forth few times and finally the name will be resolved. (Refer slide time: 23:44-24:03)


Slide time: 23:44-24:03
Server  Server Communication:
- If a server is asked to provide the mapping for a host outside its domain (and the mapping is not in the server cache): 
o The server finds a name server for the target domain 
o The server asks the nameserver to provide the host name to IP translation 
- To find the right nameserver, once again it can use DNS  (Refer slide time: 24:04-25:20)


Slide time: 24:04-25:20
Recursion:
- A request can indicate that recursion is desired - this tells the server to find out the answer (possibly by contacting other servers) 
- If recursion is not requested - the response may be a list of other name servers to contact 
So there are two versions of this DNS server, the recursive server and iterative server. If the recursion is requested and that is honored, suppose I am a name server, I will send the request to a name server whom I think may have a clue about how to do this request recursion. Now, he will make all the other contacts, the next level or may be another level contact as necessary and finally send me the answer. So this is the recursive version. In the iterative version what happens is that, he will simply give me a list of name servers to whom I can possibly contact directly to find out more. Then I make some more requests and finally get the name resolved. (Refer slide time: 25:21-25:48)


Slide time: 25:21-25:48
For example, host1 makes a request to the local names server. Then the local name server may send the request for recursion to another name server. So, it will make the necessary request, get the response and then give the response. (Refer slide time: 25:49-26:13)

Slide time: 25:49-26:13
In the iterative case, it knew in one shot but it may not be in one shot. When recursion is requested this may go deep down and finally find out and then send the request. (Refer slide time: 26:14-26:37)

Slide time: 26:14-26:37
Whereas in the iterated queries, he gives a query and gets some list, he gives it to the next name server and gets some more response. Then finally he will get it and send it for you and then it will be resolved. This is an iterated query. (Refer slide time: 26:38-26:59)

Slide time: 26:38-26:59
DNS caching and updating of records: 
- Once (any) name server learns mapping, it caches mapping 
- Cache entries timeout (disappear) after some time 
- Update/notify mechanisms under design by IETF (how the cache is to be managed)  (Refer slide time:27:00-27:46)

Slide time: 27:00-27:46
We will not going into all the details, although they are given in the slide. These are the so called resource records or RR. The RR (resource records) have a format Type = A where the name is hostname and value is IP address. This is the most common one. Type= NS where name is domain and value is IP address of authoritative name server for this domain. Type = CNAME where name is an alias name for some canonical (the real) name and value is canonical. Type = MX is for mail server records, so these are the various resource records which are handled by the DNS. (Refer slide time: 27:27-28:02)

Slide time: 27:27-28:02
This is the DNS message format. We have a header followed by some queries, followed by some resource records and some authority records if it is from alternative name server and may be some additional information. This is the DNS message format. (Refer slide time: 28:03-28:18)

Slide time: 28:03-28:18
We have in the message header, a 16-bit # for identification, 16-bit # for query, and reply to query uses the same # and then we have a number of flags (Refer slide time:28:19-28:44)

Slide time: 28:19-28:44
- QR flag identifying a query (0) or a response (1) 
- Op code is a 4-bit field specifying the kind of query: 
           0 for standard query (QUERY); 1 for inverse query (QUERY) 
- Server status (STATUS). Other values are reserved for future use (Refer slide time:28:45-29:10)

Slide time: 28:45-29:10
- AA: is authoritative answer that means this it is coming from an Authoritative Name Server
- TC: to see if the response has been truncated. Actually if it is truncated, it switches from UDP to TCP
- RD: recursion desired
- RA: recursion available etc 
- rcode: is return code (Refer slide time:29:11-30:19)


Slide time: 29:11-30:19
Both UDP and TCP are used by name server:  
- TCP for transfers of entire database to secondary servers 
- UDP for lookups. The lookups that we discussed about, usually use the UDP protocol. UDP protocol is used as it is a very simple protocol with no extra over head. But there is a limit to that UDP and in response to UDP you will just get one packet which is just 512 bytes 
- If the response requires more than 512 bytes, then the requester resubmits request using TCP. If the message is truncated the flag is set and when the client sees the flag it opens a TCP connection with the corresponding name server and then resubmits the request so that it can get a longer request (Refer slide time:30:20-30:46)


Slide time: 30:20-30:46
We have already discussed the administration zones. 
- A zone is a sub tree of the DNS tree that is independently managed. 
o Second level domains (ernet.in) are usually an independent zone 
o Most sub domains (iitkgp.ernet.in) are also independent. Independent means that what happens under this sub-domain is their business 
o Eg: Most universities have departmental domains that are then independently administered  (Refer slide time:30:47-32:24)


Slide time: 30:47-32:24
- A zone must provide multiple name servers. This server records the members in the domain 
- You typically need a primary name server and one or more secondary name servers 
- Secondary retrieves information from primary using a zone transfer, using a TCP connection 
The reason why the secondary name server is required is sometimes more than one secondary server is kept and the reason is that it is very vital for everybody. Suppose if you are administering a domain and you want to be independent then you can create extra sub domains under you where you have to maintain your own name server. You cannot maintain just one name sever but you have to maintain multiple name servers so that when the primary name server fails the secondary can immediately take over. So, from time to time the primary will cache the data and from time to time or the database will be shifted to the secondary one so that they remain more or less in sync. Therefore as soon as primary goes down the secondary can start acting as a primary and be the authoritative name server for this particular domain. There will be requests from other people to you for IP addresses in your domain and you are bound to give that. That is the reason you need to have these different name servers and good network connection to handle all these requests. (Refer slide time: 32:25-33:02)





Slide time: 32:25-33:02
There is a reverse query which is sometimes used as a sort of weak protection against spoofing. 
- Set q = ptr i.e. ptr is the reverse query for lookup. So here what is done is that, an IP address is given and the domain name is found out. Just as we have ARP and RARP similarly we have query and reverse query. (Refer slide time:33:03-33:42)


Slide time: 33:03-33:42
Reverse queries are used as a weak mechanism to avoid spoofing. This is weak because those machines may have a name but if it connects to the network through a modem and is given a dynamic IP address then sometimes it may not get reflected. So sendmail uses an identity message to identify the sender and receiver but this can easily be spoofed. ( Refer slide time:33:43-35:18)


Slide time: 33:43-35:18
- Some mail servers do not forward mail if you are not in the domain. This is for anti-spam. Spams are mails which are spuriously generated mostly by programs/machines which are automatically generated and sent to millions of people. It is estimated that the majority of the mails which go through are actually spam. If the mail is not for my domain then I am not going to relay this to other mail servers. Sometimes you can do that but sometimes you have to relay because it is your responsibility to relay. And does not accept mail if it can reverse query your IP address. As soon as a mail comes it will find out from which IP address this mail has come from and send a reverse query. By reverse query the name looks responsible may be it is coming from some known University etc then accept the mail. If you cannot reverse query it then maybe it is coming from some spurious source and it does not accept. 
- This is not totally secure because hosts on same physical network can spoof IP headers of domains which are sort of respectable, but anyway this maybe of some use. ( Refer slide time:35:19-35:35)


Slide time: 35:19-35:35
Who manages the in-addr arpa space? 
- When you get a portion of the IP space you also become responsible for handling the in-addr.arpa queries for that space. 
- This is why queries are in reverse order. ( Refer slide time:35:36-35:57)


Slide time: 35:36-35:57
Dynamic DNS: 
- DNS maps domain names to specific IP addresses. 
o This requires that each domain name is statically assigned, since the zone table is typically stored on disk. 
- This implies that a host using a dynamically assigned IP addresses means you connect to that host. ( Refer slide time:35:58-36:34)


Slide time: 35:58-36:34
This is what dynamic DNS is for. This is not widely implemented but there is an RFC for this and it uses a secure connection for doing a dynamic DNS update. You have to sort of update the name server database and anybody cannot use this name server and so you have to use a secure connection to update the DNS database. ( Refer slide time:36:35-37:58)


Slide time: 36:35-37:58
Next, we take up the topic of directories. This is similar to DNS but more general. Actually the directories sometimes uses DNS,. Many of you have used mails and email clients where you have your own address book from where you can look up the email address of those who have sent you mails. One example of a directory service could be some kind of a global address book where we can find the email address of anybody. This was the original intention for creating the directory. But of course people who produce spam mails have sort of killed this idea. So nobody wants his email address to be in a directory which is universally available so that everybody can send junk mails. Anyway, directory can be used for many other purposes as well. ( Refer slide time:37:59-38:48)


Slide time: 37:59-38:48
- What directories are...? 
o They are object repositories 
o Typically read more than written 
o Have explicit access protocols 
o Support relatively complex queries 
DNS queries are simple but it supports relatively complex queries. For example, you can have something like give me the email addresses (assuming that email addresses are still available) of all people who live in Delhi whose name contains ram etc. You may be looking for somebody so you can have more complex queries than you have in a DNS. ( Refer slide time:38:49-39:03)

Slide time: 38:49-39:03
But directories are not meant to be RDBMSs, they are just for looking up. So, lack notions of tabular views, join operations, stored procedures etc. They are not regular RDBMS but they are just for this particular service. ( Refer slide time:39:04-39:41)


Slide time: 39:04-39:41
X.500 was originally how directory was envisaged by the telecom industry. 
- The goal was to have global white pages 
o Lookup anyone anywhere 
o Developed by telecom industry 
o ISO standard for OSI networks 
- Idea was distributed directory 
o Application uses distributed directory structure 
o Application uses directory user agent to access a directory access point ( Refer slide time:39:42-39:53)


Slide time: 39:42-39:53
The picture is something like this; you have a directory user here who uses a directory user agent which in turn connects an access point to the directory. ( Refer slide time:39:54-40:39)


Slide time: 39:54-40:39
- How is the name used? 
o Access resource given the name 
o Build a name to find a resource
o Information about resource.
o These are the different uses of names 
- Do only programs look at these names? Sometimes humans also need to use the names for constructing the names recalling names. 
- Is resource static? 
o Sometimes resource may move 
o Change in location may change the name of a particular resource. 
- Performance requirements 
o Human scale ( Refer slide time:40:40-41:12)


Slide time: 40:40-41:12
Directory information base which is defined in X.501 is given as a tree structure.  
o Root is the entire directory 
o Levels are groups. For ex: country, organization, individual. 
- Entry structure 
o Unique name build from tree 
o Attributes: Type/Value pairs 
o Schema enforces type rules 
- There may be alias entries also. ( Refer slide time:41:13-41:53)


Slide time: 41:13-41:53
Directory structure may look something like this. You have these different levels, which starting from the top may represent some organizations and then some sub organizations and finally you have the objects. Now, in an object entry you will have some names and each of these names should have a series of type value pairs. There may be more than one particular name; it may have a number of attribute and each attribute will have a type/value pair. This is the general structure of a X.501 tree structure. ( Refer slide time:41:54-42:28)


Slide time: 41:54-42:28
- Query is to this system defined in X.511 
o Query is a read, get selected attributes of an entry 
o Compare does an entry match a set of attributes? 
o List children of an entry 
o Search, Abandon request etc 
o These are all kinds of queries are possible 
- Modification you can modify these records  add, remove, modify entry 
o Modify distinguished names etc ( Refer slide time:42:29-43:14)


Slide time: 42:29-43:14
- There is a directory system agent. 
o It may have some local data 
o Can forward request to other system agents 
o Can process requests from user agents and other system agents 
So, these are like the name server system with dissolvers and also this is the system using directory system agents which can do the query processing. It may get the data locally or it may forward the request to other systems. 
- Referrals: 
o If DSA cannot handle the request it can make request to other DSA just as you can make iterative and recursive queries in DNS. 
o Or tell DUA to ask other DSA, this is the iterative process. ( Refer slide time:43:15-44:14)


Slide time: 43:15-44:14
- Directory information can be protected. Actually they are usually protected. 
- There are two issues: 
o Authentication defined in X.509 
o Access control defined in X.501 
- This directory by itself does not give you security. So you have to have other components in order to ensure security. But a directory can be used for some authentication services, some security purposes, etc directory can very well be used. They are actually used that way. 
- Standards specify basic access control and individual DSAs can define their own access control. They can specify to whom they are going to allow access to their local databases. ( Refer slide time:44:15-44:59)


Slide time: 44:15-44:59
Replication: This is defined in X.525. 
- Single entries can be replicated to multiple DSAs. Just like you have a primary name server and a number of secondary name servers, similarly you can have a directory in a primary or master and then you can have replication. 
- Two replication schemes: 
o Cache copies - On demand 
o Shadow copies - Agreed in advance from time to time 
- There is a transfer, copy required to enforce access control. 
o When entry sent, policy must be sent as well 
- Modification is done at the master only 
- Copy can be out of date 
o How to handle that is defined in X.525 ( Refer slide time:45:00-45:30)

Slide time: 45:00-45:30
There are a number of protocols which are defined in X.519. 
- Directory access protocol means the structure of the query and other things would be defined. 
- Directory system protocol 
o Request/response between DSAs 
- Directory information shadowing protocol
o DSA-DSA with shadowing agreement 
- Directory operational binding management protocol. There are a number of protocols.. ( Refer slide time:45:31-46:07)


Slide time: 45:31-46:07
- Uses are of course for Look-up 
o Attributes, not just distinguished name 
o Context 
- Humans can construct likely names 
- Browsing 
- Yellow pages 
o Aliases also may be given 
- Search restriction/relaxation may be there 
- Groups may be defined that means having a number of members who will be having they are own attributes 
- Authentication information that may be contained in the directory and so on 
o Directory may be used for various purposes ( Refer slide time:46:08-47:20)


Slide time: 46:08-47:20
We will look at LDAP, which is: 
- The lightweight directory access protocol designed by the same people who designed OSI. X.500. It actually tends to be a little complex, it is heavy and intuits the OSI all the seven layers. Now for the internet purpose which actually uses the TCP/IP stack rather than the seven layer OSI stack there was a lightweight directory access protocol which can interoperate at least on one side, that LDAP can use that X.500 directory service. but this is much simpler than X.500 and LDAP is used in many places. 
- This is a lightweight directory access protocol. 
o Supports X.500 interface 
o Doesnt require the OSI protocol. This uses the TCP/IP protocol 
o So this is X.500 for the internet crowd 
- Useful as generic addressing interface 
- Like Netscape address book, etc ( Refer slide time:47:21-47:35)


Slide time: 47:21-47:35
- The LDAP or lightweight directory access protocol is a networking protocol for querying and modifying directory services running over TCP/IP. 
- An LDAP directory usually follows the X.500 model (Refer slide time:47:36-48:11)


Slide time: 47:36-48:11
- It is a tree of entries, each of which consists of a set named attributes with values 
- An LDAP directory often reflects various political, geographic, and/or organizational boundaries depending on the model chosen. When you do that you can also define your security policies based on this directory and based on this boundary, especially authentication service. (Refer slide time:48:12-48:21) 


Slide time: 48:12-48:21
- A directory is a tree of directory entries 
- An entry consists of a set of attributes
- An attribute values pair 
- The attributes are defined in the schema. (Refer slide time:48:22-49:58)


Slide time: 48:22-49:58
This would be the protocol stack for LDAP. You have a directory based application, some authorization service or access to some information which may be there for the organization which uses LDAP. LDAP may use TLS (Transport Level Security). Actually you could use SSL also. In future we will give one lecture to security because it has become so important. Now, in an organizational context, a directory may be an important component of the entire security arrangement. Security is the complex issue. But for LDAP we require that we communicate securely in many cases. And many LDAP implementations support this TLS Transport Level Security. We can also use SSL or SASL and these uses TCP. TCP sits on the IP which sits on the other layer etc. So it comes in between the directory based application and the TCP layer. This is where it stands in the protocol stack. (Refer slide time: 49:59-50:50) 


Slide time: 49:59-50:50
This mentions some of the operations which are in LDAP,: 
- Bind - authenticate and specify LDAP protocol version. This actually starts the process. 
- Start TLS - protect the connection with Transport Layer Security to have a more secure connection. Since you are giving some access to information, you need to put some security feature in that. So that is the start TLS. 
- Search - Search for and/or retrieve directory entries. If you go through the access controls then you can search for some records. (Refer slide time: 50:51-51:08)


Slide time: 50:51-51:08
- Compare - test if a named entry contains a given attribute value 
- You can add a new entry 
- You can delete an entry 
- You can modify an entry. These are various LDAP operations. (Refer slide time:51:09-51:37)


Slide time: 51:09-51:37
- Modify DN - move or rename an entry 
- Abandon - abort a previous request 
- Extended operation - generic operation used to define other operations 
- Unbind - Close the connection, not the inverse of bind.  But anyway this is to close the connection, so these are roughly some operations in lightweight directory access protocol. (Refer slide time:51:38-53:18)

Slide time: 51:38-53:18
We will just touch on this security issue: 
- Notion of security for a network protocol is comprised of at least these axes: 
- Identity and Authentication: 
o Who are you and who says so? So identify yourself and then it should have some protection against spoofing that somebody is claiming to identity which is false. 
- Confidentiality: Whatever information is being passed around, other people should not be able to snoop into it so confidentiality is important. You might use some kind encryption for this confidentiality purpose. 
- Integrity - Did anyone muck with this data? This means, did any one change the data? If there is a change which has done by some person who is authorized to do that you might want to keep a log or audit trail for such changes otherwise you want to be sure that on the way somebody has not changed this data. It may be necessary for some applications to maintain some signature kind of a thing to see that the data has not been changed. 
- Authorization: Yes, you can do that, but no, you cannot do that other thing. This means there some organizations in access control user. These identity, confidentiality, nativity and authorization kind of accesses are definitely there. ( Refer slide time:53:19-53:44)


Slide time: 53:19-53:44
- One needs to separately consider each of the four security axes in the context of anticipated threats. 
- Also need to consider security from the perspectives of 
o the information stored in the directory, and 
o Attributes of the requesters 
- Data security is not equal to access security. ( Refer slide time:53:45-55:06)


Slide time: 53:45-55:06
- Some typical security features of LDAP implementations: 
- Simple password based Authentication. 
- SSL on a particular port 636. 
- SSL is secure socket layer and TLS is transport layer security. This is on port 389 
- There is some Access control
- There is some Configurability. There are other things that you can do with the directory especially in the context of an organization. Therefore because of spams and other issues the idea of actually generating a very global white pages for everything under sun, such a grand idea did not really work out but in the context of specific organization with its own security boundary and its own needs, LDAP is a well defined is a protocol which can be used. 

Good Day. The topic for today is Congestion Control. (Refer slide time 55:07-56:18)


Slide time: 55:07-56:18

The performance of computer networks on a large extent depends on the kind of congestion present in the network. So, actually this is a large topic and we will just touch upon some aspects of them. One thing we know by now is that, in general network namely data network or internet in particular, although multimedia and other content are coming in, first of all this is a packet based network and a large data network where we make only best effort of delivering a packet. What exactly do you mean by best effort? Most of these best efforts have to do with how we handle the congestion? (Refer slide time 56:19-56:52)


Slide time: 56:19-56:52
What is congestion? When too many packets are pumped into the system congestions occurs leading into degradation of performance. Here you can give selective acknowledgement that you have got something in particular but have not got that. Issue: Reno and new reno retransmit at most one lost packet per round-trip time.
Selective acknowledgement: The receiver can acknowledge non continuous blocks of data which means SACK Selective Acknowledgement of 0 to 1023, 1024 to 2047 and so on. (Refer slide time 56:53-57:17)


Slide time: 56:53-57:17
- Multiple blocks can be sent in a single segment 
- TCP SACK: 
o Enters fast recovery upon three duplicate ACKs 
o Sender keeps track of SACKs and infers if segments are lost Sender retransmits the next segment from the list of segments that are deemed to be lost, like fast retransmit (Refer slide time 57:18-58:22)


Slide time: 57:18-58:22
Many people have tried various kinds of heuristics to improve the performance of TCP. There are two competing demands. One is that we have to maximize the throughput and if you can maximize the throughput, naturally the overall delay, congestion will be small and at the same time you will get your job done faster. But in order to push this maximum throughput we should not get into congestion or absolutely no congestion collapse so we try to guard against that. These are the various versions or various flavors of TCP for doing that. So, we are going to look at some topics associated with this congestion control and one is traffic engineering. This means, can you shape or handle your traffic in a particular way so that congestion is less likely to occur. (Refer slide time 58:23-58:53)


Slide time: 58:23-58:53
All these build to give quality of service in a network. At routers, these may depend on Packet Classification and Packet Scheduling. At network entrance it may have to do with Traffic Conditioning. At routers or somewhere in the network you may do admission control. Between hosts and routers you may do signaling. Therefore these are the different components of QoS network. 



COMPUTER NETWORKS
Prof.Sujoy Ghosh
Department of Computer Science and Engineering
IIT, Kharagpur
Lecture-35
(Refer start time 00:42-57:33)

Good day. The topic for today is congestion control. 
(refer slide time 00:48-02:00)

SLIDE 00:48-02:00
The performance of a computer network depends on a large extent on the kind of congestion that is there in the network. Once again this is a large topic. We will just touch upon some aspects of them. One thing we know by now is, in general network, may be data network or inter network in particular although multimedia and other contents are coming in, this is a packet based network and a large data network that is announced where we make only the best effort of delivering a packet. Now, what exactly do you mean by best effort? Most of these best efforts depend on how you handle congestion. 
(refer slide time 02:01-02:38)

SLIDE 02:39-04:18
So, we know what congestion is. When too many packets are pumped into the system, congestion occur leading into the degradation of the performance. Congestion tends to feed upon itself and backs up. Congestion shows lack of balance between various network components. Moreover it is a global issue because the congestion may happen in some intermediate router because of packets being pumped from various sources, so in that sense it is a global issue. (refer slide time 02:39-04:18)

SLIDE 04:19-04:45
We have this intermediate node or channel etc and the demand is in the form of various sources pumping in packets at various rates namely ?1 to ?n and this is being serviced by the channel capacity or the router capacity at the rate ? and going to the various destinations. The problem is, the demand outstrips available capacity. So this is basically the congestion problem and added dimension to this problem comes from the fact that although these ?, ? etc are coming from the general queuing theory and for simple queuing theory these are the arrival rate and service rates etc. Although queues are there, but in general the statistics which the data networks follow are rather complicated. So traditionally telecom networks would follow some Poisson distribution with exponential interval time. But in data network it is seen that it follows something called as a self similar traffic or heavy tail distribution and that is a complex distribution. One of its key features is bustiness. That means data tends to come in busts and then there is comparatively long quotient period and again another burst comes. So, that is the problem which has to be handled and when several bust arrives at a node at the same time that particular node may get overloaded.  (refer slide time 04:19-04:45)

SLIDE 04:46-05:43
So, if information about ?1, ?? etc is known in a central location where control of ?n and ? effected instantaneously with zero time delays then the congestion problem is solved. Unfortunately we cannot do that because we have incomplete information and we require a distributed solution with time varying time delays. This is what makes the problem a little difficult. (refer slide time 04:46-05:43)

SLIDE 05:44-06:12
Already we have seen this kind of throughput versus load curve we have seen already when we saw alocha networks, ccna cd etc. But, in general this is what happens. As the load increases the throughput also keeps on increasing at the same rate and then it starts sort of going down and it keeps going down because of the intermediate delays and other bottlenecks coming into the picture. And then there is an area where the throughput does not increase any longer. If you increase the load beyond that there is a catastrophic fall in the throughput. So this part is known as the knee and this part is known as the cliff and this catastrophic fall is called congestion collapse. (refer slide time 05:44-06:12) 


SLIDE 06:13-06:42
So, knee is the point after which throughput increases very slowly and delay increases fast. Cliff: Point after which throughput starts to decrease very fast to zero and this is congestion collapse and delay approaches infinity.
 Note in an M/M/1 queue delay =1/(1- utilization).
It does not follow this kind of a simple formulation. 
(refer slide time 06:13-06:42)

SLIDE 06:43-07:03
If this was the previous curve and as you plot the delay, the delay is low in the beginning and starts increasing. And in this area where there are lots of packet losses and there is a congestion collapse and there are throughput collapses the delay also becomes very high and it becomes a hyperbolic curve. So obviously you have to take all precautions not to fall into this area. 
(refer slide time 06:43-07:03)

SLIDE time 07:04-07:45
So, we talk about congestion control whose goal is to stay on the left of cliff that means not to go into the congestion collapse. Congestion avoidance: Goal is to stay left of the knee and right of cliff is of course the congestion collapse region.
(refer slide time 07:04-07:45)



SLIDE time 07:46-09:15

So, the goal of congestion control is to guarantee stable operation of packet networks and a sub goal is to avoid congestion collapse. To keep networks working in an efficient manner, for example: high throughput, low loss, low delay high utilization are the goals not always achievable especially because we have distributed systems with insufficient information about the global picture but anyway that is there.
 (refer slide time 07:46-09:15).


SLIDE time 09:16-10:47

To provide fair allocation of network bandwidth among competing flows in steady state. So, there has to be some kind of fairness, Sometimes all are not taken as equal. First of all you must see, if there is congestion at an intermediate node, what would happen is that there would be lot of packet loss over there. So, various packets from various sources would be lost and the delay would become high. Many of them were running a TCP protocol. Therefore the question is who would again start retransmitting. Hence what would happen is that, more packets would be lost and more and more packets will keep on getting pumped. This is just like a traffic jam, it starts at one place and then if the jam does not resolve soon then it becomes bigger and bigger and it starts getting pushed towards the source. So the overall network throughput goes down and people tend to push more packets. These are the kinds of scenario we would like to avoid.  (refer slide time 09:16-10:47)

slide time 10:48-13:18
Now, there are various policies at various levels that we can take for congestion control. Let us look at the data link layer, the open loop policies. One is retransmission policy. How would you retransmit? One example of this re transmission policy is, suppose you have a Ethernet network with csmacd going on and then you have detected a collision, the question is, are you becoming persistent or non persistent or you do a random back off or exponential back off  or what is your retransmission policy. Hence you will try again.
Similarly there are other things like out of order policy. Out of order policy is when you receive a packet when it is out of order. Acknowledgement policy is, do you acknowledge or do not acknowledge it. For example, if you have an acknowledgement then the acknowledgement for each packet also takes up resources. So, if you acknowledge every packet then there is going to be as many packets sent as many acknowledgements. Therefore this is a lot of acknowledgement for the network and there is a high overhead. May be you take a policy of not acknowledging all the packets. It could be some kind of a flow control policy. Therefore we have seen some kind of flow control in TCP. We will look into more details and variations of it. (refer slide time 10:48-13:18)

SLIDE time 13:19-14:25
In the network layer you can have the virtual circuit versus datagram. This is an important issue and we will look at it in more detail when we discuss QOS and multimedia communication. Suppose there is a very important communication going on between two hosts, mission critical or whatever it may be, now they will be exchanging a lot of packets. Let us say the packets are flowing in only one direction so lot of packets will be sent and we want some premium service for this. If you want to give a premium service to this particular pair of nodes, may be they pay more or something then in that case you have to distinguish among the packets and assume that they form some kind of a flow. So you need to have a kind of virtual circuit between these two points in order to distinguish them. If all packets are on their own then that is a different kind of a situation where it will be more difficult to distinguish the flow between the two specific nodes. Virtual circuit versus datagram may be an important issue. Once again we will see more details of this when we look at RSVP, diffsservent in the QOS when we discuss QoS a little bit more in detail. 
Packet Queuing and Service policy means, in the router a number of packets may come and they are going to be serviced one by one and they are going to be put in a queue. Now the question is, do you put them in one queue or do you put them in several queues? Do you have the same priorities for all the queues or do different priorities for different queues and so on. Packet discard policy has to do with the buffer management of the router. If the buffer becomes full, which packet do you drop?
Routing algorithm: What kind of routing algorithm will you use?
 Packet lifetime management: This means the lifetime of the packet is over and you drop the packets. These are important in the network layer. So far we were discussing about open loop polices. (refer slide time 13:19-14:25)

SLIDE time 14:26-15:26
Now let us take a look at closed loop control. This means monitoring the system to detect when and where congestion is occurring. Pass the information to places where actions can be taken. Adjust system operation to correct the problem. This is more sophisticated and better. The point is, if one router in between is congested and now if it can identify the chief sources of trouble where it cannot handle a lot of packets, therefore if you could send a feedback back to the source so that he can control this behavior by sending less number of packets then the situation can be handled. Or you can adjust some system parameters, may be the window size in TCP etc. These are the examples of the kind of thing you can do with close loop control.
(refer slide time 14:26-15:26)

SLIDE time 15:27-16:07
If you say that there is some congestion then we need to have some metrics for measuring congestion. Some examples are percentage of all packets discarded due to lack of buffer space. This may be one measure.
 Average queue length in the buffer
 No of packets that time out and are retransmitted
 Average and standard deviation of packet delay
 These may be metrics with which you measure congestion. So, if these metrics go beyond a certain level then you might decide that some congestion is taking place and you need to take some action in order to prevent the performance degradation in a sharp manner. (refer slide time 15:27-16:07)

SLIDE time 16:08-16:44
Feedback mechanisms: It can be many. As we have mentioned, router on sensing congestion sends a control packet to the source. A bit in every packet can be reserved to announce congestion. Explicit probe packets can be sent to ask about congestion. Implicit algorithms make only local observations. 
(refer slide time 16:08-16:44)

SLIDE time 16:45-17:56
Then you can try adjusting system operations. Adjust time constants to a near optimal value. Decrease the load selectively if possible. May be if one source somehow can decrease then all the others can be served very well because it goes below a threshold. Increase resources if possible, this is usually difficult. .(refer slide time 16:45-17:56)

SLIDE time 17:57-18:29
Now, let us look at the one aspect of congestion control which is very important and which is done by TCP all the time. TCP congestion control, if you remember uses a sliding window protocol. We have a window and a sender can send right up to the window size to the other side and it will wait for acknowledgements and he will keep on acknowledging and once he gets the acknowledgement the window will slide. This is about the basic TCP. What we are going to see now is some variance of TCP. Since TCP is a very important protocol and application protocols like FTP, STP, etc use TCP, a lot of important traffic on the net is actually carried on TCP and that is why whatever we do at the TCP level is very much important. And one of the chief tool for doing any congestion control by TCP is by adjusting the window size. So, there are various variants. 
(refer slide time 17:57-18:29) 

SLIDE time 18:30-19:16
TCP has a mechanism for congestion control. The mechanism is implemented at the sender. The sender has two parameters, congestion window with a variable called cwnd and slow start threshold value with a variable called ssthresh. So, initial value is the advertised window size. So, with a TCP connection there is an advertisement of window size and this window size is taken as the initial ssthresh value. (refer slide time 18:30-19:16)

SLIDE time 19:17-20:06
Congestion control works in two modes. One is slow start and the phase is slow at start when the cwnd value is less than ssthresh and congestion avoidance means that cwnd value is greater than equal to ssthresh. So, basically we are trying to figure out whether we are on the left of knee or in the right of the knee. So, if you are on the right of the knee but left of the cliff we are going to be careful. If you are on the left of the knee and if things are going fine then we can try to increase the load stress to increase the overall throughput. This is the basic idea. (refer slide time 19:17-20:06)

SLIDE slide time 20:08-21:17
Knowing initial values in a slow start i.e. set cwnd = 1.
Naturally if the window size is small i.e. one so one unit will go and the acknowledgement will come back and then only something else will go from this side. That is why we are being very conservative and we are sending only a small bit of information. 
 Note: the unit is a segment size i.e. one of a second. TCP is actually based on bytes and increments by 1 MSS (Maximum Segment Size).
 The receiver sends an acknowledgement (ACK) for each packet. So this is the slow start. So, the receiver must acknowledge every packet, so the first packet it receives it can send an acknowledgement. 
Note: Generally a TCP receiver sends an acknowledgement (ACK) for every other segment.
 (refer slide time 20:08-21:17)

SLIDE time 21:18-21:35
 Each time an ACK is received by the sender, the congestion window is increased by 1 segment. So what happens is that, the sender has sent one packet so it has got the acknowledgement, so actually the sender decides that things are fine and may do better. That means it is the increase in congestion window size (cwnd). So we increase cwnd by 1 i.e. cwnd = cwnd + 1.
 We make cwnd = 2. If an ACK acknowledges two segments cwnd is still increased by only 1 segment. That means for every ACK it increases by 1. If it acknopwledges onlys one segment or two segments then cwnd is increased by one only. Actually the reason to acknowledge every other segment is to decrease the number of acknowledgements. Now, even if ACK acknowledges a segment that is smaller than MSS bytes long cwnd is still increased by one. So, at anytime you get an ACK you increase cwnd by one when you are in the slow start phase. Although it starts slowly does it increment slowly? Not really. In fact, the increase of cwnd is exponential.
(refer slide time 21:18-21:35)

SLIDE time 21:36-22:24
the congestion window size grows very rapidly, cwnd rises very rapidly. For every ACK we increase cwnd by 1 irrespective of the number of segments ACK?ed.
 The TCP slows down the increase of cwnd when cwnd > ssthresh .
(refer slide time 21:36-22:24)

SLIDE time 22:25-23:00
As you can see, suppose if it sends one segment it receives one acknowledgement, the cwnd is increased from one to two. Now you can send two segments, segment two and segment three. It will get back the acknowledgement for segment two and acknowledgement for segment three. Now cwnd has become 4. It will send 4, 5, 6, etc, so three of them it has sent and the acknowledgement for 4, 5, 6 will come. Now cwnd has become 7 and it will send more. You can see here, 1, 2, 4, 7 is increasing quiet fast because for each acknowledgement it is increased by one and when you are sending so many segments at a group you will get many acknowledgements. Therefore cwnd is increasing exponentially. (refer slide time 22:25-23:00)

SLIDE time 23:01-23:21
Congestion avoidance phase is started if cwnd has reached the slow start threshold value (ssthresh). 
If cwnd>=ssthresh. Then each time an ACK is received, increment cwnd as follows: i.e. cwnd=cwnd+1/[cwnd] where [cwnd] is the minimum or the larger integer and is smaller than cwnd. So this is only increased by a fraction while sending. Of course you will not send a fraction and whatever be the current cwnd value that is floured that many segments you can send. (refer slide time 23:01-23:21)

SLIDE time 23:22-23:42
So, cwnd is increased by one only if all cwnd segments have been acknowledged. That means, if all the cwnd have been sent or acknowledged, then cwnd increases only by one. So we are very cautious while we move to the right of the knee. 
(refer slide time 23:22-23:42)

SLIDE time 23:43-24:26
So, assume that ssthresh is 8 therefore what will happen is, round-trip time = 2, 4, 6, etc. As time is going so cwnd is first increased exponentially. It reaches the ssthresh value and then it increases slowly. (refer slide time 23:43-24:26)

SLIDE time 24:27-25:02
TCP assumes there is congestion if it detects a packet loss. Now, what is the response to congestion? TCP assumes that if congestion detects a packet loss a TCP sender can detect a lost packet via timeout of a retransmission timer or receipt of a duplicate ACK. Duplicate ACK has been received which means that previously may be some acknowledgement has been dropped and there is a duplicate ACK. So, when something is dropped it means that there may be congestion. Now there are different ways to respond to this congestion. (refer slide time 24:27-25:02)

SLIDE slide time 25:03-25:20
One is, TCP interprets a time-out as a binary congestion signal which means there is congestion as soon as there is a timeout. Therefore when the sender performs cwnd is now reset to one
i.e. cwnd = 1 so once again it becomes very conservative.
 ssthresh is set to half the current size of the congestion window Ssthresh =cwnd/2.
Before sending it to one whatever be the cwnd value you divide it by two and make it the new threshold value and enter the slow start again. 
(refer slide time 25:03-25:20)

SLIDE time 25:21-25:36
so initially: cwnd is equal to one 
i.e. cwnd =1 and ssthresh = advertised window size.
 New acknowledgement (ACK) is received: If (cwnd < ssthresh)
/* Slow Start*/ cwnd = cwnd + 1; else 
(refer slide time 25:21-25:36)

SLIDE time 25:37-25:58
Congestion avoidance cwnd = 1/cwnd + 1/cwnd
Cwnd = cwnd + 1/cwnd.
 If there is timeout it is multiplicative decrease 
 i.e. ssthresh = cwnd /2 and cwnd = 1.
 (refer slide time 25:37-25:58)

SLIDE time 25:59-26:32
this is the typical plot of cwnd for a TCP connection(MSS=1500 bytes) with TCP Tahoe: TCP Tahoe is one flavor of TCP we have been discussing. We will discuss about some other flavor also. So, if cwnd goes on increasing, decreasing and then after sometime again increasing while things are good then this may be a typical plot. (refer slide time 25:59-26:32)

SLIDE time 26:33-27:36
TCP Tahoe uses one flavor, 
slowstart for every acknowledgement
Congestion avoidance that means only beyond the ssthresh it increases slowly
Fast retransmit.
In TCP Reno there is also another version of TCP   
Uses fast recovery
 And then there are some versions like New Reno, SACK,RED, etc. (refer slide time 26:33-27:36)

SLIDE time 27:38-28:42
Acknowledgements in TCP: Receiver sends acknowledgement (ACK) to sender
Acknowledgement is used for flow control, error control and congestion control. In error control if the acknowledgement is not received then you send a retransmit. In congestion control we find that ACK is used for controlling this. ACK number sent is the next sequence number expected. 
Delayed ACK: TCP receiver normally delays transmissions of an ACK for about 200 ms because it allows the packets to arrive thinking that it can send less number of acknowledgements this way, and ACKs are not delayed when packets are received out of sequence i.e. a little out of ordinary, may be they came from two different paths, so you do not delay the ACK but send it immediately. (refer slide time 27:38-28:42)

SLIDE time 28:43-29:50
Now fast retransmit, if you remember that the TCP RENO uses fast retransmit. If three or more duplicate ACKs are received in a row, the TCP sender believes that a segment has been lost. This means ACKs have come meaning some earlier packets are gone. This means, possibly the later packets or segments may be lost. So what it does is, without waiting for the timeout to occur for this particular segment which has been sent it assumes that it has been lost and it sends one more again. TCP performs a retransmission of what seems to be the missing segment without waiting for a timeout to happen and then it enters slow start. That means it brings down the multiplicative decrease of ssthresh and sets the cwnd to one.
i.e. ssthresh = cwnd/2 cwnd = 1. This is fast retransmit.
 (refer slide time 28:43-29:50)

SLIDE time 29:51-30:18
In Fast Recovery the slow start is avoided after a fast retransmit. That means after a fast retransmit intuition Duplicate ACKs indicate that data is still getting through or at least the duplicate ACKs are through. After three duplicate ACKs set retransmit lost packet, i.e. decrease ssthresh to half so ssthresh = cwnd/2 but
cwnd= cwnd + 3 and then you enter congestion avoidance. So increment cwnd by one for each additional duplicate ack. This is a fast recovery but then after this you enter the congestion avoidance. That means, basically this is trying to tune the performance of TCP to get the maximum throughput without causing any congestion. When ACK arrives that acknowledges new data cwnd = ssthresh
  After that we enter the congestion avoidance. So this is fast recovery. 
(refer slide time 29:51-30:18)

SLIDE time 30:19-30:42 
TCP RENO: For Duplicate ACKs it does fast retransmit and fast recovery. Fast recovery avoids slow start. And if there is a time-out you retransmit and go to slow start. TCP RENO improves upon TCP Tahoe when a single packet is dropped in a round-trip time. But if multiple packets are dropped then of course the TCP RENO cannot handle that and for that we have a TCP NEW RENO.
(refer slide time 30:19-30:42)

SLIDE time 30:43-30:58
 When multiple packets are dropped RENO has problems.
 Partial ACK: Occurs when multiple packets are lost. A partial ACK acknowledges some but not all packets that are outstanding at the start of a fast recovery, takes sender out of fast recovery. The sender has to wait until time-out occurs. 
(refer slide time 30:43-30:58)

SLIDE time 30:59-31:42
In new RENO partial ACK does not take sender out of fast recovery. Partial ACK causes retransmission of the segment following the acknowledged segment. New RENO can deal with multiple lost segments without going to slow start. (refer slide time 30:59-31:42)

SLIDE time 31:43-32:07
There is a selective acknowledgement (SACK). Here you can selectively acknowledge. In an original TCP when you give an acknowledgement, that is the next segment you are expecting and all segments before that are acknowledged. Here you can give selective acknowledgement stating that you have got all these but not that particular one.  
Issue: Reno and new Reno retransmit at most one lost packet per round-trip time. Selective acknowledgement: The receiver can acknowledge non continuous blocks of data. That means SACK selective acknowledgement of 0 to 1023, 1024-2047 and so on 
(refer slide time 31:43-32:07);

SLIDE time 32:08-33:12


Multiple blocks can be sent in a single segment.
TCP SACK enters fast recovery upon three duplicate ACKs. Sender keeps track of SACKs and infers if segments are lost. Sender retransmits the next segment from the list of segment that is deemed to be lost like fast retransmit. (refer slide time 32:08-33:12)

SLIDE time 33:13-33:43
To improve the performance of TCP, there are two competing demands here.
 One is that we have to maximize the throughput and if you can maximize the throughput, naturally the overall delay, congestion etc will be small and at the same time you will get your job done faster. But in order to push this maximum throughput we should not get into congestion, a collapse so we try to guard against that. These are the versions or various flavors of TCP for doing that. So, we have looked at TCP. Now we are going to look at some other topic once again associated with congestion control and the other one is traffic engineering. That means, can you shape or can you handle your traffic in a particular way so that congestion is less likely to occur.
(refer slide time 33:13-33:43)

SLIDE time 33:44-34:00
All these build to give a quality of service in a network. And at routers these may depend on Packet Classification and Packet Scheduling. At network entrance it may depend on traffic conditioning. At routers or somewhere in the network you may do admission control. Between hosts and routers you may do signaling. So these are the different components of QoS of a network. 
(refer slide time 33:44-34:00)

SLIDE time 34:01-35:02
So, let us say you have a sender and receiver here, these are the intermediate routers then you can do the traffic conditioning at the edge of the network. You can also do admission control here or somewhere else. So, these are the different components.
(refer slide time 34:01-35:02)

SLIDE time 35:03-36:56
Traffic conditioning mechanisms at the network boundary need to enforce that traffic from a flow does not exceed specification.  
So, we will look later at what kind of specifications we are talking about, what kinds of things people may agree on, or negotiate about that what are the parameters. But suppose from some source we had negotiated certain parameters and we find that the source is not sticking to that parameters and it is going out of that then we have to do some policing. So, policing is a drop traffic that violates the specifications. The specification as was agreed between the service provider and the sender. Shaping means the buffer traffic that violates specifications. Marking means mark packets with a lower priority or as best effort, if the traffic specification is violated. (refer slide time 35:03-36:56)

SLIDE time 36:57-38:10
Let us look at Traffic shaping first. Regulating the average rate of data transmission allows control algorithms to work better. So this is to be understood. As I mentioned earlier, in computer network specifically data networks or internet traffic etc they are inherently very bursty in nature. When it comes it comes in one big bunch and then for long periods there may be no traffic. Now the trouble is, if the burst peak to the average ratio may be as much as 1:1000, we have to accordingly design your buffer and other network provisioning. So, we have to decide upon whether we are doing it for the peak or doing it for the average or may be doing something in between as designing for the peak. If you design it for the peak everything works fine but that becomes very expensive and not practical in many cases. You cannot do it for the average also and that may be to lower so it may be somewhere in between. Therefore one inherent problem is the burstiness of the traffic. Now, if you could somehow make the burstiness smooth, then all your system will work much better. One way of doing is to buffer it somewhere.The shape of the traffic is related to some statistics about data transfer rates as well as its sensitivity to error, delay jitter etc. 
(refer slide time 36:57-38:10)

				       
      SLIDE time 38:11-
One famous algorithm is the Leaky Bucket Algorithm. It is a single server queue with a constant service rate. If you have a bucket which is leaking drop by drop that means water will come out at a constant rate. Therefore the same thing happens here. If you have a single queue and then you service it at a constant rate this is the rate at which you are pumping the data into the network. So, if there is a burst then it will get absorbed in your buffer at the edge so that in the core of the network the burst will not come and it will be more of a steady kind of a flow. A steady average kind of flow is also something beyond the capacity of the intermediate nodes. Then of course the capacity of the intermediate nodes has to be increased. That is the leaky bucket algorithm in short. So the input buffer allows a bursty flow to be smoothed out to an even flow onto the network. It may be implemented in hardware or the OS Operating System. It may be implemented either in hardware or software. (refer slide time 38:11-)

SLIDE time 39:04-39:58
Underutilized slots are written off. By this what we mean is, the packets are being serviced by this network at a particular ray, let us say once every T unit of time. Now after another T unit of time it will try to service and finds that the buffer is empty so it will not send anything. Again after T unit of time and if something has arrived by that time it will send one packet. The algorithm can work on the volume of the traffic rather than number of packets. Only problem here is, a somewhat slow response time for inherently bursty traffic which is quiet often in the node.
(refer slide time 39:04-39:58)

SLIDE time 39:59-41:19
One way to handle a little bit of burstiness is by a token bucket. This again improves the throughput a little bit and it can accommodate burstiness to a certain degree. We cannot allow all kinds of burstiness because then the burstiness will flow into the core of the network where it will be more difficult to handle. So this is the token bucket, it limits the input to specified burst size (b) and average rate (r). 
So traffic sent over any time T<= r*T+b, also know as linear bounded arrival process (LBAP). So there is bound on an arrival process. Excess traffic may be queued, marked or simply dropped. (refer slide time 39:59-41:19)

      SLIDE time 41:20-41:44
So, Tokens are generated for the buffer at a fixed rate which can be accumulated. So this is the main point where is the token differs from the leaky bucket. In the leaky bucket the underutilized slots were written off. But here if your time comes you can get a token and you can collect and accumulate so many tokens. And then when a burst comes up to that many tokens can be sent. .The longer time average is helpless because there is a limit to the number of tokens you can really accumulate because after that you cannot accumulate tokens anymore. And at the same time a little bit of burstiness is allowed if your source is inherently bursty, and if you can allow some amount of burstiness that will improve the throughput. So, for each token only one packet can be sent but tokens can be accumulated up to a certain maximum.. A variant is to allow k bytes per token. Essentially it allows bursts up to a regulated maximum length that is maximum number of tokens. A leaky bucket may follow a token bucket also in order to make it absolutely smooth.
(refer slide time 41:20-41:44)

SLIDE 41:45-42:18
So this is the diagram, the bucket holds up to b tokens and there are so many tokens per second accumulating there. And when a packet burst comes then it waits for the tokens. If the tokens are not there then it cannot send. But if the tokens are there depending upon as many tokens that are available the tokens are removed and the packets are sent into the network. (refer slide time 41:45-42:18)

SLIDE time 42:19-43:08
Now having talked about this, let us just mention what are the kinds of traffic parameters that are important or that may be negotiated between the sender and the network service provider. One could be maximum packet size that defines how big the packet is. The token bucket rate: Defines what the average rate is. Token bucket size defines how burst it will be. Maximum transmission rate tells us the exact maximum transmission rate. 
(refer slide time 42:19-43:08)

SLIDE time 43:09-43:39
Loss sensitivity: Is this flow very sensitive to losses.. If you are just doing some file transfer it will be sensitive to losses, but if you are sending some voice it may not be that sensitive. Loss Interval: At what interval it is a loss, if the loss is very bursty or if the loss has to be averaged out, etc. Burst loss sensitivity (packets): in terms of the number of packets. Minimum delay noticed, and maximum delay variation which are allowed. These are again very important for multimedia traffic. Quality of the guarantee: Is it just a best effort or better than the best effort is what it tells about. These are the flow specifications of services. (refer slide time 43:09-43:39)

SLIDE time 43:40-44:06
We will come to admission control and signaling in more detail when we discuss RSVP in the next lecture when we disscuss QoS and Multimedia traffic. But just to mention it here, Admission Control is a function that decides if the network has enough resources. Admit new flow if enough resources are available. Reject the flow otherwise. 
(refer slide time 43:40-44:06)

SLIDE time 44:07-44:20
You do some reservation of capacity through some protocol like RSVP which we will discuss later. And if you find that you can reserve the capacity for this kind of flow that is the flow with these kinds of parameters then you admit it but otherwise you do not admit it. This assumes that we have some kind of a virtual circuit. (refer slide time 44:07-44:20)

SLIDE time 44:21-44:48
There may be Distributed Admission Control instead of central admission control at the beginning. For example, it may be end to end delay which must be less then than a delay bound D. So calculate d1, d2, etc and you reserve resources. (refer slide time 44:21-44:48)

SLIDE slide time 44:49-45:12
And what would you do is, the D is specified by the source and as it travels some reservation signal it calculates the delay d1, d2, d3 etc and
if D < d1+d2+d3 then you reject the flow and if it is greater then you accept it. Send reject message to sender and release resources.
 Therefore if D>d1+d2+d3 accept flow, commit resource reservation and notify sender. (refer slide time 44:49-45:12)

SLIDE time 45:13-45:39
Some signaling protocol is used to reserve and release resources and to do admission control. So you reserve one mbps that the request goes through. (refer slide time 45:13-45:39)

SLIDE time 45:40-47:01
So, this is a Congestion Control in virtual circuits. One approach is admission control, not allow new VC till congestion goes away or route new ones around problem areas. Other is, negotiate flow specification when new VCs are set up. This requires resource like buffer space, bandwidth etc, and reservation along the way. This may waste resources. (refer slide time 45:40-47:01) 

SLIDE time 47:02-47:59
One topic we mentioned earlier is the TCP IP source quench or sometimes called as choke packets. This may be used as a crude mechanism for handling congestion. Each router monitors each output line and calculates the utilization as a weighted sum of current and past utilization. Above a certain threshold a choke packet with the destination is sent to the source and the original packet is tagged and sent along. On receiving a source choke packet the source is supposed to reduce the traffic to that destination by some percentage. If that happens and if it works then that is very fine. When congestion is detected the source is sort of distributed and they are remote to each other. So, if you could send this feedback instantaneously then you could control the congestion much better but that is not possible. you have a distributed algorithm where you work  only with some local information and something that might come along with some particular packet .(refer  slide time 47:02-47:59)

SLIDE time 48:00-48:16
The source waits for some time before acting again on the next choke packet because there may be multiple choke packets coming. Therefore for the same burst it has created ripples of congestion along the way and all the routers sending choke packets so multiple choke packets does not necessarily mean these are independent but they may have come because of the same source so it waits for sometime before acting on the next choke packet. For high speed lines with a lot of hops, choke packets to the source is too slow. So, choke packets may operate hop by hop thus by distributing the pressure on buffers.  These choke packets add to the network traffic and it operates hop by hop thus distributing the pressure on buffers. 
(refer slide time 48:00-48:16)

SLIDE time 48:17-49:42
Scheduling: This is another way to handle congestion. (refer slide time 48:17-49:42)

SLIDE time 49:43-51:17
Packet scheduling has to be done by deciding when and what packet to send on output link, usually implemented at the output interface. Suppose you have some switch or some router or network node and a number of packets are coming out so what you might want to do is to classify these packets. In this context this could be the worst effort. There are premium services and other kind of services etc and the rest are the best effort for with the rest. What you may have is that you may have various classifications for these flows which may go into different queues and then there is a scheduler which schedules as to which queue to be serviced next. A scheduler has a vital role to play on the kind of services on each of the packets at the micro level and each of the flows in general at a higher level they get.
(refer slide time 49:43-51:17)

SLIDE time 51:18-52:33
Typical Internet Queuing: In Internet queuing what we do is, we use FIFO + drop tail. What is FIFO? FIFO is, First In First Out. It is a simplest choice and is used widely in internet. This First In First Out implies single class of traffic which is essentially means that we have a single queue. So whoever comes in first, he is the one who would be attended first for servicing. This FIFO has to do with scheduling. And there is a drop tail which means the arriving packets get dropped when queue is full regardless of which flow it belongs or regardless of its importance. So, if the buffer is full whoever comes next will be dropped. So FIFO has to do with the scheduling discipline and drop tail that is the drop policy has to do with the buffer management. This means how much of your buffer is kept empty or whether you allow the buffer to get full or whom you drop out when the buffer gets full etc are some of the buffer management policies. Now let us look at scheduling. Actually the scheduling policy and the buffer management policy always go hand with hand and the buffer management policy always come in pair.
(refer slide time 51:18-52:33)

SLIDE time 52:34-53:35
FIFO Issues: In a FIFO discipline, the service seen by a flow is convoluted with the arrivals of packets from all other flows. So there is no isolation between flows and no policing. Send more packets and get more services. We have one single queue so whoever is pumping in more packets into it he is more likely to be serviced. Of course, you will lose some packets also but other people will also lose some packets but in some sense it favors somebody who is pumping data at a higher rate. So he gets more service or may be he requires it or it is just some kind of a row node. You do not differentiate between different flows at all. There is no isolation between the flows. If there is a flow which is very important but sends less number of packets he will get much lesser service compared to the one pumping lots of packets. So that is the Issue with FIFO. (refer slide time 52:34-53:35)

SLIDE time 53:36-54:02
Drop-tail issues:
Routers are forced to have large queues to maintain high utilizations, that is a problem. Larger buffer implies larger steady state queues or delays so the delay is more. Synchronization: End hosts react to same events because packets tend to be lost in bursts. So what happens is, when the buffer gets full, different packets coming from different sources would be dropped. So, all the sources would know that the packets may be timed-out or something so they would act again in unition and this acting in unition is always bad, then you take another step which again is wrong in some way, this makes it become more bursty 
 Lock out: A side effect of burstiness and synchronization is that a few flows can monopolize the queue space. So these are the drop-tail issues
(refer slide time 53:36-54:02)

SLIDE time 54:03-54:11
Priority Queuing: classes have different priorities, and class may depend on explicit marking or other header info, for example IP source or destination, TCP port numbers, etc. Transmit a packet from the highest priority class with a non empty queue. This has preemptive and non preemptive versions. This is the kind of scheduling policies we have. (refer slide time 54:03-54:11)

SLIDE time 54:12-54:27
So, Routers must be able to 
classify arriving packets according to QoS requirements. This is known as packet classification and packets are transmitted in order to meet the QoS requirements which are known as packet scheduling. 
(refer slide time 54:12-54:27)

SLIDE time 54:28-54:41
You have Class A service which is very premium. Class B services and Class C services are also there. You might attach different priorities to different queues and serve them that way.
(refer slide time 54:28-54:41)

SLIDE time 54:42-55:22
So, each router must implement some queuing discipline. Queuing allocates bandwidth and buffer space, so bandwidth tells which packet to serve next (scheduling) and buffer space tells which packet to drop next (buff management). Queuing also affects latency. (refer slide time 54:42-55:22)


SLIDE time 55:23-57:33
 One thing which is very widely used is Weighted Fair Queuing. Router maintains multiple queues for each output line one for each source. The queues are serviced in a round robin fashion. Instead of packets the volume can also be examined and packets sent in order of their finishing. Some sources can be given a greater weight than others. The point is, even if you do not give a greater weight, then the service which is premium, may be much less number of people might be there. So, since the round robin is between the queues, automatically those which have the premium class get a better service since the population is low.. 
(refer slide time 55:23-57:33)

SLIDE time 57:43-57:51
Use a few bits in header to indicate which queue (class) a packet goes into (also branded as CoS. Lower delay and low likelihood of packet drop for high end users.  Priority, round robin, classification, aggregation etc are the different mechanism which we use. With this we come to a sort of the end of short handling of this congestion control issue, it is not a very easy issue because as we know this is a global problem but you have to take some local action so that it works fine. 
In the next lecture we will take up Quality of Service, Quality of service as we have already mentioned today, we have different kinds of quality requirements for different sets of people. As I mentioned that if you are transferring a file, you do not want any bit to be lost because it may be a very vital bit so it may be a binary or a source or something that the whole thing may become junk. If you lose one bit, it is very difficult and if it goes in a jerky fashion or takes longer time then you may not mind. So that is one kind of quality you require. Another kind of quality you might require is, when you are doing some kind of multimedia transmission like audio, video, etc where I may sort of be insensitive to a few packets or a few bits being lost here and there but if the delay is too large or keeps on varying too much then I have a problem with the quality of reception. So that is a different kind of quality. So how to handle different kinds of quality and how multimedia transmission etc can take place in a network etc would be the content of our next lecture.
 Thank you. 
refer start 57:34
Good day, so our topic for today is QoS and Multimedia, that is (refer slide time 57:43-57:51)

SLIDE time 57:52-58:43
Quality of Service and Multimedia: We will just look at these one by one. 
(refer slide time 57:52-58:43)
 
Quality of Service:
What is Quality of service?
 QoS refers to traffic control mechanisms that seek to either differentiate performance based on application or network operator requirements, or provide predictable or guaranteed performance to applications, sessions, or traffic aggregates. It talks about a lot of things. The basic notion is that there are some applications which require one kind of quality of service. The quality of service may mean different things but the most important of them are the network delay and packet loss, so it is delay and various ways of delays. 
