Design and Analysis of Algorithms 
Prof. Abhiram Ranade
Computer Science Engineering Department
Indian Institute of Technology, Bombay

Lecture - 1
Overview of the course

Welcome to the course on design and analysis of the algorithms. Today?s lecture is going to be an overview of the course and I want to introduce you to the main problems in the course and also give also convey a spirit of the course. Let us start with the fundamental question. Given a certain problem how do you solve it on a computer? For many problems it is relatively easy to design algorithms that will somehow solve them. However, quite some cleverness is needed and designing algorithms that are also fast that is algorithms which give answers very quickly. This will be the major challenge in the course.
(Refer Slide Time: 01:34)
 
Our main course goal as said is to design algorithms which are fast. Design of fast algorithms. As you may realize designing anything be it computers, be it cars, be it clothes is an art. So, in some sense you have to be creative and it cannot be taught, but in some other sense there are also some very well defined design techniques which have evolved for this purpose. And the goal of this course is to study these techniques we will also have lots of exercises in which you can apply these ideas. And our hope is at the end of the course you will be able to solve algorithm design problems that may that you may encounter later on in your life. There are some prerequisites for this course and let me just state them.
(Refer Slide Time: 02:36)
 
The prerequisites are that you should be familiar with some programming. You should have done some amount of programming in some common language say c or scheme or basic. You should also have done a course on data structures and you should have some discrete mathematics background. As you may see this is not an elementary course and this background is going to be necessary for us.
(Refer Slide Time: 03:27)
 
Our approach is this course is going to be analytical. In the sense that will build the model of a computer analytical will build a model of a computer that is mathematical build a mathematical model. And on this mathematical model we will be designing algorithms. So, we design algorithms and we study their properties of algorithms on this model. The whole point is to reason about algorithms. So, this is very important if we want to mathematically prove properties and we want to prove properties the emphasis on this course is proof. Prove properties about speed say prove facts about time taken. All this so there is going to be a number of things that we have to do, and of course there is a whole course for this.
(Refer Slide Time: 05:04)
 
So, I want to give an overview of the course. In the next few lectures we will develop the basic framework what I mean by this is that we will define the mathematical model. We will say what fast means when I say fast algorithm what do I mean? This is what we will say. Then we will embark on a fairly long stretch which involves techniques for designing fast algorithms while doing this we will also be surveying many problems. So, we will look at problems from optimization graph theory some problems from geometry also some others. It will turn out that there are some problems which do not really quite respond to our algorithm design techniques. Of course, we will see many problems for which we can design really god algorithms our techniques work beautifully, but there are also some problems where our techniques do not work so well. And for these problems a fairly intricate theory has developed over the last over the last several year?s 10 20 30 years and this theory is the, so called theory of N P completeness. So, we will be studying this theory as well.
(Refer Slide Time: 06:59)
 
Let me now, come to the topic for today. The main topic for today. So, today I do not want to be too formal, but I nevertheless want to convey the spirit of the course to you. So, here is what we are going to do? We are going to take a fairly simple problem which everybody will have no should have no trouble in understanding and let me state that problem right away. The problem is given 2 numbers m and n find their greatest common divisor. So, everybody understands this problem. You are going to see 2 algorithms for this. One is a very simple algorithm which you probably have learnt in school. It is probably the algorithm that you were taught in first standard or fourth standard or something like that and which probably most of us used to solve this to find the greatest common divisor where we need to such as when simplifying fractions.
Then we will study another algorithm and this is one of the earliest algorithms which was ever invented. And this was invented by the mathematician Euclid who you might know from plane geometry. Yes Euclid did invent this algorithm even though there were no computers in his time. So, we will study Euclid?s algorithm and from this we will get a sense of what a fast algorithm is you will see that Euclid?s algorithm. Even though we have not defined what fast is you will intuitively understand that Euclid?s algorithm must be much faster will be certainly much faster than the simple school level algorithm that we are talking about. And Euclid?s algorithm is also clever and that makes it more exciting and that is also again the spirit of this course.
(Refer Slide Time: 09:31)
 
So, let me begin with the simple school level algorithm for factoring. So, everybody knows this, but let me state it anyway. So, basically there are 2 or 3 steps . Let me write down the input it is 2 integers m and n. What we need is the greatest common divisor which is also the largest integer that divides both and when I say divides I mean without leaving a reminder. So, here is what the algorithm will look like. So, step 1 we are going to factorize m. What does it mean? It means finding primes that we call them m 1 m 2 till m k such that m is m 1 times m 2 times all of this. The next step is to factorize n. What does that mean? Again find break n into its factors. So, write n as n 1 time n 2 times till some n j. Note that the same factor may appear several times and of course, we are here to write it separately. The next step is to identify common factors and then multiply and return the result. We are going to take an example of this and in a minute and we will see we will see what exactly these steps do.
(Refer Slide Time: 12:19)
 
Let me now, state Euclid?s algorithm. So, I am going to write this as a procedure. So, Euclid is a procedure which will take 2 arguments m and n. And I am going to invent a pseudo language as I go along and that is again going to be in the spirit of the course we are not going to be too picky about how we write down algorithms? So, long as what I mean is clear to you perfect everything is fine. So, you could express your algorithms in the most in the most suitable nicest syntax. So, that it is easy for you to get your meaning across. We will come to all this in somewhat more detail in the next lecture or, so what does Euclid do? So, the first is a check. So, we are going to check whether m divides n while m does not divide n we are going to do the following.
So, first we are going to calculate the remainder. So, we are going to calculate r of n mod m. Then we are going to set n equals m and then we are going to set m to be r. Al these steps are going to be done inside the loop. So, that is the end of the loop over here we will prove soon that eventually this loop will have to terminate. And after it terminates all that Euclid?s algorithm does is to return the value that m has at that point and that is it that is all there is to the algorithm. It is not clear when you first look at this algorithm that in fact, this algorithm works. It seems to be doing some divisions and taking some remainders, but it is not actually factorizing any of the numbers that you want who is greatest common divisors you want. So, let us now, take some examples and see whether or not this algorithm works.
(Refer Slide Time: 14:19)
 
So, let us first take let us first take a small example where we have m equals say 36 n equals 48. How will you do it using our simple algorithm. Well we factor m is equal to a product of prime factors. So, the idea is going to be that we are going to test numbers one after another. So, we start with say 2 and yes 2 is a factor. So, we have to write it as 2.So, then we divides 36 by 2 we get 18. So. In fact, now, you have to factorize 18 well 2 is again a factor. So, then what remains is 9. Now, 2 is not a factor. So, you go out to the next prime number which is 3 which is a factor and then only the factor 3 remains. So, we have factorized m what about n? Again we do the same thing we start with 2 yes 2 is a factor. So, that gives us 24. Again 2 is a factor that gives us 12. Again 2 is a factor. So, that leaves us 6 again 2 is a factor and then that leaves us 3. So, then we want to identify what factors are common well these 2 are common and then 1 3 is also common. So, common the common factors are 2 2 and 3 and, so G C D is equal to 2 into 2 into 3 or 12 nothing terribly difficult in this. This is all of course, school level stuff. So, let us contrast this with what Euclid?s algorithm does. So, let me bring Euclid?s algorithm back. So, we are going to start with m equals 36, n equals 48.
(Refer Slide Time: 17:11)
 
So, let me write it down as we go along. So, doe?s m we have to check whether m does not divide n. So, 36 divide n no, so when, so therefore we enter the loop. So, now we calculate r equals n mod m. So, in other words r is equal to 48 mod 36 that is equal to 12 then we calculate n is equal to m. Let me write down over here that well we know that m is equal to 36 and n is 48. So, n takes the value that m originally had. So, n is now, going to become 36 and m takes the value which r has, so r is going to become 12. So, at this point the iteration has ended. So, at this point our iteration has ended and we have left with m equals 36 and n equals 36 and m equals 12. But now, we have to go and check out the loop again, because well that is what the while loop says. So, this time n equals 36 and m equals 12 and again we had to check whether m does or does not divide 12 m r does not divide n.
(Refer Slide Time: 18:43)
 
So, does 12 divides 36. This time the answer is yes and therefore, we are going to exit the loop. And in fact, at the end of it we are going to return the current value of m which is equal to 12, so we will be returning 12. So, let us now, compare these 2 algorithms. So, here you can see roughly what the work done by the 2 algorithms is. The simple algorithm had to do factorizations then it had to collect common factors and it had to do it had to multiply the factors together and return the answer. Euclid?s algorithm on the other hand did 1 division. So, first time it divided 36. It divided it checked if 36 divides 48 which is the value of n that we had. And when it found that answer was false the division was not possible then it took the remainder. Then it simply exchanged the numbers basically and then again it did 1 more division.
So, I can summarize the work over here by saying that this did roughly 2 divisions. How much work does this do? Well it calculated these, so many factors. So, it calculated 4 factors, so it had to at least do 3 divisions to get each factor. Here it calculated 5 factors. So, it had to at least do 4 divisions. So, it at least did 9 divisions probably it did more. You can see in a quick example immediately where it will have to do many more than these. As you can see in this school level very simple factoring algorithm we use 9 divisions whereas, in this somewhat sophisticated algorithm we use really 2 divisions. So, clearly you have done less work. And although i have not proved yet that Euclid?s algorithm in fact, works you can see that it is returning the correct answer. So, but we will show later on that. In fact, Euclid?s algorithm does do does work correctly.
(Refer Slide Time: 21:17)
 
So, let me take one more example m equals 434, n equals 966. So, these are somewhat large numbers and let us see what will happen over here. So, suppose you want to factorize these numbers first of all it will take some work, but let us see what the answer is going to be. So, m can be written as 2 times 7 times 31, n can be factorized as 2 times 7 times 139 what is the answer? The common factors are 2 and 7 and the G C D therefore, is 14 . Let us now, come back to Euclid?s algorithm.
(Refer Slide Time: 22:31)
 
So, let me write down again m is equal to 434 and n is equal to 966. The first step is to compute the reminder of n mod m. So, then r is going to be according to this step r is going to be 966 mod 434. Well 966 is equal to 2 times 434 which is 868 plus 98 under 4 I can write down r as equal to 98. After this we just had to exchange values basically. So, now, n is going to take the old value of m. So, n is going to be 434 and m is going to take the value of r, so which is 98. So, at the end of one iteration of Euclid?s algorithm we have new values of m and n and these are the new values. So, we started off with the values 434 and 966. After 1 iteration we have the values 434 98 and 434 in that order m and n. So, now, we just have to repeat the same thing with these new values, so this is the end of the first iteration. And in the second iteration we are again going to calculate r equals n mod m.
(Refer Slide Time: 24:26)
  
So, let us do that. So, if you calculate r equals n mod m we are going to get the new value of r to be equal to 434 mod 98. So, you have to do the division over here and which is 434 can be written as 98 times 4, so this is 392, so plus we are going to get 42. So, r is going to be equal to 42, but of course, when we began this iteration we had to check whether m does or does not divide n or whether 98 does or does not divide 434, but clearly it is leaving the remainder and therefore, we will enter this iteration. After that we are going to set n equal to m, so m the value of n is 98 and m will be equal to r which is 42. So, at the end of the second iteration these are the values that we have. So, n is equal to 98 and m is equal to 42. So, we will have to go into the third iteration for the beginning of the third iteration, let me just write down the values.
(Refer Slide Time: 25:56)
 
So, we have m equals 42 n equals 98 again we are going to check does m divide n. It does not and, so we will enter the loop when we enter the loop we have to find out r equals n mod m. So, that is equal to 98 mod 42. 42 times 2 is 84, so 98 mod 42 is 14. Then there is just a matter of setting n and m correctly. So, n will now, equal the old value of m. So, this is 42 and m will equal the value of r we just calculated. So, it is going to be 14. So, these are the values at the end of the third iteration. So, now, again we are going to come back and execute this loop and again we are going to check does m divide n or not. And this time we will see that 14, in fact does divide 42 and at this point the loop will be exited.
(Refer Slide Time: 27:20)
 
And at the end we are going to return the result as m. The value of m right now is 14 and therefore, 14 will be returned. So, what has happened? We took 3 iterations. So, summary of this all this is we took 3 iterations what happened in each iteration well we did 1 division per iteration. And before quitting from the loop we have to do one more iteration one more division and therefore, that took 4 divisions essentially. So, Euclid?s algorithm for this somewhat complicated problem m equals 434 n equals 966 where we are asking to find the greatest common divisor of m equals 434 and n equals 966 took 4 divisions. Let us now, check what happens with the simple algorithm.
(Refer Slide Time: 28:36)
 
So, here where the factors that we found. So, how many divisions did it take? At first glance you might see you might think that it is only going to take 2 divisions to calculate m and 2 more divisions to calculate n, so which is 4 divisions, but that is not quite correct. To check that there is no factor between 2 and 7 we would require checking for 2 3 for 3 five as well. Now, after dividing by 7 we had to check whether 139 is a prime. So, that would also involve checking all the numbers until 39. So, that would require substantially many divisions. So, here again we require many divisions many more than Euclid?s algorithm. In fact, you will see that if we take bigger and bigger numbers factoring them becomes much harder. You will have to do a lot more work whereas, in Euclid?s case we will just do divisions and we are we will. In fact, show now that as we do the divisions then numbers will become smaller and smaller and the algorithm will terminate. So, the next topic that we are going to get into is I want to argue that Euclid?s algorithm actually works.
(Refer Slide Time: 30:10)
 
I am not going to give a fairly very detail proof I just want to indicate the main idea. The main idea is really a fact about divisibility and remainder and things like that, so let me indicate that. So, this idea says that if m divides n then G C D of m and n must; obviously, be equal to m, because clearly m will be the largest number which divides both m and n. If not then we can write G C D of m and n is the same as G C D of n it is the same as the G C D of n mod m and m how do you prove this? I will leave it as an exercise, but let me just mention the main idea. The main idea is suppose g is the G C D then we can write m as a times g and n as b times g where a and b are relatively prime having written them in this manner you should be able to just substitute into what we have in the fact and then you should be able to get the answer. Actually the fact itself is very similar to what we had in Euclid?s algorithm in the fact that we have written down. So, let me just show you the iteration the loop part of Euclid?s algorithm. So, the fact says that if you want to calculate the G C D of m and n then you might as well calculate d G C D of n mod m and m what does our loop do?
It wants to calculate we want to calculate d G C D of m and n. It first checks what the remainder is and then it calculates the fact is essentially n mod m. So, it is essentially this term over here and we need and then it sets m to be equal to r. So, the first argument is set to r and the second argument is set to the old value of m, but this is precisely what the fact says. The fact says that if you want to calculate G C D of m and n instead calculate G C D of n mod m and m. In fact, once we have once we are given this fact the proof of Euclid?s algorithm the correctness of Euclid?s algorithm is at least partially done. Because what we have accomplished is that we are sure that as we go through iterations we will never be we will always maintain integer?s m and n and who?s G C D will be the G C D of the original values of m and n. So, this is the invariant that we are going to maintain. So, m the specific value of m and n might change, but their G C D is never going to change. So, let me write this down.
(Refer Slide Time: 34:45)
 
So, as the loop executes m and n might change, but their G C D does not. And hence what we have is that we will preserve the G C D and eventually when we exit the loop will exit with the same G C D. We just established that as the loop executes m and n might change, but curiously enough their G C D continues to remain the same. As a result if we ever get out of the loop then that will be, because m divides n, but notice that even at this point the G C D will still be the same as the old G C D the G C D of the old m and n. 
But if m divides n then the G C D will simply be n and that is what you will return and therefore, in fact, we have established that if the loop in fact, terminates then we will be returning the correct value. Now, you want to argue why the loop should terminate? Why we need to exit? Why we will always exit from the loop? This is actually fairly straight forward. Let us examine Euclid?s algorithm again and see what happens in each iteration. So, long as the loop is executing. So, initially the values of m and n might be taking some values. What are the values of m and n after one iteration? Well, the first step is calculating r, but notice that immediately afterwards we are setting n to have the value m.
(Refer Slide Time: 36:42)
 
So, if the original values are m and n then at the end of 1 iteration n will have the value m and what will the value of m be m will have the value r. So, it will be n mod m. So, what has happened is that the values m and n have now, changed to n mod m and m, but n mod m anything mod m is actually going to be smaller than m itself. So, notice that the first argument is always continuously is going to decrease in every iteration of the loop. The value of m is always going to decrease. How long can it keep on decreasing? Well, it has to remain positive. It cannot even become zero and it might decrease. It will have to decrease at least by one in each iteration. As a result of this we can conclude that the loop has to exit at some point and we know that if the loop exits then the correct value is returned. So, that proves that Euclid?s algorithm is correct. Our next step is to argue that Euclid?s algorithm in fact, runs reasonably quick. So, the basic idea we want to prove is going to be something like this. Well before that we are going to assume we are going to we are going to prove something about cost Euclid?s algorithm and in doing this we are going to assume that the first argument that we sent to Euclid is always smaller than the second.
(Refer Slide Time: 38:21)
 
So, this means that we will always call G C D 36 48 and not G C D of 48 36. This is only for the purpose of analysis. Actually the algorithm as we wrote will work fine if make this call as well. If we make this call you will see that internally we will start with m equals 48 n equals 36, but in one iteration we will be exchanging these values. The first iteration will only be spent for exchanging these values. So, m will become 36 and n will become 48. So, then we can as good as assume that in fact, we will start off our execution in this manner. So, we will assume in fact, that we are not analyzing this because that only adds 1 iteration. So, our assumption is that we are analyzing calls of this kind where m is less than n well of course, if m is equal to n then there is no question. So, the loop wills the Euclid will immediately return . So, let me now state the main result that we want to prove. The main result that we want to prove is something like this.
(Refer Slide Time: 39:44)
 
So, if Euclid is called with values m with values p and q that is Euclid of p q is called I want to make a distinction between the variables m n and their values. So, we will think of p q as values and m n will be variables. So, you are going to call Euclid with values p and q then if p is less than q then in each iteration the sum of the values of variables m and n will decrease at least by a factor 3 halves. So, initially if the sum is 6 then in one iteration the sum must go below 4 become 4 or go below 4. If it is 16 in 1 iteration it must go below 40 or stay or become 40. What is the implication of this theorem? This theorem establishes that the sum will drop very fast. And in fact, the number of iterations this establishes that the number of iterations is equal to log to the base 3 halves of p plus q. Well, it is less than equal to, so this theorem will put a good upper bound on the number of iterations that Euclid is going to execute. And therefore, all that we need to do is to prove this theorem and we have a bound on how many iterations Euclid takes. So, our goal now, is to estimate what happens to the sum of the values taken by m and n in each iteration. So, let us say that we start off the whole process and at the beginning of the iteration m and n take values p and q.
(Refer Slide Time: 42:27)
 
So, at the beginning of iteration m is equal to p and n is equal to q. After the iteration and I mean just one iteration let us say m takes the value p prime n takes that value q prime. So, what is our goal we want to estimate by how much the whole thing drops. So, you want to estimate p plus q upon p rime plus q prime. So, if we prove that this ratio is at most 3 halves then we are done. So, all that remains now, is to express p prime and q prime in terms of p and q. So, for that will need our Euclid procedure again. 
So, what does the Euclid procedure do if it does not terminate and that is the case that we are looking at currently? Then it computes r equals n mod m and then set n equals m. So, in terms of this the new value of n is the old value of m. So, going back to this the new value of n must be the old value of m. So, therefore we get q prime must be equal to p. The new value of m is the value of r which is the old value of m mod old value of n mod old value of m. The old value of n is simply q mod m. The old value of m is p and therefore, this whole thing p prime is q mod p we need one more fact in addition to all this which concerns p prime plus q prime.
(Refer Slide Time: 45:26)
 
Well, what is p prime plus q prime? P prime is the remainder when q is divided by p and q prime is p itself. So, this is remainder plus divisor and we know that the divisor is strictly less than the dividend. Divisor in our case is p and the divided is q and we assume that p is in fact, less than q. And therefore, we can conclude that this whole thing has to be less than or equal to the dividend. In other words this is q, so here are the 3 facts that we wanted p prime plus q prime is less than q. Let me align it q prime is equal to p and p prime is less than q mod p. If p prime is less than q mod p then I can conclude that p prime has to be less than both. In fact, it has to be less than p. So, now, it is just a matter of algebra we are going to add this and this and this last inequality, but just to get the terms right we will multiply this last inequality by 2 times. So, if we do that let me adjust this.
(Refer Slide Time: 47:50)
 
So, if we do that addition we are going to get p prime plus q prime plus p prime plus q prime this comes from over here. The whole thing is less than this p plus this p plus this q, but let us takes this 2 times therefore, we will get 2 q. So, now it is just a matter of simplifying this. So, what is this equal to this is nothing but 3 times p prime plus q prime and we have shown that it is less than 2 times p plus q and this is exactly what we wanted. So, we have been we have showed that the original value of the sum has reduced by a factor of two-thirds. So, p prime plus q prime is less than p plus q upon 3 halves that concludes the analysis of the algorithm. And we have been able to show that in fact, the algorithm will execute a fairly small number of steps.
(Refer Slide Time: 49:19)
 
 So, let me now conclude this lecture and highlight the main points. The very first point that I want to make is that is the difference between the 2 algorithms. The school level algorithm basically uses the definition. Euclid?s algorithm uses some more interesting deeper properties deeper mathematical properties of the quantities that we are going to calculate. So, we study properties of whatever we computing and this helps in designing fast algorithms. There is also one more point that I want to make which is that the basic way in which we did this analysis counting iterations will be useful in the rest of the course and the precise details of all of this we will cover in the subsequent lectures. So, that marks the end of this lecture.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 2
Framework for Algorithms Analysis

Welcome to the second lecture of the course on Design and Analysis of Algorithms. In today?s lecture, we are going to develop a Framework for Algorithm Analysis. In this course, we will be designing many algorithms for solving many different kinds of problems. We will want to compare these algorithms and even just plain evaluate them. And for this, we will need some sound mathematical bases.
And that is, what we are going to do. We are going to design a formal framework using which we can evaluate algorithms. And we will and also compare them. This is going to the topic of this lecture and also the next few lectures. The framework that we designed could be used not only for comparing the execution time of algorithms, which is what we primarily mean; when we say algorithm analysis. But, it could also be used for comparing other resources that an algorithm might use.
For example, an algorithm might use varying amounts of memory. So, we could use essentially the same framework that we are going to discuss very soon. And use that framework to formally compare the memory requirements of different programs or different algorithms. The basic idea in designing in the framework that. We are going to discuss today, is actually very related to the kind of analysis that we did in the first lecture.
Except we are going to make it a little bit more formal. Essentially we are going to make a mathematical model of a computer. And then, we are going to take our algorithm and mentally execute that algorithm on that model. And then through this execution and by mentally executing, we will be able to tell how much time. The algorithm takes and that is essentially going to be involved in doing the analysis. That is what the analysis is going to have.
(Refer Slide Time: 02:58)
 
So, let me write that down the basic idea. We will make a mathematical model for computer. And we are not going to execute our algorithm on any specific real computer. But, will execute it mentally will imagine it is execution on this mathematical model. So, let me write that down as well, mentally execute algorithm on computer model and evaluate the time. This is the basic scheme; this is the basic idea that we are going to develop.
In order to develop it, we need to answer several questions. So, the first question naturally is what is this mathematical model going to be? Which essentially is the same thing as saying how long should be assign, what time should, we assign for each of the operations. That is comprised that is used in our algorithm. So, we need to answer questions like. What is the time required on the model for every operation that an algorithm might perform.
Then, we also said that we need to execute. We need to mentally execute the algorithm on this model. However, every algorithm or most interesting algorithms will require data. Some input that needs to be given to this algorithm. So, an important question that we need to answer is, what data should we be giving, what should be the input data. This is an extremely important question, because the time of execution in general will depend upon the input.
So, when we say we want to estimate the time taken by an algorithm. We have to be very clear in saying what input is being given to that algorithm. We may make mathematical models. And we may develop them and we may estimate the time taken on those models. Of course, there is the important question, which we need to answer, which is how does all this relate to real computers?
(Refer Slide Time: 06:17)
 
So, how does our model relate to real computers? If our model is terribly different, then our conclusions for the model might not be too useful for real computers. And of course, we do not really care that much about the mathematical model. We want our conclusions to eventually apply to real computers. And therefore, this is an extremely important question that we need to consider.
Over the next few lectures, we are going to answer these questions and also many of the other relevant questions. And you will see that all these questions can be answered nicely. And in that, we will be we will finish our development of our framework.
(Refer Slide Time: 07:32)
 
Here is roughly what I am going to talk about in the next few lectures. I am going to start by discussing some fairly basic terms. So, we will try to formally define or at least semi formally some basic terms. Then, we will present our mathematical model. After that I will discuss, the general overall analysis strategy which we are going to use.
These will for example, answer questions like what should be the input. We will also be taking a number of examples of algorithms and their analysis. And finally, we will conclude with a discussion of the limitations of the model. This will essentially be an answer to the question of how well do our conclusions to the conclusions that we draw for the mathematical model relate to real computers. And of course, I do not strictly I would not strictly discuss these points in the order I have written them.
I will discuss examples and I will discuss limitations and may be alternate a little bit. But, this is basically going to be the gist of this lecture and the next. Let us, begin now with some basic terms that we are going to use. In day to day life, we often use the same term to mean different things. In scientific discussion, it is important to fix the meanings for every term. So that we do not confuse ourselves later on and we do not end up with fallacies of any kind.
(Refer Slide Time: 10:06)
 
So, let us start by discussing the very first very common term that we are going to use which is problem. Before, I give a definition of a problem I would like to give some examples. And from those examples, I will try to motivate this definition. When I say problem in this course I will mean, what we usually mean is, in the sense of the problem of computing the GCD of two numbers.
Or say something like the problem of finding the shortest path on a map or maybe say finding the meaning of a word in a dictionary or may be even something like given an X-ray determine if there is any disease. You may notice that when we are talking about a problem. There is typically certain input which is which needs to be supplied and a certain output that needs to be generated.
Let us take an example of this. So for example, if you are asking about the GCD of two numbers. We could say that the input consists of say numbers like say may be 36 and 48 the GCD of which will obviously, be the number 12. Say for the problem of finding the shortest path in a map may be the input could look like say name of a city may be Mumbai and say a city say Aurangabad.
And we would have to supply which map we are going to use. So maybe, we use the western India Automobile Association map and that will also have to be supplied. That will also that map will also have to be supplied as a part of the problem definition. For finding a word in a dictionary may be we have to supply the word.
Say for example, we take the word evolution. And we will also have to name what dictionary we use, say may be the oxford dictionary or something like that. For the last problem, we will have to supply an actual X ray. Say some actual picture and in this case the output would be something like either there is disease. Say we just a yes or no.
For the shortest path, the output would be say the actual map, the actual path on the map. For the evolution for finding the meaning of the word evolution the output would have to be the actual meaning that you would get while after looking at the dictionary. At this point, we have I think we have a good sense of what a problem is and we can write down a reasonably simple definition.
(Refer Slide Time: 14:19)
 
So, let us do that. So, when we say problem in this course. We will mean a specification of what are valid inputs and what constitute acceptable outputs? Acceptable outputs for each valid input. So, we looked at this earlier. So for example, for the GCD problem 36 and 48 constitute valid inputs. And for these the acceptable input is 12.
Finding the shortest path names of two cities in the map constitutes a valid input and acceptable output would be the description of the path and so on. Of course input which is valid for one problem need not be valid for another problem and typically is not. So, numbers will not make sense as input for say the dictionary problem or and words will not make sense as inputs for the GCD problem; obviously.
We often use the phrase input instance and this is nothing but a valid input value for a given problem. So, I will say that a value x is a input instance for problem p if x is a valid input as per the specification. So, 36 and 48 are 36 and 48 together constitute an instance for the GCD problem. Mumbai Aurangabad and map constitute an instance for the GCD problem for the shortest path problem and so on.
(Refer Slide Time: 17:17)
 
Another important term that we need is that of a size of an instance. We will often not necessarily use the term input instance, but we will just stick with instance. Instance will always mean input instance or we could even say problem instance. So, when we say the size of an input instance, we mean in a formal sense we will mean a following. Will mean the number of bits needed to represent the input, the input instance. Let me just clarify that. So, a specific input instance will have a certain specific size.
So, again let us go back to our examples. (Refer Slide Time: 10:06) So for example, if you look at 36 48 which constitutes the input instance for the GCD problem, then we will have to ask the question, how many bits are needed to represent 36x and 48? So, here there is a question of how we represent numbers in the first place.
So suppose, we say numbers are going to be represented in binary. Then, 36 will require 6 bits. And so will 48. So in this case, the input instance will have length 6 plus 6 or 12. As far as, the shortest path in the map is concerned, somehow or the other we will have to represent the map. There are various ways of this representation we will see some of them later on in the course.
In general a map can be thought of as a graph which you have probably seen in the prerequisites for this course. And a map could be represented as a matrix. And a matrix could be represented as an array bits if you like and in that way we can represent maps as well. This definition that, we have given. This formal definition that we have given is often a little bit inconvenient for directives.
So often, we settle for a somewhat more informal definition. But in fact, this typically is something that is more useful. And informally we might say the size of an instance and we might mean any parameter which roughly grows with the official definition of the size of the instance.
(Refer Slide Time: 17:17) So, let me write that down. Any parameter which grows roughly the growth may not be exactly predicable with the formal notion of size. So, let us go back to our GCD problem. There we said that the size was the size for 36 48 was 12 bits. But, instead of making this is the definition of size. We could say that the size is simply the sum of the numbers. So, 36 plus 48, which is 84.
So, we could think of 84 itself as our notion of size of the input instance rather than 12 bits. In fact, if you go back to the first lecture you will see that this was the parameter that we used when we analyzed the GCD algorithm in the first lecture. So, we said that the size the sum of the numbers u and v will keep on decreasing.
And in fact, this is really the reason why we are interested in the notion of size. Usually we will expect that the time taken by an algorithm will increase with the size of the instance. And therefore, if you are going to evaluate an algorithm it is only natural and it is only fair in some sense that we also mention what the size of the input instance is.
So, if an algorithm takes a long time on a large instance on an instance of large size. Then, that is but if it takes large time on an instance of a small size. Then, we should potentially say that that algorithm is not a good algorithm or at least it is not a fast algorithm.
So, let us go back to the other problems and maybe we can think of what the notion of size is going to be over there. So, going back to our shortest path problem, a notion of size could be the size of the map, so the number of roads in the map or the number of roads and the number of cities together. So, clearly finding a shortest path in a map which only involves one road is going to be really easy.
And therefore, we should expect we should an algorithm which takes us takes us short time on such a small instance should not really be thought of as a great algorithm. On the other hand if an algorithm takes a small amount of time on a map which consists of 1000 cities and 2000 roads. Then, that algorithm we should certainly say is a god algorithm. So, essentially that is the idea, we want to when we evaluate algorithms. We want to evaluate that in comparison evaluate the time taken in comparison to the size of the input instance.
For the dictionary problem the size of the dictionary the number of words in the dictionary that is would be a good indication of the size. And for the x ray problem, we will somehow have to take that x ray and convert it into bits of some kind. So, we could say for example, that the size of the x ray say the number of the if the x ray is has a resolution 1000 by 1000. Then, they could say that the size is a million or something like that.
We often use the phrase problem size also to denote the size of the instance. So, if you say if you hear the phrase problem size it is really talking about the instance of the problem rather than the problem directly. But, that is a term which is very commonly used on the literature.
(Refer Slide Time: 24:37)
 .
They next important term that we need to discuss is algorithm. When I say algorithm, I mean an abstract computational procedure which takes some value or values as input and produces a value or values as output. I use the term abstract in order to denote that an algorithm can be expressed in many ways. So, a program is an expression of an algorithm.
So, the same algorithm might give rise to different programs say in different languages. A program has been concrete and algorithms has been abstract. Of course even for eve for discussing algorithms, we will need to have an ocean of a language. So; however, this notion is not going to be as rigid or as strict as the notion that we have when we discuss programs. When we discuss programs, we have a very well defined very, very strict language which has very, very strict rules for syntax.
We will not be worrying about all of that when we discuss algorithms. We would like to think of algorithms as the idea behind the program. And so long as we are able to convey that idea in as in very clear terms you will happy. So, the basic our goal in this course is going to be description of algorithms. So that human beings can understand, what is being said and we will not worry so much about the precise syntax that is used.
Initially, we will describe algorithms at a fairly great level of detail. As the course progresses, we will abbreviate our descriptions. And it will become clear to somebody who has gone through the course exactly what is being met. The reason for describing algorithms is of course, one reason is to convey what is the idea. And the other reason why we will be discussing algorithms in this course is of course to evaluate their time.
So, I tell you what an algorithm is. It should be clear to you, what exactly are the operations that I have in mind. And you should be able to write the program, but not only that. It should also be clear to you how that program will execute on a machine. And especially, on the model machine that we are going to talk about and that is going to be another important purpose another important point that we want to keep in mind, when we discuss algorithms.
So, we have to describe algorithms at such a level of detail. That it is fairly easy to analyze how long they will take on our mathematical model. All these issues will become clear, when we describe our mathematical model which we will do right now.
(Refer Slide Time: 28:48)
 
The mathematical model of a computer that we are going to use in this course is called RAM. And RAM stands for random access machine. This is a very simplified computer model. And it only consists of basically consists of two parts. So, there is a processor which will be executing programs. And then, there is going to a memory.
The memory is going to be a correction of locations. And in fact, it is convenient to think of the memory as an array with numbers on it. So, the locations start with a certain say 0 and there might be say m minus 1. The last number could be m minus 1, if there are m locations overall. So, each location has a number which is also called its address.
So, we can refer to locations by assigning by describing the number. Of course that is going to extremely inconvenient in general. And so while writing algorithms, we will want to do something which is more pleasant. And let me start describing, how we I will describe, what exactly, how we are going to refer to the locations.
(Refer Slide Time: 30:58)
 
And in fact, as we describe the RAM model I will also be describing how we program the RAM model or how we design algorithms for the RAM model. So, the first thing to notice that although the RAM model contains locations which are addressed by numbers we will in fact allow variable names. So when we describe algorithms, we can say that say the value is contained in this variable a. A certain value is contained in variable rather than a certain value. We stored in the location fifty three or something like that.
In fact, we will allow a variety of data types. Say, we will allow plain, simple plain simple variables. But, will also allow say arrays and will also allow structures. I would like to think of these two as sort of the primitive data types. And of course, let me write down simple variables along with them. In addition of course,, we will allow other things like trees lists and so on as well.
You will be able to build your own data structures as well, but somehow or the other they will have to be built out of these data structures. So, this is as far as the memory is concerned. There will be a memory which will store the program as well. But, we will think of it as being quite separate. So, the program and data do not mix. So, here is again our picture (Refer Slide Time: 28:48) of the RAM model. So, there is a memory and then there is a processor.
(Refer Slide Time: 33:10)
  .
Now, I have to tell you what the processor can do in each step. So, basically this is going to be a description of the instruction set of the processor. So, the processor is going to have a number of instructions and will assume for simplicity that all instructions execute in one step.
There are basically three, four groups of instructions that we will have. So, one group is arithmetic and logical operations. So in this, you will be allowed in your program to say take two locations from memory. Add their contents and deposit them in a third location. Let me, write down how you will actually express this, when you write programs. And do not worry; it is going to quite in a quite friendly. This is going this can be represented in a very friendly pleasant manner.
So, for example, you could say A equals B plus C as a part of your algorithm. And this is going to be one instruction. As we said an instruction is going to be taking two operands B and C which are stored in two locations, add them up and put them back. So, this will happen in one step.
Then, you will be allowed to have conditions jumps and conditional jumps. And this will also execute in one step. So, correspondingly as a part of your program you will be allowed to write something like go to. This will happen in one step or you will be allowed to write say something like if A greater than B then go to. This will all happen in one step.
Defining our model, we want to keep this definition reasonably simple. You may be wondering at a stage, real computers probably do not look like this. And you are right and we will take that question a little bit later. I would like to make another comment over here. Although, the very second group of instructions that I am talking about concerns go to is this does not suggest. This should not suggest to you in any way that when we design algorithms, we recommend use of go to is far from that.
Algorithms as I said are intended to be read mostly by human beings. And therefore, structured programming presenting the algorithms in a nice readable manner is extremely important. However, when we talk about machines go to is are a very convenient mechanism. And that is the reason, why we have go to is in our instruction set. We will soon come to instructions which are more structured which, but that will be built out of those will be built out of our basic instruction set.
So, they will take several instructions and several cycles of execution. We will come to that very soon. There is a third group of instructions which is important and which I will call as pointer instructions. So, these are simply operations of the form say B equals star C, where I am using C style pointer notation. So, I am going to think of C as a pointer or C itself contains the address.
And I am going to fetch that location whose value is contained that location whose address is contained in C and B will get that value. I can also have a store based on pointers. So say for example, I could write star C is equal to B. And all B?s and both of these will also be executed in a single step. All of these algorithmic actions will take just one step.
Pointers and arrays are very related and the C language in particular mixes pointers and arrays a lot. And in fact, our machine our random access machine is also going to treat pointers and arrays in a very similar consistent manner. So in fact, in this group itself I will put down array operations and here I mean one dimensional array. So for example, you are allowed to say A of I equals B or B equals C of I. I do not mean this C to be the same as this C. Just C is just some array which you have declared.
Going back to arrays, let me just make one comment about that. So, we said that arrays that our machine will contain arrays and structures will assume the usual C like representation of arrays. So, if an array has size 100, then we will assume that the array is stored in 100 contiguous locations in memory. Similarly, if an array if we have a structure which consists of three components then the structure will be stored in three contiguous locations in the memory.
So coming back, we have a processor and a memory a processor whose basic instruction set I have just described, a memory which consists of a location. I have not told you what a location is really. A location simply is a collection of bits. It has to be a fixed number of bits.
Say does not really matter what number it is, but it has to be fixed once. And for all say it could be a number like 64 which is the number which is used in most modern computers. So, there is a notion of a word and a notion of a word really goes along with this notion of a location.
(Refer Slide Time: 40:38)
 
So, we looked at the basic instruction set and the basic algorithmic actions that are possible, what I am going to do next is think about more complex algorithmic actions more complex algorithmic statements. So, we said that for example, we allow these instructions, but naturally these should suggest to you some more complex instructions or statements that you would like to have.
For example, we would like to write down a statement which looks like this. So, say A equals B plus C times D minus F, how long does this take. Well our rule is very simple. We will have to break this down into out elementary instructions. So here, we have three operations and therefore, this will take three elementary steps. The three elementary instructions and therefore, this will take three steps.
So, we will allow use of such statements in our algorithm. But, when we count the time we will have to count 3. We will also want to use arrays in such expressions. So for example, we might want to say A of I equals B of I plus C of I. Again, we have to see, how this statement is going to be represented in our basic instruction set. So, here is how this could be represented.
So, first we need to fetch the value of B, the element of B. So, this could get translated to say something like X equals B of I. And this, itself is a primitive statement that we allowed and we said this takes one step. Then, we can similarly fetch C equals sorry say Y equals C of Y. And this can also again be done in a single step, because this is our basic instruction itself. Then, we could compute Z equals X plus Y.
So now, we have ahead of the two values one of B of I and C of J and the second of C of J. And we have computed their sum and all that remains is that this sum needs to be stored into A of I. So now, we can write A of I equals Z. So, this simple statement well simple which we wrote down very simply in that sense. Really has to be translated into four machine instructions so to say. And therefore, this entire statement will take four steps during execution. We could also have multi dimensional arrays.
So for example, we could have an instruction which looks like C equals A of I comma J. So here, we will have to decide, how two dimensional arrays are stored. But, we will assume that two dimensional arrays are built on top of one dimensional arrays.
(Refer Slide Time: 44:19)
  .
So for example, if we have a two dimensional array which looks like this. So, say this is an array A which has two rows and four columns. Say here at two rows and four columns. Let us say it stores elements a b c d. In the first row, e f g h in the second row. Let us say that the array is indexed c style. So, let us say this is row index 0, row index 1. This is column index 0 column index 1, 2 and 3. Now, on our machine on our RAM, we are going to store this using one dimensional array. We are essentially going to simulate it using the one dimensional array. But of course, the array must have the same number of elements. And so that is 8 2 3 4 5 6 7 8 and these elements will have to appear in this one dimensional array somehow.
Let us say they are stored row wise. Many possibilities are there, but we are just picking one. So, a b c d e f g h, so essentially every time you want to access this two dimensional array, we will be accessing some element of this one dimensional array. And by the way remember that the index set of this is 0 1 2 3 4 5 6, how do we know, which element of this array we access. In order to access a particular element of array of this array well there is a very simple correspondence.
So, A of i j if you want to access the i, jth element. So, row i column j, then that corresponds to an element of A prime which element well it is simply i times m plus j where m is the number of columns in a which in this case is 4. So, wherever we see a i j we really should be reading it as this as far as the problem of accounting how much time it takes.
(Refer Slide Time: 46:54)
 
So, let us do that. So, C of A I J well we should really be thinking of as C equals A prime of I minus 1 times m plus J. But once, we think of this statement in this manner. Then, estimating the time taken for it is fairly straight forward because now we know that we have to do one subtraction. We have to subtract 1 from I.
Then, we have to do one multiplication in the multiplication with m. Then, we have to do one addition and then we have to do a plain indirect axis. So, this whole thing will take 4 steps. Let us, now turn to some structured computing statements.
(Refer Slide Time: 47:54)
 
Let us take a loop. So, let us say we have for loop. For i equals 1 to n, let us say we do something like C of i equals A of i plus B of i. So as I said, we have to translate these instructions into our basic constructions and here is a possible translation. So, we are going to have to start off by initializing. So, we will write i equal 1. That is how the initialization step will be.
Then, we have to have the loop test. So, we will write this as if i greater than 1 greater than n, then go to end of loop. Then, we will write something like fetching the ith element of A. So, we write something like X equals A of i. Then, we will write Y equals B of i and then we will write say Z equals X plus Y in the manner that we just discussed and now we have to store it back.
So, we will write C of i equals Z. At this point we have to step through the loop. So, we will write i equal i plus 1. And now we will just go back. So, we will go to let me number these statements 1 2 3 4 5 6 7 8. And so we are going to go back to 2 and this should jump out of loop. So, this has to go to 9. So, this is going to be a translation of this.
So, although in our algorithms, we will write this for statement and just for completeness may be we will have an end for as well. But, as far as the purpose of accounting is concerned, this is the time, this is the code that we should be considering. So, when we want to analyze the time taken, this is the code that is going to be of interest.
(Refer Slide Time: 50:44)
 
So, let us try and analyze this code. So, the analysis is going to be reasonably straight forward. We are just going to go over each statement and we have to see how many times it is going to be executed. So, let us try statement number, how many times will statement number one we executed. Well this will just be executed 1 time.
This part A of i to this part is what might be thought of as the body of the loop. This will be executed n times. This loop counting and this jump back will also be executed n times. And this loop test let me write that down here. This will be executed once for each iteration. But, it will also be executed one more time. Because, that is when the machine is going to determine that we need to exit. So in fact, this statement will be executed n plus 1 time.
So, the total number of steps taken for all of this is going to be b times n, where b is the number of instructions in the body. Plus these 2 steps plus these 2 n steps plus 2 n plus these n plus 1 plus this extra 1. So, this is going to be nothing but 2 plus n times b plus 3. So, this is our final answer. Let me write that down in big letters over here 2 plus n times b plus 3. This is going to the number of steps needed to execute this loop. Of course for this loop b has the value 4 and therefore, that is going to be the time.
(Refer Slide Time: 52:48)
 
We will also allow functions calls in our programs. And when we will assume for simplicity that the number of steps is needed for the function calls are going to be the number of arguments passed. So, time equal to number of arguments. Of course I should say something about the syntax of the function calls. And the syntax is going to be pretty much like the C language, what I mean by this, is that arrays and structures will be passed by reference which means that you can modify them in the called procedure.
And the modifications will be seen by the calling procedure, whereas variables will be passed by value. So, this basically concludes the description of our random access machine. There are a number of issues that we need to consider and I will mention some of them.
(Refer Slide Time: 54:17)
 
Now and these have to do with how the how this machine relates to reality are real computers like this or are they different. Then, we have to ask questions about we will want to take algorithms and see their complete analysis. And finally, we will want to say what part of our analysis is really interesting, when we consider real computers, what features of our analysis are really relevant for real computers. So this, we will do in the next lectures.

Design and Analysis of Algorithms 
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institution of Technology, Bombay

Lecture - 3
Algorithm Analysis Framework ? II

Welcome to the course on design and analysis of algorithms. This is the second lecture on the algorithm analysis framework that we are going to be developing algorithm analysis framework part two.
(Refer Slide Time: 00:51)
 
Let me quickly remind you, what we did last time. So last time, we defined an abstract computer model called RAM or the Random Access Machine. This was the model which we said we were going to be using throughout this course. Somewhat implicitly, but at the beginning, we want to make it fairly we want to discuss it in fair detail; so that it is well understood.
So, we said that this model consisted of a processor and a memory. And we said that the data would be stored in memory. The memory would include would be able to contain things like variables structures, arrays the usual, data structures. And there would a separate program memory. Then, we said that the instruction set would be reasonably simple.
So, let me just mention which instructions are important. So, basically the instruction set would be performing operations of the form A equals operator C. So, this would be the first kind of instructions, where A B C are simple variables. This is one kind of instruction. Then, we would have control instructions or jump instructions or control transfer instructions rather. So, jump instructions and conditional jumps.
And then, we would have array operations or array or pointer operations. So, these would be instructions of the form say A equals star B where we are using the C like syntax or it could also be something like A equals B index I. So, this instruction is sort of a standard array axis. And we said last time that these instructions would be single would take a single step would take a single cycle. So, the idea was that we were going to design algorithms. And then, you would reason about what instructions those algorithms would need to execute. And then, we would estimate the total time taken.
(Refer Slide Time: 03:50)
 
The goal for today is to do the following. So, let me write that down. So, the outline for today. So, we are going to start by comparing our RAM model with real computers. Actually, it is not just real computers, but also real compilers, because compilers also will make a difference. In this comparison will see what parts of a RAM model are of interest and which parts or which details is really something that we need to define for the sake of completeness. But, which really should not be taken too seriously.
Then, we are going to define our general strategy for algorithm analysis. And then we will take some examples. And after that we will have summary as usual.
(Refer Slide Time: 05:25)
 .
So, let us now come back to our random access machine and try to find out in what sense this differs from real computers. So, first of all unlike our random access machine. In which we said that there was a single processor and a single memory real computer have a much more complicated architecture much more complicated.
So, for example, there are many different kinds of memory. So, there could be our standard memory which we will call main memory. Then, there could be a memory called cache memory. I am sure you have heard of cache memory. Most computer advertisements do talk about cache memory. There could be memory which is called registers register memory.
Then, a very tricky complication arises in a very tricky idea is used in designing computers these days and that idea is called pipelining, what this means is that? Several instructions might be executed simultaneously at in; they will be at different stages of execution. Not only there is pipelining, but there is also something called superscalar execution.
Essentially this means that say somewhere between 1 and 4 instructions might be simultaneously executed. This is a fairly complicated idea and analyzing this idea is definitely beyond the scope of this course. However, you will see that to a certain extent these complications do not matter.
We said we defined an instruction set for our RAM machine. The instruction set for real computers is somewhat different in the sense that there are instructions which transfer data between different memories in the system. So, there is memory to memory instructions memory or memory to register instructions, memory to register copying instructions. And then arithmetic or any computation is only done in registers. So, as you can see this architecture is much more complicated.
(Refer Slide Time: 08:42)
 
And I will just take one more example just to illustrate what i mean by the use of registers. So, this will drive home one of the points. So, we said that on a RAM suppose you have an instruction like A equals B plus C. So, this really happens in one step on a RAM. Our abstract on our abstract computer, this happens in a single step.
On the real computer this actually needs several steps. So, A might be a will be a variable in memory. So, first you will have to load it into a register. So, on real computers this is what you will do. So, we will load B into say register 1. Then, you will load C into register 2. And then, you will add register 2 register 1 into register 3. So, the value contained in register 2 is added to the value contained in the register 1 and the result is placed in register 3.
And then finally, you will store register 3 into location A of main memory. So, A is a main memory location. Just stress B and C as well. B and C are also main memory locations. But since, we cannot deal directly with data stored in main memory. We will have to deal with we will have to first move them into registers and then deal with them. This is the not the only way in which real computation is different from our idealized memory. There can be differences between real computation and our idealized computation.
(Refer Slide Time: 10:42)
 
Also because of the use of compilers or because of intelligent compilers, let us take a simple example; say a program which consists of just two lines. So, this is a program say which looks like if X of I equals 1. Then, do something or then go to if X of I equals 2 then do something.
 A simple translator for this, a simple translator would operate for this would have to extract the value of I. Because remember, we cannot do conditional operations directly on array elements. We first have to get them into some variable. So, this first statement would be translated into some fragment which will cause X of I to be loaded.
Say temp is the name and here we load X of I. This can be done in one step on a RAM as well as on a real computer very almost certainly. So, this is going to be one of the instructions in translation of or compilation of this first statement. A simple minded translator would also do a similar translation for the second statement which would also require 10 equals or temp 1 equals X of I, because even in the second statement we are accessing X of I.
The fact that X of I has not is not changing in between these two statements may or may not be noticed by the compilers. So, as result a high level language statement such as this one or a sequence of high level language statement such as this one could be translated into a different number of statements depending upon the compiler intelligence.
And a simple compiler would translate the first statement and a first statement translation would require a fetching of X of I. And the second statement would be translated separately and that would also require a fetch of X of I. But, an intelligent transfer an intelligent compiler would realize that this fetch is really not needed. Because, it is already there and it can already use the value which is present earlier because that value is not being modified in between.
So, in general, when we write algorithms an intelligent compiler would be between us and the machine on which the algorithm executes. And therefore, we really should be thinking about what are the capabilities of that compiler. So in summary, I would like to say that we have an idealized model of what a computer is. It differs from what a computer what a real computer actually is. It is it is also idealized, because we are not really talking too much.
About the compiler which sets between a high level language algorithm or a high level language program and what actually gets executed on the machine. So in some sense now you might be thinking that it is actually a big surprise that whatever we say about real computers whatever we say about our idealized model applies to real computers as well. And indeed we have to be somewhat careful about how we make this application and we are going to see that next.
(Refer Slide Time: 14:50)
 
So, now let me indicate our General Analysis Strategy. Our measure for each algorithm is going to be a function T of n, where T of n is going to denote the maximum time taken by the algorithm on the RAM for any instance of size n. So, this is a very crucial definition. T of n is going to be maximum time taken by our algorithm.
Here, let me call it algorithm A and let me say T sub A of n. Maximum time taken by our algorithm A to solve any instance of size n. So, as we noted in the last lecture, there could be several instances having the same size. So, if you are sorting a data set the size typically could be thought of as a number of values in that data set. And for different values even if the number of values is the same the time taken could be different.
So, when we talk about T of n or T of n for this algorithm A. We are asking, what is the largest time taken, say for sorting or whatever that algorithm is doing for all the input instances of size n and that is what we define as T of n. So, we will have a value which will be defined for T of 1 T of 2 T of 3 T of 4. For every n we will have a value. In principle, we can determine that value by executing our algorithm on all possible instances of size n.
This is only for the definition, but I just wanted to say that just to indicate that our that this definition is very down to earth. And this expression is very well defined. So, T of A T sub A of n is going to be our measure of goodness of this algorithm A. Notice that, we are not saying we are not evaluating a single number for our algorithm. So, you give me an algorithm and you ask me how good is this algorithm.
I am going to tell you here is a function. This is how good this algorithm is. You might want me to tell you a number, but sorry I cannot do that. I am just going to tell you a function. And now for every algorithm that you might give me, I will indicate a certain function. And it is your job to find out which algorithm is actually good by looking at those functions carefully.
(Refer Slide Time: 18:28)
 
So, I would like to make a few more remarks on this definition. The first remark is that this definition is a conservative definition. So, it says that the time that I am going to think is significant for a certain input size is the worst case time is sort of the maximum possible time. I could have said, well let me consider all possible inputs of size n and let me take their average time. This would be a reasonable definition.
However, this averaging of often turns out to be turns out to be difficult to perform. Primarily the reason is that you might enumerate all possible instances of size n. But, those instances may not appear in actual day to day life with equal probability. So unless, they appear with exactly the same probability for all instances taking that average is usually not so significant.
Even if they did appear with equal probability even then taking an average introduces an extra complexity to this which we will deal with once in a while. But by and large, we will stick to this reasonably simple definition which happens to be conservative. So, this is sort of the worst. So, this is a conservative definition in the sense that if I say that T of n is 5.
I know that no instance of size n will take time less than 5 will take time more than 5 and. So, it provides me sort of a very solid guarantee. And since, I am talking about the worst possible time that an instance of size n can take. This measure is also called the worst case measure.
Now, I would like to comment on how the time taken on a RAM relates to real machines. So, we said earlier we pointed out the differences between the real computers and RAMs. And we saw there that a single instruction on the RAM might correspond to several instructions on the real computers and also perhaps vice versa. And also because of compiler technology we might get differences.
So then, the question arises whatever analysis we do? For the RAM is it really of consequence for real computers. So, the point the important point that we will see soon. But, which I would like to just mention right now is that the precise value of T of n. The precise the precise numbers in the expression that we get for T of n. May not be of great consequence of real computers and real computations, what is going to be of consequence; however, is the form of T of n. 
Or let me write this as the functional form of T of n, what I mean by the functional form is the following. So, I am going to ask questions such as is T of n linear function of n. By linear I mean functions which are of the form A n plus B or is it a quadratic function of n. By this I mean functions which are of the form A n square plus B n plus C and so on. So, say cubic and other complex functions as well.
But essentially, I am not talking about the precise values of A and B. When, I talk about a function being linear, but I am just talking about the shape of that function. So, they crucial idea is going to be that the shape that this function T of n has is going to be independent of what computer it is on which you run your algorithm.
And since, we are really interested in characterizing the algorithm rather than the computer itself. It is important for us to pick a measure which depends entirely on the algorithm. And in fact, it is going this going to be the shape or this functional form, whether T of n is a linear function or a quadratic function or a cubic function. And we will we will soon see examples of this in a minute.
Very often, we will not be able to exactly estimate T of n. We will not be able to give exact bounds on T of n. Here, we will say something like what is an upper bound or we will say something like what is the lower bound. If the upper bound and lower bound match then great our analysis is complete. But, this does not always necessarily happen.
So in that case well, we will have to live with incomplete our work done incomplete. And bridging the difference between the lower bound and the upper bound will be subject of research. Finally, I will also comment that we will mostly be concerned or it is customary to be concerned about getting good estimates of on T of n for large n.
So, we always have in our mind that we are solving large problems. As our problems become larger and larger which algorithm does better that is the question that we should be asking. So, let me answer that question very quickly. If an algorithm has time quadratic, then it is going to take more time than an algorithm which has time linear. Certainly as the problem size n goes to infinity.
And so in the end, what we will end up saying is that a quadratic time algorithm is worse than a linear time algorithm. Does not matter, what the precise constants are whether it is 3X square plus 9 or whether it is 1000X plus 56, because as n goes to infinity as n goes to infinity linear time algorithm is going to take less time eventually as n goes to infinity than a quadratic time algorithm.
And this idea is justified, because typically we use computers to solve large problems. If a problem is small, then either we solve it by hand or it does not take too much time on a computer. Computers are very fast these days. And so, analyzing it is really not of interest. So, this is our overall analysis strategy. And this is a general idea about when we say an algorithm is good. When we say an algorithm is bad and so on.
(Refer Slide Time: 26:21)
 
So, let us now take some examples to make these ideas more concrete. The first example is going to be matrix multiplication. Our input is going to be 2n by n matrices say A and B which are n by n matrices. Output is a matrix C which is also A n by n matrix and C will be A times B, where A times B is the usual matrix product. So, in particular C i j is defined as summation over k of a i k b k j. Everybody knows this definition. I am just writing it down just for completeness. There are many algorithms for matrix multiplication. But I am going to consider relatively simple the most natural algorithm which just uses this definition. That is all nothing more.
(Refer Slide Time: 27:30)
 
So, here is that simple algorithm. So, we are going to have three loops. So, for I equal 1 to n for j equals 1 to n. And then, we are going to set C i j equals 0 over here. So, C i j is an entry in a two dimensional array n by n of size of dimension n by n. And so, we will assume that all over matrices are represented by two dimensional arrays and they are in memory.
And having set each C i j to 0 we will just use our definition of C i j to calculate the value of it. So, for k going from 1 to n C i j equals C i j plus A i k times B k j. And then we just close all the loops. So, this is our algorithm. We would like to find out how long this takes on our random access machine RAM.
 (Refer Slide Time: 29:13)
 
Let me just point out that the problem size can be thought of as just n itself. So, the first thing to note in analyzing this algorithm is that for every instance of the same problem size the time taken is the same. This is not typical of algorithms, but we have chosen this algorithm just to illustrate the analysis ideas. And later on we will see algorithms where the time taken is different and we will analyze those as well. Then, this algorithm also has a very simple structure. Again we will be analyzing more complex structures later on. This basic structure in this algorithm is that of a loop. And we have already seen in the last lecture how to analyze loops.
(Refer Slide Time: 30:05)
 
So, let us write that down. So, we said last time that if I have a loop in which the body takes b steps and the number of iterations is n. So, we have a for loop in which the body takes b steps and the number of iterations is n. Then, we said that the total time taken is going to be 2 plus n times b plus 3 steps. This is what we concluded in the last lecture.
If you do not remember these numbers 2 and 3 do not worry about it so much. We are in the end going to see that these two numbers are not important. But right now, we are going to do pretty exact analysis. Knowing how to do this exact analysis is important for the sake of completeness and we will do it just once. It might look it might feel a little bit painful, but it is important to do it once. So that, we are sure that we have understood the RAM model completely.
Later on we will see why not, worry about some details is important. And in fact, the second algorithm that we will analyze we will analyze without doing without paying too much attention to all the small detail. But, for now we will stick to exact analysis and will note that for an exact analysis. The time taken for in a for loop is going to be 2 plus n times b plus 3, where b is the time required for the body. The second idea that we needed from last lecture was that if I am accessing a two dimensional array an element of a two dimensional array it takes me 4 steps.
(Refer Slide Time: 32:22)
 
So, we will use these ideas to analyze our program our algorithm over here. So, let us look at this statement. So, this statement has four accesses to two dimensional arrays. So, this is one access. This is another access. This is another access and this is another access and then there are two arithmetic operations that it does.
So, the time it takes must be 4 times 4 for the array accesses plus 2. So, 10 cycles totally. So, a single a single execution of this statement itself forget the loops just a single execution of the statement itself will take time 10 steps no matter what the values of C i j A i k and B k j are. But once, we know the time required for this single statement. We can calculate the time required for this outer loop over here this loop.
(Refer Slide Time: 30:05) How do we do that? Well we wrote down that if you have a for loop in which the body takes b steps. And if there are n iterations, then the time taken is 2 plus n times b plus 3. As a result of which this for loop will take time. So, it is 2 plus n is the same over here. It is still has n iterations. B; however is 10 plus 3.
So, it is going to take time 13 and plus 2. 18, so this will take time this. So, in the reverse it will take time 21 and plus 2. But now, this is the time for this particular part. So, we just have to worry about this additional time over here. So this entire portion, all the way from here to here comprises the body of for this loop for the, for loop on j.
This takes an additional 4 steps for this access and 1 step for this assignment. So, this will take time 21 n n plus 7. So, that is the time for the body of this loop. The total time taken for this outer loop now this one over here is again going back to our formula for body the time required for the for a loop in terms of its body and number of iterations.
Well the body of this loop requires time 21 n plus 7. So that is all that we need to plug in into this expression over here. So, if we do that then for this loop we are going to get 2 plus the number of iterations is still n times 21 n plus 7 plus that additional 3. So, this is the time that we are going to get. For this outer loop in j and now all that remains is to do this outer loop in i itself.
So, the total time for the outer loop well we know the time required for the body of that outer loop and that is going to be. So now, it is going to be 2 plus n times whatever time the body of this loop takes. And that is nothing but so if I substitute this I am going to get 21 n squared plus I will get a 10 n out of this and plus I will get a 2. So, this is going to be the time taken for the entire program and. In fact, this is going to be the time taken for our for our algorithm.
So, let me write that down. So, it is going to be something like 21 n cube, total time is going to be 21 n cube plus 10 n square plus 2 n plus 2 something like that. The important point you will see are not. In fact, these very numbers, but the fact that there is a n cube term in all this. But, anyway unless I have made a mistake in doing this arithmetic the expression should come out to be something like this. Some number times n cube plus some number times n square plus some number times n plus some number.
Now, let me ask you what is going to happen if I execute this algorithm on another computer which has a different instruction set, what well change? Well the time required for each of these individual instructions could change, because that would correspond to a different number of basic instructions on that computer.
There might be some intelligent compiler which might realize that for example, fetching once you fetch this then storing it to the same position you do not need to recalculate where you are going to store it something like that. But, the important point is that no compiler is going to be able to change the fact that this in a loop must execute n times. On no computer and with no compiler this fact is going to change.
Similarly, with no computer and with no compiler, the fact that this outer the second inner most loop is going to execute n times is also going to change or that the first loop is going to execute the outer most loop is going to execute n times. That is neither going to change. So, no matter where we run this program on what compiler we use the time taken is going to involve this n cube term. So, it will be 21 n cube or it could be some 36 n cube or something like that, but there is going to be a n cube term.
(Refer Slide Time: 39:23)
 
So, let me write down sort of the major conclusion. So, the main conclusion is that the time is cubic in n. So, what I mean what do I mean by cubic. Let me write that down again that the time is going to be some function of the form A times n cube plus B times n square plus C times n plus D. We do not really care so much about what the values of A B C D are when we say the time taken is cubic in n.
And in fact, saying that it is cubic in n provides us, precisely the level of the level of exactness that we are looking for. Because, we really do not know, what these constants are going to be, unless we look at the precise architecture of a computer. So, if we are going to analyze an algorithm if we are not going to analyze a computer itself. But, if we are going to restrict ourselves to the analysis of the algorithm itself then all that we can say is that the time taken is going to be A n cube plus B n square plus C n plus D.
Or make this statement which sounds almost qualitative which is that the time taken is cubic in n. But, this is exactly what we want to be saying in this course. We want to say whether the time taken is cubic or quadratic or linear or something like that because this weak looking statement is exactly the kind of statement which we can justify and which we can claim about all possible computers.
 The important point is that although we are making a weak statement. When we say we do not know what the constant A is. We are making at the same time our statement is actually a very strong statement, because we are saying this about all computers. If we made on down this a to be 105, then that statement we would have to make about a specific computer say some specific Pentium whatever computer or some other architecture computer and that is not what we want to make.
We do not want to talk about computers in this course or of precise computer architectures in this course, but we want to talk about algorithms. And therefore, we are going to restrict ourselves to making statements of this kind that. The time taken is cubical and or quadratic and so on. And our methodology is going to be similar to what we followed just down. We are going to execute we are going to mentally think about how long an algorithm executes. And then, we will estimate the time is cubic or quadratic or whatever.
In fact, our methodology is going to be simpler than what we just saw. Because now we know that we are really not interested in whether it is 21 or 2 plus n or whatever 2 plus n square or whatever. We can just say we will just ask about is it cubic is it quadratic. So and will take it in that manner.
(Refer Slide Time: 42:46)
 
I will take one more example, to illustrate this idea of analysis of algorithms and this example is that of finding the median. The input in this case is n numbers. So, let us call them say X 1 X 2 all the way till X n and our goal is to find out the median. So, the median element is the element which appears in the middle in all those numbers. Or in general, it is defined as that element which is smaller than utmost n over two elements and larger than utmost n over two elements. So, let us first try to it is a very simple program for finding the median. This is not the best possible program, but it is actually it is a very simple program.
(Refer Slide Time: 43:46)
 .
So, let me indicate that. So, let us say let me just write it separately over here. This program will look like this. So, let us assume that the input that is given to us is stored in this array X. So, the idea of the program is going to be I am going to look at successive elements of X. And I am going to check whether it satisfies the median definition, what; that means, is I am going to compare it every other element.
And see if the number of smaller elements is utmost n by 2 and the number of larger elements is also utmost n by 2. If we find that both of these conditions match then we are done. So, here is the code which does this because we are going to say n equals length of X. Then, we are going to have a loop where we consider each element in turn. So, for i equals 1 to n, we are going to say the number of smaller elements is equal to 0.
We are going to initialize the number of larger elements to also 0. Then, we are going to check. So now, we are going to check the i th element with the every j th element. So, for j equal to 1 to n, if X of i is greater than X of j then we will increment smaller. If X of i is strictly less than X of j. Then, we have found an element which is larger. And therefore, will increment the count for larger.
That ends this for loop and at this point we will check, whether we have median in fact. So, the check is going to be something like this. If smaller is less than n b y 2 and larger is also lesser than n by 2, then exit. Because, we have found, otherwise we repeat. We repeat with the rest of the loop. So, this matches this for over here. That is the very simple algorithm.
(Refer Slide Time: 46:50)
 
Now, the question is how do we analyze it? So, we are going to do that analysis in the abbreviated style which i indicated. So, again we are going to look at the inner most loop. So, here is the inner most loop, how long does this take the body of this take. Well the body of this is going to take some C steps.
I will leave it to you as an exercise to show that C further will be no larger than 7. But, I am not going to worry about whether it is 7 or 8 or some or whatever it is, but I will just say it is some C. So then, what can I say about the time taken for a single execution of this loop.
Well it executes n times and so I can write down this time as being say some say linear in n, what does linear in n mean. It means that it is going to be some A n plus B sometime which is A n plus B. Now, we will focus our attention on this outer loop, how many times is this outer loop going to be executed. Well we said that it is going to be executed utmost n times, but it could execute fewer than n times as well.
But, let us make a simple assumption a simplifying assumption and say that it never exits early or in the worse case or let us say since we do not know whether it exits earlier or not we will just calculate an upper bound. So, now, we are in the business of calculating an upper bound. Since, we are not being too clever in analyzing when it will when it will exit early if it will exit early.
So, if we make this assumption that it does not exit early, then the time taken for this is going to be linear in the time taken for the body of this loop. But, that itself is linear in n and therefore, the time taken for this loop is going to be quadratic in n. But, that finishes the analysis of the entire algorithm. Because in fact, the time taken for the entire algorithm will have just this extra step and therefore, the time taken is going to be quadratic in n.
(Refer Slide Time: 49:07)
 
So, what have we concluded? We have concluded that the time is utmost quadratic in n. So, remember again the time when I say the time I mean a function. So that function is smaller than some function which is quadratic in n. That is what I am saying over here, what do we need to do? In order to complete this analysis we need to ask is there a lower bound we can actually put. So for that, we need to we need to figure out what happens to this exit condition. Is there a situation in which this exit condition is not really met or met very, very few times?
(Refer Slide Time: 50:04)
 
So in fact, I will leave this as an exercise for you and that exercise is if X 1 X 2 to X n is the input. And if this last number happens to be the median and all other numbers are distinct. Then, this exit condition will be met only at the very end. And then therefore, we will say that for this particular instance the time taken is going to be at least quadratic in n as well. So, let me write that down. So, this is our conclusion. It is at least quadratic as well as utmost quadratic. This does not mean for every instance it takes time at least quadratic. This just means for the worst instance it takes time at least quadratic.
(Refer Slide Time: 51:00)
 
So, let me now conclude this lecture. So, what have seen in this lecture? In these two lectures well we have seen in the RAM model. Then, we have indicated an analysis strategy. This strategy is based on the notion of worst case analysis. We said that the RAM model is not equal to real computers. But, conclusions about the form of T of n are still valid. And that is what is going to be of importance? And finally, last point. For most of the most of the course this is going to be this is going to be our framework.
(Refer Slide Time: 52:02)
 
We will consider worst case, but once in a while we will also look at the average case. So, let me just leave it leave you with this summary and that is the end of the lecture.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 4
Asymptotic Notation

Welcome to the course on Design and Analysis of Algorithms. Our topic today is Asymptotic Notation. Let me begin by setting down this topic, in the context of our overall course goals. We said last time that one of the main course goals was to well first design algorithms.
(Refer Slide Time: 01:10)
 
And then, we want to analyze their time. Analyze the time taken on the RAM model, which we defined. We also said that the results of the analysis are not directly applicable. So, there is some care needed in understanding, how to interpret the time taken on the RAM model? And how to use that predict, what happens on other computers. So, for example, so in all this we need some level of, we need to be a little bit imprecise.
We need to throw away some details of our analysis, in order to predict what happens on other computers. So, the entire analysis or what I really mean the entire detailed analysis not applicable to other computers. We said that suppose the time taken in the RAM was something like say 10 n cube plus 5 n square plus 7. Then, all that we can say for other computers is that the time is going to be cubic in n.
However, the same conclusion would be arrived at say of the time was 2 n cube plus 3 n plus 79. Even here, our interpretation our conclusions for the computers at large would be that the time taken is only cubic in n. So, this is what our conclusions will be, for any computer or all computers. So, you see that we start off with the precise number over here. But, over here we are going down to a very rough statement.
And in some sense, we are saying in all this that this function, this expression which we are going to think of is a function in n. N is the problem size. So, this expression 10 n cube plus 5 n square plus 7. And this expression or this function, 2 n cube plus 3 n plus 79 are really in the same class. So, you want to define the notion of classes of functions.
(Refer Slide Time: 04:07)
 
So, the idea is that we want to put functions in the same class and really think about the entire class. So, our conclusion will be that instead of saying that the time taken is 10 n cube plus 5 n square plus 7. We really want to say something like cubic, but we want to be a lot more systematic and formal about it. So, that is really the goal of today?s lecture. So, we would like to develop the notation, which allows us to talk nicely about classes of functions.
So, asymptotic notation, so is a formal way or formal notation to speak about functions and classify them. Asymptotic analysis refers to the question of classifying functions or classifying the behavior of anything, but in this not too precisely, but by putting them into classes. So, let me start out by writing down what do, we need from the classes, that we are going to define. So, we want really two kinds of features.
(Refer Slide Time: 06:05)
 
So one, we would like to put functions such as say 10 n cube plus 5 n square plus 17 and 2 n cube plus 3 n plus 79, should belong to the same class. Because, we said that in some sense we are going to be classifying these as cubic. And we want them to get together. Another way of saying this is that constant multipliers should be ignored. So, the constant multiplier over here is 10. The constant multiplier over here is 2.
So, we are going to ignore that. And that is our desire. Because, eventually what we can really say is that, the time taken on the RAM is this. And the time taken on any computer has to have the form, like something times n cube. So, we want a class notation, which allows us to nicely ignore constant multipliers. Our class notation should also really worry about, what happens as n tends to infinity. So, we should give more importance to behavior as n tends to infinity.
So, it is also seen in this example itself as n tends to infinity. Really the 5 n square plus 17 and this 3 n plus 79, these two parts of these functions will go out. And therefore, we will really be worrying about 10 n cube versus 2 n cube. And then, out first property or first feature which we said, we want in our class definition will take over.
And it will say that, really 10 n cube and 2 n cube are really the same thing. So, that is the spirit. So, we want a notation a class notation, which will allow us to conclude that say functions of this kind are really similar or are in the same class. So, let me give an outline of today?s lecture.
(Refer Slide Time: 08:43)
 
So, we are going to define three main kinds of notation today. So, one is the theta notation. One is the O notation. This is the capital or the big O. And then, there is the omega notation. And these will define function classes. And they will do exactly, what we said we want. And we will have lots of examples throughout. But, at the end we will also have a series of examples. So, let us go in order. And let me start off with the theta notation.
(Refer Slide Time: 09:38)
 
So, in what follows we are going to have functions, say something like f and g. And these functions are always going to be say non-negative functions, which will take non negative values functions of non negative arguments. This is natural in the sense that, we are going to be talking about time or may be sometimes the memory or any kind of resource. And these values will not really, there would not be any occasion when functions will need to take negative values.
So, now theta of g where remember g is a function, is the following class. So, it is the class of all functions f where. So, let me write that down. So, f is a non negative function such that, there exists constant c 1 c 2 and n naught. Such that c 1 times g of n is less than or equal to f of n less than or equal to c 2 times g of n. And this is true, not necessarily for all values of n, but certainly for n greater than or equal to n naught.
This is bit of a big definition. But, let me just indicate the spirit of it. So, let us go back to the properties that we wanted. So, we said that whenever whatever class structure we defined, should give importance to behavior as n tends to infinity. So, this is the part of the behavior. This is the part of that requirement. We are saying that for, only that we are only really bothered about, what happens as n is bigger than some n naught.
So, we are not worried about smaller values of n. Then we said that we should really not be worrying so much about constant multipliers. So, this is also what is going on over here. So, it says that we want f of n to be sandwiched between c 1 time g of n and c 2 times g of n. So, let me draw a picture here.
(Refer Slide Time: 12:35)
 
So, this is where I plot the function values and this is n. Then, c 1 times g of n will look something like this. Say in general, that is going to be something like. This is c 1 times g of n. This is going to be c 2 times g of n. And let us say this is n naught. Then, our claim our requirement is that, if f occupies this region entirely and does not go beyond this region, go outside this region. This is the region for f, somewhere inside anywhere inside.
So, then it is sandwiched between c 1 times g of n and c 2 times g of n. Then we will put f in the class theta times g. So, notice that we are not caring what happens, if for values of n below. Over here, f could go outside this. That is ok. We are not worried about that. But, beyond n naught f must only lie between this sandwich region. So, essentially we are saying that, we do not worry about what that constant factor is.
So, it is bracketed below by some multiple of g. It is bracketed above by another multiple of g. So, essentially it behaves like g and that for large n. That is exactly, that is exactly in the consistent with the features that we wanted. So, let us take a few examples.
(Refer Slide Time: 14:12)
 
Let us take our two functions, which we started. So, I will write down. Say f of n equals or let me call it f 1 n equals, say 10 n cube plus 5 n square plus 17. Then, this function belongs to theta of n cube. Let me write down. I will prove this, but let me write down the other claim as well. So, let me write f 2 of n another function, which is say 2 n cube plus 3 n plus 79. And this also belongs to the same class n cube.
So, I just want to reassure you. That the goals with which we started namely, developing a class notation which will enable us to put these two functions in the same class. Or those that goal is actually being met. So, let me now say in one what basis I am concluding something like this. So, let me go back to the old definition. So, in order to classify a function as being a member of this classed functions, this set of functions.
All I need to do is, to find suitable constants c 1 c 2 and n naught. So, if I find these suitable a constant c 1 c 2 n naught such that, these properties are met. Then, I am really done. So, let us take one. So, let me write this down as proof of one. So, clearly 10 n cube is less than or equal to f 1 n. Because, there is a that additional 5 n square plus 17 term. I can write down f 1 of n is certainly less than. I will just raise all these to n cubes, instead of keeping them n to the zero n square over here.
So, this is certainly less than 10 plus 5 plus 17 times n cube. Or this is equal to 22 n cube or 32 n cube really. And this is true for all n. So, I have established that if I take c 1 equals 10 and c 2 equals 32. Then, c 1 times n cube is less than or equal to f 1 of n less than or equal to c 2 of n for all n greater than or equal to say even 1. That does not really matter. So, these constants c 1 c 2 and n, and 1 is equal to n naught having found. And we know that the functions. And these constants satisfy the properties that, we wanted.
And therefore, we can legitimately claim, that f 1 belongs to this class. So, the conclusion from this is that f 1 belongs to the class n cube. And in fact, the same kind of analysis where example is to prove, that f 2 also belongs to the same class. Let me take one more example, which actually illustrates that our notation is a bit more is going beyond what we really started off.
(Refer Slide Time: 17:54)
 
So, I am going to argue now. So, that suppose I take f 3 of n is equal to n cube plus, say n log n. Then, even this is in the class theta of n cube. This is not something that you would classify, as being cubic, because cubic has the connotation of cubic polynomial. So, it has to it has to have the form something times n cube plus something times n square plus something times n plus a constant.
Whereas here there is something funny that is there is a term, which is not a polynomial term. However, note that it still is true that 10 n cube is less than equal to f 3 of n, because n log n is certainly always at least zero or certainly greater than zero in fact. And in fact, I can since I know that, n log n less than n cube. I can also write this as less than or equal to 11 n cube. So, I have found c 1 equals 10 c 2 equals 11.
And say n naught equals 1 for which, my whole definition holds. And therefore, I can write fully claim that this function f 3 also belongs to the class n cube. So, this is a good idea. It is good that in fact, this function belongs to this class n cube. Because, as n increases as n tends to infinity then, this function really is the same as 10 n cube essentially, because this term is going to be negligible as for large n as compared to this term.
And so therefore, since it is essentially the same as n cube. It is a good thing that our classification system, is putting it in the same class. Let me write down a few more examples. Actually before that, let us come back to this definition itself. So, theta of g is a set of functions or class of functions. And we are going to think of g as being sort of a representative or v g s being a sort of a prototypical function.
So, instead of talking about a very detailed function like, say 10 n cube plus n log n or 10 n cube plus 5 n square plus 17. We will say, we are roughly going to say that it is n cube. If we are ignoring constant factors and as n goes to infinity. And we will instead be saying that, it is that the class theta of n cube. So, think of g also as a representative of all this, all these functions. So, let us take a few more examples.
(Refer Slide Time: 20:53)
 
Some, I will write down may be that 5 n log n plus 10 n, belongs to the class theta of n log n. So, the important point over here is that, this 10 n is grows slower. So, as n becomes large this term is going to dominate this term. And therefore, its behavior should be essentially the same as that of n log n. And therefore, it is in the same class. We should really prove this. And I will leave that as an exercise.
We should really prove that, a function of n which is 5 n log n plus 10 n belongs to this class theta of n log n. And by that, I mean you should exhibit constant c 1 c 2 and n naught. Such that c 1 times n log n is less than or equal to this, which in turn is less than or equal to c 2 times n log n, for all n greater than the n naught that we defined. That is a fairly easy task, but if you certainly do it. So, as to make sure that you are fully conversant with this definition.
Let us take a slightly more complicated, but complicated looking example. So, let us say we have a polynomial a of n which goes something like. Say summation i going from zero to k of a i, a i n to the power i. And let us assume that, a k is greater than 0. Other terms could be smaller, but a k is bigger than 0. Then, I will claim that this function a of n in general, any polynomial of k th degree is in the class theta of n to the power k.
Again you should prove this. But, this proof is really similar to what we have done earlier. And there should not any difficulty what so ever. Again the message is the same that, if you have a function. You look at the most, the largest determinate. And that is really is, it is class. That really is its asymptotic complexity class. Let me now write down, some properties of this definition. So, suppose we have f belonging to theta of g 1. And say h belonging to theta of g 2.
Then, I claim that f plus h belongs to theta of g 1 plus g 2. This also you should be able to verify, fairly straight forward. And furthermore, if say g 1 this is the same as g 2. Then f plus h when I write f plus h, I really mean f of n plus h of n, the function which returns for every n, f of n plus h of n. And this function belongs to theta of g. This is also understandable, because if you believe the previous result. So, g 1 plus g 1 equal to g 2 equal to g.
So, g 1 plus g 2 will be equal to twice g. And the class two n cube theta of two n cube is really, the same as the class theta of n cube. Again you should be able to verify this. You should verify this. And it is really not surprising, because we started off by saying that we really do not want to worry about constant factors. And therefore, it does make sense or it should make sense, to have theta of n cube with the same as theta of 2 n cube.
We however, never ever write theta of 2 n cube. And that is, because it is much simpler and much nicer to say, theta of n cube. So, when we write theta of g we do not, we drop off the constant multipliers, if any that might be present in g. So, we have defined the first class of functions that we wanted. And it does indeed, it is indeed consistent with our intuition. And the goals we set ourselves. However, although this is a class of functions, in computer science and mathematics there is a funny style evolved, as far as writing down these classes is concerned.
(Refer Slide Time: 25:38)
 
So, let me write that. Let me write that down. So, I am going to write down a note, on writing style. So, suppose f belongs to the class theta of g. Now, very often or most commonly this is not written in this manner. But, the more common writing style is, to write f is equal to theta of g. Unfortunately, the assignment operator is again being badly abused over here. This seems to be a tradition in computer science.
We use assignment to mean, we use the equal to operator to mean assignment. We use it to mean. Well, first of all it has the value equality. Then, it has this value. It has, it is used to indicate assignment. And here, we are actually using it to denote inclusion. However, you will see that you will not really be bothered by this. It will become very clear by the context that is, what we mean.
Actually the situation is really similar to our use of English language words. So, for example, we might write Rose is Red. When we write this, we really mean that Rose belongs to the class of red things or the set of red things. So, as you can see even in the English language, the verb is used to indicate equality. That is perhaps the more common use. But, it is also used to indicate some kind of say conclusion.
So, anyway instead of saying f belongs to theta of g, it is very common to say f equals to theta of g. We never however, write theta of g equal to f. This is never written. Just as, it does not make sense to say red, well I guess in poetic English. It does make sense to say red is Rose, but we never write this in computer science. I will add one more note on the writing style. So, I have been writing functions as functions by their names directly. So, I might write something like f equals theta of g. But, from time to time I might also write f of n is equal to theta g of n.
(Refer Slide Time: 28:34) 
 
If n is it is sort of understood, n is clearly understood as an unbound variable. The argument, the possible argument that f can take. So, these two really will be think of these as being the same. This I might write it in this manner. Just to emphasize the fact that, f is a function. But, if it is clear that, f is a function then I am it might be god to write it in this manner. Let me take one more example.
(Refer Slide Time: 29:07)
 
Let me define f 5 of n as 2 plus 1 over n. So, what can we what class can we put this in. So, it turns out that there is actually a nice class into, which we can put this. And this class is simply the theta of 1 class. So, here let us say g of n is always equal to 1. And then, we have to argue that. In fact, f 5 belongs to theta of g, which is what I have written over here. Let us just do this. Just to make sure, we understand this.
So, clearly 2 is less than or equal to 2 plus 1 over n, which is equal to f 5 of n. And in fact, this is less than or equal to say 3. And therefore, we have c 1 equals 2 c 2 equals 3 and this is true for all n. So, we can have n naught equals 1. And we have these three constants, satisfying our basic definition of theta. And therefore, we can write f 5 as belonging to theta of 1. So, theta of 1 is the class of all functions, which are essentially constant.
They may have some minor perturbations, but they really are like constants. So, now we will come back. We will come to our other two definitions. Our other two classes and we will define those. So, the first class is O of g.
(Refer Slide Time: 30:53)
 
So, this is a class of functions f, where such that where f is a non negative function. And there exist c 2 and n naught, such that f of n is less than or equal to c 2 times g of n, for all n greater than n naught. Omega of g is the same thing as above. So, it is f but. So, f is non negative. But, now we are worried about c 1 and n naught. Such that f of n is less than c 1 times g of n is less than or equal to f of n. And we do not have anything on the upper for all n greater than n naught.
And let me just refresh you, what the theta of g definition was. The only difference was that, we wanted there to exist c 1 c 2 and n naught. Such that, c 1 times g of n is less than f of n and is less than c 2 times g of n. So, you can see that the class omega relaxes one of the conditions, which was present in the class theta and so it does O. O relaxes the lower bound condition. And omega relaxes the upper bound condition.
So, in omega the lower bound condition is present. But, we are not saying anything about, whether f of n is bounded above by some g. Most of an algorithm analysis, we find it is easy it is reasonably easy to say that the time taken is at most something like this, at most this function. So, for example in last lecture. In the last lecture, we said something like if we look at this program. And we count the number of iterations.
Then, we can certainly argue that there are at most n cube iterations or something like that. And therefore, the time taken has to be at most cubic in n. There we later on did argue that, the time has to be at least cubic. But, suppose we just had argued that it was utmost cubic. Then, we would have used this O notation. So, we would have said that the time taken belongs to O of n cube. So, let me write that down.
(Refer Slide Time: 34:02)
 
So, if we know say the time taken as a function of n is less than or equal to say 15 n cube plus 17 plus 7 n square plus 35 or something like that. Then, we can conclude that t of n belongs to O of n cube. If in addition, we prove that t of n is also greater than say 2 n cube plus 37. Then, this would imply let us go back to our definition. So, let me put it on top. So, let us see. So, here we are establishing that t of n is bigger than 2 n cube plus 37.
Or I can write this as, say 2 n cube is less than or equal to t of n. So, which is exactly the condition that, we wanted over here. And therefore, we could argue that t of n belongs to the class omega of n cube. Over here in the first case, in the first case we said that t of we know that of, n is less than 15 n cube plus 7 n square plus 35 and which I can write down as in fact, less than 35 plus 7 plus 15, so 57 n cube.
And that is really satisfying this condition. And therefore, I can conclude that t of n is belonging to this class. But, what happens as a result of both of these. So, I have really established that, this t of n is bounded below by c 1 times g of n. Simply, this 2 n cube. And it is bounded above by c 2 times g of n, which is simply this 57 n cube.
(Refer Slide Time: 36:29)
 
So, as a result what has happened is that, I can conclude from both of these things that t of n is belonging t of n belongs to theta of n cube as well. So, let me make this point again, because it is an important point. The class O of g is the class of functions, which are bounded above by g. So, if we know something about what is bigger than these, than the function that we are considering.
If we know a function, which is bigger then, we can say we can put it. Put the unknown function in this class, in this O of g class. So, if you know an upper bound on a function then, we should be looking at expressing that upper bound as O of g. If we know a lower bound on that function, we should be looking at expressing this knowledge as this f belongs to omega of g. And if we know both upper bound and lower bounds in terms of the same function g then, we should write that this function f belongs to theta of g.
(Refer Slide Time: 37:48)
 
So, as you must have already guessed the class theta of g is simply the union of the classes O of g, the intersection of the classes O of g and omega of g. So, let us take some examples of O.
(Refer Slide Time: 38:13)
 
This is something like 3 n square belongs to O n square. 3 n square in fact, belongs to theta of n square. And therefore, it certainly belongs to O of n square, because O of n square in fact, is bigger than theta of n square. Here is another example. So, say 10 n cube plus 5 n plus 7 belongs to O of n cube. Similar logic, but something like 10 n cube plus 5 n plus 7 also belongs to O of n to the fourth. Why is this? Because, 10 n cube plus 5 n plus 7 is less than or equal to I can certainly write this as being less than or equal to say 32 n to the fourth.
And that is all my definition of O really cares about. So, this also belongs to this. This function belongs to this class. No surprise, you should be surprised by this. Because, we are really saying that g serves like an upper bound on this function. And if I can, if n cube is an upper bound then certainly n fourth is an upper bound as well. 
(Refer Slide Time: 39:45)
 
Let me summarize that f belongs to theta of g, should be read as f is nearly the same. Or let me write it as similar to g. f belongs to O of g, should be read as f dominated by g. Actually, not dominated. f is no larger than g. Sort of like less than or equal to, but it is not exactly less than or equal to. Because, we are ignoring constants just as this is sort of like equal to. f belongs to omega of g, like y should be thought of as f greater than or equal to g. But, again we are ignoring constants. And also lower order terms.
So, we now have defined our three main functions classes, theta of g, O of g are also called big O of g and omega of g. I have defined these classes in the context of the times taken by algorithms. But of course, these are just plain old function classes. And the functions could denote, not necessarily the time taken, but any old thing. So, for example let me define a general function. Which just, it is the sum of n numbers.
(Refer Slide Time: 41:33)
 
So, let me say S of n is equal to summation i going from 1 to n of i itself. Well you do know, from say some of the mathematics courses that you have done. That S of n is nothing, but n into n plus 1 upon 2. However, getting into a result like this, requires some amount of cleverness, this result is a very precise result. If you prove that S of n is exactly this, it is a very precise result.
But, sometimes you might say, you might not have enough time or you might not have enough cleverness to get on exact result like this, which is to say that S of n is exactly n into n plus 1 upon 2. But, suppose you might be happy with a weaker results. So, you might want to know well, does S of n grow or does S of n belong to n square into the class theta of n square or does it belong to the class theta of n.
At first glance just by looking at this, it should it is not clear at all whether S of n belongs to the class theta of n square. Or whether it belongs to class theta of n cube or anything like that. So, what I want to stress or what I want to give an example of right now is that, even without getting to this exact expression, you might be able to determine the class to which a function belongs. And in fact, we are going to prove that S of n belongs to the class theta of n square, without actually calculating S of n precisely.
And this is an instructive example, because something like this will happen when we analyze algorithms. So, S of n is equal to summation i going from 1 to n of i. But, note that, this i is always going to be at most n. And therefore, I can write this as summation i going from 1 to n of n itself. But, what is this? This is just n plus n plus n n times, because every term in this sum is n. And therefore, this is nothing, but n square.
So, what have you established? We have established that S of n is less than or equal to n square. But, that right away puts S in the class O of n square. So, this implies that S of n belongs to the class O of n square. Can we argue that S of n belongs to the class omega of n square? That is, what we are going to do next. So, again we observe something very simple. So, S of n is summation i going from 1 to n of i.
Now, I am going to ignore the first in our two terms. So, this certainly is greater than or equal to. If I ignore the first in our two terms, this is going to be i going from say n over 2 plus 1 to n of i itself. But, now note that this i is always going to be at least n over 2, because it starts at n over 2 and goes all the way till n. So, therefore, I can write this as summation i going from n over to 2 plus 1 to n of n over 2 itself.
So, term by term this series this sum is bigger than the corresponding term over here. But, what is this? This is simply n by 2 added to itself n by 2 times. So, therefore, I will write this as equal to n by 2 times n by 2, which is n square by 4. So, I have argued that S of n is bigger than or equal to n square by 4 and at most n square. So, we have bracketed S of n. Well before that, just once we argue that S of n is bigger than n square by 4. We can conclude that, S of n belongs to omega of n square. Just going back to our definition, we have proved that. Let us just do this once.
(Refer Slide Time: 45:59)
 
So, we need to argue that f of n which is S of n now, is greater than or equal to n square by 4. And the c 1 is now 1 by 4, but that is. We do not, we did not necessarily say we did not say over here that, c 1 has to be greater than 1 or anything like that. Any real number, any positive real number is fine. There exist, I should write this as there exists c 1 on n naught positive all through. All the c 1 naught that, I have written should be positive. That is what, that is exactly what we have proved over here S of n. So, we have argued that S of n belongs to n square as well and from this and this, what can I conclude?
(Refer Slide Time: 46:48)
 
Well from those two things, I can conclude that S of n must be theta of n square as well. So, notice that in doing any of this I did not actually exactly evaluate S of n. In this case evaluating S of n exactly is possible, but very often it is not. As I said, it may require exceptional cleverness once in a while to exactly evaluate a function. But, giving bounds on it is easy. And the bounds on it, can be nicely stated in terms of the class notation that we have right now.
So, if you know an upper bound we can state it as S of n belongs to O of something. If you know a lower bound, we can state it as S of n belongs to omega of something. And if we know both, we can state S of n as theta of something. In this manner, I would like you to prove. So, let me write this down as home work. Prove that, say t of n if t of n is equal to summation i going from 1 to n of say i square. Then, prove that t of n belongs to theta of n cube. The proof is really more or less identical, but you should. But, again you should persuade yourself of this. Let me take one more example.
(Refer Slide Time: 48:34)
 
So, let us look at the Fibonacci series which is defined by f of n equals f of n minus 1 plus f of n minus 2. And f of 1 equals f of 0 equals 1. Let me claim that, f of n is always greater than or equal to 2 to the power n by 2. This is also home work. What can you conclude from this? You can conclude from this that, f of n is an omega of 2 to the n by 2. Let me make, one more claim. Well, let me not make that claim.
Let me instead tell you the real result, actual result. It is possible to show that f of n is theta of 1 plus root 5 upon 2 whole to the power n. So, it is not exactly this number, but it is within a constant multiplier of this. The nth Fibonacci number is within a constant multiplier of this. Proving this exact bound, takes a lot more work. But, by something really simple we have at least argued that f of n is actually going to grow at least as 2 to the n by 2. Or in fact, I can write this as omega of root 2 to the n.
So, something like this is commonly called exponential growth. Some by a very easy logic, by very easy reasoning we can argue that the Fibonacci number grow exponentially. By more complicated reasoning and by reasoning, which involves essentially involves finding out a precise formula for the nth term. We can get a much tighter results. But, the importance right now is that our theta notation and our omega notation allow us to express our knowledge or lack of it, in a very compact manner. So, let me summarize now.
(Refer Slide Time: 50:59)
 
So, one we have defined theta, O and omega notation. These capture the idea that, ignore constant multipliers. Consider n goes to infinity, which is equivalent to saying that ignore consider leading terms. So, our time estimates of algorithms will be expressed using these notations. The second thing that I want to mention is that, in fact this notation is just a general notation on functions. And it can be used for other things as well and sometimes.
And it really allows us to express, what we know and what we do not know in a very compact manner. So, even if we do not know even if we know a little bit about it then, we can put it in a certain class and that, this partial information that we have can be nicely expressed. So, this is useful for thinking about functions in general. 
Thank you.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 5
Algorithm Design Techniques: Basics

We begin with discussion of some Basic Design Techniques for Algorithms. We will start with fairly simple problems that many of which you may have seen. But, we will hopefully see solutions to non trivial problems, which you may not have seen. The first example, I would like to start with is finding the minimum element in an array.
(Refer Slide Time: 01:21)
 
So, the input problem is Find Min. The input is an array A. And the output is a minimum element in A. A element with whose value is the minimum. This is a problem, which most of you would have see earlier, in your. Perhaps the first programming course may be later even in your course on data structures. Let us anyway go over the solution. So, the standard solution is, you have you take a temporary variable.
There is a temporary variable, which contains the current minimum. So, this contains the current minimum and it is updated at each step. So, you start with the first element in the array. And then, you scan the array element by element. The First element then the second element third element and so on up to the last element in the array. Each time, you compare the array element with temp and update temp if necessary.
This simple technique actually is used is very powerful. And used often, which is if your input is in the form of an array or a list, then if you can solve the problem for the first n minus 1 elements of the array. Can you solve it for the n th element? This is the step, which is put in a loop. So, let me just write this down. So, the important step is this. If A i is less than temp, then temp is set to A i.
This is the crucial step. And this is usually put in a loop i varies from 1 to n, where let us say n is a size of the array. And at the end of this temp will have, the minimum element in the array. So, I will be when I discuss algorithms, often I may not write the full code. In fact, I will not write the full code. The idea is to give you the main ideas, behind the design of this algorithm. I may not take care of some of the stray cases. I may not initialize variables properly etcetera, etcetera.
So, all these programming details I will not get into. The main idea is to, get the main the techniques, and the ideas behind the algorithm which solves the problem. And you should be able to sort of take these ideas together. And write the program, which actually works. Yes. So, if you look at this, this is the level at which we will describe algorithms may be even less. I could just say, scan the array element by element. And update the current minimum as required.
(Refer Slide Time: 05:15)
 
There are, so there is two things couple of things that I would like to point out. So, this thing this design technique is often called well induction. Both recursion and iteration actually go with this. I will explain this a bit more. The second technique, the second sort of thing we will keep in mind is ordering. And the third thing is to store value, already computed. I must add store necessary this value. So, let us go each of these one by one.
So, induction what you mean by induction is this. You want to solve a problem. Your input has some size. The array has say size n. Now, the one way to solve it is, supposing you can solve this problem for smaller values of input. If this problem can be solved for smaller values of input, now can I extend these solutions to a solution for the bigger input. If you can solve the problem for the smaller values of input, can you extend the solution to a bigger value of the input.
For instance, if I can solve a problem for all arrays of size n minus 1, can I solve this problem for an array of size n? Now, this step which takes you from n minus 1 to n is often put is the crucial step. And once you come up with this step, we just put this step in a loop or you use recursion. You first recurse on an array of size n minus 1, and then extend it to an array of size n. Or you scan the array one by one, element by element. And you update the solution as you go along.
So, this is what I mean by induction. And it is at the base of every algorithm design technique. You can call it the mother of all algorithm design techniques. You will learn some more fancier things later, but this is the very crux of design of every algorithms. Second point that, I would like to make is ordering the input. Looking at the input in the right order often helps. In this case, it is simple. You know, you look at the array elements in the increasing order of the array index.
In fact, you could look at the array in any order. It really does not matter. But, there are cases, we will see cases where ordering plays a very crucial role in solving the problem. The third thing, which is also fairly simple here is to store some of the values that you already computed. That is the third point, we want to make. Now, even in this case it is simple. You just store the previous minimum, the variable temp the temporary variable which we had.
So, these are values that you would like to use in future. Almost every algorithm design technique that, we will study are a combination of these three. Some of them will use just induction and storing whole values. Some of them will use all three. Some of them will just order the input and use induction. So, these three are things that you must keep at the back of your mind, when you design any algorithm. Let us analyze this algorithm.
(Refer Slide Time: 09:23)
 
So, here is the here is an analysis. Every algorithm that we design, we will analyze. Analysis is as important a part of this subject as design. And we would like algorithms to be as fast as possible, in the worst case. So, all our analysis pertain to worst case. And we would like to design algorithms, which are as fast as possible. So, in this case we look at every array element one. And we make one comparison for array element. This leads to, we make n minus one comparison.
We do not really compare anything with the first element. But, with each of these subsequent we make one comparison. n is the size of the array. So, we make n minus 1 comparison. Along with comparison, we need to we also store this new value in temp. But, the number of times we do this exactly equals the number of mean. It is let us say one more than the number of comparisons, we make. So, if you are not really worried about constants, then you can just focus on the number of comparisons.
As long as I bound the number of comparisons, the other small operations that I do, like incrementing the index variable of the array or storing the value in temp. They all are of the same order as the number of comparisons. So, the time taken here we would say is order n. The time taken is order n, but our focus is just on the number of comparisons. I think the question that one can ask and one should ask is this the best?
Can I find the minimum element in an array in using less than n minus one comparison? Well, we just think about it. Or even, if you do not think about it. I guess, most of you will jump up and say, of course you need n minus 1 comparison. That is the sort of first reaction that, most people will have. Without making n minus 1 comparison, how can you find the minimum? Why is this so?
Can you logically argue that, you actually need n minus 1 and you know you cannot do with less than n minus 1. This argument is easy, but not absolutely trivial. Let us see. So, why should we make n minus 1. Can we do it with less? Well, every element you have to look at least. Every element in the array must be compared to something or the other. If it is not compared then, you may actually make it the minimum. It could be the minimum.
If you had output something else as a minimum then, this could have been made the minimum. Or you can easily make some other element of the array the minimum. So, without comparing if you do not compare an element then, you cannot tell the minimum element in an array. Now, this gives you this does not give you n minus 1. It gives you n by 2, assuming n is even. So, at least n by 2 comparisons are needed.
The reason every element must be compared, must be in some comparison. What I mean is, every element must be compared with some other element. This can be done with n by 2, by the way. So, the first element is compared with the second element. The third element is compared with the fourth element and so on. The fifth element is compared with the sixth element and so on. So, with n 2 two comparisons, if n is even or n by 2 plus 1, if n is odd.
Well, ceiling of n by 2 comparisons are all that is necessary. To sort of satisfy this condition that, every element must be in some comparison. Now, clearly n by 2 is not the right answer. n minus 1 is the right answer. And why is it that 1 is n minus 1? So, here is an argument. See initially, when you have not made any comparison there are n candidates for the minimum. Each element in the array can actually be a minimum.
You do not know, which of these n elements are minimum. Now, when you make a comparison, you can get rid of only one of these candidates. When you compare, let us say at some stage you have candidates x 1 x 2 up to x k, some k elements are candidates for the minimum. Each of them based on the comparisons that you have made, previously each of them is equally likely, I mean each of them can be a minimum.
Now, if you compare two of these candidates you can get rid of one. Whichever one is smaller, that still remains a candidate. Whichever one was larger, that no longer remains the candidate. But, you can only get rid of one candidate. So, with each comparison I can only get rid of one candidate. I have n candidates to start with. So, I need n minus 1 comparison. This is actually a complete proof, though a bit hang wavy. You can make it more rigorous also.
Here is one more way of looking at it. Supposing, you draw the following graph. So, initially we have I have all these nodes. Let us say the elements x 1 x 2 up to x n. These are elements and the array and also vertices in our graph. Let me just put a circle around these two, indicate that these are also vertices. Now, when you compare x i and x j. Here is x i and here is x j, I draw an edge between these two.
Now, when you compare let us say x 1 and x 2 and x n, I draw this edge. Now, x j and x n are compared I draw this edge and so on. As you make comparisons, I keep drawing these edges. So, once your program ends terminates, you have done all these comparisons. Now, I look at this graph. And I look at each connected component in this graph. If there are more than one connected components in this graph, then I will not be able to tell the minimum.
Now, in each connected component I know which is the minimum? There will be one which is a minimum, but I can surely give values to these. So, that I can pick the global minimum from, any one of these connected components. Here, this is arguments that after the comparisons are over, once you finished all comparisons I better have one connected component. This means from a discrete structure?s class, you know that you need n minus 1 edges.
So, this is just the same argument. Both of them have the same idea behind both. For instance, here you start with n connected components. And each time you add an edge, you can decrease the number of connected components by 1 at most 1. So, it is the same argument. So, you need n minus 1. And this sort of simple scan of the array does it with n minus 1 comparison. Let us look at a slight variation of this problem. Now, I want to find not just the minimum in the array, but also the maximum.
(Refer Slide Time: 18:29)
 
So, this problem is called Max Min. So, the input is in array A. And the output is the maximum and the minimum elements in A. So, I want both the maximum and the minimum element. Now, if I just wanted to find the maximum, the procedure is clearly the same as the one for the minimum element. What if I want to find both the maximum and the minimum? Well I could first find the minimum and then, I can find the maximum. How many comparisons does this take?
Well n minus 1 for the maximum. n minus 1 for the minimum. And that makes it 2 n minus 2. So, the number of comparisons that a na?ve algorithm. So, this is the number of comparisons that, the na?ve algorithm makes. We can ask the same question. Is this the best? You can try the previous argument that, we had of connected components. And so, you can show that you need m minus 1.
That is fine, but when you see more than, that is very difficult to prove. It is not absolutely impossible, but it is difficult. And if you try to certainly increase it to 2 n minus 2, it is impossible. You will not be able to prove it. Let us look at some small values. Supposing I have four elements, i have let us say x 1 x 2 x 3 and x 4. The na?ve algorithm took x 1 compared with all of them. Found the minimum and then we were done.
Then, we took again took x 1 we compared it with all of them. And while maintaining the maximum, the temporary maximum. Now, many of these comparisons are repeated. So, you see that many of some of these comparisons that you make, when you on roll the whole thing out, are repeated. So, our aim is to sort of get rid of these unnecessary comparisons. Now, in four elements here is what you can do. I first compare x 1 and x 2.
So, supposing x 1 is less than x 2. So, this is the first comparison. x 1 is less than x 2. Now, I compare x 3 and x 4. Supposing x 3 is greater than x 4. This is my second comparison. Now, where do I find the minimum? I mean, how do I find the minimum? The minimum clearly is either x 1 or x 4. It is the smaller of x 1 and x 4. So, I compare these two and find the minimum. Similarly, I compare these two and find the maximum.
So, how many comparisons I have made? I have made 1 2 3 and 4, so four comparisons. What does our old algorithm say? It says 2 n minus 2. n is 4. So, this is 6 when n is 4. So, we seem to have done certainly, better than 2 n minus 2. When there are four elements, we have certainly done better than 2 n minus 2. And well the trick was, once we found the minimum and maximum between x 1 x 2 and x 3 x 4 then, for the minimums I only need to look at x 1 and x 4.
The smaller on the left hand side and the smaller on the right hand side. I do not have to bother about the bigger one and similarly for the maximum. So, this trick can be applied recursively. Well, it is certainly worth trying and let us do it. So, what we do is this.
(Refer Slide Time: 22:57)
 
Here is the let us say I have you have an array of size n. So, let us divide this into two parts. Recursively find the maximum in this part. Let us say Max L. This is the left part and that is the right part and Min L. Recursively, find the maximum minimum on the right hand side. So, that is Max R and Min R. Now, how do I find the maximum and minimum? Well, I need to compare these two to find the minimum. I need to compare those two to find the maximum.
So, here is an algorithm that seems natural. I divide this into two parts. Divide this into two parts. Let us say two equal parts. Two equal halves. Halves are always equal. I find the maximum and the minimum on the left hand side. The left half, I find the maximum and minimum in the right hand side. Now, I compare the two minimums to output the minimum of the array. I compare the two maximums, to find the maximum of the array.
How many comparisons does this algorithm take? Let me write down the algorithm. But, in future with this explanation you should be able to write the algorithm. So, divide into halves. Let us say left and right. Then, recurse on both parts on the left and on the right. And the answers are Max L and Min L and Max R and Min R. Then, put these things together, to get the minimum and maximum. And then, compute final solution from the solution of the two parts.
Well, I have written the essence of the algorithm without really writing details. I hope, you can fill in the details. Define procedures and write down recursive calls. And you know, do these two comparisons and output the minimum maximum. Do how many comparisons does this take? That is the question, we need to answer.
(Refer Slide Time: 25:53)
 
So, let us say T n is the time taken by max min on arrays of size n. It is the time taken by max min on arrays of size n. Then, T n is there are two problems of half the size. You solve two problems of half the size. There is 2 n by 2 plus; two more comparisons, one between the 2 maxs to get the new max, one between the 2 minus to get the new minimum. These are this is for the recursive call. So, you call the left hand side, that is n by 2 right hand side n by 2 and then 2.
Well, if n is odd I would have a ceiling and floor somewhere. But, let us not worry about it for the time being. Let us assume that, n is even. You can assume n is the power of 2. We also know that, T 2 is 1. For two elements, I can find it in one comparison. So, what is the solution to this recurrence? Let us see. So, the easiest way to solve all this is to check, how this recurrence behaves. So, T n is nothing but 2. And now, I open this out.
This is 2 T n by 4 plus 2 plus 2, which is 2 square T n by 2 square plus 2 square plus 2. You can right this down, once more. And essentially we want to see how, what pattern this follows? Well, it is not too difficult to guess what the pattern is? The pattern is this. 
(Refer Slide Time: 27:55)
 
So, T n is 2 to the i T n by 2 to the i plus 2 to the i plus 2 to the i minus 1 and so on. All the way up to 2. So, now we set n by 2 to the i to be 2, because we know that t of 2 is 1. Then, we have t of n is 2 to the i. This t, this becomes 2. So, t of 2 is 1plus 2 to the i plus 2 to the i minus 1 and so on up to 2. So, this is nothing but 2 to the i plus 1 plus well 2 to the i minus 1 and so on up to 2. You can check that this is nothing but we also know that 2 to the i plus 1 is n. And I hope you can solve this.
I will leave it for you, to solve this. This is nothing but n. And this sum, you will get as n by 2 minus 2. If you sum this up, using the usual geometric series and use this fact, that n is 2 to the i plus 1. You will get that, this sum is nothing but n by 2 minus 2. Well, put this together. You get t of n is 3 n by 2 minus 2. You can check that this, when n is 2 this is 1, which is what we want. And this also satisfies the recurrence that we had.
The recurrence was let me refresh your memory. The recurrence was T n is twice T n by 2 plus 2. So, if I put T n equals 3 n by 2 minus 2, this satisfies the recurrence. You can prove that T n is this ((Refer Time: 30:12)). Well, so the number of comparisons that we seem to make using this method is 3 n by 2 minus 2, which is certainly better than 2 n minus 2. We seem to have done something fairly mechanically and we seem to have improved the number of comparisons made, quite drastically.
So, this technique is called divide and conquer, is used by the British in the last century, I mean last century and even before that. We will put it to good use, in designing algorithms. So, let me write down the main steps of this technique.
(Refer Slide Time: 31:02)
 
The first step is to divide the problem into, I will say two parts. Often we will want these parts to have equal sizes, often of equal sizes. The next step is recurse on each part. So, you recurse on each part and solve each of them. Both the parts, if there are two. And the final step is put these solutions together. Put these solutions together, to get a solution for the original problem. Often, you can just do this blindly.
In fact, for the max min we could have done it blindly. Take this array of size n, divide that divide this array into two arrays to size n by 2. Find the maximum and minimum on the left array, the maximum and minimum on the right array. And once you have these two solutions together, now you find the maximum of the whole array comparing the two maximums. Find the minimum of the array by comparing two minimums.
Again the essence is this induction. In the sense that, if you could solve problems of smaller size which is what you are doing in this recursion. You are somehow putting these together to get a solution, for the big problem. How you will find these small problems varies from problem to problem. Let us look at another problem, where we will apply this method blindly and we will see what we get?
(Refer Slide Time: 33:14)
 
This problem is to find both the minimum and second minimum. The minimum is the smallest element in the array. The second minimum is the next one, the second smallest element in the array. So, your input is an array. And you are going to find both the minimum and the second minimum. The usual way you would do it is, you first scan the array and find the minimum. Now, you again scan the array and find the second minimum.
The other way to do it is, to have two temporary variables Temp 1 and temp 2. In temporary 1, I store the current minimum. In temporary 2, I store the current second minimum. Now, when I get to an array element i let us say the ith element. I compare this first with the minimum then, with the second minimum that I have. And based on the result of these two comparisons, I update minimum and second minimum.
Now, this will take we have seen how many comparisons this will take. It is 2 n minus 2 as it was in the max min case. And let us just apply our divide and conquer paradigm, blindly to this problem and see what you get. So, how would we do this? So, here is the array. The array is of size n. I divide this equally into two paths. I find a minimum and second minimum here. So, let us say Min left and the second Min left, Min right and S Min right.
So, I have found these four values. And now, I want to find the minimum and second minimum for the entire array. The minimum is not a problem. So, I just compare Min L and these two values and I can output the minimum. The smaller of these two is the minimum. Now, what do I do about the second minimum. Now, supposing Min L was smaller than Min R, so without loss of generality this Min L. So, assume Min L was less than Min R, which means at this point Min L has been output. Now, what are the candidates for the second minimum? Clearly, Min R is still a candidate for the second minimum.
Second minimum of the left hand side S Min L is also a candidate for the second minimum. But, one of these elements we have sort of thrown out, which is S Min R. This does not figure in the picture at all. So, we need exactly one more comparison to get the second minimum, which is supposing this is true then compare the second minimum on the left and the minimum from the right. So, you need to compare just these two elements.
And you can see that, the minimum of these two will give me the second minimum. The minimum of the entire array I get by comparing these two minimums, the minimum the left hand side minimum the right hand side minimum. And the second minimum I can get by comparing the minimum element, which lost the first comparison which was larger and the second minimum of the element that one.
So, that will give me the second minimum. So, how many comparisons have we does this take. Well, if you write down the recurrence this seems to be very similar to the previous one. So, what is?
(Refer Slide Time: 37:23)
 
So, if T n is the time taken by the algorithm, we have two sub problems each of size n by 2. That takes time t of n by 2. And then, we have two more comparisons, one with the two minimums and one to find the second minimum. We also know that, t of 2 is 1. If I have two elements, one comparison suffices to find both the minimum and second minimum. And these set of equations are exactly the same as the set of equations, we had before.
So, the solution is T n is 3 n by 2 minus 2. So, the number of comparisons we make is 3 half n minus 2. And it is not 2 n minus 2. It is much less. One can ask, is this the best? Can we do better than 3 half n minus 2? This question can be asked both for max min and also for min and second min. Well, it turns out that these two problems behave differently. For max min 3 half n minus 2, is the best we can do.
So, 3 half n comparisons is the best we need 3 half n comparisons. While in this case, when minimum and second minimum you can do with actually less. The divide and conquer sort of paradigm gave us 3 half n, but that is not the best. So, why? So, let me give you a reason why you can do better here. To better this, you need to understand a bit more as to how this algorithm works? Let us unfold the recurrent, the recursion out and see what this looks like.
(Refer Slide Time: 39:18)
 
Initially, I have array elements x 1 x 2 x 3 x 4 and so on, all the way up to x n. What we do is, we just divide it into two parts. We divide it down the middle. And then, you recurse on these two. On the left half, we divide it again into two halves, recurse divide recurse divide recurse. Now, we come down all the way down to, when they avail as size 2. This is when the comparisons start happening. The recurrence sort of bottoms down, till you reach arrays of size 2.
Now, I will find the minimum of x 1 and x 2 that goes up. The minimum of x 3 and x 4 is pushed up. Also the second minimum is pushed up. But, let us not worry about that for the minute. The maximum case also it works very similar. You put the max min, max mins are pushed up at each level. We just focus on the minimum element. So, the minimum element is pushed up. From here, the minimum element is pushed up from there.
At the next level, I will compare the minimum of these two. This contains a minimum of x 1 x 2. This contains a minimum of x 3 x 4. This contains a minimum of these two, which is actually the minimum of these four and so on. This would be a bigger tree and so on. All the way up to the root and this root, you can the minimum is known. So, for instance here x n minus 1 and x n are compared. This is the minimum. At the next instance, you compare it with the other two.
This would be x n minus 2 and x n minus 3 and so on. All the way up to the root, where the minimum is known. Now, this looks like if n was say power of 2. This looks like very familiar complete binary tree. So, there are log n levels. There are n leaves. There are log n levels. And in each level, we sort of have some sort of minimum and some portions of the array and these are pushed up. Now, where was the minimum element?
The minimum element sits somewhere in this array. And at each stage, it is pushed up. It sort of wins, its comparison each time at each level of this tree. And it finds its way to the top. Somewhere with perhaps, came from the left. May be it came from the right, came from the left came from the left and so on. So, it does traverse some root all the way from the root node to a leaf. And this is where the minimum element resided.
Now, what can you say of the second minimum element. Now, well the crucial sort of observation that you need to make to speed up the algorithm is that, at some stage the second minimum must have been compared with the minimum element. This is absolutely crucial. If the second minimum element were never compared with the minimum element then, you really do not know which of these two is the minimum, because in each comparison the second minimum one, it was smaller than every other element.
So, was the minimum element which of these two is minimum, to know that you must have compared the second minimum element with the minimum element. Now let us look at this picture. Here is the picture. How many elements did the minimum element, you know win against. How many elements were compared with the minimum element? If you look at this picture and you follow this, at each stage in this path down from the loop to the leaf, the minimum element was compared with exactly one element.
The length of this path is log n. So, the minimum element was compared with at most log n elements in this tree, which means log n elements in this array were compared with a minimum element. And one of these log n elements, remember must be the second minimum. So, to find the second minimum all we do is this. Find the minimum using this tree. You can do it recursively, if you want.
Once you find the minimum element, collect all elements that the minimum element one against was compared against. If you have this tree in front of you, you can certainly go down the tree and figure out, which were these elements. Among these elements, find out which is the minimum? And that will give you a second one. There are log n elements.
(Refer Slide Time: 44:24)
 \
So, initially you made n comparisons. May be, it is n minus 1, n minus one comparisons to find a minimum. And then, you need about log n minus 1 comparison more to find the second minimum. This then is actually optimum, though we will not do it in this course. There is an argument, which shows that you need n plus log n. But, it is surprising that you can actually do this in n plus log n. And this problem in this way, it differs from the previous problem.
A straight forward application of divide and conquer does not work. You need to use some more intuition. You need to understand a problem a bit more come up with new ideas. And that is what algorithm design is all about. Often, there are problems which are hard. You really do not know what to do? And when you come up with a smart answer to an algorithm, you feel really you feel nice.

Design and Analysis of Algorithms
Prof. Sunder Viswanathan
Department of Computer Science Engineering 
Indian Institute of Technology, Bombay

Lecture - 6
Divide and Conquer ? I

(Refer Slide Time: 01:11)
 
The three of them are first 1 is induction. Then, ordering the input and the third is storing old values. So, let me sort of let us talk about all these three once more. Induction essentially says, supposing you can solve the problem for smaller inputs. How do you solve it for larger inputs? So, this is the first term design principle. So, supposing you know, you can put together, these solutions for these smaller inputs. And somehow generate a solution for the larger input.
Then, your algorithm is clear. You sort of recurse on the smaller inputs solve them. And when you get the solutions back you put them together. And get the solution to the larger input. That is induction, you could use either recursion or it could even be once you know what the smaller inputs it could even be ((Refer Time: 02:42)). The next point is ordering the input, which is you look at the input in some order. For instance, if it is an array you look at it. Let us say, in increasing order of indices.
For other structures it is could be different. If it is a list again, you will have to look at it element by element. For other data input and other data structures, you could this could vary. The other thing is store values, which you may use later on. If you are not going to use a value, which you have computed later on then you can of course, discard it. Otherwise, this very simple principle will help us design algorithms in the future good. So, we have seen, some algorithms simple algorithms for simple problems using these.
(Refer Slide Time: 03:44)
 
The paradigm, that we considered last time was divide and conquer. So, the basic idea was this, if I have an input let us say an array. But, it could absolutely anything. Divide it into two parts. Solve for the left, solve for the right say. And then, put the solution back together. So, you divide input. Solve each part and put them together to get a solution for the big problem. And well often it pays to divide it into equal parts. We will in fact, see an example why this is so?
But, even in the previous cases for instance max min or finding the second minimum. The two examples that we saw, you could try breaking at breaking the input up into unequal pieces. And then, try and solve the recurrence. And see, what answer you get. So, you should I sort of encourage you to try this out. So, the next problem, we are going to consider is the very familiar, should be very familiar to all of us it is sorting.
(Refer Slide Time: 05:22)
 
Only we are going to look at it now from the new sort of design principles, that we have learnt. And we are going to try and apply those to sorting. So, let us take the simplest sort of case. You have an array. An array of n elements and you would like to sort the elements in increasing order. Let us, assume that elements are distinct. This is not going to change any of our design principle or that or the algorithm we come up with, but it will just, it is just to keep your mind less cluttered.
So, you have an array of let us say, n elements and you would like to sort them in increasing order. Let us put the first design principle into practice. Supposing you can solve it for a smaller array, how do you extend it to a bigger array? And the most natural smaller arrays let us say, the first n minus 1 elements. Supposing, I could solve this problem of sorting the array for the first n minus 1 elements. How do I, now extend it to the bigger array.
So, here the problem, here is what the input looks like. This will be first n minus 1 elements and this is the last element. So, you first sort this is by recursion let us, say and now I want to place this in the right position. Now, I guess most of you know binary search by now. So, the position is very easily identified. So, let us say, this is now sorted. So, this portion is sorted and this position now is identified. Let us say, they are you do a binary search. And identify exactly where this goes in.
Well now, what you have to do is to insert this in the right position. You have to move, you have to make essentially make space. So, from this point onwards, you need to move everything one step to the right. Every element here, you have to move one step to the right. Move one step and then insert the element in the right order. So, here is an algorithm. So, let us go over this again. You recursively, sort the first n minus 1 elements. Figure out where the last element sits and then put it in place. This is called insertion sort.
So, in you previous courses, you have dealt with this algorithm. You have learnt insertion sort there people I guess told you what insertion sort is all about. Here, we are trying to understand, how it is that people come up with these algorithms. So that, when you are faced with a new problem, you can come up with an algorithm of your own. That is the idea. So, this is how you come up with an insertion sort. It is just putting you know the principle, we had into practice.
((Refer Time: 09:03)) how much time does this take. Well, if we just look at number of comparisons. If just want to find out the number of comparisons. The last element, you use the number of comparisons is log n, because you are just doing a binary search. So, and every other element use less time. It was for the i th element it was log i. The time taken was log i. So, the total time is in fact, roughly order log n, order n log n. I mean it is order log n per element, there are n elements and it is n log n.
But, the mains step here, which takes time is not comparison, but this movement. Let us, look at the last step. The last step says, once I find this place I have to move everything to the right. Now, this place could actually be the beginning of the array, in which case you are going to shift the entire array by 1. So, while you just took log n time to find the place to move elements, you may take you know n units of time. This is the expansive step in insertion sort.
And if, implemented in this in this manner, each time you take order n steps. For the i th step, you take i units of time to move it to the right. You may take I units in the worst case, in which case the time will be order n square. So, I will leave it for you to see if you can use better data structures. So, that you can sort of avoid this movement. But, implemented this way insertion sort takes order n square time though the number of comparisons here is still n log n.
Let us, put our other design paradigms into play into practice. One was to divide the array equally. Work one each piece and then put them together. Let us look at this.
(Refer Slide Time: 11:15)
 
So, here is my so sorting. So, this is the second algorithm we are going to try. So, here is my array. You divide it into two equal halves, two halves. Sort each piece, sort each piece and then you have to put these things together. So, let us, write this down. So, divide into two pieces. Sort each piece and then put them together. So, the algorithm is clear, except for this step. What does it mean to put them together? So, far it is okay. Dividing into two pieces no sweat, sort each recursion.
You recurse on each piece you sort each. Well, here is the crucial step that we need to implement, which is putting two sorted arrays together. Let me, also say that this you could work with other data structures. Instead of arrays you could have list. And you can apply the same design principles to list too. In fact, we will do this with arrays. But, you it is really does not matter. So, when I mention array, you could you could put a list there equally well. Especially, for this method, so we divide into two pieces.
We sort each of them by recursion. And then we put the solutions together. What does this step entail? Well, this let us focus on this step. Because, once we implement this the algorithm is ready. And you can program it at your leisure. So, this step entails the following. I have two sorted arrays, we sort it. I need to combine this to get one big sorted array. This is what I want. I have these two pieces, which are sorted. This piece is sorted, so is that piece. We need to put these two sorted pieces together to get a big sorted piece. So, this is the problem that we would like to solve. So, this is called merging two sorted arrays or lists. It does not matter.
(Refer Slide Time: 13:54)
 
So, we are given elements. These are sorting let us say, sorted in ascending order and I would like to build a bigger structure, which is which contains elements in both of them and which is sorted. So, I guess the procedure should be fairly natural. And most of you should have gotten this by now. Which is well, what is the smallest, so here is my new list or array. This is my output. What is which element will occur will occupy the first position. Well it has to be the smaller of these two elements.
So, what I do, I first compare the first elements of these two lists or arrays or sub arrays. And put the smaller of them here. So, I can have two pointers or two temporary variables, which point to some places in the array. I compare these two put the smaller one here and move that corresponding pointer. Let us, call this A and B, supposing A 1 is smaller than B 1. Then C of 1 I should assign the smaller of 2, which is A 1. And I should move this pointer.
So, let us call these two pointers something. So, small a and small b. So, c small c is small a and then I need to increment. So, increment so a, which is pointing here will point to the next element and so on. So, I just keep doing this at any stage. So, what is the generic step at some stage I have these arrays A and B.
(Refer Slide Time: 16:24)
 
I have pointers small a and small b. And then I have pointer small c. I compare these two elements, put the smaller of them here and increment the corresponding pointer. This is the algorithm. So, I just essentially, somehow scan these two and keep filling in these elements in this big array. So, this is merging. I hope you can write code, if need be. If the idea is very simple and if they are less of course, it is it is trivial, you just you have these two pointers you sort of compare these two values.
Put the new value in a new list and increment this pointer. That is the way it is done. And arrays if you want to you may actually, have sub arrays of the original array, where you are doing this work. You could have a temporary array, where you create this merge list and then write it back into the original array. So, this is the algorithm. So, the merge step is also now done. This is the algorithm. How much time does this take? This algorithm is called merge sort by the way. So, it is called merge sort. And how much time does this take.
Let us go back to this ((Refer Time: 17:52)) scribble piece of paper. So, here is merge sort. It says, divide into two pieces sort each of them and put them together. This is essentially, essentially merging two sorted sequences into one big sequence. That is the third step. The divide step well does not take any time. Sorting each is a recursive step. We sort on roughly half the array. And we need to figure out how much time does it take to put these things together. Good. So, what is the time taken? Supposing T n is the time for an array of size n.
(Refer Slide Time: 18:43)
 
By time again, we will focus on the number of comparisons, because even here. Every other operation is big o of number of comparisons. So, the number of, if I can bound the number of comparisons, the total time is a constant times the number of comparisons. So, please check this. For instance moving the pointer etcetera, etcetera, etcetera. And if, you have list copying one list to the other etcetera. They are all bounded by the number of comparisons.
So, I can just focus on this one quantity, which is number of comparisons. So, let T n be the number of comparisons that we make, we would now like to bound the total number of comparisons needed for an array of size n. Well the divide step does not take any time. So, but there are two sub problems of roughly equal size, so that that gives you 2 T n by 2. Again, we will not worry about floors and ceilings for ease of calculation. And then, there is a merge step. How much time does merge step take on an array of size n. So, this is something we need to do. So, what is the merge step on an array of size n.
(Refer Slide Time: 20:20)
 
The merge, I have two arrays of size n by 2. Two sub arrays of size n by 2 or two lists of size n by 2. And I merge this to get something of size n. How much time does this take? Well, there are many ways of figuring this out. So, let me tell you one way of doing it. This is a smart way of doing it. And then, I will tell you a way, which is just using the, we will again design this algorithm using design principles, that we have already followed and then analyze it that way also. So, we will do it two ways.
So, what is each step? Each step you compare two elements and an element gets filled in the array C. The crucial sort of observation here is that. Each time you make an, each time you make a comparison. One element in C in this new array, the final sorted array gets filled. Once again, one for each comparison an extra element a new element gets filled in the total array. How many elements get filled? The total number of elements that gets filled into a new array is n.
So, the total number of comparisons you make is utmost n. Let me say, state this analysis again, because this is this is quite important. Each time you make a comparison an element of C gets filled. So, if I make k comparisons, then I fill k elements in C. The total number of elements in C is n. So, the number of comparisons I make must also be bounded by n. So, the total number of comparisons is n. The reason is for each comparison, we add an element to call this array C.
From these two smaller arrays A and B, I pick an element and add it to C. The total number of elements in C is n. So, the total number of comparisons I make is also n. Let us, do this merge differently. So, that total time now is n., we now this. And we can now, write the recurrence and we will solve it. That is fine. But, let us again do this merge a bit differently. So, we are now going to apply our design principles, which is induction like a merge to smaller arrays. How do I merge larger arrays?
(Refer Slide Time: 23:40)
 
So, let us look at this again. So, here are my two arrays ((Refer Time: 23:46)). Now, I know that the first element in C is the smaller of these two elements. The first element here is the smaller of these two elements. So, let us say this is smaller. Now, I look at the input without this. The input now consists of one less element from here and some elements from here. Now, the sizes need not be same. So, I can supposing I solve my problem for this smaller input.
Can I solve the problem for the larger input? Well, the answer is should be you know vociferous yes. It can be solved. This is the smaller of this shaded is the smaller of these two. And now, I have two arrays here. So, the total size is smaller. One element I have removed. So, these two I merge to get this portion. And the smaller element I put here, this goes in here. And recursively, I can sort of merge these two to get this portion. So, this is the other way of doing this.
Here, you would naturally use recursion. And if not iteration at least initially, may be later on you will see that this can be implemented iteratively. And you could put them together iteratively. So, now how much time does this take? If there are two elements. So, let us write the recurrence here. So, T, so this could be n and m, because they could have different sizes. So, if it is, let me not use n here. Let us say m 1 and m 2. T of m 1 m 2 is what, well one of them decreases by 1. We do not know which one.
So may be, I can just write a recurrence on the sum of these two sizes. So, I can actually, write it on n. This n stands for the sum of these two sizes, which is the total number of elements in the input. This is nothing but t n minus 1 plus 1. Where does this 1 come from? This is two, when I have two lists. For the first comparison I make. I make I compare these two elements remove the smaller 1 and apply recursion. This 1 comes for the first comparison I make.
And you can check that T n the solution to this. So, T of 2 is 1. The solution to this is T n equals n minus 1. So, to merge two arrays the sum of two sizes is n. The number of comparisons is n minus 1. So, these are two ways to do this merge. When you write the code actually, the comparisons they, the both the algorithms do will be, if you unroll the recursion you will actually get the iterative process back. So now, we can go back to merge sort and look for the what is the time that we need.
(Refer Slide Time: 27:19)
 
So, it is 2 twice T n by 2 plus order n. So, in fact, well I just put n here for simplicity. You can put a constant times n and calculate this, it will make no difference. These constant will just come out of the calculation. So now, we need to solve this recurrence. I guess most of you would have hopefully do know how to solve this. Let us, anyway do this. So, T 2 is 1. Well, I should strictly write T n is less than or equal to 2 T n by 2 plus order n.
If I am writing comparisons for the simple reason that, if I have two arrays whose let us say, they are not of equal size. Then I could use less than n. In this case, it is n and it is equal. But, often you should sort of check whether, this should be equal to or less than or equal to. In this case, it is in fact, equal.
(Refer Slide Time: 28:39)
 
And T n is twice T n by 2 plus n. So, this then is so let us open this out, twice T n by 2 square plus n plus n. So, this T n by 2, I have replaced I have again used this recurrence. I am sorry. So, this should be n by 2. I have just used recurrence. I use the same recurrence on T n by 2. So, that will be twice T n by 4 plus n by 2, but just using this with n by 2 in place of n. So, that is what I get. So, what is this, this is 2 square T n by 2 square plus n plus n.
So, if I do this I times, well you could do it once more to see what the pattern looks like. If I do it I times, I get 2 to the i times T n by 2 to the i, you can plus there will be a number of n. So, how many times will I have n?s here, it will be i times n, this you can check. You should do it once more, you will get 3 times n. So, here I have 2 times n. If I do it once more I get 3 times n. The next step you get 4 times n. So, you check, you sort of guess that this is what is going to after I steps.
And you can actually check this by induction on i. Once, you have guessed this you can check that T n is in fact, equal to this by induction on i. That I will let you know. Now what, well we continue till n by 2 to the i. So, let me let us start here.
(Refer Slide Time: 30:58)
 
Now, we know T of n is 2 to the i times T n by 2 to the i plus i times n. Now, I know t of 2 is 1. So, I will let n by 2 to the i to be 2. So, then what do I get. If I choose an I, so that this is true. Then, I get T n is well 2 to the i is n by 2. This is t by t of 2, which is 1 plus i times n. what is i. Well, I know 2 to the i plus 1 is n. So, i is log base 2 of n by 2. This is what i is, so plus n times log base 2 n by 2. This is order n log n. So, the time taken by merge sort is n log n.
There is one more way to do this divide and conquer business on arrays to sort. And it in fact, gives rise to another well known algorithm, which you may have studied. Let us see, what this idea is. So, here is the array.
(Refer Slide Time: 32:37)
 
I still want to do divide and conquer. So, I still want to divide the array into two parts. Somehow work with each part. And then put the solutions back together. But, the way I do, it will be slightly different in this case. So, what I do is this. So, what I do is in the divide part, I ((Refer Time: 33:02)) start rearranging the elements of the array. So, I now have two parts to the array. But, these are not divided as in the previous case.
Remember in the previous case, we just picked up the array and just divided arbitrarily into two parts. This time we are going to be more careful. What we would like is every element landing up in the left is smaller than every element landing up in the right. So, there is a x here and y here. I know that x is less than y. Again, every element in the array must occur in one of these. So, somehow, I have divided this array into two parts left and right. So, that elements in the left are smaller than the elements in the right.
This somehow we have done this. Now, what we do is just recurse on these two parts. So, you sort the left separately. You sort the right separately. Let us see, what happens once you do this. Once you do this, I claim that the entire array is now sorted. Why is that so? Well this portion is sorted right. This left portion is sorted. This right portion is sorted. I also know that every element on the left is smaller than every element on the right. So, the largest element here is smaller than the smallest element here.
So, when I look at the entire array, you can check that the entire array is now sorted. So, this is also an example of divide and conquer. The divided step is where we did work. In the divide step, which I have still to specify you somehow divided this array into two parts left and right. So, that elements in the left are smaller than the elements in the right. This was the divide step. Now, it is recursion ((Refer Time: 35:06)) sweat.
You just recurse on these two parts and recursion does it for you and you sort these two. Putting things together is trivial. You just have to do no work. Both of them are sorted, when you put them together in fact, the entire array is sorted. The only thing we need to figure out, it how to divide the input. So, that everything on the left is smaller than everything on the right. Well, some of you may have noticed that, this is an algorithm that you have seen before and it is called Quick sort.
Again, let me sort of emphasize that. Earlier on you were told what these algorithms are. You were given code for ((Refer Time: 35:54)) while this is quick sort, that this sorts an array. Our emphasis here is to see, how do people come up with these algorithms. How does somebody come up with an algorithm called quick sort. Well, this is how they come up with. So, you start with divide and conquer at the back of your mind. And you figure out, how do you put this paradigm into place.
So, now how do people, how do we split an array into left and right. Well, you pick a pivot, xome element in the array, which you call the pivot. Left consists of every element, which is smaller than the pivot. And right consists of every element, which is greater than or equal to the pivot. So, the smallest element on the right is the pivot. And you know that the left of the array every element is smaller than the pivot. So, that does it. Let me just write this down. So, that we have quick sort in front of us.
(Refer Slide Time: 37:03)
 
So, pick a pivot. This is quick sort. Pick a pivot divide into two parts smaller than the pivot elements. Let us, call this p pivot p. And elements greater than or equal to p. These are elements less than p and elements greater than or equal to p. These are the two parts. Recurse and put them together. That is the last step. Well, unlike the difference between merge sort and quick sort is essentially this. In quick sort this is where you spend most of your time. Putting them together is easy.
In merge sort dividing into two parts is easy. Putting them together is where you take time. So, this is the this is this is quick sort. How much time does quick sort take? Well, it depends on which element you pick as a pivot. So, let us see, what really happens. So, supposing you pick the i th element. What do i mean by i th element, means in the sorted order it is the i th element. So, let me make a definition here, which we will use later on.
(Refer Slide Time: 38:53)
 
The rank of an element is it is position in the sorted order. Remember recall that, we are assuming that each element in the array is distinct. So, each element has a distinct rank. If there are i elements smaller than x, say x is an element i elements are smaller than x the rank is i plus 1. The minimum element has a rank 1. And the maximum element has a rank n, which is the size of the array. So, the rank of an element is it is position in the array. So, if everything depends on the rank of the pivot.
So, if the rank of the pivot is i. Then, the time taken there will be i minus 1 on 1 side and n minus i plus 1 on the other side right. So, is T of i minus 1 plus T n minus i plus 1 plus the time taken to partition the array. What was this time? How did we partition the array? Well, we compared each element to the pivot. Put the ones on the left on the, put the lesser ones on the left and the larger ones on the right. So, we compared every element to the pivot. So, number of comparisons we made is n minus 1.
For ease of calculation, we will just say this is less than or equal to n. It will not bother us much. What is the constant between ((Refer Time: 40:54)). So, if the rank is i then it is utmost T of i minus 1 plus T n minus i plus 1 plus n. This is the time taken to partition the array, because every element was compared to the pivot ones exactly once. So, what is the solution to this recurrence. Clearly sort of it depends on i. It depends on what i is and various values of i it varies, for instance if I picked the minimum element.
If i picked the minimum element then, then what happens. Then i was 1, oh sorry this should be minus 1 ((Refer Time: 41:47)) n minus i minus 1 or it should be n minus i plus 1. That is what fits in here, N minus i minus 1. So, if i were 1 then there is nothing on the left. And there are n minus 1 elements on your right hand side. There is something wrong here. This should just be n minus i. There is no 1 here. This is exactly n minus i. I have the pivot. I have i minus 1 here. And i have n minus i elements on this side.
The pivot sits at the center. So, the right hand side had size n minus i. So, if I has chosen the minimum element, then I get T of n to be T of 0, which is 0 plus T of n minus 1 plus n. And if I kept picking the minimum element as the pivot what is this time. So, let us write this again.
(Refer Slide Time: 43:14)
 
So, T n looks to be T of n minus 1 plus n. Let us expand this further. It is n minus 2 plus n minus 1 plus n so on and so on. Well, it comes down to 1 plus 2 plus so on up to n. This is n times n plus 1 by 2. This is order n square. So, if the pivot turns out to be the minimum element in each case, then the total time taken is order n square for quick sort. And you can check that if I start increasing now. Let us say, the pivot was a second element. There is a time decreases. It will decrease till you reach the middle element.
Let us see what happens, if I pick the roughly the middle element. Then, T of n roughly, I will have two problems of equal size. That is 2 T n by 2 plus n. This does not change. Because, every element I have to compare with the pivot anyway. This recurrence I know what then solution is. This looks like order n log n. So, let us sort of write all this on one slide and look at it.
(Refer Slide Time: 44:47)
 
I started out with saying let us say, T n with T i plus T n minus i plus n. Now, if i equals 1, there is i minus 1 here. If i is 1, then i get order n square. This i is roughly n by 2. Then i get order n log n. Well, one can check that, if I plot T n versus i then it initially falls. If we start with n square it initially starts falling, till I roughly n by 2. And then, again it starts rising. So, if I pick the maximum it is very similar to picking the minimum. The array sizes split much the same way.
And it again goes back to n square. So, the worst case time is n square. And if you manage to pick the pivots correctly, I mean if you pick the middle element at each stage. Then the time taken by quick sort is n log n. So, we are only worried about the worst case time. So, if somebody asks you, what is the time taken by quick sort. Well, it is n square. This implementation of quick sort, takes order n square time, because that is the worst case time.
If somehow, we could pick an element of rank n by 2 efficiently. If you can pick an element of rank n by 2 efficiently, what do you mean, what do I mean by efficiently here. It means the recursion should not change much. So, if I know I get n log n with the following recurrence T n is 2 T n by 2 plus n. In fact, I could add a constant here. And it would still be n log n. This will still give me order n log n. So, you can check this. This constant will just come out of the calculation.
If I have c n then i get n log n. Now, if I can somehow find the middle element of an array, which means an element of rank roughly n by 2. So, if I can find an element of rank n by 2 in constant times n time. Then, I can implement quick sort to take time order n log n. The way I do it is I first pick I spend c n time pick this middle element. The element of rank roughly n by 2. Now, I do the usual. Use this pivot divide the array into two parts.
Now, the array is divided into two roughly equal parts. Once, I divide them into equal parts. Now, it is the usual recursion and I am done if I take n log n. So, I can implement quick sort. There is an implementation of quick sort. There will be an implementation of quick sort, which takes n log n time, provided I can find this element of rank n by 2 in linear time. Which is what we do next? Before we do that let me so point this fact out, that in the recurrence, it was desirable that the two parts were of roughly equal size.
If they were not of equal size, then the time was proportionally it was higher. So, often in these divide and conquer kind of situations. We try and see, if we can somehow split the input into exactly two parts in two halves. So, once we have halves, some because of the way this recurrence these recurrences work the time taken is usually smaller. So, our job next is to figure out, how to pick this element of rank n by 2 in linear time. So, let me state this problem. So, this element of rank n by 2 is called the median.
(Refer Slide Time: 49:28)
 
So, the median is an element of rank n by 2. Let us say, floor of n by 2. You could take the ceiling of n by 2. It does not matter. So, n is odd. You get the floor of n by 2 and our job. So, what we want to do is this. The algorithmic problem given an array find the median. How do you find this element of rank roughly n by 2? Well, the easy way to do it. Is sort the array and pick the middle element. That takes time n log n. The time for that is n log n.
So, if you for instance use merge sort. That is not good enough. Can we do it faster? The answer is yes. You can in fact, do it find median in linear time. And this will be the first non trivial algorithm that you will see. It is a really smart algorithm and as we present, I hope you appreciate the beauty of the solution.


Design and Analysis of Algorithms
Prof. Sunder Viswanathan
Department of Computer Science Engineering 
Indian Institute of Technology, Bombay

Lecture ? 7
Divide and Conquer ? II Median Finding

We had looked at algorithms for sorting, especially quick sort. The way quick sort worked was you picked a pivot, and compared each element in the array with the pivot. And split the array into two parts. Those elements, which were less than the pivot and elements, which were greater than the pivot, and then we recursed down these two parts. And you recursed on these two parts and recursively sorted them. And this then gave you the sorted order of the entire array.
The time taken by quick sort, crucially depends on the position of the pivot on the array. We would like to pick an element, which is the middle of the array. It is called the median. So, if we could always pick the median fast. Then perhaps we can make quick sort work in n log n time. And that is our goal. That is the next goal to see if we can pick we can given an array, we can find the median in time linear in the size of the array. So, let me make a few definitions just to set the ball rolling.
(Refer Slide Time: 02:25)
 
The rank of an element is it is position in the sorted array. So, given an array and an element to find the rank of this element sort the array and the position of this element in the array ((Refer Time: 02:57)). We will assume for simplicity that elements in the array are distinct. Every element is distinct. All algorithms that we designed will also work when the elements are not distinct. But, this is just for simplicity. This is the rank and rank of an element and the median is an element of rank n by 2. That is the floor of n by 2, an element of rank floor of n by 2. Our objective is to find the median in an array. How fast can we do this?
(Refer Slide Time: 03:52)
 
So, the problem is this. The input is an array A, the output the median. Now, clearly this can be done in order n log n time, by just sorting the array and picking the middle element. Sort the array pick the middle element that gives you the median. And sorting takes n log n time. Our objective of course, is to do this faster. We want to find the median faster. How does one go about doing this? Let can we apply divide and conquer for instance. Supposing, we want to apply divide and conquer.
How do we like to divide the array? How do you divide the array? Again, it seems sort of to think about this and you are up against the wall. So, supposing somehow, you can find a let us say, not the exact median. But, some kind of approximate median. What do I mean by an approximate median? Let us say an element whose rank is greater than n by 4. And rank of x and it is greater than by 4. That is greater than equal to n by 4. Less than equal to 3 n by 4, n is the size of the array.
So, these elements I will call approximate medians. They are not quite the middle element. But, they roughly sit around the middle element. Supposing, you could find supposing somebody gives you an approximate median. Then, what can you do. I mean can does this help. Well, it may help. I mean in the following sense that, so here is my array. I take the array.
(Refer Slide Time: 06:24)
 
This is my array A. x is an approximate median. Let us, not lose focus of our goal, which is to find the exact median. x is an approximate median and well you just come to know of it somehow. What you could do is this. You split the array as less than x and greater than x. So, here are elements less than x. Here, are elements greater than x. This is less than x. This is greater than x. Once, you do this, you know where the median falls. If this is less than n by 2, if the rank of x is less than n by 2 then the median falls on the right.
It sits in this portion of the array. On the other hand, if the rank of x is greater than n by 2, then the median falls in the left portion of the array. You know where, it which portion of the array it sits. So, the other good thing about this partitioning is that, see both these parts have size at least n by 4, the elements which are less than x, as size at least n by 4. The elements greater than x has size at least n by 4. So, if you can get rid of one part, let us say the smaller part. Then the bigger part is of size utmost 3 n by 4.
So, you have shrunk the size of the array by a constant factor. Does this is that does this help or actually it does. Supposing I can find this approximate median in let us say, linear time somehow may be you do not know how that is done. So, let us see, how much time for finding the exact median will take. So, what we would like to do is this. We have an array of size n, so let us say the time is T n. We find approximate median, which takes n time. And then, we would like to recurse on a small part.
That part has size 3 n by 4. Well there is still a problem, which we need to address. But, supposing you can do this. Then, you check that this recurrence gives you order n. You can just open out a few terms here. And you can prove that this in fact is order n. There is a problem. The problem is this. I want to find the median of the big array. I somehow get this approximate median in linear time. This we still do not know how this is done. I know that the median is here somewhere. This is the element I want to find.
The element could sit here not exactly in the middle. So, what I want to find in this array is not exactly the median. The element that I want to find depends the rank of the element I want to find depends on the left hand side of the array. So, what I want to find let us say that the numbers. So, let us call these two things let us say, a left and a right. Let us, assume that the size of a left is less than n by 2, floor of n by 2. We will assume this without loss of generality. Now, what I want is the median of the entire array.
What is it is rank in A R. So, it is the element that I want. What is the rank in A R. Supposing that rank is R. What I want is r plus 1 plus size of A L should be equal to n by 2. In other words, I want r to be n by 2 minus size of A L minus 1. So, r is the rank of the median of A in A R. The small r is the rank of the median of A in this array A R. Let me, draw a picture and sort of explain this again.
(Refer Slide Time: 11:32)
 
This is A. ((Refer Time: 13:34)) this is sorted here is my middle element. This is n by 2. Now, my element x sits somewhere here. This is this is element x, which is approximate median. And this portion is A L. This portion is A R. When the size is A L this portion is A R. Now, the rank of the median is just the rank, the number of elements in this portion. What is that, n by 2 minus A L plus 1. So, it is nothing but n by 2 minus size of A L minus 1.
So, the rank of this element, which is the median, this is the median that we want to find. So, this is the median A. The rank of this element in A R is this. So, the problem when we recurse we do not really want the median in A. We are not looking for the median. We are looking for an element of rank this. We are looking for an element of rank n by 2 minus A L minus 1. And this need not be the middle element in A L. This is the first problem.
So, when we recurse, we do not really want the i mean it is not just the median. But, we want an element of some particular ((Refer Time: 13:15)). Well, why not look for a procedure to find an element of any rank. So, pay careful attention here. This is a very sort of important design principle. We started off with trying to find the median. We tried to find the recursive procedure. Along the way we have encountered a problem, where we actually want to find an element of some other rank.
Not just rank n by 2, but some other rank. So, this seems to be a more general problem. Finding the median a special case of this problem. Sometimes, it is easier to solve a more general problem, than a specific problem. And in this case, that is what we will do. So, the problem we would like to tackle is not just finding the median. It is finding the element of any rank. So, the input let me write this problem now.
(Refer Slide Time: 14:17)
 
Input is array A and r. The output is an element of rank r in A. Now, when r is n by 2 we get the median. This is a more general problem, than the one we started out with. And this is what, now we would like to solve. The reason this problem came up is that during the recursion, I mean at least the recursion that we tried we started out with the median. But, when we try and get the recursion going the problem we end up with is this. So, this is the problem we will try and solve.
Now, you see that there is no problem with the recursion at least. So, if I solve, if I find an array A, I start with an array A. Supposing, this is still a step which is left undone. I get x, which is an approx median. I split the array into two parts A L and A r and x. I want to find an element of rank r. If r is less than the size of A L, I recurse on A L. If r is greater than size of A L plus x, I recurse on A R. That is it. That is the algorithm.
So, as long as I can find a approximate median, I can find the element of any rank, by this divide and conquer scheme. You can write down the recurrence relation. And see that, if we can find this approximate median in linear time. Then, you can find the an element of any rank till linear time. So, how will you find the approximate medium. This is what, this is seems to be as difficult as finding the exact median of any rank. So, it is and the solution that I am going to present. Now, is really smart. So, the way you find the approximate median is this.
(Refer Slide Time: 16:49)
 
First take the array A. So, unfortunately I cannot tell you exactly how people came up with this. I am just going to present the solution to you. Sometimes, we just have to be really clever and come up the solution. There is no real recipe for finding algorithms all the time. There are the few general recipes that one follows, but often you know these things are so problem dependent. That you just have to think about it think about it and at some point, you know some light bulb goes off somewhere. And you know you come up the algorithm that works. So, here is the algorithm that works. So, take the array A. Split up the elements in the following form.
(Refer Slide Time: 17:32)
 
So, here is A, I split the elements into groups of 5. So, these are 5 elements. These are the first 5 elements, the next 5 elements and so on. And I have n by 5 groups. I have n by 5 groups in a and each of them has 5 elements. Now, find the median of these 5 elements. This will give me some median y 1. This will be some median y 2. And the last one will give me some median y n by 5 these are medians of the groups. There are n by groups. Each of each consisting of 5 elements and this is the median of these groups.
Now, the approximate median, which we will pull out is the exact median, of these n by 5 elements. Approximate median that will output exact median of these n by elements. So, let me repeat this again. You split them into groups of five. There are n by 5 groups. For each group you find the median y 1, y 2 up to y n by 5. Each group, you find the median. Now, you find the exact median of these n by 5 elements. And this is an approximate median. There are number of questions.
Why should this be an approximate median? That is the first question. Second question, we started off trying to find the exact median. We ended with trying to find the approximate median. And we are again finding the exact median. To find the approximate median, we find these y 1 through y n by 5. And again we are finding the exact median. What is going on. Well, the second thing is not so serious. In the sense, that initially we started out trying to find the median of n elements.
We are now finding the exact of median on n by 5 elements. So, the size has reduced. And we can apply the, we can recurse on this smaller size. Remember I said, we can find solve the problem for a smaller size. You know, you can put the solution back up. So, the exact median that we want to find among these n by 5 elements. We just do it by recursion. We just recurse and find the exact median here. That gives you what we want. We said, we want to find an element of any rank.
This is what we want to do. If somehow, we can find the approximate median we have done. To find the approximate median, we have to solve an exact median problem on an input of smaller size. This can also be done by recursion. I have to still convince you, that the exact median of these n by 5 elements will be an approximate median. Let us see why this is. To see this, let me arrange these 5 elements in each group in descending order.
(Refer Slide Time: 21:49)
 
So, smallest element will be at the bottom say a 1 1, then the next element a 1 2. Then, a 1 3, a 1 4, a 1 5. So, all of these, I will all of these small groups, let me first write them down in increasing order upwards or decreasing order downwards. The next thing I am going to do is look at these middle elements, a you know the medians of these groups. And I will write them in sorted order towards the right. So, a 1 3 will be the smallest element among these medians.
So, a 1 3 is less than a 2 3, less than a 3 3. So on and a n by 5, 3. So, this is also part of a group, which I have written in this order. This is a 2 2. This is a 2 1, a 2 4, a 2 5. Similarly, here a n by 5, 2, a n by 5, 1, a n by 5, 4 and a n by 5, 5. So, the entire array, I have written out this way. Well the last row may not be one of these rows, may not may not have all 5 elements. But if the size of the array is divisible by 5. All of these columns will have sorry all of these columns will have 5 elements.
So, let me again explain this picture. You first, split each of them into 5 parts. There are n by 5 groups. Now, you sort, you sort them in increasing order. Look at the middle elements. Sort them middle elements in increasing order, lay them out. And lay out the entire array in this fashion. So, a 1 3 is this smallest element among the middle elements. It comes in some group. It may be the 20th group or the 15th group or whatever. Whatever it is, I will call it a 1 3.
So, the first group here I get by taking that part of the array, where the middle element was the smallest. When, I look at the middle elements this has to be the smallest. So, then comes the second group and then the third group and so on. And this is the last. I will just arrange the elements of the array this way. Now, what is the median of this, this portion. Remember, that we said that the median of this portion is an approximate median. Median of this middle portion is what we claim to be an approximate medium.
So, what is this middle what is this. So, it sits somewhere in the middle here. So, it is the way we have written this, it is a n by 10, 3. That is the element, which is the median of this, this portion. Now, how many elements are smaller than this medium? Where do you find elements in the array, which is smaller than this medium. Well, if you look at this middle row, everything to the left here is smaller. Also, everything down here is also smaller, which means this entire portion consists of elements, which are smaller. These are smaller elements.
Similarly, if you look to the right these are all larger elements. And also, if I go right and up I also get larger elements. This is also larger than this. So, this portion again consists of larger elements. This portion consists of larger elements this portion consists of smaller elements. And you can see that, each of these this portion is at least 1 4th. In fact, it is greater for our purposes this is at least 1 4th of the entire array. This portion is at least 1 4th of the entire array. That is the reason, this is an approximate median.
We have just shown that, at least n by 4 elements in the array are smaller than this. At least n by 4 elements are larger than this. That is why this is an approximate median. And once, we have the approximate median, well we know what to do next. We take this approximate median and recurse on these two parts. And I have well in sort of vague disjoint way I have given you all the ideas needed to design this algorithm to find a median.
Let us just put all of this together. Let us just write down each step. Put all these steps together stare at it. And then, see if you can analyze it and see what the running time. So, I will not be giving you code. But, I will write down each step carefully. So, here goes.
(Refer Slide Time: 28:10)
 
I want to find an element of rank r in an array A. So, let us call this find rank, find element. Let us say, r, A this finds an element of rank r in array A. Now, what do we do. Well the first thing is to somehow gets this approximate median. So, for that well, the first step is this. So, partition array, partition A into n by 5 groups of size 5. Find the median of each group. Now, I want to recurse. I want to find the median of these medians. There are n by 5 groups. So, there are n by 5 elements.
So, each group and put them in an array. Put them in array B say. Array B has size n by 5. Now, I want to find element essentially, I want to find the median in this array So, find element of rank n by 10 in the array B. This is again recursion. So, this portion, find the approximate median. This well this should return an element. So, call this x. So, this returns x. This element x is returned by this call. And this x is the approximate median that we want. Now, what do? Well, we use this is something we have done before. We use x partition array into two parts and then recurse on the right part, that is it.
(Refer Slide Time: 31:14)
 
So, partition A into A L and A R. So, elements in A L are less than x. Elements in A R are greater than x. That is how you get A L and A R. Now, let us see, so now, we are left with cases depending on what the rank of x is we have to either recurse on A L or A R. So, there are 3 cases. The first case is if, x luckily happens to be the element we are looking for. So, when does that happen? So, if size of A L is let us say, n by 2 minus 1. Then, return x, x is the median that we are looking for.
Now, if I am sorry, so size of A L should be r minus 1. We are looking for an element of rank r. So, we go back to here ((Refer Time: 32:40)). What we want to find is an element of rank r in a, not just the median. So, if the size of A L is r minus 1, then rank of x is nothing but r. So, x is the element of rank r and we return x. So, if size of A L is greater than r minus 1. Then, we know that this element of rank r sits in array A L. The element of rank r sits in array A L. So, we recurse on array A L. So, we say find the element.
So, we want still to find an element of rank r. And we now recurse on A L. An element of rank r in A L will also be an element of rank r in A. That is easy to see. And the last case is when size of A L smaller.
(Refer Slide Time: 34:02)
 
The last case is size of A L is smaller than r minus 1. Then, we need to find an element of rank. So, here is a let us quickly draw picture to find this out. So, here is x. This is A L. And I know that r is somewhere here. This is what I want. Now, this is r, this there are r elements here. There are size of A L elements here, one element here So, the rank of this element in this portion of the array is nothing but this distance. That is nothing but r minus size of A L minus 1. This entire distance is r. This distance is size of A L and x is just one element.
So, we want to find now an element to find element. The rank is r minus size of A L minus 1. And we want to do this in A R. This portion of the array, that portion of the array is A R. That is it. That ends the description of the algorithm. So, let us look at this algorithm again. We want to find an element of rank r in array A. Partition A into ((Refer Time: 36:01)) n 5 n by 5 groups of size 5. Find the median of each group. Now, find the median of these medians.
This returns an element x, which will be an approximate median. And we are now going to partition with respect to x. So, partition A into A L and A R with respect to x ((Refer Time: 36:27)). These are elements smaller than x and those larger than x. And now are going to recurse on one of these two parts. That depends on the rank of this element x. The rank of this element x is nothing but size of A L plus 1. So, if the rank of the element x is r exactly r. Then we return x as the answer.
If the rank of element is x is larger than r that means, the element we are looking for is on the left half of the array. So, we just find an element of rank r in the left half of the array. If this does not work, well the element that we are looking for lies on the right half of the array. And well this is what we do. If the rank is less than r minus 1, then we find an element of rank r minus A L minus 1 in the right half of the array. This is the entire procedure. How do we analyze this? We have to analyze this procedure.
So, we have seen reasons, why this procedure works. Why this we expect that this procedure should run in linear time. Our expectation at this procedure runs in linear time. And we will see why, this procedure actually works in linear time. So, to once so the once you have designed an algorithm. How do you analyze, the easiest way to do it is write down the algorithm. Write down perhaps code for the algorithm. Look at each step. And figure out how many times each step is executed. The step could be a recursive step.
(Refer Slide Time: 38:38)
 
So, here is let us say the, you have written an algorithm, for some problem. There are there are many steps. One of them could be recursive. So, this is an algorithm call it A. So, you recurse on a with smaller input. And then, there is a loop may be and there are steps inside the loop. This is your normal structure of a program. To analyze this, I need to you need to find out, how many times is each step executed. Those outside a loop are executed once. That is fine.
For the recursive call, you need to estimate the size of this input. So, for instance, if initially the input size were n. And this is let us say n by 10. This could be n by 10. And now, you have these things in a loop let us say and you have to find out how many times each of them is executed. Now, supposing there are you know 5 of them, 5 steps here. And each is executed n times.
Then the total time that you will take T n well T n by 10, that comes in the recursive step. And these, totally the total time is 5 times n. There are 5 steps each takes ((Refer Time: 40:14)). Each of them is executed n times. And that gives you 5 n. So, we are going to do something similar to our algorithm for the median. Look at these each of these steps. And we will see how much time each of them takes. So, T of n is the time taken on an array of size n.
(Refer Slide Time: 40:40)
 
So, we are going to write a recurrence for T of n. So, let us look at the first slide ((Refer Time: 40:45)). Partition a into n by 5 groups of size 5. How much time does this take nothing. This partition is easily done. Find the median of each group. How much time does this take? Well each of those 5 elements, each of those n by 5 groups I could even sort. That will take let us say 25 sort of comparisons for each group. I mean even if I do bubble sort.
And there are n by 5 such groups, so it is some constant times n. So, this step finding the median of each group takes constant times n for the entire array. So, let me write this down. So, this is the first, first step to the cost. C 1 times n. This is to find the median of these groups of each group. There are n by 5 groups. For each of them I spend some constant time. And that gives me a total of C 1 times n. What about this. What about this step. Well, I recurse on something of size n by 10. So, sorry I recurse on B.
This is the rank I want to find. I recurse on B and B had size n by 5. So, I recurse on a problem of size n by 5. So, that is T n by 5 plus. Now, we get here we partition ((Refer Time: 42:41)) this into two parts. So, how much time does this take. This takes n time, because each element is compared with x. So, this takes an additional n. This is to partition this array into two parts. Once we find this x. After, we do this partitioning, now we recurse well, if we are in this case we are done, we return.
If we are in this case or if we are in the next case, we recurse on a smaller part of the array. So, we recurse on either A L or we recurse on A R depending on where this what was the rank. What was r and what was rank of x. We either recurse on A L or we recurse on A R. Now, we know. So, this is the last step. So, that is what I need to write. So, this T of something this is the recurrent. So, what do I put here. Well, both A L and A R notice, the size of A L I know is at least n by 4. Size of A R I know is at least n by 4.
Or this implies that size of A L is less than equal to 3 n by 4. Why is this. This is because size of A R is at least n by 4. So, size of A L is utmost 3 n by 4. Similarly, size of A R is utmost 3 n by 4. So, whichever side, I recurse the size the maximum size it can be is 3 n by 4. So, I can put 3 n by 4 and I put less than equal to. What I put here the size, can only be less than or equal to 3 n by 4. So, the maximum size an array can have when I do this recursion is 3 n by 4. So, the recurrence equation I get is this.
T n is less than equal to C 1 n. This is to find the median of those 5 of these groups n by 5 groups. Here is a recurrent recursion to find the exact median. This is the time for portioning. And this is the last recursion. We recurse either on A L or A R. And a maximum size of A L and A R is 3 n by 4. So, this is the recurrent. So, T n is less than equal to let us say, C 1 plus 1 n plus T of n by 5 plus T of 3 n by 4. So, what is the solution to this recurrence? That is what we want to see.
So, here is something I want to sort of share with you. So, look at the recurrence. Look at where we recurse. One is a problem of size n by 5. This is a problem of size n by 5. The other is a problem of size 3 n by 4. The sum these two is less than n. n by 5 plus 3 n by 4 is strictly less than n. In such cases usually, you will end up with something which is linear. So, let us try and prove this. So, we know and in such cases instead of writing down this recurrence the way we have been doing there is an easier way to do this. So, I will assume that T n is less than equal to C n. I want to prove this. So, I want to prove this. So, let us just substitute this into the previous equation and see what we get. So, I get T n, so let me do this on a new sheet.
(Refer Slide Time: 46:59)
 
I know this T n is less than equal to C 1 plus 1 n plus T n by 5 plus T 3 n by 4. And I want T n is less than equal to C n. I want to find a right constant here C. So, let us just plug this into the original thing. I get T n is less than equal to C 1 plus 1 n plus well C n by 5 by plugging in C n plus 3 by 4 times C n. Now, if this is less than or equal to C n. If I can find C, so that this is less than equal to C n then we are done. If this is true, we are done.
Because, we can actually, prove this by induction. Even, if you do not understand why we are done at this point. We would like to find a C which satisfies this. So, let us find that. So, we want C 1 plus 1. n plus this plus this less than equal to C n. So, n cancels from these. So, I want C 1 plus 1 less than equal to C minus C by 5 minus 3 C by 4. We want C plus C 1 plus 1 utmost C minus C by 5 minus 3 C by 4. So, this will imply the previous statement.
And just simplifying this, this is nothing but C by 20. C times on by 20. So, as long as C is at least 20 times C 1 plus 1 we are done. So, if C is greater than equal to 20 times C 1 plus 1. We see that, this quantity is less than equal to C n. So, we can choose C to be 20 C 1 plus 1.
(Refer Slide Time: 49:32)
 
So now, it looks like T n is less than equal to 20 times C 1 plus 1 times n. So, this is a constant. This is the constant C. This now, we can prove by induction on n, that T n is less than or equal to this C times n. We can prove now by induction of n. For n equals 2 it is true, because you make just one comparison. And for the inductive step for the inductive step we just observe that, T n by the recurrence is utmost C 1 plus 1 n plus T n by 5 plus 3 n by 4. And now I can use induction.
So, ((Refer Time: 50:23)) T n by 5 us utmost C n by 5. And this is utmost this much. And the way we have chosen C, this sum is utmost C n. So, the proof now follows by induction. In fact, initially we guessed that T n is utmost C times n. And the plan was to use induction. Somehow, use induction and prove this. And you just follow your nose in the induction substitute C. And find out ((Refer Time: 50:54)) what is the C that helps you finish the inductive ((Refer Time: 50:58)).
So, this is the other way of solving recurrence relations. If you can guess the answer somehow, then you know that you want to prove this by induction. So, try proving it by induction and come up with the right constants. And then you can once you have found it, once you plug it in and then find out what and figure out what constants to use then you can write the whole thing by induction. So, this proves that the time taken by our algorithm is in fact, linear in n.
So, you can find the median in time, which is linear in the size of the array. If you use this, use this algorithm with quick sort then the worst case time for quick sort would be n log n. But that is if you pick the pivot as a median each time. The normal way you do it of course, it turns out to be order n square.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 8
Divide and Conquer - III
Surfing Lower Bounds

We will again look at sorting. Many of these algorithms merge sort, quick sort and 
some of the other sorting algorithms that we have studied. All of them seem to take n log n time. I think it is a good question to ask, if we can do faster. And we this n log n, which most of the sorting algorithms seem to take. And this is the question, we would like to address in this part of the course.
In fact, we will show that for a large family you have sorting algorithms. N log n is the best that they can do. If we sort of restrict, the kinds of things that algorithms can do on inputs. And n log n is the best that we can do. Now, we look at merge sort. Let us take merge sort, but whatever I say is actually true even for, let us say quick sort. For merge sort the what really happens.
You split the array into two parts. And then, recursively sort these things. And then, you merge these two sorted arrays. That is the crucial step. What happens when you merge two sorted sub arrays, or when you merge two sorted lists. Well, you compare two elements and then, move them out. Compare the first element in the list. That is the smallest and we put it as a first element. Then, move your pointers and so on, and so forth.
So, the most important step there is comparing two elements and then, some reordering. So, the only operation that, you actually do on these elements in the sorted, in the array is comparing two elements and moving them out. So, this property actually holds even for quick sort. So, always quick sort term, you pick a pivot, by your favorite procedure. And now you compare every element with the pivot.
Once you compare every element with the pivot and then, you move these elements around. So, the basic operation on two elements of the array is compare these two. Figure out which one is larger, which one is smaller. So, again the usual sort of operation is compared, is looking at two elements in the array, comparing them. And then, based on the result, I mean if one is smaller than the other.
Or first one is bigger than the other, then you sort of either switch them around or change the order. So, this is the crucial operation, that you do on the array element. You do not add them, subtract them, divide them. So, the operations that, we use in at least these popular sorting algorithms, is compare two elements. May be swap them or move them around. So, if these are the only things, that you can do with array elements.
Then, you need n log n comparisons to sort an array. So, this is our goal. That is what we are going to show. Then, how do we go about doing this. Well, even if you recall, we showed that, find the minimum you needed n minus 1 comparison. And even there, it was not an obvious, sort of the solution was not absolutely obvious. You had to do little bit of work. And for this actually we need to do a little bit more of work, but it is not difficult. So, the first thing is to notice that these algorithms can be written as flowcharts, or comparison trees. What is a comparison tree? Well, basic block is comparison between two elements.
(Refer Slide Time: 04:52)
  .
So, I compare two elements, a i and a j upon array, for a branch this way, if a i less than a j. I branch this way, if a j is greater than a i. We will assume that, array elements are distinct, which means I will never have two of them equal. So, compare two elements, branch this way if a i is less than a j, branch the other way if a i is greater than a j. Now, using these as building blocks, I can build the large flow chart. So, somewhere here, maybe I can compare two elements, two other elements a k and a l.
Then again branch and so on. And at the end, I will have these leaves where I will output. So, the output for us is the order, is the sorted order. So, we will look at flow charts, which look like this. The input to a flow chart is just an array a, the input is an array a. And you go through these flow charts and at these leaves, will output the sorted order. This is the model that we are looking at. Let us do a small example.
(Refer Slide Time: 06.35)
 
So, for instance if I have two elements, then let us say a 1 and a 2, I compare a 1 and a 2. If it is less than the order output is a 1, a 2. This is in sorted order. If a 1 is larger, the order I output is a 2, a 1. So, here is a small flow chart, that sorts 2 numbers. So, it sorts two elements an array, basically works on arrays of size 2 and sorts these arrays. So, let us say 3, what if the array size is 3? So, let us see what to do.
(Refer Slide Time: 07:20)
 
So, you compare, let us say a 1 and a 2. Now, if a 1 is less let us say I compare a 1 with a 3, say a 1 is less. Now, I know at this point, that a 1 is the smallest. So, I would like to compare a 2 and a 3. So, here a 2 is greater than a 3, here a 2 is less than a 3. The sorted order here is a 1, a 2, a 3. Here it is a 1, a 3, a 2 and so on. So, you can sort of fill this up. So, you can write down a flow chart like this, with these are the leaves where you get the answer, which is the array in sorted order. Now, merge sort can also be depicted like this. So, let us see, why that is do.
(Refer Slide Time: 08:34)
 
So, let us look at merge sort or when four elements. So, I have four elements a 1, a 2, a 3, a 4. And I want to sort of run merge sort on this. Well, we divide this into two, recurs on this part, then recurs on this part and then, there is a merge. When we recurs on this part, the first comparison we do is between a 1 and a 2. So, this is the first comparison we do; and this could be a 1 is less than a 2.
This is a 1 could be greater than a 2, we do not know what this is. Here the sorted order which is returned here is a 1, a 2. On this side it is a 2, a 1. Whichever branch we choose, now we have to recurs on the right hand side. So, this is the next comparison we do, a 3 and a 4; and well these give again two sort of outputs. Now, at this step, we are up to the merge. The lower things return, these two sorted orders.
And now we are down to the merge, the top level of recursion. So, let us take this portion. What are the two elements, that are compared? Here I know that a 1 is greater than a 2. So, the first one returns, this side returns a 2 a 1. And here, I know a 3 is less than a 4. So, the second list returned is a 3 a 4. These are the two lists. So, the two elements compared here, are a 2 and a 3.
The smallest elements from the two lists are compared now. So, this is what is compared here. So, let us go down one more step and see how this looks like. So, here a 2 is less or a 3 is less. Supposing, we followed this branch, what is the next clue, which are compared for merge sort. Well, a 3 is less than a 2. So, a 3 is moved to the new list and our pointers are at a 2 and a 4. So, the next two things, which are compared are a 2 and a 4 and so on.
I can draw this tree out and at the leaves of this tree, are the solution. I mean at each leaf, I will tell you what is the output, what is the sorted order which it output. In fact, if you look at this tree, this tree is a binary tree. At each internal load, we have a comparison. So, we compare two elements, that you branch out into two. And for each branch again there will an internal node, you compare two elements branch into two, that is it.
So, if I just look at it as, if I forget that, there these comparisons etcetera, etcetera. The whole structure looks like a tree. In fact, the binary tree and there are leaves, and at these leaves we have the output. Output mean in the sorted order. So, when I sort of follow this tree down to a leaf, I get the answer. This leaf whatever is the answer is the output. The one thing to notice is that, this tree is different for different sizes of the input.
For four elements, I can tell you what the merge sort tree looks like. The comparison tree for merge sort, it has some structure. And in fact, just the way I have written part for the tree here, you can actually write down the entire tree. It will take some time and space, but if you have the patience, you could try this out. For five elements again, there is a tree. For six elements, there is a different tree and so on and so forth.
If I take, if I fix the number of elements to n, then for merge sort I can write down the comparison tree, which does the comparisons in the same order as merge sort. So, if I follow the tree down for a particular input, the comparisons made will be the same. The order of comparisons will be the same, as that made in merge sort. And the answer will be as given by merge sort. So, how much time, what is the notion of time required on this tree.
Well, the time taken for, let us say given an input, the time taken is just the number of comparisons you make till you hit a leaf, in this tree. So, given an input you make comparisons, then you branch either on the left or right. Then, you make some other comparisons branch etcetera, etcetera, finally you hit a leaf. The number of comparisons is just a path in this tree from the root to a leaf. So, the time taken is the number of comparisons, which is the length of the path followed from the root to a leaf.
So then, the worst case time, what is the worst case time by merge sort, would be just the height of the tree. The worst case time for merge sort will just be the height of this tree; which is the longest root leaf path in this tree. I can show that this is long enough. Then, the time taken for merge sort will also be that much. And that is the goal; and we have most of the ingredients in place. We just have to make certain observations.
Here are two critical observations. So, we have an algorithm as a flowchart. We have an algorithm which sorts n numbers. And it is depicted as a flow chart where or a comparison tree, where at each node you compare to a level that you branch, depending on which one is greater. At leaves we output sorted out, this is the thing. The question we ask is, how, what is the height of such a tree, how tall is such a tree.
(Refer Slide Time: 15:55)
 
Here are two sort of crucial observations, if this flow chart does sort. Then, every input order must leads to a leaf. Different input orders lead to different leaves. So, why is this true? Well, firstly we will assume that inputs, in our input every element is distinct. We do not have two copies of the same element. There are n factorial orders, that are possible as input. And for each order you make certain comparisons and you trace the path to a leaf. And at the leaf, we have to give the right order, which is a permutation of the input.
And for every input order, you must end up with a different leaf, because the permutations are different. So, for instance, if I look at 1 2 3 this is already sorted. If I look at 1 3 2, the sorted order is 1 2 3. So, I have to interchange the last two elements. This order must appear at a different leaf, which means all of these n factorial permutations; must appear in one of these leaves. And different permutations must appear at different leaves.
So, because of the order of the input is different and must land up in different leaf. I land up in the same leaf, I will give the same answer for both orders, which is incorrect. So, this means, there must be n factorial leaves in this binary tree. And each of these different orders, must land up in the leaf. And different orders must land up in different leaves, which means there must be n factorial leaves in this binary tree. So, intuitively you can see, why the height must be large.
So, if I have a binary tree with large number of leaves, the height better be large. So, we will invoke theorems, which you must have seen in discrete structures; and we will finish the proof. So, if I look at this flow chart or this decision tree.
(Refer Slide Time: 19:06)
 
Then, I have a binary tree and I know that number of leaves is at least n factorial. Using these, I want to conclude that the height is at least something. So, if I can put something here, the height at least something. Then, I know that the time taken by the algorithm must also be at least this much. So, why can I say this is the height is most ((Refer Time: 19:42)). Well, supposing I have a binary tree of height h, what is the maximum number of leaves, that this can have.
This is something which you have done in discrete structures, so let us state this. So, binary tree of height h has at most, well 2 to the h leaves. The number of leaves can be at most 2 to the h. Well, let us I hope you can sort of prove this. Let us give a proof anyway. So, proof is by induction on h, so the base case I will let you do this. H equals 1, I will let you do this, so the inductive step. So, how do we ((Refer Time: 20:46)) the two steps.
(Refer Slide Time: 20:59)
 
Well, let us look at a tree, that T be a binary tree of height, let us say K plus 1. So, we will assume that, the statement is true for all binary trees of height k or smaller. And we would like to prove, the statement for a height K plus 1. So, the inductive hypothesis is that, for every tree of height k or smaller, the statement holds. And we would like to prove this for tree of height K plus 1. So, if I look at T, T has a root and then, there are two sub trees.
This is what it looks like T 1 and T 2. Now, well I remove the root and now I am going to apply induction to T 1 and T 2. Apply the inductive hypothesis to T 1 and T 2. Well, this had height K plus 1, which means both T 1 and T 2 have height k or less. One of them has height K, the other could be k or it could be less. So, the height of T 1 is less than equal to k. Similarly, height of T 2 is less than equal to k. That is the reason, we can apply the inductive hypothesis. Both of them have height less.
Now, we can apply the inductive hypothesis. So, the number of leaves in T 1, is less than equal to 2 to the k. Similarly, number of leaves in T 2 is less than equal to 2 to the k. And now you can see, basically the number of leaves in T, is number of leaves in T 1 plus the number of leaves in T 2; which is this plus this which is at most 2 to the k plus 1. And that is the end of the proof.
(Refer Slide Time: 23:28)
 
So, let us just write this, so number of leaves T equals number of leaves T 1 plus number of leaves T 2. That is less than 2 to the k plus 2 to the k 2 to the k plus 1. That finishes the inductive proof, that on a tree of height h. When I have a binary tree of height h, the number of leaves is at most 2 to the h.
(Refer Slide Time: 24:02)
 
Number of leaves is less than equal to 2 to the h. Well taking logs, I get log the number of leaves is the lower bound on the height. Well we know remember that in our binary tree, the number of leaves was n factorial. So, the height this means, the height is at least log base 2 of n factorial. Well, this is the height of any of the tree, which is related to any this flowcharts, which sort flowcharts. For sorting we looked at these flowcharts.
So, we said that number of leaves is n factorial. And we have just seen that, the height must be at least the log of n factorial. So, what is this, we are very close. Log of n factorial is at least log base 2 n by 2 to the n by 2, why is this true? Well, this is true, because n factorial is n into n minus 1, somewhere n by 2. And then, all the way up to 1, now if you look at the top n by 2 terms, all of them are greater than n by 2.
There are n by 2 terms and all of them are at least n by 2, all of whom are greater than equal to n by 2. So, n factorial is at least n by 2 to the n by 2. Well, that is why this is true and this is nothing but n by 2 log n by 2. This is omega n log n, so the time taken is at least n log n. This is what we have proved some constant times n log n. So, this shows that merge sort, for instance we know it takes maximum time n log n.
But, there are inputs on which it takes, I mean log n, it must take. So, similarly quick sort and any other algorithm in this model, which is if the only thing which the algorithm can do, is look at two element, compare them. And perhaps move them around, swap them for instance or move them around. If this is the only thing, that is possible. Then, the time taken by any such algorithm for sorting must be n log n. There are sorting algorithms which do not fit into this model. We will not cover those, hopefully we have covered some of them in your data structures course.
These are bucket sort radix sort, well these algorithms differ in that. They you can look at the least significant digit. You can divide, you can take floors of these numbers. If they are integers, you can take floors and then, do funny things with them. The difference between these algorithms and let us say something like merge sort. Is merge sort, when you look at 2 numbers the only operation, that you can do is compare them.
On any element, if I take an element of the array. Only thing I am allowed to do on this element is compare this element with some other element in the array. I cannot sort of divide it by 10, 20, 100 whatever. I cannot look at it is least significant bit etcetera, etcetera. That is as far as sorting is concerned. We will continue with our discussion of algorithm design techniques, by looking at some other problems, which are fairly interesting.
And where this divide and conquer paradigm sort of works, and gives you better results. In some cases perhaps you the algorithm, that we desire.
(Refer Slide Time: 28:58)
 
The first one is this, so you are you are given a 2 k. So, this is the input by 2 k square. And what you want as output is a tiling with, I will tell you what all this means, with this. What do I mean by tiling, a tiling essentially means, I must fill up all these squares. And try and fill up as many squares as possible with pieces, which look like this. So, each piece is, let us say a kind of a domino, which sort of looks is of this shape.
And I want to try and fill up, as many squares as possible in this. So, well you can see immediately, I may not be able to fill up all square. Even for k equals 1, if I have a 2 by 2 square, I have a 2 by 2 square. Well I can put one of them. So, I can put something which fills up these three squares and this will be an empty square. So, here is a tiling of a 2 by 2 square with a domino of this form, which does everything except one square. So, what about 2 to the k cross 2 to the k.
Well, the number of squares, we look at the number of squares is 2 to the 2 k. These are the number of squares, the number of squares and this is 1 mod 3. I will let you figure this one out. The number of squares is always 1 mod 3. Well it is 2 here and you can see, that it is always two. If you want you can prove it by induction on k, but even otherwise you can sort of check that, the number of squares will be always 1 mod 3. Maybe we will just look at a equals 2.
So, then it is a 4 by 4, how does this look like. So, this is a 4 by 4 thing and you have well 16 squares; and that is 1 mod 3. When I divide this by 3 I get 1, so 3 times 5 is 15 and 1, one gets left out. So, that is true whatever k I choose, it is always 3 times something plus 1. So, that is what I mean by 1 modulo 3. When I divide by 3 I get 1, the remainder is 1.
So, can I sort of now put these dominos. So that, all except 1 square is covered. So, I have this 2 by 2 k 2 to the k by 2 to the k square. So, I will square with side length to the k. I have these dominos, which each of them has 3 squares put together. Can I arrange these on these squares, so that all except 1 square is covered, this is the question; the answer is yes. And we would like an algorithm to do this and well, we would like to do it using divide and conquer.
That is the goal, let us do this 4 by 4. And then, we will see what is the scene. Can I do a 4 by 4 with, so here is a 4 by 4 square, can I do this. Well, since I am going to do something like divide and conquer. I would like to divide this into two parts, or more. In this case four parts, tile each of them and put them together.
(Refer Slide Time: 33:24)
 
Well, let us take a bigger one and see how this is done. I know how to tile this, so that one is left, I know how to tile this, so that one is left and so on. Now, how do I tile the whole thing, so that one is left. Well supposing the one left here is this. One left here is this and one left here is this. Now, I am able to tile all four of them. So, I tile this, so that this is my first domino, I use my first domino there, I put it around like this.
This is my second domino, I put it around like this. Here is 3, it does not matter how I put my fourth as of now. We will we will change this later, supposing the fourth is like this. Well now I can put the fifth domino here, here 5 5 5. So, I managed to sort of tile at least 4 by 4 using 5 dominos. I have just one left which is what I want. You would like to repeat this for 2 k by 2 k. So, how does that look.
So, here we again would like to do a divide and conquer, we would like to divide this. I would like to first tile this, tile these 4 and then put them together. Well, by looking at this, this example I can do it if provided this square left off here is this. The square left off in this, is this. The square left off here is this. Here I really do not care. If I can tile the entire region here, except this corner and so on. For these 3, then I am able to tile the entire region on top leaving one squares.
That square left will be in this big sort of region. This is 2 to the k minus 1, 2 to the k minus 1, 2 to the k minus 1 and so on. So, this whole thing is 2 to the k. So, if I can do this, then I can fit in this entire, this extra sort of domino here. There will be one square left over in this region, good. So, again notice a crucial thing, to be able to carry this inductive process, to completion. I should be able to have the extra square. I mean the square left over as a corner square.
When I tile a region which is like this, the left over square must be a corner square. So, this is a square left over or left un tiled. I have tiled the entire region, except this small square. If I can do this, then clearly I can do the big thing. So, I started out my initial objective was to tile to 2 to the k by 2 to the k. That is what I started out with, I want to apply this divide and conquer kind of thing. Well, actually the process is really induction always.
If I can do it for a small square, can I push it up to a bigger square, that is the goal. Well, we want to do divide and conquer. So, we saw that, we break it up into these 4 smaller squares. Now, I want to tile these smaller squares and put them up, to a tiling for the bigger square. Well, each of these smaller squares, that is one square, one small square which is not tiled. One piece which is not tiled. Now, this happens to be the corner, then I can push the induction upwards.
But, for the induction to work even the final, so let us look at this again. For the induction to work in this big square, I want the one square which is not tiled to be a corner. Well, it cannot be corner here, because everything else is tiled, this cannot be a corner, it cannot be a corner. Well, can it be this corner, the answer is yes. It is like I leave this square un tiled, so leave this...
Now, when I look at these 4 small problems, they all look absolutely alike. I want to tile everything, except a small square in the corner. I want to tile all of it, except this square in the corner. Except this in the corner and except this in the corner. And I put them together like this. This tile is the entire big square, except for this one thing in the corner. And well this is the algorithm, break it up into 2.
Tile these 4 recursively and put them together like this, and in the middle I just put one domino. One domino fill up these three squares, so I have tile everything except this. So, this is a well divide and conquer sort of strategy to tile squares. So, this problem looks a bit different from the other problems. And that is why I picked this, because the strategy can be used not just for computer algorithms or puzzles, or whatever you have. And the other sort of lesson, that we learn through this example. Is that you start with some problem you want to solve.
We have seen this before, along the way you would like to apply, you want to find solutions to smaller sub problems. You want to put these together to get a solution to a larger problem. And while doing this, you may want to solve something slightly more general. And then, you see this general problem can be solved. So, you try and solve the general problem now, again using the same technique.
The smaller sub problems, you solve the general problem. And now you see, if you can put them together, to get a solution to the general problem for the big input. And often this sort of gives you the solution that want. We saw this in two occasions, one was this tiling problem, the other was a I hope you can remember. It was when we looked at this median. We wanted to find the median, but we ended up looking at the problem of finding an element of given an array and a positive integer r.
So, we found the element of rank r, that is what we did. So, let us look at one more problem, yet another problem rather, which uses divide and conquer. So, this is a familiar problem of multiplying to two integers. So, let us look at this.
(Refer Slide Time: 40:58)
 
So, I have 2 n bit integers, x and y, this is x, this is y, these are n bits. Each is n bit long, I want to find the product of x and y. The normal way is well, we formed the product of each of these bits with LSB, then with the next one and so on, and so forth. So, there are n square multiplications. Well, in this case actually it is not, you can just do are and since they are bits. But, you can think of these as instead of bits, you can think of them as digits, it really does not matter.
So, each of them could be n digits long and we multiply these digits. So, there are n square multiplications, this is the high school multiplication algorithm, that we know. Can we do faster, that is the question. Let us try applying divide and conquer. I have divide this into two parts. This is x left x right, y left y right. And now I want to compute the product of these two, well what is the product first of all. X is nothing but 2 to n by 2 times x l plus x r y is 2 to the n by 2 y l plus.
So, this means x y is 2 to n x l y l plus 2 to the n by 2 x l y r. X l y r plus y l x r plus x r times y r, this is the product x and y. So, we could recursively sort of find these products, these four products and then, compute this. This is just shifting by n bits, we are just shifting by n by 2 bits. So, that is an order n operation, so if T n is the time taken, this what is the recurrence.
There are 1, 2, 3, 4, 4 problems, so 4 times T n by 2 plus order n. This is some constant times n, which is for shifting and adding. Adding two n bit numbers, it takes alternately. So, what is the solution to this recurrence? Well, I will let you do this and the solution to this recurrence. You do a solution to this recurrence, it is order n square. So, we are back to where we started. Compare these two, well we have done divide and conquer, but we do not seem to have conquered anything.
We are back to spending order n square times, even with this divide and conquer approach, but well all is not lost. I will first indicate this algorithm. And then, we will analyze, and see how we can decrease the number of multiplications. So, well the trick is this, let us see.
(Refer Slide Time: 45:02)
 
So, the terms that we want to compute are x L y L and then, x L y R plus y L x R and x R y R. These are the three terms, that we would like to compute. The way we did it, was each of them separately that gave over problems of size n by 2. So, here is a smart way of doing this. Well, these two I compute as they are, the third thing I compute is not quite these two, it is the following. I computer x L plus x R times y L plus y R, I add x L and x R, I add y L and y R and then, multiply them together.
What is this, this is x L y L plus x L y R plus x R y L plus x R y R, that is what this is. So, I do three multiplications and not four, this is the first multiplication x L y L. X R y R is my second multiplication and this is the third. This I do not do, as of now I have not touched this, this is the third multiplication I do. Well, if you look at this x L y L is present, x r y r is present; and if I remove these two what is left is this. X L y R plus x R y L is exactly this.
Well, I have just written x R y L, but it is same as y L x R good. So, how do I compute this, well from this product I just subtract these two to get this, so call this Z. Now, I just do Z minus x L y L minus x R y R I get this, good what have I done. Well I have a few more additions, I have 1, 2, 3, 4 more additions. The number of multiplications has gone down by 1. Multiplying 2, n by 2 bit numbers I do 1, 2 and 3. There are now only 3 multiplications that I do.
So, what is the recurrence now, well T n is 3 T n by 2 plus order n. Earlier remember it was 4 T n by 2 plus order n. Now, it is 3 T n by 2 plus order n and it is not surprising, that the solution to this recurrence is smaller. In fact, we will show the solution to this recurrence, is order n to the log 3 base 2. Roughly it grows as n to the 1.59, which is certainly less than n square, which is what we started out with. So, this the number of multiplications one makes is significantly smaller.
(Refer Slide Time: 48:23)
 
Is 3 T n by 2 plus some let us this take n, it does not matter, so T n is this. We will do the usual thing which is 3, so 3 T n by 2 square plus n by 2 plus n. This is 3 square T n by 2 square plus 3 times n by 2 plus n. Let us do it once more, to see what happens, so this is 3 square T well n by 2 cube. I am expanding this out, so there should be a 3 here, 3 times plus n by 2 square plus 3 times n by 2 plus n.
So, this is now 3 cube T n by 2 cube plus, well 3 by 2 whole square times n plus 3 by 2 n plus n. Now, one can guess the general term, in this when I go down i steps.
(Refer Slide Time: 50:11)
 
So, T n will be 3 to the i T of n by 2 to the i plus 3 by 2 to the i minus 1 n plus 3 by 2 i minus 2 n so on, to n, good. So, now we would like n by 2 to the i should be 2 or say 1, for ease of computation. If this is one, if I have one bit and the time is 1, so the time is just this becomes T of 1 is 1. So, if this is true and T of n is 3 to the i plus, well n times 1 plus 3 half and so on, up to 3 half to the i minus 1. Well, let us do this, so this is 3 to the i plus n times, well what is this? This is just a geometric series.
So, it is a roughly some constant times 3 half to the i. You can do it more a bit better by doing the, writing the exact formula, which is 3 half to the i minus 1 by 3 half minus 1. So, it is at most, let me put it at most this is perhaps 3 half minus 1 which is half, so C could be 2, so C is most probably 2. In any case, this is bounded by something like this, 3 to the i plus. So, what is i? Well, 2 to the i is n, so i is nothing but log base 2 n.
So, this is 3 to the log base 2 n plus n times constant times, well 3 half to the i, which is 3 half to the log based 2 n. Now, 2 to the log base 2 of n is nothing but n, these two things cancel. So, I have 3 to the log base 2 of n times constant and 3 to the log base 2 of n here. So, this is nothing but some C prime times 3 to the log base 2 of n, this is the I time taken.
(Refer Slide Time: 53:26)
 
So, what is this, well so the time, the total number of multiplications is 3 to the log base 2 of n. This is 3 to the log base 3 of n divided by log base 3 of 2, I could take this up. So, this is nothing but 3 to the log base 2 of 3 times, use just some general manipulations. So, that is 3 to the log base 3 of n, I take this inside, n to the log base 2 of 3. This is nothing but n to the log 2 of 3 as promised. So, this is roughly n to the 1.59. So, the time number of multiplications, we got it down from n square.
So, the initial thing to roughly n to the 1.59. Of course, we increase the number of additions, but if multiplications are more expensive than additions. Then, we have saved on time. This trick of using three multiplications instead of 2 is actually a trick. It is something you have to really need to come up with somehow. There is no real sort of way in which you can do it. There is no method and that is the beauty of algorithm design.
That somehow not every problem can be sort of tackled using everything that, you could come up. Often you come up with, all the time you come up problems which require you to design new methods, derive new methods, find new methods. Rack your brain and sort of come up with good algorithms. And when you do come up with these new algorithms, you feel again, the kick that you derive, the pleasure that you derive is something else.
This trick by the way or this multiplication is due to Euler. And he used this for the multiplying complex numbers. We have just used this trick was then used by other people, who saw that. It can be applicable in multiplying 2 n bit numbers.
Thanks.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 8
Divide and Conquer - III
Surfing Lower Bounds

We will again look at sorting. Many of these algorithms merge sort, quick sort and 
some of the other sorting algorithms that we have studied. All of them seem to take n log n time. I think it is a good question to ask, if we can do faster. And we this n log n, which most of the sorting algorithms seem to take. And this is the question, we would like to address in this part of the course.
In fact, we will show that for a large family you have sorting algorithms. N log n is the best that they can do. If we sort of restrict, the kinds of things that algorithms can do on inputs. And n log n is the best that we can do. Now, we look at merge sort. Let us take merge sort, but whatever I say is actually true even for, let us say quick sort. For merge sort the what really happens.
You split the array into two parts. And then, recursively sort these things. And then, you merge these two sorted arrays. That is the crucial step. What happens when you merge two sorted sub arrays, or when you merge two sorted lists. Well, you compare two elements and then, move them out. Compare the first element in the list. That is the smallest and we put it as a first element. Then, move your pointers and so on, and so forth.
So, the most important step there is comparing two elements and then, some reordering. So, the only operation that, you actually do on these elements in the sorted, in the array is comparing two elements and moving them out. So, this property actually holds even for quick sort. So, always quick sort term, you pick a pivot, by your favorite procedure. And now you compare every element with the pivot.
Once you compare every element with the pivot and then, you move these elements around. So, the basic operation on two elements of the array is compare these two. Figure out which one is larger, which one is smaller. So, again the usual sort of operation is compared, is looking at two elements in the array, comparing them. And then, based on the result, I mean if one is smaller than the other.
Or first one is bigger than the other, then you sort of either switch them around or change the order. So, this is the crucial operation, that you do on the array element. You do not add them, subtract them, divide them. So, the operations that, we use in at least these popular sorting algorithms, is compare two elements. May be swap them or move them around. So, if these are the only things, that you can do with array elements.
Then, you need n log n comparisons to sort an array. So, this is our goal. That is what we are going to show. Then, how do we go about doing this. Well, even if you recall, we showed that, find the minimum you needed n minus 1 comparison. And even there, it was not an obvious, sort of the solution was not absolutely obvious. You had to do little bit of work. And for this actually we need to do a little bit more of work, but it is not difficult. So, the first thing is to notice that these algorithms can be written as flowcharts, or comparison trees. What is a comparison tree? Well, basic block is comparison between two elements.
(Refer Slide Time: 04:52)
  .
So, I compare two elements, a i and a j upon array, for a branch this way, if a i less than a j. I branch this way, if a j is greater than a i. We will assume that, array elements are distinct, which means I will never have two of them equal. So, compare two elements, branch this way if a i is less than a j, branch the other way if a i is greater than a j. Now, using these as building blocks, I can build the large flow chart. So, somewhere here, maybe I can compare two elements, two other elements a k and a l.
Then again branch and so on. And at the end, I will have these leaves where I will output. So, the output for us is the order, is the sorted order. So, we will look at flow charts, which look like this. The input to a flow chart is just an array a, the input is an array a. And you go through these flow charts and at these leaves, will output the sorted order. This is the model that we are looking at. Let us do a small example.
(Refer Slide Time: 06.35)
 
So, for instance if I have two elements, then let us say a 1 and a 2, I compare a 1 and a 2. If it is less than the order output is a 1, a 2. This is in sorted order. If a 1 is larger, the order I output is a 2, a 1. So, here is a small flow chart, that sorts 2 numbers. So, it sorts two elements an array, basically works on arrays of size 2 and sorts these arrays. So, let us say 3, what if the array size is 3? So, let us see what to do.
(Refer Slide Time: 07:20)
 
So, you compare, let us say a 1 and a 2. Now, if a 1 is less let us say I compare a 1 with a 3, say a 1 is less. Now, I know at this point, that a 1 is the smallest. So, I would like to compare a 2 and a 3. So, here a 2 is greater than a 3, here a 2 is less than a 3. The sorted order here is a 1, a 2, a 3. Here it is a 1, a 3, a 2 and so on. So, you can sort of fill this up. So, you can write down a flow chart like this, with these are the leaves where you get the answer, which is the array in sorted order. Now, merge sort can also be depicted like this. So, let us see, why that is do.
(Refer Slide Time: 08:34)
 
So, let us look at merge sort or when four elements. So, I have four elements a 1, a 2, a 3, a 4. And I want to sort of run merge sort on this. Well, we divide this into two, recurs on this part, then recurs on this part and then, there is a merge. When we recurs on this part, the first comparison we do is between a 1 and a 2. So, this is the first comparison we do; and this could be a 1 is less than a 2.
This is a 1 could be greater than a 2, we do not know what this is. Here the sorted order which is returned here is a 1, a 2. On this side it is a 2, a 1. Whichever branch we choose, now we have to recurs on the right hand side. So, this is the next comparison we do, a 3 and a 4; and well these give again two sort of outputs. Now, at this step, we are up to the merge. The lower things return, these two sorted orders.
And now we are down to the merge, the top level of recursion. So, let us take this portion. What are the two elements, that are compared? Here I know that a 1 is greater than a 2. So, the first one returns, this side returns a 2 a 1. And here, I know a 3 is less than a 4. So, the second list returned is a 3 a 4. These are the two lists. So, the two elements compared here, are a 2 and a 3.
The smallest elements from the two lists are compared now. So, this is what is compared here. So, let us go down one more step and see how this looks like. So, here a 2 is less or a 3 is less. Supposing, we followed this branch, what is the next clue, which are compared for merge sort. Well, a 3 is less than a 2. So, a 3 is moved to the new list and our pointers are at a 2 and a 4. So, the next two things, which are compared are a 2 and a 4 and so on.
I can draw this tree out and at the leaves of this tree, are the solution. I mean at each leaf, I will tell you what is the output, what is the sorted order which it output. In fact, if you look at this tree, this tree is a binary tree. At each internal load, we have a comparison. So, we compare two elements, that you branch out into two. And for each branch again there will an internal node, you compare two elements branch into two, that is it.
So, if I just look at it as, if I forget that, there these comparisons etcetera, etcetera. The whole structure looks like a tree. In fact, the binary tree and there are leaves, and at these leaves we have the output. Output mean in the sorted order. So, when I sort of follow this tree down to a leaf, I get the answer. This leaf whatever is the answer is the output. The one thing to notice is that, this tree is different for different sizes of the input.
For four elements, I can tell you what the merge sort tree looks like. The comparison tree for merge sort, it has some structure. And in fact, just the way I have written part for the tree here, you can actually write down the entire tree. It will take some time and space, but if you have the patience, you could try this out. For five elements again, there is a tree. For six elements, there is a different tree and so on and so forth.
If I take, if I fix the number of elements to n, then for merge sort I can write down the comparison tree, which does the comparisons in the same order as merge sort. So, if I follow the tree down for a particular input, the comparisons made will be the same. The order of comparisons will be the same, as that made in merge sort. And the answer will be as given by merge sort. So, how much time, what is the notion of time required on this tree.
Well, the time taken for, let us say given an input, the time taken is just the number of comparisons you make till you hit a leaf, in this tree. So, given an input you make comparisons, then you branch either on the left or right. Then, you make some other comparisons branch etcetera, etcetera, finally you hit a leaf. The number of comparisons is just a path in this tree from the root to a leaf. So, the time taken is the number of comparisons, which is the length of the path followed from the root to a leaf.
So then, the worst case time, what is the worst case time by merge sort, would be just the height of the tree. The worst case time for merge sort will just be the height of this tree; which is the longest root leaf path in this tree. I can show that this is long enough. Then, the time taken for merge sort will also be that much. And that is the goal; and we have most of the ingredients in place. We just have to make certain observations.
Here are two critical observations. So, we have an algorithm as a flowchart. We have an algorithm which sorts n numbers. And it is depicted as a flow chart where or a comparison tree, where at each node you compare to a level that you branch, depending on which one is greater. At leaves we output sorted out, this is the thing. The question we ask is, how, what is the height of such a tree, how tall is such a tree.
(Refer Slide Time: 15:55)
 
Here are two sort of crucial observations, if this flow chart does sort. Then, every input order must leads to a leaf. Different input orders lead to different leaves. So, why is this true? Well, firstly we will assume that inputs, in our input every element is distinct. We do not have two copies of the same element. There are n factorial orders, that are possible as input. And for each order you make certain comparisons and you trace the path to a leaf. And at the leaf, we have to give the right order, which is a permutation of the input.
And for every input order, you must end up with a different leaf, because the permutations are different. So, for instance, if I look at 1 2 3 this is already sorted. If I look at 1 3 2, the sorted order is 1 2 3. So, I have to interchange the last two elements. This order must appear at a different leaf, which means all of these n factorial permutations; must appear in one of these leaves. And different permutations must appear at different leaves.
So, because of the order of the input is different and must land up in different leaf. I land up in the same leaf, I will give the same answer for both orders, which is incorrect. So, this means, there must be n factorial leaves in this binary tree. And each of these different orders, must land up in the leaf. And different orders must land up in different leaves, which means there must be n factorial leaves in this binary tree. So, intuitively you can see, why the height must be large.
So, if I have a binary tree with large number of leaves, the height better be large. So, we will invoke theorems, which you must have seen in discrete structures; and we will finish the proof. So, if I look at this flow chart or this decision tree.
(Refer Slide Time: 19:06)
 
Then, I have a binary tree and I know that number of leaves is at least n factorial. Using these, I want to conclude that the height is at least something. So, if I can put something here, the height at least something. Then, I know that the time taken by the algorithm must also be at least this much. So, why can I say this is the height is most ((Refer Time: 19:42)). Well, supposing I have a binary tree of height h, what is the maximum number of leaves, that this can have.
This is something which you have done in discrete structures, so let us state this. So, binary tree of height h has at most, well 2 to the h leaves. The number of leaves can be at most 2 to the h. Well, let us I hope you can sort of prove this. Let us give a proof anyway. So, proof is by induction on h, so the base case I will let you do this. H equals 1, I will let you do this, so the inductive step. So, how do we ((Refer Time: 20:46)) the two steps.
(Refer Slide Time: 20:59)
 
Well, let us look at a tree, that T be a binary tree of height, let us say K plus 1. So, we will assume that, the statement is true for all binary trees of height k or smaller. And we would like to prove, the statement for a height K plus 1. So, the inductive hypothesis is that, for every tree of height k or smaller, the statement holds. And we would like to prove this for tree of height K plus 1. So, if I look at T, T has a root and then, there are two sub trees.
This is what it looks like T 1 and T 2. Now, well I remove the root and now I am going to apply induction to T 1 and T 2. Apply the inductive hypothesis to T 1 and T 2. Well, this had height K plus 1, which means both T 1 and T 2 have height k or less. One of them has height K, the other could be k or it could be less. So, the height of T 1 is less than equal to k. Similarly, height of T 2 is less than equal to k. That is the reason, we can apply the inductive hypothesis. Both of them have height less.
Now, we can apply the inductive hypothesis. So, the number of leaves in T 1, is less than equal to 2 to the k. Similarly, number of leaves in T 2 is less than equal to 2 to the k. And now you can see, basically the number of leaves in T, is number of leaves in T 1 plus the number of leaves in T 2; which is this plus this which is at most 2 to the k plus 1. And that is the end of the proof.
(Refer Slide Time: 23:28)
 
So, let us just write this, so number of leaves T equals number of leaves T 1 plus number of leaves T 2. That is less than 2 to the k plus 2 to the k 2 to the k plus 1. That finishes the inductive proof, that on a tree of height h. When I have a binary tree of height h, the number of leaves is at most 2 to the h.
(Refer Slide Time: 24:02)
 
Number of leaves is less than equal to 2 to the h. Well taking logs, I get log the number of leaves is the lower bound on the height. Well we know remember that in our binary tree, the number of leaves was n factorial. So, the height this means, the height is at least log base 2 of n factorial. Well, this is the height of any of the tree, which is related to any this flowcharts, which sort flowcharts. For sorting we looked at these flowcharts.
So, we said that number of leaves is n factorial. And we have just seen that, the height must be at least the log of n factorial. So, what is this, we are very close. Log of n factorial is at least log base 2 n by 2 to the n by 2, why is this true? Well, this is true, because n factorial is n into n minus 1, somewhere n by 2. And then, all the way up to 1, now if you look at the top n by 2 terms, all of them are greater than n by 2.
There are n by 2 terms and all of them are at least n by 2, all of whom are greater than equal to n by 2. So, n factorial is at least n by 2 to the n by 2. Well, that is why this is true and this is nothing but n by 2 log n by 2. This is omega n log n, so the time taken is at least n log n. This is what we have proved some constant times n log n. So, this shows that merge sort, for instance we know it takes maximum time n log n.
But, there are inputs on which it takes, I mean log n, it must take. So, similarly quick sort and any other algorithm in this model, which is if the only thing which the algorithm can do, is look at two element, compare them. And perhaps move them around, swap them for instance or move them around. If this is the only thing, that is possible. Then, the time taken by any such algorithm for sorting must be n log n. There are sorting algorithms which do not fit into this model. We will not cover those, hopefully we have covered some of them in your data structures course.
These are bucket sort radix sort, well these algorithms differ in that. They you can look at the least significant digit. You can divide, you can take floors of these numbers. If they are integers, you can take floors and then, do funny things with them. The difference between these algorithms and let us say something like merge sort. Is merge sort, when you look at 2 numbers the only operation, that you can do is compare them.
On any element, if I take an element of the array. Only thing I am allowed to do on this element is compare this element with some other element in the array. I cannot sort of divide it by 10, 20, 100 whatever. I cannot look at it is least significant bit etcetera, etcetera. That is as far as sorting is concerned. We will continue with our discussion of algorithm design techniques, by looking at some other problems, which are fairly interesting.
And where this divide and conquer paradigm sort of works, and gives you better results. In some cases perhaps you the algorithm, that we desire.
(Refer Slide Time: 28:58)
 
The first one is this, so you are you are given a 2 k. So, this is the input by 2 k square. And what you want as output is a tiling with, I will tell you what all this means, with this. What do I mean by tiling, a tiling essentially means, I must fill up all these squares. And try and fill up as many squares as possible with pieces, which look like this. So, each piece is, let us say a kind of a domino, which sort of looks is of this shape.
And I want to try and fill up, as many squares as possible in this. So, well you can see immediately, I may not be able to fill up all square. Even for k equals 1, if I have a 2 by 2 square, I have a 2 by 2 square. Well I can put one of them. So, I can put something which fills up these three squares and this will be an empty square. So, here is a tiling of a 2 by 2 square with a domino of this form, which does everything except one square. So, what about 2 to the k cross 2 to the k.
Well, the number of squares, we look at the number of squares is 2 to the 2 k. These are the number of squares, the number of squares and this is 1 mod 3. I will let you figure this one out. The number of squares is always 1 mod 3. Well it is 2 here and you can see, that it is always two. If you want you can prove it by induction on k, but even otherwise you can sort of check that, the number of squares will be always 1 mod 3. Maybe we will just look at a equals 2.
So, then it is a 4 by 4, how does this look like. So, this is a 4 by 4 thing and you have well 16 squares; and that is 1 mod 3. When I divide this by 3 I get 1, so 3 times 5 is 15 and 1, one gets left out. So, that is true whatever k I choose, it is always 3 times something plus 1. So, that is what I mean by 1 modulo 3. When I divide by 3 I get 1, the remainder is 1.
So, can I sort of now put these dominos. So that, all except 1 square is covered. So, I have this 2 by 2 k 2 to the k by 2 to the k square. So, I will square with side length to the k. I have these dominos, which each of them has 3 squares put together. Can I arrange these on these squares, so that all except 1 square is covered, this is the question; the answer is yes. And we would like an algorithm to do this and well, we would like to do it using divide and conquer.
That is the goal, let us do this 4 by 4. And then, we will see what is the scene. Can I do a 4 by 4 with, so here is a 4 by 4 square, can I do this. Well, since I am going to do something like divide and conquer. I would like to divide this into two parts, or more. In this case four parts, tile each of them and put them together.
(Refer Slide Time: 33:24)
 
Well, let us take a bigger one and see how this is done. I know how to tile this, so that one is left, I know how to tile this, so that one is left and so on. Now, how do I tile the whole thing, so that one is left. Well supposing the one left here is this. One left here is this and one left here is this. Now, I am able to tile all four of them. So, I tile this, so that this is my first domino, I use my first domino there, I put it around like this.
This is my second domino, I put it around like this. Here is 3, it does not matter how I put my fourth as of now. We will we will change this later, supposing the fourth is like this. Well now I can put the fifth domino here, here 5 5 5. So, I managed to sort of tile at least 4 by 4 using 5 dominos. I have just one left which is what I want. You would like to repeat this for 2 k by 2 k. So, how does that look.
So, here we again would like to do a divide and conquer, we would like to divide this. I would like to first tile this, tile these 4 and then put them together. Well, by looking at this, this example I can do it if provided this square left off here is this. The square left off in this, is this. The square left off here is this. Here I really do not care. If I can tile the entire region here, except this corner and so on. For these 3, then I am able to tile the entire region on top leaving one squares.
That square left will be in this big sort of region. This is 2 to the k minus 1, 2 to the k minus 1, 2 to the k minus 1 and so on. So, this whole thing is 2 to the k. So, if I can do this, then I can fit in this entire, this extra sort of domino here. There will be one square left over in this region, good. So, again notice a crucial thing, to be able to carry this inductive process, to completion. I should be able to have the extra square. I mean the square left over as a corner square.
When I tile a region which is like this, the left over square must be a corner square. So, this is a square left over or left un tiled. I have tiled the entire region, except this small square. If I can do this, then clearly I can do the big thing. So, I started out my initial objective was to tile to 2 to the k by 2 to the k. That is what I started out with, I want to apply this divide and conquer kind of thing. Well, actually the process is really induction always.
If I can do it for a small square, can I push it up to a bigger square, that is the goal. Well, we want to do divide and conquer. So, we saw that, we break it up into these 4 smaller squares. Now, I want to tile these smaller squares and put them up, to a tiling for the bigger square. Well, each of these smaller squares, that is one square, one small square which is not tiled. One piece which is not tiled. Now, this happens to be the corner, then I can push the induction upwards.
But, for the induction to work even the final, so let us look at this again. For the induction to work in this big square, I want the one square which is not tiled to be a corner. Well, it cannot be corner here, because everything else is tiled, this cannot be a corner, it cannot be a corner. Well, can it be this corner, the answer is yes. It is like I leave this square un tiled, so leave this...
Now, when I look at these 4 small problems, they all look absolutely alike. I want to tile everything, except a small square in the corner. I want to tile all of it, except this square in the corner. Except this in the corner and except this in the corner. And I put them together like this. This tile is the entire big square, except for this one thing in the corner. And well this is the algorithm, break it up into 2.
Tile these 4 recursively and put them together like this, and in the middle I just put one domino. One domino fill up these three squares, so I have tile everything except this. So, this is a well divide and conquer sort of strategy to tile squares. So, this problem looks a bit different from the other problems. And that is why I picked this, because the strategy can be used not just for computer algorithms or puzzles, or whatever you have. And the other sort of lesson, that we learn through this example. Is that you start with some problem you want to solve.
We have seen this before, along the way you would like to apply, you want to find solutions to smaller sub problems. You want to put these together to get a solution to a larger problem. And while doing this, you may want to solve something slightly more general. And then, you see this general problem can be solved. So, you try and solve the general problem now, again using the same technique.
The smaller sub problems, you solve the general problem. And now you see, if you can put them together, to get a solution to the general problem for the big input. And often this sort of gives you the solution that want. We saw this in two occasions, one was this tiling problem, the other was a I hope you can remember. It was when we looked at this median. We wanted to find the median, but we ended up looking at the problem of finding an element of given an array and a positive integer r.
So, we found the element of rank r, that is what we did. So, let us look at one more problem, yet another problem rather, which uses divide and conquer. So, this is a familiar problem of multiplying to two integers. So, let us look at this.
(Refer Slide Time: 40:58)
 
So, I have 2 n bit integers, x and y, this is x, this is y, these are n bits. Each is n bit long, I want to find the product of x and y. The normal way is well, we formed the product of each of these bits with LSB, then with the next one and so on, and so forth. So, there are n square multiplications. Well, in this case actually it is not, you can just do are and since they are bits. But, you can think of these as instead of bits, you can think of them as digits, it really does not matter.
So, each of them could be n digits long and we multiply these digits. So, there are n square multiplications, this is the high school multiplication algorithm, that we know. Can we do faster, that is the question. Let us try applying divide and conquer. I have divide this into two parts. This is x left x right, y left y right. And now I want to compute the product of these two, well what is the product first of all. X is nothing but 2 to n by 2 times x l plus x r y is 2 to the n by 2 y l plus.
So, this means x y is 2 to n x l y l plus 2 to the n by 2 x l y r. X l y r plus y l x r plus x r times y r, this is the product x and y. So, we could recursively sort of find these products, these four products and then, compute this. This is just shifting by n bits, we are just shifting by n by 2 bits. So, that is an order n operation, so if T n is the time taken, this what is the recurrence.
There are 1, 2, 3, 4, 4 problems, so 4 times T n by 2 plus order n. This is some constant times n, which is for shifting and adding. Adding two n bit numbers, it takes alternately. So, what is the solution to this recurrence? Well, I will let you do this and the solution to this recurrence. You do a solution to this recurrence, it is order n square. So, we are back to where we started. Compare these two, well we have done divide and conquer, but we do not seem to have conquered anything.
We are back to spending order n square times, even with this divide and conquer approach, but well all is not lost. I will first indicate this algorithm. And then, we will analyze, and see how we can decrease the number of multiplications. So, well the trick is this, let us see.
(Refer Slide Time: 45:02)
 
So, the terms that we want to compute are x L y L and then, x L y R plus y L x R and x R y R. These are the three terms, that we would like to compute. The way we did it, was each of them separately that gave over problems of size n by 2. So, here is a smart way of doing this. Well, these two I compute as they are, the third thing I compute is not quite these two, it is the following. I computer x L plus x R times y L plus y R, I add x L and x R, I add y L and y R and then, multiply them together.
What is this, this is x L y L plus x L y R plus x R y L plus x R y R, that is what this is. So, I do three multiplications and not four, this is the first multiplication x L y L. X R y R is my second multiplication and this is the third. This I do not do, as of now I have not touched this, this is the third multiplication I do. Well, if you look at this x L y L is present, x r y r is present; and if I remove these two what is left is this. X L y R plus x R y L is exactly this.
Well, I have just written x R y L, but it is same as y L x R good. So, how do I compute this, well from this product I just subtract these two to get this, so call this Z. Now, I just do Z minus x L y L minus x R y R I get this, good what have I done. Well I have a few more additions, I have 1, 2, 3, 4 more additions. The number of multiplications has gone down by 1. Multiplying 2, n by 2 bit numbers I do 1, 2 and 3. There are now only 3 multiplications that I do.
So, what is the recurrence now, well T n is 3 T n by 2 plus order n. Earlier remember it was 4 T n by 2 plus order n. Now, it is 3 T n by 2 plus order n and it is not surprising, that the solution to this recurrence is smaller. In fact, we will show the solution to this recurrence, is order n to the log 3 base 2. Roughly it grows as n to the 1.59, which is certainly less than n square, which is what we started out with. So, this the number of multiplications one makes is significantly smaller.
(Refer Slide Time: 48:23)
 
Is 3 T n by 2 plus some let us this take n, it does not matter, so T n is this. We will do the usual thing which is 3, so 3 T n by 2 square plus n by 2 plus n. This is 3 square T n by 2 square plus 3 times n by 2 plus n. Let us do it once more, to see what happens, so this is 3 square T well n by 2 cube. I am expanding this out, so there should be a 3 here, 3 times plus n by 2 square plus 3 times n by 2 plus n.
So, this is now 3 cube T n by 2 cube plus, well 3 by 2 whole square times n plus 3 by 2 n plus n. Now, one can guess the general term, in this when I go down i steps.
(Refer Slide Time: 50:11)
 
So, T n will be 3 to the i T of n by 2 to the i plus 3 by 2 to the i minus 1 n plus 3 by 2 i minus 2 n so on, to n, good. So, now we would like n by 2 to the i should be 2 or say 1, for ease of computation. If this is one, if I have one bit and the time is 1, so the time is just this becomes T of 1 is 1. So, if this is true and T of n is 3 to the i plus, well n times 1 plus 3 half and so on, up to 3 half to the i minus 1. Well, let us do this, so this is 3 to the i plus n times, well what is this? This is just a geometric series.
So, it is a roughly some constant times 3 half to the i. You can do it more a bit better by doing the, writing the exact formula, which is 3 half to the i minus 1 by 3 half minus 1. So, it is at most, let me put it at most this is perhaps 3 half minus 1 which is half, so C could be 2, so C is most probably 2. In any case, this is bounded by something like this, 3 to the i plus. So, what is i? Well, 2 to the i is n, so i is nothing but log base 2 n.
So, this is 3 to the log base 2 n plus n times constant times, well 3 half to the i, which is 3 half to the log based 2 n. Now, 2 to the log base 2 of n is nothing but n, these two things cancel. So, I have 3 to the log base 2 of n times constant and 3 to the log base 2 of n here. So, this is nothing but some C prime times 3 to the log base 2 of n, this is the I time taken.
(Refer Slide Time: 53:26)
 
So, what is this, well so the time, the total number of multiplications is 3 to the log base 2 of n. This is 3 to the log base 3 of n divided by log base 3 of 2, I could take this up. So, this is nothing but 3 to the log base 2 of 3 times, use just some general manipulations. So, that is 3 to the log base 3 of n, I take this inside, n to the log base 2 of 3. This is nothing but n to the log 2 of 3 as promised. So, this is roughly n to the 1.59. So, the time number of multiplications, we got it down from n square.
So, the initial thing to roughly n to the 1.59. Of course, we increase the number of additions, but if multiplications are more expensive than additions. Then, we have saved on time. This trick of using three multiplications instead of 2 is actually a trick. It is something you have to really need to come up with somehow. There is no real sort of way in which you can do it. There is no method and that is the beauty of algorithm design.
That somehow not every problem can be sort of tackled using everything that, you could come up. Often you come up with, all the time you come up problems which require you to design new methods, derive new methods, find new methods. Rack your brain and sort of come up with good algorithms. And when you do come up with these new algorithms, you feel again, the kick that you derive, the pleasure that you derive is something else.
This trick by the way or this multiplication is due to Euler. And he used this for the multiplying complex numbers. We have just used this trick was then used by other people, who saw that. It can be applicable in multiplying 2 n bit numbers.
Thanks.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 9
Divide and Conquer - IV
Closest Pair

We will continue with our study of algorithm design techniques. We had looked at some examples or which we used this divide and conquer strategy. We also looked at sorting the strategy, applied to sorting to give a lower bound of n log n for certain sorting based algorithm. We continue along the same way. In fact we look at yet another problem which some meanable to divide and conquer. This problem comes from computational geometry. So, it has the basic is bit different, but the same strategies, algorithm design strategies applied even to field. So, the problem is this. 
(Refer Slide Time: 01:54)
 
The input the set of points in the plane say the output I want the closest pair of points. So, given some points, I can compute the distance, we can compute the distance. And among these points, that you have given in the plane, you want to find the closest way. For instance, the points could be given with the x y coordinates, that what we will assume.
So, for each point, I give you the x and y coordinate. And actually there are n points and given these n points, with their x y coordinates. I want to find a pair of points, which are closest to each other. So, given any two points the distance; the distance is square root of x 1 minus x 2 squared plus y 1 minus y 2 square. So, the usual notion of distance that you have in the plane.
So, for each plane of points, there is a distance and among there are n choosable distances. So, between these pairs of points, if I have n points, there are n choosable distances. I want to pick the minimum distance. And the points, which actually witness this minimum distance. So, there is an obvious way to do it. It is to compute for each pair a distance, for each pair you compute the distance.
Now, you have n choose to distances and you find the minimum, so that takes n choosable time, can you do faster, can you do better. So, for instance, do you really have to compute all distances to find the closest pair. Well, that is the big question there, and the answer is no. If the answer is yes, then I guess I would not be discussing this problem here ((Refer Time: 04:30)). So, the answer is yes and in fact, we will do it much faster than, then n square.
So, somehow you do not really need to compute all pairs, distances of all pairs, that is the moral of the story. Let us see how we go about designing. Well, on the face of it, if you would think a bit. You start wonder, why should I need to compute every pair of distance. What can I mean, can I for instance, look at geometry of the points to certain distances, maybe there are many ways of thinking about it.
We should also be thinking of an inductive approach, which is... Supposing, I remove one point to compute a distance, shortest distance between the other points. Now, adding this point, can I get rid of some points, may be yes, may be no. I mean, so let us look at this approach. This will not be the approach finally follow, but this is something that I wanted to think about for every problem. So, that is the reason I wanted to go over this again. So, the classic sort of inductive approach to this any such problem is remove a point. The recurs some the rest, put the point back and see what you get, good.
(Refer Slide Time: 06:13)
 
So, let us see this is the point I have to move, here the point set in this, let us say in this region. There are other points, this huge big star, this dot is what I have remove. Now, I find the smallest distance, now what can I do? I mean although I compute, what is distance. Well, it looks like, I mean the obvious thing is to compute distances from this to every other point. But, well that is not true way. You are back to the original to where you started.
Because, there are n minus 1, such other points and you will compute n minus 1 distances. And this recurrence you will see, will lead to in order n square solution, because your recurrence will be T n is n minus 1 plus T of n minus 1. This will lead to order n square, so you can check this. Well, can I get rid of some of these points now. If you could do the following, suppose in the minimum distance here, over the entire set, except this point this is delta.
The minimum distance, you have calculated recursively is delta. Then, you know that any point, the points that are actually in contention, are points in a circle of radius delta, around this point. Only points in this circle are in contention. The other points are well too far away. If you can quickly compute these points, then you are in business. I took n minus 1 times to extra time, but supposing I can do this much faster, somehow I do not know how.
But, if you can compute this much faster, then you are in good shape. Then, you can may be reduce this n minus 1 somehow to something else. For instance, if you reduce this to log n, your n really good shape. Then, maybe you can push this further, so your think is should be along these lines. How do I cut this n minus 1 log.
(Refer Slide Time: 09:02)
 
So, before we go further, let us do this in one dimension. So, what is this? That mean, you are given a line and some points. You are given some points. And you want to find the closest pair along this line. You want to find the closest pair, how will you do this? Well, I think some of you should see that, you do not need to really calculate all the n chooseful distances.
There are n points, there are n choose to sort of distances this. May be you do not really need to do n chooseful, look at all n chooseful. So, how will you sort of do this, well let think to notice. In all these things, you need to notice something about the problem. Some property of the problem, that sort of pushes you up. That let us you do things faster.
So, in this case, well the thing to notice is that, supposing I am looking at this point, the only candidate points are the adjacent ones. These are the only candidate points. If for each of these points, I can identify these candidates. Then, I am done because, for each point I need only to check two distances. So, that leads to 2 n. For each point I need to check two distances. So, for n points I need to check 2 n distances.
But, which 2 that is a big question. For this point, how do I find these two, which are neighbors in some sense. If I can do that, then just 2 n distances, then well looks linear. Well, how do I find these two, may be just solved them, so in one dimension, sort. Well, once you sort them, look at them in increasing order of from left to right. And for each point look at the previous and then, the next one.
Compute these distances and choose the minimum. Then, you choose the minimum by scanning these points, sorting takes order n log n. So, we started out with n chooseful distances, but really time taken is order n log n. Sorting dominates this procedure and if I sort of first sort it and then, I do this then it takes the time taken is n log n. I have reduced it from n square to n log n. There are some morals in this story.
First one is you should always try a simpler problem. You are given this problem in two dimension. It always pays to first check out what the problem means in one dimension. You will learn a little bit, that is the first thing. And you at least now know that, may be it is possible to do it in less than n choosable time. Because, one dimension certainly you can do it in n log n.
Question one asks now is about what about two dimension, can you do. Is there something like sorting that I can do, which will help me out. The answer is actually, yes and let us see how this is done, it is quite a smart algorithm. So, we will try and apply, divide and conquer approach, reasonably blindly. But as we proceed this fact we use some some trick that speed up ((Refer Time: 13:16)).
So, the first step is something you can think of, I want to divide the point into two parts. Find the minimum on the left part, find the minimum on the right part. Then, once you find the minimum, now I am going to use these minimums to compute the overall minimum. Will this help, will this not help, well will have to see. Before, we do this I think it is possibly illustrative to do this on the line. We first sorted in founded the minimum, that was n log n. Let us see what this gives, this divide and conquer strategy gives on the line.
(Refer Slide Time: 14:06)
 
So, I am given these n points, can I get something, can I do something better. What would the divide and conquer strategy be, well divide the points into two parts. One on the left, one on the right and recurs on the both; and what? Once we get the minimum distance from this and that. Now, the only sort of thing that we have not check, is if the minimum distance between points on the left and point on the right.
I find the minimum here, I find the minimum there. And then, the only distance I need to now compute is the point on the left, the last point on the left. The first point on the right, find this distance and check this against the minimum, that I have computed. So, I found let us say, delta L, delta R and this is delta. So, I need to find this minimum. So, once I compute delta L and delta R and delta, I just check the minimum of these and return the minimum of these three.
I need to make sure, that I find this point on the left recursively. And I make find this point on the right recursively. So, again this, if I want to do by divide and conquer. Then, I need to strengthen in ((Refer Time: 15:39)), which is that not only do I find minimum distances. But, also to find the last point, right most point and the left most point. Once I do this, then I can sort of put these two things together to get a solution.
I do not want to get into with this in detail, because the problem here is again splitting the input into two parts. Now, how do you split this into two parts. Well, we need to split it into two equal halves, then you need two halves. Then, we need to find the median, that takes linear time. So, if I look on the recurrence, it looks like T n, this is 2 T n by 2 plus order n. And this we know is n log n. The solution to this is order n log n.
This is the time taken to find the median and divide, this is the divide step. So, this does not really use sorting in any sense, except to find the median. So, this is another way of doing this, using this plane, divided and conquered on these points on the line. Let us do this to points in the plane. You have given points on the plane and I am going to divide this into two parts. Let us say based on the x coordinate, the first n by 2, the left most n by 2 points, it will form one set.
The right most n by 2 points will form the other set. I recursively find minimum distance, closest pair of points in the left, closest set of points in the right. And now I need to find, closest pair for the entire thing. And let see if this can be done fast. That is the goal of this lecture.
(Refer Slide Time: 17:50)
 
So, here let us see my points are points line this blob, divided into two parts. There are n by 2 points here, so this is the left and that is the right. There are n by 2 points, n by 2 points here. Now, I find closest pair points in the left, closest pair of points from the right. Let us say, that delta L is the closest, is the minimum distance on the left, delta R minimum distance on the right. Delta be minimum of delta L and delta R, this is the smaller of these two.
Now, I need to find closest pair. And I know that, if at all there are candidates or closest pair, it has to be that one point has to be on the left and one on the right. Any candidate pair has to be of this form. They cannot both be on the left and both on the right. One of them has to be on the left; and one of them has to be on the right. What else can be said, let us also sort of at the same time the recurrence in place. So, I am looking T n, so T n is the time. I have already done this plus what? Well, I would like this ideally to be.
If this is say order n, then this recurrence is n log n, I mean good shape. So, this is the amount of work that I will need, to find the closest pair. One from this side and one from that side; where one point is from this side, one point is from that side. Suppose, I do it well the natural way is to just pick every point on the left, every point on the right. And sort of find the distances, for every pair. This could be as large as, well that could be n by 2.
There are n by 2 points and n by 2 points here, that gives rise to n square by 4 pairs. So, if I compute n square by 4 distances I am ((Refer Time: 19:58)), that is just too much. In fact, we will back to our usual n square. We will be, you can check that, you compute all distances, all pairs of distances, the n chooseful distances. So, this is not what we want to do. So, how can we speed things ((Refer Time: 20:20)). This is the question, that you need to answer.
Now, at this point, it is left to your ingenuity. It is left to little bit of luck, but there are no clear. It is not as if at least I do not know of any thing that I can teach you, which you can use. This is left to just, your own intelligence and perseverance. You can try small problem, small examples etcetera, etcetera. But, the idea from this point onwards are completely new.
They are problem dependent, depends on the problem. And there are no general techniques, that I can tell you which you can use. There are no formulae's that you can apply. And this in fact, is the beauty of the speed, that it is not that I pump you full of ideas. And then, you are all ready to take on a problem. That is not the way to us. Each problem has a flavor of it is own.
And each candidate also has his own capability. And each person has his own capabilities. And is a good chance that, if you try this problem, you will come up with your own algorithm, your own passed algorithm. So, from this point onwards, I am going to tell you a few tricks by which we will reduce this. These observations are which I will make or we will find reasonably simple initially, but these we have to find on your own.
There is no teacher to teach you this, you have to learn it yourself. It just depends on your analytical skills, good. So, let us now get back to this problem. The first thing I am going to note is, where do these supposing I have these candidate points on the left. And where can they live, so let us look at this. So, this is my dividing line, dividing vertical line. Things are the left, where can they live, well they cannot, they all have to be, we draw a straight lines here.
They have to be within a distance delta of this middle point, similarly on this side. If a point lies here, then the distance between this point and any point on the right is greater than delta. If it lies on this side, this point and anything on the other side, this distance is greater than delta. So, I can forget about these points. So, the only points of interest to us are points in this small, on these small bands on either side of the middle line.
Each of these bands as bridge delta, these are the points of interest. That is the first observation, ((Refer Time: 23:19)) so far it is all fairly simple. Now, just this is not enough. The same problem could apply in the sense that, all n by 2 points on the left could lie in this band. And all n by 2 points on the right could lie, you know the other band. In which case we are again back to computing n square, roughly n square distance is the other smart thing. The intuition is this, so let us magnify this small band around the centre.
(Refer Slide Time: 24:00)
 
So, here my, this is the centre line and here are the two bands. This is delta and this is also delta. So, when I look to the left, if I take any point here, then I know that, if I draw a circle of radius delta, around this. There is no point inside this circle. The reason is this closest pair of points on the left, this is the left, this is delta. So, there is nothing closer than delta to this. There could be something at distance delta, that is fine. So, there is nothing closer than delta.
These points are actually spread, they cannot be a large number of points clustered together. They are all spread out nicely, distance delta. Similarly, the points on the right. We would like to use this fact, this is point number 1. That these points on both left and right are sort of spread apart. So, there cannot be too many points close by, somehow we need to use this. What else can be said, well let us look at this point on the left.
So, let us look at the point on the left, what about points on the right. Where can they live, let us draw this line. Well, I know that any candidates for this point will lie over this does not look like a square, but think of this as square. So, any candidates for p, must lie within either this square or this square. Please strain your imagination and imagine this to be a square. Then, any candidate for this point must lie, either in this square or that square.
They cannot be, if it is outside here, then the distance is greater than delta. In fact, distance to this line is greater than delta. So, this point is also greater than delta. So, everything has to lie here, and that is the first. The second thing we know, that if I take any point here, then this precludes many points from appearing in region of size delta. If I have a point, I have a point in there.
Then, in a circle of radius delta no other point appears. So, there cannot be, I mean intuitively one feels that, there cannot be too many points inside each square. This is delta by delta square, if there are too many points inside this. There are bound to be two points which are close by. I cannot have every point as far away as delta. This is what we are going to use, but how do we use this.
The intuitively you feel that, there cannot be too many points inside the square. The reason is essentially this, that if I have a point, there is nothing within a distance delta, good. So, how do we put this intuition into practice. Well, we use a very simple principle, that you have heard many time. It seems very simple, but putting it to practice often requires some thought. The principle is called pigeon hole principle, well what does it say.
It says that, if there are n plus 1 pigeons and there are n holes. And you stuff these pigeons into these holes, then at least one hole, must have two pigeons. There are more pigeons and holes. So, if you stuff these pigeons into holes; and at least one hole must contain two pigeons. Very simple principle, we all immediately understand the statement, but often very difficult to figure out where to apply and how to apply. Really nice problem, solutions based on this principle, which smart people have figured out. So, let us go back to this square problem.
(Refer Slide Time: 28:37)
 
So, I have this square of size delta. And I know that, if I have any point in this square. Then, no other point is within a circle of radius; which means given any two points, the distance is at least delta between any pair. So, the distance between any pair of points in this square, is at least delta. How many points can I put in inside this square. Well, here is the trick, what you do is divide this into four parts. These will be our holes, you have to see where the pigeons are.
Well, what is the maximum distance inside the small square. Well, this small square as side length delta by 2. Maximum distance is along the diagonal, which is square root of delta square by 4 plus delta square by 4. So, this is the square root of delta square by 2. This twice this which is nothing but  delta by square root 2, which is strictly less than delta which is what we want. So, if I take one of these small squares, then the maximum distance is delta, which means I cannot have two points sitting inside this small square.
If I take any two points, the distance between them is less than delta. So, at most one point can sit inside the small square, which means in the big square I cannot at most four points. If I say the distance between any pair of points is at least delta. Inside this big square, I can have at most four, this is the big trick, that we will use. So, this is the trick we will use, so let us get back to the original picture here.
(Refer Slide Time: 30:58)
 
So, I have p and how many points on the right, do I have to really compare. Well, four inside this, there could be at most 4 candidates in each square, if at all. 4 or less than equal to 4, to be less. It looks like as of now, for each point I need to at most look at 8 other points and compute this much. And this certainly looks to be linear. For each point I need to compute at max, let us say, 8 other distances. Then 8, 10 distances is what I need to compute and my recurrence I will have 2 T n plus by 2 plus T n; and this is n log n.
How will we do this, we still not done. I mean, we have all the ideas in place now, it is implementation. Smaller implementation details are we need to worry about. For instance, for each point on the left, how do you compute these 8 points, you have to do this fast remember. You cannot take a lot of time to. We look at all the points for each points, then your sort, because that is n square already. But, again looking at this picture, you should again get ideas.
So, let us go back to this picture. So, here is the point, the 8 points are somehow located in the neighborhood of this point. They are at distance delta from here delta from here delta. These are the points that I need to and this ((Refer Time: 32:44)) I need to pick up. So, let me put up another picture and after this picture hopefully the algorithm will be clear.
(Refer Slide Time: 32:54)
 
So, here is my centre line and here are these various points the left in the right. Here are the various points. If I take any point on this side, if I consider this. Then, I only need to look at a window size to delta, delta on this side, delta on that side. This is what I need to do? As I go up, if I look at this point this window shifts up, it somehow if I can scan them upwards, in this order, scan from bottom top.
If I can scan this points from bottom to top, somehow this window also keeps you think from bottom to top. And it again looks like I can do this efficiently, all we need is these things altered by y coordinates. If I have these points altered by y coordinates, then I can scan the left side from bottom to top, the right side from bottom to top. And I can maintain this window easily, this is the last trick in the algorithm. There is a detailed left. So, if I have these points sorted by the y coordinates. Then this is what I do? I start looking at the points on the left in the narrow band.
I look at these points, look at them in increasing y coordinates. And for each point I have I also maintain this band of this sort of window on the right. So, when I look at a point, I have a corresponding window on the right. When I move to the next point I correspondingly move them window up. This is all I do, so somehow I need to move this window up. And you can see that, it is not it should not be too difficult, to move the window up.
And the time that it takes is linear with and the very similar to the merge sort idea. The window moves up each time. On the left each time, you know a pointer moves up, on the right side the window could move up, but it can move up only n time. The window can move up only n time, because there are only n points. So, at least the movement, the number of times the pointer here moves or pointer here moves, that is linear.
And once you have fixed a window and a point on the left, I need to compute distances in this window. And I know, because of the geometry that there are at most eight points in this window, four on top four on bottom. There are at most eight points I need to compute these eight distances and choose the minimum that is it.
So, let us put all this together and see what we get? That is before we do that, I just want to point out one thing. So, we need the points sorted on to five coordinate. Now, if we do it on the fly, once we partitioned now if we look at points in the each band. And now we sort these points along the y coordinate you take n log n time sorting takes n logging time. And then, the recurrence looks like this.
(Refer Slide Time: 36:50)
 
Your recurrence would look like T n equals T n by 2. Well, 2 T n by 2 plus n log n, because this sorting will dominate. Once, we sort we can go over them in linear time. But, this sorting will dominate and take n log n time. This the solution to this recurrence is still not n long n, the solution you can check that the time to find what the solution recurrence is n log n square and it is not n log, we would like to be positive.
In fact, you would like to get rid of this n log n and replace this with order n. So, that overall we get n log n. And again the trick is we tree sort the points in the x and y coordinates. We do not do it at each recursive call. So, all I want is the point sorted on y coordinate. Suppose, I can do it once in for all initially. And use this effectively and that is what I want. To this n log n for sorting along both x and y coordinates I do right in the beginning, that is the one time cost I do not repeat this in each literature, it is a onetime cost I sort them I am done with.
And now I have this sorted order each time I make a recursive call. So, each time I make a recursive call I have this sorted order in place. In which case, perhaps I can put order n instead of n log n and you get order n log n in total time. So, let us look at this algorithm, now we have I have told you all the pieces, we just have to put them together and analyze the algorithm and see how much time it takes.
(Refer Slide Time: 38:52)
 
We first sort to, so input a set of points. First step 1, sort them on both x and y coordinates. At this point I would like to make an assumption, which is that each point has distinct every point has a distinct x coordinate. And every point has a distinct y coordinate, which means if I take any vertical line at most one point lies on it. If you take any horizontal line at most one point lie on that. This is not really a big assumption. It will not it affect us for instance what I could do is take the initial set of points.
Now, pert up each points slightly, add a small Epsilon to each point it sort of move them around a little bit. So, that the minimum distance does not change, these perturbations are very, very small; these perturbations are very small. The minimum distance to change was a points do not change. The points participate in the minimum distance will that does not change.
While this perturbation, if I make sure that if I take any vertical line two points do not lie on this vertical line. Actually the algorithm I give right now, will assume that you know no two points have the same x coordinate or y coordinate. You can, then try and modify this algorithm to work even in the case. There are points on with the same ((Refer Time: 40:36)). So, I sort the points on both x and y co-ordinates.
So, I have two arrays the first array where points are sorted on x coordinates. Second where points are sorted on y coordinates. This is done right at the beginning I am not going to do this recursively. Now, the actual procedure starts. So, the input to this procedure, the input is points sorted on x and y co-ordinates. Points sorted on x coordinates, same points sorted on y coordinates. So, I have two sorted arrays. This is the input, what I do the first step, let me call this step 0. This is the initialization phase.
Step 1 here is divide, step 2 recurse on both, divide into halves. And step 3 put them back. Look at these two solutions and now look at these points and find the minimum and ((Refer Time: 42:06)) broadly these are the three steps. So, let us expand on all these steps, here is step 1.
(Refer Slide Time: 42:19)
 
Step 1 is the divide step, this is the divide step. Well, I have the point sorted on x coordinates. So, I can find the left most, so since points are sorted on x coordinates, find the left most n by 2 elements and right most n by 2 points, these are points. This sort this give you the two points, the two sets. Now, I need to do this even for the points sorted on y coordinates. So, I have two arrays remember.
So, let me call them, let us say A and B. So, A is sorted on x coordinate, B is sorted on Y coordinate splitting A is easy, leftmost n by 2 elements and right most n by ((Refer Time: 43:22)). So, this algorithm that we have just seen is perhaps as you realize is more complicated, than what we have seen so far. And we seen all the pieces actually I have given you all the details.
What they are going to now is summarize the whole thing, put them together. And then, right out the recurrence, which will not be difficult once we put them on the steps. And once we do that, we will see that this actually runs in n log n time. So, let us summarize and write down this algorithm in one piece.
(Refer Slide Time: 44:06)
 
The first step is initialization, it is initialization. And here we sort the points on x coordinates and on y coordinate. So, given the set of input points we form two lists. So, we have two arrays. One array which will store the points in increasing order of x coordinates. The other array will store the points in increasing order of y coordinates. And this will be the input to the procedure.
This initialization is done before we start the algorithm. And each recursive step will pass down this information. So, the next step is the divide step, so it is step 2, it is divide step. So, here we first look at the median. So, let the value of the median, when sorted on x coordinates, let us say x median. So, now we split the input into two parts, points to the left of x median and points to the right of x median. And then, we would like to recurse on these two parts.
To do this, we have to split both the arrays into two parts, both the arrays which sorted on x coordinates and the array which is sorted on y coordinates. The x coordinate business is easy, because you know where the median is right it is n half the element. So, you just split the array into two parts in a very natural rate. For the y coordinates what you need to do is go through the entire array. Look at each point, look at it is x coordinate and then put it into an appropriate array.
So, now let us say these two arrays, where it is points are x coordinate. So, these two arrays are called, let us say capital X and capital Y. So, split let me write this in a new paper.
(Refer Slide Time: 47:18)
 
So, split X into X L for X left and X R points in X L have their x coordinate less than equal to x median, the median value. And the others are in X R, the other points X R contains the other points. Similarly, split Y into Y L and Y R. Remember, Y sorted on Y coordinates, Y L and Y R will also be sorted on Y coordinates. Y L contains the same points as X L only sorted in Y coordinates. Similarly, Y R. So, Y R contains the same points as X R
So, this actually divides the input into two parts. Initially you had this sorted arrays X and Y. And now you have created X L, X R, Y L, Y R. The next step is the recurse step. So, let us write that, so pass. So, the input is divided into two parts X L, Y L and X R, Y R. These are the same set of points, in X L they sorted in increasing X coordinates and Y L they sorted in increasing Y coordinates. So, here is the recurse step. So, you recurse on these two, when you get the shortest the distance of the shortest the closest pair on both sides, on either sides of x median.
(Refer Slide Time: 49:56)
 
So, this is the recurse step. So, step 3 is the recurse step. So, what you get is let us say d L is the distance of the closest pair on input capital X L capital Y L. Similarly, d R is the distance of the closest pair on the input X R, Y R. On input X R, Y R the distance of the closest pair on the right hand side is d R. The one on the left hand side is d L. So, now you look at these two distances d L and d R and d compute d as the minimum of d L and d R. So, that is the next step, where you compute the minimum of d L and d R.
Now, let me quickly review what we have to do next. Around x median, we look at a band of size d on both sides. So, let me draw a picture. So, here is x median around this on both sides at a distance of d, we look at the band. This is the on the left side, that is on the right side. Now, we look at points on these two sides. And we know that, if you need to find points which are closer than d. Then, one point should come from the left. One point should come from the right and they must be in this band. So, let us first prove X L, Y L, X R, Y R to only points in these bands.
(Refer Slide Time: 52:27)
 
So, step 4 is putting them together, is putting the answers together, so that is the next step. So, X L prime will be points from X L whose x coordinates are, well greater than or equal to x median minus d. So, just look at this figure ((Refer Time: 53:13)). This is x median, we need points in this range. So, any point will have it is x coordinate at most x median minus d, so that is step 4. So, similarly Y L prime, so the Y L prime same y coordinates of the same set of points, as in X L prime sorted on y coordinates.
So, you get these by pruning X L and Y L. So, you look at X L and Y L and only look at these points in the band and you prove, remove some of those points. So, you are left with X L prime and Y L prime. Do a similar thing for right hand side, so you get X R prime and Y R prime. So, X R prime will be points from X R whose x coordinates are. So, these are points from X R with x coordinates, which are less than equal to x median plus d. These are points from Y R with x coordinates less than equal to x median plus d.
So, these are the same set of points, only X R prime is sorted on x coordinates; Y R prime is sorted on y coordinates. Our focus from now will be on Y L prime and Y R prime. We will only look at these points from increasing y coordinates. So, we are still into step 4. So, let me recall what we do now. So, here are the points let us say we have X R prime and Y R prime, X L prime and Y L prime.
(Refer Slide Time: 55:28)
 
So, I look at, so here is my array X L prime and here is my array, let us say X R prime. These arrays could have different sizes, because we have proved points from X L and Y L, X L and X R. This should be Y L prime, because you want to look at the points and increasing y coordinates. So, Y L prime and Y R prime are what we are concerned with. So, maintain three pointers, let us say p, q and r.
P will point to something there, that is p, q and r will be pointers to Y R prime. So, let me quickly recall what we want a do. For each point here, so here is what we are going to do. So, recall that, for each point in Y L prime, we will find at most 10 points. This at most 10 follows from a discussion that we did earlier. That in a square with 5 points, there will be two who are close as half the diagonal.
So, we will find at most 10 points whose y coordinates are, so supposing this point is, so for each point, let us say this point is some x prime, Prime. Y coordinates are in the range y prime minus d to y prime plus d. So, we find these points and then, compute the distance from x prime y prime to these points. And this will add 10 more distances. We do it for every point in this array. Once you do that among all these distances, 10 times the size of number of points in Y L prime, we compute the minimum. That is our goal.
(Refer Slide Time: 58:17)
 
So, a generic step is supposing, so a generic step is this, so I have... Let us say I have computed up to this point p is done, here is the letters q and r. So, between q and r, we find the points with the correct range of y coordinates. So, supposing we have done up to this. Now, in a generic step, I will raise p by 1, p is p plus 1, so this moves one step, now this is your new p. So, now correspondingly q will move and r will move. So, the next step is move q appropriately, then move r appropriately.
What do I mean by appropriately, you increment q till, if this value is not in the range y prime minus d to y prime plus d, which means it is smaller than y prime minus d. So, increment as long as y coordinate is less than y prime minus d. And similarly you move r, as long as it is less than y prime minus d. Finally, when you do this, then you will have these two values for the next value of d. And now you compute the distances. Then, compute the distances of Y prime L p and all points between Y prime R q and Y prime r small r. So, then this is the generic step in the loop, put this in a loop. And once you are done, you found these distances, compute the minimum. So, the last step is compute the minimum of these distances.
(Refer Slide Time: 61:08)
 
So, say it is some delta, then return min of delta and d. And that is, ends the algorithm. Now, if you look at the analysis, then there is an initial. So analysis, so the sorting initially is order n log n. There are two recursive calls, so that is 2 T n by 2. And then, if you look at the whole analysis, the total time is O n. So, we can quickly do this, while splitting does not take time. The only problem is this pointer manipulation. ((Refer Time: 62:13)) In this pointer manipulation, how many times this p, q and r incremented.
Well p can be incremented at most n times. Q can be incremented at most n times. R can be incremented at most n times, so this is order n. The total number of distances we know, we calculate is at most order n. So, that is why this is order n and T n satisfies this recurrence. And the solution is T n is O of n log n. So, this is the analysis of this algorithm, it is pretty simple, and this ends our discussion on divide and conquer.
Thank you.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 10
Greedy Algorithms ? I

(Refer Slide Time: 00:59)
 
We will look at yet another algorithm design technique. This one is interestingly called greedy algorithm or greedy techniques for designing algorithms. So, the keyword here is greedy and well, it has something to do with greed as we know it is only a little bit. The main technique main sort of technique for these for this algorithm design technique is the somehow piece together your optimum solution small piece by small piece by piece. So, construct the optimum solution somehow by getting a small piece first then sort of enlarging it and so on and so forth till you get the final answer. 
So, we will start with an example and we will see how this technique works. In some sense it is a bit difficult to listen you have to some zero in on part of the optimum output. The optimum has some output you have to somehow zero in on some part of it. Then slowly enlarge it and how do you end up with this small piece. How do you enlarge this? That is the crux of design technique here often there are no clear cut guiding rules that I can give you. It is left to your sort of imagination and intuition hard work whatever perhaps luck that you come up with the right ideas that build in solutions. So, let me write down the sort of basic ideas here.
(Refer Slide Time: 03:19)
 
So, the greedy approach, the basic idea the basic step is construct the optimum solution or output piece by piece this is the basic step. So, generic step in your algorithm which will typically be in a loop is having constructed some part of the optimum solution extend this you extend this by identifying the next part. This is the crucial generic step which you will have to design. So, once you get the problem you somehow have to get this generic step. Usually one starts off by figuring out what the first step is. Then the generic step and then you just put the generic step in a loop and that is how the algorithm looks like. Well, the crucial question that you will have to answer for each of these problem is how do you identify you know the next part of the optimum. So, you have got some part of the optimum somehow how do you zero in on the next part? This is the sort of crucial question that you will have to answer.
(Refer Slide Time: 05:41)
 
We will have to answer as we go along for this course the question is how you zero in on this extension? By extension I mean extension of this optimum solution like you is building. So, how do you get this? Well, for this course its intuition perhaps hard work well, some amount of luck and I will sort of sort of just say local improvement. Do not worry too much about what this means. I will explain as we go along. This may be the only key I can give you at this point. This local improvement trick is not very formal it is sort of an informal idea that I am going to give you. And we will see that in many of these problems you can sort of use some of these small tricks that I give you to get the algorithm. But often you know you just have to rely on your own engineering as a bigger hint as to where all these algorithms comes from. There is some there are some there is some theory behind it which we will not which as this says beyond the scope of this course, but I will tell you where at least some of it comes from there are 2 subjects.
So, let me sort of tell you what both are and I will strongly encourage you to read about them. The first so, this is for this course so, future I encourage you to read the following. One is Matroids theory this is a theory of Matroids and the second one is linear programming duality. So, that is so, called primary dual method in linear programming duality which gives you a sort of method in using that method many of these so, called greedy algorithms can be devised. In the way you pick up the next part often is dictated by this by this geometric structure. So, but this is for the future forget about this after this course we have the time ((Refer Time: 08:28)). Please do read about these 2 they are very important algorithm design technique. So, here is the exchange trick which I am going to tell you in a very sort of in a hand wavy fashion.
(Refer Slide Time: 08:45)
 
So, here is the exchange trick or the local improvement trick. So, this trick in this trick what you do is this? So, supposing you have some solution some generic output for the problem. There is an input we want the output. Well, usually we want an output which satisfies which is maximum in some respect or minimum in some respect optimum in some respect. So, supposing you take some solution which may be which may not be optimum. 
Well, the question asked is can you improve the solution by adding or subtracting. Let us say 1 element can you improve the solution by adding one element to it or subtracting one element from it? So, this question will sort of dictate how the algorithm proceeds. This question will keep asking and we will see how answers to this question. We will sort have trigger our algorithms; we will sort of push the algorithms forward. So, in some sense we may have partial solution. You see how to improve the solution improves it and so on. So, that is for all the theory behind greedy algorithms. We will now get down into business and solve a few problems. The first problem is the input I need to define a term which you may have seen before.
(Refer Slide Time: 11:20)
 
It is a notion of an independent set and independent set in a graph G is Well, it is a subset of the vertex set such that the there are no edges between any pair of vertices. It is a subset let us say U of the vertex at V of G such that if I take any 2 vertices in U there is no edge between them such that for any x y belonging to U, x and y are vertices in U x y does not belong to the edge set of G there is no edge between any pair. So, let us take an example suppose this is a graph let us complicated a graph. So, let us give them some labels a b c d e f g h while you look at this graph let us see 1 independent set could be a. If I put a in the independent set then b and d cannot be part of the independent set, because this is an edge between 2 of them. So, I could have c in it and f. This is an independent set, a c and f is an independent set.
So, I could have other independent. For instance a and c by this is also an independent set a and c this is an independent set. There is no edge between a and c. Similarly, I could have let me put one more let us say b f. So, if I have b and f this is an independent set I could have e h let us see I have e and h then c a this is an independent that is c a. That is another independent set. So, I have sort of ((Refer Time: 14:18)). So, there are many others. For instance each vertex by itself is an independent set. Just take an vertex itself it is an independent set. So, it is just a collection of vertices. So, there is no edge between that is an independent set. Now, for instance if these are radio stations it is a mobile radio stations and they are transmitting they want to transmit and 2 of these cant transmit if they sort of if they interfere. So, then only an independent set can transmit at a time. If I take if a b c d e f g h are radio stations.
And I suppose I draw an edge showing that these 2 fellows cannot transmit at the same time, because of the same frequency, because of interference then at one particular frequency or at one particular time only an independent set can transmit. So, that is the, that is roughly the motivation where independent sets come in. There are other motivations there are other places where one looks for independent sets. So, general problem is given a graph you want to find an independent set large size. That is the general problem. In fact, an independent set of maximum size that is the problems in that have been traced. This problem tells out to be hard and we look at it later on in this course. For now we will restrict the graphs ((Refer Time: 16:00)). So, here is the problem in totality.
(Refer Slide Time: 16:09)
 
So, the input the tree T by tree I mean a graph that is connected and acyclic. The output I want a maximum sized independent set in T. I want a subset of the vertex set of maximum size so, that there is no edge between any pair of them. So, this is the problem that we would like to solve. Well, how you solve such a problem? How do you find maximum sized independent set in T? Well, if T has n vertices let us say T has n vertices one way to do it is too we sort of group force way ((Refer Time: 17:24)) sort of compute one to one ((Refer Time: 17:28)) is to look at all possible subset or vertex set. And look at see they are independent and choose the one which has maximum size. Now, this is just too expensive. The time taken is least number of subsets of n vertices 2 to the n let us do it we are not going to do it much faster what else could one do. Well, pick a vertex if you pick a vertex in the independent set then the neighbors of this vertex will not be in an independent set. So, that is by definition. So, you pick this pick a vertex throughout its neighbors we will look at this graph. Again pick any vertex that remains throughout its neighbors and so on. This for you will get an independent set that is fine, but I do ensure that the size is maximum. If you pick the wrong vertex at any point then you may end up in trouble.
So, here is simple example. So, let us say this is the graph If I pick this vertex if I pick this vertex then what ((Refer Time: 18: 48)) you get is an independent set of size 1 the optimum is of size 3. The solution here the solution to this problem is a subset or a vertex set and you have to somehow pick the right subset. And the greedy solution greedy way says pick it one by one somehow which vertex would you like to put in the independent set ((Refer Time: 19:39)). The independent set is large will you think about it for a while intuition tells you that you should pick a vertex of smallest degree in this graph. So, if a vertex has this small degree then the number of neighbors that you throw up remember when I pick a vertex I have to throw out its neighbors number of neighbors I throw up is small. In particular if I leaf in the independent set. If I pick a leaf in the independent set then the only vertex I just throw out one vertex per leaf right and this actually works. In this case this algorithm works though we need a proof to show that this technique actually it produces maximum size independent set. The algorithm is just this. The just pick a leaf throw out its neighbor and continue. At each stage you will have a forest.
If you have a vertex of degree 0 it is not adjacent to anything else which you may come across as the algorithm proceeds just put it in the independent set blindly. If no such vertex exist then I pick a leaf from there will be many trees as the algorithm proceeds pick any leaf and put it in the independent set and continue. This algorithm actually works. It; however, needs a proof. You need to prove that this simple algorithm that we just described actually works. It is greedy in the sense that we start you piece you put the solution together piece by piece. You pick a piece and slowly sort of proceed in this way. Once you have a part of the solution you do not change it. This is the other sort of property. If I have a few word vertices in the solution I just build the solution up further I do not sort of throw somehow and then put something and then do back tracking. In this case we sort of; obviously, to extend the solution which is to pick the vertex a minimum degree it works. We will see many examples where simple things are not working. There is yet another way of looking at this algorithm which I would like to describe. This is a bit more general than what we have studied and this is the sort of exchange trick I promised which we will use over and over again.
(Refer Slide Time: 22:28)
 
So, another way to come up with the same algorithm. So, let us take an example to each. So, let us say this is a tree; tree in question and we would like to pick a maximum independent set in this tree. Supposing you have some subset which is an independent set. Let us say this and so on. This is the algorithm has not picked. This is just to sort of get your thinking process going. Supposing you have you have let us say somebody gives you an independent set this is an independent set somebody gives you this independent set. So, the question is when can I exchange can I sort of add and subtract to this solution to increase the number of vertices in the independent set? Can I add and subtract to this solution to increase the size of the independent set? Well, let us look at this here I can add these 2 and remove this I can add these 2 and remove this. From this example actually its look like I can always do this with leaves the sense that I can add these two and remove this.
So, this is a natural way to lead on to the question will leaves always do all leaves always fall in some optimum independent set. So, this is the question we would like to answer and you can see the answer is yes why is the answer yes? Well, supposing this is the lets say this is the tree this is any tree and I have a leaf here. I have any tree and I have a leaf. Well, what I can say well, one of these 2 better be in the independent set. In the optimum independent set one of these better be in, because if this is not in I can always put the leaf in into an independent set. So, one of them always has to be in the maximum independent set. Well, supposing I have this in the independent set and this is not ((Refer Time: 25:07)) and then you notice that I can remove this from the independent set and put the leaf in. The size of the independent set does not change. The set still remains independent and the size of the independent set does not change. I have picked one element out and I have put a element in and the size does not change and this is the exchange which we did earlier on when we tried to increase the size of the independent set.
So, this in fact, is a one line proof all leaves must be in you know in some optimum independent set. So, you can start of by putting all leaves into the independent set you know there is an optimum which contains these remove their neighbors now, you can recurse. This is the good thing with greedy algorithm. If you pick a part of the output then you have a smaller input to deal with and you can just apply the same technique over and over again and the proof follows by induction. In fact, the algorithm also is inductive you can right it as a recursive algorithm you move part of the output. Now, you have a modified input on which to work on and ((Refer Time: 26:23)). So, that is the algorithm for this problem we move put all the leaves in the optimum independent set remove their neighbors. The tree will now break into many parts and you just recurs in each part. So, that is the algorithm for this problem. So, here is the next problem.
So, imagine that you are the systems are your administering a system. Let us say you have one really fancy computer at your command and a lot of people want to use it. So, they come and give you one so, you have a 24 hour sort of time slot in which to schedule them and they give you each of them comes and give you a time slot. Somebody comes and says 3 am to 5 am, somebody else says 2 pm to 7 pm these intervals could vary. Somebody could 12:30, 12:31 pm to 12:32 pm 1.05 am to 3.51 pm and so on. So, they give you sort of intervals. Each person gives you exactly 1 interval. We cannot spread his job we cannot each day he has just one interval in which he can work we can give 1 interval. 
So, many people come and they give you intervals in which they would like to occupy this machine. And they do not want shorter intervals they wanted for their entire duration which they have asked for. Your job is to pick these intervals some intervals essentially schedule people on this machine. So, that firstly, at each time just 1 person on the machine 2 people cannot work on the machine and the second thing is maximum number of people get to use the machine in a 24 hour slot there is a 24 hour slot. Your objective final objective is maximum number of people get to use the machine. It should not your policy should not be dictated on by any other ((Refer Time: 28:58)) like the amount of money they are willing to give you etcetera.
(Refer Slide Time: 29:17)
 
So, here is the, an abstract sort of origin of this problem you are given a set of intervals so, the input is a set of intervals. You can take them as closed intervals if you want. The output is a subset of non overlapping intervals of maximum size. You must output a subset of non overlapping intervals with maximum size. So, let us see what this looks like. So, here is let us say this is 0 to 24 hours so, the input looks like here is an interval. So, people give you various intervals some of them should overlap. All kind of funny ways 2 people may want the same thing that is also possible and so on. So this is how the input looks like. So, this is person A B Q Z each person gives you an interval and you have to pick a subset of this intervals. So, that they do not overlap. So, if I pick A I cannot pick Q. I can pick B, but I cannot pick Q and I want to pick a maximum number of these intervals we said that I want to subset on these intervals which is of maximum size. So, that if I look at the intervals in the subset they do not overlap. So, a solution to this we will see in the in the next class. So, given a set of intervals you want a subset of overlapping intervals of maximum size our maximum size I mean the subset of maximum size. 
So, this is the size of the subset which means number of intervals in the subset that should be maximum. For instance here I can if I can take 4 intervals then that are better than picking 3 intervals. Even though the 3 intervals may be longer that is not our goal our goal is to pick numbers just numbers. As many intervals that I can schedule you should pick that is the problem. So, the problem you are solving is this. The input in a set of intervals and we would like to pick a maximum number of them. So, that pair wise they do not overlap. So, if I take 2 intervals they do not overlap they are separating. So, how do we do this? Well, greedy technique the so, called greedy technique says builds a solution piece by piece. So, I am going to build the optimum solution interval by interval. So, which interval would you pick first which interval can you say in lie in some optimum solution which is what does your intuition say. Well, I want you want to pick intervals. So, that a large number of intervals. So, intuition I think the first thing that sort of strikes you is that if I pick intervals in small size. So, that the span is small maybe I can pick a large number of them.
(Refer Slide Time: 33:33)
 
So, let us consider this. Let us call this algorithm 1 pick intervals interval of smallest size smallest span let us say this span or the interval. Suppose the interval is let us say goes from a to b; b minus a is the span of the interval. So, pick the smallest interval with smallest span. Well, smallest length is the one which you want. This seems to be a reasonable algorithm. This is what perhaps intuition suggests, but this intuition is faulty. It is very easy to construct examples where this algorithm fails. Here is on here is an example where the algorithm fails. Well, let us take a small interval then take 2 large intervals that intersect us the algorithm says pick this one.
So, this is the input; this 3 intervals form the input. The algorithm says pick this once you pick this you cannot pick these 2 fellows. Clearly optimum consists of picking these 2 big fellows these 2 big intervals. The optimum has value 2 value of this picked one. So, here the first sort of first thing that you can try fails. So, picking the interval of smallest length not fails. Well, let me think a bit more. Now, we start thinking a bit more what else could work here. If you look at this example, this small even though this interval is small it overlaps 2 of them while these big ones even they are big they overlap only 1. So, may be the right thing to do is to pick interval that overlaps with smallest number of intervals.
(Refer Slide Time: 35:45)
 
So, here is the algorithm 2; pick the interval with which overlaps the smallest overlaps with the smallest number of intervals. So, you see that the first algorithm fails you construct an example where the algorithm fails and here is an algorithm that at least works on that example. It seems fairly reasonable also that you pick an interval which overlaps the smaller. So, when I pick this interval I throw away the smallest number of intervals. I pick an interval I throw away the smallest number of intervals and now, I recurse. Once I pick an interval every interval that overlaps with this interval I am not going to pick. So, I throw those away and I recurse this should look very familiar to the previous problem. There you pick vertex throw away its neighbors and go forward. Here you pick an interval throw away the intervals with which this overlaps and you proceed forward.
Well, they are extremely closely related. In fact, we could form a graph right here. For an interval you have for each interval I can you can have an vertex and 2 there is an edge between two vertices if the intervals overlap. Then all you are doing for this problem is picking an independent set of maximum size in this graph. The way you get the graph is for each interval you have a vertex and you join 2 vertices in the intervals overlap and you are picking an independent set of maximum size that is what the problem is. This is an independent set of maximum size. So, like in the previous case why not pick a vertex with smallest degree. That is what this is an interval with minimum overlap minimum number of intervals that it overlaps is nothing, but a vertex of smallest degree in this graph.
So, pick a vertex of smallest degree then you get its neighbors and recurse while I have written just one sentence there is a recursion going on. You pick this you pick an interval of smallest degree throughout all the intervals that it overlaps. And now you recurse you again pick an interval of. So, that it overlaps the smallest number in what remains throughout everything that it overlaps and so, on and continue. So, that is the algorithm does it this work? Well, it worked in the previous case. In the first problem this worked unfortunately in this case it does not work. So, here is the example. So, this is a bit more complicated the example now, which I have to construct. So, that this algorithm does not work for work in this case is a bit more complicated. So, here is the example.
So, far it looks like the old 1 we are going to now, apply more intervals and interval is 2. This is let me it ends here. I do not want this to overlap with this these 2 do not overlap with each other. Well, similar similarly here, I can have many copies let us say k copies. Similarly, here k copies we can have many copies. I just want to make sure that this interval does not overlap with those. Let us look at this example. So, these intervals there are k copies of these intervals. So, each interval overlaps with you know k minus 1 over here and 2. So, k plus 1 intervals is what this overlaps. This interval overlaps with k of them this interval overlaps with k plus 1 this similarly, these are sort of mirror image. This side is a mirror image of this side.
So, these intervals have a large overlap. The smallest overlap is this interval is a interval with smallest overlap with smallest overlap. In fact, it is 2 these 2. So, your algorithm will say pick this and recurse. Let see what happens when you pick this? So, when I pick this while I will throw away these 2, because these 2 overlap with this. So, now, I am left with imagine that these three intervals have gone. I am left with this side these intervals on this side. You can say that I can pick only one of these all of them pair wise overlap. So, I can pick only one of these I can pick only of these. So, and I have pick any one. It really does not matter all of them have the same degree. So, the solution size of the solution I construct is 3. The number of intervals that I have picked is 3; the first interval that I have picked interval with the smallest overlap and then 2 others after 2 others.
Well, if you stare in this paper a bit more carefully. So, if you stare at this a bit more carefully you can see that there is an optimum which has value 4 1 2 3 4. So, these 4 intervals do not intersect with each other. So, this algorithm produced this I mean 1 2 3 sorry 1 2 3 intervals which do not overlap with each other on the other hand optimum value is 4 1 2 3 4. So, this algorithm also does not work. The algorithm for roughly the same kind of problem as the previous one does not work in this case and you can see that constructing examples become harder and harder. I mean you can come up with really complicated algorithms for which it will be it may not work, but to prove that it does not work you have to really come up with all kind of complicated counter examples. So, now, what do we do what is the algorithm that works? Does any algorithm work? Well, what does the optimum look like? What is the picture of the optimum?
(Refer Slide Time: 43:21)
 
Let us look at this. So, it looks like this. The optimum looks like this. So, any solution must be a set of intervals that do no overlap with each other. So, they have to look like this. Now, the exchange trick what is the exchange trick here what do I mean by exchange trick here? Well, can I add an interval here can I add an interval to this solution well, certainly if I have space. Supposing let us say this is how the solution looks like. I can certainly add one here if there were one. Supposing these spaces are filled out can I add I may not be able to add, but can I add and remove one is that possible? Well, let us see supposing I add let us say add something like this. Now, this could intersect with more than 1 intervals. It can intersect with this it could intersect with well, it may be something like this in which case adding this makes no sense. There is no exchange that sort of you know keep you going I cannot exchange this with some others. I mean if I exchange this then I need to remove more than one and the size of the optimum falls. Is there an interval that I can always add to this optimum solution and maybe we move at most one? Is there some interval means I can always do?
This I can always you know add to this solution that I have and maybe we move at most one. This requires a bit of thought will tell you that there is an interval and this is the interval that ends first. So, let us see what this mean? So, supposing what do I know about the interval that ends first it has to end before this. So, the interval sends somewhere here. This is the interval that ends first. 
Well, now we notice that I can add this and remove at most interval that is the first interval. This will certainly not intersect with the second interval, because the starting time of the second interval comes after ending time of the first interval and ending time of this interval is either this or before. So I can always add the interval that ends first and remove the first interval it maybe. If it ends before ((Refer Time: 46:20) and increasing the size of the solution. So, exchange trick well, exchange trick plus some intuition plus whatever tells us that I can add the interval that ends first you give me any solution. I can add an interval that ends first may be remove the first interval certainly the size does not go down. And this is then this is what, now leads us to the algorithm 3 which actually works.
(Refer Slide Time: 47:00)
 
Algorithm 3 says add interval that ends first remove the intervals that overlap with this and recurse. This is the algorithm 3 and this actually does give us what we want. So, instead of recursing of course, you could sort the algorithm the intervals by ending time so, you can write this as a iterative procedure. You sort the intervals by ending times and pick the first interval the interval that ends first throughout the interval that this overlaps and then continue the iteration. So, this also this works so, why does this work? Actually it is this exchange trick that makes exchange trick that we saw that is what makes it work and it will prove it by induction really.
First thing to prove would be show the interval that there is some optimum solution which contains the interval that ends first. And the proof we have actually seen so, what you do is look at any optimum and look at the interval that ends first go back to previous figure. Supposing this where the optimum solution? Here is supposing it already contain the interval that first we have done. So, there is an optimum solution that contains the interval that ends first otherwise take the interval that ends first it looks like this. It has to end either at the same point or before add this remove the first interval and we now have an optimum solution which contains the interval that ends first. There is some optimum solution that contains the interval that ends first.
So, I pick this as part of my solution. Remove the intervals that overlap with this and now recurse on what remains, and since I am recursing on a smaller set by induction the algorithm will find the optimum in what remains and ((Refer Time: 49:40)). So, when I remove this and when I remove the intervals that overlap the interval that ends first. The size of the optimum falls by at most one that you can see just by looking at this figure that we have. I added this interval to remove this and what remains was this you know the rest of it. It just dropped the optimum drops by 1 by induction I will find something of this size. The algorithm finds something of this size and the algorithm output this plus 1 that is this interval. So, the size of the solution that ((Refer Time: 50:20)) output is the same as the size of the optimum which is what we want.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture ? 11
Greedy Algorithms ? II

We are discussing Greedy Strategies for Algorithm Design. The main principle is to somehow construct solution to your problem piece by piece. We saw two examples, last time. Let us quickly short of revise the main lessons that will learn.
(Refer Slide Time: 01:18)
 
So, greedy strategies for algorithm design, has been reviewed. Main point is, you somehow construct the solution, piece by piece. So, two things are important. One is, how you, what is the next piece of your solution. That is the crucial question that you have to answer. And once, you answer this question, you just put it around in the loop and you get your algorithm.
The other thing we solve, especially with the 2nd problem, 2nd problem was to find the, input was a set of intervals. And you are the output was, maximum number of intervals that do not overlap with each other. Pair wise, they do not overlap with each other. Now, we saw that two obvious strategies for this problem field. One was to pick, smallest interval or the strategy of picking and interval which overlapped, smallest number of other intervals, both of these seen to be natural strategies to consider in this field.
The strategy that worked before last time was to pick first the interval that ends first. To pick an interval that ends first, remove the intervals at overlap with this. And when you request on the rest, again you pick an interval that ends first and so on. So, the set of moral of that story was obvious strategies may not work. What is obvious to one may not be obvious to the other. But, certain obvious strategies, which appear obvious to most normal human beings, may turn out not to work.
So, the point that I want to stress is a proof of correctness is important. You come up with the greedy strategy; you need to argue logically, that this strategy works. So, let me write system, this is important. So, hence a proof of correctness is necessary and important. Because, obvious things do not work, if you are giving in obvious solution to your problem, then you go to say that proof that this obvious solution is indeed correct. So, this is important.
There are usually two ways of writing a proof for these greedy strategies. One is induction. Since, you are constructing the solutions, piece by piece. You find the first piece, argue that this is correct. And then you proof of correctness, usually works by induction on the rest of the input. So, you argue that on the rest of the input, your strategy by induction works.
And added to the first thing that you pick, it gives your optimal solution. This is, what we followed yesterday. Now, this one more way of writing this, which is also quite popular, which I will call exchange. So, this is proof by the exchange techniques. This is very similar to the exchange trick that I have been mentioning throughout. And we look at it the bit more detail.
What we will do is, take up this 2nd problem that you look at. An actually, write the proof derive a proof by using this exchange business, just you get gathered into ((Refer Time: 05:41)). So, here is of the proof by exchange words.
(Refer Slide Time: 05:51)
 
Let us review the algorithm first. It is says, pick, this is among the intervals that remain, the interval that ends first. Discard intervals that overlap with this interval. The whole thing is in a loop. This whole thing is in a loop and you are done, when if discarded all the intervals. We have an empty set. Start out with the set of intervals, do this in a loop till you do not have intervals to deal with. Now, this was the algorithm, we know that, this works. Let us now proof, using this sort of exchange strict that disworks. The proof is actually logically the same, only it is written the bit differently.
(Refer Slide Time: 07:18)
 
How does this work, suppose, so here the proof of correctness. It is the 2nd proof of correctness. Suppose, the algorithm picked intervals of say I 1, I 2, I 3 so on to I k. Let us also, suppose that, they are sorted, say sorted by does not matter, whether it is finishing times or finishing times. Whether, it is sorted by finishing times, they are also sorted by starting time, because these intervals are disjoint. So, the intervals, we look like this.
So, this is I 1, it is I 2, I 3, so on, next I k. So, the intervals are disjoint. So, sorting by finishing times and starting times is the same thing. Suppose, the algorithm, pick these intervals I 1, I 2, I 3 up to I k. Now, we have to somehow of show, that there is an optimum solution, which also picks these intervals. If this is lead to, take the optimum, if this is an optimum solution, which as I 1, I 2, I 3 up to I k, then you are done.
Because, every other interval in the input, must overlapped with at least one of these inputs. The way the algorithm works, it picks an interval, throughout the intervals, which overlap with this. And finally, we have an empty set. So, there is a set of intervals, that we picked as I 1 through I k, it is set. The rest of it has been discarded. Because, they have been, they overlapped with one of these intervals.
So, if any optimum solution has I 1, I 2 to I k in it, then this is it, your optimum must be must have size k. So, let us pick. So, this is what we need to show that, you know optimum, this is an optimum solution. Suppose, it is not so, there is no optimum, which as I 1, I 2, I 3 up to I k. Then, pick the optimum solution, which has maximum number of initial intervals in common with for the algorithm pick.
Let me write this term. So, suppose, so the claim that you want to make is, there is an optimum solution containing I 1, I 2 and so on up to I k. Well, in this case, we just saw that, if contains these, it must be exactly this, which case this is optimum. So, if you prove this claim, we have done.
(Refer Slide Time: 10:44)
 
So, the proof is, why by contradiction, proof of the claim by contradiction. So, we start of by saying, suppose not, which means; suppose the statement is false, there is no optimum solution, which contains I 1, I 2 all the way up to I k. That is, why this is. So, then, pick an optimum solution, which has the maximum number of initial solutions, initial intervals in common with I 1 up to I k.
What do I mean, so there maybe some optimum solutions, which contain I 1, which do not contain I 1. There maybe some, which contain I 1 and I 2, but not I 3. There may be some with contain I 1, I 2, I 3 all the way up to I 17, but not I 18 and so on. So, all these optimum solutions pick 1. So, that the maximum number of initial intervals or in this optimum solution.
So, suppose then, this optimum solution has I 1, I 2, I 3 up to I j. So, this has I 1, I 2 up to I j, but not I j plus 1. So, this says that, there is an optimum solution, which has I 1, I 2 up to I j and no optimum solution has I j plus 1. When, I look at all these optimum solutions, there could be many, none of them has I j plus 1 and then. But, there is one, which as I 1, I 2 up to I j. So, let us look at, just picks this and look at it.
Now, since it has I 1 through I j, every interval, which overlaps with these is not present in the optimum. So, every interval, which overlaps with I 1, I 2 up to I j is not in the optimum present in this optimum. In fact, it is not present, even in the algorithms. So, let us look at I j plus 1.
(Refer Slide Time: 14:20)
 
So, here the optimum, it has I 1, I 2 up to I j. Then, it has some other interval, Kolid interval k, j plus 1th interval in the optimum is k. So, let the j plus 1th interval in the opt solution be k. So, it is look like this and then, there are others. This is of the optimum looks. Now, I j plus, so k looks like this, this is k. Now, the claim is that, I j plus 1, ends before the ending time of k. So, this is the crucial, let me write this down.
The ending time of I j plus 1, this is what the algorithm picked. It is less than equal to ending time of this interval k. Any time of I j plus 1 is less than equal to ending time of k. Well, which means I j plus 1, it must look like this. I have k, this is k and I j plus 1 should look like this. It could be the same as k. But, it is suddenly less than equal to this and I j sits like this. This one, this is the picture that we have.
Now, we notice that, I can put I j plus 1 into this optimum and remove k. So, look at this picture. So, in this picture, your optimum has other intervals k and so on. This is what the optimum looks like. I can insert I j plus 1, the only other interval, it intersect with this k. So, I can insert I j plus 1 and I can remove k. Because, this is the way the algorithm picked I j plus 1 and because, you pick and interval with this smallest ending time. That is the reason the ending time of I j plus 1 is smaller than equal to ending time of k.
So, I can insert I j plus 1, remove k and this is the contradiction. Because, now I have an optimum of the same size, which has I j plus 1, that is the contradiction. Let me just write the term.
(Refer Slide Time: 17:16)
 
So, hence, replacing k with I j plus 1, gives yet another optimum solution, which contains I 1, I 2 up to I j plus 1 now, which is a contradiction; that is it. So, this is the proof by contradiction, proof by this, exchange techniques. By somehow, the exchange what the optimum has, with what the algorithm constructor. As I said earlier, you could write the same proof by induction 2. That is another way of writing the same proof and you can pick, whichever way you want to write, whichever way you like.
So, this was the previous example. Let us look at some more problems. The idea is, to give you a flavor of some kinds of algorithms are problems and algorithms. Some of them are easy and you know, you follow your notes and you get, exactly what you want. Some of them are will like this, one they may be slightly more non-trivial. So, this is just to give you a feel of behinds the things that even do with greedy algorithms.
(Refer Slide Time: 19:17)
 
So, here is the next one, the input set of intervals as in the previous case. The output is, to partition. This set into a minimum number of parts, such that, in each part, you have, only have, non-overlapping intervals. So, this is similar, the input is similar. So, let us go back to the scheduling problem; that we had previously. Previously, you had in the set of intervals. Each interval is associated with the user.
So, the user gives an interval, where she wants to work. And your job is to sort of, you know, satisfy as many users as possible. Only, two users cannot work on this machine at the same time. So, the intervals should not overlap. Now, you still want to schedule them. Only, you have more machines, you have many machines, on the same machine, you cannot schedule to users at the same time. So, I cannot schedule to intervals, which overlapped on the same machine. And I want to minimize the number of machines.
So, users come with these requirements, which is an interval. Each user comes within interval. You have a number of machines. You want to minimize the number of machines that we use. So, which means, have to tell, for each user, which machine, he or she should sit-down. And it should be search that, for each machine, two users, whose times overlapped, should not sit on the same machine.
So, if I look at the schedules for one machine, the intervals should be non-overlapping or disjoint. So, this is the problem, then abstract way the input is the set of intervals and the output is to partition. This set of intervals into minimum number of parts. These are thing of each part is the processor. Within each part, I must only have non-overlapping intervals.
In other words, within each part, I cannot have to intervals at overlap. So, this is the problem; that we would like to solve. So, how this one, go ahead doing this, again the sort of, since we are discussing greedy techniques. I guess, one would like to do it by, you know, this keep partitioning. These intervals putting them in different parts, one by one till you are done. So, look at these intervals in some orders say and you know, this side, which part, which processor that person going to sit down.
Again, the crucial question is, which orders do you look at intervals. You can try ending time, through I mean, since the previous time, in the previous problem, you know, we looked at intervals on based on ending times. For interval at end it first was schedule on, was looked at first in the interval that in the next, we looked at. Since, so on this is the order in which we looked at the intervals. Does this work, I let you figure out in this works.
So, that is an exercise for you, just see, if you can look at intervals based on ending times, when you get a new interval, put in on one of these processors, which processor, well we have to decide that. The set of greedy way of putting, you know, putting these intervals on to processors this. Order these processors 1, 2, 3, 4, 5, 6, 7, 8, 9 and so on. And when you get an interval, put it on the first processor; that you can.
So, there is some interval on the first processor to which this overlaps. Then, I do not put it in and so on. So, I just keep going on to 1, 2, 3, 4 and the first place, where I can put it, I put it. With this sort of greedy back drop, supposing you look at intervals by finishing times, this is what, I want to check, whether this will give you an optimum solution. Well, we look at it differently will first sort by starting times.
We will sort intervals by starting times; we consider an inputs in this order. And what will do is this, when I come to the ith interval, I check, I order these groups 1, 2, 3, 4, 5. I check, whether I can put it into the first group, if I can well and good. If not, I will look at the second group and so on. So, I put it into the first group, where I can, this is the algorithm.
So, let us write this down. Then, we will see, why if at all this is optimum. They does not seem to be any, I mean on the top on set of on the phase of it. There is no real intuition as of now guiding me. Mean, why should I choose intervals, why should I order them by the starting time. I could order them by the ending time; I could have order them by sizes or any other parameter that you may have. But, well, we will do it by starting times, means see what happens.
 (Refer Slide Time: 26:22)
 
So, here is the algorithm, start consider the intervals in the order specified by increasing starting times And so for an interval I, let us say I t. This is the t th interval, when considered in this order. Put it in the first; put it in the first group or part; that you can. So, you consider intervals in this order and when I look at an interval I t, put into the first parts that you can, which means in the first part in which it does not have a non, it does not have an overlapping interval.
If I put it into part L, say; that means, when I look at part 1, part 2, part 3 up to part L minus 1; in all these, there is some interval, which overlaps with I t. So, this is algorithm and why does it work, does it work. Well, it does, the algorithms does work. Now, let us in fact, prove it does works. So, well, you run the algorithms on a set of inputs. So, initially, the first interval goes into the first part. 2nd interval, if it does not overlap, if the first one goes into the first one. If it does overlap, it goes into the 2nd part and so on. So, you just sort of scan.
Your objective is some of to figure out, what happens to this to the number of parts, you are minimizing the number of parts. To supposing the number of parts working, the algorithm produced k parts; it is broken up into k parts. Somehow, we need to argue, I mean, if it is in fact optimum, that any algorithm will produce these k parts.
 (Refer Slide Time: 29:28)
 
So, let us see why? So, let us say suppose the algorithm partitioned a set of intervals into k parts. Somehow, we have to now argue; that any algorithm on this ends set of inputs would use k parts. Why should this be true? Well, let us looks at small values of k. So, often, the other sort of trick, which is never sort of mention in text books is you should always trial small values of these parameters, you see, what really happens.
For instance, if the algorithm took two parts, if it divided into two parts, then you can you can see that, even the optimum will take two parts. Why is that so? Where, look at anything that landed in the 2nd part. Why was this interval put into the second parts? Because, it overlaps with something different parts. Now, if you have two overlapping intervals, you better you two parts.
So, if you two parts, you are through, anybody else in this you know working with the same input will use two parts. What about three parts? Well, what do you know; you know that, if something was put into the 3rd part. It must overlap with some interval in the first part and some interval in the 2nd part. Let us look at this picture.
(Refer Slide Time: 33:48)
 
So, here let us say I have an interval. So, this was put into the 3rd part. This interval was put into the 3rd part, which means, there must be an interval that overlapped, which was the first part and second part. Now, what are the various things that can happen, then I can have, this was in the first part. And let us say, this was in the 2nd part. This is one seen. So, what is the other seen in that possible. Let us say, this was in the first part and let us say, this was in the second part.
Let us look at these 2. Now, in this scenario, I claimed that, any other algorithm will also use three parts, why is that? Well, at this line here, when I look at this line here, all three of them intersects. So, all three of them must be put in different locations in different parts. So, if it is this situation, then we are done, we have doing optimally as usual. Let us look at this situation. Now, in this situation, we cannot see the same thing. And optimum could have up put these two into one part in which case we have ((Refer Time: 33:08)).
Well, can this situation happen, elsewhere, we see, you may progress, because remember we started by, we sorted these by starting times. So, it looks like this cannot happen. This 2nd scenario cannot happen. So, focus on this. So, in this scenario, it looks like this fellow, starts after this, which means it should be consider afterwards. So, this scenario cannot happen.
And in fact, our ordering, when we order, the ordering thing is to in fact, get read of these. So, I guess, you know, how will an algorithm design or go about doing this. Well, you say, let me look at in some order, intervals are looked at in some order. Now, look for a proof, when you look for a proof, you see, how you have to tune your algorithm. So, that the proof works and that is how the algorithm develops.
So, here, we want to look at these intervals in some order and put them into parts, put them into the first parts that it can. We see that, if there is a point, where all of them intersect, when you are through, the only way, this cannot happen is, you know, if you have a figure like that. So, have an interval and have two intervals at do not intersect into different parts.
But, if this happens, one of them must have started after this 3rd interval. One of them must start, after this interval that I am looking at right now. Then, you go back and see, can I get read out this will, can I get read out this in my proof. To get read of this, now you notice that, if you had looked at these intervals in increasing order of starting times, this case will not arise.
And so, in rank to prove, you are actually refining your algorithm, you see that, where you are previous algorithms can go wrong. And you refine this algorithm to take care of this case and you know, hopefully come up with this right algorithm. Before, looking at the problem, where you have a set of intervals and you want to partition, this set into minimum number of parts. So, that each part, you have non-overlapping intervals, if this is the problem.
And the algorithm, we will like to consider is sort in intervals by starting time, look at them in increasing order of sort in staring time. And when you look at an interval, put it into the first part; that you can. The first interval goes into the first part is 2nd interval will go into the 2nd part, if it overlaps with the first and so on. So, this is the algorithm and in fact, you would like to prove that, this algorithm is optimum. So, to prove this, well we say supposing the algorithm produced k parts. We need to show that, any algorithm will produce k parts. So, let us look at this.
(Refer Slide Time: 36:40)
 
Since, there are k parts. Let I be, let us say the first interval, the rest put into the kth part. The first interval does not matter, any interval you can pick from the kth part. But, let us look at the first one, interval that the algorithm put in the kth part. We would like to now argue that, any algorithm will use k parts, why is that. So, here is I, so this my interval I. Well, if I take any other interval, it has to start before I.
Any other interval, which is already been considered. So, if I look at any other interval, that is already being consider just start before I, this much I know. I also know, when that in each of these parts. In the parts, 1, 2, 3, 4 up to k minus 1, first k minus 1 parts, there must be an interval, which intersects with I. How is that look like? So, let us say in the the first part, how can interval will be.
Well, it has to start before this and it has to be intersect this. So, it has to end after this point. So, it has be something like this. It could, where it could start here and it could end after I, that is fine. But, it has to start before this point and it should end after this point. That is the crucial thing. So, these intervals in the first part 2nd part and so on. That I overlap with, must start before this and end after this, which means, this point must be common to all of these intervals.
So, all of these intervals overlap at this point and they must be put into different parts and you have k parts. There are k intervals, which overlap at this point and hence, they must be put into k different parts. So, any algorithm will use k, k different parts, this is the ideal. So, let us write (Refer Time: 39:07)).
(Refer Slide Time: 39:09)
 
So, let I be the interval a b. In every part and this is one less then equal to every part P. So, one less than or equal to P, less than or equal to k minus 1, there is an interval, which overlaps with I, because I was not put into this part, which overlaps with I. Since, I was not put into any of these parts; it was put into k, part k. Now, what can be say, this means this interval, let us say I p.
So, what can we say about I p, well I p starts before a and ends after a. It is starts before a, because I consider the intervals in increasing order of starting times. And it ends after a, because it has to overlap with I. So, this point a, is in all of these intervals. So, point a, is in the intervals, I 1, I 2 up to I k minus 1 and I. So, there are k intervals, which has point a. So, we optimum must have size k. That is the end of the proof.
I am not hiding that last sentence, just fill it in, your optimum must have size k, because of this, because of k intervals, which has a point and all of them. So, the algorithm produces an optimum solution. So, that is what, we want to proof. So, well, what I want to point out with this, this problem is that in the previous case, we looked at them in increasing order of ending times.
And this case, we look at them with increasing order sorting times, you could look at them with decreasing order of ending times and that will work. I will leave it you, again is an exercise. So, I can look at these intervals in decreasing order of ending times and then, do this partitioning and that will also work. Same proof, you just the whole thing is similar. I guess you can see, because I can either look at the intervals from left to right or right to left, it really does not matter.
So, things that work in one problem, you know the ordering, which works in one problem, may not work in some other problem. And this ordering of the input, the order in which you consider the input is a crucial strategy in greedy algorithms. So, remember, when I started, I said there are three set of paradigms, there are three techniques, that is run through all algorithm design, all these algorithms that we design.
One is induction. That is, we can solve it for a smaller problem, how do you extend the solution to a bigger problem. 2nd is ordering the input, look at the input in the right order. And the 3rd was storing old values, some of store value, which may be reusing. In greedy strategy, this greedy strategy that you looked at, you can see that, there are two things induction and ordering, which has so for lead the crucial role.
So, let me go back and tell you, bit more about this exchange trick. Then, we will see a problem, where it couple of problem, where this works beautifully. So, the exchange trick usually works, when your algorithm looks like this. When your problem looks like this.
(Refer Slide Time: 43:37)
 
So, you have a set of elements as input. You want a subset as output. And usually, you want this subset to satisfy some property. So, this is, how the problem looks like, your input is the set of elements and you want a subset of this. So, let us say, that is this, you want subset of this with satisfy some property as output. The first two problems were absolutely like this.
What was your input, initially it was a 3. It is set of vertex and well there are (Refer Time: 44:39)). There is an input with some structure, embedded in. You want a subset of the vertex set. So, that you know they are independent, you want an independent set as output, which is maximum in size. So, the first problem had this property. This 2nd problem also had this property.
You want to set of intervals, you want to subset with maximum size. So, that the intervals do not overlap. Both these problems are this property. The third one was slightly different, you wanted to partition, means and intervals into many parts. So, the first and second fitted into this roughly this frame work. So, what is this exchange trick that I am saying, roughly, it is this.
(Refer Slide Time: 45:37)
 
To supposing you have a solution, supposing you have, let us say candidate. These things are to help you design the algorithms. I am not giving you algorithm as search, but telling you tips as to, how do design this algorithm. So, your candidate algorithm solution is a subset. Now, the exchange trick is this. Supposing, you remove, let us say an element, x from this solution. So, called this T, so this is subset T from T and add, let us say y.
The question you ask you, does the solution improve, we ask this question. So, if we can some more identify, when the solution improves that, you give as a clue as to how to design these algorithm. And that happened in the first two cases. So, let me look, tell you recall your ((Refer Time: 47:05)) memory effect. So, the first case, we set that, if we did not have a leaf. So, I can put this leaf in and I remove something else, solution may improve.
In the 2nd case, I said that, you could pick, you could put, always put the interval that ended first into the solution and remove something else. That is how, we sort of came off to that. Now, we will see examples, where this was, this exchange trick was, the problem is this. So, actually fairly the simple problem and even without this exchange pick should be able to come up with algorithm.
(Refer Slide Time: 47:54)
 
It is again from scheduling. So, you have a set of jobs. The input is set of jobs with processing times t 1, t 2, up to t n. There are n jobs, they have processing times t 1, t 2 up to t n. There is a single processor. You have one processor, you want to schedule these. So, obviously, will schedule them by one by one, there is no preemption. In fact, in all over scheduling problems, that you have taken up so for; there is no preemption.
So, you cannot stop a job in the middle, you lock at the processor. The processor runs to completion. So, you want to schedule this, one processor, no preemption. So, the output in the schedule on one processor, it is no preemption. Well and we beyond want to minimize something, want to we want to minimize, we want to minimize the some of the finishing times. Schedule of one processor, no preemption, minimize the sum of the finishing times.
So, what do I am mean by this, in fact, what do I am mean by schedule. The schedule means, as to tell you, which order you put these jobs on to the processor. So, supposing I put them in order t 1, t 2, t 3 up to t n, this is the order in which I schedule them on to the processor. The finishing time on the first job is t 1. The finishing time of the 2nd job is t 1 plus t 2. The finishing time of the 3rd job is t 1 plus t 2 plus t 3.
The ith job is, t 1 plus t 2 up to t I. This is the finishing time of the ith job. I want to minimize the sum of the finishing times. So, each job as a finishing time, I add up all finishing times. And I want to minimize the some of the finishing times. This is the problem. So, if I just look at this sum, what is this? So just take a minute.
(Refer Slide Time: 51:13)
 
So, the first finishing time f 1 is t 1, f 2 is t 1 plus t 2, so on, f i, the ith finishing time t 1 plus t 2 plus 2 t i. Now, what is sigma f i, f j, let us say j equal to 1 to n. This is what I want to minimize. Well, it is t 1 plus t 1 plus t 2 and so on up to the last 1, f n is t 1 plus t 2 and so on plus t n. It is this some, if you sort of look at this some, it is nothing but n times t 1 plus n minus 1 times t 2 and so on up to plus t n is 1.
And when you look at this sum, immediately realize that I better have this modules job here, sitting here, t 1 should be minimum among the t?s, t 2 should be the max minimum and so on, t n is the maximum time. It is a just look at this, you see that is what, that is the way should be and in fact, the proof is also obvious. And that is the algorithm. So, you just sort of the jobs by increasing times. And that is what; it should be, just by looking at that is a very simple algorithm for this problem. We will look at these problem, again just to sort of pushing in a point, when we start next time. And then we look at you know slightly more complicated problems from the greedy perspective.


Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 12
Greedy Algorithms ? III

(Refer Slide Time: 01:03)
 
Next problem, we want to discuss is called Fractional Knapsack. You want to be correct; you can call it the fractional knapsack problem. Well, imagine that you are a bugler and always wanted to start in algorithms lecture this way. So, here goes, so imagine you are a bugler, it is say successful one at it. You set up to bugler house and you have a bag with you or a knapsack. That is why, the word knapsack comes on.
So, you have a sack with you and you enter the house. So, you are a ((Refer Time: 01:45)). So, you enter the house. Then, you find that, you are really hit the jackpot in the sense that houses all kinds of stuff in it. But, you see your sack has limited capacity. You cannot just take everything away, which is, where in the house. So, the crucial question, there you have to answer at this point is, what you stuff into your knapsack? What do you stuff into your back?
Now, each item as a size and there is some profit associated with it. So, if you sell it, there is some cost of profit, that you some money, that you get out of it. Each item has a profit and size. Now, the total amount of item, set you can take back with you is, restricted by the size of your sack. Your objective is to fill this sack with items, such that, the total profit is maximized.
So, you like to somehow decide, which items to put into this sack. So, that the profit is maximized. Well, this is the knapsack problem without the fractional part. Well, if you put take fractions of each item, then it is called the fraction in knapsack. In the sense that, imagine that, you not bugling an ordinary house, you have bugling cake shops. Then, there are all kinds of cakes, fancy cakes tune around and you can take fractions of each cake and select you cannot eat it.
So, you cannot have the cake and eat it too. So, you have to sell this cakes, you can take a fraction of each cake and you like your job is to sort of choose, those which can fit into your bag and make sure not to squish them. And you will again to maximize profit. So, this is the fractional knapsack problem. So, let us get back to our real occasion, which is designing algorithms and let me define this problem formally.
(Refer Slide Time: 04:08)
 
So, there are n items. So, each item has a profit and size, associated with it. So, this is P i and this is S i. Let us say, 1 less than equal to I, less than equal to n. Each item has a profit P i and size associated with it. You have knapsack of capacity B say, this is input. So, this is the input, which is each item has a profit and size and knapsack of capacity B. And the output is, for each item, you have to say, what fractions of this item, you going to take.
So, you are given the each item, there only unit, one unit of an item, which is available to you. You have to take a fraction of this item. So, each item, you have to say, what fraction. So, that the total size is at most B, you cannot take, more than what knapsack, the knapsack and contain and you want to maximize the profit. So, let us again write this down mathematically. Let me recall, you are given.
(Refer Slide Time: 05:56)
 
So, you are given P 1 through P n, S 1 through S n, these are profit and sizes and B. So, this is the capacity of the knapsack. What do you want is, so you are going to find x 1, x 2 up to x n. That is, what for action of each item, that you want to pick. So, what should be satisfy, well you want maximize the profit. So, for one unit of item 1, the profit is P 1, for x 1 units, it is x 1 times P 1.
For instance, if x 1 was half, the profit work P could be t 1 by 2. So, you want to maximize sigma x i p i, I going from 1 to n. And the total size of these items; that you pick should be at most B. So, that, sigma x i s i is less than equal to B, i equal to 1 to n. And let me remind you, that x i are fraction. So, 0 less than equal to x i, less than equal to 1 for all i.
So, mathematically, this is the problem you want to solve. The input is P 1 through P n, S 1 through S n and B. Output is x 1, x 2 up to x n. You want to maximize this profit function, which is sigma x i p i. And the x i should satisfy these two constraints, which is, first one is this, that the total size that at most B. And the 2nd is, they must live between 0 and 1. So, this is the fractional knapsack problem. This is the problem; that you like to solve.
So, how does one, go about doing this, where they still within the real muff greedy algorithms. So, I guess, one just has to be greedy and pick as many cakes as one can. So, what you like to do is, pick these items 1 by 1. So, that is what a greedy approach to this will be. So, the output is, roughly looks like a subset. It is a fractional subset not quite subset in the real terms. But, still we would like to do this greedily, in the sense that; we would like to pick elements by 1 by 1.
(Refer Slide Time: 08:48)
 
So, we would like to use the greedy approach, which means pick items 1 by 1. So, the crucial question here is, the crucial decision that you want to make is, how do you order items to pick Well, I can come up with all kinds of orderings. You can say, so here is 1, for instance, decreasing profit. So, you could look at items in decreasing profit, according to decreasing profit and pick them, well increasing sizes.
And you could sort of put profit and sizes together into various functions. And let us say, profit by size or profit by size squared, profit squared by size, etcetera, etcetera. And you know, you can ask, which of these work. You can try, all of these, do these work. Well, that is one way of set of approaching this. Try all these, you know various possibilities that go through your hidden, do not go through your hide.
And you know, trying prove that, these possibilities work. Remember that, coming of the possibilities is not too difficult. You just listed out many, coming out with the right kind of order is the crucial, is the sort of key here. And so, how do you do this. One is, when we try the sort of one way is to you know, come up with these things and see, if you can prove one way or the other.
Either come up with examples, so that the strategy works of fails. I mean strategy fails or you try and prove somehow, that this strategy works. What we will like to do is, to use this in defined exchange trick; that we mentioned. And see, we can use act to design, this algorithm. So, that is what we going to do. So, we going to take this exchange trick somehow, you know exchange item.
Basically, put one item in and exchange one item for another and see what happens to the profit. And that, hopefully will guide us, as to how to come up with this order, ordering on a limits. This does not always work, but in cases, where it works, it works. So, let us try this. So, supposing, let me raise, write this term. So, we would like to use the exchange trick here.
(Refer Slide Time: 12:11)
 
So, remember, this is just the way to design the algorithm. This is some kind of trick that you can use and I am not guaranteeing that this will work. Always, it is just hints that I give you. So, try this, what does exchange trick looks like, we would like to exchange, some portion of item i for item j. See, there are two items i and j. I would like to put more of item j and take out some amount of item i.
Then, I will like to see, what happens to the profit. Of course, I have to make sure that, the bag does not bust. So, we can assume that, you know you started out with solution in mind. And you know, you can assume that the bag is full. If it is not full, I can put, you know, I can start filling in arbitrarily some items in the bag is full. So, we take, let us say y 1 of item 1, y 2 of item 2 is so on, y n of item n, such that, the bag is full.
And from now, with this in mind, now I will exchange one item for other for another. So, that the bag still remains full. And then, we will see what happens to the profit. So, that is the sort of calculation that I would like to do. So, will assume y t unit is of item t picked. And so, what is the, we will also assume that sigma y t s t, t going from 1 to n, it is exactly B. So, the bag is full. So, what is the profit, the profit is nothing, but sigma y t p t, t going from 1 to n, this is the profit. Now, what would I like to do with this is, remove some of item, some amount of item or high an increase some amount of item j.
(Refer Slide Time: 15:06)
 
So, in my new thing, initially I had y 1, y 2, somewhere had y i, somewhere had y j, y n. This is what I have. Now, what I would like to do is, make this, let us say y i minus Epsilon, for some small Epsilon. And this, I would like to increase by some amount. Let us say y j plus Epsilon prime. This is what I would like to do, which is I have replaced some portion of the ith element with some of the jth element. I would like to maintain the bag to be full, what this transactions to be such that the bag is still full.
Then, I would like to see what happens to the profit. So, what it mean that the bag is full, it means, S 1, y 1 plus S 2, y 2 and so on, plus S i and y i minus Epsilon plus 1 S j into y j plus Epsilon plus S n y n equal to B. So, this is sigma S i y i sorry, S t, let us say y t, t going from 1 to n, plus S j times, this is Epsilon prime. Epsilon prime minus S i times Epsilon equal to B.
Well, this fellow I know already B, because, initially I started out with the bag being full. So, we have that S j Epsilon prime equals S i Epsilon. If I want the bag to be full, then I guide that Epsilon prime and Epsilon I can choose in the ratio of S j to S i.
(Refer Slide Time: 17:26)
 
So, let us see, what happens to the profit. So, what happens to the profit? So, what is the new profit? The old profit we know. The new profit is, new is P 1, y 1 and so on, plus p i into y i minus Epsilon. So, on plus p j into y j plus Epsilon prime and we go on p n times y n. So, this is nothing, but the old profit which is P 1, y 1 and so on, p i y i etcetera, p j y j plus p n y n plus minus p i time Epsilon plus p j times Epsilon prime.
So, the new profit is nothing, but the old profit minus this plus, that good. So, now the profit, there is a profit increase this is. So, this is what we want. When, is there profit increase, well this fellow is greater than 0, which means p j times Epsilon prime is greater than p i times Epsilon, which means p j by p i is greater than Epsilon by Epsilon prime.
So, there is a profit increase, this is 2. Let me just full back and old slide ((Refer Time: 15:06)) on this is s j times Epsilon prime is s i times Epsilon which means Epsilon by Epsilon prime is s j by s i. Now, so let us just put this bag into that other inequality. We are going to put this equality into other inequality. So, this Epsilon by Epsilon prime is nothing, but s j by s i.
So, there is a profit increase if p j by p i is greater than s j by s i or p j by s j is greater than pi by s i. There is an increase in profit if p j by s j is greater than p i by s i. This tells you, what to favor. I remember this happen, if I replace part of item i with item j. I should favor item j if p j by s j is greater than p i by s i. That is what this calculation tells and that is what my algorithms will give.
(Refer Slide Time: 20:58)
 
So, you can see, how just using the exchange trick. We have actually device an algorithms and we will, in fact, also proof that this algorithm is optimum. So, these score of an item is nothing, but profit divided by size. And the algorithm is simple, you say, the algorithm says order items by let us say decreasing scores. And when pick these items in this order and pick them in this order, till you fill the knapsack. This is the algorithm in one sentence.
So, how will they output look like, well you pick the first item, the one with top score, you pick the item, you pick the next item and so on. And at some point, you know you picked these items and the next item; there may not be enough space in the knapsack. So, you do not pick the entire item, you pick a fraction. So, the first few items, the first, if I pick k items, k minus 1 item will be, I will pick the entire item unit. And the kth item will be a fraction.
So, in my solution, there will be one fraction, which is a last item and the rest of them will be units. That is what the outputs looks like. So, the algorithm, if I look at, let us say x 1, x 2, so on, this is the output. And let us say 1 is the item with maximum score. So, this is items by decreasing scores. This is the maximum score and so on. This is the least score.
Then and this is the output is output. Then, it is of the form, this sequence is and let us say, it is of the form 1, 1, 1 up to some level. And then some let us say delta, which is between 0 and 1, and then 0, 0. This is how the output looks likes. We need to now proof that, this in the works. This algorithm, the output that this algorithm gives you is optimal. So, let us prove that this is optimum.
(Refer Slide Time: 23:51)
 
So, we would like to compare this with any optimum and shows that, this is optimum. This is the, next way we would like to do it and we would like to use an exchange trick again. But, now to prove that it is indeed correct. And we will use it the same way, we use it earlier. So, please set of review that material, if you forgotten. So, the proof goes this way.
So, supposing, so let z 1, z 2, z n be an optimum output. Let me recall that, our outputs looks like this, ((Refer Time: 20:58)) it is x 1, x 2, x n. It looks like 1, till something. These are in decreasing order of scores. Then, you have a delta and then 0?s. This is how our outputs looks like good. So, let z 1, z 2, z n be an optimum output. Again, the items are, the order of the item is at the same.
In both, z 1, the item 1 is an item with the top score. This is the way the optimum picks, if it is of course, 1, 1, 1, and then, delta and 0. Then, you have done, because this exactly equals z exactly equals x. Now, let it is possible that z 1, equal to x 1, z 2, equal to x 2. But, if they are different, they will be first index, where they differ. There will be a first index, where they differ.
So, what you are going to do is, we picked more of these item in the first index exit ith index they differ. Then, here we thick more of I and he picked less of I. Because, we picked one unit of I and he is picked, let say, less than one unit of I. Then, we would like to see what happens. If we increase, we take the optimum increase the ith and we decrease something some other, where which as a lower score, we can do this.
And using the previous sort of calculation that we did during the exchange trick, we will see that the profit goes out. This is the argument. So, let us just formalize. So, let j be the first coordinate, where z j is less than x j. This is also the first coordinate, where they differ. Well, I will let you observed or proof that, this first coordinate, where they differ, z j will be less than x j, it will not be greater than x j.
That is the way, we have picked exist. They are once still some stage and then, there is a delta total sum is B. If I increase any one of them, it is cannot be, the once I cannot increase obviously. And if all the initial things are 1, I cannot increase this delta. Because, then it will be greater than B. So, the first coordinate, where they differ z j will be less than x j.
So, then there is some i, which is greater than j, where z j is greater than x j, z i is greater than x i, some x i, where this is true. Because, the total of both of them is B. Now, what I do is, I increase the profit. So, one can increase the profit here by doing this. So, take z 1, z 2, z j plus Epsilon and so on, z i minus Epsilon and so on z n. So, look at this Epsilon prime, do the same calculation that you did. And you can see, that the profit, remember as an end of it, we came up with the new profit, which is greater than the old profit.
Of course, we need to choose Epsilon and Epsilon prime properly. And I like to look at, I like to do this. So, go back to this old calculation that we did. Figure out, what Epsilon the Epsilon prime should be. I again have enough items to fill enough type size of B. With the profit as strictly increased, which means at the original z 1, z 2, up to z n was not optimum. That is the contradiction.
So, this is, for a contradiction and calculate the profit for a contradiction. I am said towards, that is the end of the proof. I let you sort of frame this better and write it properly. That is one small thing about this proof which I should say, supposing, so let us say that, these items are same score. They all had different scores and then, there is no problem in this proof, it was perfectly.
If the scores are the same, then you need to worry about it. You need to modify the proof slightly. Basically, I could up chose in you know an item with, among items in the same score, I can sort of chose any one of them write. And I may differ, from the optimum in this respect, but it is only in this respect. That part of the proof, I will let you sort of film, but module that, this finishes the proof of correctness of the algorithm to fill up knapsack.
So, again, let me emphasis this, that the right order that we want is to pick to look at the profit by size. This is also; you could up come up with this intuitaly also. You know, your intuition could have told you, in fact, even I mentioned initially, that you look at profit by size, as I measure, which we should follow. But to sort of the way, we got it was just by trying out this exchange idea.
And just exchange two things and we saw what happened and profit by size, came out naturally from this from this exercise. So, that is just see, that is the model that I want to commit you. That often, when you are face the problem this kind. Just do the exchange trick. Run over this exchange tricks; see, what is the quantity that comes out. And that quantity, hopefully, will let you dictate, what your algorithm should be.
The next problem that we look at comes from the area of information transmission and it is related to codes. So, we could like to design codes, which have some properties. That is the next problem.
(Refer Slide Time: 32:50)
 
So, the problem is like this. So, you have symbols x 1, x 2 and so on up to x n. So, you have n symbols, each of them has of frequency f 1, f 2, f n. So, these are symbols that as sent on a channel from let us say place A to place B. These are symbols, which are same from one place to another and this is the frequency at which they are send. And what we would like, so this is the input. So, what we would like is codes c 1, c 2. We would like to code them it is using c 1, c 2 up to c n.
These are, let us say, binary codes. Each of these is a binary string and we would like this binary strings associated with each of these symbols. So, for instance, if I send c 1, c 1, c 2; it means, you have received, you send symbols x 1, x 2; the word x 1, x 1, x 2. So, it sends these symbols, I just send this codes, which a sort of decoded at the other end. And what you send is not symbol at a time, but huge word at a time.
So, you have this huge sort of string, which keeps sending from the left. Another right, you sort of full of those strings and you decode it. So, this code is the output. So, if it not for the frequency, there is nothing much to this. I have n symbols, I just write down; you know and sort strings as scored. Then, I send them out the frequencies that make things are being difficult. So, let us take an example.
So, for instance, I have symbols, whereas, in example, I have let us say A, B and C; free symbols. For frequencies are, 100, 2, which means, it is send 100 times 2nd; B send 2 times the 2nd; C 2 times the 2nd. These are the frequencies that which average frequencies that which a send through. Now, we would like code, these are the frequencies. We would like a code, well here is a good code, a is 0, this is 1 0 and this is 1 1.
So, let us look at this code for A, B and C. Well, I use stands to resend that I would like to use small number of bits for A because, it sends many times. So, I would like to send this smaller number of bits across. So, this has 1 bit, then I have 2 bits for B and 2 bits for C. So, then, it is sort of, I use lesser number of bits, which I transmitted across from from left to right.
Now, this code has another property. So, if I look at the strings of 0?s and 1?s going across, decoding is easy. The reason is this. As soon, supposing the first symbol is 0, I note it is an A for sure. If I hit a 1, if the first symbol is 1, I just look at the next symbol, it is either 0 or 1,depending on whether, it is 0 or 1, I know it is B or C . So, let us do one example here.
(Refer Slide Time: 36:45)
 
So, I have A, B and C, 0, 1 0 and 1 1. Let us take a binary string, let us say 11001000101. So, how do I decoded. So, I am getting the symbols from left to right. I get 1, I check, I know it has to be either an A or B or C. So, this gives me C. I get this 0, this has to be an A. This has to be an A, 1 0 means B, 0 is an A, 0 is an A, 1 0 is a B. 1, well there has to be something else, here let us say 11 and that give you C.
I can just look at this symbol as they arrive the bits as they arrive and decode. Decoding is very easy. This code, which have this actually has a property, which I will tell you, which is that. None of these, is a prefix of another, 0 is not a prefix of any of these and these two are suddenly not prefix of each other. So, such codes are called prefix pre codes.
So, if I have code C 1, C 2, up to C n, C i is not a prefix of C j for any i j, then it is called a prefix pre code. And these, you can see are easy to decode. I just keep looking at the symbols and as soon as I get a code word, I am done. I know, what about is. So, I write the word, I remove these and I keep scanning. So, we would like to construct prefix pre codes.
So, that is an objective, given these symbols, I would like to construct prefix pre codes and given the frequencies. But, each frequency, there will be a, with each alphabet, there is a frequency. I would like prefix pre codes, which minimize the average in your length of the message with send, which means, the length of a message, I will think of, the length frequency times, the length of the code word.
It has the code has some length l. So, the frequency times length is a length is a average length. And the some over all elements I would like to minimize. So, before I state this formally, let us observe that, prefix pre codes have a close connection to binary trees. So, for instance, this code A, B, C, I can represented this way, 0 1, 0 1, A, B, C. So, basically I want to show that, given any prefix pre codes, code, there is a binary tree and given any binary tree like this, I can get a prefix pre codes.
(Refer Slide Time: 40:05)
 
So, what is the secret of this transformation? Well, take any binary tree; the crucial things are code words at leaves, the leaves, where you look at the code word. Left branch is 0 and the right branch is 1. So, when I take the left branch, I think of it has 0, right branch I think of it has 1. And when you reach a leaf, you know, you have traverse a binary string. Now, that will be the code word, associated with that with that symbol.
So, for by go left, left, right, left and there is a there is a code word there, will be 0 0, 1 0. So, given a binary tree, with these code word, at leaves, with these words at leaves. Then, I can assign a code word to each of these symbols. So, maybe, I should say symbols, given a binary tree with symbols at leaves, I can assign a code word to each symbol, using this 0 and 1 business.
You can check that, that this is prefix free. So, why this so, because take any node in the tree. So, this has some binary string, associated with it, which is you followed the path from the root to leaf. And if you go right, you have a 1, if you go left, you have a 0. So, you get, there is a binary string associate. Now, what are the prefix of this binary string? The prefix are exactly those string, which you get at nodes, which are in the path from the root to this node.
If I only take leaf of a binary tree, no tree, no leaf is a prefix of another leaf, no leaf sets on a path from another leaf to a root. Let us say just not on. So, the codes that you get this way of prefix free and given any prefix pre code, you can construct the binary tree. Just the simple fashion you just go 01, 01 at each stage and you follow your follow this 0 and pattern.
And when you hit a pattern, which you want, just remove everything below it and that will be your leaf. I am the word associated there with that will be the code word. Will the symbol, which is associated with that code word. So, these are prefix pre codes, what we want is, you want to construct prefix pre code. Before, this let me just take a new set of paper and write this entire problem properly.
(Refer Slide Time: 43:34)
 
So, the input is frequencies f 1 and so on up to f n. What is the output that we desire? We desire prefix free codes or in other word a binary tree. And we would like to minimize the following function sigma h i f i, I equal to 1 to n. The frequency is come from there. Well, things with large frequencies must be very, must have sort codes, which means they should be close to the root from the tree.
So, h i in fact, is the height of symbol i in the binary tree. So, the input of frequencies f 1 through f 1, f n, I would like to output a binary tree. And with each leaf, I associate one of these works either1, 2, 3 up to n. One of these symbols and I would like to minimize h i f i with the some h i f i, where h i is the height of the symbol I in the binary tree. F i is the frequency of symbol I. So, this is what I want.
Now, how do I go about doing this? So, given a binary tree, supposing that, I give you a binary tree with n leafs. Now, can you assign these words to this leaves. Well, or to minimize this function, when the answer is, it is easy. So, given a given a tree, since I want to minimize the h i f i, I put items with the largest frequency as close to the root. So, basically, I take these leafs, I sort them in decreasing order of height.
Saying increasing order of height, I take frequency, I sort them into decreasing order of frequencies and I just tied them up. So, if the tree is given, then this is easy. The heaviest item of the most frequency must set close to the root and those with whose frequencies of the least will sit for just away from the root in this tree. The question is, what the tree should look like; there are many binary trees with n leaf.
It is a good exercise to see, how many. If there are large number of binaries trees with n leafs, which of these threes we pick. Once, you pick a tree we are done, we know, how to sort of fit these symbols into the leafs. So, how do we pick these trees? Well, do you like to do something some kind of exchange trick here. Even here, what could the exchange pick really mean, you know, you want to come up with the binary tree. And you have these items, which set of these symbols, which are sitting.
What does mean to pick? What is the exchange trick mean? So, what we would like to do is, construct the correct tree. That is, what we want to true, you want to do. And so, what does it mean to do an exchange trick. So, supposing you have some tree, you know that the bottom of the tree, which means node with the longest height. You have the two items with the largest frequency, will sit there. So, this much, we know.
(Refer Slide Time: 48:07)
 
So, we know, we really know some part in the tree, which is, this is a binary tree. Now, the bottom most points, somewhere here it is a bottom most points. I have, once with largest frequencies. The two items with the largest frequencies will sit at the bottom on the tree. The other leafs are somewhere there. This much, I know. So, I know part of the tree.
What about the rest of the tree. Can I sort of somehow compute the tree part by part, well, so that is one thing. What could an exchange trick look like here? What do I exchange items. But, if I exchange items, remember the tree remains the same. So, the trick, the exchange trick that you would like to try is exchange sub trees. So, given a tree, I pick two sub trees and exchange them and see what happens. So, this is the calculation we would like to do. And once we do this calculation, we would like to see, what really happens to the average height. And from this hopefully, will get a clue as to how to construct in the tree.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 13
Greedy Algorithms ? IV

We will look at this problem from information transmission. Let me recall the problem, we had n symbols x 1 through x n. And each at a frequency associated with it, which was the frequency of occurrence. We wanted to construct the code for this set of symbols. So, for each symbol we read the binary string. Now, if the codeword such that ((Refer Time: 01:26)), length of these string, whether same for all of them.
Then, there is really no problem there is we do not have a problem to solve, but if you allow the coding to be such that, these codeword's can have variable length. Then, we do have a problem to solve. Essentially, symbols which appear more frequently, we would like to give a shorter codeword. I just one way to see this is for instance, if you had a large file and you had n words, which these word are repeated.
The all have frequency associated with them, which is nothing but the number of times that the codeword appears in a file, that file. And we would like to encode this file as... So, that in the codeword's are prefix free, which means no codeword is a prefix of another. This is how we want to encode this file and send it across.
Then, the size of the file once encoded the size of the file is nothing but the frequency times for it is some over every word. The frequency of occurrence of the word, times the size of the codeword. And this quantity, which is the size of the file we would like to minimize. So, this is exactly what we had. So, let me put that put it file across.
(Refer Slide Time: 03:08)
 
So, the input, the input is a set of frequencies f 1 through f n. And we would like a prefix free code, this is from yesterday. Well, I just get to this binary tree in a minute. Essentially, we want to prefix free code and you want to minimize the following h i times f i, h i here we say it is the height of the symbol i in the binary tree. But, this is the same as a length of the codeword h i is the same is also length of the code word for symbol i.
So, then this tree is nothing but a size of the file, f i is the number of times the word occurs, this is the length of the curve codeword. So, this is the this quantity is nothing but the number of bits used to encode the file. And you would like to minimize this. So, we also saw that prefix free code of prefix free code corresponds to a binary tree. There is a one to one correspondence.
So, given a binary tree and symbols at each leafs, you can construct the prefix free code. Now, this gives you a prefix free code for the symbols. Essentially, when you traverse right, you think of it as one, if you traverse left on the tree you think of it as a zero. You traverse a path from root to this leaf and that gives you a codeword. Each time you going to right you right a one, each time you going to left to right a zero. And when you end up with leaf this code word associated with it.
Similarly, given prefix free code for n symbols, you can construct the binary tree with n leafs for which this is n or more leafs, which as which the same property. So, essentially if your code codeword is say 1 1 1 1 0. Then, you get to this leaf by traversing right, right, right and then left. That gives you the position, similarly for any binary string, you sort of get two a leaf.
So, you traverse right if it is a one traverse left if it is a zero. And then, gives you part of the binary tree, you can fill up the rest of the binary tree if you want. Since, it is prefix free each time you will end up with leaf. So, all these symbols will sit on leafs. So, you can check this using small example. In fact, I encouraged do it, that will help you understand this better.
So, this is what we want to do. So, we want to output prefix free code or a binary tree, which corresponds to this which minimize this something like this. We saw that once this structure of the binary trees fixed. Then, we know how to associate these words to the binary tree, with two with the smallest frequencies, will see that leaves that two with smallest frequencies will sit at leafs, which are at the bottom 
(Refer Slide Time: 06:43)
 
So, here is my tree, so it is a binary tree. And let us say somewhere here at see it is important, it need not be sort of balance you can even be skew like this. Let us say these two are the bottom most leafs, then I know the these will have the least frequencies. These two the frequencies will be the smallest. Because, take supposing it was not the case and you know somewhere here, you had you had a leaf. Now, the quantity of minimizing is sigma h i f i.
Now, if this ((Refer Time: 07:37)) like closure in the h of this was smaller, h of this was larger. So, let us say this is the h 1 and this is the h 2, h 1 smaller than h 2. H 1 is smaller than h 2, this is lower down and that is higher up. Now, you like to put smaller frequency here. So, that if I had f 1, f 2 supposing, let us say this way f 1 was smaller. Then, I could like f 1 this to sit there and this to sit there. Because, then it will be f 2 h 1 plus f 1 h 2 which is smaller. Then, f 1 h 1 plus f 2 h 2, you can check that this is smaller and that.
So, this is what we want, which means things which are smaller in frequency, must be lower down in the tree. Things, which are larger in frequency should be higher of in that tree, which stands to reason. Because, the larger the frequency, this smaller should be the codeword, which means should be higher up in the tree. So, this much we know... So, given the shape of the tree, we can certainly fill it up with these frequencies, that is easily done.
But, what is the shape of the tree, that is the question that we would like to answer yes. So, that is what we would like to answer and we would like to use this exchange trick. So, what you like to do this? Supposing you are given some tree and you filled it up somehow.
(Refer Slide Time: 09:29)
 
Now, suppose there are these two parts from the root. This is let us say node i, next node j. There is a tree sitting here call it T i and there is a tree sitting here call it T j, here is the root. Now, so what you like to do is exchange these two sub trees. So, remember the or exchange trick was the somehow exchange some part of the output. Supposing, we had some constructed solution, we would like to change twist the solutions slightly and see what happens.
And earlier it was exchanging part to the solution with something, which is outside. Well, here since we are constructing trees, what you like to see is suppose your exchange to sub trees. And what is the result of this action? What happens with this action? What happens to the you know the function we are trying to minimize, which is product to the height times the frequency. So, we would like to exchange these two sub trees and see what really happens.
So, let us do this calculation. So, let us say this is at height, so this let us say is height h, let us say this portion is h prime, this is all the way up to the root. So, the root two node I is height h. So, h is length of path from root to i. H prime is from root 2 j, that is the length of the path from root 2 j at h prime. And we would like to now compute this function. So, see when you exchange these two sub trees, the rest of the elements remain that they are.
So, there contribution to this cost remains the same, we are not changing that at all. So, the anything we need to worry about is the contribution change in the cost, because of the exchange. So, let us calculate the old cost, because of these two sub trees. The new cost because of these two sub trees and we will see what the differences. So, here the old cost.
So, let me do it for T i so for every element e in T i. So, I have the frequency times the length of the part from root 2 that element, which is h length root to i plus the length of the path inside this sub tree. Let me call this l e. L e is the length of the path from i to the element e in T i. So, this is the cost old cost for T i and the old cost for T j is sigma.
Let us say f c h prime plus l prime c and c is in T j the similar cost for T j h prime is this l prime c is the length of the path from j to the element c f c is a frequency. This is the old cost of elements in T i and elements in T j. Similarly, we can write the new cost, let me actually I guess I will have to write this again.
(Refer Slide Time: 13:50)
 
So, let me try and write all this in one. So, here the old cost sigma e belong to T i f e times h plus l e plus sigma c belongs to T j f c times h prime plus l prime this is old, a new cost, well here and just exchanging. Let us go back here ((Refer Time: 14:10)) I am exchanging T i and T j. So, this h will remain the same, but here I will have T j instead of T i sigma e belongs to T i. Now, T i shifted from h to h prime. So, it will be f e times h prime plus l e plus sigma c belongs to T j f c remains the same h plus l prime.
So, this cost this length inside the tree, inside the sub tree remains fixed. But, earlier it was attach to i that is why I add h prime here. Now, it is attach to j, so earlier it was attach to j, that is why it is h prime, now it is attach to i, so it is h. So, this is new cost, so what is difference in cost. So, I want to compute old minus new. So, when I do old minus new, this f e l e cancels in both.
Similarly, f c l primes c cancels, so with f c when I subtract h minus h prime is what remains. So, this is nothing but h minus h prime time's sigma f e plus h prime minus h times sigma f c. So, this is nothing but and this rewriting this h prime minus h times sigma f c minus sigma f e. This is c belong into T j and this is E belong into T i.
So, let us go back from slide h prime I will assume is greater than h ((Refer Time: 16:19)) I have done a shift of this if h prime is greater than h what is the result? So, h prime is greater than h, then this quantity is positive. So, if this quantity is negative, then we have achieve something which is good, the cost as decreased, I mean in the cost as increased. So, this is old minus new. If old minus new is greater than 0 at means the old cost is greater than the new cost, we have decreased the cost.
So, which is what we want, so when is this, this is greater than 0. So, this is greater than 0, then we are in good shape. This fellow is greater than 0, then we have achieve something. So, which means, so let me just write this term, so just conclude.
(Refer Slide Time: 17:33)
 
So, here is i T i and somewhere here is j and T j. If sigma f c c belongs to T j is greater and sigma f e e belong to T i c belongs to T j e belongs to T i this is true is then, exchanging leads to smaller cost. So, we have two sub trees, one is this T i that is T i and that is fellow T j. If the some of the frequencies of the elements in T j is greater than the sum of the frequencies of elements in T I, then if I exchange I get a smaller cost.
Remember, it is just this when I look at a sub tree I need to look at only if some of the frequencies inside, that is what these two things it. Only the some of the frequencies nothing to do the length inside the tree, etcetera. I do not care what this tree looks like. As long as the some of the frequencies of elements inside this sub tree is greater than this I should move it up.
That is what the exchange trick tells us and our algorithm. In fact, we will use this fact. So, when I look at a sub tree the quantity that I am going to keep looking at is the some of the frequencies of elements in this sub tree. Not the average length or anything of that kind. So, this will be the mantra that we will follow during the course of this algorithm.
So, now what so we would like to follow greedy approach, which is we would like to build this sub trees one by one slowly. Initially each element will be a sub tree of it is own. So, I am going to build this sub trees bottom up, slow increase the size of the sub trees that I build slowly. Initially, each element will be a sub tree of it is own this is the bottom thing and I know the first step.
(Refer Slide Time: 20:25)
 
So, if I have let us say I have these elements, let us say this as frequency f 1, f 2, f n minus one and f n. Suppose these are the frequencies and let us say, they are in decreasing order. So, f 1 is greater than equal to f 2 and so on two of the smallest once are f n minus 1 and f n. Then, I know part of the sub tree, I know that my next step I can have f 1, f 2 and so on. I can join the last, this will be a sub tree I know. So, this is f 1 minus 1 and this is f n.
The question is what next? Now, the crucial trick is rather than treat them as leafs, I treat all of them as sub trees. This is one sub tree, this is another sub tree well a leaf is a sub tree. This is sub tree, that is a sub tree, this is sub tree. Now, which are the two sub trees that I want to be at bottom. Remember, the previous exchange thing as the bottom I want the sub tree with whose some of the frequencies of the nodes inside it must be the smallest.
So, I now look at these frequencies I have f 1 f 2 and so on. So, this is f n minus 2. Now, this I treat us one sub tree and it is f n minus 1 plus f n. So, this is my new f prime let say n minus 1. These frequencies remain the same, now this I look upon as a sub tree with this frequency. Among these I chose two of this smallest and I put a new node and join them together and this is my algorithm. So, I just keep doing this as I go up. So, my generic step of the algorithm is this, I have sub trees. So, initially treat each element is treated as sub tree and as I go long I will have may sub trees.
(Refer Slide Time: 22:59)
 
So, the intermediate stage is this I have sub trees, let us say t 1 t 2 so on up to T k. Now, I associate weight to each of them, which is just the some of the frequencies of the elements in side each. So, let us say f 1 prime f 2 prime so on f k prime. So, f i prime is the sum of frequencies of elements in T i or symbols or words. So, just sum of all the frequencies and that is my f i prime.
Now, I pick two of minimum f prime and join them together and make a new sub tree. For instance, if f k prime and f k minus 1 prime where the smallest once. Then, I would take T k minus 1 and T k and join them together. The others remain as they are T k minus 2 and so on. This is if T k and f k prime was the smallest, let us say and f k minus 1 prime was the next smallest.
When, in that iteration I take this as input and I create this. So, number of tree is as decreased by one I have just merge these two sub trees together. The new frequency of this will be just the sum of the frequencies of these two things. This will remain f 1 prime and so on. Because, here you have just the elements of these two just get union.
So, this is the generic step and I put this into a loop. And this is my algorithm, you can see that the algorithm terminates ease to see. Because, initially start with each leaf, each element as a sub tree. So, these will be the leafs and there are n of them, each time the number of sub trees decreases by one. So, finally, I will just have one sub tree, this will be my binary tree, this will be a the binary tree that I want. And this will give me the prefix free code that I am looking for. So, why does this algorithm work. So, let us write a proof of correctness and again it will just invoke the same exchange principle, that done to write a proof. So, here is a proof.
(Refer Slide Time: 26:16)
 
So, what we would like to argue is that, at each stage see at each stage you have ((Refer Time: 26:26)) these sub trees T 1, T 2 up to T k will the proof statement, that we would like is that. At each stage, there is an optimum tree which has the sub trees, that the algorithm constructs as sub trees. So, what does it mean. So, if the algorithm for instance here if the algorithm had sub trees ((Refer Time: 27:19)) T 1 T 2 up to T k, there must be an optimum tree which has these as sub trees.
You do not know how these are connected, but the optimum must have these as sub trees, there must be some optimum tree with these a sub trees. Now, this statement is if you prove this statement, then we are done. Because, once algorithm terminates we just have one tree. And we just said that an optimum must have this as a sub tree, which means optimum must be this tree.
So, if this statement is true for all stages of the algorithm, it should be true when the algorithm terminates. In which case we have what we want, so why is this true. So, again the proof you can write this proof by induction on the stages. So, we would like to prove this maybe I should shift this here. So, proof by induction on stages, the first stage at the beginning.
So, the base case well it is true. Because, what you have as sub trees I just the leaves and these should be leaves even in an optimum tree. The base case you have n elements which are not connected to anything. And this is true even in the optimum case. So, the base case is... So, what is what about the inductive step, well what happened in the inductive step. So, here inductive step.
(Refer Slide Time: 29:13)
 
Well, you started out with trees let us say T 1 up to T k and you ended up with T 1 up to T k minus 2. Two of them let us say the last two you join, this is T k minus 1 and T k this is what the algorithm ((Refer Time: 29:37)). You know by induction, that there is an optimum which at these as sub trees. There is an optimum tree, which at these as sub tree. So, let us look at this optimum. So, by the inductive hypothesis, there is an optimum tree with T 1 up to T k as sub trees.
Now, we would like to argue that there is an optimum, which as all these as sub trees, voice are true well the reason is this. Let us look at this optimum tree, there is an optimum tree let us say call it T let us look at T. Now, among these? So, how this look like. So, it is something like this, then there is a sub tree maybe goes as a sub tree somewhere around this is a sub tree, then the branches of line this maybe there is a sub tree here and so on.
So, there are these sub trees T 1 T 2 up to T k sitting here. So, look at the lowest sub trees. I clean that I can put T k here, the lowest sub tree must contain T k. Now, why is that is so supposing it is not supposing something else it is here. What I do is some T i? What I do is exchange, supposing let us say T k for here and you know T i for here.
Then, I exchange T i and T k I do this sub tree exchange. And by the previous argument that we did, we saw that the cost decreased. When we did the exchange trick and we did the calculation, we saw that the cost actually decreased. So, this should not be possible, in which means in the optimum I should always have T k sitting at the bottom.
Similarly, T k minus 1 must also these sitting at the bottom, which means the height must be again maximum. So, what I can do is take the bottom to nodes, take the bottom to nodes. And I can always exchange whatever sits here, I can exchange it with this should be T k and this as T k minus 1. Remember, that these frequencies are the smallest T k was the smallest and this was the second smallest.
So, I can always exchange any of those trees with these two, these are the bottom most. And I will always get cost which is less. So, I can assume that there is an optimum solution with T k and T k minus 1 and the bottom. Well, now I am done because this structure I just pull out from here. This is T k that T k minus 1 and here is the other nodes. So, this structure I just pull out from here good.
So, there is an optimum tree which as T 1 T k minus 1 and this as sub trees. And we are we are actually done. So, finish the proof that this algorithm is optimum. So, each time you look at you merge to trees, which I have the least weight, the minimum weights of tree and the second minimum. And you create the new tree by merging these two into a binary tree.
So, that is the algorithm and networks, this was done by Huffman and it is called this coding is called Huffman coding. So, let us summarize our discussion of greedy algorithms.
(Refer Slide Time: 33:42)
 
Well, the main thing was we build the solution output, solution piece by piece each time we somehow get the right piece two imagine it is a kickshaw. And you know pulling out pieces to fit. And each time put the right piece and place, that is the crucial thing to help us we used what is called exchange trick.
Essentially start with any solution and change solution to obtain better one. This is just something that you do to figure out, what your algorithm should be doing. This is not what exactly the algorithm does, this is just you help you figure out what the algorithm does, which is you start with any solution. Now, we start of ((Refer Time: 34:50)) the solution a bit and see what happens.
And typically it is exchanging part of the solution for something else. You takes something out of the solution puts something in, you do a better exchange. Then, you see what happens to the profit or the objective function. If it is increases, if it is gets better than some tells you gives your ((Refer Time: 35:14)) how to proceed. So, this is about greedy algorithms I would recommend, that you study metroids, theory of methorids and linear programming, especially the so called primary dual method, which is available in most text books and linear programming. And these two may give you a better sort of feeling for greedy algorithms, in how you know the strategies work.
Good luck.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 14
Pattern Matching ? I

We next look at the problem called pattern matching. This is something I am sure you used before.
(Refer Slide Time: 01:02)
 
So, the problem is this you given as input, text and pattern. And you want to find, if you going to determine if this pattern occurs in the text and where. Output is find an occurrence of the pattern in the text. You could changes to say find all occurrences, it really does not matter for. So, the time being we concentrate on this, we want to find and occurrence of this pattern in the given text.
Grep is something that does it. So, I guess you are used uniques, we use uniques. Then, the command that does is it grep, you can grep for a string in a file. So, intake of file and the string as input. And look for this string in this file and I am sure every other operating system provides this facility, it is a very basic sort of facility, which is provided and which is used quite extensively.
So, what we going to do is design and algorithm for this problem, which is given a text and pattern, find an occurrence of this pattern in the given text. So, what is the first thing, first algorithm that you would write. Well, take the pattern and you sort of look for the pattern at every possible position in the text.
So, the text is say n characters long, you started the ith character in check whether, the pattern. There is an occurrence of pattern starting of the ith character do this for every i. So, for every possible position, you check whether the pattern does. In fact, occur that position you could do this in a loop. So, you can sort of scan the text, character by character starting at you know from left to right say. And for each character from this character onwards is there is a pattern exist. So, you check for this.
(Refer Slide Time: 03:53)
 
So, symbolically let us say this is the text, well that is the pattern I start matching character by character here. If there is a complete match, then I know that the pattern occurs here. If it does not, let us say there is a mismatch somewhere here, then I shift the pattern by one, one character to the right. And now I start comparing from this second character onwards. I check whether the second character, that text equals the first character the pattern etcetera.
So, I check whether the patters occurs here, if it does output. If it is does not I again shift this pattern by one more and so on. So, each time I sort of shift the pattern, along the text. And well when the pattern, when the sort of pattern is at some place let us say here. So, this is the pattern in that is the text, I just check whether the match character to character. I just check that these two character as same. If all of them are the same, then I actually found the pattern here of the text and I can output this index.
This is where the pattern occurs. So, this is sort of a ((Refer Time: 05:19)) and the sort of the first thing that you would do, first algorithm that you would come up with to search for a pattern in text. How much time I guess is the question, that we asking throughout this course and we will ask again.
So, how much time does it takes, first you this can be sort of return in a I mean I hope all of you can write code for this ((Refer Time: 0547)) algorithm is putted in the loop. And you sort of you know move the pattern across the text. So, this is should be able to do and if you doubtful I would suggest you, you just try this before we proceed further. Then, we come back to this question how much time does it take. Well, you compare let us say that of the size of the text.
(Refer Slide Time: 06:19)
 
Let us say the text as n characters and the pattern as m characters, size of the pattern is m, size of the text is n. Then, for each starting point in the text we search for the pattern. So, in the loop there are I have to check for starting with n characters. And each time and the worst case I may make m comparisons. So, the total sort of time is O m n, this is the total time. This should be clear, because for each of these n characters ((Refer Time: 07:11)).
So, I when I start comparing from here I could go you know most of the pattern, I could sort of go down most of the way and then pattern discover a mismatch. In which case, you know the total time taken is o m for each of these indexes and there are o n indexes. So, well the total time is order m n or objective is to can we do faster. So, this is the question that we would like to address.
And in fact, we will do faster. So, by the time this lecture ends. You see that we can do much faster than o m n, that is you have written. To see how we can how do we go about doing this, the first thing do is to check out the few examples, which is what we will do. So, we will take some patterns, some simple pattern we will see how would once search for this pattern in a given text.
(Refer Slide Time: 08:26)
 
So, let us say so here sample 1. So, supposing my pattern is a b to the k, this means it is a b b b there are k occurrences of this. You can take k to be 20 for instance, real exact value of k does not matter. But, the pattern looks like this and your searching for this pattern in a text. So, let us just see what happens here. So, let us say this is the text and you started searching from this point onwards.
So, this is the pattern you sorted searching from here this is a let us say. So, there is a match, then you sort of check maybe, this is also a match. You go down you know of few characters their own matches and then there is a mismatch. So, here I have a mismatch, before the pattern ends, which is the pattern ends here. So, somewhere here is a mismatch, which means this is not a b I know that this is not a b.
Now, what can we do well the ((Refer Time: 09:55)) algorithm you know what ((Refer Time: 09:57)) algorithm have done, it will have shifted this pattern by one character. And then, started comparing, this a would have been compared again this b. You would have immediately got a mismatch, then again go down one more you would have a again have a mismatch.
Because, well all of these are b?s up to here, because the pattern is match. So, for all of these are b's. In fact so all these when I start shifting, all the initial shifts are useless in a sense. Because, when I shift by let us say this amount up to this. I know this a is go to have a mismatch with the b. So, I am going to start shifting again I am not going to find the pattern.
So, the pattern this pattern is suddenly not going to start anywhere here. If at all it is going to start here. So, if there is a mismatch I can for this pattern, I can move this all the way up to here. So, I can start comparing here now. So, I move the pattern all the way here and I start comparing. I do not need to sort of shifted one by one I can shifted all the way here and I start comparing.
So, there was a mismatch at this position I move the pattern all the way and now I start comparing. So, for this pattern this algorithm works. So, I start comparing, you know the text in the pattern, here the text in pattern. And if there is a mismatch in the ith position of the pattern, I just move the pattern all the way up to here. And I start comparing now with the first character of the pattern.
So, essentially we have some more use the factor, you know the text as match the pattern up to here. So, we know that these are all b?s. And my pattern must start with an a. Since, there is a mismatch here, there is a good chance I mean there is some chance that there is an a here. So, we cannot sort of move it even further, this is what we move the text of tool.
So, supposing we follow this algorithm for this pattern, if there is a mismatch I am move it all the way here. And know again start comparing, I compare this with a I go down here. Suppose there is a mismatch I move the pattern all the way here. So, how much time does existing. Well this is the crucial argument, because some such argument will use in general.
When, the time the total number of comparisons by time will just say the total number of comparisons is actually 2 n at most 2 n comparisons now why is that. Well we look at each position of the text I claimed, that for each positional the text. I will make at most two comparisons. Why is that well, let us see we start here for these positions of the text I make only one comparison. Because, once there is a match here I never make anymore comparisons for this position in the text.
If there is a mismatch I may make one more comparison, which is the pattern shifted all the way. Now, I match a with this, if this does not match I shift the pattern and I move ahead. So, for each position in the text I make at most two comparisons. So, the time here the total number of comparisons that I make for this, that I need to make for this pattern this 2 n. So, well there is a ((Refer Time: 13:49)) that maybe we can do faster than m times n. So, let us takes another example. Again any simple example, but this will again illustrate what we are trying to do.
(Refer Slide Time: 14:05)
 
So, example 2, so earlier we took a b to the k that is now take a to the k times a to the k b which is nothing but a, a, a k times followed by b. This is the pattern we are looking for in a text. So, how much time does it take, how many comparisons do we have to make, if we sort of do things not just namely, but in an intelligent way.
So, again let us see what happens. So, here is my text I start comparing, let us say from here. I have done all the way up to here and I am sort of comparing from here. And let us say that is the pattern and I have a mismatch here. So, I have a, a, a here all the way up to b this is the pattern. In the text I know that I have a is here. Here, there is a mismatch, this could be anything. This is the mismatch well what can I do.
So, in the ((Refer Time: 15:28)) sort of thing I would have shifted this by one, this would have shifted by one and I will was started comparing again. And you know that again there will be a mismatch here. There will be matches all over here and there will be a mismatch here. This is known, because I know what the patterns. So, if you want to do it intelligently, you know that starting the pattern anywhere here all the way up to here is useless.
Because, I need you know string of size a is of size there should be k a?s and they do not occur here. This is not an a, so I can in fact, start the pattern right here. So, once as a mismatch I can start comparing right here. So, I can move the pattern all the way after the mismatch. Once, as a mismatch I just move the pattern up to here. So, I know because the reason is I cannot find pattern, anywhere here I have to start here.
Because, pattern has to start with an a and then I need to see k a?s. And I do not see that here, this is not an a. So, again I use some information about the pattern to move the string all the way. There is one more case here, which is when the mismatch is at b. But, before we do that let us maybe do that and then come back and see how much time takes.
(Refer Slide Time: 16:54)
 
So, here is my text now the mismatch is here. So, this is the mismatch, but text matches all the way up to here, except you know there is a mismatch now what do I do. So, now there is a good possibility, that this is a. There is a good possibility that the place, where there is a mismatch, in the text it actually is a. In which case, if this is a b the next thing is a b, then you will miss I mean you cannot move it by more than one.
So, I have to try this the pattern shifted by one. This is quite possible a, a b if there is a mismatch at this position, it is quite possible that the pattern occurs here. So, I can at most shift the pattern by one. So, this looks like you know worst case I cannot shift by more, the pattern by more. This looks like a worst case and be a back to looking something looking at the ((Refer Time: 18:12)) algorithm.
But, while we are shifting the pattern by one, there is something you are gaining. What we are gaining is that, we know that there are up to this position there is a match. While I shift the pattern by one I do not have to again match all of these as I did in the ((Refer Time: 18:31)) case. Remember, then ((Refer Time: 18:35)) algorithm I shifted by one, then I started my comparisons here do this match, do this match, do this match and so on.
Here, I do not have to do that, I can start comparing from here. I shift the pattern by one and then I start comparisons here. So, this is an a if there is a match, then I move up again, this fellow moves up a again this moves. Now, I see whether this is b, if there is a mismatch I am moving the pattern by one, but the text pointer remains where it was. So, the text pointer remains where it was.
So, how many, so I hope if the algorithms in this case is clear it is slightly more complicated than a b to the k. But, I hope the algorithm is clear. So, how much time does it take. So, what is the time, which means how many comparisons do we make in the case of a to the ((Refer Time: 19:35)).
So, let me again let us over this ((Refer Time: 19:37)). Suppose, this a mismatch in one of these positions I move the pattern all the way here. I the move the pattern here, if there is a mismatch at b well I am move the pattern by one by one unit. But, I start comparisons here, this takes with this pattern. Now, of course there could be a mismatch, if there is an mismatch, then I move the pattern all the way up.
If there is a match, then I move this point by one, this point by one and I compare. That is the ((Refer Time: 20:10)) algorithm works. How much time does it takes, well the claim is again for look at each position in the text. By look at this position in the text, then the number of times is compared is at most 2. If there is a match, then the text pointer moves forward. If there is a match text pointer moves forward and that position is never compared again.
So, it at most once a match occurred at a position with text at most once. How many times can a mismatch occur, will again I claimed that the mismatch will occur at most once. Well, if the mismatch occurs at a position, which is not b at one of the a?s, then the text moves forward. So, this is the case, if the mismatch occurs at one of the a?s, then the pattern is moved all the way up here.
And the text pointer, which was here now moves up by 1. If there is a mismatch text pointer moves by 1 I never compare this again, I never compare this character again. If there is a mismatch at b, the text is again here, but the pattern has moved. Now, check what can happen, either there is a match in which case the text moves or there is a mismatch at an a again the text moves.
So, there can be at most two comparisons per text character. So, for each character in the text, there can be at most two comparisons. So, the total time is 2 n number of comparisons. So, number of comparisons at most 2 n. So, in both cases we see, that the number of comparisons is at most 2 n. And this gives as scope, that maybe we can do it you know faster. For instance, it is ((Refer Time: 22:47)) asking can we do it o n time for all patterns.
(Refer Slide Time: 23:00)
 
So, what is the scene, suppose it should be able to do this, what is the scene? Well, the question is... So, if I have this is the text and I have, now this is the pattern. And I match some of them and here I have a mismatch. In each of the earlier cases, we some more used this matched information. This information, that this portion of the string matches with this portion the text was used in both cases.
So, this is what we would like to use. So, can we use the match information, which means the information that this portion of the string actually match this portion of the text can we use this. And this is what we would like to do, somehow use this information. What you would also like to do, like in the previous case is try and shift the pattern as much as possible.
(Refer Slide Time: 24:23)
 
So, here is one goal, so shift pattern as much as possible. So, what does this mean, I mean. So, here is my text we going to see many of these two line highways throughout this, this stock. So, here is my pattern in fact, if you see this figure this is a text in ((Refer Time: 24:54)) pattern I am not going to keep writing this text pattern will miss. Now, let us say this is a mismatch here.
Now, when can I say that moving it by one is not necessary. When can I say that this is useless. Well, this is useless if this portion will not match this portion of the text. If this portion does not match this same portion of the text, there is no point and moving it by one. We in some ways no this portion with text, because it has exactly match this portion of the pattern.
All this I have a match, in the text up to here I have a match this is my first mismatch, which means if this portion of the pattern, does not match with this portion of the pattern. Forget the text for the time being, just look at the pattern. This is the original pattern and here is the pattern shifted by one character. If this portion of the pattern, does not match this portion of the pattern, there is no point and shifting it by one, because if there is a mismatch somewhere here, there is going to be a mismatch there also. How about two characters, again it is a same thing only now I am concerned about this portion of the text, this portion and you see that portion of the pattern. In general, so let me draw this again, well the crucial idea again is that. Once I have matched up to here, then whether to shifted by one or two or three or four it is only something do with the pattern, it has noting do with the text in some sense, because the text is already matched. So, this decision I can make by looking at only the pattern what do I mean by this.
(Refer Slide Time: 27:20)
 
So, here is my pattern. So, thing of the text sitting up there, I have a mismatch here. And let us say, you know the best way to do it is to shift by you know some i units. Essentially, I do not by shifting I do not want to miss and occurrence of the pattern, in the text. So, that is what I do not want to miss, otherwise I would like to shift as much as possible.
So, here is the text, supposing this is shifted by i units here, supposing you know if i is the best you can do, what is this mean? This means, that this portion must match this portion. This portion which is a prefix of the pattern, must match the suffix of this portion of the pattern. This prefix must match this suffix and this must be the largest prefix I cannot have a larger prefix matching this, for instance if instead if I move the pattern by less. Let us say I minus 1, then I have a and there is a match, then I should not you know miss that opportunity. So, what I am saying is this. So, here is my text, so here is the pattern, let us say this is shifted by j, which is less than i, this is j which is less than i. The mismatch is at the same position, this is the same position is that.
Now, I can safely do away with this j's, if this is not a prefix. This prefix is not a suffix of this. So, for all j less than i this prefix is not a suffix of this portion of the pattern. So, this is this portion of the pattern is the same as this, these two are the same. So, any prefix which as larger than this length, larger than this will not match suffix, then I can move it forward.
And essentially I want you know the largest prefix of the pattern, which matches the suffix at this point. So, if there is a mismatch that is what I want. Then, I can move it all the way up to that portion and then start comparing. So, this is what I really want to do. So, this data is only pattern dependent, that is the first thing.
(Refer Slide Time: 31:32)
 
So, this only pattern dependent this is the data, that tells you how much to shift. And what is this data. So, supposing you have a mismatch at the ith position. How much to shift the pattern by that is what we want to know. So, how much to shift the pattern by well.
(Refer Slide Time: 31:45)
 
So, let us look so here is the pattern, this is the ith position I have p 1 through p i minus 1. And supposing I have an optimal shift, which looks like this. So, p 1 so on and this is p i minus, now let us say t minus 1. So, supposing the optimum shift is by t characters, which means if I shift by less than something less than t character. Then, this is not going to work I mean in the sense there will be a mismatch.
So, what is that mean. So, if it is less than t to here is my pattern p 1 to p i minus 1. If I shift with by less than t then there will be a mismatch somewhere here, mismatch if shift is less than t. So, this means that there is no point and time out the shift, I am it as well shifted all the way up to t.
And if the optimum shift is t, this means that you know there is I have a match all the up to here, which means p 1 and so on up to p i minus t minus 1 is a suffix of p 1 to pi minus 1. So, this is the word and that this is the word with i minus 1 characters. And this word is a suffix of this, that is what this shows that these two of the same, these two of the same and so on, and because it is a mismatch somewhere here. We also know that p 1 through p i minus, let us say j is not a suffix of p 1 through p i minus 1 for small for the values is j which are smaller than t minus 1. So, for j less than t minus 1. So, the values of j is less than t minus 1. So, the shift is less than t which means, the shift is less than t. Then, this is not a suffix, when I shifted by t units, then I have a match all the way, which means this prefix of this pattern is also a suffix of p 1 through p i minus 1.
So, given a pattern what we trying to do is for a mismatch. We want to find out how much we can shift the pattern by, we would like to shifted by as much as possible, without missing and occurrence of this pattern in the text, that is what we are doing. So, supposing the... So, let us look at this figure. So, supposing the mismatch is at the ith position in the pattern, you the text sort is to here there is mismatch at this position.
How much should I shift the pattern by. And we saw that, I would shifted by t units, this is t units. If the following thing holds, which is this prefix p 1 through p i minus t minus 1 i minus t plus 1. This prefix is a suffix of this portion and this is the largest such string. If I shifted by less than this, then there will be a position where there is a mismatch, that is what it says. For all smallest shift there will be a mismatch, here I get a perfect match, there is a match between these. So, just says that this is the longest prefix of the string, which is also a suffix of the string. I want actually then to be proper prefix of this, which is the suffix. So, then the crucial thing, the thing that we want to find out this is.
(Refer Slide Time: 37:02)
 
So, the first thing is this quantity is a function of the pattern and can be pre-computed. So, I can pre-compute this value as to for each mismatch how much to shifted by. So, what is this function of the pattern, that I want to compute is for each i, for each position let us say find the largest prefix which is also a suffix. So, I have actually not defined prefix and suffix I hope you know what it is?
So, given a string a prefix is all the initial portions, the initial the first i portion is a prefix. And suffix is just a last portion, this is suffix and this is the prefix. So, given any string this is the prefix and that is the suffix. So, given any string I want to find the largest prefix, let us say largest proper prefix which is also a suffix. Clearly, if I take a string, the string is a prefix of itself and suffix of itself. But, I want the largest proper prefix of a string, which is also a suffix. So, this is what I want to compute. Supposing, I have computed this.
(Refer Slide Time: 39:05)
 
So, I have pattern p 1 through p n is the pattern. Let us say f i is length of the longest prefix of p 1 up to p i minus 2, which is also a suffix of p 1 up to p i minus 1. This p i minus 2 is just to make it proper prefix. So, any prefix of this. So, length of the longest prefix of this, which is also suffix of this I could have written p i minus 1. But, then I will mention the longest proper prefix.
So, supposing I have computed this for each i, for each position in the pattern of computer this f i, which is the length of the longest prefix of p 1 through p i minus 2, which is also a suffix of this, which means. If I take any other prefix which is longer, than f i then it will not be a suffix of this pattern. Now, on mismatch at position i what can I say?
I say, that you can shift the pattern by i minus 1 minus f i. I can shifted by this much, given f i which is defined this way. Then, on mismatch at position i, I can shift the pattern by i minus 1 minus f i. So, let us see this.
(Refer Slide Time: 41:31)
 
So, here is my text, this is the pattern and this is the ith position in the pattern and there is a mismatch. Now I know, so let us see what this is ((Refer Time: 41:58)) this says f i is the length of the longest prefix, which is also suffix. So, let me if I put the term here. So, this is the string that I am considering now. I am only looking at ((Refer Time: 42:15)) p 1 through p i minus 1 the mismatch is at i. But I am looking at p 1 through p i minus 1 good.
So, this is the string I am looking at. And I know that which is the longest prefix of this same string, which is also a suffix. This I know as length l i, which means if I move the pattern all the way up to this position. Then, there is a match here. If there is a match here I know that the text as match everywhere here. So, there is a match in the text I also know, that this is a longest prefix which means if I shifted by any less, there will be a mismatch at some position, which means you have be a mismatch in the text.
So, the among time move is this the among the shift the pattern is this, this is i minus 1. So, among the shift the pattern is i minus 1 minus l i which is what we had f i this is f i. So, I must shift this by i minus 1 minus f i. So, this is what I have said in the previous step. So, once I compute these f i's if there is a mismatch of the ith position of the pattern, if p i mismatches the text. Then, I just shift the pattern by i minus 1 minus f i. I start comparing now. So, let us right this code ((Refer Time: 44:02)) see what this terms out. So, initially I am looking for a pattern in a text.
(Refer Slide Time: 44:12)
 
So, the pointer to the pattern I will refer by x, the pointer to text I refer by y, the variable will be y. Initially my initialization is x is set to 1, y set to 1 and a start comparing. Now, what is the generic step look like. So, if p of x is text at y, this is a text, this is the pattern. If there is a match then what you do? Well, I need to increment x and y by 1 and then continue. Then, x is set to x plus 1, y is set to y plus 1 and I continue, I will have to go back in a loop.
What happens otherwise, if there is a mismatch, then I know that I am a shift the pattern by this much ((Refer Time: 43:35)) I shift the pattern by i minus 1 minus f i. So, this is where the mismatch is... So, this is where my y this pointer on the string is y, the pointer in the text was i here. Now, what do I set it to, I set it to f i plus 1, if it your I initially I set it to f i plus 1. If it your x initially I set it to f i plus 1.
Because, I have moved it by i minus 1 minus f i or i minus 1 minus f x remember my point of ((Refer Time: 46:21)) x here it does not i. But, x if there is a mismatch, I shifted by x minus 1 minus f x. So, the point next character I am going to check in the pattern is nothing but f x plus 1. So, let us do this. So, else x is set to f x plus 1, this is what I want to do.
There just one case that we need to take care of which is, this actually is true when x is not equal to 1. So, this is when x else, if x note equal to 1 then you do this. Now, if x equals 1, what is this mean? Let me the first character of the pattern, there is a mismatch with the text. And comparing the pattern with the text, if x equals to 1 is the first character.
So, the first character of the match pattern mismatches with the text. Then, I just move both pointers I move the pattern by one and I move the text pointer by one. So, let me right this all it is inside else. So, if x equals 1 then y is y plus 1. So, I have I just increment the text pointer by one. So, all this it is inside the else. So, this is inside the else and the whole thing is in a loop.
When do have success, when do I stop. Well, I stop when x becomes size of p plus 1. So, let me write that to success, if x is size of p plus 1 or m plus 1, which means I have match successfully the m positions of the pattern. Then, I have found the pattern in the text, this is the success and the whole thing is in a loop. So, in a while loop you can check whether x is p plus 1 or not. So, this is the procedure this is the algorithm given f x. In this function f x for each f i for each position i. Then, the algorithm for searching, this pattern in the text is this. And now let us see how much time this takes?
(Refer Slide Time: 49:31)
 
So, what is the time, the time let me quickly write the main loop again, it is says if p x equals T y, then x is made x plus 1, y is made y plus 1 else the crucial step is... If x not equal to 1, then x is f x plus 1. So, you shifted the pattern if x equal to 1. On the other hand, then y is y plus 1 in the new loop, this is algorithm and then, you check for success.
So, how much timed does it take, well it is a number of times these statement are executed. So, how many times are these executed? Well, I can count number of comparisons, this will tell me the order of the algorithm. Now, the number of comparisons, that I make totally over is nothing but number of successful comparisons, which is number of matches plus number of mismatches.
So, now let us look at each of these in term, this is very similar to the merge sort. So, I would, once you look at this analysis go back and check out merge sort. So, now what is the number of successful comparisons. Now, each time there is a successful comparison, why increases by one, check out that y increase by 1 y never goes down. The pointer on the texts never goes back always goes forward.
So, the number of comparisons each time y goes up by one it is start set 1 ends up at n. So, this is at most n minus 1, because y this smallest value is 1, the largest value is n. And each time there is a successful comparison it increases by 1. How number of mismatches, well in one case x drops by at least 1, in the other case y increases by 1.
So, if I look at y minus x, if I look at this function y minus x. This always increases, when there is a mismatch. If there is a match y minus x remains the same both of them go up by 1 nothing happens to this. If there is a mismatch, this increases initially this is 0, the largest this can be is is at most n minus 1. So, number of mismatches is also n minus 1.
The other way to see that the number of mismatches at most n minus 1 is that each time there is a mismatch. I shift the pattern by at least one. Remember, there is a mismatch I shift the pattern. So, I shift the pattern by at least one, the number of time I can shift the pattern is at most n minus 1. So, the number of mismatches, number of unsuccessful comparisons is at most n minus 1.
So, the total number of comparisons is twice and minus 1, which is what we had for those small examples. But, this now shows that true for any example. All we need to do now which we do next time is to compute this function f x. Once we compute that we are through.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institution of Technology, Bombay

Lecture ? 15
Pattern Matching ? II

We were discussing Pattern Matching in the last lecture. The problem was given a pattern and text. You want to find let us say the first occurrence of this pattern in the text. And we were on our way to finding linear time algorithm for this problem. So, the first thing was that the notice that, what we would like to do on. So, we start matching the pattern to the text and once this mismatch. We would like to shift the pattern. The worst case was algorithm the proof of the algorithm shifted the pattern by 1. And started comparing from the beginning again. We would like to do much better than this.
(Refer Slide Time: 1:52)
 
And we will let us quickly go away what we did last time. So, you have the text and you have let us say the pattern and let us say you are comparing this position starting from this position. Now, you compare till you find a mismatch. Let us say that is mismatch and your matches is all over this place, what you like to use is the fact that. The text in this portion exactly equally pattern in this portion. This is the information that we would like to use. And on a mismatch we would like to push the pattern as much as possible.
So, we would like to push the pattern the maximum possible without missing and occurrence of the pattern in the text that is the goal. And for this, we set that what we want to know is for each position in the pattern. So, what is the longest prefix of this, this portion of the pattern which is also suffix of this portion of the pattern. So, this is something that we want to compute; because if there is a mismatch and we shift the pattern across.
Let us, say to this position. Then we notice that up to here. This is the prefix of the pattern which is a suffix of this portion of the pattern. There is a mismatch here. So this portion, if this has to match prefix of the pattern has to be equal to the suffix of the pattern. And since, we do not want to miss out and occurrence of the pattern in the text. We find the maximum search, I mean in the maximum search which means a minimum shift which gives you this.
So, we find the maximum length of a prefix of the pattern, which is also a suffix of this portion of the pattern, the mismatch that occurred here. So, this is what we want to find. And the algorithm was once we determined this. This is a function of the pattern and can be pre-computed. So, once this is computed, then the algorithm was clear, I keep matching. Once I have a mismatch I move the pattern over as much as I can. And I start comparing at the same position starting from the same position on the text. The pattern is of course shifted.
(Refer Slide Time: 04:53)
 
And we argue that this let us look at this algorithm once again. This is from yesterdays notes. So, pointer to the text is y the pointer to the pattern is x. The generic step is this you check if these are equal if there is a match. If there is a match then you go on to the next character. You go on to the next character in the pattern and next character in the text. That is what this portion does x is x plus 1 y is y plus 1.
If there is a mismatch ignore the x equal to 1 part. Well if x equal to 1, then you are the first position in the pattern which means your pattern shift and the text also and the pointer on the text also shift. The pattern shifting does not make any changes, because x remains 1. But, y increases by 1, because now we are going to start comparing the next character in the text.
Now, if x is not equal to 1 that is the crucial stack that we set. Then, this is where you shift the pattern why remains the same. So, you start comparing from the mismatch onwards this is what happens ((Refer Time: 1:52)). This y remains the same, but now the pattern shifts. So, initially x was pointing here now x will point here. This is a new x this is the old x this a new x.
And how do you find this new x, new x is nothing but this length plus 1 and that length we have calculated already and which is f x. So, f is an array then you want to write you know f of x. If f is stored in an array, then you would write want to find a prefix? So, this gives you the longest prefix of the pattern up to x minus 1 which is also suffix of the pattern up to x minus 1, only you need the prefix to be a proper prefix.
So, let us say the prefix of p 1 up to p x minus 2 prefix of this which is the suffix of p 1 up to p x minus 1. So that is what we wanted. So, this moves the pattern up that is what this says. And you start comparing again this is in a loop. The question was what is the number of comparisons we make, because this dictate the order of the algorithm.
Now, this I can write as number of successful comparisons in number of unsuccessful, which mean number of times I take the, if else number of times I take the else close. Well in the, if close we notice that each time, this is a match y moves up by 1. The pointer in the text moves by 1 increases. We also notice that the pointer in the text never moves back it always moves forward.
So, the total number of comparisons can be at most n minus 1. Total number of successful comparison can be at most n minus 1, because why increases each time. Now, what about unsuccessful comparisons here? Well again let us look at only x not equal to 1, when x is not equal to 1. Then, the pattern shift the text pointer remain as it is with the pattern shift to the by at least 1 unit.
So, the total number of times a pattern, which shift is at most n minus 1 and well that is why this is also bounded by n minus 1. And y minus x plus 1 actually gives you the position of the head of the pattern. So, if you look at y minus x it points to the position before the pattern. So, x points here y points here, this position is actually y minus x. So, this is giving me y minus x.
So, if I look at y minus x since the pattern shift this will strictly increase. In this case in fact, in both this in that you can see that, this strictly increase. Why increase in the first case the second case. I mean in the first case x decreases in the second case here when x equal to 1 y increases. So, this thing always goes up it never goes down. Even when there is a match it does not go down.
The pattern remains as it is that is what that is what this is. So, this follow initially starts out being 0 and the largest value it can be is n minus 1. So, maybe this is n. So, the largest value is n. Actually, x will be 1. So, this is n minus 1. So, the total number of comparisons is order n, it is let us than equal to 2 n clearly which is what we wanted to show. So once we compute this fx, which is a which is a property of this. If the pattern we can do these pattern matching in linear timing? So, we still left with the problem of computing this function f. So that is something we are to do?
(Refer Slide Time: 10:48)
 
And we will. in fact, do this do it now. So, what is it that we want? So, we have given so input is the pattern p 1 p 2 p 3 p m. It is m characters long now what we want as output is for each i, find the largest prefix largest or longest maybe longest prefix of p 1 p 2 up to p i minus 1, which is also a suffix p 1 p 2 up to pi. So this length, we call say li. And this is nothing but f of i plus 1 from the previous this is the f which was defined earlier. So, for each i for each position I want to find the longest prefix of this string, which is the sub string of this, which is also a suffix of this.
(Refer Slide Time: 12:42)
 
So, again pictorially let me draw this. So, I have this is p 1 p 2 up to pi pi minus 1. So, I want the longest prefix which is suffix of p 1 up to pi which means the longest prefix will look like this p 1 this is pi minus 1. So that the minimum, I have to shift, so that there is a match up to this place. This length is what we want and this length is called li it is also equal to f i plus 1 as of the previous.
So, let us focus on li will not look at fi as if not. So, this is what I want shift the pattern a minimum. So that, there is a match here suffix of this string is the prefix of that string this is a prefix and that this is the suffix. This is what we want this is what we want to compute and this is a proof force way of doing this. So, what the na?ve way, well I just shifted by 1 and c. If it matches shifted by 2 and c if it matches shifted by 3 and stop the first I am I get the complete match.
If it does not match at all the value is 0, this length is 0. I mean if at no stage you are find the match here; that means, this value is 0. So, the na?ve way is for each position up to i, I do these comparisons and it looks like I have to do about for position j I need do i minus j comparisons. So, this will just be sigma i minus j, j going from this is the position j.
So, this is firstly for i going from 1 to m. For each I, now I need to compare for each i I need to start this off from j, j going from 1 to i it is i minus j. So, this is too much well I let you figure out what this sum is, but this is suddenly not linear in the size of the pattern this is too much time.
So, I guess you can write this it needed, but the time taken by this just too much. It is suddenly not linear in the size of the pattern. So, we want o m time. So, this is not so. Well then, what we do? So, we will put over algorithm design sort principles to use some extend. So, we would like we again. So, this is the problem on arrays roughly it is look like a problem on arrays or less.
So, we will use our favorite inductive kind of approach will use our inductive approach and assuming which means assuming that I have d1 it for you first i minus 1 places, how do I compute the ith value? So, the value of l I have computed let us say up to the first I minus 1 places and I want to compute li, l1 l2 up to li minus 1 is d1. So, how do we do this? The question also is can be use this l 1 l 2 up to li minus 1.
So, let us see, what this is? So, you thing is to keep working at the problem and trying and understanding what it really means sometime thing added. Some point and time thinks just start to click.
(Refer Slide Time: 17:30)
 
So, here is my pattern this is the ith thing which I want to calculate li is what I want to calculate. So, what I want is shifted by the least amount. So that, it matches algorithm this matches up to here it all matches upwards, what is this is what I want. So, shifted by least amount or the longest prefix whichever way you want to look at it.
So, the first thing to notice is that, when if this portion matches this portion exactly. This portion up to the previous character also matches this portion. If this big portion matches all the way up to i this portion up to the previous character also matches up to this portion which means if I want a prefix to be a suffix up to i. I want to prefix also to be suffix up to i minus 1.
So, I want that to happen, I know this length is at most l of i minus 1. So, I can start of by shifting. So, I can start by shifting pattern by i minus 1 minus l of i minus 1. These have already computed by induction, I have already computed. So, I do not need to start of here I can start of by shifting it by this much and now I start comparing. I know that there is a match up to this place; I only need to compare these two.
If there is a match here also, then I know what li is just li minus 1 plus 1. If there is no match here, then what we do to shift this pattern further. But again, we use we make use of the fact that if calculate the l calculated l values previously.
(Refer Slide Time: 20:13)
 
So, what happens here? So, here is my pattern. So, here is the same pattern shifted. And this is the ith position, ith position in the top. So, this is something else. Now, this I know is l of i minus 1, this is my first shift day shift this is my first shift. And now I am comparing these two I know that shifting it by any less is useless.
So, shift by this and now I am comparing these two. If there is a match or found li if this does not match. Now what should I do well I need to shift this further. I need to shift this pattern further, why by how much how much do I shifted. Well again we notice that if you shifted more. Then this is the i minus 1 at position, if I look at this pattern. And this I need a match from here. Forget this for the timing focus on these two focus on these two.
Now, I know that these two patterns must match in this portion which means this prefix of the pattern must be a suffix of this part of the portion, this part of the pattern. This index is l of i minus 1. So, this is nothing but l of l of i minus 1. So that is, what this length is. So, let us this is just 1 level of because itself. So let us see what is exactly going on.
So, I have a mismatch here. Now, I want to shift the pattern further. This is the original pattern now I want to shift this pattern. Supposing this is the correct shift after this, this is the correct shift. Now, we observe that this portion of the pattern must match this portion which is what we want the middle part for the timing. This portion must match this portion correct.
Now, put the middle portion back this portion of the middle part match this portion of the text. And so, must match this portion. So, if I look at only the middle part and the bottom part, this portion of the pattern must match write above it. So, if I omit the top part what is see is, this is a similar phenomenon, what I want, is the longest prefix of this portion which is also suffix.
So, this initially this length was i and I got l of i minus 1. Now, this length is l of i minus 1. So, what I get is l of l of i minus 1. So, essentially I use the l values to shifted further that is what this length is. So, I know what the shift is? So, this shift will be nothing but l of i minus 1 minus 1 minus l of l of i minus 1 that is what I shift. Now, again if I find the mismatch again I do the shift only now I work with this rather than l of i minus 1 and so on.
So, you can see even though it is sounds bit complicated see roughly, how we use the previous values of previous l values, to shift the pattern forward. So, let us write what happens well it could so happen that you keep shifting and you shift all the way to 0. So, it will happen that you shift it all the way across, there is no place to you get any you get complete match which case l of i could well turn out to be 0.
(Refer Slide Time: 24:39)
 
So, let us write this algorithm down. So, the position on the on input pattern let us say is i. So, li is what I want to fill l 1 l 2 up to li minus 1, I have filled. So, pointer on the prefix, this is the shifted, this I will call j. So, this is my initial pattern that is i and this will be j. So, initially j for instance here is li minus 1 plus 1, then it becomes l of li minus 1 plus 1. So that is what j the shifted pattern. So, now what do you have, so again it is let us see l 1 is 0. Let us do the initialization first l 1 is 0, i as set to 2 j i set to 1, this is my initialization. So, I have to find l 2 l 3 and so on.
(Refer Slide Time: 26:10)
 
Now if so here is the procedure, if let us say note down separate things and need the space. If pattern at i equals pattern at j. Then, well here there is a match so both pointers move forward. So also, where was this, if there is a match at any time, then I know what the l value is. If there is a match at any time I know what l value is its nothing, but j. So, li equals j, this is the longest prefix that I can now increment i and j.
Now, I want to find it for the next value of i this is inside if else this is the crucial part. Well again, if j is not equal to 1 which means you are not looking at the first character in the pattern in the shifted pattern then what can we say. Then, we just shift the pattern some more and what is the shifted value it is nothing but l of j minus 1 plus 1. So, this is what this is exactly what we had.
So, let me recall this portion it was l of we will looking at i. (Refer Slide Time: 20:13) So, it is l of i minus 1 plus 1. Similarly again here, it will be l of i minus 1 minus 1 l of that is what we want. So, j will be l of j minus 1 plus 1, this is j is not equal to 1. If j equal to 1 this is an exceptional case. Then, which means we are looking at the first we shifted the pattern all the way.
So that, the first character match is looking is matches is below the ith character of the pattern and this does not match which means li must be 0 li equal to 0 and I can move forward. So, i is i plus 1. So, I calculated li and I move forward. This now goes into a loop this thing goes into a loop and that is it. So, we just keep calculating this way. So, what is the time taken? Well again, we break it into two parts 1 is the, if and else the successful match and unsuccessful comparison.
So, pi equals pj and pi is not equal to pj. So, plus number of times you execute the l close. So, how many times do you execute this well again we notice that here I goes up by 1. So, look at I, i goes up by 1 time which is the pattern you fill out this entry. You filled out 1 entry in the pattern is li set to j and i goes up i never decreases anywhere. So, the number of times, this will happen is m. So, each time this happens i is incremented by 1 the largest value of i is n.
So, this is at most m, how about this else clause well either i is incremented or j is decremented which means pattern below its shifts again it is the same argument. So, I look at i minus j the maximum value, this is always incremented here. It remains the same here; this is always incremented in this case. So, the maximum time in the time taken here is m. So, the total time taken by this procedure is twice m. I just realize that I made will small mistake let me correct that.
So, everything was light except this. So, just focus on this. Here I have I wrote this minus 1 which is not true what I said earlier was perfectly correct. The reason the j value is actually n of I minus 1 plus 1. So, what I have here is l of j minus 1. When I take l of j minus 1, now it becomes l of l of i minus 1, this minus 1. Now, the new j value will be this plus 1 and so on. So, what I earlier said was correct, this length there is no minus 1, here this length is exactly l of i minus 1. I am sorry for this for this mistake.
So, let us go back to the algorithm, we just finished looking at ((Refer Time: 32:59)) y the time is twice m. So, in linear time we have actually computed the l values, well if you want to write l as an array I have written as subscripts. I have this is the way I describe this algorithm you can of course, write l of j minus 1.
If you are storing the l values and array and this is what this, how you write it. I just written it as a subscript. So, if there is a match then I have computed the l value for i and i go ahead and I am going to try and compute the l value for i plus 1. That is what this portion says; this portion says if there is a mismatch. Then, I need to shift the pattern. And how much do, I shifted by, what do I shifted by well, that is why this j comes in j i set to l of j minus 1 plus 1.
(Refer Slide Time: 33:06)
 
The reasoning is exactly what we did earlier, because so here is my pattern. The ith position is what I am trying to compare and I have shifted and this is the jth position on this bottom pattern. This is i n that is j and there is a mismatch, how much you are shifted by well, if I shifted by some amount.
So, let us say I shifted by this amount and this is the value then up to here this portion of the pattern matches this portion of the pattern. And I need to shift by minimum. So, that this happens which means this length is nothing but l of j minus 1 this length is l of j minus 1. So, the new j will be here, I am going to compare start comparing this with i. So, this is nothing but l of j minus 1 plus 1 which is exactly what we have. So, if j is not equal to 1 j is l of j minus 1 plus 1. If j is 1 which means I have comparing the first character, if which means it looks like this.
(Refer Slide Time: 34:25)
 
So, let us do 1 figure. So, what happens when so this is i and the pattern actually looks like this j is 1.Now, if there is a mismatch, then I know there is no prefix which is the suffix till of this portion. So, li equal to 0 and I shift I now move everything over. So, I move i 1 and I move the pattern by 1 and I start comparing from this same position. That is what this is the whole thing is in a loop and you end when you finish scanning the entire pattern. The total time is m, because the number of time this is executed is at most m, number of time this is executed is also at most m. So, the total time is 2 m and in 2 m steps, we can compute this length l.
And once we have the length l, we can compute the f that we wanted by just shifting the l values 1 step to the right that is all the risk. Because, fi plus 1 is li and once we have computed f for the pattern. Now, we can scan any text for this pattern using the algorithm that we saw last time. And that also works in linear time.
This algorithm is due to ((Refer Time: 36:57)). This is not actually use in practice an algorithm, even though this is efficient. And algorithm that is found to be more efficient in practice is due to Boyer and Moore and that what is used in ((Refer Time: 36:20). And Boyer Moore, the idea is very similar you want to shift by as much as possible and use the fact that has been a match in some portion of the pattern in text.
The difference between Boyer Moore and this Growth Moore is track is there Boyer Moore starts comparing from the last pattern I mean in the last character of the pattern downwards. Here, you start from the first character of the pattern upwards and Boyer Moore you look at the last character of the pattern downwards that is what they do. But, the concepts are very very similar.
Let us before, we windup this thing. Let us go back and look at those two examples that we looked at write at the beginning. And see what happens to the f function and how this pattern matching algorithm works on that. They were two strings that we worked at we looked at the beginning. So, let us look at them and see what we have.
(Refer Slide Time: 37:30)
 
So, the two strings first string was a b b b this is k times. In other words it is a b to the k. So, what happens in this case? Suppose this is the pattern what we do. So, let what are the l values. So of course, for a it is 0. If I look at a b, what is the longest prefix, which is also a suffix, while if I look at a b the only prefix proper prefix I have is a.
And certainly it is not a suffix. So, this value is also 0, how about a b b? Well the two proper prefix is a and a b and meet the algorithm is the suffix here a is not a suffix ab is also not a suffix I will this matches this does not matches. So, there is no proper prefix of a b b which is also a suffix of this is 0. And. In fact, all of them as zero for n1 of them is that a proper prefix which is also a suffix, so that is the l value. So, f values are also 0.
Now, what is so when there is a mismatch, what is what does 1 shift by let us recall this. So the shift value, if there is a mismatch at the ith position is i minus 1 minus fi. In this case it is i minus 1. If there is mismatch of the ith position I shift the pattern to the right by i minus 1 position this is exactly what we did earlier.
So, we basically shift the pattern all the way up to the position, where this a mismatch almost to the position, where there is a mismatch. So that was for that pattern and this is exactly what we did earlier before we did all this you know analysis about prefix and then suffixes. But, this is the idea that the sort of intuitively used use then.
(Refer Slide Time: 40:07)
 
So, let us look at the second 1 which was a a a b this is k times. The string of a, so this is the second pattern that we looked at. So, what are the l values? Well it is 0 for 1 the first 1 if I look at a a the longest prefix which is also a suffix well the only proper prefix is a that is also a suffix. So, this value is 1, how about a a a? Well the longest prefix which is also a suffix is a a longest proper prefix which is also a suffix is a. So, this value is 2 and so on.
This value will be k minus 1 for this it will be 0. These are the l values what are the f values? Well the f values are just all of these shifted to the right. So, I have 0 0 1 2 so on k minus 2 k minus 1. These are the f values. And if there is a mismatch, then you shifted by i minus 1 minus fi and in this case and this case this will be i minus 1 and fi you can see its nothing but i minus 2. So, this is 1.
So, you actually shift the pattern by just 1 unit which is also what we did there. If there is a mismatch we only shifted it by 1 unit. But, the crucial thing is we start compare now we start comparing where there was a mismatch. We do not go back and compare everything again. This was the other lesion be we had we are learned, we also done this earlier.
That once the pattern once you have matched up to some level of the pattern. We know the previous things are all a?s in this case. Some we do not have to again sort of match all over them. We know that the shift that portion will always match with the previous thing we just need to look at the new position in the text. So that is it for pattern matching from us I would recommend strangely that.
You take more examples and do this yourself take examples of text strings take examples of pattern take these patterns compute the l values compute the f values see, how these values you know sit in. Then, take text and see how the algorithms runs with this pattern and takes what happens to the pointers how the pointers move how do you shift, what exactly this 1 mean by shifting a pattern etcetera, etcetera. So, I would like you to run this algorithm. You let you hopefully understood on a few examples that you can cook up yourself. So that you get a better hang of what exactly is going on.
Thank you.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer science Engineering
Indian Institution of Technology, Bombay

Lecture - 16
Combinatorial Search and Optimization ? I

Welcome to the course on design and analysis of algorithms. Our topic for today is combinatorial search and optimization. The general problem that we are going to address is something like the following.
(Refer Slide Time: 01:27)
 
So, we want to find an object and this will usually be a discrete object say a set of numbers or a graph. And since it is a discrete objects object when we talk about a set of number. We will usually mean set of integers. So, even perhaps set of zero and values. And this object must satisfy certain given constraints.
Find us find an object such that first it satisfies some given constraints. And further more optionally then might be a second requirement that of all objects satisfying those constraints. We want an object of the lease cost, where the cost is a function defined on the objects? Is a function from the space of objects to real numbers?
(Refer Slide Time: 03:24)
 
Optionally, we might also have an alternate definition in which instead of saying we want least cost. So option 2, it could be stated differently instead of asking for the object of least cost we want an object of maximum benefit. Find object of maximum benefit of course in addition to satisfying the given constraints.
Benefit is also a function from the objects to real numbers. Given note initiately that these are not really too distinct for simulations. But, we can set benefit equal to negative of the cost. And then instead of maximizing the benefit it is the same as minimizing the cost. So, this is an abstract definition and let me give you a few examples initiately.
(Refer Slide Time: 05:09)
 
So, let me first start with the familiar example in which there is no objective function given. So, this cost function for example, or this benefit function is also often called an objective function. And so in this first example, we are not given an objective function, but here only given constraints.
So, this example that I am going to talk about this is so called 8 queens problem. Probably, most of what this is so, we are given usual chess board. And we want to place queens on it. So, place 8 queens on usual which is an 8 by 8 chess board such that no 2 queens capture each other or no 2 queens are in the same row same column or same diagonal.
We remainder by this is a problem in the sense, we have described. Well, we can always modulate as problem on numbers an integer. So, we can think of this as find Q 1 Q 2 to Q 8. Let us say Q i represents where in column i the ith queen is going to be placed. So, find these Q such that Q of i is not equal to Q of j. So, interferic Q of i as equal to the row in which the queen in column i is placed.
So, this condition simply says that the green which get placed in the row in column i. Does not have as it is row number value, which is the same as the value in which value which the queen in the jth column is placed. So, in other words the ith queen in the jth queen are not placed in the same row. And we will assume that the ith queen is placed in the ith column and the jth queen is placed in the jth column.
But this is not all. So, we have already taken care of the first condition that they are not placed in the same row. Well, they are not placed in the same column. Because, we are only defining 8 variables and we are saying that k of i is the position. The queen which is going to a fixed in that which is going to be placed in the ith column and we are only giving the row.
Now, we need to take care of the condition that they are not placed in the same diagonal. And this condition is easily taken care of by asserting that Q of i minus Q of j. This is simply the distance along the rows the vertical distance. The distance between the rows and if I take the absolute value it will actually be an unsigned and this should not be equal to i minus j. This is true for all i j distinct.
So, we have now formulated the 8 queens problem as the problem of finding a sequence of numbers Q 1 through Q 8 such that they satisfy these 2 conditions. Well actually these are not 2 conditions between these conditions which apply for all i and j. So, this is say, whatever 8 times 8 upon 2 conditions this is also 8 times 8 upon 2 conditions.
(Refer Slide Time: 09:15)
 
Of course, this is really a toy or game problem, but we can do we can have more serious problems as well. So, let me give a few of those. The second problem that we are going to look at is so called knapsack problem. In this case in fact, we are going to have an object function as well.
((Refer Time: 05:09)) So in this case, our question was to their exist such numbers. That is satisfy these conditions. But, it did not say that if there are several such 8 tuples of number which one we are interested in. We said the just find any such set or any such sequence.
Here, also going to have an objective function. So here as, we are going to have an input and the input consist of say numbers V 1 V 2 all the way to V n. And thing of V i as the value in rupees say of ith object. So implicit is that, there is a set of n objects over here each object has a value and that value is known to us.
In addition each object also has a weight. So, say W 1 W 2 all the W8 W n and say such that W i is the weight saying in kilos in kilo grams of the ith object. And imagine that, we are given a knapsack which has capacity C, C is weight, capacity by weight. The problem should be this manner; we want to determine which of these objects. We should place in the knapsack such that we do not exceed the capacity of the knapsack and such that we are picking up objects of maximum value.
So, we want to find a subset of the object such that value of objects. The total value of selected objects is as large as possible, while capacity while total weight is less than or equal to 0. So although, this problem looks like it arises in the context of objects and values and weights it can also arise and other context. So maybe, there are jobs and their profits associated with the job. And we want to decide which job to select something like that.
So, again there is a constraint over here well first of all there is a combinatorial object that we need to take which is this, which is a subset of the given objects or crops call these objects items. So, we want to pick a subset of these given items. And there is a constraint on them that the weight of the selected items should not be bigger than C that the constraints. And then there is also an objective function.
The objective function is that there value should be as large as possible. So, this is a maximization problem and the objective function given to us can be thought of is a benefit function. Because after all, if we pick as many objects of large value. We are maximizing the benefit to ourselves.
(Refer Slide Time: 13:32)
 
Let me give you one more example, this is another classic example this is so called travelling sales person problem. The input over here is a weighted graph. And the output is a tour through the graph. That visits every vertex exactly once and objective we want to minimize the weight of the edges in the tour.
The names come from the association that we can think of this weighted graph as representing a map. Map of a city for example, I am sorry a map of a country or a map of the world vertices represent towns and edges represents roads and weight represent the length of the edges. So, imagine that there is a sales person who wants to visit every city for his for selling his products.
However, he wants to make sure that he travels as little as possible. So, this is a problem that he might he or she might want to solve she might. So, he would like to find the tour that visits every city exactly once such that the total distance covered along all the edges is as small as possible.
Again on one hand this may seem like a problem relating to sales persons and cities and roads. But, this problem arises in many many contexts. For example this problem arises in robotics; this problem arises in computation of biology and many other fields. Before carrying on let me do an example of this problem just to clarify what is meant over here.
So, let us say that we have four cities. These are our four cities and so this is our graph. So typically, we will have a complete graph over here the graph I am going to draw is going to be a undirected graph. But, this problem makes sense in the directed case as well, so this is the graph. And let us say there are weights over here. So for example, this weight could be 1 this could be 2 say 9. This could be 6 this could be 2 and say this could be 4.
So, there are various ways in which the sales man could tour through this cities. And so, for example, 1 way is this outer most this outer tour, what is the cost of the? Well the cost of that is 1 plus 2 plus 9 plus 4. So that is 16, here is a better tour. So for example, something like this. So, this figure of 8 this tour, what is the cost of that? That is 2 plus 6 plus 2 plus 4. So that is 14. So, this is in fact, a better to over than just the outer tour.
And in fact, you should be able to check that this is the best possible tour. In any case the pointers we want to we want to make sure that we have found the tour which has the minimum such cost at the minimum such length if you like. These are only three examples of the kinds of problems we have in mind. And in fact, later on in the course will be considering more will be considering more problems of a similar kind.
(Refer Slide Time: 18:10)
 
But let me now state the topic for today. The idea today is to look at we want to start a study of strategies for solving combinatorial optimization problems. Combinatorial optimization and search problems, I may drop the search occasionally, but that is what that is imply. Today I am going to talk about a few basic strategies one is so called well one is based so called backtrack search, but it really a sort of a Brute force search strategy.
So, under this we will be looking at what is called backtrack search. Then, we look at the strategy called branch and bound. Will also be looking at a strategy called dynamic programming and in fact, it is this strategy that we want to study the most. Because, this is the strategy which we can analyze and very often it gives us very fast algorithms as well.
But, it requires as considerable about among the diff level analysis. But, I am wanted to start talking about this strategy today. There is also a strategy which is often called greedy strategy which is also going to be discussed on this course. Because, this is also gives us very fast algorithms for solving combinatorial optimization problems.
Let me, point out right away that this strategy the backtrack search strategy is inner most was general. This can be apply to almost any problem. This is the more restricted strategy, but it becomes little bit more efficient than backtrack search problem search strategies. Dynamic programming typically is more efficient than branch and bound. But, it requires as to do a lot more analytical work. We need to actually exploit the problem and it is not always the case the dynamic programming ideas will be applicable.
(Refer Slide Time: 21:02)
 
So, let us come to Brute force search strategies. The name should suggest to you that we are going to do something fairly simple minded. And in fact, let us just catch how you may do this simple minded; we might do something simple minded to make sure. However that we do get an answer correct we correctly.
So, the basic idea as you might guess or as in such as is to systematically somehow a numerate or generate all possible objects which can potentially satisfy the constraints. For each object here is, what we do? Then, we first check whether in fact the constraints are satisfied. If so, we evaluate the cost or benefit function. If the cost is better than the cost so for that is if the cost of this object is less than the cost of the best object so for then we storage.
So, we record cost if necessary. Similarly, if there is a benefit function given to us instead of the cost function. We evaluate the benefit function and check this benefit function with the best benefit function found so for. If this new object has a benefit function which is better than the best of best benefit function founds so for. Then, we record this benefit function and also the current object record cost, if necessary and current object.
So clearly, if we generate all possible objects and we check whether each of them satisfies the conditions this procedure must certainly work. This procedure must will be guaranteed to give us the correct answer. Just because, we are just going over we are doing everything that can possibly have to be done.
(Refer Slide Time: 24:04)
 
A question over is how do we systematically generate all possible objects. And that is where slight amount of cleverness comes in. So, generating all possible objects are how to generate, the basic idea is think of the object that we want as a collection of parts or a collection of slots. Then, consider each possible way of generating each part or if you want to think of this object as a template in which there are slots to be filled.
We want to think of each possible way possible way of filling each slot, how do we go about actually doing this? Well we go over the parts or the slots one after another. So, the basic idea is that initially the sort of have an empty template. So, all?s slots are empty. Then, we fill in the first slot of course; this can be done in several possible ways. So here, we have to consider all the alternatives. Then, we fill in the next slot and again we consider all the alternatives over here and so on.
(Refer Slide Time: 26:17)
 
Let us, take an example and that will make this idea very clear. So, let us take the example of 8 queens, but sense this page is small, let us just make it 4 queens. So, what we have well we have very wells Q of 1 Q of 2 Q of 3 and Q of 4. So, we can think of these four as the smaller parts comprising our big object that we need to find or we can think of this as a template with four slots and it which is given to us and we want to fill in values into the slots.
Instead of looking at these numbers themselves. I am going to think of this I am going to interpret these numbers and pictorially draw the board that corresponds to this. So initially, we are going to start up with an empty template. So that corresponds to a board which is completely empty. Then, we are going to consider the ways in which the first slot in this template can be filled. So Q 1, we said must have a value between 1 and n where n is 4 over here.
And so, there are four ways in which this can be done. So for example, something like this. So, we can placed the first queen in this first position over here. But, this is not the only way in which this slot can be filled. There are many other ways in which the slot can be filled as well. And so, here is another. So from this empty template, we can come to this way this possible way of filling the first slot. That is that we placed the first queen in the second row.
Of course, it is not necessary that we placed in the second row, but we might place it in the third row as well. So, this is that corresponding position and of course, we can place it in the fourth row. I am not drawing the entire board. But, I am just telling you where the queen is going to be placed. So, these are all possible ways in which the first queen is placed first queen can be placed.
And we started out with the empty board and at the end of one way of one considering how the first slot can be filled. We have arrived at four possible positions four possible candidate objects each of edges only partially built. So, we started up with one partially built object candidate object and now we have four partially built candidate objects. But, they have been they have been extended.
So, we started up this object and these are object which are full arrange substance or more build up than this previous object. But, we can keep going in this manner. So from this point, we will again consider all possible ways of placing the second queen. The first queen is placed over here, the second queen could also be placed in the same row or it could be placed in the next row or the row after that or finally, in the last row.
So, when we come down to this level. We have our object even more fully constructed. So, we have placed 2 queens, we are not placed all of them here. But, this is at a higher level of construction higher level completion than this one which internal at higher level of completion and this one. And of course, each of these objects will have to be constructed further.
So, there will be something over here and meet everything which represents the ways of placing the third queen. And then and read, this there will be something which represents the ways of placing the fourth queen. In this case, we can see what is going to happen. So, we started with one node, then we have four nodes we have four bolts in this next level.
Then, we have a 4 square bolts to be consider at this level 4 square partially constructed objects. Then, we have a 4 cube partially constructed objects at the next level 4 times as many as the when the previous level. And finally, 4 to the 4 objects at the leaf level, in fact, I have already told you what this looks like in some sense. So, we have leaves at the bottom, we have root at the top and in fact, this is going to look like a tree.
In fact, in almost every in every exhaustive generation method we will be following something like this. So, we start of in the empty object, we keep on extending it. So, the extension will correspond to children of the parent node. Their extensions will correspond to object which are even fuller. And until we get to a point at which we cannot extend object any further. That is the point at which we have got to leaves and that is the point at which we can terminate our generation step.
Once, we get to a leaf in this case for example, we know that we have all the queens completely placed. So, what is the next step? Well at this point we are going to check if the queen position satisfy our conditions. If they do then in this case, we can just report we can just print out saying yes they do in fact satisfy all the conditions. If they do not well, then we have to consider the other leaves.
This is as for as considering problems in which there is no objective function. If there is an objective function then when we get down to this level, we need to not only consider whether the leaf the leaf objects satisfy the constraints. But, we also need to evaluate the cost function. So suppose in fact, we have a cost function rather than a benefit function. This for each leaves, we can think of the cost function. And we can evaluate the cost function and our problem is to find a leaf such that it is satisfies all the constraints and such that whose cost functions is as small as possible.
(Refer Slide Time: 33:07)
 
So, let me, said as summarize what I said in terms of a programming idea. So, first we need something so that we represent our objects properly. So for example, in the case of the 8 queens problem, where the four queens problem? We used a data structure of an array of four element array to represent the four queen positions.
In case of the travelling salesman problem of the travelling salesperson problem, we want to represent the tour again if the graph as n vertices we could use an array of length n, where the ith element of the array denotes the ith city in the tour. In the case of the knapsack problem is simply need to say whether a certain object has been selected or not. So, corresponding to each object we could have a bit which is equal to 0 meaning that object is not replaced in the knapsack or if that bit is 1, then the object is replaced in the knapsack.
So some of the other, we need to have a representation of the object that we are looking for. After that, we need a procedure for filling in the next empty slot in the object. Of course, there will be many ways in which this can be done. And so, this procedure should allow us to provide all possible ways. At this point in the algorithm, we are simply going to recurse. So then, we will recurse and we will recursively fill in the sub sequential loss, what happens next? When we have recurse we will it is prevalent to starting at say the root.
And we filled in the object filled in the first slot and then when we recurse we filled in the next slot and so on. Eventually, we will get the leaves. So viral you need to have a check over here, just to see whether we have in the leaves. So for leaf objects, we need a procedure that checks constraints. I am it written true if all constraints are met it written false if the constraints are not met.
We also need a procedure for evaluating cost. That is not all however. So this way, we keep going down and we go down to a leaf. But, you also want to go back up, and go to the other leafs and that is why this whole procedure is called back track search. So, we get to the leaf and then we go back up again in the tree. And then, we go to the next leafs then from that leaf, we go back up again.
(Refer Slide Time: 37:10)
 
So over here, we said that we need a procedure for filling in the next empty slot in the object. But similarly, we need a procedure for removing the last value filled in the last slot. This is how we will go back up into the tree and make sure that all the objects will get generated.
(Refer Slide Time: 37:50)
 
So, let me try to write this as a very vaguely as a procedure just give you the idea of what is going on. So, our search procedure is going to look something like this so it going to take this candidate objects as an argument. And let me, just note that this while have a return while have return on object every here I really mean this template which could be partially filled.
So, what does this do? So, if object has all slots filled which is to say that it is a leaf. Then, we are going to check constraints. We are going to evaluate the objective function. If constraints satisfied and in fact we will return the value, if the constraints are not satisfied we will return. So, if the constraints are not satisfied, then will return infinity if we are given a cost function. That is to say that this leaf found is to be disorder. Infinitive will just say that this is a useless leaf.
If you return a minus infinitive or a zero if this is a benefit function. Assuming that the benefit is positive we can even return zero. So this is what we do at a leaf? If object has unfiled slots then what we need to do? Well we need to do this branching business.
(Refer Slide Time: 40:09)
 
So, this has an unfilled slot. So, we need to filled for this slots, if object has unfilled slots. We are going to consider all possible ways of filling slots filling next slot. So, you will fill in ith possible manner. We will recurse on modified object. And this recursion will give as value. So, let me call this V sub i and this will be in the end of the loop. So, this think of this consider as being a loop.
And now the idea is something like this. So, we are going to be go over all possible leaves underneath. So, starting at this or starting at this we will be visiting this will be visiting this will be visiting this will be visiting this will be visiting this. And we are going to associate with this node a cost function which is the value of the smallest cost function found anywhere over here. That way, we can written that and eventually will make sure that the smallest cost gets written to the top.
(Refer Slide Time: 42:13)
 
So, here we are going to be return in smallest V sub i calculated above. So let me, do this again. So, we start up with an empty object, then we consider all possible ways of doing it. But really in the first instance, we just consider we just go along this path. Then, we go along this path then we go along this path and so on until we get to a leaf. Say let us say we get to a leaf over here.
Then, we return the value of the cost as seen at this leaf. Then, at this point we consider other waste of generating it and we get to another leaf for example, then we return this cost. Then, we go back again then we return the cost over here. And let us say there are no more ways of filing in that last slot. Then at that point what gets return over here is the smallest of these cost values.
So, in general in this manner, what is going to get return to the root, is going to be the value generated at along some path in this graph. And that is going to be the least cost leaf the value of the least cost leaf. So, this complete at description of this search procedure. In fact, if you have studied this in the data structures scores, what you have done really is a depth first search over this search space. Search space is simply set of leafs. We are looking at all the leaves somehow the other. And so that is of called search space. And what we are doing is? We are putting a tree on top of that search space and we are just doing a depth first search of it.
(Refer Slide Time: 44:04)
 
There are some improvements possible to procedure. We do not need to do our condition with checks our constraints checks at the very end. We can do them as early as possible. So for example, in case of the 4 queens problem. Suppose, we placed the first queen in this first column in this first row then when we placed the second queen in the first row as well. We could say look these 2 queens already capture in each other. Or the condition that Q of 1 equals to Q 2 to not equal to Q 2 to is not being made.
And therefore, we can say we do not really need to go and further check where condition where queens three and four placed. So, what happen to be here was that we started up with empty board. Then we placed the first wheel and then we placed the second wheel as well. And then, we can safely ignore this part of the search tree immediately.
Because, we have checking the condition right here and this condition which we check should be something which will be valid no matter what will be get over here. And in fact, that is true, these 2 queens are not going to change their position at any of these leaves at these leaves only these queens will get placed. And therefore, this condition ((Refer Time: 45:18)). So, the possibility of early conditional checks condition checks can improve up on efficiency.
(Refer Slide Time: 45:36)
 
I would also like to note that this procedure is; obviously, correct. And that really follows from the fact that we are considering all possible ways of filling each slot.
(Refer Slide Time: 45:56)
 
Let we go to our second example which is the travelling salesperson. So, I am just going to give a quick description of what this search tree is going to look like. So at the root, we are going to have the empty tour. So, let us see this is our graph. Then, we can say that let us say just designates this vertex of the starting vertex it does not really matters. Since, we are going to come back to vertex anyway. Since, the tour is cyclic.
But suppose, this is the starting vertex, then we can ask how many ways are there in which the tour we can go out of this vertex. And that gives us that is sort of closing in the first slot in the tour. So for example, we can come to this point at which the tour has proceeded along this edge or we can come to this point at which the tour has proceeded down diagonally or we can come to this point at which the tour has gone now. Then this tour can further we extended over here. So starting at this, we can go down directly or it can go back diagonally and so on. So, eventually we will get to a leaf where will be having complete tour.
 (Refer Slide Time: 47:29)
 
So, let me say something about the running time of backtrack search. So basically, the idea is that we are going to look at every possible leaf. So, the time taken will be will essentially have to be at least in the worst case have to be the number of leaves. So, let us consider this for the case of the n queens problem. Specialized generalized it 2 n queens rather than 8 queens or 4 queens.
So in this case, how many leaves to be have? Well the first queen can be placed in n ways the second can be placed in n ways and so on. And so, there are n to the power n leaves and so time is at least n to the power n. If you implement the early checks, then the early checks will improve the situation some part. And in fact, a simple very simple early checks is to say let me make sure that this new queen is placed in a different row from all the previous queens.
So here, our output will simply be a permutation on the numbers 1 through n. And so, here there will be n factorial leaves. So, the time is going to be omega n factorial. This is still to still quite large, what about the TSP, how many tours do you have, through n cities. Well starting at any city the first city you can be placed can be picked in n minus 1 factorial ways. Then, n minus 1 ways then the next n minus 2 ways and so on and therefore, we will have n minus 1 factorial different leaves in this search space.
So simply put, there are three ways of getting to the first the vertex of the first level. The next there are two children and in this case there will be 1 child and so on. But in general, if you start you can n city tour there will be n minus 1 children of the first level n minus 2 children at the second level for each of the vertices in the previous level. So therefore, we will have n minus 1 differently used and again here the time will be omega n factorial.
(Refer Slide Time: 50:10)
 
For the knapsack problem, it should be possible for you to improve that there will be 2 to the n leaves corresponding to all possible ways for mean subsets of n objects.
(Refer Slide Time: 52:00)
 
So, let me summarize this co situation. So, what have you done? So for, we have looked at very general method. It is Brute force in that it generates every possible object and it does not try to be two efficient or two clever in saying look I do not. I am not going to generate this class of objects, because this is not going to be useful to me.
And typically it takes long time. In fact, typically takes time exp1ntial in the size of the object. So either, due to again or n factorial or n to the n or something like that and we have defined terms likes search space and tree that set on it as a search tree. And let me end this lecture by mentioning one more term which is combinatorial optimization combinatorial explosion. This term is commonly used in connection the back track search.
The idea is that at the first level the tree might look manage level, because it has a small number of children. And the second level the number of children the grand children of the root will grow. At the third level the number of children will be a further. And eventually even you get believes you will have many, many, many children or many many, many vertices. And so, enzymes your tree will explore and this think and this phenomena is commonly called combinatorial explosion.
(Refer Slide Time: 52:40)
 
So let me, just look at what we are going to do next. We are going to study improvements to backtrack search. As mention before we are going to study a technique called branch and bound. And then we will look at dynamic programming and greedy strategies. And this is really the focus of this course, but it is important to view these as part of a grand picture in which sort of the simplest backtrack search ideas are present as well as these more sophisticated strategies are present.
We should also note that the backtrack search is a very, very general strategy, whereas these are very, very sophisticated. But very specialized strategies, these are likely to be fast as we will see in the next lectures. This is likely to be slow, but it is likely to ((Refer Time: 53:52)) It is going to very, very general and applicable in almost any combinatorial optimization problem.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture ? 17
Combinatorial Search and Optimization ? II

Welcome to the course on Design & Analysis of Algorithms. Our topic today is Combinatorial Optimization and Search. And this is going to the second lecture, on this topic. Let me start by summarizing, what we did last time.
(Refer Slide Time: 01:09)
 
In the last lecture, we gave examples of what we mean by combinatorial search. And then we discussed, a fairly general technique called backtrack search. This is sort of the obvious idea, just executed systematically. So, let me just summarize by saying that it is a general technique. It will work essentially for every combinatorial optimization problem or combinatorial search problem. And the basic idea, over here is to systematically try out all possibilities.
The problem of course, with these techniques was so called combinatorial explosion. As we explained last time, this just means that the number of all possibilities, which we are going to trial is really large. And therefore, this technique typically takes a lot of time. However for many problems, this is the only way to go. Our topic today is to consider cases, in which we can do better. So, today?s topic is going to be how to improve upon backtrack search.
(Refer Slide Time: 02:50)
 
And specifically, we are going to see a variation called branch and bound. This is going to be the main topic. But, we will eventually also look at other ideas called dynamic programming and also greedy algorithms, which are also used to solve combinatorial optimization problems. But this will come later, all this will come later. And we will deal with dynamic programming, as well as greedy algorithms are quite some length. And today, we will mostly focus on branch and bound. So, the main question is, we know how to do backtrack search and we will review it, in a minute. How do we make it more efficient that remains the main question. So, let me start by reviewing what backtrack search is?
(Refer Slide Time: 03:57)
 
So, the idea over here is that we are expected to find the combinatorial object. Let us call it some X star, which satisfy certain constraints. So, we know all the constraints and we require that the combinatorial object X star must satisfy all those constraints. And further there is cost function, which is also given for these objects. And we are, we need to minimize the cost. So, there is a set of possibilities for X star and somehow, we have to be sure that we have found the one with the least cost among them.
And furthermore of those, we first once which satisfy the constraints; and from those we find the one which minimize the cost. So, last time we said that a good way of organizing, this search is to start up with an object techniques called X. X is really not an object in the sense that we have defined over here, X is sort of a template. So, X think of X as consisting of slots, which need to be filled.
The first slot of X can be filled in several ways. And corresponding to that will have branches going out of this node. So, the first way of filling the first slot, will give me a partially constructed object, let me call it X 1. The second way of filling, this first slot will give me partially constructed object X 2 and so on. So, the first slot can be filled in many, many ways. And I will get, different partially constructed objects based on; however, choose to fill that slot.
I can keep going in this manner say from X 1 or from any of these. Now, I take any of these partially constructed objects. And from these, I am going to filled the second slot. So, let me denote this by calling this new object X 1 2. Well it is not a fully constructed object yet X 1 1. But two of its slots have been filled. The first slot has been filled, in the first possible way. The second here, has been filled in the first possible way as well.
But, the second slot could also be filled in another way. And that another way, could be written down in this manner. So, in this manner I keep going and eventually, I will get the complete object constructed over here. Of course, when I get the complete object constructed, I have to stop. So, at this point, when I get to the leaves of this tree I have to stop and at each leaf I am going to make sure that may constraints are satisfy.
And then I will evaluate the cost function, given to me and then from that, I will pick the object with the minimum cost. So, let us say the minimum cost object X star appear somewhere over here and that is how I am going to find it. And that is the way that I am going to written. We said, last time as well that this is a tree, which is often called the search tree, this tree which is the search tree is explored in a depth first manner.
So, from X we generate, we fill the first slot. And we generate X 1, from X 1 we filled the second slot and which of the rate X 1 1 and so on, till we get to the end. At this point, we check the constraints, we evaluate the cost if necessary. And then we go back and then we consider the second way, of the next possible way of filling, the next possible slot or the same slot. And then we go down in the tree, but in the different direction now.
And. So, whenever we come to the leaf, we go back. And in this manner, we go through the entire tree, going back and forth and this back, this going back gives a name backtrack search for this method. So, we will have to explore, the entire tree and in order to be sure that we have found the minimum cost leaf extra. So, this is the organization of backtrack search.
Although, I have drawn the tree in parallel, I have drawn the entire tree for you. Remember that the way a program would execute this procedure. It will start with the top node under it and will go along some particular path, then come back, then go down again. So, at any point and time, it will only be having certain path in this entire search tree. And it will either go down that path or it will roll back that path. And then take a different path that is how the algorithm will execute.
So, this is the search tree that we mentioned and now the natural question is, do we need to explore the entire tree.
(Refer Slide Time: 09:36)
 
This is an extremely important question. We said that there is combinatorial explosion and that because the number of leaves is a huge number, the leaf coming back to this picture. At every time as we go down the tree, the numbers of nodes doubles are get multiplied by a large number as we go down. And therefore, the number of leaves is enormous. So, suppose it was possible for as to say, that look anything that is underneath these X 2, really does not need to be explored and without even seeing, anything underneath it, If we could do that, then we will be saving ourselves a lot of work.
And so the idea that we are looking for over here is called pruning. So, another way of expressing the same version is, can we prune the search tree. So, if we could do that, then you reduce our work and then we would have, a more efficient way of searching.
(Refer Slide Time: 11:11)
 
So, the branch and bound is just one possible heuristic of pruning, is a pruning heuristic. So, let us come back to this picture here again.(Refer Time: 11:28) So, let us say that we have explored this whole thing, we found certain solution X over here and we evaluated its cost and this cost turned out to be some C of X. So, we went down along the left branch in the tree and we found, a certain C of X.
Now, suppose this cost function C can be used to evaluate the cost of X, which is a complete object or let me call this say some X bar, X bar is a completely defined object. But, let us suppose that this cost function C is also, will also be able to tell us the cost of partial object. So, this is an important idea. So, we are given a cost function C, some C of say X bar, which gives the cost of a complete object X bar.
But, suppose C of say some Xij is also defined. Suppose so as we construct our object little by little, we can still associate a cost with it. This may not always be true, but in some cases, it will be and we are talking about those some cases and we are going to give an example in a minute. And furthermore, suppose this extended cost function, now has the following interesting property.
So, in addition to this, this is the first condition that we need, in the second condition is that if say X bar is obtained by extending Xij, then we know that cost of X bar is greater than or equal to cost of Xij. Suppose the cost function satisfy these two properties. What are the two properties that it must be defined, even for partially constructed objects and furthermore, we should only increase as we go towards completely building that object.
Another way of saying the something is that, this is our tree.(Refer Time: 14:26) So, the cost function should be defined at every node in this tree. And furthermore, it has to only increase, actually does not have to increase but it cannot decrease, it should not decrease as we go down from the root towards the leaves. So, if these two properties are met then we can have a branch and boundary, ((Refer Time: 14:46)).
So, let me write that down and then I will explain why. So, this implies that branch and bound is possible, so here is the idea. So, suppose I have found some solution over here, a complete object and its cost.(Refer Time: 15:13) And now I am exploring this tree and I go back and I go back to this point X 2, at this point avail you evaluate the cost of X 2. Now, if the cost of X 2 is itself larger than the cost of the best solution that I have found so far, then something interesting has happen.
I know because of the property, of this cost function that no matter how I complete this object (Refer Time: 15:39). How I extend this object into a full object, the cost is only going to increase. But, this cost itself is bigger than the cost of the best object found so far. And therefore, there is no point in searching further below X 2, this is an important idea. This is the idea of branch and bound. So, let us take an example.
(Refer Slide Time: 16:06)
 
So, my example is the travelling salesman problem. So, I am going to use a very simple graph, to explain how branch and bound can be used with TSP. So, here is my simple graph. So, it is just a problem on four cities. So, here are the edges and vertices in my graph, this is of course, ridiculously small problem, but it will do for explaining our ideas.
Let me put down the weights along with this. So, may be this edges weight 2, let us say this edges to be 1, let us say this edges be 15, this edge over here has be 10, say this edge has be 3 and this outer edge has be 1. So, this is the graph that we have given and our goal is to find a tour through this graph, a tour is simply a sequence of vertices, which contains all vertices and this is the complete graph. And furthermore, the weight of the edges traverse according to the tour should be as simple as possible.
I am going to first explain, how backtrack search should work on this. And then I will tell you how, a branch and bound heuristic will allow us to prove away a lot of the searching. So, let us start with the empty tour, the empty tour is, so these are my four vertices and let us say this is my starting point, I could have anything as my starting point, does not really matter. 
So, this is the starting point. At this starting point, my idea is going to be that I am going to extend the tour, just I have been given a partially constructed object, an object which represents a tour which is not fully constructed here. So, I am going to extend it edge by edge. So, the first edge I can add in several ways. 
So, for example, here is one possibility. So, I can start over here and I can go down. So, these are my other two cities. So, I could have gone down over here. Of course, I could have something else what could I have done, I could have say taken the horizontal edge instead. This I will explore little bit later, but right now this is what I am going to explore. So, as we said earlier the algorithm is going to proceed in a depth first manner. So, the algorithm will make a choice that the first edge is the downward edge over here.
Next, it is going to extend the tour starting at this vertex. So, how will you do that, well it will consider all possibilities out of that. So, from here, from here there are two possibilities, now either it can either take the horizontal edge or it can take this diagonal edge going back. So, let us say take this diagonal edge, the horizontal edge will come in somewhere over here.
Then from here what are the possibilities. So, from here the possibilities are that till valid it is gone down and it is gone up. And this point really there is only one thing left to do, it has to go down here again, it has to go down. And then finally, from here there is only one possibility. In fact, so there is no, even here there was one possibility; here as well there is one possibility. So, this is what we had done so far, and then from here it will go back.
So, this is the tour that we would have got, by doing depth first search. So, take the first alternative at each step and we would come to this tour. At this point we would ask, what is the cost of the tour? What is the cost of the tour here, so it is 2 for this 2 plus 1 3 3 plus 3 plus 1. So, the cost of this is 6. So, in backtrack search, we would just record this 6 and we would say this is the best object we have gone, so far.
What we do next then we would go back from here. When we go back here, we say is there another way in which this tour could have extended no. There is no other way because from here, we have to go back to the starting point. So, there is no other way. So, then we go back over here again. So, at this point again we need to ask is there another way of, in which this tour could have been extended. 
Well even here, there is no other way because we cannot go back to town which we have already seen, because we have to go through all the towns first. So, the only way could have been to go down, which is what we did over here. So, even here there was no other choice. So, we come back over here and at this point, we could ask well instead of going in this direction could be go straight. So, yes we could go straight. In fact, and therefore, we would go and we would explore this part of the search page
So, from here, instead of going back diagonally we could go down horizontally. Now, from this point onwards backtrack search would go down and search the rest of the tree ((Refer Time: 21:43)). But, here is how branch and bound would make a different decision. Now, branch and bound could say, let us evaluate the cost of the tour that we have constructed so far. 
So, what is the cost of this tour, the cost of these edges is 2 the cost of this edge is 15. So, the total cost of this partial object that we have constructed is 17. We know that as we add more and more edges into this, the edge length that the total distance covered is only going to increase and this is based on the important assumption, which is that all edge lengths are positive.
Let me write the term because that is really an important assumption, all edge lengths are non-negative. If this assumption when not true, there we would not have a branch and bound algorithm at least not the way, I have described so far. But, this is the very natural assumption and so we can conclude at this point that. In fact, any possible way in which this tour is to be extended, does not need to be considered, because its cost is going to be bigger than 17.
And therefore, it cannot beat this best cost tour that we have found so far. So, branch and bound would not do, any of the work of exploring anything below this. So, at this point branch and bound will say, oh I do not need to do anything and I would return back. And it would keep the 6 as the only tour that we have found so far. And that is the best and it would know that with assurance ((Refer Time: 23:34)).
We have essentially proved. In fact, that noting underneath over here needs to be explored. What would happen next, we come back over here, ((Refer Time: 23:44)). And then we say, instead of exploring in this manner, instead of taking the first edge itself going downwards, let us take it some other way. So, what is another way, it is this way, let us take the horizontal edge. Again branch and bound will evaluate, what is the cost of this partial tour that we have constructed.
What is the cost over here, the cost of this partial tour is10, because this horizontal edge has cost 10. Again, without exploring things below this vertex, we can safely conclude that this is large enough, that we do not need to explore anything underneath it. Because whatever comes underneath is, is going to be obtained by extending this tour, which already has cost 10.
And therefore, we can say forget it, the cost of anything below this has to be bigger than at least, it has to be at least 10, but we already found that tour of cost 6. And therefore, we can return back over here and then we can take the next stage and so on. So, in any case in backtrack search we would have explored everything underneath this, in backtrack search we would have also explored everything underneath this.
In branch and bound, we come to this vertex and we know we do not need to do this work. Also we come to this vertex and we do not need to do this work. And therefore, we have saved on the total amount of work that we do. So, let me just summarize this.
(Refer Slide Time: 25:16)
 
So, let me just write this as a comparison of backtrack and branch and bound. So, this is backtrack and this is branch and bound. So, branch and bound backtrack is a general method, branch and bound needs suitable cost function. And there might be some amount of cleverness that might be needed, in defining this cost function. This may do more work, because more nodes of the tree are visited. In this, there maybe pruning and there may be fewer notes of the tree, which might get explored in this.
But there is this small overhead of evaluating cost at every node. In general branch and bound, if you have reasonable cost function will work substantially better than backtrack. And I might go as far as saying, that whenever you can find a reasonable cost function, which has the property that I just mention. You would invariably use it, because the gain from the pruning that you get, will typically be much more than the over head of maintaining, the cost function is concerned. So, let us take one more example, before we go on to something else.
(Refer Slide Time: 27:26)
 
So, we are going to look at branch and bound, but we are going to look at branch and bound this time, for the knapsack problem. We introduce this problem last time, but let me defined it again. So, the input r is the vector is r two vectors, say V 1 2 then V n. So, we have been given n numbers, Vi represents the value of the ith object. And we are also given numbers W 1 W 2 till W n where, Wi represents the width of the ith object. And we are given one more input, which I will call C, which is the capacity.
So, we have been given think of it, as in this manner. So, we have a back of capacity C where, C is also measured as a weight. So, say we have been given a bag, which can carry at most 50 kg. In front of us r n object, we know the weight of each object as well as we know the value of each object. Our goal is to select objects, such that we do not overflow our bag. So, we do not put too much weight in our bag, which might break it. But, subject to this constraint we should put in as much value as possible, that is the goal.
So, I want to explain how they will do branch and bound, how we can implement the branch and boundary heuristics for this. But before that I would like to start with the backtrack search, how backtrack search will work on this. And like the last problem where, there was a natural cost, the natural objective function over here is a different function. 
So, the natural function over here is a benefit function. So, our goal is naturally expressed as maximize total value. So, this is; however, object; however, object is naturally expressed we want to pick up objects, such that we get maximum value, subject to the constraints that our weight, the total weight that we pick up is at most C. So, how will backtrack search work on this, well we need to define the nation of, we need to develop this idea that when we construct solutions, candidate solutions for this. 
We have to, can we do that as step by step process in which we start of within empty candidate object and we extended little by little. So, that eventually we have a complete solution, so our nation. So, what we are constructing over here is a selection, selection of objects, which is essentially a subset. So, clearly we should think of this as, we start of by looking at the empty subset. 
Then the first decision point that we take and that point we need to, we simply decide, do we take the first object or do we not take the first object. Let us make a firm decision. So, either we take the first object or we reject the first object. So, say let us say we decide to take the first object, then the subset that we have selected so far is consist of 1 and the subset that we have not selected so far consist of 2 through n. So, let me here there is an additional number, which is 1 to n. So, the pointers that our search let me just write this separate, this is beginning fill up to small.
(Refer Slide Time: 32:28)
 
So, initially our search can be characterized by writing down, what objects we have selected and what are the objects, which are yet to be selected. So, initially we have selected no object what so ever. This is the set of object, about which we have not made a decision. So, this is the starting point, now the idea is that we are going to make decisions about objects. We will either decide to take an object or either will reject an object.
In the first decision point, we will make the decision about the first object. What could the decision be, well there are into two choices, we will either pick that object or not pick that object. So, if we decide to not pick that object we will get this state. So, we will not have picked any object whatsoever. But the set of objects about which we have to yet to make a decision or going to be 2 to n this time. So, here we have said that we are definitely, not interested in the first object.
Why are we not interested, well there is no real reason for it, this is just one of the possibilities that we are considering. Remember the back track search involves, exploring all possible decisions. So, this is just an exploration of this possibility in which, we do not take the first object, the other possibility is that we in fact, include the first object. So, here will have 1 and those objects about which we have yet to make a decision, will appear over here.
Of course, we will not be building this set immediately, we will first just make a decision about this and will come to this point. Then what will be do, then will we will make a decision about the second object. Again there are two possibilities and therefore, we could get something like this, again may be we decide no, we are not interested in the second object either. So, we will get 3 to n over here or we could decide that we are, in fact interested in the second object this time. So, we will get something like 3 and 3 to m and similarly over here.
So, in this manner, if we proceed we will get to two to the leaves, which will correspond to all possible ways of taking subsets of n objects. And then at that point, we could evaluate our benefit function and then we would have to keep track up of the best possible benefit function. At the best possible completely generated subset with what its benefit is.
Going back to our definition; however, we said that in order to apply a branch and bound, we need to write down a proper cost function. ((Refer Time: 35:54)) And further more the cost function must have this property, of course we could do the same reasoning with the benefit function as well that is indeed possible.
But I am going to take slightly different rule, I am going to keep our definition of branch and bound the same, that is we will vary about cost function rather than benefits function. And now I am going to express the knapsack problem, in terms of a benefit functional rather than a cost function.
(Refer Slide Time: 36:26)
 
So, our original function is maximize value of selected objects is there a natural cost function, which we can minimize. I could simply do the negative of this, but that negative is not really very interesting, it does not really give us any insight into the problem. Instead of that, why not ask for minimizing the value of rejected objects. I claim that these two are identical.
So, if you give you a subset, in which the value of the selected objects is as large as possible, I claim that the value of the rejected objects must also be as small as possible. This is simply waste of the observation that the total value is fixed and therefore, if you select large value then you are rejecting the small value. So, this is the key to applying branch and bound to this problem. So, now we are going to think of this knapsack problem, not in terms of maximizing the value of the selected objects, but in terms of minimizing the values of rejected objects. In fact, let us take a numerical example, which will make this idea completely clear.
(Refer Slide Time: 38:13)
 
So, our numerical example is that our array, this is our array V, so V consists of value 20 3 7 and 5 let us say and W consists of say 5 2 4 and 1. So, the third object has value 7 and weight 4 and let us says our capacity is equal to 8. What will be the branch and bound tree for this looks like. So, originally we will start with no selection. So, the selected set is this, the set of out which we are to make a decision consists of all four objects 1 2 3 4.
Then we make a decision about the first object. So, let us say that is n says that the first object is not to be taken. So, now we still have to make a decision about 2 3 4, over here let us say we do make. In fact, let me change my idea little bit, let us say here we make the decision that the first object is to be taken. So, here the decision could be that the first object is not taken and 2 3 4 we have again really made up our mind yet.
So, if we take the first object, we will have used up V 5, so now we need to make a decision about the next object. So, let us say all the left going paths are the once in which we are greedy, we are keep taking those objects. So, what would happen over here, so we will take the second object as well. So, our bag would contain or our knapsack would contain both 1 and 2 and we will not yet have made a decision about 3 and 4.
And then at this point, we could either decide to take third object or we could check the constraints, remember do not have to be always we checked at the end, we could make the check earlier as well. So, if we decide to check the constraints earlier, if we have already taken the first two objects then taking in the third object would make the weight be 11, which is bigger than 8. And therefore, there is no question of taking in the third object.
So, we could say we could definitely go down to 1 2 and only 4 over here. So, we have rejected object three. So, from this, we will go down to leaf which is 1 2 and 4, this is an acceptable leaf because 1 2 and 4 are these objects which have a weights 5 2 and 1, which adapt to eight, server capacity is not being violated. And at this point, I am putting an empty set over here, indicating that all our decisions have been we made, we have made decisions about all the object
So, we have come to leaf and we have found the solution and what its value, well this is an object which we took, this is an object that we took and this is an object that we took. So, the value is 20 plus 3 plus 5, which is 28. So, here found a leaf, we have found a solution with value 28. So, now we can go back and we could explore this, I would like you to focus your attention on what happens, when the search reach us this point over here.
Well this is the benefit, but we let me just remind you that we said that we are not going to worry about the benefit, we are going to worry about the cost. So, what is the cost over here. Remember that the cost is nothing, but the value of the object, which were rejected. So, what is the value of the object which is rejected, well the object which was rejected is the third object and so the cost over here is equal to 7, this is the benefit, which is interesting of course, because in the end we want to report the benefit.
But the cost of this solution is 7, in the 7 that we defined, the cost as the value of the object that was rejected. So, if we want to execute branch and bound on this, what is going to happen. We are going to evaluate the cost at each particular point, in this whole exploration. So, when we come to this point, we will also be evaluating the cost. So, now I would like you to tell me, what the cost of this partial solution is.
At this point, the object that we have firmly rejected is the first object, because that is the decision that we made, so what is the cost of this. So, the cost of this is going to be the cost of the object which we have definitely rejected, all objects which we have definitely rejected, so far. We may reject further objects in the future, but that we are not going to cover. So, the cost of the object that we have definitely rejected, so far is 20. So, now we can conclude that no further exploration underneath this, are really interesting.
Because as we go down, we will only reject more objects. We will include more objects as well, but remember that right now we are focusing on the object which will reject. So, in that sense if we think of the cost, the cost of this is only going to increase if at all, it is not going to decrease. And therefore, whatever leaf we reach underneath this is going to have cost at least 20. 
If it cost at least 20, then what is the maximum benefit that we can; however, here well the total value is going to be 20 plus 3 plus 7 plus 5 which is 35. So, the total benefit, the total value is 35, we have already rejected an object of cost 20. So, when we come down to over here, our benefit is going to be at most 15. The interesting thing is that we know that even without seeing all possible subsets. And that is what makes the method powerful.
So, at this point itself, we can reject the entire sub tree and we can say forgive it, go back and this has to be the best cost subset. Whereas, backtrack search could, in fact has search everything. Because it is not keeping track of what is the best and it is not putting, the bound on how much, how good as a solution we can get. So, again even in this case branch and bound will work quite well.
(Refer Slide Time: 45:41)
 
I want to give an exercise. So, basically if you want to use the branch and bound heuristic, you need to come up with good ways of constructing these cost functions. So, we already saw two problems over which, we constructed cost functions in one case the cost function was fairly natural. In another case we have to do a little bit, we have to be slightly cleaver in order to construct the cost function.
So, let me give you a variation of the TSP problem. Say let us call this the geographical TSP problem, the Euclidean TSP problem. In the Euclidean TSP problem, what we know well think of this as a Euclidean or the geographical TSP problem, which means that, in fact we think of vertices as towns and edges as roads correcting them. So, suppose in addition to knowing the road distances, we also know the latitudes and longitudes or the X Y coordinates of the town is themselves.
So, then there is a notion of the shortest distance or the straight line distance or which is often called the distance as the crow flies. So, this distance is definitely going to be a lower bound, on the length of any road correcting, because a road code wind does much. If it goes through other towns it might even reverse, but the road could wind and therefore, there is going to be this direct straight line distance which is going to prove a lower bound, which is going to give a lower bound on the distance possible.
So, in addition to the input that you already have, so the input in this case is this distance matrix. Plus all possible straight line distances. So, using this I would like you to construct a cost function, which will be better than the cost function which we have constructed earlier. What does better mean, that if I give it partially constructed tour, it should give me a value which is bigger than the value which I had earlier.
Now, why could that mean. So, that would have to be because, it is somehow has to take into account that I have committed to using these edges. But, I still have to visit all these towns and if I want to visit these towns, I would at least have to have some amount of distance added to the distance that I have calculated so far, so for doing this, the straight line distance information will come ((Refer Time: 49:12)). So, this should be an exercise I will like it to try. So, let me summarize again, just I have summarize this. So, let me write down in the final word.
(Refer Slide Time: 49:28)
 
So, there is some cleverness needed in constructing cost functions. That is an important idea, but once the cost functions are constructed, usually substantial pruning happens. So, let me look ahead a little bit well, actually know let me look back little bit. 
(Refer Slide Time: 50:23)
 
So, there are number of issues, which you have not talked about in both branch and bound and backtrack. So, the first issue that we have not talked about is, how to organize the search space, what I mean by this is let me come back to this example, ((Refer Time: 50:51)). Here we said that will look at the first object first, that is what necessary, we could have used some other heuristic of deciding which object to look at first.
So, this initial that ((Refer Time: 51:05)) we do might have great pay offs in the end. So, maybe we could say that we want to look at the object, which has the highest value first or which has the highest value to weight ratio first. So, at a given node, which edge to select first to explore first. Again coming back to this ((Refer Time: 51:46)) at this given node, here we select at the edge in which we included the object. Here we did not include the object.
In this case, we explore this edge first at not this edge and you saw that was the more interesting thing to do because, we got a good object with good cost. And as a result of which could prove in this. If we have gone in different direction, then maybe we did not have what this pruning effect. So, clearly how we which has to explore first is an interesting decision. So, there are additional heuristics possible for all these questions, both these questions.
There are lots of works and there are internet articles as well, these days which will tell you about these things. And which will tell you, how the different heuristics works for different kinds of problems. For our next lecture, we will take a complete different viewpoint.
(Refer Slide Time: 53:12)
 
And we look at dynamic programming, which is an even more interesting way of doing combinatorial search and optimization and this stop here.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 18
Dynamic Programming

Welcome to the course on Design and Analysis of Algorithms. Our topic today is Dynamic Programming. Dynamic programming is a powerful technique for designing algorithms, and it actually finds applications in many areas.
(Refer Slide Time: 01:10)
 
The name itself is not really very, very enlightening. So, I will not talk about it, there is an origin is somewhat obscure. And so let us not worry about it. But, as I said the techniques finds applications in many areas, including operations research, signal processing, computational biology, geometry and many more. We are going to view, dynamic programming as a technique for combinatorial optimization. And in some sense, will think of dynamic programming as an optimization of the backtrack search technique that we have seen so far. 
(Refer Slide Time: 02:29)
 
So, we will think of dynamic programming, as in some sense are optimization of backtrack search. So, here is what I am going to do today. I am going to talk about, I am going to take an example to illustrate the technique and this example is going to be our familiar knapsack problem. So, I am going to review the backtrack search solution, then I am going to say how we can really ((Refer Time: 03:03)) slightly differently and how we can optimize it. And that is how we will lead into the dynamic programming idea.
And then I will describe some details and then summarize. So, let me take an example of an knapsack problem. So, let us say our knapsack problem, let me remind you that our knapsack problem involves filling a knapsack with objects of the maximum value. The inputs to this problem are two vectors.
(Refer Slide Time: 03:55)
 
Let us say V, V is one of the vectors, which gives the value of each object. So, let us say we have V value is which are 7 2 1 6and 12. This juts mean that the first object has value 7, the second object has value 2, the third object has value 1, fourth object is value 6 and so on. Second thing of the value has been given to us a rupees or something like that, then we are also given another vector. So, this vector gives the weight of each object.
So, for example, we might have that the first object weights 3 kilograms, the second object based 1 kilogram, the third object weights 2 kilograms the fourth object weights 4 kilograms and say the last object is 6 kilograms. And finally, we are given a knapsack and it is capacity. So, this is our last parameter C and say this time C is given to be 10. So, let me remind you what the problem is, we are suppose to pick up objects from the set, such that the total weight is at most 10.
But, we want to pick up the objects of maximum value. So, this is the problem. Let me now remind you, how we did backtrack search on this. So, we become the backtrack search, by saying that at present, we have all possible objects in front of this and we have not picked up any object. So, that is corresponds to the first node in our search. So, let me draw that node out here. 
So, this first brace says that we have not really made any decision yet, on any object. We have not picked up any object yet and the second set 1 2 3 4 5 gives us the list or this is the set of objects, which I still need to make a decision about. So, this first vertex just says that I have to make a decision about all objects. And backtrack search, systematically goes through the entire search space and determines the value, when different objects are picked or not picked.
The first decision point is where we decide, whether or not to pick up the first object. On this side, I am going to consider the case that the first object, in fact is not picked. So, for example, I will write here, nothing has been picked yet. But to indicate the fact that I have made a decision about the first object, I will remove the first object from the set of objects, which about which I have to make a decision. So, I will just write 2 3 4 5 over here.
On this side, I will make the other choice which is, I will pick up 1 and then on this side I will include 2 3 4 5. This means, that I have decided to pick up the first object and these are the objects about which, ith have to make a decision. In a similar manner, I will keep on examining each of these nodes, I will make more and more decisions. And I will systematically evaluate all the possibilities.
So, for example, on this side I could have said that, so next decision is about object 2. So, maybe I will on this side, I will not pick up object 2 and this side maybe I will pick up object 2. So, now the set of object that remains unpicked is 3 4 5 or the set of object, which remains undecided is 3 4 5. On this side, similarly let us say I go to the left and as I have said we have by convention decided that on the left side, I will not picking up the corresponding objects.
So, again the objects that I have selected are only one. Whereas, I just made a decision about object 2 and I said, I am not going to pick it up. So, the objects that are remaining are 3 4 and 5 and so on. I will explore the tree in this manner and we remark long ago that if we do this, we will generate a tree with height with total number of leaves 2 to the n. If n denotes the number of objects that clearly is two large at time for even for moderately large n. So, we clearly need a better algorithm than this. Now, I am going to take a different view of this backtrack search procedure that I just described. Let me first remind you, what the backtrack search view is?
(Refer Slide Time: 08:53)
 
So, the backtrack search view of a vertex in this tree, corresponds to a partially constructed solution. So, for example, I wrote 2 and 3, 4, 5, this just says that I have included 2 in a solution. I have decided not to include object 1 and this is the part about which, I have not yet made any decisions. But, here is an alternate view, suppose I have decided to include 2. So, let me go back to my old problem.
So, if I decide to include 2 and not 1 ((Refer Time: 09:50)) and I have capacity 10 then; that means, that out of the capacity that I had of 10, I have used up capacity equal to 1. Because the second object has weight 1. So that means, corresponding to this vertex. So, this choice of mine really says that our new capacity after I have decided to include object 2 is 10 minus weight of the second object. In this case, 10 minus the weight of second object happens to be 1.
And therefore, the new capacity happens to be 9. And then, I have not yet made decisions about objects 3, 4, 5. So, rather than thinking about this remaining part, as an extension of the old solution, this new view encourages us to think. Let us forget our original capacity. Let us say deal with what really is remaining. What really is remaining is 9. So, we have capacity of 9, which is remaining. And we have to select the 3, 4, 5, these objects 3, 4, 5.
So, we are allowed, now to take objects from this set 3, 4, 5. Our capacity is 9; that means, the weight of objects, which is select from this must not go beyond 9 and within that, we want to maximize our value. Clearly solving this problem, extending the solution over here is exactly the same as this, as solving this independent problem. That is because even here, I will have used up 1 unit of capacity, out of the 10 units that I have already had.
And clearly, it makes sense for meet to fill up the remaining capacity with the best, the most valuable objects from this set. Whose total weight is less than 9. And that is exactly what happens here as well. So, this is; however, a substantial improvement because, now our view is that, we are not really thinking of about extending the solution. But we are thinking of solving and independent problem. And further that problem is of the same kind as the original problem.
So, rather than saying that let us extend the solution, we will just say that look in order solves this original problem. Let us solve this alternate problem, but this problem is not an extension, of any extension of any solution. But, it is a simple new problem of the same kind. So, this first of all introduce as simplification in our outlook. And in some sense it makes programming easier, so the benefits of this new view.
(Refer Slide Time: 13:15)
 
The first benefit is that, this new view is simpler or I would state that as the outlook, our outlook overall is simpler. This in term such as our programming is going to be easier. I will talk more about this in a minute and you will see that indeed, the programming gets simplified. However these two are not really, the main important benefits. There is an additional benefit, which is much more important, but about which I am not going to tell you right now.
Let us wait, until I tell you of how to do the programming and then I will tell you, about this surprise benefit. And you will agree with me then, that this is really a very exiting benefit. So, let us start, let us worry about, how we going to program this. So, the idea was that in order to solve the knapsack problem. Instead of thinking about extending the solution, we said let us construct a new problem of the same kind and then solve that maybe, one or as you have seen maybe, more problems of the same kind and solve that. So, this naturally means that we should be thinking of recursion. So, let me write down a recursive procedure, to solve this knapsack problem. So, let me first define that procedure, let me write down this specifications of the procedure first.
(Refer Slide Time: 15:18)
 
So, I will call my procedure Ks and it will take two parameters C and i. C is going to be the capacity, remaining or actually I should say rather, than remaining that the capacity given for solving the problem. So, let me just strike this off, capacity for containing solution. I is a parameter, which is the index of the first undecided object or in fact, instead of thinking about the first thinking of it as a first undecided object, I could think of it as the first object, which is allowed.
So, the object which are allowed for me ri, i plus 1, i plus 2 all the way till n. So, my original problem that I needed to solve was say something like Ks of 10, 1. What does it indicate that I want to solve the knapsack problem, on knapsack of capacity 10 and I want all the objects, in the first object onward until n to be considered. I have told you what n is and let us assume that n is a global variable. So, there are some global variables over here.
So, n is a global variable, the total number of objects, similarly the value array is a global variable and the weight array is also a global variable. And let me just remind you, that there are n components in each of these arrays. So, these are the specifications of the procedure that I am going to write. So, let me now write the procedure. So, let us deal with the base case first that is fairly easy.
(Refer Slide Time: 17:49)
 
Ks of C i, so first the base case, so if i is greater than n. So, that means that I have been asked to solve the problem, starting from the n plus first object or some object beyond n. That means I really have made up my mind about all the previous object ((Refer Time: 18:19)) or in essentially what is am saying is that, I am not allow to choose any objects at all. In that case we should be return in 0, as the value of the solution. I should say before I get into this, that in this problem that this specification that I have wrote down. ((Refer Time: 18:42))
I should also say that we will only we worried about the value of the optimal solution. So, the idea is that case of 10, 1 will not actually return, the set of object which are chosen. But, in fact it will return the best possible value that I can get out of it.
Now, (Refer Time: 19:14) I think this is a (Refer Time: 19:15) simplification of the problem that is given to us. And in some sense it is, but later on you will see that. In fact, once we know what the value is, it is very easy to go back over our calculations and actually figure out what that set was. So, we will leave that for the future, later in this lecture. Right now we will just concentrate on determining, what is the maximum value I can get by filling my knapsack, so that it does not overflow, it is in overloaded.
This is the base case, if I have already gone past n then; that means, we have considered all the possible objects and then we should returning 0. Because there is nothing to add to our knapsack, so you have already return. Otherwise, the first object that we are going to be considering is the ith object. So, what can you do about it, well we need to consider it only if the capacity is in fact, able to take that object.
So, if C, if the capacity happens to be less than the weight of the object, then we cannot do anything right. So, if the capacity happens to be is less than the weight of the object. Then that object cannot be considered, which means, we should just go on to the next object and return whatever we can with the remaining object. So, in that case, we are going to return Ks of C and i plus 1.
The interesting case appears, if C is in fact, bigger than W of i, so else what happens. So, else we have the possibility that we can in fact, include this object in our knapsack of course, it might not always be wise to include that object. Simply because, if we include that object we have to reduce, we have to use of some of your capacity and maybe the benefit that we get because of using of that capacity is not adequate enough. Or because we might be losing some better options later on.
So, therefore, to cover both possibilities, here is what we should do. We should return the best of those towards. So, the first is we will ignore this current object, in which case we should be returning Ks of C, i plus 1. And then we need to consider the other possibility, which is that we include this ith object. But, if we do include the ith object there are two effects.
First of we gets it value. So, we should be returning in whatever we return, we should improve that value as well. So, we are going to return V of i, but will also get some value form the remaining objects. However, when we go for the remaining objects we would not have the full capacity C to consider. But, instead will have a capacity, which is a little bit reduced.
So, we should we should be returning, V i plus Ks of C minus width of i, because this is the amount by which the capacity series is going to be reduced. And whatever choice we can make from i plus 1 from the remaining objects. So, this is what we should return, the best of this quantity and this quantity. So, this finishes the recursive implementation of the backtrack search that I have just mention. Just make sure, that we understand this. Let us actually execute this, on a sample on our own example, in fact.
(Refer Slide Time: 23:49)
 
So, let me write down what our Vs and Ws are. So, our Vs and Ws were V was 7 2 1 6 12. W was 3 1 2 4 6 and our capacity was 10. We started of by making the first call, which was Ks of 10, 1. Ks of 10, 1, now let us go back to our code and our value of n was equal to 5. So, if we go back to our code and ask how this code could execute, the first step we have to check is i bigger than n. n is 5 and i is 1. So, clearly it does not happen and. So, we come to situation.
So, the weight of the first object is 3. (Refer Time: 24:46) And the capacity is certainly bigger than that. So, this is not case that we execute, but in state we execute this case. In that case, we are going to consider two possibilities. One is searching the knapsack possibilities, with the same capacity and i plus 1. So, let me draw this as a recursion tree.
So, here I am going to get Ks of 10, 2 and of this side I am going to get whatever is whatever corresponds to this. ((Refer Time: 25:19)) But, corresponds to this we are going to use up, out of the C capacity we are going to use a W i capacity. So, W of 1 capacity is going to get used up the W of 1 capacity is 3 units. So, on this side I am to I am going to make a call Ks with the reaming capacity, which is 7, 2. Because, now I am going to only consider objects starting at the second object.
In this manner, we are going to execute. So, let me do a few an executions, just to make sure that the idea is understood. What happens, when we try Ks of 10, 2 again even here, we will see that ((Refer Time: 25:57)) the weight of the second object is 1 and that is still smaller than the capacity. And therefore, we will use this part of the statement. So, the first thing will execute is Ks of C, i plus 1, which is with the same capacity will try to go for the remaining object.
So, will be executing Ks of 10, 3 and on this side, we will be executing recursive call corresponding to this call over here. ((Refer Time: 26:33)) W of 2 now is this. So, will have to decrement that 1 from that and therefore, the call that we execute over here will be Ks of 9, 3. Let us do one more of these calls. So, let us see what happens when case of 9, 3 is executed further. So, again if you go back to this procedure, you will see that even here the capacity is larger than this W I, W of 3.
W of 3 is 2 and it is still larger and therefore, this will get executed.(Refer Time: 27:09) So, again there will be two children, two recursive calls. So, the first one will simply be Ks of 9, 4 and on this side, the call would be Ks of C minus W i. So, C minus W i this time is this. So, it is going to be 7, 4. Let us do something on this side and. So, what happens over here. So, case of 7, 2 if we execute we are going to get something like this.
So, first we will execute Ks of 7, 3 this corresponds to the possibility that we do not choose the third object. They will of course, be corresponding exploration on this side where, we do choose the third object. But, even here there will be two possibilities Ks of 7, 4 and on this side the possibility that we choose the third object. So, whatever that is. At this point I would like you to do two things, first of all I like to make sure that this picture is understood. 
What we have drawn in this picture is, what is popularly called recursion tree. So, this is the first recursive called we need, that call give raise to this recursive call and this recursive call, that in terms give raise to these two recursive calls just will give raise to this call and some other call over here and so on. But that is of course, quite routine, the most interesting parts of this picture are these two calls. 
This should really write as Ks. So, there is something very interesting about these two calls. They are identical. What is it mean, in this part of the tree I am going to make a procedure call with parameter 7 and 4. And I am also going to make procedure call this side, also with parameters 7 and 4. So, this is the beauty. So, this is where the optimization can come in.
Once I explore this search tree underneath this, I won not need to explore this search tree again. If I store the value, that I get from this call, then when I come to this portion. If I just need to look it up, I will just look it up and I will get it. In fact, that is going to be my default idea. Whenever I calculate the value of a certain recursive call, I will actually stored it in a table. 
Before embarking on any recursive call, I will first check if the table already contains are value, if it does I will just use that value. If it does not then I will calculate that value. But at the end of it I will stored it in the table that is basically the idea that is basically the optimization that I was talking about. And you can see that by viewing, what remains to be done rather than thinking of this search tree as an exploration, in which you are extending solutions.
It is in this new view of things; it is possible to determine that the work over here is the same as the work over here. And thereby, we can do this optimization that we that I just mention. So, let me let me now flush out this optimization for you. So, we will go back to the same code and now I will write down, what is the corresponding code with the optimization. So, let me write that down in different color. So, if i greater than n then return 0 is the idea over here. So, we said that before returning any value, we are going to store it in some table.



(Refer Slide Time: 31:29)
 
So, instead of that instead of just returning 0, this part of the code will be replaced by this part. So, we are going to still check, if i is greater than n. But if it is then we are going to we are going to set table of C, i equal to 0 and then we are going to return table of C, i. So, this is what, this statement is going to be replaced by in our optimized version. Let me just remind you that, that the idea is to remember value is before we calculate them.
(Refer Slide Time: 32:21)
 
So, we are going to have another global array called table. And table is going to have, this is going to be a two dimensional array. So, we are going to have, the first index correspond to all possible capacities and the second index correspond to all i?s, all the possible values of i. So, this is going to be, i is going to be from 1 to well n and as we saw our base case actually takes at beyond n. So, it is going to be n plus 1. So, the second dimension is going to be n plus 1, C is going to be in the range 0 through whatever value, so the capacity given.
So, in fact, let us say this is little c and. So, little C 0 is less than or equal to little c, less than or equal to capital C. So, the first index of the first dimension will have this range, this is the two dimensional array in which, we are going to store our values. What about this if C is less than W i return Ks of C i plus1. We just define the rule and we are just going to follow it. So, instead of returning case of C i plus 1, we are going to first check. So, the way we check, I will just write down the code corresponding to this.
So, we are going to check, if table of C, i plus 1 is not calculated and that I will denote by null. Then we are going to actually calculated. So, will calculate table of C, i plus 1 is equal to Ks of C, i plus 1. And then we will return table of C, i plus 1. So, this is going to be, what this code is will replace this code over here.
Similarly, we will have code, which will replace this as well. So, corresponding to this we will just make a check in C i plus 1 here. So, this part will really with the same as this and instead of this, will make a check in a slightly different position. So, let me write that down as well. So, let me write that down separately over here. So, return ((Refer Time: 35:19)) of C i plus 1 is going to be done by first checking whether case of C i plus 1 has been calculated.
(Refer Slide Time: 35:28)
 
So, that is as good as saying if table of. So, return this expression is going to be done by checking this. So, we are going to check if table of C i plus 1 is equal to null. So, in that case we are going to calculated. So, we are going to set table of C i plus 1 equal to Ks of C i plus 1. Then we are going to check whether this has been calculated as well, ((Refer Time: 36:15))
So, that is as could as saying, if table of C minus W i, i plus 1 is equal to null. Then we actually calculated, table of C minus W i, i plus 1 is equal to Ks of C i plus 1. And then finally, we will just return the maximum of these two quantities ((Refer Time: 36:54)) and that is done simply by saying, return max of table of C, i plus 1.And v of i plus table of C minus W i, i plus 1. So, this code will replace this last code in your program.
So, should be clear to you, that this new code that we have return. Because, it is going to do less work because some parts of the search tree, which were explored several times in the original code will now be explored exactly once. But, now you would like to prove by how much precisely the work gets reduced.
(Refer Slide Time: 37:56)
 
So, we come to the analysis. Normally when you write recursive algorithms, the idea is that will write to recurrence for the time taken and we will saw (Refer Time: 38: 07). In the case of dynamic programming and in particular, where we stored these values in a table and use them as needed, this recursive estimate is not going to be very good. It is going to be an over estimate. So, we need something much more precise, we can produce an estimate; we can produce a way of doing that estimate, which gives us much sharper bounds.
So, here is the idea. So, think of this way as the computer executes each line of the code. Suppose, it writes the diary, in the diary it is going to write the following things. So, I am going to call this diary, it is customary to call this diary a transcript. So, this is going to be a transcript of the execution. So, the computer is going to write down, the line number in the program. Then it is going to write down, the value of C that is the correctly it has and then the value of i that it currently it has?
It is going to write down a triple, like this. So, it will write down once such triple every time it executes a line in the program, where we did in this. This will become clear in just a minute, if the idea is clear. So, let me take an example.
So, here is my code. So, let us say we have number the lines in this code, some going to for simplicity. I am going to think of this has been my code or actually I could do this as well. ((Refer Time: 40:06)) So, this is my this is line number 1, this is line number 2, this is line number 2 and so on. So, give numbers to every line in this program. And I have added some lines over here. So, I will give numbers these lines as well, maybe this is line number 14, (Refer Time: 40:22) this is line number 15, this is line number 16 and so on.
So, I have given numbers to everything whatever, this is line number 20. So, when the programs starts executing, I am going to be making the call Ks of C i. So, what will the computer write the first time around. So, this is going to be my line number 1 whatever, this is going to be my number 1, this is going to be line number 2 and so on. 
(Refer Slide Time: 40:54)
 
So, my transcript of my program is going to be the line number, which is 1, what will be the value of CB, the first time the value of C is 10 and the value of i is 1. So, this will be the first entry in my transcript. The next entry in my transcript is, I am going to execute the next line of the program 2, 10 and 1. As I keep writing at some points, things will change, of course the line number will change almost every time. But, at some points this 10 will also change, the 10 will change according to my execution tree, which I drawn over here.
So, in this execution tree as I started up with Ks 10 1, but then I went on to Ks 10 2. So, at some point during this execution, I am going to get again the 1, 10, 2. So, this corresponds to the first recursive call. And similarly, the different recursive calls will come out over here. So, this is going to be my transcript. How will you lines will there be in this transcript. How many triples will there be in this transcript, clearly number of triples is exactly equal to the number of time steps.
Because at every step, the every step the computer takes, it is going to make one entry into its diary or it is transcript and that is going to be triples. So, the number of triples is exactly equal to the number of time steps. So, if I want to estimate the time taken by this computer on this particular problem, all I need to do is to count how many triples, I have in my diary or in this transcript. You might now wonder, what is the big deal we want to estimate time, now we want to estimate triples there is actually rather interesting property to do with these triples.
(Refer Slide Time: 43:08)
 
And this idea is, that every triple in this transcript must be different transcript will have to be different. So, what I am cleaning is that if this is the transcript that my computer wrote, then subsequently I am not ever going to see the entry 1, 10, 2 again. So, this is never going to happen, why is that the answer to that comes in the changes that we made. So, if some entry reappears, then that means the computer is executing that same statement, whatever the statement is with the same two previous parameters, same two values of C and i.
But then; that means, the Ks must have been called with exactly those two values C and i again twice at least. But, that was precisely the point of the changes that we made. So, we said that before we make a call to Ks, the computer actually checks. Did I already execute this call, if I did execute the call, I am just going to pick up that value from my table. And therefore, we know that the computer never makes a called to Ks, twice with the same values.
So, that from that it follows, the every triple in the transcript will have to be different. That gives us a good way to estimate the total length of the transcript. We just count, how many different triples can there be. So, let us go back to the place where, we wrote down whatever triples were. So, this is our definition of triples. So, since every triple has to be different, ((Refer Time: 45:12)). 
(Refer Slide Time: 45:13)
 
The total length of the transcript will be, at most the number of different triples that are possible. So, the number of different triples that are possible is simply, the number of lines in the program, times the maximum value of the capacity times, the different values that I can take. The different values, that I can take n the number of objects. So, this is the number of entries that can there be in the transcript. So, let us complete our example.
(Refer Slide Time: 46:16)
  
So, in our case the number of lines in the program, we just said was say something like 20 times the capacity we said was in 10 and n was 5, well actually we shall said n plus 1. Because, we allowed the program, the procedure to be called not with just the number of objects, but one beyond the number of objects as well, so in that case is 6. So, then we can estimate that this has to me 60 times 2 equals 1200. So, our program will require 1200 steps of execution.
In general, the number of lines in the program, does not depend on the capacity that is given to you nor to the number of objects and therefore, I can write this as o of 1 times C times n or the total time taken is simply o of n times c, which is much, much less than 2 to the n, which was what we would get with backtrack search. And this is what, we get with dynamic programming. So, let me. So, let me now summarize the main ideas in all this, the main ideas in all this are 2 3.





(Refer Slide Time: 47:44)
 
The first idea is. So, let me just review this. This is the review; the main ideas are first think about whether the optimization problem can be expressed as a sequence of decisions. This is something that we need even in order to do backtrack search. But, beyond that in dynamic programming we do something more. 
(Refer Slide Time: 48:46)
 
What we do more in dynamic programming is that rather than think of extending solution. So, do not extending solution, but solving smaller problem of the same kind, that is the important dynamic programming step, dynamic programming idea. Let me see, this is the first dynamic programming idea. And in the second dynamic programming idea is, check if the same problem is being solved again if. So, keep a table and save time. So, you keep a table and we save time by not repeating that calculation. So, these are three ideas in dynamic programming, well. The first idea was really similar to was really common to backtrack search, but these are the two new important ideas. 
(Refer Slide Time: 50:24)
 
And then there is also a fourth idea which is for the analysis. The fourth idea says that do not use the recurrences, recurrence relation to estimate time. But, instead use this transcript idea. So, these are the important ideas. So, let me say a little bit about what we are going to talk about in the next lecture. In the next lecture, we are going to use a slightly different formulation of what we have seen so far. And that will actually, end up eliminating the recursion.
And in some ways, it will simplify further our view of this whole procedure. And then we will also use dynamic programming, on some other problems. On these other problems, the expression and our program will become a little bit more complicated. But the basic idea as you will see will remain more or less the same.
Thank you.

Design & Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute Technology, Bombay

Lecture - 19
Longest Common Subsequence

Welcome to another lecture on Design and Analysis of Algorithms. Today?s lecture will also be on dynamic programming. The topic for today is the Longest Common Subsequence. Let me, start by defining the problem.
(Refer Slide Time: 01:09)
 
So, the longest common subsequent will abbreviated as LCS. The input to this problem consists of two strings. So, there is one string A of length M, and another string B of length N. I might string occasionally, but I do means sequence, it does not really matter. The sequence will be a sequence of characters typically. So, I will use string and sequence interchange again.
The output is also going to be a string, let me call it C. And say it will have some length P, which is as it unknown. And we required the C have the following property. So, C should be a subsequence of A as well as B. That is what a common subsequences. I will define subsequence, more formally in a minute. So, C should be a subsequence. And we want C to be the longest possible subsequence. So, this is your problem. Let me define what a subsequences.
(Refer Slide Time: 02:34)
 
So, we will see that X is a subsequence of Y, if X is obtained by dropping elements of Y. And of course, the remaining elements, which hidden drop should be kept in the same origin. So, for example, a d is a subsequence of say a b c d e. So, these are the elements that we dropped, and as we said common subsequences or just sequences of both. So, let us take an example of LCS.
(Refer Slide Time: 03:42)
 
So, let us say A is a sequence of characters a b c d e. And let us say B is a sequence of characters say b c d or let me say, let me write it as b c e a d and some q. They do not have to give the same length. So, clearly b c d is a subsequence of both. And therefore, it is a common subsequence. a d also is a common subsequence. So, a d for example, appears over here and over here. b c d. For example, appears over here and over here. So, now we know what common subsequences are we would like to define, but the longest common subsequences.
So, LCS of A and B for example, is b c d. You can look through the sequences and you will figure out, the only common sequences of length 3 are very few. And in fact, there is no common sequence of length 4. And so therefore, this is A long common subsequence. In fact, you can see that b c e is also a common subsequences. So, let me just underlying that, so say b c e appears over here. And over here it appears over here. So, the goal now is to find one of these common subsequence.
So, goal in this LCS problem find any longest common subsequence of A and B. We do not care which one, but we want one of those. When we speak of LCS, we will mean either every LCS or one of the LCS?s. And what we mean should generally, we clear by context otherwise will explicitly ((Refer Time: 06:21)).
(Refer Slide Time: 06:26)
 
Let me give you a ((Refer Time: 06:26)) motivation about, why we care about this LCS problem. The length of the LCS of say A and B can be thought of as representing the similarity between A and B. Clearly, the longer this LCS?s the more similar A and B ((Refer Time: 06:58)). You might ask, why do we care about strings similarity. The answers to that are fairly well known and obvious probably. So, say we have two DNA sequences. And we want to know are they very similar.
If they are very similar maybe there is a common evaluation in those sequences. And so therefore, some measure of similarity is useful, suppose we have a miss pelt word. We would like to know the word, which is most similar to ((Refer Time: 07:38)) from our given dictionary. Again, we need some measure of what similarity is. So even here, the nation of an longest common subsequence is useful. As it terms out, LCS is not the only measure of similarity, there are others as well. But, interestingly enough the algorithms we develop a LCS will we somewhat similar. And you can probably extend them for other measures of similarity as well. Let me, start by considering a Brute force algorithm for LCS.
(Refer Slide Time: 08:14)
 
This is probably a good idea before embarking on trying to discover a complicated algorithm. It at least tells you something. If, not you have something to fall back up on. So, what could be the brute force algorithm. So, the idea could be to say let us generate all possible subsequences of A. This will be the first step. Then we will check, which are also subsequences of B. And then, we retain the longest. As we said, there could be several longest, but we do not care we will just keep one of those. And that could be the answer that you could print.
Is this algorithm good enough? Well, this is algorithm good, well certainly will written an answer. But the question is will it be fast. Now, it is not going to be very fast, because a number of all possible sequences of A is quite large. In fact, you know that if sequence as length M. Then there are 2 to the M possible subsequences. So, therefore, the time for this algorithm is going to be at least 2 to the M. In fact, if you want to write down the sequences the subsequences, you will also a multiply this by the by M itself.
But, any in case that is at least Exponential M. So, that is not what you want, we would like something faster. And in fact, we have said earlier, that we are going to apply dynamic programming for this. And our hope is the dynamic programming will give a something better. So, let me review, what the dynamic programming ideas are.
(Refer Slide Time: 10:16)
 
So, the first idea was that we should try to express the problem as a search problem. By that I mean, that you should clearly identify what the search space is. So, what is it, what is a possible candidate that you are looking for? So, in our case, the search space consists of common subsequences of A and B. So, we are going to search through the common subsequences. And so, in that sense is first step is over well not quite. So, there is also one more thing, which is we want to determine the objective function.
So, what is the objective function? So, let us says of all the elements in the sub space, which is the one that you are going to pick. So, in our case, that easily stated it is a longest one. So, we have a space consisting of all subsequences of A and B. And we want from it. That string which is the longest. Of course, we are not going to generate this space explicitly. We are not going to ever write down anywhere all the common subsequences that will take too much time. This is just for the purposes of thinking.
So, let me write the term. So, this is only help in thinking. We are not actually going to generate the subspace any of these spaces. Then, what we are going to do is. We are going to write the recursive procedure. Listed explicitly what a search space is having defined explicitly, what a search spaces.
(Refer Slide Time: 12:28)
 
The next step is to write a recursive procedure, which looks something like this. So, it is going to be a search. And it will take as argument the space has which we want to search. And now, the procedure typically will divide S into subspaces. Say, let me call those S 1, S 2 say some S k. So, there are k subspaces. And the idea is that will find the best in each S i. So, by best we mean in the one in the element, which is which maximize as our objective function, which is which optimizes our object function.
So, in our case, it is going to be the longest subsequent belonging to each of these S i?s. And then, we will return the best of the best. By this best I mean, the best computed above here. Simple enough, now the important point is going to be, how do we find the best in each S i? This is the recursion is going to come in. So, this if we can express recursively will at least have a recursive procedure. Of course, this is not your dynamic programming stops.
(Refer Slide Time: 14:13)
 
So, the next step is that we are going to characterize the calls the recursive calls. So, we are going to ask what are the arguments to the recursive calls? In fact, to all the recursive calls, which are made during the life time of that recursive procedure. So, we are going to define a table then. So, let me call it T, which will store the results evaluated for each recursive calls. Now, here comes an importance which, now we are going to ask that we should write non recursive procedure.
And this, I am going to call fill, which fills entries of T, assuming other entries are already filled. In particular, it fills say entry i j of T assuming all other entries are filled. The key time is going to be that, the key point is going to be that as a result of this the total time will be small. In fact, right now I can list what the total time is going to be. So, total time is going to be size of the table times filling time. So, if we can do this with a small enough table.
And if we can show, that each entry requires will be a small amount of time to be filled. Then will have a fast algorithm. And your hope is that this strategy will give as a fast algorithm for the longest common subsequent problem. So, let us see, how to do this in the case of LCS. The first step is to define the search space, which we already did. The next step ((Refer Time: 17:04)) is to write this procedure and this requires just divide S into subspaces S 1 to S k. So, just do that one.
(Refer Slide Time: 17:17)
 
Remind you, S is the space of the common sequences of A and B. So, this is what we are going to search. And now, we go to partitioning. So, let me write down one strategy. There can be several strategies. And I am just going to write down 1. So, one idea might be, that I will defines I will partition S as S sub a union S sub b union S sub c all the way till S sub z. The idea is that S sub a is subspace of S consisting of common subsequences starting with letter a.
S sub b will be elements of S, which start to the letter B. S sub c will be the elements of S, which start to the letter C and so on. So, here is one partition. Now, dynamic programming will require as the idea of dynamic programming will require as to find to write a recursive procedure, which recursively search as S a, S b, Sc and so on. And then, pick the best of the best of those answers. This is the 1 strategy. This will give us, better than exponential time solution. But, this is not the only possible strategy. So, let me give you another strategy as well.
(Refer Slide Time: 19:13)
 
In this strategy, we are going to take S and we are going to partitioning it, only in two subspaces. So, let me tell you, what S 1 is going to be. So, S 1 is going to be all subsequences, which start with the first letter of A. So, remember A is already known to us. A and B are known to us. And so we can ask questions about what A 1 is. So, S 1 is going to be all subsequences, which start with A 1. S 2 is the rest, that is start with something else, some other character.
Now, this is the strategy we are going to develop further. But will see that the idea of this development can also be used for the previous strategy. So now we need to define a recursive strategy, recursive procedure for searching S 1.
(Refer Slide Time: 20:21)
 
So, our question is how to search S 1. So, let us let me remind you, what is S 1, S 1 consists of all subsequences, which start with A 1. So, when we are looking for something it is usually a good idea to write down explicitly what is it, that we are looking for. So, let us say, R equal to longest element of S. So, let us say R as P as length P of course, P is not known to us, but let me just define notation P.
What is to be know about R. Well we know that R of 1 the first letter of R is the same as the first letter of A. Not comes from the definition of S 1. We just said that, S 1 only consists of subsequences, which start with A 1. Now, R is a subsequence of A as well as a subsequence of P. So, which means that R 1 must appear in B as well. So, without loss of generality we can assume. Let us, say that k is the smallest, such that B of k is equal to R 1 or actually it is equal to A 1.
Now, it may term out, that even does not at all appear in B. That is possible, in that case this search this space S 1 will be null. So, that is going to be a better of a degenerate case. So, for now, for a minute I am going to assume that this does not happen. And that k is well defined. So, there is some k. So, now once we know what this k is, what can we say about, can we give something more, can we say something more about R 1. Well, here is what we can say.
So, we know now that R of 2 to P is a subsequence of A. Well not of A, because A 1 is already taken up by R 1. So, the rest of A. So, 2 to M and also of B from k plus 1 onwards. So, it is going to be B of k plus 1 to N. So, this just comes from the definition of R. So, R has to be a common subsequence of A as well as B. The first element of R appears at A 1 by definition of S 1. It appears at k, at B k in B. And the rest of it also must be a subsequence. Otherwise, it would not be a common subsequence.
And therefore, R must be a subsequent of this as well as the subsequence of this. So, now having characterize what R is our question is how do we find R. Usually, terms out that if you characterize something adequately it helps some finding. And so, in fact, we are almost ready to find. And in fact, we can claim that sense R has to be a subsequence of both perhaps it is a longest such subsequence. So, let me write it down as a formal plain.
(Refer Slide Time: 24:13)
 
So, R my claim is R 2 through P is an LCS of A 2 through M and B k plus 1 through N. Where, k is as we defined a minute ago. k is the index of the first occurrence of A 1 inside P. Terms out that this claim is true. And the proof is not terribly hard either the proof if by contradiction. So, let us assume that R 2 P is not an LCS of this. So, let us say, there is some S is some LCS of A 2 to M and B k plus 1 to N. And in fact, for the sake of contradiction we have to argue, we have to assume that S is longer than R 2 to P.
So, that is length of S is greater than the length of this, the length of this is P minus 1. So, let us say, it is greater than R equal to P. Now, the interesting thing is that suppose I consider the sequence A 1 concatenated with S. Can we say anything interesting about this? Well, S is a subsequence of A to M. So, this must be a subsequence of A 1 concatenated with A 2 to M. But, this is simply A 1 to M. Similarly, this is also a subsequence of A 1 concatenated with B k plus 1 to N. But what is this? Well, this is nothing but even is also B k. And therefore, this is B k forward by B k plus 1 to N. And therefore, this whole thing is in fact, B k to N. So, what have we found then?
(Refer Slide Time: 27:07)
 
So, we have found, we have discovered that this S is a common subsequence of A 1 to M. And B k to N. But, this is itself is a subsequence of B 1 to N. So, I could just the right this as 1 to N. And also, S starts with let me put it here. A starts with A 1. Now, what is the length of S. S has length at least P. So, this really up to be A 1 concatenated with S is a common subsequence of this. And in fact, A 1 concatenate with S starts with A 1. So, A 1 concatenated with S has length at least P plus 1.
But now, we have a contradiction. Because, we said that R of length P was an LCS. So, then that means, there cannot be longer sequence, which is common to both. But, which we have just proved has to be there. And therefore, our original claim must have been true. So, what was our original claim, that R 2 P is an LCS of A to M and B K plus 1 to N. So, what is the conclusion from this? So, let me write it that down. So, the conclusion is that we know now, how to search S 1.
(Refer Slide Time: 29:09)
 
So, longest element in S 1 can be found as follows. So, 1 determine smallest k such that B of k equal to A of 1. Second find capital L equals LCS of A of 2 to M, B of K plus 1 to N. Why did we do this well this? Well this ((Refer Time: 30:04)) immediately from our claim. So, ((Refer Time: 30:09)) R 2 P we claimed is an LCS of this. So, we do not know what this is. So, we just find it. So, we just use recursion for it. And finally, return R equal to A of 1 concatenated with this L. So, we claim that this we showed, that this has to be along a common sequence of both A and B. Such that it lies in S 1, and so that is what we have constructed. So, we have constructed a procedure for searching S 1.
(Refer Slide Time: 30:53)
 
So, the next thing is to find a procedure for searching S 2.Well, as before let us write down what we are searching for. So, let us say, we are searching for some R again. So, say again of lengths P, which is longest on S 2. Can we characterize are further. Well, we know that R is a subsequence of A 2 to M. Why 2 to m. Because we know that A 1 does not appear in it. So, there is no question of R being a subsequence of a whole thing. So, it has to be 2 to M. And R is also subsequence of B 1 to N.
So, note that over here, we have use a defining property of S 2. So, this is how you have characterized R. So, at this point it might be tempt into claim or at least conjecture that perhaps R should be the longest common subsequence of both of these. That is the nice conjecture. But, unfortunately it is not true. I am not going to explain it into much detail. But, let me just give you, a hint of why it is not true and then you can prove it for yourself. So, just consider A as a sequence of letters a a a a. And B say as also the same. Then, work out what X 1 and X 2 are going to be, and the you see why this is not true. So, R is not the longest common subsequence of both of these. However, something equally useful is true. So, let me write that down. So, what is true is the following.
(Refer Slide Time: 32:54)
 
So, I claim that R as defined here is no longer then LCS of A 2 through M and B 1 through N. I will tell you, why this is useful in a minute. Let me, prove it first. The proof is actually a single line. We know that R is a common sequence of both of these. So well, so it cannot be longer than LCS of these two sequences, so done. So, let me now compare to this conclusion that we have written earlier. So, we said that we want to here is a procedure for looking for the longest element in S 1.
(Refer Slide Time: 33:56)
 
So, we also want to procedure for looking for the longest element in S 2. So, let me write that down over here. So, what will that procedure look like? So, I claim, that I am going to define R prime equal to LCS of A of 2 through M, B of 1 through N. So, this will be generated by recursive call. And this is no smaller, no shorter than longest of S 2. So, what we have, we have a procedure to determine the longest element and S 1, and an element, which is no shorter than the longest of S 2. So, is this useful, so what was our goal. So, let us go back to our goal.
(Refer Slide Time: 35:04)
 
Our goal was to determine the longest of the longest the longest of S 1 and the longest of S 2. And of course, so long as and we would happy. So, long as it is longer itself in S. So, this we know, this is our, this is what we just said or and this is R. What if I put R prime over here. So, this is no shorter. And so if I take the larger of this it will certainly be no shorter than all of this. And it will also lie in our original space. And therefore, this is also, so the longest of S and so if I substitute R prime over here. That is fine.
And therefore, this is nothing but longer of the longest of S 1. And no shorter than longest of S 2 provided this also lies in S. So, if I do this. I will still be happy. And therefore, this conclusion that we have written ((Refer Time: 36:27)) is good in the sense that, if I compute R prime in this way. I just have to return the longer of R prime and R, and that will give may the best answer for the original space itself. So, now we are ready to write down our general search procedure. So, let me write that down.
(Refer Slide Time: 37:08)
 
So, recursive procedure, which I am going to call LCS. It will take as arguments A of 1 through M, and B of 1 through N. And now, since we are going to write it as a procedure, let us we careful about boundary conditions. So, if A equal to null or B equal to null. That is as equal to saying, say these are strings of 0 length. So, say first index is bigger than the second index return null. So, clearly the longest common sequence of 2 null sequence or 1 null and something else is null sequence.
Now, now we have to worry about the main case. So, let us start by doing that. So, we are first going to see how S 1 is going to be searched. So, k is equal to smallest. Such that B of k is equal to A of 1 Remember, we are searching S 1 and that is what we did. If k equal to null as here or k is undefined that is my short form for that is k equal to null is a short for saying that. Then, again will return null. Otherwise this will ((Refer Time: 38:37)). So, we are going to do all this.
So, we are going to write or I will just write it jointly. So, we are going to compute R equals all of this. So, let me write that down. Otherwise, will compute R equals A of 1 concatenated with LCS of A of 2 through M. And B of k plus 1 through N, then they will compute R prime. Now, this time we are searching S 2. So, that as LCS of A of 2 through M and B of 1 through N. And then we will return the longer of R and R prime. So, we will get R and R prime explicitly will compute there lengths.
And will return the longer of the two. Now, I should point out some technicalities. So, in this call we wrote down A of 2 through M. The moment we write down A of 2 through M, we are implicitly assuming that M is bigger than or equal to 2, which may not be true M might be equal to 1. In this case, I am going to assume that an expression like this will be evaluated as null. So, instead of passing, so this expression will be passed as null if M is in fact, smaller than 2.
So, what will happen then, when this comes over here in the recursive call and null will be return. But, which is exactly what we want over here. And so therefore, this interpretation is fine. So, I do not have to write that is special case, whether M is greater than or equal to 2. It is just handle by saying that, this just represents null strings or null sequence. So, this is our basic recursive procedure. Now, one possibility might be to write down the recurrence to estimate the time taken for this.
And I will let you do that. And you will see, that the time taken is actually quite large. It will be exponential. However, that is not where we stop dynamic programming require first go further. And that is that next step is in fact, the one which is going to improve the time for us. So, what this dynamic programming require. So, dynamic programming as such to characterize what recursive calls happen in this. So, let us look at that. So, first the recursive call is this.
After that, there are two recursive calls. So, there is a recursive call over here. And then there is a recursive call over here. So, is there any nice property, which we know for this recursive calls. Well, if I look at A the argument pass for first string it is always going to be some part of A. And in fact, the second index is always going to be M. Similarly, if I look at the second argument to the recursive call it is always going to be B. The second argument is always going to be N.
The terminal index in that array is always going to be N. In the first index could be anything, it could be k plus 1. And if you recurs again, this property is going to stay they say.... So, we are, so when we recurs the key idea is that when we recurs our calls are going to be something like this.
(Refer Slide Time: 42:39)
 
So, all calls, which are going to be made are going to have the form LCS of A of i to M, B of j to N. Where, i and j could be anything. So, this is an important observation. And this is an important part of the whole dynamic programming strategy. So, once we know that all calls are of these kinds. So, let me actually note that down (Refer Time: 43:10)) going to be a this kind. So, let me write this term say, where 1 is less than or equal to i is less than or equal to M. And 1 is less than or equal to j is less than or equal to N.
Dynamic programming says let us define a table let us called T. Such that it is entry is will save the solution answers to these calls. So, T i j will store LCS of A i to M, B j to N. So, actually I am going to use not I ranging over just 1 through M, but over 1 through M plus 1. And j will arrange over 1 through N plus 1. And the reason for that will become clear in a minute. It will serve as a certain kind of ((Refer Time: 44:18). So, this will be two dimensional table with M plus 1 rows, and M plus 1 columns.
And now our goal is simply to fill in the entries of this table. So, once we filling the entries of the table T of 1,1 will be LCS of A 1 through M and B 1 through M. And that is the solution we are looking for. So, if we can generate the procedure for filling these entries. And it has to be a non recursive procedure. Then we will be done. So, long as we say, in which order the entries are going to be filled. So, I claim that from this procedure of ours, there is a fairly nice way of filling in the entries. So, why is that? So, let me define fill i j as follows.
(Refer Slide Time: 45:20)
 
So, what is will i j going to do. It is going to fill the entry T i j. But, what is T i j. T i j is going to be the LCS let me write that term. So, it is going to be LCS of A i through M, B j through N. So, let me comment it. So, this is what the entries suppose to be. The fill is suppose to written this entry. Now, if we look at our original procedure then this is after all a string and this is another string. And we could just as will pass this string or these two subsequences. So, all I have to do is to mimic what happens in this procedure.
And see, how fill should really behave. So, the first step is, if A is equal to null or B is equal to null, then return null. So, in this case, what does being null mean. So, being null is simply if you want to saying, when does this expression not having any meaning at all. So, I will interpret that as saying if i is greater than n or j is greater than n, then it does not make sense. And then so we will return null. So, that is that take care of that. In the next step is I have to find out, what this smallest k is such that B k equal to A 1.
Now, here the string was starting from the first index A. Here, we are starting from i. So, naturally instead of 1 over here, I should be looking at the ith element over here, because this is nothing but a starting element of this string. And therefore, this should be the element that I look at. So, our corresponding step over here is going to be k equals smallest, such that B of k equals A of i. And of course, k is greater than or equal to j.
Why this because we want to search to happen from starting from j over here. Within this string we do not want this search to go before B. The next step over here ((Refer Time: 48:03)) was if k equal to null then return null. So, we could put that in. But, we do not as a terms out we do not need to. So, the next step is going to be R is equal to A of 1 concatenated with this LCS. But, remember that we do not want to recursive calls. So, we cannot write filled of all of this.
However, we are allow to assume that the table is already full. So, in which case what can we do. We can write over here T of A of 2 through M. But, ((Refer Time: 48:59)) A of 2 through M has to be interpreted in light of this A of 1 through M, which in our cases i through M. So, this 2 through M really is nothing but i plus 1 through M. So. In fact, we will be writing instead of writing A of 2 through M we will be writing T of i plus 1 comma k plus 1.
The next thing is R prime and again over here instead of LCS of 2 through M. Since, we started with A which is going from i through M this will be i plus 1. And this is not going to be 1 through N, but this will be j through N. So, this will mean that R prime will be slightly different. So, R prime will have to be table entry not LCS again, but the table entry. So, i plus 1 comma j, so this is what it was. And now we are going to return the longer of R and R prime. So, now here essentially done? So, we have return the fill procedure. So, let us see, how the fill procedure is working out.
(Refer Slide Time: 50:36)
 
So, let us look at our table. So, this is our table. So, in our table, say I is going in this direction, j is going in this direction. In the first step is, ((Refer Time: 50:48)) if I greater than M or j greater than N return null. So, this step is essentially says, that if this is the M plus 1th entry. So, this part of the table and this part of the table, so this is the N plus 1 entry over here or going to be all nulls, in this entire thing. Now, if we look at i j, how to fill i j it depends on i plus 1 k plus 1 and i plus 1 and j, these are the two table and ((Refer Time: 51:20)) depends upon.
So, let me write that out. So, if I want to fill, so this is some index i and if I want to fill this. So, this is this j over here. Then this entry is going to depend upon i plus 1 j. So, it is going to depend upon this entry. So, let me write that is an arrow over here. And it will also depend upon i plus 1 k plus 1. Just going back to this ((Refer Time: 51:46)) it depends upon i plus 1 k plus 1 as well. So, i plus 1 k plus 1 could be something in this. So, it could be some entry of over here. So, it will also depend upon this.
So, if I want to fill this entry. I need to have these entries already filled. But, that is very easy. How do I do it. Well, if I start filling the entries from the bottom. Then I am essentially, then I will ensure that when it comes time to fill these entries, these entries are already filled. And then the fill procedure will work correctly. So, that is all. So, that is what I need to write down next. So, that is very simply done. So, I just want to tell you. what the code is going to look like. So, first I want to make sure that these two things these two are properly set.
(Refer Slide Time: 52:43)
 
So, that will be done by saying for i equal to 1 to M plus 1. What happens, T of i comma N plus 1 is equal to null. And for j equal to 1 to M plus 1. T of M plus 1 comma j is equal to null. That just set the bottom row and the right hand side column to null. Now, comes the main point. So, for i equal to M to 1, for j equals N to 1 will just fill T i j is equal to fill of i j, and because our rows are going M to 1 backwards. Will make sure that fill of i j will find the table entries already filled properly. So, that takes care of the whole thing.
So, at the end of this is simply return T of 1, 1 that is going to be your final answer. So, what remains now, is to estimate the time taken. So, this time is going to be simply O of M. This is going to be O of N. And this is going to be O of M N. Because, of these loops over here and the time taken for fill i j.
(Refer Slide Time: 54:18)
 
So, how long this fill i j take. Well the only place, where time is taken there are two places where time is taken. So, one is this. So, this could take time proportional to the length of B. So, this could take time proportional to N. What about this. So, we are going to do the concatenation. Now, if you store the subsequences, which have been computed as arrays, then this could take time O of M. Because, we are going to take the subsequence and we have to concatenated. Or in fact, it would take time, it could be that the subsequences the entire things. So, it could be max of M and N. And maybe the longer computing the longer will also take time this much. So, over all the time taken for this is going to be O of max of M N. So, coming back this last part fill is going to take time O of ((Refer Time: 55:24)) max of M N. And so the total time is going to be the product of these two things.
(Refer Slide Time: 55:39)
 
Or in other words, total time is equal to O of M N times Max of M N. Now, let me just to summarize, let me just say that I will given you dynamic programming algorithm, which runs in so much time. There is a simple modification that you can make to this, using which we can reduce the times further. So, it could be it can be reduce to time O of M N. So, I am just going to write down the code for fill and we will not prove it correctness. But, the correctness will be include in the manner analog is it will be only slightly more complicated. And so I will just do that and I will stop this lecture.
(Refer Slide Time: 56:24)
 
So, this is the code, which will give you faster result. It will require that you store the result being calculated. The table entry should be stored as a linked list. Rather than storing the entire subsequence in each entry in the table, store it as a linked list. If you do that, with the little slight trick you should be able to reduce the time taken just down to O of M N ((Refer Time: 57:58)). So, with that I will stop this lecture.
Thank you.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 20
Matric Chain Multiplication

Welcome to another lecture on Design and Analysis of Algorithms. Our topic for today is Matric Chain Multiplication. Let me begin with an informal statement of the problem that we have.
(Refer Slide Time: 01:02)
 
So, our problem is something like this. We are given n matrices. Let us call them M 1, M 2, all the way till M sub n. And we would like to multiply them and form this product. So, this is the usual matrix product. We would like to do this as fast as possible. Here are some of the issues involved in doing this. If the first issue is what algorithm should be use, when we just multiply to matrices together, because we are going to use this algorithm repeatedly. That is how we are going to solve this problem today.
The second issue is since matrix multiplication is associative. How do we parenthesize this product. Because, that will, it will be clear zone. That will also affect the time required to do the entire to perform the entire product. So, let us look at the first issue.
(Refer Slide Time: 01:55)
 
So, let us say we want to compute C equal to A times B. Where, A is a p by q matrix. B is q by r matrix. A and B are given to us. And C is a p by r matrix, which is to be completed. The simplest algorithm for doing this is to use a definition of matrix multiplication. Matrix multiplication is defined as C i j is equal to summation over k, k going from 1 to q of a i k times b k j. Computing the single term such as C i j will require O of q time. C has p r elements overall C is a p by r matrix. So, it has p r elements. So, the total time will be O of p q r.
Now, terms out that there are faster algorithm possible. But, we are going to strict to this algorithm for simplicity. So, we will assume for the rest of the lecture. That if I want to just multiply 2 matrices together my time is going to be O of p q r or to wake it even simpler. I am going to just say p q r is the time required for multiplication of 2 matrices. p by q matrix by and q by r matrix.
(Refer Slide Time: 03:13)
 
The second issue concerns associativity. Let me explain this with specific example involving only 3 matrices So, say n is equal to 3 over here. Now, we need to compute the product M 1 times, M 2 times, M 3. There are two ways to do this. One possibility is multiply M 2 and M 3 first. And then multiply that with M 1. Or multiply M 1 and M 2 first. And then multiply that with M 3. Let us, take a specific example.
(Refer Slide Time: 03:55)
 
So, let us assume that M 1. M 1 is a 3 by 2 matrix. M 2 is a 2 by 4 matrix. And M 3 is a 4 by 1 matrix. So, now if you want to multiplied according to this manner ((Refer Time: 04:23)). Then here is what will happen. So, we will first compute matrix M 4, which is M 2 times M 3. So, M 2 was 2 by 4 matrix. And M 3 was a 4 by 1 matrix. So, the product will be following over formula p q r, it will be 2 times 4 times 1, or it will be 8. Now, given M 4 our final answer can be computed just by multiplying M 1 with in.
M 1 is a 3 by 2 matrix. And M 4 we calculated ((Refer Time: 04:58)) 2 by 1 matrix. So, this will require time 3 by 2 by 1 or 6. So, this will require 6 matrix multiplication, 6 scalar multiplication this will require 8s scalar multiplications. Then the total number of multiplication required will be 14. Another possibility, which I have indicated over here, is to first form M 1 times M 2. So, we compute M 5 equals M 1 time M 2.
Remember, that M 1 was in 3 by 2 matrix. And M 2 was a 2 by 4 matrix. So, the total time over here, to calculate M 5 is equal to 24. The answer now, can be obtained by multiplying this M 5 which is just this, which we calculated by M 3. Now, M 5 in the product was 3 by 4. So, it is P and Q are 3 and 4. M 3 is 4 by 1. So, Q by R is 4 and 1. So, this product requires time 3 times 4 times 1 or 12. The total time for this way of doing things is therefore, 24 plus 12 which is equal to 36. So, clearly this first way is better.
(Refer Slide Time: 06:18)
 
The problem now is that, if we are that we can take our product. And we can parenthesize it in this way or we can parenthesize it in this way. We could speak of parenthesization. But, I would like to point out that these parenthesization can be you can also be thought of it rooted trees. So, let me explain that.
(Refer Slide Time: 06:43)
 
So, here are our metrics to be multiplied M 1, M 2, M 3. So, if I want to first form the product M 2 times M 3. Let me represent it in our usual fashion by drawing vertex over here. And this vertex represents there product. The next thing I do is, I take the product of these two. And this is my final answer. So, this corresponds to the parenthesization M 1 times M 2 times M 3. Alternatively, I could also have done something like this. So, I could take product of M 1 and M 2 first. So, this is the first way of doing things.
This is this is what I do first and then I take the product of those. So, the parenthesization corresponding to this is M 1 times M 2 whole times M 3. So, this is this we have doing things which we had early indicated earlier. In this particular example, we found that this ((Refer Time: 07:49)) superior. But, of course, with other choices of the matrix to mentions, other way could be superior this could be superior. We want multiply M matrices ((Refer Time: 08:04)) just 3. So, let us do one more example, what if we have 4 matrices.
(Refer Slide Time: 08:07)
 
So, in this case, we have addresses M 1, M 2, M 3, M 4. So, the parenthesizations here, will corresponds to 4 trees with 4 leaves. So, one possible parenthesization, which I might indicate is something like this.
(Refer Slide Time: 08:32)
 
So, say for example, I could compute say the product of M 2 and M 3 first. Then, maybe I compute product of M 3 and M 4 next. And then finally, I compute this. So, this is my, this is the tree. Of course, there are many such trees possible. And in fact, in this case let me just write down all the values all those. So, say for example, here is another tree. So, here is another tree, in which we first multiply and 3 and 4. Then multiply that with M 2 and then with M 1.
But, that is not only one here is another. And then, we also have things like and also. So, in this case, it terms out that there are 5 trees in which M 1, M 2, M 3, M 4 can be the leaves. And therefore, 5 ways of parenthesize in this product. As you saw to go from 3 to 4 we went from 2 to 5. If you keep doing this, you will see that the number of tree is grow is very fast. In fact, you can prove you should be able to prove that it grows exponential. So, in the pointers that we want to pick a tree from one of these possible trees, and we want to pick the 1, which minimize this the total time taken. So, we are now ready to give a formal statement of the problem.
(Refer Slide Time: 10:31)
 
So, our input consist of an array d zero through d n. So, this is going to represent the dimensions of the matrix. So, d i minus 1 and d i respectively will give the number of rows and columns of matrix M i. So, this means that the columns of the number of rows of matrix M i plus 1, will be d i. So, which will be equal to the number of columns of matrix M i. Which is as I should be, because only then can be think of the product M i times M i plus 1.
So, although we have M matrices, which are being multiplied and this is supposed to represent the dimensions of N matrices. We only have a, have N plus 1 numbers over here. Our output is suppose to be a rooted binary tree with M 1 to M n as leaves. And this whole thing should give us the least time for computing the product. Let me now, state as state again. That if you give me a tree, I can estimate the time required for the tree. And the algorithm here is actually fairly straight forward.
So, if we give me a tree, then the time required for the left sub tree can be estimated recursively. The time required for the right sub tree can be estimated recursively. And then, I just add up the two times. But, that does not finish the time the total time. I have to take the product represented by the left tree. The product represented by the right tree and I have to multiply them together. So, I need an additional term over here. So, just to clarify suppose, we have we consider a tree, in which the left sub tree has I leaves.
(Refer Slide Time: 12:28)
 
So, say M 1 through M i belong to the left sub tree. And M i plus 1 through M n are leaves for the right sub tree. And so we take the product over here. So, this has some cost let me call it C L and let me call the cost of this C R. So, what is the cost of this entire tree? Well, So, the cost is going to be C L plus C R plus computing the product of these matrices with the product of these matrices. But, what is the product of these matrices, what is the dimension of the product of these matrices.
So, that is simply the number of rows in this matrix, which is d 0, times the number of columns in this matrix. So, let me write over here. So, this are here, this product is going to be d 0 times, the number of columns over here. So, times d i, this product is going to be d i times d n. So, as a result, what we are going to get is the total time or the time to computer this lost product, which is the only term remaining over here is d 0 times d i times d n.
So, stated here, the time taken is, the time for the left sub tree, time for the right sub tree plus d 0 d i d n. So, here I was using subscripts. Here, I will just use array accesses, but you know there will be in the same thing. I mean the same thing by them.
(Refer Slide Time: 14:40)
 
So, how do we solve this problem. So, here is a simple brute force algorithm. So, we generate all possible n leaf trees. And will use, will calculate the cost for each or cost is really the same as time in optimization ((Refer Time: 15:00)) more customer is to think of cost. So, I might use or I will use cost to represent time in the rest of the lecture. So, we calculate the cost of each of these trees. And then, will be return the 1 with the minimum cost. As remarked earlier, this is going to be too slow, because a number of trees is going to be exponential in n.
(Refer Slide Time: 15:22)
 
We would like to solve this problem or we would like to see, if dynamic programming can be used to design an efficient algorithm for solving this problem. So, let me give a quick overview of what dynamic programming the whole dynamic programming ideas. So, in dynamic programming the first step is to cost the problem as a search for an object over a certain search space. So, this certain search space, we have to define a ((Refer Time: 15:50)) clearly, just to clarify our thoughts.
Then, we also need to define clearly an objective function, which is to be minimized or maximized. In our problem, we have really done this step. Because, the search space is simply the search space is simply the space of all possible trees. And the cost function is the time and that is to be minimized, the objective function or the cost function. The next step is to design an algorithm, which partitions search space. And each partition is searched to find the best, the best element in it or in our case the best tree in.
Now, here is the key idea, each partition is to be searched, but we can do this recursively. So, we find the best element in each partition. And since, we want the best element in the entire space, which is staying the best of these best. Dynamic programming does not stop at this step 2. The last and most important step perhaps or the most unusual steps perhaps is that we are going to characterize, the recursive calls which over made over here.
So, in this we will making several recursive calls, we will ask the question. Is there anything interesting about those calls, is there any property that we can identify for those calls. Or somehow, can be say that the calls are only made on these arguments. So, once we get a characterization, we are going to build the table, in which the results of all these calls are going to be stored. So, we have identified, which are the calls which are going to be made over the lifetime of this algorithm.
And will have an entry in this table corresponding to each such call. Then will define a procedure, which I will call fill, which will be able to fill the table entries using other table entries. The key requirement is that fill should not be recursive function. If we can do this, we will have a dynamic programming algorithm. And if everything works right this will the a fast algorithm. So, let us see how we can make this work. So, the first thing is to look at the search space. So, let us quickly take a look at that.
So, the search space in our case is all possible trees with M 1, M 2 the matrices at the leaves. Note, that we have not actually been given the matrices. And we do not need them, we do not we are not actually going to calculate the matrix product. We are just going to design scheme for calculating the products.
(Refer Slide Time: 18:50)
 
So, even though I say, that M 1, M 2, M n are the leaves. I really mean, so I really mean there are some place holders M 1, M 2, M n where the matrices can eventually be placed. So, there are these place holders M 1, M 2, M n which are going to be the leaves. And we are going to consider all possible trees and that going to be our space S. Our object function is going to be a cost. We call it a cost, because that is something that we want to minimize.
So, our cost is going to be the time required for a tree. And we have already seen, how we calculate this time. So, our goal is to find the minimum cost tree from this S. The next step is to partition this S. Now, there are several ways. in which you could partition. If you have so this is the place, where we require some creativity. However, I am going to point out a very natural way of doing this partitioning. So, S is the space of all trees. I am going to define S sub i is that sub space.
So, that sub space in which consists of all trees, in which left sub tree of root has M 1 through M i at the leaves. And right sub tree of root has M i plus 1 to M n at the leaves. So in fact, it is going to be a tree which looks like this. So, ((Refer Time: 20:22)) in this picture, I have M 1 through M i at the leaves of the left sub tree. So in fact, let me call this sub tree L and let me call this R. Now, I have told you, what structure L is going to have, it could have any structure what so ever.
I have told you what structure R is going to have, that could also have any structure. So, all such trees, which have this form R going to be in this sub space S i. So, I am going to consider as a sub space for each value of i, i going from 1 to n minus 1. Because, I want at least 1 leaf in this side and 1 leaf in this side. So, if I take the union of these subspaces S 1, S 2 all the way till S n, S n minus 1. I will get my original space. So, every tree in this original space is place, somewhere in one of these subspaces.
The next question is how do I search S i. So, if we can design an algorithm for searching S i. Then will be able to find the best in S i. And then, will take the best of the best as we has said earlier. So, whenever we look for something it is usually a good idea to characterize to design some property to define some property of it. So, as I am looking for a tiger in a forest. Well, it will be useful, if I say that a tiger has strips or it is yellow or it is a large animal.
So, if we can define some properties for the best tree that we are looking at. So, let us say T is the least cost tree in this S i. And if we can define some properties for it, then it will help us in actually finding T.
(Refer Slide Time: 22:18)
 
Now, here is the main property of T that we are going to use. And it is actually a fairly natural property. So, the main lemma that we are going to prove is this. So, left L be the left sub tree of T, which is a least cost tree in the side. I am saying a, and not b least ((Refer Time: 22:41)) cost tree, because there could be several least cost trees. So, T is just a least cost tree in our subspace S i. And L is it is that sub tree. The interesting observation or the interesting ((Refer Time: 22:57)) perhaps right now.
Is that, if t is a least cost tree. Then somehow, we expect that L also should be a least cost tree. But, L is only over the L only is computing the product M 1 through M i. And so, our claim is that L must be a least cost tree for computing the product M 1 through M i. And the proof of this is very follows in the standard dynamic programming ((Refer Time: 22:28)) argument. So, we are going to assume ((Refer Time: 22:30)).
So, we will assume that L prime be the optimal or the best tree for this product M 1 through M i. And it is cost sorry and the cost of L is actually, greater than the cost of L prime, which has to be the case, if you want to claim that L is not optimal. So, L has cost strictly bigger the L prime. So, let me draw picture over here another picture.
(Refer Slide Time: 24:02)
 
So, this is L, this is R and say this is my tree T. Now, what we are just claimed is that there is T L prime. And this has leaves M 1 through M i. And the cost of this is smaller in the cost of this. Of course, L must also have leaves M 1 through M i. And let me write down the leaves over here as well this should be M i plus 1 through M n. Here, is what we are going to do. Here is sort of the standard dynamic programming idea. I am going to construct another tree T.
But, this time I am going to place L prime instead of L. So, my tree t is tree T prime is going to looks something like this. So, I am going to have L prime over here. And I am going to retain R as it was. And I am going to connect these two together. So, this is my tree T prime. So, I start, so T is known to be optimal. So, this T is optimal and I do not know anything about this. But, I would like to estimate the cost of this. So, let us look at the cost of T first.
So, cost of T is equal to cost of L plus ((Refer Time: 25:35)) cost of R plus d zero d i d n. So, this just comes from this observation, which we had made earlier. Now, what we know about cost of L, cost of L is bigger than cost of L prime. So, if I substitute L cost of L prime over here. I get that cost of T is bigger than cost of L prime plus cost of R plus all of this. Now, this entire expression really, if you look at thus picture over here, you can see is nothing but cost of T prime.
So, this entire expression ((Refer Time: 26:16)) is equal to cost of T prime. So, what have you proved, we have proved the cost of T is bigger than cost of T prime or ((Refer Time: 26:27)) T prime has a smaller cost than T. But we had made a claim about T. So, we had claim the T is optimal. So, T is optimal for this space S i. So, T is the best possible tree of this form with I least on the left side it is a least cost tree. But, we have just found ((Refer Time: 26:52)), that t prime has even smaller cost than that. So, we have a contradiction.
So, we have a contradiction and therefore, our basic statement must be true. That L must be ((Refer Time: 27:07)) L must also be a least cost tree. So, if T is a least cost tree, then its left sub tree L must also be a least cost tree. But, ((Refer Time: 27:18)) we can apply the same argument to the right side as well. So, we can argue that the right sub tree of T must also be the optimal sub tree over it is leaves. So, we are now, ready to design a recursive algorithm.
(Refer Slide Time: 27:39)
 
So, we will call our recursive algorithm MCM stands for metrics chain multiplication. It will take as argument then array, the array which gives the dimension of the matrices. And it will return the optimal tree. We do not have the leaves the matrices themselves. So, the tree will just P the tree will just have an adequate number of leaves. And the leaves will serve as place holders for the metrics. We are only interested in the structure of the tree anyway.
So, we are not actually going to perform in the matrix multiplications themselves. But, we are just going to indicate the order in which the matrix multiplication should be performed. The basics for writing this algorithm is going to be the lemma that, which has proved. But we need to first take care of some base cases. So, suppose our array only represents a single matrix, in which case this n could be 1. We could be given just two number, the number of rows and the number of columns.
In this case of course, there is no multiplication to be done. But, this is the base case and here we will written a tree. But, this tree will just consist of a single vertex. Now, we are ready to generate the algorithm that we use the lemma generate our algorithm. The basic idea is that we are going to explore each subspace S i. So, here is the code for doing that. So, I here is going to be the subspace that we are going to be exploring. And I will take value is 1 to n modulation one.
So, for each subspace we will first calculate, the best left sub tree that must be made that must form, the that must be a part of the optimal sub tree optimal tree for that. So, as for the previous lemma the left sub tree of the optimal sub tree optimal tree for this subspace must itself the an optimal tree. So, to do that, will recursively call MCM. But, this time we only want the optimal tree over the first I leaves. So, likewise we will only we passing the i plus 1 elements of this array.
So, i plus 1 elements will define the rows and columns of the sizes of the number of rows and columns of the first i matrices and that is what we will pass. Likewise, will construct R i, which will be the optimal right sub tree, which will be the part of the optimal tree for S i. So, this will be done by this call MCM of d i through n. And this will simply be the dimensions of the remaining n minus i metrics. C sub i is going to be the cost of this. This is going to be computed using this observation.
So, it is going to be the cost of L plus the cost of R ((Refer Time: 30:50)) and the cost of multiplying the two the results together. So, this is what we have over here. So, it is going to be cost of this tree, cost of this tree that we get and the time required or the cost required to multiply the results corresponding to these two. Note over here, that I am using this, this expression cost of L i. And you may think of it either as a function, but more accurately you should think of it as a field selector.
So, when we return the tree itself, the tree will also contain its cost. So, we are just extracting the cost out of it. So in fact, as we designed the algorithm we should consider that we are not only written in the tree. But, we are also written in the cost of the tree. Next, we will have all these C i ?s and we just want to find the minimum C i. And let us call that i for which C i is minimum. And let us call that i, lets us use i to denote time.
And then, we are simply going to take the corresponding tree in the corresponding left and right sub trees and make a tree out of those and written that. So, this will typically we something like L i will be a pointer to the left sub tree or I will be a pointer to the right sub tree. So, we will construct the new node, which will be the new root. And we will make the root point to align R i. And since we also want to written a cost, we will attach an additional field, which is C i over here. So, this is the end of the procedure. So, this is the end of the recursive algorithm that we wanted.
(Refer Slide Time: 32:39)
 
Now, we could work with this, but to simplify a matrix. We will only consider will consider an algorithm, in which we only compute we only compute the cost and not the tree itself. So, the idea is going to be very similar to the previous algorithm in that it going to be simpler. So, earlier we said that, if n is equal to 1 then be written single element. In this case, we are not going to written a element, but we are going to written its cost. So, in this case, we are going to written 0.
In the previous algorithm, we L i equal to MCM of this. But here, now we are going to use MCMC. So, MCMC is the cost the cost of the optimal tree. So, will L i will now become the cost, arrive will also become the cost. And so the total cost is going to be not cost of L i, but just L i plus R i plus whatever this was. And now, we are going to find the i for which this is the minimum. And then we will write this C of i. So, we will work with this and the rest of the dynamic programming procedure will work will use this.
(Refer Slide Time: 33:51)
 
Actually, we would like to simplify, we can simplify the whole thing. And here is however, simplify it. So, let me go back to this and let me try doing this one paper ((Refer Time: 34:01)). So, you can see both the things at the same time. So, the idea over here was that we are going to calculate this expression and take the minimum. So. In fact, this whole part I can replace by the following expression.
(Refer Slide Time: 34:25)
 
So, I will just write this as return the minimum over all possible values of i, of what are say in fact, i going from 1 to n minus 1. And what is it that we are going to take the minimum of it is just going to be this C of i over here. So, instead of L i, I will just substitute this MCMC. So, I will write that as MCMC of d 0 through i plus MCMC of d i through n plus d of 0 times d of i times d of n. So, I just taken this expression ((Refer Time: 35:25)) and I have substituted MCMC over it here.
And this MCMC over here and I get this expression. So, that entire part could be represented as this. So, we are going to take this expression for all values of i and we are going to find the minimum fit. And that is what we are going to return. So, this is the resulting expression ((Refer Time: 35:47)). So, if n is equal to 1 again will return 0 otherwise will return this, which is what I have return over here. The final step in dynamic programming is to characterize the recursive calls.
(Refer Slide Time: 36:01)
 
So, our recursive calls, which we made ((Refer Time: 36:04) for this procedure MCMC of d 0 through n. And we called d 0 over i over here and d i n over here. So, we MCMC ((Refer Time: 36:14)) was called these arguments. So, if we look at how MCMC would progress further, you will notice that each time we are taking our original range. And then we are splitting get in some way. So, here we are taking a prefix of the range. Here we are taking the suffix of the range.
If we take a prefix of the suffix then we will get some range for not necessarily a suffix or prefix of this. But, never there is always some sub range of this. So, our characterization could be something like this. That MCMC is always called is a sub range d j k, where j k is the sub range of 0 through n. So, our entire array was d 0 through n. But, will call it with some d j k. So, this allows just define a table.
Since, we know what all the recursive calls are going to be, we are going to allocate one entry for each possible result. So, here is the possible result. So, d j k, where 0 less than, j less than, k less than n. So, this result of the call MCMC on this is going to be store in table entry T j k. Our next step is to define a non recursive procedure fill, which will be used to fill this. So, if we call fill with j k, we should get exactly what is suppose to be filled over here or it should return MCMC of d j k.
So, MCMC of d j k, so you need to figure out what exactly is MCMC of d j k. So, let us try and execute MCMC with the arguments d j k. So, for that we need to go back to the definition of MCMC. So, here is what we need to do, we want to execute the call MCMC of d j k.
(Refer Slide Time: 38:09)
 
So, here is our basic procedure ((Refer Time: 38:25)) MCMC. So, this is being called the parameters over here are the entire sub range 0 through n. However, we want to call it with this sub range. So, assume of course, that the compiler will rename and make this appear like 0 through n. But, when it does that, what exactly will this return. What exactly how exactly will this execute. That is what we need to understand. So, the condition n equal to 1 over here. Just say, whether this last index is 1 plus this first index.
So, this condition really should be thought of us. If the last index k is equal to j plus 1. In that case return 0. Otherwise, it is going to return this expression. So, we are still going to return this expression, but wherever the first index 0 appears will just put in j and wherever the last index n appears we just put in a k. So, that exactly what we are going to what that procedure should be doing. So, MCMC even called on d j k will work something like this. So, it will return say min our i going from not 1 to n minus 1.
But, instead it will go from j plus 1 to k minus 1. And it will compute MCMC of d of j i plus MCMC of d of i k plus instead of this we are going to have the corresponding right bounds ((Refer Time: 40:29)). So, we are going to get d of instead of 0, we will get j times d of i times d of k.
(Refer Slide Time: 40:45)
 
So, this is the value that should be return by fill. So, here is what fill in fact, as. So, fill is going to do exactly this ((Refer Time: 40:56)). However, fill is not really allow to reference MCMC and in fact, filled does not need to reference MCMC. So, when it wants to compute MCMC of d j i. It fill knows that this is going to be stored in d j i. So in fact, instead of MCMC fill is just going to return d of j i. Instead of MCMC of d i k fill is just going to return T i k. And then this part is going to be the same as before. In fact, that is what fill is going to be returning.
So, this procedure fill that we have defined over here is the right procedure it is non recursive. It does not think talk about MCMC does not call anyone else, but it is look at the table entries. So, this is what we needed, we want it to have a procedure, which tells us how to fill the j k th entry assuming that other entries are full. So, this is what it is doing. So, now we are really pretty match ready to right the dynamic programming algorithm. So, what we need to do next is to characterize how fill works. So, let me write down over table first.
(Refer Slide Time: 42:22)
 
So, our table T is going to look something like this. So, here is the first index, index which is i here is the first index, which is the second index which is j, sorry our first index is j and the second index is k. So, let us now look at the first step the first statement and fill. So, it is says that fill ((Refer Time: 42:56)) the table must contain as 0, if k equals j plus 1. So, this is 1 1. So, this will be the entry 1 1. So, this is not of interest. So, if k is equal to j plus 1. So, this entry is going to be a 0. This is where k equals j plus 1.
But, that is not the only entry. In fact, another entry, which will be 0. Because, of this will be this entry and so on. So, these are simply entries above the mean diagonal. So, this could be the mean diagonal. And all the entries above the mean diagonal will be 0?s. And in fact, the whole thing is going to be define for j less than or equal to k. So, in fact it is going to be defined on this side. This side is not going to be use, this side and the diagonal are not going to be used at all.
Now, let us examine how does fill ((Refer Time: 44:01)) a particular entry j k. So, let us say this is my j over here. And so this is the entry that I want to fill. And this is the index k. So, the jth row kth column this is the entry that I want to fill. How does filled ((Refer Time: 44:21)) fill this entry. So, it considers the values of T j i, where j ranges sorry T j i where i ranges from j plus 1 to k. So, these are simply table entries in this region. So, let me put them in a different color, so it so these are the table entries.
So, these are the table entries, which are needed for updating this. So, let me put an arrow going from here to here. But, it also uses table entries i k ((Refer Time: 44:53)) again I going from j plus 1 to k minus 1. So, which are these entries. Well, these are the entries, which are below over here. So, the entry is above the main diagonal. But, below this element are also going to be used. So, if these entries have been have already been defined. And these entries have already been defined.
Then fill can fill in this table entry. So, then the entire procedure is extremely simple. What is that we have to do. We must ensure that when it comes time to fill this entry. All these entries must already have been filled. So, ((Refer Time: 45:41)) that such as is a very simple order in which we can fill these entries.
(Refer Slide Time: 45:47)
 
So, here is my table. So, here is the main diagonal. So, this diagonal I am not going to do anything with. But, the entry is above that I am going to mark as 0?s. Then, I am going to start filling entries. But, I should fill entries such that if I want to fill this entry. Then, everything on this side and everything on this side is already filled. So, next I can fill this entry maybe. So, I will put a 1 over here. After that, I can fill this entry. I can fill this entry. Then I can fill this entry, this entry, this entry, 7, 8, 9, 10. In general, I can let me use another picture. I can take my matrix.
(Refer Slide Time: 46:39)
 
So, this is my matrix, this is my table. I will filled the second diagonal with 0?s. Then, I will fill the entries, the rest of the entries in this order. First, I will fill these, then I will fill these, then I will fill these, then I will fill these and so on. The point is that, if I have fill entries in this order. Whenever, it comes time to fill say an intersome where over here. I would already have filled these entries and these entries on which this entry depends. So, that is the key idea. So, if these entries are already filled then you can fill this entries. So, the only thing that remains to be done now is to figure out, how much time the whole thing ((Refer Time: 47:28)) will take.
(Refer Slide Time: 47:32)
 
So, the total time taken is clearly equal to number of entries in the table. Times time to fill 1 entry. So, this is already telling us, everything that we want to know. How many entries do you have in the table. Well, T has all this entire range j k, where j k lie between 0 and n So, we have a two dimensional table with n rows and n columns. We are only using half of it. But, clearly the number of entries is therefore, O of n squared. What about the time to fill a single entry.
So, T j i and T i k are already known, when we ((Refer Time: 48:29)) are going to have to compute this product. And we are going to do these additions. But, and this has to be done these many times, so far every value between j plus 1 and k minus 1. This range can be as large as n. And therefore, the time to fill a single entry is going to be O of n again. So, the whole thing is going to be O of n cube. So, that really ((Refer Time: 48:58)) finishes the entire algorithm.
(Refer Slide Time: 49:02)
 
So, the total time is n square times n which is equal to O of n cube. Now, I just want to make one more comment. In the comment is about this simplification that we tell. So, we said that, we are not going to compute the exact tree that we wanted. But, we are just going to compute, the cost of the optimal tree. So, I want to leave you with an exercise, which is how can you modify this algorithm. So, that you actually, calculate the tree and not just the cost.
(Refer Slide Time: 49:44)
 
So, this is exercise. So in fact, the idea is something like this. So, I am giving your hint over here. In each table entry, store the root node of the tree along with the cost of that. The root node will contains pointers to the left and right sub trees. Now, if you go and ask the question, how do I update a single element of the table. You will have to construct the root node as well as these pointers and as well as these cost. But, you will see that each filling now will still be possible in O of n time. And since the number of entries are O n squared the total time should still be O of n cube. So, that concludes this lecture.


Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 21
Scheduling with Startup and Holding Costs

Welcome to another lecture on design and analysis of algorithms. We will see 1 more problem which can be solved nicely using dynamic programming today. The problem we are going to solve is scheduling with startup and holding costs. Let me define this problem for you.
(Refer Slide Time: 01:10)
 
So, in this problem we are given a machine which has the capacity of producing 1 unit of something whatever it is per day. However, if I want to start a machine a machine on some day which is so, what I mean by that is that if the machine was not on today. And I want to get it working today then I have to pay a startup cost and that startup cost is some S units which is defined as the part of the input. Clarifying again if the machine was on yesterday then I do not pay a startup cost today if I want to keep the machine working. If the machine was off yesterday, but I want to start it today then I will have to pay a startup cost. We also have a warehouse to hold units which have been produced. So, if I produce something today and it is not to be delivered today itself then it goes to a warehouse. But of course, if I place something in the warehouse I have to pay some rent. So, let us say that we will call that the holding cost and that holding cost will be some H rupees per night. The main input to the problem consists of a daily demand for n days. So, we are given the vector D of 1 through n in which D of I represents the demand for the ith day.
We are also given the startup cost S the value of S and the value of H the holding cost and what we are supposed to produce is a schedule. So, we are supposed to produce a vector P of 1 through n in which p of I denotes whether or not the machine is to be kept on the ith day or in other words whether the machine should be producing anything on the ith day. So, the final requirement which is which might be obvious perhaps, but let me state it nevertheless is at the end of n days all the units that have been manufactured must have been delivered. So, we cannot be left with any inventory at the end of n days and of course, it is possible that the demand that is being given to us is just not satisfiable. Because after all our machine can produce only 1 unit per day. So, in n days the machine can produce n units. So, certainly if the total demand is more than n we will not be able to meet. There will be other conditions under which the demand could not be met, but whatever these conditions are our algorithm must report if the given demand cannot be met.
(Refer Slide Time: 04:01)
 
So, let me now take an example to illustrate this problem better. So, in this example this is the demand vector that is given to us. So, what this means is on day 0, day 1 nothing gets to be delivered nothing needs to be delivered, day 2 nothing needs to be delivered. On day 3 there is a order of 2 units which has to be delivered day 4, day 5, day 6 nothing has to be delivered and on day 7 there is an order of 3 units. The holding cost that is the cost of storing 1 unit in our warehouse is 1. So, it is 1 per unit and the startup cost is five. So, here is 1 possible schedule or 1 possible production plan which will meet this. So, I have shown this in a tabular form over here. So, we have days on the x axis. So, 1 through 7 since there are 7 days the demand is given as 7 days on the third day there is a demand of 2 as specified over here. And on the seventh day also there is demand and in this case the demand is 3. Here is a production plan which will meet this demand. So, notice that on this day 2 units have to be delivered and on any day we can deliver only 1 unit. So, which means we have to start producing earlier.
So, say in this case we have to start producing on day 2. So, we will produce on these 2 days and then for to meet for this demand we will produce on these 3 days machine will be idle on this day and not this day. Since the machine is going to start having the idle on the previous day there will be a startup cost on day 2. So, this will be a cost of 5 similarly on this day the machine will have to start again been idle on this day. So, there will be another startup cost on this day, day 2 whatever has been produced is not going to be delivered. It is only going to be delivered the next day. So, it will have to be held in the warehouse and. So, there will be a inventory cost of 1. Similarly, on this day whatever has been produced will have to be held in the warehouse whatever is been produced in the warehouse on this day will also be held. So, at this point there will be 2 units in the warehouse. So, holding cost of 2 will be incurred here this is of course, not the only plan here is another.
So, in this case what we have done is instead of keeping a gap on day 4. We have begun the production earlier, but notice if we begin the production earlier then we have our holding cost here as well. So, infact, the 4 dates we have to hold in our warehouse whatever unit has been produced. On the fifth day whatever has been produced has also to be held in addition to whatever have produced yesterday the day before that. And finally, whatever is produced in the sixth day there have to be held and all these 3 units will have to be delivered on the seventh day which of these is better. Well, let us calculate the time taken and the cost of each the cost of the first plan that is simply the startup cost which is 5 plus 5 here plus 1 plus 2. So, this cost for the first plan is 14 the cost for this plan is 5 startup 5 for the startups and then 1 plus 1 plus 2 plus 3 which is written over here and this adds up to 12. So, eventually this plan is better our question in general is going to be to consider all such possible plans and figure out which is the best 1.
(Refer Slide Time: 07:56)
 
So, this naturally suggests a brute force algorithm. So, we will generate all possible plan for each will evaluate the costs and then will pick the best. Unfortunately the number of such schedules or such plans is going to be exponentially n where n is the number of days. So, this is going to be just slow and we would like to have a faster algorithm this is where the dynamic programming comes. So, here is a quick view of the dynamic programming. So, the first idea is to find some kind of a repulsive solution. How do we do that? Well, we cast our problem as a search for some object over certain search space. So, it is useful to define the search space quite clearly as well as it is important to define clearly an objective function which is to be minimized or sometimes it has to be maximized whatever it is it has to be defined very clearly. Then we design a algorithm which searches the space typically the algorithm is going to partition the search space. 
So, it is going to say let us divide the search space into spaces and each space or each part of the space each subspace is going to be searched separately. So, I search the first subspace and I get the best solution. And I get the optimal solution in it what I mean by that is the solution which minimizes the objective function. Or if we want maximization the solution which maximizes that objective function. So, we calculate the best optimal in each search space and then return the best of the best. So, that is the general idea of getting a recursive solution dynamic programming; however, does not stop at this point. It proceeds further in the following sense the idea is that will characterize what the recursive calls are in this part. So, essentially we are going to ask the question what is that we are what are the problems that we are solving? So, we essentially make a table of all those problems and in each cell of the table we would like to store the results of those. 
So, we will define such a table then will define a procedure for filling table entries and this procedure will be a direct procedure it would not very particular and it will assume that if I want to fill a certain table entry I can use entries which have been filled earlier. So, this will fill entries in the table given that other entries have been already filled. So, in this recursive procedure the recursive procedure might be slower, because it might calculate the same quantity several times. Whereas, in this case when we make the table we will carefully think about what it is that needs to be calculated and will calculate that exactly once. So, that is going to give us our efficiency. So, what we would like to do now is apply this whole idea to our given problem. The first step in this is to find the recursive solution. So, we want to cast our problem as a problem in the recursive fashion our algorithm in a recursive fashion.
(Refer Slide Time: 11:19)
 
So, here is how you might think of a recursive algorithm for our problem. So, we would like to solve a n day problem. So, typical steps in that might be that we somehow solve the first day and then we recurse on the remaining n minus 1 days. Now, unfortunately recursing on n minus 1 days will depend on what happens on the first day. So, say for example, if the machine has been switched on on the first day then when I recurse this information is very useful to me. So, if the machine is on then on the first day of recursion I do not have to pay the startup cost. So, which means our recursion must somehow include additional history information the most natural way of doing this is to generalize our problem.
(Refer Slide Time: 12:18)
 
So, let us see that. So, our generalized scheduling problem looks something like this it has the same inputs as before except there are some more. So, it has inputs the demand as inputs the demands for n days the startup cost the holding cost. And then there are 2 additional inputs and input I which gives the initial inventory. What I mean by inventory is that how many units have already in stock on day 1? So, when I begin the whole thing it is possible that I already have some units in stock? So, that number is specified as a part of the input and that is this number I. Then on the day which I begin the machine might be already on or off and that is specified by this variable m. So, M can take values on or off I can take values any integer the rest is similar well I should point out that our old problem which we had defined earlier corresponds to having I is equal to 0 that is no inventory when we start and M equal to off. So, this is the generalization in the sense that now we allow I and M to take on a wider range of values our output is as before. We are supposed to produce a production schedule and the requirements are same that at the end of n days everything that has produced must be consumed.
(Refer Slide Time: 13:54)
 
So, let us now try to define a recursive algorithm for this problem. So, the first step was to design or think about a search space for this problem and the objective function. So, let me call S the search space. So, what is the search space for the input problem as given. Well the search space will contain all possible schedules for this instance under instances characterized by these inputs. So, a instance is defined as D S H I M where D is a vector of n elements if it helps us to think about will think of each schedule as being as n bit vector. So, each bit specifies whether the machine is to be on or off on the corresponding day. The objective function is the cost and it just consists of the sum of the holding and the startup costs.
And our goal is to get the minimum cost schedule from this space S. Next comes how we are going to partition this here is a natural way in which the space can be partitioned? So, we will ask what does what happens on the first day. So, in sub space S sub p we will put all schedules in which machine is producing on day 1. In space sub space S sub I we will put all schedules in which machine is idle on day 1. Now, this union this is this in the sense that in every schedule in every element of s. Either the machine is producing on the first day or it is idle on the first day and therefore, this is equal to this union this. So, we have partition S and so the next question is how do you search each of these sub spaces.
(Refer Slide Time: 16:13)
 
So, let us look at S sub p first and we will try to think of how to search S sub p. Here is the key idea in devising recursive algorithms for optimization problems. So, if p is the least cost element of S sub p then the question we should be asking is will parts of p themselves be least cost solutions some smaller instance. If they are then we can use recursion. So, this is called the so called optimal sub structure idea. So, if p is an optimal solution in this subspace S sub p. The question is will parts of p will also be optimal for a smaller sub problem and in fact; it turns out in our case that that is true.
(Refer Slide Time: 17:00)
 
So, let P 1 through n be a least cost schedule in S sub p will show the parts of it will have to be optimal for a sub problem for a smaller instance. So, if p is the least cost schedule what do we know about it well we know that P of 1 is equal to true, why because p is the least cost schedule in S sub p and S sub p is a set of schedules in which the machine is on in the first day. So, P of 1 must be true P of 2 n is the rest of the schedule, but now I can think of P of 2 as a schedule for the residual instance from D 2. 
So, let me clarify this. So, our original instance was this D S H I M when n integer vector. The residual instance is just a part of this which begins in the second day. So, in the residual instance we have only demands for days 2 through n then S is the same the startup cost does not change. The holding cost does not change; however, the inventory on day 1 is going to be different on day 2 is going to be different on day 1 the inventory is 1. And on the first day we produced of the inventory that was present on day 1. We are going to deliver D of 1 which was what the problem require has to deliver and then it will add whatever we produced on day 1.
So, this is the inventory for this instance beginning on the second day the final component is the machine status. So, over here the machine status could have been whatever now we know since we did produce on the first day the machine status had better be true. So, this I the residual instance and clearly p 2 n must be a schedule which satisfies this residual influence. Here is the key lemma this lemma says that not only does it satisfy the residual instance, but in fact it has to be a optimal schedule for this instance. Or in other words it has to be a least cost schedule for this instance the argument for this is fairly straight forward. And it is of the typical argument in dynamic programming arguments. So, well do the argument by contradiction. So, we will assume that p 2 n is not an optimal schedule which means there are better be a schedule Q 2 n which is optimal for this instance and it is cost be better smaller than the cost of p 2 n. 
So, cost of Q 2 n must be smaller than cost of p 2 n, but now let us consider this schedule R where R follows p on the first day. Or in other words R is true on the first day and on the remaining days it follows Q clearly this is a valid solution for the original instance right. Because well on the first day we produced and then we were left with this instance and that is what Q took care of. Now, what is the cost of R? Clearly cost of R must be smaller than the cost of p why because cost of R is nothing but the cost for remaining days plus the cost of p. 
The cost of the first day this is the cost of R cost of p is cost with the first day plus cost of the remaining days which is not Q 2 through n, but p 2 through n. But in p we are adding this whereas; in R we are adding this; this is smaller than this. And therefore, the cost of R must be smaller than cost of p, but remember we assume that p is a least cost schedule in S sub p. But here we are showing there exists a schedule R which has even less cost than p and also the first element of R is true, because it is P 1 itself. So, R also belongs to S sub p and therefore, we have a contradiction. And therefore, our basic assumption must have been false or in other words our lemma must have been true then this is also optimal for instance the residual instance. So, this tells us how to search the space S sub p and a similar idea works for the S sub r.
(Refer Slide Time: 22:00)
 
So, let us say I 1 through n be a least cost schedule in S sub i. Then I 2 through n is a least cost schedule or is optimal for the instance D 2 n S H I minus D 1 false or this is the residual instance. So, see that the last component the machine status is false, because S I consists of schedules in which the machine is off on day 1. Therefore this is false and the original elementary was i but on day 1 we delivered something without producing. So, the new entry is I minus D 1. So, this is the residual instance and this remark claims that I 2 through n must be optimal for this residual instance. So, we have accomplished the goals that we set. So, we have shown how to search S sub p and how to search S sub I and this has been done by using recursion.
(Refer Slide Time: 23:14)
 
So, we are essentially ready to build a recursive algorithm for sorting this problem. Here is a outline. So, we will call our algorithm opts schedule arguments the entire demands and when I write D 1 through n over here I simply mean that D is going to be a vector with n elements. It will take additional argument which is the startup cost the holding cost the current inventory at the beginning and the machine status at the beginning. There will have to some best case that will have to take care of, but this is sort of the main core of the recursive algorithm. So, this part in this part we are going to search the space S sub p. So, we are going to find the optimal schedule for the recursive residual problem. Assuming that on the first day we do produce and that will be our schedule P 1 through n I 1 through n will likewise be the optimized schedule for S sub I and in this case. On the first day we are not going to produce anything and this is just the residual problem given that we do not produce anything on the first day.
So, these are the problems which we exactly looked at in the 2 lemmas that we just saw. And then we are going to look at the cost of the first schedule the cost of the second schedule and we are just going to return the best of these. So, this is the best of best idea. So, this is the best solution in the first sub space this is the best solution in the second sub space what we are returning is the best of the bests and number of details have to be filled up over here. However, you should be able to argue that this algorithm will take time O of 2 to the n see just try to recurrence and this will just come over. So, what we are going to do now is not to fill up the details the remaining details and there are a few important details which are missing over here. But what we have done is we have essentially identified what kind of recursion we are going to have. So, we will just proceed to the next step of our agenda.
(Refer Slide Time: 25:34)
 
So, the next step is the dynamic programming idea. So, this step is the key step of dynamic programming which is characterizing the recursive calls. So, the first observation is that opts schedule is always called using arguments D J through n and S H I M where J can be any number. So, let me just observe let me just point out why this is. So, initially opts schedule is going to be called with the entire input. But later on it is going to be called with the residual problem in which we are only going to pass the sub array beginning with the second index. Now, what happens in this call itself in this call we are again going to throw out the first element of the demand array and then we are going to call it with the rest of it. So, when we are doing the recursion on it we will be calling it with D 3 n and then when we recurse on that we will be calling D 4 n and so on. So, in other words the argument is going to be of this form that we will be that the first argument that is going to be passed will be some sub range.
Well, it will always be a terminal sub range of our input D. So, suffix of that D array and then these could be pretty much anything. So, J has to be some number between 1 and n. So, the largest index is n. So, J could be anything until n S and H get passed without any change what so ever the inventory can be at most l why is that? Well, we have n days during which the machine produces 1 unit per day and therefore, the inventory cannot ever build up to more than n. So, this argument will be an integer at most l and the last argument could either be true or false. So, we have a reasonably good characterization of all the recursive calls that could get made in our recursive algorithm. So, the characterization says that these are the arguments that are going to be passed and each argument can vary in this manner or not vary at all. The next step of dynamic programming says well now that you have identified what the recursive calls are going to be make a table in which we are going to store the table. So, we will construct a table T in which T will have 3 indices J I and M. So, T of J I M will store the result of optschedule of D J through n S H I M. The point of making the table really is that we just want to focus we just want to make a list of what are the sub problems that we are ever going to solve.
So, this is what we will do. Now, we can work with this table T; however, dealing with schedules is a little bit cumbersome because we are going to simplify the problem just for a few minutes. So, instead of working with entire schedules we will just work with the cost of optschedule. In the table T we were planning to store in each cell the entire optimal schedule instead of that we will build a different table let us call it C for cost. It will have same very similar entries similar number of entries with similar indices and in this we will just store the cost of the optimal schedule. So, now, we are now going to work with this table and what dynamic programming requires to do is to figure out how entries in this table depend upon each other. So, in another words if the entry C J I M has to be filled assuming the remaining entries are full what is the exact computation that needs to be done.
(Refer Slide Time: 30:13)
 
So, we need to derive some kind of recurrence for C J I M let me just remind you that C J I M is the cost of this optimal schedule for the sub problem or for the residual problem actually D J n S H I M. Let me just remind you that you did not have S H in the table, because they are fixed throughout the execution anywhere. Here is what the optimal schedule for D J n is going to look like. So, I claim that the optimal schedule for D J n S H I M is going to be the schedule with the lower cost from this schedule and this schedule. This really comes from the procedure that we wrote a minute ago. So, let us just see that.
(Refer Slide Time: 31:16)
 
So, let me just write this term. So, my claim is or what I want to examine is the optimal schedule for D of J through n S H I M this is what I want to understand and I want to figure out how what will the optimal schedule to this be? So, let us go back to the code that we wrote or the recursive algorithm that we wrote. And we said that the optimal schedule for D 1 through n SH I M is going to be one of these 2 now, one of these 2, when D is passed as the entire array is this, but only when a small smaller range is passed. So, the first argument is 1 what will this be? So, the first schedule will simply be true concatenated with optschedule and this time instead of passing 2 through n. I should really pass J plus 1 through n, because this was just dropping the first element. So, if I drop the first element from this this will become D of J plus 1 through n S H I M what about this?
So, this will become false and optsched D of J plus through n S H and in the inventory I should really remove what was present on day 1. So, or not on day 1, but on day J and I should have M this is what I should have. Here instead of inventory being I i should really have I minus D of J plus 1, because since the machine was on the first day I had to have a plus 1 over here. So, you can see that this is a this is exactly what I have written over here. So, true concatenated with optimal schedule for the demands D J plus 1 through n S H I minus D J plus 1 and true and the other schedule which is optimal schedule false concatenated with optimal schedule of D J plus 1 through S H I minus D J and false. So, now we understand what optimal schedule is. So, we just now have to figure out what the cost relationship amongst the cost is going to be?
(Refer Slide Time: 34:20)
 
So, C J I M is simply the cost of this. So, what is that going to be well this is the least cost of this? So, C J I M better be the minimum cost of the cost of these 2 schedules. So, what is the cost of these 2 schedules? Well, the first schedule starts with keeping the machine on day 1. So, here there is some potential for incurring a startup cost whether the startup cost is incurred depends upon whether this n was on or off or true or false to begin with. So, if M is true then there is no startup cost or the startup cost is 0. If n is false then the startup cost is S. So, this expression which I have written down over here is to be understood as the C expression that is the C expression mark true value colon false value. So, if M is true then this bracket evaluates to 0 otherwise it evaluates to false in other words this bracket over here represents the startup cost it is either 0 or it is depending upon M. Then on day 1 there is going to be some inventory cost. So, this inventory cost is given over here. So, the inventory for day 1 is going to be this whatever the inventory subtract whatever was delivered on D J add whatever was produced. So, times H will be the inventory cost and then there is a residual cost. So, it is just the cost of this part of the schedule. But this part of the schedule the cost of this is simply C J plus 1 I minus D J plus 1 true.
So, we have related the J I Mth entry to the J plus 1 I minus D J plus 1 2 entry. So, we have to take the minimum of this quantity and this quantity and in this case the computation of the cost is actually simple. Because there is since the machine is off on the first day this is false there is no startup cost, but there is some holding cost. So, what is the holding cost on day J something gets deleted the inventory gets reduced by I minus D j. So, the inventory is just this and the inventory cost is just this and this is the cost of the residual problem. So, now, we have a recurrence connecting C J I M to other entries in the table. So, this is the one other entry and this is 1 other entry there are some problems though. So, when we wrote down these numbers, we did not we have not so far considered the possibility that I minus D J plus 1 or I minus D J might become negative that does not make sense. So, I minus D J plus 1 or I minus D J is the inventory. So, it does not make sense for the inventory to become negative.
So, we somehow have to take care of that essentially what is going to happen is what should happen is that we should check that if we want to consider the schedule. And if we are requiring that I minus D J plus 1 be negative then we should not really be considering it at all and we should just use this. Here is a very nice fix to this a simple fix. So, we are going to define C J C of J I M to be infinity if I is less than 0. So, let us see what this does? So, in this example in this alternative for example, if in this table entry this is some negative number. So, we are asking for C of J plus 1 some negative number true now this entry would be defined as infinity which would mean that this number would be infinite. But if this number is infinite then this entire cost would be infinite. But since we are taking the min over here, that would force us to look at this cost. So, essentially we would be ignoring this cost.
(Refer Slide Time 39:00)
 
So, by defining C J I M equal to infinity we are saving ourselves the trouble of checking the indices are less than 0 in this expression and automatically getting the same effect. Now, of course, you might ask if we want to do this for all I less than 0, do we need to consider a infinite all the infinite negative numbers for I? Well, we do not and there is a simple reason for it, remember that the demand on any day cannot be bigger than l. Why if the demand is bigger than n then we cannot fulfill that instance anyway. And so, we can check at the beginning whether the demand on any day is bigger than n. In which case we just reject we just say that this problem is unsolvable. 
So, this algorithm or whatever algorithm we are going to design will only be called if D of J is less than or equal to n. But if D of J is less than or equal to n then we are only subtracting n from whatever has to be a positive number earlier. So, in which case it suffices I is bigger than or equal to minus n. So, we only need to consider I only a few negative values of n that is values of I that is values going from minus l to minus n. So, in other words our table is not going to become too large the final problem that we need to consider is the base keys. So, we relate entries with the table with other entries, but this cannot go on forever. So, some entries we have to set ourselves.
(Refer Slide Time: 40:39)
 
So, these are the entries for the last day schedule. So, remember that C J I M were the indices for the table entries. If you look at the last day then the first index over here should be n. So, in this case we are asking for a single day schedule. So, we should be able to set this without much doubt. So, here is the idea if we want to figure out what C I and M are to be and we need to figure whether there is a legal schedule. So, the main condition over here is that whatever inventory we come in with should not be too large or should not be too small. If it is too large then even after satisfying the demand we will be left with inventory and that is illegal that will not constitute a valid schedule. If it is too little we may not be able to satisfy D of n at all. So, the inventory should be just enough to satisfy the last day?s demand.
So, very simply if I is the last day?s demand if I takes the value last demand then we should run with the machine we should not produce anymore units and. So, in that the cost for the last day is simply 0. So, C of n I M must be 0 if I is equal to D of n if on the other hand the inventory is just 1 less as we come into the nth day. Then what the demand is then we would better produce to make up the inventory that we want to deliver whatever is needed to be delivered. So, we will have the machine be on and now for the last day we need to run the machine. So, our cost is going to be 0 if the machine was already off was already on and S if the machine was off. So, this is again the C style expression which evaluates to either 0 or S for all other values of I other than D n and D n minus 1. This means that we have too much inventory or too little inventory in which case we should set the cost to be infinite. So, that takes care of the best case as well.
(Refer Slide Time: 43:23)
 
So, let me pictorially show what has happened here or let me pictorially show whatever table is going to look like. So, our table is going to look like something like this. So, on this axis I am going to have I am going to have the days on this axis or this is where how J is going to vary on this axis I am going to have inventory . So, this is inventory greater than 0 on this side I have I less than 0 and on this axis into the page if you will I will have M . So, this corresponds to say on and this corresponds to off. So, I will draw the off in a different color. So, it will look something like this. So, there will sort of 2 sheets. So, even the second sheet. So, our table is going to look something like this now how did we fill the table? Well, in this case we looked at the last column on the days side. So, this is what we filled out all right. So, this part we filled out. So, this is nth day and this in the second part in the off side also we filled it out. So, these values are known then, because of this we divided C J I M equal to infinity where ever I was 0. We have also filled this out this part.
So, these parts are filled out to begin with by very simple ideas. So, the recursion part comes only for this part and this part over here. So, how is that filled? Well that is filled using these expressions. So, essentially if I want to fill this entry what am I going to use. Well, I am going to use some entry in the J plus 1 th column some value from this column and also the corresponding column of the off side on the, of the machine being off, because I am getting C J plus 1 true and C J plus 1 and false as well. So, this entry depends upon the next column. So, now our calculation is quite straight forward. So, this part is already been filled up easily by the last 2 days calculation and by this calculation. And then to fill each entry we will just use this expression and this entry can be filled in constant time as I have seen over here as I have shown over here. So, basically the idea is that we fill the entire table in decreasing order of J. So, we start from here and go backwards and we fill in the table. So, the number of entries here well let us calculate that carefully J ranges from 1 to n. So, here it is 1 and here it is n I goes from minus n and it goes all the way to plus n.
(Refer Slide Time: 47:20)
 
So, there are 2 n plus values for I and M can be on or off. So, there are only 2 values of M. So, that total number of entries in the table is just all possible choices for J all possible choices for I and all possible choices for n. So, n times 2 n plus 1 times 2 or O of n square time to fill each entry our recurrence can be evaluated in constant time or each single step can be evaluated in constant time. So, that is O of 1 and as a result the total time is O of n square. So, we will fill the entire table in O of n square time we should ask well we have filled this table, but which part is the 1 that we want.
(Refer Slide Time: 48:19)
 
So, coming back to this; this is the entry which we want. So, at the end this is our final answer or I am sorry not this it is this, this is the final answer or the optimal cost is found in entry 1 0 falls of the table. So, 1 is the J value 0 is the, I value. So, it is this entry on this axis itself and it is in the off or the false side. So, this just says what is the cost of generating a schedule for the entire demand? Given that there is no inventory to begin with and that the machine is off.
(Refer Slide Time: 49:12)
 
The final topic is how do we find out, how do I find the schedule given the table? So, remember that C 1 off was the cost of the schedule. Now, this cost is going to be a minimum of some costs which costs well, we note we noted that if I want to calculate this cost it is the minimum of some cost in this and some cost in the corresponding column over here. So, C 1 off would be the minimum of some expression involving some column some element in the next column on the off side as well as in the next column on the on side. So, we simply check whether C 1 is equal to the first term or the second term it is the minimum. So, it should at least be equal to at least one of these. If C 1 off is equal to the first term what do we know? We know that this must be generated by keeping the by using the first term or by keeping the machine on during day 1. If this term is equal to the second term over here then the machine should be off on that first day. So, in this way by knowing the optimal cost we were able to and knowing the table we were able to figure out whether the machine should be on or off on the first day.
And we are also able to figure out what the corresponding entry for the optimal schedule for the residual plan is is it this or is it this So, then we can apply this argument again and again and will get machine status for every day and will does generate the entire schedule. So, let me now conclude. So, in this problem in today?s lecture we saw another problem for which dynamic programming could be used before using the dynamic programming. There was a important step that we took which is quietly an important and interesting step very often we are given problems for which to device recursive algorithm a we need to generalize the problem formation itself. So, here is 1 such example you must have seen similar example in another problem which was the problem of medium finding. If we want to device a recursive algorithm for medium finding well, we cannot do that very easily or very simply. So, what we do instead is we generalize the problem and ask for recursive algorithm for finding the arc smallest. So, some similar issue was applicable here as well.
(Refer Slide Time: 51:59)
 
I would like to make a comment on dynamic programming and the comment is simply the dynamic programming can be thought of as recursion a basic idea. The basic idea the first basic idea, basic idea is recursion and the next idea is make sure that you compute every value only once. And our table essentially made as focus on what values we were computing and we were and we thereby we could only calculate we can make sure that we have calculated every value only once. Finally I would like you to I would like to draw your attention to recursion itself and point out that when we use recursion in search problem in any search problems. It can be thought as a divide and conquer of the search space. So, with that I will conclude this lecture.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 22
Average Case Analysis of Quicksort

Welcome to another lecture on Design and Analysis of Algorithms. Our topic for today is Average Case Analysis of Quick Sort. Let us begin by discussing Average Case Analysis.
(Refer Slide Time: 01:07)
 
Let suppose A be an algorithm. Q is the set of input instances of A. And let us make it, make Q be a function of n. So, Q of n is a set of instances of instances for algorithm A of length n. T sub A of q is the time taken by A on instance q. Given this, you can define the average time taken by A on inputs of size n, which we might write down as M sub A of n. M could be interpreted as mean for example.
And M sub A of n is defined as sum over all instances in the set Q of n. Or sum over all instances of size n of the time taken by those instances, divided by the total number of instances. So, this is the usual definition of what we mean by an average. This is not the most popular definition of course. The definition we usually use is the so called worst case measure. Here the worst case time of an algorithm A on problems of size n, is defined as the maximum over all q of this.
The maximum time taken by any input instance of size n is defined as, the usual measure or the worst case time. There are several reasons for doing this. Worst case is usually easier to compute than this average. When we talk about the average, in some ways we have to talk about all input instances. Whereas, often it is easier to deduce what the worst instance is going to be. And, then we can just worry about that. For many algorithms, most of the inputs behave like the worst input anyway.
So, in which case it does not really matter, it is not really necessary to take the average in any case. Very often may be perhaps, the average case is easy to compute if at all. And it might still not be preferable, it might still not be very popular because, in practice we do not know which input instances are likely to appear more frequently. If some instances appear more frequently, then in this mean expression we would have to wait those instances more heavily.
Therefore, if we just take the mean, then that is not an indication of what might happen in practice. And therefore, again we do not really focus so much on the average case analysis. Worst case on the other hand might be conservative, but at least we know that it is conservative. And therefore, at least we can give some guarantees. Our topic for today is quick sort however, where average case analysis turns out to be quite useful and quite interesting.
(Refer Slide Time: 04:14)
 
So, let me say a few things about quick sort. Quick sort is a popular sorting algorithm, perhaps the most popular and the most commonly used in practice. It is very fast. And as I said, it is often the method of choice. The worse case time of quick sort is O of n square. The average case time on the other hand is O of n log n and will see this quite soon.
So, in some sense the excellent performance in practice might be better explained by the fact, that the average case time is O of n log n rather than by focusing on the worse case time. So, let us now take a look at this algorithm. Quick sort is based on divide and conquer strategy. And the algorithm is something like this.
(Refer Slide Time: 05:08)
 
So, as input we take an array x 1 through n by writing x 1 through n, I simply mean that x is an array whose length is n. This is an array in which we have keys. We can think of these keys for the minute as some integers perhaps. And our idea and the goal of quick sort is to sort them. That is let us say that the smallest keys have to come at the beginning and the largest ones have to go towards the end.
We begin quick sort by looking at the best case first. So, the best case just checks whether, this is an element this array has only one element. If it only has one element, then the array is sorted trivially. And therefore, we just return that array. Otherwise, we pick something which we will call it splitter. And that splitter is chosen to be the first element of this x. The first key is the splitter. Then, we built three lists.
A list which will call small is going to be small, which contains all elements of x which are smaller than the splitter. A list which we will call equal, which will contain all elements of x which are equal to x 1 and so, we begin by putting x 1 into equal. I should perhaps said less over here. But, will not we are not worrying, we are not very careful about this. And we will not be very careful about this throughout the course.
So, we will I just tell you that, we have just made equal just single list, a list with a single element. And we will also construct a list, which we will call large. And large will contain all the elements of x, which are larger than x 1. So, right now it has been initialized to null and small has also been initialized to null. This loop is simply going to build up the lists, as we just described. So, first step if so for every element other than the first.
We check whether it is smaller than splitter in which case, we add that element to small. If it is equal to splitter, then we add it to equal. If it is greater than splitter, we add it to large. So, at the end of this loop all the elements have been put into the proper lists. So, now it is simply a matter of recursion. So, small is a list which contained all small elements. So, we call qsort or quick sort in this list.
So, as a result we will get these elements sorted. Now, these elements are all guaranteed to be smaller than the elements in the list equal. And those in turn, are guaranteed to be smaller than the list in large. But we do not append large immediately over here, we quick sort it. So, as a result we have a long list which is made up by appending three lists. But, which in turn is guaranteed to be sorted.
So, this is how quick sort works. So, as I said it is divide and conquer strategy, the division part is where the interesting work happens. And, then there is a conquer part and then the combined part is trivial. Correctness is quite obvious here. You can do a induction on size, if you want to prove it formally. And I will leave that as a easy exercise. So, now you wanted to analyze this algorithm.
(Refer Slide Time: 08:36)
  
Let me use T of n to denote the time for quick sort, on size and input. So, right now I have only written n over here. But, I will actually have a specific input in mind. Just for the minute. Later on we will worry about average cases or the worst cases or whatever. So, right now let us say this is for a particular instance. So, T of n is the time taken by that particular instance. So, how do we analyze this? Well.
Usually if we write something like this, we try to establish the recurrence. So, of course no matter what input instance we feed. If it has length of only 1, then the time taken is constant. So, that is what is you write down first. Then, we need to find out how the recursion happens. So, let us just go back to the algorithm. So, over here the recursion happens by calling quick sort on small and calling quick sort on large.
And before that, we have a loop which runs about n times. So, the result we have O of n time for the loop. And T of large or T of the cardinality of large is the time taken for that instance, for further for evoking quick sort on the list large. And T of small, is the time taken for invoking quick sort in the list small. Now, we can analyze this using the recursion tree. So, this was our basic recurrence. So, let us draw a recursion tree corresponding to this.
(Refer Slide Time: 10:28)
 
So, we start off with the problem of size n. And, then as per this recurrence we break the problem into two pieces. One is the small list and the other is the large list. So, this is the small part and this is the large part. This is a problem of size n. This is the small part, this is the large part. If this is the small part this is the large part, then we are going to call quick sort recursively on these. So, furthermore this problem will get split, this problem will get split.
Of course, if the problem is going to get split, may be one part one side could be smaller than other side could be larger and so on. And may be one of the lists could be empty in which case of course, this whole thing terminates. So, in general it is going to keep on splitting. May be once in a while, a list terminates and things keep on going in this manner. So, what do we know all about this.
(Refer Slide Time: 11:38)
 
Well, here is the first observation. So, if this node has size n, then we know that in this node the number of keys which are going to be present is definitely going to be less than n, in this node. Or in fact, in this node as well, so what does that mean? So that means that as I go down from here along any branch, the size of the instance has to decrease. So, which means I cannot go down too far.
So, I start the instance of size n. It has to decrease therefore, that means this height has to be utmost n. That is the first observation. The second observation is that, if I look at any node its children have a certain size. But, that size adds up to something strictly smaller than this. So, if I look at the size over here, it is n the size over here has to be less than n. The size over here in fact, has to be smaller than this for these two.
And for these two, it has to be smaller than this. So, this also has to be less than n. This also has to be less than n. So, if I look at any level, the size of that the sum of the sizes of the problem at that level have to be at most n. But now, if we go back to our problem our algorithm at each inside the body of each invocation, we do work or we do work proportional to the size of the problem.
So, which means corresponding to each node over here we are going to do work, which is proportional to its problem size. So, now if I look at the total work done here, it is going to be O of n. Because, it is going to be proportional to this problem size this problem size added up, it is going to be proportional to O of n or it is going to be O of n. Similarly, here also it is going to be O of n. At every level, it is going to be at most n at most proportional to n.
So, now we have an upper bound on the work. Because, there are n levels at most and at each level the work is O of n. And therefore, the total work has to be O of n square. So, this is the upper bound on quick sort. Now, I will leave it as an exercise for you to construct an input instance for which, quick sort actually takes time n square. So, this is actually fairly easy. Let me give you a hint, think of a sorted list.
What if the input instance is already sorted? But, the key question is that this is the worst time. But, will it be the most time, will it take this long usually or is this some kind of an unusual case. So, if you come back to the recursion tree to this tree, then we know that at every level the work is going to be at most n. So, the real question that we want to ask is, will the tree be of a large height or will the tree have a small height because, if the tree has small height then our total work will be less.
So, in fact that is what we will see is, going to happen quite frequently. So, we did the analysis of the worst case. So, let us ask what the best case is going to be? So, clearly the best cases are the one in which tree is as small as possible.
(Refer Slide Time: 15:41)
 
If the elements that we are trying to sort are all distinct, then I will claim that the height cannot be smaller than log n. Why is that? Well, I am going to leave this as an exercise. But, again let me give a hint. So, we said as we go down the tree height must decrease. But, we also said that the sum of the nodes over here, the size over here plus the size over here must be smaller than this. But, if everything is distinct it will only be one less than this. So, if it is one less than this, then you should be able to argue that it would not decrease too fast either.
So in fact, you should be able to argue that it essentially halves at each step. And therefore, the total height will be something like log n. So, what happens in the best case? So, in the best case it turns out. That the total time taken will be O of log n, O of n log n. And in fact, there is a very simple situation in which the best case will happen, which is this? If the splitter is equal to the median, then the problem size halves. And, then the height becomes O of log n.
(Refer Slide Time: 16:57)
 
So, if the height is O of log n that I have taken as n log n, and that is the best. So, we consider two cases one case in which the splitter goes, somewhere in the middle. Another case in which the splitter was extreme, the splitter was the smallest element. Well, that was supposed to be a homework exercise. But, suppose we take splitter, the splitter happens to be the smallest element. Then, the list would be split very unevenly.
So, let us consider a case which is somewhere in between. So, in this the splitter is say larger than n over 10 elements in the list and is also smaller than n over 10 elements. So, it could be somewhere in the middle. So, of course this is an artificial case. But, you can imagine that this will happen frequently enough because, after all if I pick an element from a list, it is likely to be somewhere in the middle.
So, let us say this happens. Let us say that, every time I pick a splitter it satisfies a property like this one. What happens then? Well, let us go back to the recursion tree. So, let us redraw this recursion tree.
(Refer Slide Time: 18:18)
 
So, I start with an n node problem an n key problem. Now, I am going to pick a splitter such that, it is larger than n over 10 elements. So, if I consider, what is the most uneven distribution? What is the size? Well on one side I could get something like a list of n over 10 elements. On this side, I could get a list of say 9 n over 10 elements. So, this is good because, this list is going to shrink and its going to terminate quickly.
The height is going to be small over here. This on the other hand, might appear to be a problem, because here the height has not reduced. That the size has not reduced. If the size has not reduced, then it will keep on going in this manner. And may be the height of the tree may be large. But, what we argued was the work done in this algorithm is, the height of the tree is at most the height of the tree multiplied by n because, n is the work at each level.
(Refer Slide Time: 19:23)
 
So, let us see what happens? So, in the first level as we have pointed out, the largest problem size will be 9 n by 10. It could be smaller than that. So, it could be say half half. But, that is actually not so bad. That means, the third tree height will be actually small. So, this is trying to force the tree height to be large. And therefore, it is trying to force quick sort to take large, to take long time. So, we are sort of looking at we said that we are looking at neither the best nor the worst cases.
But, we are sort of erring on the side of the worst within this region. So, in the first level problem, largest problem level size is 9 n by 10. What happens next? Again, we assume that the problem will split in the ratio 1 is to 9. So, this will become say something like n by 100 and 9 n by 100. This will become something like 90 n by 100 and 81 n by 100. So, as you can see this rightmost branch will keep on having the largest size.
So, what will happen? At each in each step, the size of the largest problem drops down by a factor 9 by 10. And therefore, we can conclude that log of n to the base 10 by 9, the problem size will even on this right most branch will drop down to 1. And even to do that, I will take log of n to the base 10 by 9 levels. So, this is good news in the sense that, even when I am looking at a split which is lopsided.
(Refer Slide Time: 21:25)
 
The number of levels, the height of the tree is still going to be about log. Well, it is going to be log not to the base 2. But, to the base 10 by 9 and let me just remind you that, log n to the base 10 by 9 is simply equal to log of n to the base 2 divided by log of 10 by 9 to the base 2. So, this is still only a constant. And therefore, this is still O of log n. So, the height given in this case is O of log n, the height of the tree. The tree height is of log n. And therefore, the total work is n log n.
(Refer Slide Time: 22:08)
 
So, even in this middle case we have seen that the total work is about n log n. So, that is the sort of the first intuition as to why quick sort should work? Quick sort may be works while in practice. Because, unless the splitter comes from too large or too small, the two sub problems that we create will be reasonably balanced and not too lopsided. And if they are not too lopsided, then the height of the tree height of the recursion tree will not be too large.
Next we are going to actually do sort of a very systematic analysis, of the average time taken by the quick sort. We are going to do this in two ways. In one way, we are going to derive the recurrence. And we will not really solve the recurrence, but I will indicate to you how that recurrence could be solved. And it will turn out that, the solution of the recurrence is n log n. And, then I will indicate somewhat more elegant way using, which we can also derive n log n.
(Refer Slide Time: 23:22)
 
So, when I talk about average case, I have to define what are the possible inputs? So, in this case I am going to assume that, for this particular analysis I am going to assume first of all that all the inputs are distinct. All the inputs, the numbers the elements the keys which are given to us are all distinct. And if they are all distinct, I might as well assume that they are integers 1 through n for each of the n keys.
But of course, they will not be given to me as 1 through n, but they will be given to me as some permutation of 1 through n. So, now I will state exactly what my allowed inputs are. So, my allowed inputs are any possible permutation of the integers 1 through n. So, there are n factorial possible permutations. There are that many input instances for my algorithm. So, my question will be, what is the average time taken by my algorithm over all these input instances or over all these permutations?
And of course, I would like you to express it as a function of n. So, now I am going to express I am going to look at our analysis. And I am going to figure out, how we can estimate this. So, although I have been talking about different, about taking averages I can also think of this in terms of probabilities. So, I can think of this as follows. So, I have been given a set of input instances. I have constructed a set of input instances.
And I am picking one of those instances at random. And I am doing this, giving equal probability to every input instance. So, there are n factorial instances possible. Each one has equal probability or in other words each one has probability whenever we assign. So I am picking one of those. And I could also be asking under this choice, what is the expected time for that for the instance that I pick? Which is of course, the same thing asking what is the time taken, what is the average of all the times?
So, now this average can be estimated by grouping the instances into separate groups. And, then calculating the average within each group and then multiplying by, essentially by the size of the group or by the probability of picking that group. So, here is how you are going to do it. So, in the first step of the algorithm, we pick a splitter. There are n keys and the keys are going to be numbers in the range 1 through n. So, there is going to be some probability that, the splitter is going to be one of these keys.
It is to be even any one of those keys. So in fact, let us assume that we always pick a splitter at the first element which is in fact, what the algorithm did. So, in that case the question is. So, we are splitting all our input instances into those permutations first in which the splitter in which I appears in the first place. And within that group, we are taking the average time. So, let me draw this picture out here.
(Refer Slide Time: 26:59)
 
So, here is our set of input instances. So, I am breaking it into pieces. So, these are input instances which begin with 1. That is, they have 1 in the first place. These are input instances which begin with 2. These are input instances which begin with 3. And somewhere over here are input instances which begin with i. And of course, at the end there are instances which begin with n. So, I am going to pick a group.
And, then I am going to pick an element from it. Or I can ask, what is the average time taken for this group? And if all this groups are identical, then I can just take this average or I will have to wait with the size of this group. So, that is exactly what I have done over here. So, I have taken the average time for this group which is what is written over here? Average time given that splitter is equal to i.
But, given that splitter is equal to i is the same thing as saying, that the first element of the list is i. So, I am in this region of my input space. And since, I want the average over the entire space, I just want to I just multiply by the probability that the splitter is equal to i. Or the fraction which indicates how many instances are there in this group as compared to the entire group. So, this is what I get.
(Refer Slide Time: 28:42)
 
Now, what is the average time given that the splitter is i? Well, if we go back to our algorithm here. So, I pick a splitter over here. Then, I am going to have this loop anyway. So, if I am solving a problem of size n, I will do n work in any case. And, then I will have my inputs split into two lists or three lists. But, only two of which will be interesting. So, average time given splitter i is going to be O of n for that loop to take, loop to do its work plus the average time for sorting the small set.
But, what is the small set? It is the permutation of the elements of integers 1 to i minus 1. And the average time for sorting permutation of elements i plus 1 through n. Because, that is what quick sort does. It splits into groups, it sorts the first group, takes the equal elements in which case in this case there is only one equal element which is i. Sorts the last group and then concatenates them together.
So, in addition to sorting the time will require is O of n. So, you might require O of n time also for concatenation. But, in any case we have written O without actually mentioning the constant. And therefore, this is fine. Or we might have a clever data structure in which case, we do not need this O of n time. But, in any case we need the O of n time for the loop. So, this is perfectly fine.
So, now you have the average time for sorting permutation of 1 through i minus 1 and then the average time for sorting permutation of i plus 1 through n. Here is the important part. So, the first time we picked the splitter to be i and then we constructed this group. But, the key observation has to be, that the numbers the order in which these numbers will appear is not going to be particularly biased. So, we know that within the group that we selected, i is going to appear as the first element.
Since we are dealing with all possible permutations, the other elements would appear equally likely in the first space in this group or in the second space in this group or in the third space in this group. So, this group will have all possible permutations of 1 through i minus 1 as well. So, if it has all possible permutations of 1 through i minus 1, then the time average time for sorting it will be T of i or other T of i minus 1.
It does not really matter, T of i over here. The time over here is going to be i plus 1 through n or it is going to be T of n minus i. So, I think. So, what do we get from this? Well this expression has to be put in over here and as a result we get something like this.
(Refer Slide Time: 32:03)
 
T of n is equal to sum over i of this probability that, the splitter is i. There are n choices for i and since we are considering all possible permutations. Everyone is equally likely to appear in the first place. And therefore, the probability that i appears in the first place is just 1 over n. So, this is 1 over n and this we just established is this. And that is what I have written over here.
I just remarked that this should have been i minus 1. And that is what I have put in over here. Now, this recurrence can actually be solved. It is a little bit tedious algebraically, but you can certainly solve it by recursion induction. Since I am telling you that, the solution is n log n. So, that will establish that the average case of quick sort is n log is O of log n.
(Refer Slide Time: 33:02)
 
Now, we are going to do we are going to consider an alternate method for solving this. So, this is going to be much more direct. We are not going to write recurrences. We are just going to do some interesting counting. So, here we will focus on the comparisons performed by the algorithm. So, after all the important operation in all of this, is comparison. So, if you go back to the loop let us just take a look at that. We did other work as well.
Say we added elements into lists. But, corresponding to every such operation there is a comparison operation going on as well. So, certainly if we bound the number of comparisons, then that will give us a good indication of the time taken by the entire algorithm. So, that is exactly what we are going to do. So, we are going to estimate what is the number of comparisons performed by the algorithm on the average.
And we will show, then that is going to be something like O of n log n. Well, let us first determine what is the maximum number of comparisons possible? So, the maximum number clearly is n into n minus 1 upon 2. This is, if every key is compared with every other key. And of course, if the input is the worst case input, then something like this actually happens. But, this will not this will, but if the input is some permutation, then every key will not get compared with other key.
So, just to see clearly what is going on, I am just going to describe a table which shows what happens for different input instances. So, a table this table will have rows. And there will be a row corresponding to every possible comparison. So, our keys are integers in the range 1 through n. And for every i and j, we will have a row. So, I compare j that will be the label of that row. And in that row, we will have information about whether i and j are compared in every possible input instance.
And in fact, the columns will be the input instances. So, the entries are going to be indexed by two indices, one is i colon j. Well, this itself is a complicated index and the other is this permutation P. So, here for example, is a table. Of course, I have just made up the entries, just to tell you what this table might look like. So, the rows are labeled i colon j. So, starting with 1 column 2, 1 compare 2, 1 compare 3 and so on to n minus 1 compare n.
So, during the execution whether it is or not 1 is compared to 2, when permutation 1 is input is going to be written out here. So, you have left a blank over here. And that just says that node, that node will not be compare. It is just an example. On the other hand, 1 and 3 will be compared, when permutation 1 is the input. Similarly, if permutation 2 is the input then 1 and 2 will get compared, 1 and 3 will get compared and may be some other things will also get compared.
Similarly, there will be other permutations for which this will be the pattern of comparison, this will be and so on. So, there are n factorial possible input permutation. So, we have n factorial possible columns. And for each possible comparison, we have a row. And their intersection says that, whether that comparison actually happens in the corresponding execution.
(Refer Slide Time: 37:00)
 
The key question is, Are there many T cells in this or are most of the cells blank? What we really want to know is, what fraction of the cells in the column are marked? Or what is the average number of cells which are marked in a given column? We are not going to answer this question directly. We will begin by asking, what is the fraction of cells which are marked in any row?
And interestingly, that will tell us something about what happens in columns as well. Say, if I go to a particular row of this table or the row which has labeled i colon j, the question that I am asking is, Is i going to be compared with j in the first permutation or in the first input instance or in the second input instance in the third input instance and so on.
(Refer Slide Time: 38:12)
 
So, here is the key observation for i to be compared with j either i or j must be chosen as a splitter before, one of the elements between that is elements i plus j or j minus 1 gets splitter. So, let me explain this a little bit.
(Refer Slide Time: 38:33)
 
So, here is i here is j and there are some elements in between. Well, I know i plus 1 i plus 2 all the way till j minus 1. So, these are the elements that I am considering. Of course, they will not appear in my input instance in this order. They will be in my input instance, they will be scrambled up. But, I am just thinking of them as sitting in a line. Now, suppose some element over here gets picked up as a splitter, what happens?
If this element is picked as a splitter, then this element is compared with everything else. If everything else is compared with it, then this I will get input in the small list. So, i will go into the small list. j on the other hand will get input in the large list. But, remember that once an element goes into this list and another element goes into another list, there is no question of comparing them subsequently.
So, if any of the elements in between over here get picked as splitters, before any of these two elements get picked. Then, we know for sure that these elements will go into separate lists. And therefore, they will not be compared. On the other hand, before these elements have been picked suppose i gets chosen, what happens then? Well, then i is going to be compared with everything larger than it, or certainly everything which has not been found which is in the current list.
But, if nothing in this has been selected as a splitter, then this had been better been in the current list. And therefore, j will get compared with i and vice versa. If j gets picked first, then i will get compared because j will be compared with everything over here. So, which means that, these two elements must get split as splitters before these, inner elements are picked. So, what is the probability of that happening?
(Refer Slide Time: 40:59)
 
So, I claim that probability of i or j being chosen before i plus 1 or before elements i plus 1 through j minus 1 is in fact, 2 minus 2 upon j minus i plus 1.
(Refer Slide Time: 41:22)
 
So, here is i here is j. So, how many elements are these in total? These are j minus i plus 1 elements. And out of these, the comparison happens only if this is picked or this is picked. So, there are two cases which are good out of j minus i plus 1 cases. And therefore, that is the probability. So, now actually things are very, very simple.
(Refer Slide Time: 41:59)
 
So, the fact that i or j is probability that i or j is chosen, before i plus 1 through j minus 1 is this just tells us something very simple. It tells us that the fraction of T s in this row is just this. Because, that is what the probability is. We are going to pick a row at random. And we know that, 2 upon j minus i plus 1 fraction of the time we get a T or the comparison happens.
So that means, in other words the number of columns the fraction of the number of columns in which T s appear, is just going to be this much. So, what does that tell us? So, it tells us that the total number of T s in the entire table is going to be sum over all the rows of this multiplied by n factorial. Let me explain that a bit slowly. So, from this what can I conclude?
(Refer Slide Time: 43:03)
 
I can conclude that, in row i colon j contains n factorial times 2 upon j minus i plus 1 T s, what T s represent? Where comparisons happen, whether comparisons happen or not. But, if I want over the entire table I just have to sum over all possible rows. So, this is what I have written out here.
(Refer Slide Time: 43:33)
 
Except that the n factorial has taken outside because, it does not depend on what row I am looking at. Well, this expression can be written out slightly differently. So, all possible labels i j, I can now classify as all possible levels in which j is a second element. And, then the first element has to be smaller. And therefore, it is sum over i is less than j of this expression.
But, what is that, so summation over i of i less than j of this expression, well. What is the first term? So, i begin from 1 and so, first term is simply 2 upon j. And next term is 2 upon j minus 1 and so up on until 2. But, what is this? So, this is let me write it down again.
(Refer Slide Time: 44:37)
 
It is 2 upon j plus 2 upon j minus 1 plus all the way till 2 upon 2 or written differently, it is 2 times 1 plus half plus one third all the way upon till 1 upon j. And this we know simply l n n by treating this sum to an integral, converting it to an integral. So, this is a good estimate or in fact, this is an upper bound.
(Refer Slide Time: 45:13)
 
So, finally we have this whole thing as n factorial times sum over j of O of l n j. But, if you are going to take the sum over j, what do we get? Well, we get n l n and n. So, we get n l and n over here, what is n l and n? So, we have total number of T s in the entire table list n factorial times n l n n. So, what then is the number of T s per column or what is the average number of T s per column?
Well, how many columns are there are there? There are n factorial columns. And therefore, we divide this total number by n factorial and then we get O of n l n n. So, average running time is O of n l n n and why is that? Because T s represent the number of comparisons, and we said that the average that the time is in fact proportional to the number of comparisons. So, the average running time is going to be O of n l n n.
But, O of n l n n is simply O of n log n as well. So, here the base was the natural base was e or this was the natural logarithm. Here the base is 2, but that does not matter log of n and l n of n are within a constant factor of each other. So, let me conclude. So, I would just like to say that, a similar idea works for selection as well. So, suppose we want to select the rth smallest element, then something like this will also be fine. 
Thank you.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 23
Bipartite Maximum Matching

Bipartite Graph Matching in the course on Design and Analysis of Algorithms. Let me begin with a motivating example.
(Refer Slide Time: 01:01)
 
So, in this example we have some j jobs and some C candidates. The idea is that the each candidate we have given we have been given the list of jobs that they candidate can do. And we also have been given a constraint. The constraint says that, each candidate must be given at most one job. And also each job must be assigned to at most one candidate. The goal is our obvious goal.
Assign candidates to jobs such that, maximum number of jobs are filled. The bipartite maximum matching problem is exactly this, in more mathematical terms. So, I am going to first define this problem. And then, I will relate it to this job and candidate problem.
(Refer Slide Time: 01:59)
 
In this problem, our input is a bipartite graph. So, let us call it G. G is composed of two vertex sets U and V. And the cardinality of U says n 1. And the cardinality of V is n 2 and E is the set of edges. Let me remind you, what a bipartite graph is. A bipartite graph is simply the graph, in which the vertex set is in two parts U and V. And the edges only go between U and V.
(Refer Slide Time: 02:31)
 
Here for example, is a bipartite graph. So, this forms this set of vertices forms U. This set of vertices forms V. And as you can see the edges, only go from some vertex in V to some vertex in U to some vertex in V. Specifically, there are no edges which connect our text in U to another vertex in U or our text is V to another vertex in V.
(Refer Slide Time: 03:00)
 
So, let me now define what a matching is. A matching is a subset of the edges such that, at most one edge is incident on any vertex either in U or in V.
(Refer Slide Time: 03:15)
 
So, in this graph for example, the set of blue edges would be a matching. If I add one more blue edge as here, this would not be a matching, because we would have a conflict at this vertex 7. So, here we would have been having two edges of the matching, of the so called matching being incident and that is not allowed.
(Refer Slide Time: 03:38)
 
So, now let me tell you, what the goal in this problem is or what we require to output. So, we are required to output a matching of maximum possible size. A size of the matching is defined as the number of edges in it.
(Refer Slide Time: 03:54)
 
In this graph for example, we have this blue matching which consists of three edges. So, its size is 3. This however, is not the maximum sized matching.
(Refer Slide Time: 04:06)
 
This red matching for example is the maximum size matching. And in fact, it contains 1 2 and 3 and 4 edges. Maximum matchings are not unique, in this graph itself. For example, this is another maximum matching.
(Refer Slide Time: 04:25)
 
Let me now relate, this matching problem to that of our candidates and jobs problem. So, this first set of vertices which we call U, can be thought of as the candidates. So, think of 1 2 3 4 5 as being the candidates. And the jobs are represented by this second set, which we call V. We draw an edge from a vertex over here to a vertex over here, if the corresponding candidate over here can do this job. So, these two edges for example, represent the fact that candidate 1 can do job 6 as well as job 7. So, in fact the list ally that I have mentioned will contain 6 and 7 in the list of candidate 1.
(Refer Slide Time: 05:21)
 
A matching now is simply, an assignment of jobs to candidates. So, for example here is a matching which says. Let us assign to candidate 1 job 6 to candidate 3 job 7 to candidate 5 job 9. Of course, we would like to maximize the number of jobs assigned. So in fact, instead of this we should really be taking one of the red matchings that, we saw earlier.
(Refer Slide Time: 05:51)
 
Here is the outline of my lecture. So, I am going to begin with an algorithm design idea. And we will see few more ideas and refine them. Then, we will come to the notion of augmenting paths. This turns out to be a very important concept, in this whole matching area. That will understanding augmenting paths, will lead us to reasonably clean simple high level algorithm. It is correctness depends upon something called Berge?s theorem. So, we will state and prove that theorem next. And then, finally we talk about how to efficiently implement this algorithm. And we will estimate its time.
(Refer Slide Time: 06:35)
 
So, let us start with a really simple winded idea. How would you find the largest size matching? The moment you talk about larger size perhaps, the most natural thing to consider is the greedy idea. So, here is the strategy. So, we look at all the edges and maintain a set called M. And we keep on adding edges into M, till no more edges can be added. Of course, as we add edges into M you have to make sure that there are no conflicts or there are no two edges which ever are incident on the same vertex, which are placed in our set M.
(Refer Slide Time: 07:19)
 
In this case for example, if we tried this idea what would happen? Well, we would start with this edge and add it no problem. Then maybe we will try to add an edge going out of 2, but this edge cannot be added because it would produce a conflict at 6. So, we take the next edge. So, we add that one. Then, we try that this edge over here this edge also cannot be added because, it will produce a conflict over here. Then, we try adding this edge.
That also cannot be added, because it would produce conflict over here. And then, we try to add an edge out of 5. And say for example, we end up adding this edge. After this, you can see that no more additions are possible. Have you reached the best possible matching? No, because we know that there exists a matching. In fact, any one of those red matchings which we saw earlier which have four edges in them rather than just the three edges, which we have found over here.
(Refer Slide Time: 08:24)
 
So, we clearly need something better than the greedy idea. So, here is what I call the kho kho idea. So, if you have played this game of kho kho, you might remember that in that game there is a chaser who keeps running. But, who occasionally goes and knocks on the back of a person, who is sitting down. And then, that the person who was originally sitting down starts running and starts chasing the members of the opposing team.
Until he or she again goes behind and knocks in the back of another person, who is sitting and who takes over. So, this is the idea that we are going to explore. So, I need a definition for that. The definition is that of a free vertex. So, given a matching M I will say that the vertex is free, if it is not an end point of any edge in that matching. So, we will take an example on this shortly. But, let me state that idea first. So, the idea is something like this.
So, suppose we have a free edge then, that indicates that maybe we can augment or we can increase the size of our matching. May be by throwing an edge by considering an edge out of that vertex, and may be trying to include that edge include that matching that we have found so far. This may succeed if it succeeds great then, we have a bigger matching. If it does not succeed, well how will it not succeed? It will not succeed because, may be it conflicts with another edge which is already present.
So, this is where the kho kho idea comes. So, this new edge that is present. The old edge that is present will get knocked up. And the new edge will sort of set in our matching. But, now once we knock out an old edge, what will happen? Well, the other end point of that will become free. And now we will try to match that. And we will try to do on this, until we find that there is no conflict. And if we succeed then, maybe we have increased the size of our matching a little bit. So, let us try it out.
(Refer Slide Time: 10:33)
 
So, the first thing to check is which are the free vertices? So, in this case vertex 2 vertex 4 and vertex 8 are free. Because, they do not contain incident on the any edge in the matching, edges in the matching are shown in blue over here. So, let us start with vertex 4. And let us try to see, what happens if we try to match it somewhere. Well, 4 can only be matched to 7. So, I have indicated that this is our candidate for inclusion into our old matching.
If we try to include this, what will happen? Well, that will have there will be two vertices two edges incident at 7. So, essentially we will have a conflict. Conflict just means that, there are two edges incident in a matching. We just want one edge to be incident or at most one edge to be incident. Well, if there is a conflict what do we do? Well, we are going to remove this edge.
(Refer Slide Time: 11:35)
 
So, let us do that. So, I am going to use the color code green, the color green to show edges, which were in the matching before we started this entire procedure, but which we have just removed. Remember that yellow edges are the ones which we just added. So, they were not in the matching before, we started this procedure. But, they have just come into the matching. Green edges are the edges, which were in the matching earlier. But, now they have gone out of the matching.
If this edge from 3 to 7 goes out, what happens? Well, vertex 3 becomes free. Vertex 3 does not have an edge in the matching attached to it in longer. Remember this edge is just gone out of the matching. So, now we try to match 3. Well, we could try and match it back again to 7, but that seems foolish. We just removed this edge. So, we should not try to put it back again. So, let us try to do something else. So, the only else the only other thing that we can do is add an edge 3 to 9.
(Refer Slide Time: 12:38)
 
So, let us try doing that. So, now this yellow edge this yellow color says that, this edge has entered our matching and it was not there originally. Well, what does this do? It produces a conflict of time. How do we eliminate that conflict? Well, we remove this edge. So, this edge goes out of the matching. And therefore, we color it green. Now, it happens. Now, vertex 5 is free. So, we try to match it.
(Refer Slide Time: 13:15)
 
When we do match it, we match it to 8 and interestingly there is no conflict. So, this is the basic step that I mentioned. This is the kho kho sort that, I mentioned. Let us see, what it is accomplished.
(Refer Slide Time: 13:28)
 
So, we ended adding three edges. Which are the three edges we added? Well, these three this yellow edge, this yellow edge and this yellow edge. So, these three edges we added. So, now you know why I color those edges very, very carefully. So, that I can keep track of what exactly happen? What happened to the edges earlier in the matching? Well, some of them stayed. This edge was there in the matching earlier it state.
But, this edge which was there in the matching earlier went away. This edge which was there earlier in the matching, that also went away. But, two edges went away and three edges came in. So, now we have a better matching which consists of this edge, this edge, this edge and this edge. So, earlier we had three edges in the matching. Now, we have four edges in the matching. Now, the colors are going to tell us something more interesting, about what exactly happened.
And in fact, it makes sense to ask exactly how we traverse this graph in this entire process. So, as we executed this process we really traversed a path in the graph. So, we started at vertex 7. Then, we added edge 4 7. Then, we removed edge 7 3. Then, we added edge 3 9. Then, we added then, we removed edge 9 5 and added edge 5 8. So, that is exactly what happened? This, this, this, this, and this. In fact, there is more interesting pattern out here. So, let me show what that pattern is.
(Refer Slide Time: 15:14)
 
Let me take this vertex 3 and let me drag it over here. So, now you should be able to see, what that pattern is even more clearly. So, if I look at the first edge on this graph, it is yellow. The second edge on this path is green, the next edge on this path is yellow again its green again it is yellow. So, in fact the colors of the edges alternate along this path. And that is of course, to be expected why, because we alternately added and removed edges.
In fact, there is one more interesting thing. So, whenever we were adding edges we were going forward in the graph and we were following edges which were originally not in the matching. When we removed edges, we were following edges which were originally in the matching. But, we were going backwards in the graph. And similarly, again we go forward backward and finally forward again.
So, you see that what we did, the path we traced at this entire procedure has lots of interest in properties. And in fact, this path that we traced is so important that it is given a name, it is called an augmenting path.
(Refer Slide Time: 16:28)
 
So, given a matching there is a notion of an augmenting path. An augmenting path is a sequence of vertices v 1 v 2 v k. Such that the first vertex is in the set u in the graph, the left the set of vertices on the left corresponding to the vertex 4 that we started off with and it is a free vertex. That is exactly, how we started out the construction. The last vertex is also free because, that is what enabled us to discover an extra edge.
And then, the intermediate vertices well the vertex from v 1 to v 2 was originally in the graph, but not in the matching. And that is exactly, what this set E minus M supposed to denote. E is the set of edges from that, I remove the set of edges which were in the matching earlier. So, this edge v 1 v 2 is now was supposed to be in this set E minus M. The next edge on the path however, v 2 v 3 is a backward edge and it belongs to the matching.
And this edge is the forward edge. So, we go forward then, we go backward then we go forward again potentially. Again we follow an edge, which is not in the matching and we go forward. And maybe we go backward again. And we do this several times, until we end with this set v that is what a augmenting path is. The operation of taking an augmenting path p and a matching m and generating a new bigger matching which we just did, we will abbreviate as M symmetric difference P. Why is this called symmetric difference?
Well, here is the definition of this circle plus operator. We will define Q circle plus R or Q symmetric difference R, as the set of elements in Q or in R, but not in both. So, in that sense it is the symmetric difference. So, it is the sense in which Q and R are different from each other, not their similarity, but there is difference. So, in fact you can see that the new matching is the symmetric matching is the symmetric difference of the old matching and the path. So, let us take a look at that quickly.
(Refer Slide Time: 19:04)
 
So, the old matching was this matching 1 6 3 7 and 5 9. So, the old matching is the blue edges and the green edges. The path is this. So, the path is the yellow edges and the green edges. So, as you can see the green edges form the intersection between the path and the old matching. And therefore, they have been removed. And what has included in the new matching is just the difference, the symmetric difference between the path and the old matching. So, these are edges which were in the path, but not in the matching. This is also an edge this is an edge which is in the matching, but not in the path. And so, the new matching will just contain this and these.
(Refer Slide Time: 20:01)
 
So, augmenting paths play a big role seem to play a big role when we want to increase the size of our matchings. In fact, Edmonds who was a eminent computer scientist from, Ii guess from the previous century who is one of the who could even be called as one of the founders in some sense of analysis of algorithms. So, I am going to describe an algorithm which is credited to Edmond?s. So, the algorithm is essentially based on this idea of augmenting paths. So, we start off with M which is an empty matching which is an empty set.
So, there is nothing in it. And then, we keep on finding the augmenting path. So, we check if there is an augmenting path P for this M, defined in this sense. If there is then, we perform this operation and set M to the result. We keep on doing this, until we discover that we cannot augment our current matching. The moment that happens, we stop and we output M. So, this is the algorithm. Does it seem reasonable? Well, yes it seems reasonable.
We seem to be adding edges if possible, but there is a old problem which we had with our greedy idea as well. What if we discover that there is no augmenting path. Can we stop then really or it is possible that there is some bigger matching which we will find using this idea. So, this possibility can be ruled out and this is done by a theorem attributed to Berge.
(Refer Slide Time: 21:55)
 
So here is what Berge?s theorem says. Berge?s theorem says that a matching M in a bipartite graph is maximum, if and only if there does not exist a augmenting path for M. We will prove this in a minute, but I just want to persuade you that, this is exactly the theorem that we wanted. So, this theorem says that if ever we come to a point at which we cannot find an augmenting path. Then, we must have in our hands a maximum sized matching.
So, this theorem justifies or proves the correctness of Edmond?s algorithm. So, let us now prove this theorem. The proof is in two parts. Since, this is a if and only if theorem. The first part is the only if. So, what are we required to prove over here? If a matching in a bipartite graph is maximum then, there cannot exist an augmenting path. This should be quite obvious. If there existed an augmenting path, what would happen?
Well, we could add we could augment that path with our matching. We could compute M circle plus P and we would get a bigger matching. Is if that possible? No, because we said that M is already maximum. And therefore, the only if case is obvious. The interesting case is the, if case. We are going to prove this case by contradiction. So, how does this work? Well. So, let us assume the contrary.
So, let us suppose that M has no augmenting path. So, M has no augmenting path that of course, by itself does not make up the contradiction. So, we are going to assume that M has no augmenting path. And there exists a matching M such that, N is larger than M. So, this is the case that we really wanted to watch out with. We cannot find an augmenting path, neither have we got the best matching. So, this is the if case. And this is the case that we really want to rule out.
So, we are worried about such M. We have in our hands matching M. And we know that there is, we are assuming that there is a bigger matching N somewhere out there. Well, at this point it is natural to ask, what is the difference between M and N? So, that is exactly what we do. So, we will ask will say, let R be the symmetric difference of M and N. And then, let us investigate what this, what the properties of the symmetric difference are.
I claim that, so we are going to look at this symmetric difference R. My first claim is that this R must be composed of paths and cycles. Why is that? Well, before saying why that is, let me just say that the claim is equivalent to stating that the degree of every vertex in R is at most 2. Remember M and N are sets of edges. R is also a set of edges. And it is over the same vertex set u and v. So, I can talk about the degree.
So, I claim that if R is made up of paths and cycles, it is the same thing that saying the degree of every vertex in R is at most 2 obviously. The degree of any vertex in a path is 2, the degree of any vertex in a cycle is also 2. So, if I can prove this I would have proved this. Well, why is this true? Well, just examine how we constructed R? We constructed R by taking some edges of M and some edges of N. But, note that the degree of every vertex in M and N is at most 1.
The edges of M and N are such that, on every vertex at most 1 edge from either M or N is an incident. So, even if I take the union forget the symmetric difference, even if I take the union the degree of every vertex in M and N will be at most. Since the degree of M and N in every vertex is at most 1, the degree of the union will be at most 2. So, from this it follow that this is true. Therefore, this is same as this. Here is an interesting fact about R.
So, it consists of paths and cycles, but the edges in R alternate between M and N. Why is that? Well, could there be two consecutive edges in R coming from M. No, because M has been defined to consist of edges which are incident on vertices. But, at most one edge is incident on any vertex. So, in M itself two edges are never incident on the same vertex and therefore, when those edges going to R this property will stay around and similarly for N. And therefore, we know that the paths and cycles inside R consist of alternate edges of M and N.
Some more properties, what is R? Well R is the symmetric difference. So, that is as good as saying that, we take the union and then we remove the intersection. So, the size of R is size of N plus the size of M minus the size of the intersection. But note that, N has bigger size than M. So, we subtract something, but in the end we subtract something we just common to both M and N. So, in the end R must have more edges in it from M rather than from N. So, we just how to examine the implications of this.
(Refer Slide Time: 28:28)
  
Let us look at cycles first. Each cycle consists of an equal number of edges from both M and N. So, if we just look at the cycles this fact cannot be explained. So, what first happen? There has to exist a path, which must contain more edges from N. If it contains more edges from N, what do we know about such a path? Well, let us take a picture.
(Refer Slide Time: 29:03)
 
Let us draw a picture over here. So, here is this path. It contains more edges from N. So, let us say N edges are in black. So, if it contains more edges from N clearly, the first edge has to be from N then, from M then, from N then from M. And finally, the last edge must be from N. So, now what do we know about these vertices? So, will these vertices have an additional edge either from M or N going out.
Well, if there was a red edge going out then, this could be a path which have been continued. We could have considered that itself, but this is the maximal path that we are considering. So, there cannot be a red edge going out of here. There cannot be a black edge going out over here, similarly over here. So, that means that the first and the last end points of P must also be free in M and what is this.
This is simply the definition of an augmenting path. So, this path p is an augmenting path for this matching M. So, we have proved that an augmenting path exists and in other words, this you can improve your path. And we started off by saying that, M has no augmenting path. So, we have got to a contradiction. So, this proves Berge?s theorem. So, we have proved Berge?s theorem.
(Refer Slide Time: 30:35)
   
So, we know that augmenting paths are useful and not only useful, but they are sufficient. So, the only question that, remains is can we find augmenting paths quickly? So, what are the properties that we know about augmenting paths? Well, an augmenting path starts and ends at a free vertex. It starts at a vertex, say U and ends in a vertex V. So, without loss of generality or vice versa, so without loss of generality we can assume that the starting point is U.
So, we are given a graph and we want to start an augmenting path. We want to check whether there exists a augmenting path, starting at some vertex in U. If we knew where the path started, we could just try growing the path out of that vertex. So, we could try something like depth first search or something like that. And try going out from that vertex into the rest of the graph. But, we do not know where it starts. So, here is an interesting idea.
So, since we do not know where it starts, we are going to start growing from all the free vertices in U. Now, we know something more about augmenting paths, which is that the paths have to grow forward using only edges in E minus M. E difference the edges which are in E, but not in the matching E minus M. And the path must grow backwards using edges, which are in the matching. You may do this several times.
It may go forward and go backward and then, forward again several times, but every time it goes forward. It must use an edge which is not in the matching. And every time it goes backwards, it must use an edge which is in the matching. Finally, if we reach a free node or a free vertex in the set V, which is the set on the right side through any paths, whatever we are done.
So, that is how we are going to grow these parts. And if we reach a free vertex, great we are done. We would have discovered a path. Now, it turns out that you can package this set of ideas very nicely as a breadth first search on a new graph. Well, on a slightly a graph which has been derived from this M and G. This graph is going to be very similar to G, but it is going to be crucially and slightly different. So, let us take a look at this. But let us first take an example first of, how we can do this?
(Refer Slide Time: 33:28)
 
So, here is our old graph and here is our old matching. So, let me try to see how this idea will work out, on this graph. So, the first point was to look for a free vertex, actually a free vertex on the U side. We say here two vertices are free, vertex.2 and vertex 4. These are the free vertices. So, these are the vertices from which we can start the paths. So, we could say for example, that let us grow paths from 2 and 4 going in the forward direction and backward direction as we just described.
(Refer Slide Time: 34:08)
 
Instead of that, just to make our description nice and compact. We are going to throw in a new vertex, we will call S. We will also throw in two edges, going out of S to both the free vertices or to all the free vertices, whatever free vertices there are. And in fact, we are going to direct these edges. So, earlier graph was an undirected graph. this new graph is going to be a directed graph.
We can do breadth first search or any kind of search on an undirected graph, just as well as on a directed graph. So, here we are going to do it in a directed graph. So, now instead of saying that we go upon paths 2 and 4, notice that we can just say grow paths out of S, just a single vertex S. So, we want to grow out of S. And once we do and if we do that, we will naturally hit 2 and 4 which is where you want to go anyway.
What do we next? Well, we want to grow the path itself. And for growing the path, we need to use edges which do not belong to the matching. And furthermore, we know that these edges will be used only in the forward direction.
(Refer Slide Time: 35:24)
 
So, a natural way to enforce this constraint is to say, this is the constraint that we want to enforce. So, the natural way of doing that is to direct these edges in the forward direction. So, every time we grow the paths. We know that the edges which are not in the matching will be used, only in the forward direction. So, we put a direction on them. So, that forces the search to use them, only in the forward direction.
(Refer Slide Time: 35:53)
 
Now, what happens when the path the augmented path goes backwards? Well, it uses edges, which are in the matching. If it uses edges which are in the matching, it only uses them going backwards. So, we put backward arrows on this. So, the idea now is, we start over here we keep we go forward we are allowed to go forward. Then, we are allowed to go forward using edges, is not in the matching.
We can come back, we can go forward we can come back, every time we come back we need to use edges in the matching. Every time we go forward. We are supposed to use edges, which are not in the matching. And finally, we would like to end up with a free vertex in this set V. Again there could be many free vertices over here. And instead of saying, let us end up saying free vertex in this set V. Here is what we will do?
(Refer Slide Time: 36:44)
 
Instead of saying that, the path must end at any free vertex we will put a vertex t out here. And all the free vertices we will connect it to t. In this case, there is one free vertex. So, this is what we will connect to t. So, now what is the problem that we want to solve? While we want to ask the question, does there exist a path from s to t in this directed graph. Notice that by putting directions, we have essentially enforced all the constraints that we wanted on that augmenting path.
(Refer Slide Time: 37:18)
 
So, let us formalize this. Let us state this algebraically. So, we will define this directed graph and we will call it the auxiliary graph, for G and M. So, what does this graph look like? First of all, we will get symbol G prime. It is going to consist of vertex set V prime, which I will define in a minute and edge set. So, these edges are actually directed. So, this edge set is E prime. So, what is the vertex set?
V prime is the union of set U in the original graph, the set V in the original graph and these two vertices, which we added the s vertex and the t vertex. Let us define the edge set E prime consists of these edges, which goes out of s. So, just to remind you. So, these are the edges which are E s edges. E f is the edges, which are the forward edges. So, let us go over them step by step. E f are the set of edges in the forward direction that is, what this f is supposed to end up.
So, they consist of the arcs of the form u v, where u belongs to the left hand set of vertices, v belongs to little v belongs to the right hand side vertices of capital V and u v is not in the matching, but it is in the edge set. E sub b are the backward edges, the edges which we directed backwards. And these are simply all the edges in the matching. So, these are all the edges which were not in the matching which we directed forward.
These are all the edges which in the matching, which we directed backwards. So, notice that this is u to v whereas, this is v to u. And finally, we are going to put edges out of every vertex, every free vertex in V to the vertex t. So, this defines our auxiliary graph. So, now comes our main claim. Our main claim says that, G has an augmenting path for this matching M if and only if, G prime has a directed path from s to t. We will prove this, we just point out why this claim is important.
Notice that, we did not have any specific way of finding augmenting paths. However, given a graph G prime does it have a directed path from s to t. That we know how to find it completely? That is just simple, breadth first search, depth first search or whatever you like. So, this claim would enable us to find augmented paths very quickly. And that is why, it is a very significant claim. So, let us prove it. The proof really goes along.
Pretty much along the lines of the construction, as I explained it in a minute ago, but I will try to explain it formally right now, without reference to a specific graph. So, let us look at the only if part first. So, the only if part says that suppose G has an augmenting path then, G prime must have a directed path from s to t. So, the only part says that let A be an augmenting path for G M then, we will show we must show that s v 1 v k the same path, but in the new in the graph G prime, must be a directed s t path.
So, I hope there is no confusion. Because, we are using the same set of vertices, but we have carefully defined everything in this definition. And also in the previous graph in the previous picture, we actually use the same set of vertices. And we just transformed our original G to G prime. By context you should know, when I refer to over vertex whether I am referring to it inside G prime or inside G. So, we had given this augmenting path v 1 to v k. And we want to show that, this must be a directed path in G prime.
(Refer Slide Time: 42:04)
 
What do we know about augmenting paths? It starts at a free vertex in U, goes forward and backward several times and terminates at a free vertex in V. So, v 1 is a free vertex and the path goes forward from v 1. When it goes forward, we must have an analogous edge in G prime. So, all that you need to do is that, analogous edge is present in G prime. Note that in the forward direction, the path users edges in E minus M. This is inside G.
An augmenting path in the forward direction, uses edges which are not in the matching. The path is moving forward, but it is using edges which are not in the matching. Now, these edges are in fact present in G prime. These are exactly the edges. They are present in G prime and in fact, they are directed in the forward direction. So, if we look at the forward going edges in this, they are all present in G prime and they are all oriented properly.
They are oriented as per the movement of the direction, movement of the path direction. What does the path do? Well, the path can go backward. When it goes backward, it uses the edges of M. But, these edges are also present in G prime and they are directed backwards. So, again this is exactly what we wanted. Well, there is no coincidence over here, because we arranged it to be this. So, in some sense if you followed that example then, this should not be surprise to you at all.
So, again going back to this, v u is an edge going from v to u and that is how we have oriented it. If for all such edges which belong to the matching. So, what have we proved then? We have proved that, this portion which is an augmenting path which is also present in G prime and it has proper orientation. So, all that we need to argue is the s to v 1 connection and v k to t connection. So, what do we know about s to v 1.
So, s has a directed edge to every free vertex in U. And of course, v 1 must be a free vertex. So, this since it is an augmenting path. So, s to v 1 must be an edge in G prime. Similarly, v k to t also is an edge, because every free vertex has an edge s to t. So, that is also present in G prime and thus we have a path in G prime also. So, this entire thing is a path in G prime exactly as we wanted.
(Refer Slide Time: 45:14)
 
The if part, the if part says that if G prime has a directed path from s to t then, G must have an augmenting path. So in fact, you will see that exactly the reverse of this reasoning will accomplish the, if part also. So, let us summarize what we have done. So, we have defined this auxiliary graph. And using the auxiliary graph, all we have to do is just find the path in it. And we get an augmenting path. So, we find a path from s to t and we get an augmenting path.
(Refer Slide Time: 46:03)
 
So, now that brings us back to our algorithm. So, that brings us back to the algorithm. So, we just have to build up on this step of finding an augmenting path, but we know how to do that. So, here is our augmenting path procedure. So, we construct G prime just as we defined a minute ago then, we find p the path from s to t. We can use breadth first search for it or we can use depth first search, it does not matter.
But, somehow we do it. And then, if p is not null then, we delete that as t and return the augmenting path. This return path will be used over here to augment the matching. So, let us now analyze this. So, let us say n denotes the cardinality of U plus the cardinality of V or the total number of vertices. Let m denote the cardinality of the edge set of the original graph. And let us assume that, the graphs are represented in the adjacency list representation.
In fact, we will keep m also in some adjacency list representation. Just for the purpose of simplicity of thinking about this whole thing. So, what do we know now? Well, we have to analyze what the time for constructing G prime? And G prime is and then, doing the breadth first search and so on. So, here we construct this graph G prime. So, how did we do that? We took G and we took the matching and we took its union, we oriented the edges.
So, in any case that can be done in time proportional to the sizes of two graphs, which is O of m plus n. The next step over here is to find the path from s to t using BFS. BFS Breadth First Search takes time again O of m plus n. So, this step also takes time O of m plus 1. So, both of these steps take time m plus n. Time for this entire procedure, I claim this is going to be O of m plus n, this entire procedure augmenting path.
That is, because this part deleting s and t can be done in constant time. And therefore, this entire thing is just the sum of this plus this, which is O of m plus n. So, this takes O of m plus n time. So, the only question that remains is, how many times do we augment? Well and how do we do this augmentation itself? In fact, in m plus n time you can compute M augmented with P also. Because, that is just going over the graph completely once even that, will take time O of m plus n. How many augmentations do we do?
(Refer Slide Time: 48:54)
 
Well, what do you know about matching size? The matching size is at most n by 2, since the number of vertices is n. And so, the number of augmentations is going to be at most n by 2. So, now we know what the total time is, the total time is n by 2 multiplied by this or in other words, it is O f n times m plus n m is typically larger than n and so, we can write it as O of m n. So, that completes the analysis of the algorithm, the description and the analysis of this algorithm.
(Refer Slide Time: 49:29)
 
So, let me make a few concluding remarks. We can actually think of this algorithm, as an iterative refinement. What does that mean? Well, we have a matching currently and then, can we improve it by making the small change. So, an augmenting path essentially allowed us to determine if a small change can be made. This is not the fastest algorithm. In fact, an m root n algorithm is known not just m n. So, m root n algorithm is known.
(Refer Slide Time: 50:01)
 
And in fact, we can define this problem for non bipartite graphs. So, that also turns out to be useful very often. And as it turns out that, similar bounds can be found for the non bipartite case. So, maximum matching can also be found in non bipartite graphs.
(Refer Slide Time: 50:26)
 
In the same time as above, but the algorithm is much, much more complicated. And I will stop here.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 24
Lower Bounds for Sorting

Welcome to another lecture on Design and Analysis of Algorithms. The topic for today is Lower Bounds for Sorting. Let me begin with a very fundamental, very basic question.
(Refer Slide Time: 01:03)
 
Suppose, we just designed a algorithm, that takes time f of n to solve some problem, where n is the size of the problem. What would we like to know then. Naturally the question, that we would like to ask is, is this the best possible algorithm or can we do better than this. Here is the possible result, we will desire. We would like it, if we can prove that every algorithm for solving the problem, must take time at least omega of f of n.
Remember, f of n is the time taken by the algorithm that we just designed. Supposed, we proved a result like this, what could it mean? Would it be a valuable result, well first of all this result if we can prove, it is called a problem lower bound. It is called the problem lower bound. Because, it says something about the problem, it is not really saying anything about a specific algorithm.
It says every algorithm for solving the problem, must take time at least omega of f of n. So, it is a lower bound, then the time taken for any algorithm, which solves the problem. Now, if we could prove something like this. That is, if we can prove that the time taken by the algorithm, is equal to the problem lower bound or is equal to within this omega, or within some proportionality constant.
What do we know, well if we could prove something like this, we know that we have the best possible algorithm. If this equality is exact and we know, we have the absolute best possible algorithm. If this equality is approximate, well if it is of the form omega. So, we know that this bound, that we have the best possible algorithm to within a constant factor. So, this is the main motivation for studying, what are called problem lower bounds.
(Refer Slide Time: 03:05)
  .
So, here is what we are going to do today. So, we will be having a general discussion, of problem lower bounds. Then, we will consider the question of lower bounds for sorting. So, problem lower bounds, but the problem being sorting. Regarding sorting I will introduce a model of computation, called the decision tree. And then, we will prove problem lower bounds on decision trees, for the problem we are sorting. Now, you may wonder why do we care about decision trees, is it not the random access machine or the RAM model, which we have been defined and which we have been using so far in the course; is it not, it a good enough model.
Well, it turns out that whatever we do on decision trees, is actually quite relevant to the RAM model. So, we will see this relevance. And then, we will come to average case lower bounds. Again we will prove average case lower bounds, on decision trees. But, again all that will be relevant to the RAM as well.
(Refer Slide Time: 04:12)
 
So, let us start with something, which we have already studied, at least a little bit. So, we have already defined the notion of putting the lower bound on the time taken by an algorithm. So, here is what we said, we said that if we have an algorithm A for a problem P. Then, we will say it has a lower bound f sub A. where f sub A is a function. If the time for algorithm A on instances of size n, is greater than or equal to f A of n.
And of course, we are not worried about every instance, but we are worried about the worst case instance, the worst instance, the instance for which the time is a largest. So, always in this course, we have been stressing the worst case bounds. Or the worst case times, when we measure the performance of algorithms. And even here we are doing that. So, we are asking, what is lower bound on this time, the time of the worst instance i of size n of course.
So, it will be f A of n, this is what this means. This is what it means to have f A as a lower bound on a problem P, on a algorithm A for a problem P. Now, of course, it is customary to say that, this inequality holds, only for large enough n. So, there is actually a clause over here, which says for all n greater than n naught, where n naught is some number. But, let us not worry about too many technicalities ((Refer Time: 05:47)).
Now, the typical way we prove lower bounds, is by constructing instances i which take a large time. So, if we have an algorithm, we look at where the algorithm is weak so to say. And we construct instances, which show that the algorithm must take a long time on that particular instance. So, that gives us good value of f sub A, f sub A of n. And we have to do this for every instance, every size instance size. If we can do this, then we can construct this lower bound function.
Next, we will look at problem lower bounds. A problem P is said to have a lower bound f, if every possible algorithm has lower bound f. So, this statement has to apply, but it has to apply for all possible algorithms, for this problem P. Here is the more algebraic statement. So, notice that this part, is just similar to this part; and now we have an additional quantification. We are saying that this must hold, even for the best algorithm. So, the algorithm which takes the minimum worst case time, must have time bigger than f of n.
Of course, with the best algorithm has time bigger than f of n. Then, clearly every algorithm will have time bigger than f of n. So, this is what it means for f to be a problem lower bound, for this problem P. How do we prove such bounds, now we have to construct bad instances. Instances which show, that the time taken is large. But, we have to construct them, not only for all problem sizes, but for all algorithms as well.
And of course, the instance it need not be the same instance for all algorithms. It could be that for one algorithm, it is one instance which is bad. For another algorithm, it is another instance which is bad, that is ok. But, somehow we must give a construction, which shows that no matter what algorithm you give. Here is an instance on which it will take a long time. We may not do this directly, but at least indirectly something like this has to be proved, in order to prove a lower bound over here.
(Refer Slide Time: 08:21)
 
Here are some trivial lower bounds, that we can prove. Most problems have omega of n lower bound, where n is the instance size I say this is trivial. Because, no matter what algorithm is used, the algorithm at least has to read all the input. The input has length n and therefore, omega of n time has to be taken. So, in that sense, there is nothing clever about this. And it by and large applies to every problem. There could be some problems where it may not apply, but those are not probably very interesting problems.
(Refer Slide Time: 09:06)
 
What about non-trivial bounds? It turns out that on random access machines RAM?s, proving non trivial bounds is actually very difficult. And here are some of the reasons. Basically when we assert a problem lower bound, we are saying something about every possible algorithm. Now, on RAM?s, the space of all possible algorithms is really huge; and it is tricky to analyze. So, a single problem may have lots and lots of algorithms.
Enumerating those algorithms or analyzing them in some structured fashion, is often a very tricky business. It is tricky, because of some of these reasons, a RAM has many instructions. RAM has many instructions to do all arithmetic, they can be composed to do more complicated things. You can take logs, you can exponentiate, you can take trigonometric functions. You can do all such things with Ram instructions. There are also many control flow pattern, there is looping, there is recursion and so.
If I give you a RAM program, analyzing it is pretty difficult. And saying something about all possible RAM programs is even more difficult. So, here is what is typically done. So, we define a simpler computational model, which say does not have that many instructions. And which does not have that many control flow patterns. So, we define such computational models and then, we analyze that. It will turn now, that on such models the space of programs or space of algorithms is actually fairly small. Well, it is never really small, but it is much easier to visualize. So, that is what we are going to do next.
(Refer Slide Time: 11:13)
 
So, here is one such model, this model has been introduced in the context of algorithms such as sorting. So, it will instructions which are relevant for sorting, but which are perhaps not very useful for other problems, but since typically we will consider this model in the context of sorting. This model will actually be a reasonably good model to look at. So, here is what the model looks like. So, the input to every algorithm is always going to be a sequence of numbers.
And we are not going to be worried worry about inputting those numbers. We will assume that those numbers have already been read. And they are already stored somewhere in the model. A program for this model is a labeled tree. So, every non-leaf node has labels of the form i colon j, where i and j are integers. I will tell you exactly what a label means in a minute, but let me just describe the model first. Each edge also has a label and the label could be any of these relational operators. Leaf node labels are the values, that are going to be printed. So, that is what the structure of a program is.
(Refer Slide Time: 12:40)
 
So, here for example, is a program which will be used for sorting 3 numbers, which can sort 3 numbers. I have not told exactly how it sorts 3 numbers, I will tell you that. But, I just wanted to give you a picture for the program tree model, that I just mentioned. So, as you can see, the label over here is 1 colon 2. This edge is labeled with greater than and so on. And the leaf is labeled with 3, 1, 2 which is what it is going to output, as per the execution model, which I will describe next.
((Refer Time: 13:17)) The execution in this model begins at the root, at any node labeled i colon j input x i, which came over here is compared with input x j. The result of this comparison is some relational label, so it is either say actually its either less than equal to or greater than. And based on that result, the execution follows the appropriate branch. So, say for example, the result is less than. Then, we will look for branches having either the label less than. Or the label less than or equal to branch on that.
We will require that only one out of less than or equal to and less than be a label present, on the outgoing edge. This is because, we want our algorithms to be deterministic. So, once we make the comparison, we want a unique path to follow outwards. When the execution arise at any leaf, the label of the leaf is output. So, let us try this out for this program, that we have drawn over here. So, as I said this program is going to be used for sorting 3 numbers.
So, let us say the input instance is x 1 equal to 20, x 2 equal to 30, x 3 equal to 10. As we said the execution starts by looking at the root by starting at the root. At the root we compare x 1 with x 2. So, in general if the label is i and j, we compare x i with x j. So, when we compare x 1 which is 20, with x 2 which is 30, we discover that x 1 is smaller. So, if it is smaller, then we follow this branch.
So, we follow this branch and we arrive at this node. And when we arrive at this node, we have to act according to the instruction, represented by this node, which is to compare x 2 and x 3, x 2 is 30, x 3 is 10. So, therefore, we find that x 2 is bigger than x 3. And therefore, we follow this branch. Then, we perform the instruction represented by this node, which requires to compare x 1 and x 3. So, x 1 is 20, x 3 is 10, x 1 is bigger.
So, we arrive at this node and at this node, we just output. So, we output 3, 1, 2 which is representing our conclusion, that x 3 is the smallest, x 1 is the next smallest. X 2 is the largest and indeed you will see, that x 3 is 10 which is the smallest of 10, 20, 30. X 1 is the next largest and x 2 is the largest. So, at least for this instance, we have checked that this decision tree, which represents a sorting program has in fact, correctly sorted this input instance.
Notice that if I increase the size of my input, ((Refer Time: 16:44)) I will have a different program. I will have to have a different program. This is unlike what we usually do in a RAM model. But, that is we are going to allow that in this model, to complete the discussion of the model, let me just mention that the time taken by the program is simply the number of comparisons performed for each instance.
The worst case time is equal to the length of the longest path, in the program tree. Because, if in any execution you follow that path, that is the time you will end up setting. Likewise, we can also define average case time. So, here the input instances will be chosen randomly with equal probability, from all possible input instances. Or we are just asking for an average time over all possible input instances.
And therefore, we should consider all possible root to leaf paths. And it is simply the time for each path is, it is length. And so the average case time, is just the average root to the leaf path length. So, notice that, the time taken has a very nice graphical interpretation in this model. So, now here are the main claims, that I am going to make.
(Refer Slide Time: 18:03)
 
The first claim is that sorting will take time n log n in the decision tree model. This claim has to be understood properly. This claim says, that no matter what algorithm you use. And by algorithm we mean no matter what tree you use. Because, that is the space of all possible algorithms, no matter what we use that time taken will be n log n. The length of the longest path in the tree, will be at least n log n. So notice that, this is already a non trivial bound, because a trivial bound will be omega of n.
We will prove this in a minute, but I first want to relate all this to a RAM model. Because, RAM is after all what we use every day. So, a definition first, RAM sorting algorithm is said to be comparison based, if the only operations it performs on keys are comparisons and copying. So, if you go back and think about the sorting algorithms, that you have seen. There is a good chance that most of them are in fact, comparison based.
Take heap sort for example, the only operations you do on keys, are compare to keys. And maybe you copy that, similarly for merge sort, similarly for insertion sort. Similarly, for shell sort, if you have studied that. So, in fact, most of the algorithms, that you have studied probably are comparison based algorithms. And it turns out, that they are actually quite nicely connected to decision tree algorithms.
So, here is the claim, let A be a comparison based RAM sorting algorithm. Then, there exists a decision tree sorting algorithm C. That performs the same key comparisons, as A on every input instance. In fact, in the same sequence, this means that the time on this decision tree model, is intimately related to the time on the RAM model also. For this class of algorithms. The final claim says, every comparison based sorting algorithm on the RAM, must take time n log n. So, this is where we have finally, come back to the RAM.
So, here is a non trivial result. Not about all sorting algorithms, but for comparison based sorting algorithms. For comparison based sorting algorithms, we have proved that the time taken must be at least n log n. Well, we have claimed and we will have proved this in a minute. So, let us try proving this first this. This in fact, turns out to be the main, the crux of the matter. So, we want to prove that sorting takes time, omega of n log n in the decision tree model.
(Refer Slide Time: 21:04)
 
So, we are going to start with a certain input x. Remember x is that sequence x 1, x 2, x 3. And let us suppose, we have a sequence x which is the permutation of the sequence 1 through n. Permutation is just the rearrangement of that sequence. So, x is pi of this. So, what can we conclude from that, well then 1 to n must be pi inverse of x. So, if I apply the inverse permutation on both sides, I will get pi inverse over here, and 1 to n over here.
So, notice that when we do the sorting, we actually are supposed to compute this inverse permutation. So, every leaf the answer, essentially must be pi inverse 1, must be pi inverse. So, essentially pi inverse must appear as a label of at least one leaf, essentially, because that leaf must have somehow identified this permutation. And only then, can it name how this permutation can be unscrambled to get 1 to n. So, pi inverse must appear as a label of at least one leaf.
Now, this holds good for all possible permutations. Not just a particular pi, how many permutations are there, well there are n factorial permutations. And therefore, there are n factorial leaves. So, this is the key insight. We have proved that this tree, the algorithm tree must have at least n factorial leaves. Each node of the tree can only have at most 3 outgoing edges. Well there are so many relational operators which we mentioned. But, we cannot use them simultaneously, as I mentioned earlier.
((Refer Time: 23:15)) We can only have, so here is a node. If we have less than here, then we cannot have less than or equal to over here. We must have something which is disjoint from this. So, the only possibilities are we have greater than or equal to, in which we can have only 2 labels. Only 2 outgoing edges or we could have 3 outgoing edges less than, equal to and greater than.
So, at most 3 outgoing edges are possible. Now, each tree node can only have 3 outgoing, if it can only have 3 outgoing edges. Then, the height must be log of the number of the leaves to the base 3. The height of a ternary tree, is log of the number of leaves to the base 3 at least. And therefore, height must be log of n factorial to the base 3, because n factorial is the lower bound on the number of keys number of leaves, turns out however that if you are using only permutations of this, then we will never pass through this equality edge. So, which means the n factorial leaves must be accessible only passing along the strict inequality branches. So, which is as good as saying that, if our input is permutation of 1 to n. Then, we are only considering a sub tree whose degree is at most 2. And therefore, the height is at most log of n factorial to the base 2.
But, log of n factorial to the base 2, can be now thought as to expand out the product. You can see that it is this and this is actually easy to see, so let us do that. So, n factorial is n times n minus 1 times n minus 2 and so on. Somewhere in this, we are going to have n upon 2 and then, there are going to be some more terms over here. But, all these numbers you can see, are going to be at least n upon 2. And how many such numbers are there.
Well, there are n over 2 numbers each of which is at least n over 2. So, therefore, n factorial is bigger than n over 2 to the power n over 2. So, what we have now is log of n factorial to the base 2, is at least log n over 2 to the power n over 2. Well, we can simplify this. And we get n over 2, this is equal to n over 2 log of n over 2 and therefore, this is omega of n log n ((Refer Time: 26:04)). So, what you have done, so far is that we have proved this. Now, we want to look at the next claim.
The next claim is the one which relates comparison based sorting algorithms in the RAM, to decision tree sorting algorithms. So, the claim says that let A be a comparison based sorting algorithm. Then, there exists a decision tree sorting algorithm C, that performs exactly the same key comparisons as A on every input instance. So, we are going to prove this and the proof is actually going to be constructive. So, what we will do is, we will assume that we have been given this algorithm A which is a comparison based RAM sorting algorithm. And then using that we will construct a decision tree sorting algorithm C. And we have to make sure, that C performs exactly the same key comparisons as A does on every input instance, and also in the same sequence.
(Refer Slide Time: 27:22)
 
So, this is what we are going to do, we are going to construct C from RAM algorithm A. So, let me remind you what C looks like. So, C is going to be some root node ((Refer Time: 27:37)), may be some branches may be nodes over here and so on. So, if I tell you how these things are going to be labeled, then I am done. So, let me tell you the edge labels right away. The edge labels are going to be less than over here, equal to over here greater than over here. So, let us keep it really simple and similarly subsequently, but I have to tell you what the label here is going to be.
So, the question is how do I do that, so it is going to some i colon j. But, I have to figure out what i colon j, it is going to be. And here is the key insight in figuring this out. The main idea is that which keys are compared first by A, is our RAM algorithm. Does not depend upon the key values, why is that? Well, A is going to read things and then, it is going to compare. It is not going to look at the keys and base it is decision on which keys to compare on the value of any key.
Because in fact, value of any key, does not really come into the control structure of A, other than in the comparisons that A does. So, if A does a comparison, that is the only time when A can actually peep inside a key value. So, clearly the first time that A peeps inside a key value, is the first time A does a comparison. And in fact, that which keys it compares is going to be the same no matter what the values of keys are.
So, i j can be determined by examining the algorithm A. We just look at the code of A. And will know which key is going to examine first. So, whatever they are we can write that now over here, i colon j. We find actual numbers i and j and we put them down over here. So, I claim now that whatever numbers i and j be put down over here. They will be the same, no matter what the values of key are and that is what I just explained. So, next we want to find the labels of say this node over here, to do that here is what we consider.
So, suppose i is an instance, in which x i is less than x j. Now, we are going to examine the program of A to determine, which keys are compared in the second comparison for this instance i. Suppose, x k and x l are compared. So, those values, those k and l, we will put down over here. So, now what is the idea? So, for instance I what will happen, the first comparison will be this. Then of course, in the decision tree, this will be the branch taken and the second comparison will be this.
So, instance I, in fact in instance I, the first two comparisons have been the same in the decision tree. As they have been in our algorithm A, so far so good. So far we have built this part of our tree. And indeed in this tree the decision, the comparisons made on this instance, are the same as comparisons made on the instance, in our RAM model also. But, let us consider another instance I prime, what will happen in that.
So, if we consider another instance, I prime. in which x i is less than x j what will happen, which keys will be compared next. I claim that even in that case, the keys to be compared will be just be these x k and x l, why is that? So, let us examine this, ((Refer Time: 32:06)) so here is an execution of I and here is an execution of I prime. So, somewhere we start execution. So, we compare x i versus x. And we know that the first comparison is the same in both and so we compare x i versus x j.
Then, we do some instruction over here and we compare x k versus x l. So, when we do this comparisons, the question is what will happen over here. I claim that if in these instructions, no key is even touched. Then, exactly the same instructions will be executed over here. Why is that, well the control flow can change, only if there is a comparison, and if the comparison has a different result. So, in this entire part, there has been a comparison with keys, but the result over here is the same as the result over here.
Because, in both cases we said that x i is less than x j. So, during this entire portion and this entire portion, wherever there have been comparisons. The comparison outcomes have matched. And therefore, the instruction which is going to be executed over here, is going to be exactly the same. And therefore, we will also compare x k and x l over here. So, which means even for this instance I, I prime k and l, x k and x l will be compared. And therefore, we can put down this label of the node as k compared to l. Now, we can generalize this.
(Refer Slide Time: 33:46)
 
So, the general pattern is, we have constructed some super tree of C, how do extend it. So, we have constructed something like this, and so in general we might have had constructed some path. And then, we want to extend this path outwards, how do we do that. So, here is the observation, the label of a node can be determined by considering an instance, consistent with the labels on the path till the node.
So, I want to determine the label of this node, how do I do it. Well, I look at instances on the path from the root, I look at the labels on the path from the root to this node. And I construct a instance, which is consistent with these labels. So, which means that x i must be equal to x j. And then, what ever the label is over here the corresponding keys here must satisfy this label and so on.
And then, we simply look at the algorithm for A. And ask what is the next comparison, that is going to be made. And we put down those labels over here. So, we have constructed an instance, which is consistent with the labels along this path. And we look at the algorithm and decide, what is the comparison that is going to be done by the algorithm in that instance.
So, if the comparison is between some x p and x q. We put down p colon q over here ((Refer Time: 35:21)). So, exactly like what we did in this case, but just more general. So in fact, in all such instances, which are consistent with the labels along this. You can argue exactly in the same way, that x p will get compared with x q. And that will fix the label of this. If it turns, that the information which has been gleaned along this path, is enough to determine the final sorted order.
And because of which the algorithm does not do any comparisons at all. Then, well we simply take that sorted order. And put back the label and name and make that leaf. Now, the important point is, that we have constructed our decision tree algorithm in this manner. And by construction, because of the care that we took in the construction. You can see that C performs, exactly the same key comparisons as A does. And exactly the same sequence on every instance.
((Refer Time: 36:31)) So, this finishes this claim. So, now I want to argue that every comparison based sorting algorithm on the RAM, must take time n log n. But, this is actually fairly straight forward.
(Refer Slide Time: 36:41)
 
So, suppose some comparison based RAM algorithm, takes time T of n, what do we know about comparison based RAM algorithms, by the claim that we just proved. We know that there must exist a decision tree algorithm, taking time at most T of n. But, the first claim tells us something about decision tree algorithm. The first claim says that the time taken by the decision tree algorithms, must be at least n log n.
So, T of n had better be at least n log n, but then this T is the same as this T. And therefore, comparison based RAM algorithms must also take time n log n. So, we have proved this claim ((Refer Time: 37:29)). So, notice that in a single claim, we have proved a lower bound on a variety of sorting algorithms, heap sort, merge sort, shell sort, insertion sort, and many other algorithms, which we have not even though about. But, which only if they are comparison based, then this lower bound will apply to them.
(Refer Slide Time: 37:57)
 
Next we turn to average case complexity. So, basically we are going to ask the question, what is the average case, can we put a lower bound on the average case complexity. So, on the average time to sort, not just the worst case time. So, this actually first follows fairly easily, just based on structures, on some properties of trees. So, here is the first claim, if a tree has K keys, then there exist at least K over 2 keys at a distance log K over 2.
Why, well there can be at most square root K leaves at a distance, just like worst case. We can use similar arguments, to prove bounds on the average case complexity of sorting as well, on the decision tree model of course. And of course, that will imply some lower bounds, on comparison based sorting algorithms. As far as the average case complexity is concerned. And these bounds follow very simply from some properties of trees, with respect to the relation between the height of the tree and the number of leaves.
So, here is the first claim, if a tree has K leaves, then there exist at least K by 2 leaves at a distance. At least log K by 2 from the root. Well, here is the proof, there can be at most 2 to the ((Refer Time: 39:45)) l leaves at a distance l, which means there can be at most 2 to the power log K, whole thing upon 2. Leaves a distance l and this is nothing but K to the half, square root of K. So, that is what is claimed over here. The rest of the leaves have to be at larger distance.
And in fact, this says that as many as K minus square root K, have to be at a larger distance, whereas what is claimed over here is just K by 2. So, the claim is very easily proved. In fact, we can prove something stronger. But, once we have that, we can prove the following, we can prove that the average path length. Average is taken over all leaves, is at least log k upon 4, why is that? Let us count what the total path length is.
The total path length, we are going to calculate first for these leaves which we talked about. So, there are K over 2 of them. And each has a path of length log K upon 2. So, the total path length just for these K over 2 leaves is at least this much. So, now the average is going to be a K th fraction of this. And therefore, it is simply going to be this K factorial drop out, this 2 and this 2 will give us 4. So, it is going to be log K over 4.
From this it immediately follows that, the average time for sorting on decision trees, is n log n. And here is why, every algorithm tree has n factorial leaves. Thus average path length is bigger than this claim, log of the number of leaves or the number of or log of n factorial upon 4. But, log of n factorial upon 4 is n log n. And therefore, average time for sorting on decision tree is this much. And this means that the average case time, for all comparison based sorting algorithms.
Say heap sort, merge sort, shell sort all of these is n log n. So, when we did, when we studied average case complexity of quick sort. We proved that although the worse case complexity for quick sort was n square. The average case was n log n. So, in a similar manner, it may be conceivable, that heap sort may have worse case complexity of n log n. But, it is average case complexity is just n, it might have been conceivable. But this result shows that the average case time for heap sort, is also going to at least n log n. And similarly for merge sort, shell sort and any other comparison based sorting algorithm.
(Refer Slide Time: 42:51)
 
So, now I want to conclude and here are some remarks. First some implications of the lower bound, on comparison based sorting algorithms in the RAM. So, here is an important application. So, if I want to sort faster than n log n, what do I know, well I know, that I must use some other operations besides just comparisons. If I limit myself to just comparisons what happens. Well, there is a decision tree algorithm, which mimics the RAM algorithm. And therefore, I am stuck to a lower bound of at least n log n. So, I must use some other operations besides comparisons, are there such operations there actually are?
(Refer Slide Time: 43:46)
 
So, in fact a popular sorting algorithm called bucket sort, does exactly this. The main operation it performs, is that uses the key values, as an index into an array, which was not allowed in comparison based sorting algorithms. So, here is the main claim, if the keys are integers are 1 to R, in the range 1 through R. Then, sorting time is at most n plus R. So, I will just give the algorithm as the proof.
So, first we are going to build an array of buckets B 1 through R. So, these buckets will be initialized to null. So, that will take about R time. Then, we take key x i and we insert it into this bucket B of x i. So, B of x i can be compute in constant time. So, in constant time, we keep a list over there and we insert this key into that list. And we do this for all the keys, for all i. So, now all keys are now into their buckets.
Now, for i equal to 1 to R, we go visit each bucket in turn. And we print out the contents, clearly we will print out the contents in increasing order. What is the time taken, well the insertion time is proportional to the number of keys. So, because of this statement, the time taken to initialize the array of this B array, is going to be R. And printing out the answer, well we have to visit every element of the array. And we also have to visit every key, which has inserted in some bucket.
So, the time taken is going to be O of n plus R. So, the total time is going to be O of n plus R. And this is going to be faster than n log n, if R is small. In fact, this is not the only range, if you want some bigger ranges say 1 through R square. Is slight modification of this will in fact, do the job. ((Refer Time 45:56)) The second concluding remark that I want to make, is that not only sorting. But, you can use the decision tree model to study several other problems.
So, the other problems are say merging. And this will figure in the exercises, which I will show later. And also things like finding duplicates. And this is going to be the topic in the next lecture. In fact, let me point out, let me say in conclusion. That in fact, we can extend this decision tree model. So, I want to go to my second concluding remark, which is to say that the decision tree model, can be used to study other problems as well, not just sorting.
So, in particular it can be used to study merging problems, which I guess you might think as very similar to sorting. It can also be used to study problems, like finding duplicates. Or it can also be used to study problems, such as putting lower bounds on the time to calculate the intersections, between two given sets. Now, this problem, might seem like rather different from sorting. And it is perhaps somewhat surprising, the decision trees can be used for this.
Nevertheless, they can be used and in fact, we will see the finding duplicates example in the next lecture. I will also state that the idea of decision tree, is the general model of decision tree is can be extended somewhat. So that, rather than just allowing comparison, you can also allow some arithmetic incident. And so the lower bound model can be made somewhat stronger. In any case, we have seen some lower bound results. And we will continue this with the next lecture.
Thank you.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 25
Element Distinctness Lower Bounds

Welcome to the course on design and analysis of algorithms. Our topic today is Element Distinctness Lower Bound. This will be the continuation of the previous lecture, which was on sorting lower bounds. Let me start by defining the problem. So, the problem is as follows.
(Refer Slide Time: 01:11)
 
You are given as a input, a sequence of numbers. Let me call the entire sequence x and the individual numbers in the sequence are x 1, x 2, x n. We are supposed to output a yes answer, if all the x are distinct. And otherwise, if some x i equal to x j some two numbers x i and x j, distinct two numbers x i and x j are identical. Then, we are supposed to output and no answer. A no answer means, that no all the elements are not distinct.
What we are going to prove today is that, in the decision tree model, which we talked about last time. And which I will quickly define this time also just for continuity. We will prove that in this decision tree model, the time is going to be n log n. So, this is a non trivial lower bound, in the sense that it says. That you need to do a little bit more, than just examine the numbers, which would just take omega of n time.
It should be quite obvious to you, that if you are allowed to use n log n time. Then, element distinctness can easily be solved. How? Well, in n log n time we can sort all the numbers. And now, if two numbers are identical, they are guaranteed to come next to each other. In which case, we simply have to compare adjacent numbers after sorting and that will enable us to find out, whether all the numbers are distinct are not.
So, in summary in n log n time, we will be able to actually compute, whether the numbers are distinct. However, the subject of today?s lecture is not that. The subject of today?s lecture is to prove, that at least n log n time is needed. So, we want to prove a lower bound.
(Refer Slide Time: 03:03)
 
Here is what I am going to do today? So, today I am going to talk about, the decision tree model. I am going to go through this rather quickly. Because, it is going to be very similar to what we did last time. Last time we looked at lower bounds on sorting and we introduced a lower bound technique. So, I am going to explain, why that lower bounding technique does not work, that seem like a very nice technique. But, it will turn out that, that does not really work for the problem, which we are looking at today.
Then, I will talk about a new lower bound technique, which works for this problem of element distinctness. And then, we will prove the lower bound that I mentioned. Finally, I will extend the model. So, instead of the decision tree model, I will define a more powerful model I will call the algebraic decision tree model. And I will talk a little bit about it.
(Refer Slide Time 04:04)
 
So, let me start with the decision tree model. So, in the decision tree model, the input is always going to be a sequence of numbers. Say, the same sequence x 1 through x n. And we will assume that these numbers have already been read into the model. So, there are no input instructions as such, the inputs are already read. A program in this model is a label tree. And all non leaf nodes are labeled i colon j, where i and j are integers.
And these integers have to be fixed as a part of the program. Leaf node, each leaf node has a label, which just says what is the value to the output? Edge labels are relational operators. So, less than equal to greater than or not equal to less than less than or equal to greater than or equal to. Let me quickly take an example of a decision tree program.
(Refer Slide Time: 05:12)
 
So, here is a decision tree program for sorting three numbers. As you can see, each non leaf node is labeled i colon j in this case 1 colon 2. And it is each leaf node is labeled by the answer, that is to be output. And furthermore, each edge is labeled with a relational operator ((Refer Time: 05:34)). Execution begins at the root, at each node labeled i colon j, x i from this set of inputs is compared with x j.
Whatever the outcome of that comparison is, the corresponding branch is found with the corresponding label. The corresponding outgoing edges are found and execution follows that branch, that outgoing edge. So, you go down to the node, which is at the other end point of that edge. And you repeat this whole thing, until you get to a leaf. When you get to a leaf it is label is the output. So, that is the answer that you are going to compute, that you wanted to compute.
So, two things to be noted. So, as we saw in the example, there is going to be a separate program for each input size. So, there is going to be a separate program for n equal to 1, for n equal to 2, for n equal to 3 and so on. And we saw a program for size n equal to 3, not for element distinctness, but for sorting. And then, the other point we noted is, that although we are looking at a decision tree model. And a decision tree model is not exactly like our computer, like any of our computers.
It does in fact, have a connection to the RAM model or the Random Access Machine model which in fact, resembles our computers. And this relevance has been discussed in the previous lecture. And let me remind you what that relevance was. So, we said in the previous lecture, that if we have a lower bound in the decision tree model, when it applies to the RAM model, not completely. But, it applies to comparison based algorithms in the RAM model. Comparison based algorithms are simply algorithms, which compare the keys, they do not perform arithmetic on the keys. Or they do not use, the keys to induct into the arrays. They simply compare the keys and of course, they make copy.
(Refer Slide Time 07:48)
 
So, here is a quick overview of the sorting lower bound. The input to the sorting problem was the sequence x. The same sequence which we have mentioned, consisting of components x 1, x 2, x n. So, these are all numbers and we want to sort these numbers. The key thing to observe in this problem or in the lower bound argument is that there are n factorial possible answers.
So, either I could say that, this is the sorted sequence or I could take a permutation of this. And say that is a sorted sequence. I could take every possible permutation of this and that could be my answer. Every possible permutation could be an answer and there are n factorial permutations. And therefore, there are n factorial possible answers. This is a very important point, in this argument.
Now, the answer is going to be printed at the leaf of these trees of the program tree, which means that, if you want to print out n factorial different answers. Then, you must have n factorial leaves, you have no choice. Because, a single leaf can just print a single answer, it will print out that entire permutation. But, that entire permutation constitutes a single answer. So, if execution arrives to that leaf, then that is the only answer it can print out.
So, if your program tree is going to be even capable of printing n factorial different answers. Then, it had better have n factorial leaves. But, if it does have n factorial leaves. Then, the height of the tree has to be at least log of n factorial, which is n log n. And in fact, the height of the tree is the worst case time. And therefore, the worst case time taken is at least n log n. So, this is a rough sketch of the argument, that we saw last time.
This idea, that if the answer is n factorial possibilities. Or if there are n factorial different answers, in the language of information theory, can be expressed as saying that this answer has high information. So, whatever we are going to print out as the answer, if you think of it as a variable, it has a very high information content. The answer can take n factorial different values. So, it has the information quantity in it is high is something like n factorial.
In fact, information theory measures information as the number of ways, the log of the number of ways. In fact, it is going to be log of n factorial, very roughly. In any case, bounds of this kind, where we say that there are so many ways in which this variable can take a value are therefore, called information theoretical lower bounds. So in fact, you will see the sorting lower bound often refer to as the information theoretic lower bound.
Let us now turn to the element distinctness problem. How many answers do we have? Well, the answers are two really, the answer could be yes, which means all elements are distinct or the answer could be no. No means some duplicate exist. So, we only have two answers, so our answer has only two possibilities. So, log of that is not going to be very large it is going to be in fact, exactly one.
So, if we just say that there are two possibilities. And therefore, there are two leaves that does not help us much. Because, log of 2 is 1 and it just says that, the tree must have height of value, which is really a silly bound. So, we need to do something better. So, we need to have a new strategy.
(Refer Slide Time 12:01)
  
So, here is a rough sketch of the strategy. So, we will show that there must be at least n factorial leaves. Again the n factorial is going to turn out be somewhat significant. But, we will prove that in fact, there must be n factorial leaves all of them giving yes answers. So, this is going to be a interesting argument. And we will be making some rather clever use of n dimensional geometry. Do not be worried about n dimensional geometry, most of the time for the purposes of getting intuition. You can visualize, what is going on in two or three dimensions. And that usually tends to be enough which is in fact, going to be the case in our proof. However, if we want it algebraically write down things, the arguments can get a little bit complicated. But, fortunately even the algebraic argument that I am going to show you is going to be rather simple.
(Refer Slide Time 13:12)
 
So, here is our main claim again, we are going to prove that the time required for the element distinctness problem is going to be at least n log n on the decision tree model. Let me remind you what this claim means. So, this claim asserts a problem lower bound, it says that the time required for this problem, irrespective of any algorithm is this. So, it is not a bound on an algorithm. Well, if you want to think about algorithms, it is a bound on all possible algorithms, in this model of course.
Here is a quick overview of the proof, the proof is a little bit long, but not terribly so. The first idea is going to be to interpret our input sequence. So, we have our input consists of n numbers x 1, x 2 all the way till x n. So, we are going to think of this n tuple as representing, the coordinates of a n dimensional point.
So, x 1 represents the first coordinate, x 2 represents the second coordinate, x n represents the nth coordinate. And the entire thing represents a point and will call that point x. We are going to restrict these instances to the unit n cube. What do I mean by that? Well, we are going to insist that all the x i's lie between 0 and 1. We will prove the lower bound under this restriction.
So, that should not be a cause of any worry. If we prove the lower bound under this restriction of course, it works if we do not have this restriction. So in fact, if we do not have this restriction, who knows things can get even much worse. But, we are not worried about that, we just want to argue that things can certainly get at least as bad as this. And so it is to have this restriction.
Here is the first claim, that we are going to make. We will use the notation R of L, which stands for the region for L. So, the region of L, where L is a leaf is a set of all input instances for which leaf L is reached at the end of execution. So, this is the definition of what R of L means, what we are asserting is that this R of L is connected. I will explain to you what connected means in just a minute. But, it is the usual notion.
So, a region is connected, if it looks together or if there are two points in it. And they can be joined by a path, within that region. We will do this a little bit more formally and we will write it in a minute. The second claim goes something like this. Suppose we have a point x whose coordinates are distinct. So, the sequence consist of distinct elements or alternately these coordinates are distinct.
Now, we take y which is the non trivial permutation of these coordinates. So, here is this coordinates, pre reorder than. Such that this sequence and this sequence look different. That is what x and y are. So, the main claim is and this is the key claim in this entire proof x and y must reach distinct yes leaves. So, the point is this, that this will allow us to argue, that there will be at least two distinct leaves or who knows more.
And then, we will argue in fact, based on this essentially, that the number of yes leaves is bigger than n factorial. The time is simply going to be the height of such a tree. And it is going to be log of n factorial or n log n. So, this will immediately follow from this, we will see that later. So, let me go over each of these items. And let me explain each of these items.
So, let me start with this, we are going to interpret, this x as a point in n dimensional space. And of course, we are restricting the points to unit n cube. As I said, it is hard to visualize n dimensional space. So, we are just going to leave it two, we will visualize two dimensional space and we will say what happens? And you must note that, even if we work with two dimensions, we will get enough of the insight.
(Refer Slide Time: 18:16)
 
So, here is our visualization. So, our instance is this x that first coordinate is x 1, second coordinate x 2. So, this is our x 1 axis, this is our x 2 axis. Our instances come from the unit cube. So, here is the unit cube in two dimension, this entire interior is a unit cube. So, this is one, this is one, this is where our instances come from, what I mean by that is I pick a point over here. It is first coordinate is the x 1 value, it is second coordinate is the y 1 value.
So, let me continue the analogy of little bit further to show you, what we make sure we understand at least this case of two dimensions very well. I claim that all the instances which lie on this diagonal are no instances for this problem of size two. Well, what are the instances which lie on this diagonal. So, the instances on the diagonal simply are those points whose x 1 coordinate is exactly equal to the x 2 coordinate.
But, if the x 1 coordinate is equal to the x 2 coordinate, then we know that they may n dimension distinct. And therefore, these points on the diagonal are in fact, the no instances. The interior of this triangle and the interior of this triangle are the yes instances. Why, because if I pick a point in it, I know that the x and y coordinates cannot be equal, the x and y well the x 1 and x 2 coordinates cannot be equal.
Similarly, over here this is the line which divides the square into two parts. And this is the line on which the x 1 and x 2 coordinates are in fact, equal. So, what we have done over here is, we have taken our problem. And we are viewing it geometrically and you will see, that this geometric view point gives some interesting insights ((Refer Time: 20:25)). So, we have finished interpreting these instances in a two dimensional space or in particular in an n dimensional space.
So, we have finished this part. We interpreted our input instance, as a point in n dimensional space. Well, in this case it was just two dimensional space. But, you should get the idea and we also restricted the instance to be in n cube, which in this case was just the unit square. Now, you want to prove this claim. So, this and this are the two main claims.
This claim says, that if we look at the region of this cube from which instances will reach L. Then, this region is going to be a connected means. So, let me start by defining what connected means.
(Refer Slide Time 21:21)
 
A connected region in n dimensional space. So is as follows A region R is said to be connected. If for any points x or y in R, there exists a path from x to y passing entirely through R. So, we have any points x and y in R and there exists a path from x to y, which goes only through the points to R. So, what is a connected region? So, the cube that we mentioned for example, is a connected region the interior of it. Well the surface of it is also connected region.
So, let me now define a convex region. So, this is going to be needed in the proof. A region R is said to be convex, if for any points x and y in that region. The straight line path from x to y passes entirely through R. So, here we just said, that the any path that some path passing from x to y, must pass entirely through R. Now, we are making the stronger requirement. So, we are now saying, that in particular the straight line path from x to y must pass through R.
Notice, that convexity is only a special case of connectivity. So, in other words, if we know that a certain region is convex, then it has to be connected of course. So, the reason why we worry about convexity is that, if we look at only straight line paths, they are very easy to reason with. And therefore, its often much easier to argue, that is a certain region is convex. So, convexity of a region is easier to prove. And therefore, we are worried about convex regions.
But, notice that if we prove that something is convex, we are also proving that it is connected. So, some examples of convex objects are the cube, the fill the whole cube or the whole sphere. Examples of objects which are connected, but not convex say something like a torus. So, let me draw a picture.
(Refer Slide Time 23:53)
 
So, if you have a torus, so it has a hole in it. So, if I pick a point x over here and a point y over here. Then, the line joining the straight line joining them would pass through this region, which is not in R. So, this torus is itself in R, but this region is not is R.
(Refer Slide Time: 24:17)
 
A kidney shaped region or a cashew shaped region is also not convex. Because, I can take a point x over here, a point y over here and this line passes outside the region.
(Refer Slide Time: 24:33)
 
If I look at a cube and if I just look at it is surface not the interior. But, if I just look at it is surface, if I take a point over here and a point over here and this phase. Then, the line joining them has the straight line joining them, will pass through the interior, which is not in this region R. And therefore, this shell is not convex either. So, that describes what connected means ((Refer Time: 25:14)), formally and also what convex means.
(Refer Slide Time: 25:17)
 
So, here is our claim. So, we claim that R of L is the set of instances for which leaf L is reached on execution. And then, we want to argue that R of L is convex. Well, actually we wanted to argue, that it is connected. But, we will in fact, argue that R of L is convex, which will assure that it is also connected. Let me pictorially remind you, what this R of L is...
(Refer Slide Time: 25:53)
 
So, here is our decision tree and this is leaf L. So, what is this region that I am talking about? Well, if I start with any instance x and suppose I follow this path and I reach L. Then, I will say that this instance belongs to R, if execution arrives at L. So, then it belongs to R of L in the region of L. So, let me first begin by giving some intuition. So, I said that x is a set of points, such that if I start from here and I get eventually I get to L.
So, what do I know about x? Well, each node that is visited has some condition associated with it. So, may be here the comparison is between i and j. And say this is the less than path, then these two together say that, this x must satisfy x i less than x j. May be this label over here is k l and say this is the equal to path. Then, this says that the condition satisfied must be x k equal to x l. So, if any instance gets to this level L, I know that all of these conditions along this path must be satisfied.
So, this is one way to characterize the region, the set of points which reach L during execution. But, notice that this characterization is geometric. So, this just says that the ith coordinate is smaller than the jth coordinate. So, this is naturally putting our region R of L into some parts in our unit cube. So, let us start with this root itself. So, which are the instances, which can visit the root, well at the root any instance will arrive.
So in fact, any x in the entire unit cube, will arrive at the root, will start at the root. So, instances visiting the root constitute the entire cube. What about instances visiting this node? So, this node is visited by those instances, for which x i is less than x j. Now, here is the key insight.
(Refer Slide Time: 28:44)
 
So, asserting that x i is less than x j is equivalent to saying that, if this is our unit cube. Now, I am looking at three dimensions and may be this is some x 1, this is x 2, this is x 3. And if I say, that x 3 is say less than x 1 what do I do? Well, I will look at this x 3 and I look at this x 1. And then I look at first, that portion where x 3 is equal to x 1. So, it is this, it is this plane let me just shade it.
So, it is this slice through the centre of that cub. That is, where x 3 is equal to x 1 and if I want x 3 to be smaller. Then, which side should I take then x 1. So, I want x 1 to be larger and x 3 top be smaller. So in fact, this is the entire region. So in fact, it is this region, the wedge shaped region that is facing us. So, the idea is this, the moment I assert a condition, I am going to slice my current set.
And I am going to take one part of it, if my assertion was something like x 3 equal to x 1 then I won not take one part, but I will take that slicing region itself. So, notice that I started off with the entire cube which is convex.
(Refer Slide Time: 30:18)
 
And the important point is that, whenever I go to a child as in this. I am going to shrink the set of instances, which visit this ((Refer Time: 30:28)). And when I shrink them, I will be shrinking them in a convex manner.
(Refer Slide Time: 30:36)
 
So, it is sort of I will take my region I will take a region, which is convex then I will slice of a part of it. But, this slicing operation maintains convexity. So, that is roughly the idea. So, even if I do it several times, the region that I am left with at the end is going to be a convex region. And therefore, also connected region, that is roughly the argument. So, now we are going to see it more formally ((Refer Time: 31:05)).
So, here is the proof, suppose x and y are two points in R in this region. So, I am going to consider three execution. In the first execution the instance is going to be x, what do I know about this? I know that L is reached by definition, by our assumption that x and y are points in R of L. So, when I finish this execution I know that L is reached.
In execution 2 I am going to start with y. What do I know about this? Well, I know that for this point as well L is reached. So, even for this point L is reached right, again that is because I said that y belongs to R of L which is nothing but saying if I do an execution with instance my instance equal to y I will reach that same leaf.
My third execution is the interesting execution. Here, I am going to start off with an instance which I will call z. Z let me remind you has n coordinates, just like x and y. So, I will call those z 1, z 2 all the way till z n. And I am going to set z i in a curious looking manner. Z i is going to be lambda times x i, where lambda is some positive number between 0 and 1. I will tell you more I will write that down in a minute, and lambda times x i plus 1 minus lambda times y i.
So, this is how each z i is going to be set. So, it will be some kind of an average of x i and y i, where the weights for x i and lambda. And the remaining weights come from y. So, if I take the case lambda equal to 0 what does it mean. So, if I take lambda is equal to 0, then this part goes away and I get y. If I get lambda equal to 1, this part goes away and I get x.
And if lambda is somewhere in between, what do I get? If lambda is equal to half, then I get half of this and half of this. And in fact, I get the midpoint of line segment x, y. If I take other values of lambda I will likewise get, points on the line segment joining x and y, the straight line segment line joining x and y. So, this is the key behind, this is the key part of the definition set. I am going to I have defined z, so that it happens to be on this straight line x, y.
And so long as I restrict lambda between 0 and 1, it will be in the interior of this line segment x, y. So, what do I have to prove in order to I give that, this R of L is convex. Well, the definition says that straight line path must lie inside of R of L. If I prove that then I am done, that is exactly what I am going to do. So, I am going to analyze this execution 3 and figure out what happens during execution.
So, let us start with root, the root label is i colon j. And suppose, the less than branch is taken in execution 1. I am just taking this as an example, the argument will really work for every possible branch. So, the less than branch is taken in this execution. In this execution, what do I know about this execution? Well, clearly the same branch will be taken in execution 2.
Why? Well, in execution 2 we reach final finally, the same leaf. And there is only one way to get at that leaf. And therefore, it had better be along the same path. So, even in this second execution, we are going to follow the same branch. What can we conclude from that? So, from the fact that in the first execution, this branch was taken. It clearly means, that x i and x j got compared and x i turned out to be less than x j. In the second execution y i and y j got compared. And y i turned out to be less than y j.
So, this is what we know, if we assume that the less than branch was taken in this execution. Now, I am going to multiply this by lambda and this by 1 minus lambda and I am going to add these two things. So, let us see what happens. So, I claim that I get this inequality, let us check that out. From the left hand side I am going to get from this inequality lambda times x i, which I have got over here. From this I will get 1 minus lambda times y i, this is what I have got over here.
On the right side I got lambda times x j from this inequality and from this inequality I got 1 minus lambda times y j. So, what we have now is this inequality. So, this is the inequality that we got, let me just complete this argument. So, to complete this argument what we have is, that this part is simply z i and this part is simply z j, so this is z i this is z j. So, we have concluded, that z i must be less than z j.
But, that is what we wanted, why? Because, if z i is less than z j, we know that the less than branch will also be taken in execution 3. So, for this first root we have proved that execution 3 will follow the same path, as that followed by execution 1 and execution 2. So, this argument can be made if instead of less ((Refer Time: 37:38)) than we took some of the other.
And we can make it at every node along that path. And so we can argue that finally, L is going to be reached. So, what we have argued is that, z will reach L z is any point on this line segment. And so if I start with any point on this line segment I reach L. And therefore, I have concluded that R of L is convex. So, I have proved this claim. So, let us now turn to the next claim.
So, this claim says that ((Refer Time: 38:20)) if I have a point or an instance x, whose all coordinates are distinct. And y is another non is a permutation on this, which is not exactly equal. Then, x and y must reach distinct leaves. Of course, they must reach yes leaves, because their coordinates are all different. But in fact the claim asserts that they must reach distinct yes leaves.
(Refer Slide Time 38:50)
 
So, you are going to prove this. So, x consists of distinct values, means the answer of x must be yes, y is some permutation sigma. So, the answer to y is also yes. We are going to prove this result by contradiction. So, we are going to assume that say x and y reach the same leaf L. Then, we know that every the region corresponding to every leaf L is a connected region.
If it is a connected region, then there has to exist a path p from x to y, which passes entirely through L. This is what we know from the previous claim. So, what we will show, that if you tell me that it is this path I will show, that there exists a point z on this path p, such that the answer to z is no. Now, this will be a contradiction, because z lies on P, P supposed to lie inside R of L.
And what we have argued is that, the answer is no whereas, the answer for L is supposed to be yes. So, this would be the contradiction. So, this is what we are going to do? Let me start with the sub claim. The sub claim says that there have to exist i and j. Such that, the ith coordinate of x is strictly less than the jth coordinate of x, whereas the ith coordinate of y is bigger than the jth coordinate of y. I will prove this in a minute. But, let me just examine it is implications.
So in fact, this is going to give us the proof almost immediately. So, let me define a function f in this space, where f for a point w is simply the difference between the ith and the jth coordinates. So, what is f of x, f of x is x i minus x j, but x i is smaller than x j. So, f of x is less than 0, what is f of y? Well, y i is bigger than y j. So, y i minus y j is bigger than 0 and f y is bigger than 0.
So, f of x is less than 0, f of y is bigger than 0, they are joined by this continuous path p. So, what happens is we move along this path from x to y. The path is continuous, this is a continuous function. And so, by the mean value theorem, there has to exist a point z on p such that f of z is 0. If f of z is equal to 0, what does it mean? This means that z i equal to z j, but z i equal to z j, then two coordinates are equal. That means, the answer to z is no.
The coordinates are not distinct. So, the answer is a no, but this supposed to be a point inside R of L. So, the answer had better been yes, so there is a contradiction. So, we have proved our basic claim. So, all that remains now is to prove this sub claim. So, let us prove that.
(Refer Slide Time 42:32)
 
So, the claim is that there exist i, j such that x i is less than x j and y i is less than y j. So, I am going to do this with an example to help you understand what is going on. So, here is my example. So, I am looking at say five dimensional space, this is my x, the coordinates are all points in the unit cube. This is why remember that y is just a permutation of this permutation sigma. And y is such that, it is not identical. So, the same numbers as x is repeated over here.
But, it is not repeated they are not repeated identically. Well, they can be identical at one place that is. Now, here is the key step, I am going to define a permutation pi which sorts x. So, pi of x then is going to simply take these and rearrange them in increasing order. So, it is going to 0.2, 0.5, 0.7 and so on. It may not be clear to you, why I am not defining this permutation pi. But, please bear with me, the answer will hit you in a minute.
So, I know what pi is I know how to take x and generate pi of x from it. I apply the same permutation on y as well. So, what happens well I look at the column, the column of x which moved over here. And I take the corresponding value for y and move it down over here. So, it is both are 0.2 over here, so, it is both are 0.2 over here. Then, 0.5 came from here. So, the value over here must also come over here 0.7 came from here. So, below it is 0.9, so over here also there is 0.9, then we have 0.8, 0.7 from this and 0.8, 0.9, 0.5 from here.
So, we have rearranged x and we have rearranged y. Now, be patient with me just for a minute, I am going to prove this claim. But, I am going to prove it for pi x and pi y, where it is easy to see. And you will see that, they can just trace it backwards to x and y. So, what is it mean to prove the claim for pi x and pi y? Well, we are supposed to find i j of this property.
So, because we sorted and here is now the reason for sorting it, if i is less than j we know that pi of x i has to be less than pi of x j, because this is sorted order. So, pi of x sub i the ith component of pi of x must be smaller than the jth component of pi of x. So, this is smaller than this, this is smaller than this and so on. So, long as I choose i smaller than j, this first property is guaranteed to me. What do I know about y? So, here is one sequence I change that and then permuted.
So, I know now that somewhere, this new sequence has to be non increasing. This was in increasing sequence, this is a permutation of it. So, somewhere this sequence has to be non increasing. So, let us say that there exist i j, such that i less than j and if it is non increasing. Then; that means, pi y must be greater than pi y j. But, notice that then we have found these two.
So, we have found i j such that, pi y is bigger than pi y j, whereas pi x i is bigger than pi x j. So, just to illustrate in this example. Here is the case, where this is smaller than this, but this is larger than this. Now, we have proved it for this pi x and pi y. How do we go back? Well, the idea is simply we take we figure out, where these columns came from our original example.
This column came from here and this column came from here. And sure enough, this number is less than this whereas, this number is greater than this. So, I will skip the algebra. But, this is exactly what is happening? We just have to follow back and then this property will hold nevertheless ((Refer Time: 46:40)). So, what have we proved, well we have proved this claim. And this was all that was needed to prove our original claim, which was this ((Refer Time: 46:48)).
So, once we prove this claim what do we know that if we have two distinct permutations of this. If we distinct permutation of this, then that must reach a different region. But, now I know that the number of distinct permutation can be n factorial. And therefore, the number of yes leaves has to be bigger than n factorial. And the time has to be bigger than the height of the tree, which is log of n factorial or at least n log n. So, this finishes the claim that the time for element distinctness is at least n log n on decision trees.
(Refer Slide Time 47:30)
 
Here is the quick summary of the argument. So, what we did here was, that we said that the instances visiting any yes leaf, former connected region in the instance space. No instances partition the instance space, such that distinct permutations are not in the same connected region. And therefore, we conclude that the number of yes leaves must be large.
What is the implication for the RAM model? Well, for the RAM model we can conclude exactly using ideas similar to last time, that the comparison based algorithms for element distinctness, must take time n log n.
(Refer Slide Time 48:13)
 
I want to quickly extend this to the case of algebraic decision trees. So, here is a quick definition. So, again the program in this model consists of trees with outgoing branches, less than equal to or greater than. But, this time we will have these three relational operators, just for simplicity, although you can have other operators too. The node labels are no longer i colon j, but they are algebraic expressions, over the input the components of the input. So, x 1 square plus x 2 square minus 25.
The action is we are going to evaluate the label expression. And we are going to compare it to 0. So, this expression is equal to 0, then we will choose the equal to branch. If this is less than 0, then we will choose the less than branch. So, let me just take a quick example. So, if our expression is x 1 square plus x 2 square is 25 or minus 25.
(Refer Slide Time: 49:18)
 
Then, the condition x 1 square plus x 2 square minus 25 is less than 0. Simply means, that our point lies inside this. So, we are restricting our point to be inside this region. If the expression is linear instead of this, which is a quadratic. Then, our previous results actually hold. Unfortunately, if the expression is non-linear ((Refer Time: 49:50)), then the intersection of constraints can produce disconnected regions, which cannot happen if things are linear.
(Refer Slide Time: 49:57)
 
The main result is something like this, this is a deep result actually from algebraic geometry. So, we have a decision problem over inputs x 1 to x n, by a decision problem we simply mean, that we mean a problem whose answer is yes or no, just like element distinctness. Suppose A is a algebraic decision tree algorithm for the problem. Such that, the degree of each algebraic expression is some fixed constant d. So, x 1 square plus x 2 square would be degree 2.
Suppose, the no answers partition the instance space into w connected regions, within each of which the answer is yes. So, in our case for simple decision trees, this w for the element distinctness was n log n. But, in general the time required is going to be by this algorithm is going to be omega of log of W minus n. Now, you might wonder what happened to that d. So, this d actually appears inside this omega. So, there is a constant of proportionality, which depends upon d.
For decision trees the time is log of W, not even omega it is actually just log of W, a log of W to the base 2. So, what this theorem says, that the complicated algebraic model does not really help all that much. So, the lower bound that we get is almost as good. Well, it is a little bit smaller, because of this minus n and may be the proportionality constant is a little bit different, but it is essentially the same lower bound.
For element distinctness in fact, there is no change, the algebraic tree for any fixed degree will give us n log n as before. The proof idea is actually pretty difficult. Now, as we said a single leaf can correspond to a small number of connected regions, not just exactly one connected region.
So, now we have to get some heavy duty machinery from algebraic geometry to count the number of connected regions. When that you get when you take intersection of several constraints. If the constraints are linear, then it is very simple. If the constraints are high degree algebraic expressions, then this becomes rather complicated.
(Refer Slide Time 52:30)
 
So, quickly summarize lower bound theory, that we have been looking at in the last two lectures, tells us when to start searching for better algorithms. This is very good, because it is good to know that you are done. Another interesting point of this theory is that it has some connections, with some really deep mathematics. Algebraic geometry is supposed to be rather deep area of mathematics. And it has connections to many other fields also.
Here is a very simple context, in which this idea can be used. So, I will leave it as a problem for you. Suppose, you have 27 coins, such that 26 have equal weight. And one is heavier, find the heavier using 3 weighings. This is probably we have puzzle that we have solved. Now, I want you to use the ideas expressed in this lecture to formulate a decision tree model using which, you should be able to argue, that you cannot do this in fewer than 3 weighings.
Thank you.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 26
NP - Completeness - I
Motivation
Introduction to Reductions

(Refer Slide Time: 00:51)
 
Deals with the notion of NP completeness, we will study this notion and we will see how to use this notion effectively. The first thing I will sort of talk about is motivation. Hopefully at the end of this I will motivate you sufficiently that, you will be interested enough to study about the subject. So, imagine that you finished your B Tech. You have the all new subjects well, and then you gone for a interview, you land your dream job. The company pays well, you are really happy, you settled in software job.
You very happy, because it is you have to deal with designing things. And your boss is also happy with you and he wants to give you challenging projects. The first project you get is the following it scheduling jobs on computers. So, the input is set of jobs and each job as a size associated with it. So, let us say the jobs are j 1, j 2 up to j n and the sizes are s 1, s 2 and so on to s n.
These sizes could vary, they could some of the jobs could have the same size, some could have different. You can think of them as positive integers. Now, there are two processors. So, there are two processors p 1 and p 2 both are identical. And your job is to schedule, these jobs on these two processors.
So, you want to automate your boss wants you to automate this process. You have to write a program, which takes input these sizes s 1, s 2 up to s n. So, let us say a array of size n or reads it of a screen. And you have to schedule these jobs on processors p 1 and p 2. Now, the scheduling should be such, that the last job finished is fastest. So, what do we mean by this?
Supposing, I schedule all of them in p 1. And the time taken will be s 1 plus s 2 plus s 3 and so on up to s n. Let us say that job of size s i takes the same time, time s i to finish if I schedule it, it takes time s i. So, also assume that there is preemption. Once, you start a job of it has to run for completion. So, if I run all of them on one processor, the time taken is s 1 plus s 2 plus s 3 up to s n.
So, at time s 1 plus s 2 up to s 3 and so on up to s n, all jobs must finish. Now, supposing I schedule, the even jobs on p 2 and odd jobs on p 1. Let us say s 1, s 3 and so on here it is s 2, s 4. Let us say there are seven jobs s 5, s 7, s 6. So, this is how the jobs are scheduled. Now, the time taken on p 1 is s 1 plus s 3 plus s 5 plus s 7, time on p 2 is s 2 plus s 4 plus s 6.
Now, it may so happen that these are very large jobs. Let us say, these sizes are 1 and these are 100. Then, the time at which the last job completes is 300. This process finishes of in four units of time, while this takes 300 units of time. So, the last job finishes after 300 units of time, this is not what we want. So, you want to sort of distribute the jobs evenly among these two processors.
So, you want to distribute the jobs in such a way, that these large jobs say one of the s 4 should be here. And may be s 5 should be here and so on, we want to distribute as evenly as possible. So, you think about this for some time and here is the first idea, we come up with.
(Refer Slide Time: 06:07)
  
So, you take the jobs in order. So, the jobs are of sizes s 1, s 2 and so on with the first job on the first process it is p 1 I put s 1 here. The second job goes to p 2. Now, you put when I take the third one, you put it on the processor, which is least lightly loaded. For instance, if s 1 is greater than s 2, then I put s 3 here. And you keep doing this, I look at s 4 and I see which one is greater.
Assume that s 2 and s 3 are on p 2, check whether s 1 is larger than s 2 plus s 3 or is s 2 plus s 3 larger than s 1. If s 2 plus s 3 is larger than s 1 then I put s 4 on p 1. So, s 4 will go here, if s 2 plus s 3 is larger than s 1, you can break ties as you want. This is the first algorithm you come up with. You code this and it is starts working your boss is happy. But, the next day the boss shows up and tells you, that this does not work.
So, he gives you an input on which you produce the schedule, which takes some time T. So, you take some time T while the boss produces the schedule which is less than T. So, the optimal schedule on the input is smaller, the total time the optimal takes is smaller than the time that your algorithm produced. And the boss says fix this. So, at this point I would like to give you an exercise, what give an example to show that, this algorithm does not produce a optimal schedule.
So, give an example to show that this algorithm does not produce the optimal schedule. Well, you go back and figure out, there is something wrong with the algorithm in the sense, that if these jobs are mixed up. If the sizes are mixed up arbitrarily, may be that is the reason the algorithm does not work.
(Refer Slide Time 09:10)
 
So, here is your next attempt, the first thing you do is you sort the jobs by size. Now, you know that s 1 is less than s 2 less than s 3 and so on. You know that, the jobs are increasing order of size. And now you do the same algorithm, you put s 1 on the first processor, s 2 on the second and so on. Well, this seems to work, but two days later again your boss shows up with an example, which does not work.
So, let us look at this example. The example has five jobs and the sizes are as follows 2, 2, 2, 3 and 3, let us see what your algorithm does. So, here are my two processes p 1 and p 2, these are the job sizes. So, the first job goes on to p 1, the second job is on p 2, the third job you could put either on p 1 or p 2, let us say we pick p 1. The next one goes here and well 4 is smaller than 5. So, you put the last one here.
Now, the total time that your schedule takes is 3 plus 2 plus 2 which is 7. On this processor of course, you take 5 units on this you take 7 units. So, the total time to finish all jobs is 7 units. Well, you can see that this is not optimum, the best way to do this is I guess you have seen them by now, you put the 2s on the one processor and the 3s on the other. Now, there are only 2 3s. So, this takes 6 units and that takes 6 units every job has finished in 6 units of time.
So, this ((Refer Time: 11:19)) that you will come up with also does not work. Let me again repeat the algorithm, you sort the jobs by size. Now, you look at these jobs one by one. And assign these processes greedily, in the sense that, you put the first one in the first process, the second one in the second. And when I reach the ith job, you put it on the least lightly loaded processor.
You sum up the previous sizes, which are on each processor, choose the smaller one and put this new job on that processor. This was the algorithm we came up with and this does not work, here is an example that it does not work. So, now what do you do? Well, you can go back and try fix this example. When, you come up with another heuristic, which beats this example.
But, I can tell you that it is extremely difficult possibly impossible to come up with a smart heuristic like this, which does well for all inputs, which always does well. So, you after trying various heuristics most of them do not work. You are now at the end of your patience, you are really worried about what your job, because boss is now threatening you with dismissal. So, then what you do is this. So, you now sort of want an algorithm that works always. So, what you do is a brute force algorithm.
(Refer Slide Time 13:11)
 
So, you take every subset of the jobs. So, I have jobs, I have sizes s 1, s 2 up to s n. So, take all subsets ((Refer Time: 13:30)). So, compute the size of s and size of S bar. S bar is the complement of S. So, all jobs not is S which is S bar. So, essentially we want to put s on process on p 1 and the complement on the processor p 2. So, you do this and look at the maximum of these two and choose the maximum of these two.
So, this will tell you the time, the total time that this schedule took. So, do this for every possible subset and pick the minimum. Take all subsets compute these choose the maximum and of these maximum pick the minimum of these. You can easily see that this will always give you the right answer. Because, you will look at all possible ways of scheduling, these jobs on two processors and one of them has to be the minimum.
Now, this is fine, this algorithm works, your boss tries out a few inputs and it is all it works for all most all for... In fact, for all inputs till your boss feeds in an input with let us say 1000 numbers. There are 1000 jobs, which have to be read to these processors these two processors. So, we give the input as a list of size 1000.
Now, you start running the algorithm 5 minutes pass an hour passes, 2 hours, 10 hours, 1 day the algorithm does not stop, 2 days, one week. And now your boss is starting to get worried. Here is an algorithm which is got for all small inputs and now it is weak and the algorithm does not stop for an input of size 1000. Well I can tell you that this is not going to stop even after many years.
You can take years and years and this is not going to stop, in my lifetime not your lifetime may be many lifetimes. That is a problem with such brute force approaches. Let us do a quick calculation to see how much time your algorithm is going to take on inputs of size 1000.
(Refer Slide Time 16:30)
 
If you have 1000 jobs, the number of subsets of jobs that you look at is to be 1000. So, the number of subsets of jobs is 2 to the 1000. Now, let us assume that in one instruction a computer can process one of these subsets. Let us see, how much time computers will take to process all of these subsets. Now, the fastest computer runs, let me make an estimate. Let us say 10 to the 20 instructions per second.
This is the fastest computer. On the fastest, this is a big over estimate it is much smaller than this, but we will assume this, how many computers you think there are in world 10 to the 20. Let us say 10 to the 20, it is again an overestimate computers in the world. What we are going to do is put all the computers in the world, at your disposal to solve you know to run the your brute force algorithm.
So, putting these two together, then I have 10 to the 40 instructions I can do instructions per second, using all computers. So, I am using all the computers in the world and I can do 10 to the 40 instructions per second. So, how many instructions do you think, one can do in a year, let us do this calculation. So, it is 10 to the 40 instructions per second. So, in an hour I can do this time 60 in a minute I can do this time 60 another 60 for an hour. Then, I have 24 hours a day.
Then, I have 365 days in a year. So, these many instructions per year. Let me do a overestimate here, this I am going to say 10 to the 2, this let us say this is 10 to the 2. Well, 10 to the 2 since we are generous, this is 10 to the 3. So, number of instructions per year is certainly not more than 2, 4, 6, 10, 10 to the 50 good. So, I have 2 to the 1000 subsets to look at. I can look at at most 10 to the 50 subsets per year. Let us see how many years this takes. So, let me quickly.
(Refer Slide Time: 19:53)
 
So, you have 2 to the 1000 instructions and using all the computers in the world the fastest speeds. You have 10 to the 50 instructions, that you can do per year. Now, this 10 to the 50 is roughly 2 to the 10 is ((Refer Time: 20:21)) let us say is certainly less than equal to 2 to the 4 times 50. That is 2 to the 50 for the 200. So, now you can I just divide this by that to check, how many years I am going to take.
So, the number of years you going to take is 2 to the 1000 minus 200, this is roughly 800 years. You can check, how many particles there are in the universe. This is not very far away from that number of atoms in the universe per year. So, you are going to take a lot of years for this algorithm to finish. This is certainly not what you wanted to do. If the boss figures this out, you certainly fired.
In fact, even if he does not figure this out you are fired. So, what do we do? So, this is a motivation for studying NP completeness. I will give you a recipe, using which you can hopefully save your job. If your boss is going to be a little bit intelligent. So, the first part of the course, which you did design of algorithms, was to help you get a job. The second part that we do theory of NP completeness will help you quickly.
So, our final objective will be to show that, this problem is among hard problems in this world, what do I mean by hard? It means that, so far nobody else has been able to find the solution to this problem. Well, the boss can say I mean this problem, just come up with our company. You know other companies handle other problems, why are you saying that, this is not been solved before.
Well you can in fact, say that if somebody manages to solve this problem. Then, a lot of other problems, which will lot of other people have not been able to solve can also be solved. So, this is the kind of statement we would like to make. So, before we formally look at this notion of NP completeness. We will lead a few other Fonda's, few other notions that we need to imbibe. The first one is the concept of reduction.
(Refer Slide Time: 22:59)
 
So, we will first look at an example and we will see what reduction really means. So, reduction really means this, supposing you are having a library. So, you have a library where large number of problems have been very well coded by very good programmer. Now, given a new thing that you want to code, it would be very nice if we can use sub routines from the library in your code. Rather than, re invent the wheel and try and do all this by yourself.
This is all that is to reduction. How to efficiently use code for other algorithms to generate new algorithms. So, let us look at an example. This is not very easy example, it is not very hard either. So, before we start a few definitions, which we will recall hopefully you have seen this before. The first thing is notion of a matching, in a graph G equals V E, V is the vertex set, E is the edge set. A matching in a graph is a subset of the edge set is a subset of E. So, let me call this M, the subset M of E, such that no two edges share the same vertex. Such that, let me write this down no two edges in M have the same end point, let us look at an example.
(Refer Slide Time: 25:21)
 
So, let us take a graph, let us say this is a graph. Then, here is a matching, the ticked edges form a matching. If I look at these two edges, they do not share a end point. The end points of these edges are these two, the end point of this edge is these two they do not share any end point. For instance, let me label this a, b, c, d, e. if I take a, b and a, d these do not form a matching, this is not a matching. Essentially one set of edges. So, that they look like this in a graph, they do not share an environment, this is what matching is... The next definition we need is that of a perfect matching.
(Refer Slide Time 26:40)
 
We will say that a matching is perfect, if all vertices in the graph are end points of at least one edge. This is a matching such that, all end points. A perfect matching is a matching, such that all vertices are end points of at least exactly one edge in M. So, it must be matching M such that all vertices are end points of exactly one edge. So, the size of a perfect matching is just half the size of the vertex set. So, then note that size of M is size of V by 2.
So, this is a perfect matching, let us go back to this old graph of ours, do you think that this graph has a perfect matching ((Refer Time: 28:30)). The answer is no, well there are five vertices. So, if I take a matching with two edges I can cover four of these vertices, I cannot cover five. And I cannot take three edges, because then two of these edges will share an end point. So, this graph does not have a perfect matching, but for instance if I change this graph slightly.
(Refer Slide Time: 29:04)
 
Let us see I have a, b, d, e, c let me add one more, let us say f. Now, this graph has a perfect matching. In fact, it has many at least two perfect matching's, may be two perfect matching's, here is I take these three edges a, b, e, d and c, f is a perfect matching a d, e b and c f is also a perfect matching. It ticks for one perfect matching and the crosses form another perfect matching, this graph ((Refer Time: 29:55)). So, now that we have defined these two problems. Here is a problem that we would like to solve. So, there are two problems.
(Refer Slide Time 30:20)
 
So, here is let me say one is problem perfect match PM is for perfect matching. The input is a graph G and the question you ask, does G have a perfect matching. This is your first problem, the input is a graph G does G has a perfect matching. The other problem we are going to look at this, it is very closely related to the first one. So, this is maximum matching, the input is the same, input is a graph G. The output I need is a matching of maximum size.
So, let us look at these two problems. For this problem I only need to say yes or no. Ii just want to say whether the graph is perfect matching or not. We actually need an output, which is I need to know the I need edges, which will represent in some matching of maximum size. We have seen that a matching a graph could have more than one matching's of maximum size, any one of them will do.
Now, supposing in your sub routine library, you know you have sub routine coded for this for problem maximum matching which. So, given you have an algorithm, so that when you feed in a graph G, it will output a matching of maximum size. Can we use this to solve all this problem, the answer is yes. Just feed in the same graph G, look at the matching.
If it has V by 2 edges, V is the number of vertices. If this has V by 2 edges, then the graph has a perfect matching. If it is less than V by 2, then the graph does not have a perfect matching. So, given a algorithm for this second problem, very easy to construct one for the first problem. How about the other way around which is given.
Supposing, this sub routine library has an algorithm for this, which is given in graph G, it will tell you whether it has a perfect matching or not. And now, you want to output matching of maximum size. Your job is to construct an algorithm, which does this. And you would like to use this algorithm, effectively. Because, this is a very good implementation and you would like to use this effectively. Let us see how this is done. So, I hope the objective is clear, let me write this down.
(Refer Slide Time: 34:00)
 
So, we are given an algorithm for a perfect matching. And we want to design one for max matching. This is what we want to do? Well, how do we really use this algorithm, that is the crucial question? In fact, let us take something which is sort of intermediate between these two, how about time to find the size of the maximum matching. We will look at this problem maximum matching ((Refer Time: 34:54)).
The output should be edges, set of edges in a matching of maximum set of size. That is what the output should be. Now, supposing we only want to know the size of the maximum matching in the graph, can we solve this let me write this problem down and will try and do this. So, this is objective 1 and intermediate objective is this design and algorithm to find the size of a maximum matching in G. So, we are given an algorithm for perfect matching, you want to design an algorithm to find the size of the maximum matching in G, this is how you do it. So, you take this graph G.
(Refer Slide Time 36:14)
 
So, here is your algorithm that does perfect matching. Now, you first feed in G. And ask whether it has perfect matching? If G has a perfect matching, then you know the size of the maximum matching is hopefully. It is if G has a perfect matching, then the size is V by 2, where V is the number of vertices in the graph. So, let me write this down, if G has a perfect matching, then the size of maximum matching I is a V by 2.
So, supposing, it says that G does not have a perfect matching now what you do? Well, what you do is this you take here is G. So, step 2, so you take G add a vertex, this is a new vertex connected to all vertices in G. Now, call this graph G 1, now you feed G 1 into this algorithm for perfect matching. You have taken G, you added vertex, which is adjacent to every other vertex in G. And now you feed this into this perfect matching. Again two things can happen, either it can come back and say it has perfect matching or it can say no. Let us see what happens in both cases.
(Refer Slide Time 38:48)
 
So, G 1 is G plus a vertex connected to all vertices in G, that is what you want. Now, supposing G 1 has a perfect matching. Then, what can you say about G. Well, if you look at a perfect matching in G 1, this vertex this extra vertex is will in the matching, it will be connected to something in G. This is the picture here is G, here is the extra vertex and I just going to draw the edges in the matching.
So, this fellow will be connected to something here in this matching. The other vertices in G are all match somewhere over the other. This is if G 1 has a perfect matching, then this is the figure, this is how it looks like, which means except for this vertex. If I remove this vertex out, the rest of the graph is a perfect matching, which means there is a matching of size, this implies that G has a matching of size V minus 1 by 2.
And this is the size of the maximum matching really. We know that, because V did not have a perfect matching, it does not have a matching of size V by 2. On the other hand, it has a matching of size V minus 1 by 2. This is if G 1 has a perfect matching, what if G 1 does not have a perfect matching. If G 1 does not have a perfect matching, then we know that G does not have a matching of this type. So, then we create graph G 2.
(Refer Slide Time: 41:05)
 
So, how do we create G 2 well, similar you take G 1 plus a vertex connected to all vertices in G 1. Now, you ask G 2 has appropriate matching and so on. Then, you form G 3 and ask whether G 3 has a perfect match. And you stop as soon as the perfect matching fellows says yes. So, let me write the generic step. So, you get G i the graph G i minus 1 plus a vertex connected to all vertices in G i minus 1.
Well, this is how you get G i. So, now you look at G 1, G 2, G 3 and so on. And ask whether G 1 has a perfect matching, G 2 has a perfect matching and so on. So, let us say k is the first index, where the perfect matching algorithm says yes. So, which means...
(Refer Slide Time: 42:32)
 
So, assume that G k minus 1 does not have a perfect matching and G k has a perfect matching. Now, what is G k minus 1 and G k look like. So, here is my original graph G for G 1 I added one vertex with this connected every single region this is G 1, G 2 I added two vertices. Now, this is connected to this and also connected to everything and so on. So, G k minus 1 I have k minus vertices here, they are connected through each other and they are also connected to everything in G.
Similarly, this is G k minus 1 and G k looks like this I have the graph G and k vertices here. These are connected to each other and they are also connected to each other k vertices which are connected to everything each other. Now, this does not have a perfect matching and this has a perfect matching, what does this mean. Now, let us look at the perfect matching here.
Now, actually I must say that G 1, G 2 all the way up to G k minus 1 does not have a perfect matching. So, k was the first time that we encountered a graph with perfect matching G 1, G 2, G 3 up to G k minus 1 and does not have one and G k has one. So, let us look at G k. So, what can a perfect matching look like, can it have a edge like this, the answer is no.
Why is that, supposing it has an edge like this in the matching, you remove these two and this graph now looks like G k minus 2 and this as a perfect matching. But, we know that G k minus 2 does not have. So, all edges in the matching must be from vertices here to vertices inside G. So, here is the picture here is G.
(Refer Slide Time 45:07)
 
So, this is G k these are k vertices, these k vertices are matched with some k vertices in G. There are n minus k other vertices in G, which are matched amongst themselves. So, there is a matching inside G of size n minus k by 2. So, there is matching in G of size n minus k by 2. And we would like to observe that, there is no smaller, there is no matching of size larger than n minus k by 2. This is the largest matching that you can have, now why is that so.
Supposing, first this has to be even m minus k has to be even, because this is an integer right. So, if there is a matching of size larger than this. That means, so suppose there is matching of size, let us say n minus k plus 2 by 2 which is nothing but, n minus k by 2 plus 1 one more edge. So, supposing there is a matching of this size. So, the claim is that one of the previous graph. In fact, then G of k minus 2 has a perfect matching. So, this will be a contradiction. Supposing there is matching of size larger than this, then G k minus 2 will have a perfect matching. So, we will see this.
(Refer Slide Time 47:26)
 
So, here is G and here is a subset which is of size n minus k by 2 plus 1. So, here I have n minus these many which is... So, n minus n by 2 plus k by 2, that is this size, which is n by 2 minus and minus plus k by 2 minus 1.
(Refer Slide Time 48:20)
 
So, let us look at this let me just refresh ((Refer Time: 48:24)) we are here supposing there is a matching of size n minus k plus 2 by 2. And we have to show that G k minus 2 has a perfect matching. So, this is G, so it has a matching of size n minus k by 2 plus 1. So, the number of vertices in the matching is twice this, so the size, so number of vertices here is twice this, which is n minus k plus 2.
So, the number of vertices here is n minus, so which is k minus 2. This portion has a perfect matching, which was our assumption. So, let us look at G k minus 2. So, G k minus 2 had G, this is G this in fact, is G. And I had k minus 2 other vertices, which were connected to everything in G, and also amongst themselves, if this was G k minus 1.
What I do is this, I take this matching in this portion. As if and each of these k minus 2 vertices I match to the other size. You can see that, this gives perfect matching. Matching on this portion remains the same as in this portion here. And these k minus 2 vertices I match to the other side. So, this shows that G k minus 2 in fact, has a perfect matching ((Refer Time: 50:20)).
So, let us recap we were given a algorithm for perfect matching. And the objective the intermediate objective was to design a algorithm to find the size of the maximum matching in G. And what we do is this? ((Refer Time: 50:38)). We define graphs G i, where G i's G i minus 1 plus the vertex connected to all vertices in G i minus 1 ((Refer Time: 50:49)).
We find the first graph we look at G 1, G 2 and so on. And we find the first graph G k which has a perfect matching, which means G 1, G 2, G 3 and so on up to G k minus 1, they do not have a perfect matching. G k has a perfect matching ((Refer Time: 51:05)). Then, we claim that there is a matching in G of size n minus k by 2 and no larger.
So, the size is the largest matching is n minus k by 2. This shows that given an algorithm for finding the perfect matching. I can design an algorithm for finding, the matching of maximum size in a graph by calling the other algorithm repeatedly I do not call it too many times. I call it at most k times here. You can actually do it much faster, you can call it faster than k, so in the following sense.
This k can it can go down all the way up to 4, 5 up to a constant, which means you can call it about n times. So, we have shown that given an algorithm to find the perfect matching in a graph. I can design an algorithm that finds the size of the maximum matching.
(Refer Slide Time: 52:19)
 
The way you do it is you look at graphs G, G 1, G 2, and so on. Find the first graph G k which has a perfect matching. And the size of the maximum matching, then is n minus k by 2. If k is 0, it is G and the size of the maximum I think n by 2 and so on. If G k is the first time you get a perfect matching, the size is n minus k by 2. In the worst case, you would call the perfect matching algorithm n times. I would call G 1, G 2, G 3 and I can go all the way up to n.
Now, there is a smarter way to do this using binary search, which I will let you do, which let you figure out this k, using only log n calls to the algorithm for perfect matching. It resembles binary search and is an exercise for you. ((Refer Time: 53:17)) essentially it shows that by calling this algorithm at most n times I am able to determine at least the size of the maximum matching in the graph.
But, let us go back now, the objective we had was to find this perfect matching. So, here now I have full filled this intermediate objective ((Refer Time: 53:39)). I can design an algorithm to find the size of the maximum matching, I want to actually find the edges in the maximum matching how do I do this. Now, here is the trick, you take this graph G.
(Refer Slide Time: 54:01)
 
So, I take G I find the size of the perfect, size of the maximum matching. So, let us say the size is l, the size of a maximum matching. Now, what I do is, pick any edge in G and remove it, remove a edge from G. And now ask the same question, what the size of the maximum matching? If it remains l, then I throw away this edge and I concentrate on the graph at remains. If it is not, if it decreases then I put this edge back. This edge is part of the maximum matching.
So, I leave this edge I look at the edges e 1, e 2 let us say up to e m 1 by 1. When, I look at a edge I remove it and I ask, if the resultant graph has a matching of size l. I find the size of the maximum matching the resultant has. If it is l, then I throw away this edge. Otherwise, I put this edge back by the time I am through with all these edges. But, what I will be left with is a graph which will be just a matching. It will just be a matching of size l. Let us do an example. So, let us take a simple graph.
(Refer Slide Time: 55:43)
 
So, here so let us take a simple graph e 1, e 2, e 3. The size of maximum matching here is 2 I remove e 1 and I ask what is the size? Now, the size is fallen by one the size is one. So, I put e 1 back I remove e 2, I ask what is the size? Size is 2. So, I throw away it, at this point the graph looks like this, because I have thrown away e 2. Then, I ask for e three I remove e 3 and the size falls by one. So, I put e 3 back. So, I am left with e 1 and e 3 and when I look at the graph that remains is the matching.
In fact, it will be a matching of size l, apart from the matching of size l. If there is any other edge, you know when you remove this edge, the graph will still have a matching of size l. So, you will throw this edge away. So, at the end of this procedure, what you will be left with is the matching of size l, which is what you are looking for.
So, this completes our objective which we started out with. We were given a algorithm for perfect matching. And we wanted to design an algorithm for maximum matching. What we did first was we designed algorithm ((Refer Time: 57:04)) to find the size of maximum matching. And using this we were able to design one for maximum matching. So, we had this in our sub routine library, we added this to the library by using this algorithm. And once we had this in the library it was easy to design a algorithm here. We started with this and we ended.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institution of Technology, Bombay

Lecture - 27
NP-Completeness ? 2

(Refer Slide Time: 01:48) 
 
Last time we looked at the concept of reduction. So let me just quickly review, what we mean by reduction? So, reduction means this given an efficient algorithm for a problem pi 1. So, you are given a efficient algorithm for problem pi 1. Then using this, we design an efficient algorithm for a different problem. So, we essentially showed that this implies that there exists an efficient algorithm for problem pi 2. So, given that there exists an efficient algorithm for problem pi 1.
So, one problem we show that there is an efficient algorithm for problem pi 2. The way we do it is imagine that this pi 1 somebody has coded for pi 1 an algorithm for pi 1 and it exists in some library. Then, we use this to design an algorithm for problem pi 2. So, this algorithm that we design it takes as input for problem pi 2. It perhaps does something to the input and then calls this sub routine for an algorithm pi 1 may be once may be twice repeatedly.
And at the end of this it outputs an answer and this answer is for problem pi 2. So, essentially we have solved problem pi 2 assuming that somebody has given a solution for problem pi 1. The two problems, we looked at last time was pi 1 was given a graph. Does, there exist a perfect matching in the graph that was pi 1. And the algorithm, we designed was given a graph output a matching of maximum size that was pi 2.
And we did in fact, repeatedly call this pi 1 after modifying the inputs likely. So, why is this called the reduction? It is called the reduction in the following sense that you actually want to call, you actually want to solve a problem pi 2. And you have somehow reduced this problem solving the problem pi 1.
In the sense that you can solve the problem pi 1, you can solve problem pi 2. So, you have reduced the problem pi 2 to a problem pi 1. And that is how the word reduction comes. But, the essential method is this given an efficient problem algorithm for one problem find an efficient algorithm for another problem. And you have to use the algorithm for the problem given to construct this efficient algorithm. So, this was a concept of reduction.
We will in fact, see one more example of this very shortly. Before, we see an example here is a word which seems a bit new which is efficient. Problems have been defined, we have seen many problems, algorithms have been defined. We have seen many algorithms well what does this efficient means.
So, let me define what I mean by efficient? We had seen earlier a solution to one of these scheduling problems, where the time taken was just too much? That was a inefficient algorithm. An efficient algorithm, we would like to be something that on reasonable size inputs finishes in reasonable amount of time.
(Refer Slide Time: 05:03)
 
So, let me tell you algorithms which are not efficient to start with. So, these are usually brute force algorithms are not efficient. So, what do you mean by brute force algorithms? Brute force algorithms are algorithms that look at all possible solution sets. There may be many possible solutions given your input there could be many possible solutions. And you want to pick one which is good or the best.
So, the brute force algorithm would look at all possible ways of doing this and pick the best one. For instance, if you want to look for a matching of maximum size a brute force algorithm would look at all possible collection of edges. All possible subsets of edges, then check whether each subset is a matching and also the size. And from this you can certainly find out a matching of maximum size. But this algorithm takes too much time.
So, if there are m edges then you take 2 to the m, you have to look at 2 to the m subsets and we have seen that this is just too much. So, these usually search exhaustively through the entire solution space. And if the input is of length n that is the other sort of characteristics of this brute force algorithm. The input size is n typical times taken by these brute force algorithms is 2 to the n, because we have just looked at all subsets. Typical times are 2 to the n and this is just too much.
And we saw that you design such algorithms you may get fired. So, these are not the algorithms that we are looking for these are not efficient, what do I mean by that. So, by efficient I mean that the running time of the algorithm is bounded by the polynomial in the input size.
(Refer Slide Time: 07:38)
 
Let me write this down for us this would mean this, the running time of the algorithm is bounded by a polynomial in the input size. So let me restate this again, what do I mean by polynomial? The running time which is bounded by let us say n to the constant. So let me, just state this again which means I am just going to restate this. There exists a constant c such that the running time T of n is big O of n to the c, where n is the input size.
So this is, what efficient for us will mean? Most of the algorithms, we studied so far are efficient in this way. For instance sorting, we can do in n log n times. That certainly bounded by a polynomial n square bounded by n square. The other algorithms which you have studied in your divide and conquer or dynamic programming they are all bounded by a polynomial in the input size n square n cube. Shortest path finding minimum spanning trees all most any algorithm that you have studied so far.
They are all their running times are all bounded by a polynomial in the input size. And these are the algorithms we have studied so far are efficient, why polynomial, why is the notion of efficiency, why not some other function of n.
(Refer Slide Time: 10:19)
 
So, the reason is this, if you compare n to the c if you compare n to the c. And let us say 2 to the n. So, these are 2 to the n is what brute force algorithm would take and n to the c is what our notion of efficiency is will be. If you look at these two functions then 2 to the n grows just much faster than n to the c. So, even for inputs of size 1000 square and 2 to the 1000 are just two different things.
If you have something which is let us say n square then if you program an algorithm, which runs in time 1000 square that will take a few minutes to complete. If your algorithm takes 2 to the 1000. Well, it is not going to stop at least in our life times may be much more. So that is why, this motion of efficiency has been is prevalent.
The other sort of reason is the in practice usually when the problem has an algorithm which runs an polynomial in input size. So usually in practice, if there is a constant, so that you can bound the running time by n to the c. This c happens to be very small 2 or 3, there are very few algorithms, whose running times are more than n cube.
If there is a polynomial time algorithm for this problem which means the running time is bounded by a polynomial. Then, using this polynomial is small like n n square n cube log n. So in practice, if there is such a c, usually less than equal to 3, so that is why one uses this notion of efficiency and this is the notion that we will use for the rest of this course
((Refer Time 1:48)). We said that if there is an efficient algorithm for problem p i 1, there is an efficient algorithm for problem pi 2.
(Refer Slide Time 12:54) 
 
So, let us again look at the matching example that we did pi 1. So, we said that lets assume that there is an efficient algorithm for perfect matching. So this, we said implied an algorithm for finding the size of the maximum matching and this implied an algorithm for finding a maximum matching.
This is what we saw last time? Now even though this algorithm, it is efficient. It is not necessary that when you do this transformation. The new algorithm is efficient. In this case, it actually is true the reason is this algorithm is called at most n times. So, the running time were let us say n to the c this was called at most n times running time here is roughly n to the c plus 1. And this called this at most n times, so this turns out to be n to the c plus 2.
So, if there is a c if there is a constant c. So that this running time is bounded by n to the c. There is some other c possibly may be c plus 2 2 c. So, that the new algorithm runs in time n to the c. So that is what I mean by saying that if there is an efficient algorithm for problem pi 1. There is a efficient algorithm for problem pi 2.
(Refer Slide Time: 14:54)
 
For our next problem I need to define a couple of terms. These are concepts of Hamiltonian path in Hamiltonian cycle some of you have seen this before, but let me define this anyway. So, a Hamiltonian path in a graph G is a path of length n minus 1, where n is the number of vertices. So, path of length n minus 1, where n is the number of vertices. In other words it is a path which spans all vertices.
Other words, it is a path that contains all vertices. So, it must be a path in the graph and all vertices in the graph must be present. Clearly, the length is n minus 1, the number of edges which is length n minus 1, this is Hamiltonian path. The Hamiltonian cycle is very similar; it is a cycle which spans all vertices one single cycle which spans all vertices.
(Refer Slide Time: 16:30)
 
Let me write this down to a Hamiltonian cycle in a graph is a cycle which spans all vertices. So, let us look at an example, so supposing this is a graph. Now, here is a Hamiltonian cycle in the graph, this edge, this edge, that edge this edge. So, this is a cycle which has all the vertices in the graph. And if I remove any one of these edges it gives a Hamiltonian path in the graph.
Any one of these edges in the Hamiltonian cycle, this gives a Hamiltonian path in the graph. So, if a graph has a Hamiltonian cycle, it has Hamiltonian path. The reverse of course may not be true. So for instance, the graph is just a path. If the graph is this path then it has a Hamiltonian path and it has no cycle.
(Refer Slide Time: 18:43)
 
So now we have defined these two terms, let us put them to use. We are talking of reductions. So we are going to see, how closely are these two problems or two problems that I am going to define right now related. One is the problem of finding the Hamiltonian paths and the other is the problem of finding the Hamiltonian cycle.
So, the first problem is called Hamiltonian cycle. Let us see, it is this input is a graph G. The questions you ask Does G have a Hamiltonian cycle? This is problem HC. Similarly, I have problem HP not to be confused with Hewlett Packard. And the input to the graph G, the question Does G have Hamiltonian path? So for this problem, you want an algorithm that says you fit in a graph G.
And this algorithm should say either yes the Hamiltonian path or not the graph does not have that is for this problem. For this problem, it should say yes put the graph as Hamiltonian cycle otherwise No the graph does not have Hamiltonian cycle. So, the input is a graph, the output is yes or no, yes if it has this structure either the path either the Hamiltonian path or cycle no, it have.
So, these are the two problems and clearly they seem to be related I mean Hamiltonian cycle is a cycle that spans all vertices. Hamiltonian path is a path that spans all vertices. So, one would expect these two problems to be similar in some way. In fact similar now, we have a clear notion of what similar must be; we want to say something like this.
If we can solve HC which means if there is an efficient algorithm for HC, there is a efficient algorithm for HP. And there is a efficient algorithm for HP, there is a efficient algorithm for HC.
(Refer Slide Time: 21:10)
 
So, these are the questions that we are going to address now an efficient algorithm for HC which is Hamiltonian cycle. Design a efficient algorithm for HP, this is Hamiltonian path. So, how do we do this? So here is my here is the algorithm HC. If I feed in a graph to this algorithm it will say whether or not it has a Hamiltonian cycle. Now, what I want to design is one for HP. I want this feed in a graph this would say yes if it has a Hamiltonian. So, supposing the input is some graph G.
I want to determine if this graph has a Hamiltonian path or not. And I have to use this algorithm somehow. Supposing I feed the same graph into HC, two things can happen HC can say yes or it can say no. So, let us try this let us see what happens in each of these two phases.
(Refer Slide Time: 22:42)
 
I want to determine if G has a Hamiltonian path, does G have a HP? This is the question. So, I feed G into HC. There are two cases this fellow can say yes and what do you think you can conclude. If it has a Hamiltonian cycle does it have a Hamiltonian path? Well absolutely it must have a Hamiltonian path. So, if G says this HC says yes for G you are done great it has HP. So, you can say with surety that if it has a Hamiltonian circuit it has Hamiltonian path.
What if it says No? Now, we really do not know may be it does not have an HP in which case it does not have a HC and so we are all fine. The problem is the graph could have a Hamiltonian path, but it need not have a Hamiltonian cycle. So that case, we are not just able to distinguish using just a blind way of using this sub routine. So, we have to be a little bit smarter in using this sub routines and that is the duty of the subject.
You have to figure out how exactly do I use? This sub routine HC effectively to get a Hamiltonian path. This is what you do? You should may be try it you see the solution I am going to give right now. In fact, these transformations to the graph that I am going to do you have seen earlier it is a hint. So, what we do is this.
(Refer Slide Time: 24:52)
 
So here is G, I take G as it is I added this is G, I added new vertex u connect u to everything in G, u is connected to every vertex in G. Now, this graph I feed into HC I have an original graph where I want to determine if it has a Hamiltonian path or not. I add a vertex I connect it to every other vertex in G and feed it into HC.
Now, if HC says it could say yes or no. So, what I am going to do is if HC says yes then I will also say yes, the graph G has a Hamiltonian path. If it says no, I say G does not if HC says yes then the output of the algorithm for HP will say yes. If HC says no I will say no on this new graph; however, it is on the new graph.
So, does this work does not this work. Well it does work we will see why there are 2 things we have to show we have to show that if it answers yes. This answer must be correct which means this graph G must have a Hamiltonian path. And if it answers no the graph should not have.
(Refer Slide Time: 26:27)
 
So, the two statements that we need to prove is the following are the following here is G vertex u is adjacent to every other vertex in G. So, call this new graph G prime new graph is G prime. So, I want to show that G has a HP if and only if G prime has HC. This is an if and only if statement, so if there are two things. The two things correspond to yes in output. For instance, we have to show that G has an HP this implies that G prime has a HP.
And the no output G does not have a HP this implies that G prime does not have HC. So, these are the two things. So, assume that G has a HP, we need to show that G prime has an HP. And I think this should be fairly obvious, there is G here is my HP. I do not know how this looks; it is a path which runs between, which has all the vertices in G.
Now, I need to construct an HC which is a Hamiltonian cycle in G prime and that is easy I just add these two edges. So, this path starts at some vertex and it ends at some vertex. It starts at x and ends at y, the Hamiltonian cycle, I get by just appending u to x and y. I add the edges u x and u y and I get a cycle in G prime.
And you can see that I can go the other way also. So, given the Hamiltonian cycle in G prime I can construct a path a Hamiltonian path in G and the construction is similar. So here, supposing this is graph G prime and I have a Hamiltonian cycle it goes around like this in G prime.
Now, this vertex u sits somewhere, this is my vertex u. If I remove this vertex u from the graph what I am left with is the Hamiltonian path in G. This path will start from this vertex x and will end at this vertex y. So, the path starts at x it will go around and end at y and this path is in the graph G. So, we have proved statement both ways that if the original graph had a Hamiltonian path the new graph have a Hamiltonian cycle. If the original graph did not have a Hamiltonian path the new graph will not have a Hamiltonian cycle. What about the other way round? We said given an algorithm for Hamiltonian cycle. We could construct one for Hamiltonian path.
(Refer Slide Time: 29:47)
 
What about the other way round? The other way round would be this given an algorithm for Hamiltonian path constructs one for HC and from now on it, if you are given a algorithm it is efficient. And you want to construct one that better be a HP. So, these are all well I just said algorithm, we want efficient algorithm. The previous one was clearly efficient. I had to call the Hamiltonian cycle routine exactly once.
I do a small modification to the graph the input size does not go up by much and I call the Hamiltonian cycle routine exactly once. So that running time is roughly this the old algorithm was sufficient the new one was. So, how about this? So, you are given a algorithm for HP which means if you are given a graph G it says yes or no and what I want is for HC? The given let me call this G prime to distinguish from this G. So, this is what we want, this is what we want this is given to us.
So, somehow again I have to use this routine for HP efficiently to get this routine for Hamiltonian cycle. So, how do I do? I could let us try the usual trick feed this graph into HP and it says yes. If it says yes then well it could have a HC or it may not have an Hamiltonian cycle. On the other hand if it said no then I am here that the graph does not have the Hamiltonian cycle. In fact, does not have a Hamiltonian part. So, if it says no I have no we are all fine, but if it says yes, again we run into a similar problem. We cannot decide whether it has a Hamiltonian circuit or not.
(Refer Slide Time: 32:20)
 
So, what do we do? So, here is there is an attempt take the graph G. So, I want to use let me just put this on the side. So this is, what I want to use and I want to create one for HP. So, what do I feed into HP. So, take this graph there is a input graph G. So, take G now remove edges from G one by one. So, let us say I remove edge e. Now I feed into G prime and I will see whether it says yes or no.
Now, if it says no for any edge, then clearly the graph will not have a Hamiltonian cycle, why is this? So, supposing G has a Hamiltonian cycle then if I remove any edge from G, the graph still has a Hamiltonian path. So, if I take G and remove any edge the Hamiltonian paths must keep saying yes. Is this a good algorithm I mean can I sort of say that remove every edge from G feed into HP? If it says yes all the time then yes G has a Hamiltonian cycle. If at least once it says no then G does not have Hamiltonian cycle.
Let me write this algorithm. For each edge e call HP with G minus e. If HP says yes for all edges then G has an HC. So, you output yes otherwise output no. So, if it says if HP says no for any input then you say no. So, this is the algorithm. Well the question is does this work what do we mean by this question what does it mean for this to work or not.
It means that whenever you output yes. So, whenever you output yes the graph better have a Hamiltonian cycle. And whenever you output no the graph should not have a Hamiltonian cycle. Let us take both of these in turn and see whether we can prove this.
(Refer Slide Time: 35:29)
 
So, let us take no first, so that is easier. So, you output no if for some edge e prime G minus e prime does not have a Hamiltonian path. So, if I remove this edge e prime then the graph does not have a Hamiltonian path. We have seen that this implies that G does not have Hamiltonian cycle. If it had a Hamiltonian cycle then if I removed any edge out I would still have a Hamiltonian path in the resultant graph.
So whenever we output no, we are in the clears we are correct the algorithm is always correct, what about the yes case? It seems very reasonable that for every edge if I remove it. And ask there is a Hamiltonian path and it says yes there should be reasonable that there should be a Hamiltonian cycle. So, if there is a Hamiltonian path after removal of any edge is a good chance that one feels that graph should have a Hamiltonian cycle, but this is false. So, this statement is false.
(Refer Slide Time 37:11) 
 
Let us see an example, so here is an example. So, this is an example of a graph such that removal of after removal of any edge graph has a Hamiltonian path, but the graph does not have Hamiltonian cycle. So, there are two things to be checked with this graph. That it does not have a Hamiltonian cycle, but if I remove any edge from this graph any edge at all then this graph must have a Hamiltonian path. Let us check both of them.
Now, this graph does not have a Hamiltonian cycle and to see that we focus on this vertex in the middle. If I remove this vertex, then this graph becomes disconnected. There is a vertex in this graph. So that if I remove this middle vertex and the graph becomes disconnected. This cannot happen if the graph has a Hamiltonian cycle. If the graph has a Hamiltonian cycle, if I remove any vertex the resultant graph will have a Hamiltonian path and will in fact be connected.
So, if a graph has a Hamiltonian cycle if I remove any vertex it must remain connected that is not true for this graph. That is the reason why this does not have a Hamiltonian cycle, we still have to prove one more thing which is that if I remove any of these edges in this graph then it should not have it must have a Hamiltonian path.
(Refer Slide Time: 39:26)
 
So, let us look at these edges one by one. Let me draw this graph again. So, let us focus on this edge supposing I remove this edge. Does there exist a Hamiltonian path? Well the answer is yes choose this edge choose that edge choose this edge. You come down this way then you choose this edge. Then, you go down this way to this edge that edge. There are other ways of doing this.
So, you go up and down like this and this you can see the Hamiltonian path. So, removal of this edge here causes no problem. So, let us look at some other edges. So, let us remove this edge what if I remove this edge? So in this case, I can start here come down here go up here there it says this and this. So, this will give a Hamiltonian path. So, you start here and you go up and down like this go up go down up to this and this you can see gives a Hamiltonian path.
So, removal of this edge is also not a problem. Let us look at one more lets remove this edge. If I remove this edge, the graph still has the Hamiltonian path this this this this. So, you go down this way go back keep going down all the way up here go down and back here. So, if I remove this edge too there is a Hamiltonian path. Now we just see that all edges in the graph are similar to one of these three.
For instance this edge is similar to that right this edge at the bottom is similar to this. So, all edges on this side is taken care of. These two edges are taken care of, this is the middle edge these two are similar. And the graph is symmetric about this vertex. So, all edges on this side on the left hand side are also taken care of. So, these three cases are enough to enumerate to sort of hopefully convince you that removal of any edge in this graph leads the graph with a Hamiltonian path.
(Refer Slide Time: 42:18)
 
So, this type the algorithm that we described is wrong this fails. So, it looked like a reasonable thing to try, but it fails.
(Refer Slide Time: 42:35)
 
So, what you do is this? So, here is my graph. So, here is let me take some edge u v. Now, what I will do is remove the edge u v remove this edge and I will attach to other vertices this is u prime that is v prime. So, I have a original graph G I remove the edge u v. So that is g1 from here and then I attach two additional vertices u prime and v prime.
Now, I ask does this graph have a HP, does this graph has a Hamiltonian path. The question we wanted to answer was does this have a Hamiltonian circuit. Now let us see, supposing this graph did have a Hamiltonian circuit. Supposing this had a Hamiltonian circuit not only that the Hamiltonian circuit pass through the vertices u and v. In order which means the edge u v was present in some Hamiltonian cycle in the graph.
Then, let us notice that this new graph has a Hamiltonian path. So, what does it mean for this graph to have a Hamiltonian cycle passing through u v? It means there must be a cycle this way it goes through all other vertices like this and also u v. So, let us look at the same cycle, where I take the same cycle and I add these two edges and I get a Hamiltonian path in the new graph.
So, if the old graph had a Hamiltonian circuit. Then, if I picked an edge which was present in the Hamiltonian cycle and ask if this has a Hamiltonian path. Then, it will have a Hamiltonian path that is the first thing to notice. Now here is the second thing to notice supposing I took a graph like this. So, I took a graph and here is G. I took a graph G edge u v I removed u v attached u prime and v prime and I ask if this has a Hamiltonian path.
Supposing this says yes what does the Hamiltonian path look like? Now, the path has to these two vertices have degree 1. So, they have to be the end points of the path u prime and v prime have to be the end points of Hamiltonian path which means the Hamiltonian path has to look like this. It has to start at u prime and then go through all vertices in G and then go back up to v prime. This is how the Hamiltonian path should look like.
Then, what we do is this? We remove these two edges. And then put this edge u v which you have removed this will give you a Hamiltonian circuit in the original graph. Now, our algorithm is almost complete, what we do is? We do this for all edges in the graph. We remove that edge attach this u prime v prime and ask if it has a Hamiltonian path. If any edge it says yes then we say yes it has a Hamiltonian cycle. If for all edges it says no then we say no, it does not have a Hamiltonian cycle. So, let me write this algorithm, then we will argue that this algorithm is in fact correct.
(Refer Slide Time: 46:19)
 
So, the algorithm is this for every edge e, do the following. So, remove e from G, let us say e is equals is u v. Then, add vertices u prime and v prime and connect u prime to u and v prime to v. This lets say graph G lets say e, for an edge e I get this graph G e. Now feed G e; that means, input G e to an algorithm for HP. If this algorithm says yes for any e output yes, if it says no for all e you output no. So, otherwise output no.
So you output no, if it says no for all edges you do this you remove this edge add these two extra vertices and ask whether it has a Hamiltonian path. If it says no for all these edges then you output no. So, this is the algorithm and we have to show that this is correct which means if the original graph had a Hamiltonian circuit. Then, this algorithm will always say yes.
If the original graph did not have an Hamiltonian circuit, it will say no and it is efficient. If the Hamiltonian path algorithm runs time polynomial in the input size, so does the new algorithm. So, these are the three things to check, let us just make sure that the algorithm is efficient first. You call it once per edge the Hamiltonian path routine is called once per edge and you do not change the input size by much.
You just remove one edge and you add two more edges. So, the number of edges goes up by one. So, the input size does not go up by much and you are calling it at most m times. So, the original running time was bounded by some n to the constant. It is still bounded by some n to the constant again there are two cases. Case 1 is when the algorithm outputs yes then it has to be correct. Second case is the algorithm outputs no this no answer also has to be correct.
(Refer Slide Time: 49:54)
 
Let us take both these cases. So, case 1, so the algorithm outputs yes. So, this implies; that means, for some edge HP must output yes. This means there is there exists an edge e in G such that G e has Hamiltonian path. So, here was G and here is e I get G e by removing and adding these two. So, this had a Hamiltonian path. And now we have seen this argument that if this has a Hamiltonian path then this has a Hamiltonian cycle. Essentially you take the Hamiltonian path here remove these two edges and add this edge back together Hamiltonian cycle here. So this case is done.
(Refer Slide Time: 50:59)
 
The algorithm outputs No this means for every edge e G e does not have Hamiltonian path. So, we need to see that this implies that G does not have Hamiltonian circuit. So, this is what we need to show? So, we need to show that for every edge e if G does not have Hamiltonian path, then G does not have a Hamiltonian circuit. Now, it is easier to prove the contra positive, which means this statement, which I have written is equivalent to saying the following that this let me write the equivalent statement on the right hand side.
So, I am going to say that G has a Hamiltonian cycle this implies if G has a Hamiltonian cycle what should happen? This means this statement cannot happen. It is not the case that for every edge e G does not have a Hamiltonian cycle. It means there exist an edge e such that G e does not have HP. So, these two things are the same. So, these two are equivalent. So, let me just sort of say what I am doing. So, you want to prove that A implies B you are proving that NOT B implies NOT A. I want to prove that this implies this is A and this is B.
So, I want to prove that A implies B all I am doing is NOT B is this implies NOT A which is this because I am going to prove that if G has a Hamiltonian cycle. There exists a edge such that G does not have a Hamiltonian path. And you can see this also we have proved.
(Refer Slide Time: 53:37)
 
If G has a Hamiltonian cycle, so that is G. So that is my Hamiltonian cycle, if I remove any edge in the Hamiltonian cycle. So, if I remove this edge and attach these two vertices this resultant graph has a Hamiltonian path which is what we wanted to prove. So, we want to prove that G has a Hamiltonian cycle.
There exists an edge such that G does not have sorry G does have a Hamiltonian path. I am sorry about this G does have a Hamiltonian path and this we have just proved. In fact, this edge e can be any edge in the Hamiltonian cycle. So, remove any edge in the Hamiltonian cycle for that edge removal of that edge G e will have to remove that edge attach those two vertices. This new graph G e will have a Hamiltonian path.
(Refer Slide Time: 54:37)
 
So, when we looked at these problems there were two kinds of problems we looked at. For some problems we said that we wrote input output. For certain other problems we said input question, now when I looked at problems like with finding a matching of maximum weight of finding a matching with maximum number of edges. They were of the form input and output. Because, the output was edges which were there in a matching of maximum size.
Now, for instance does the graph has a Hamiltonian cycle that is the question the input is a graph. Does the graph has a Hamiltonian cycle I ask a question the answer should be yes or no. Now, the difference was for these kinds of problems. The answer was a single bit, it was yes or no. For these problems the output spend many bits. So, these had a single bit as the output these had many bits as output for instance edges in a matching.
In fact, let us look at the Hamiltonian cycle problem. We said input is a graph G, does G have a Hamiltonian cycle if yes or no. So, the answer is yes or no. On the other hand I could have asked for a output for saying output the Hamiltonian cycle input is a graph G output the Hamiltonian cycle. Now, this problem seems to be somewhat similar to the previous problem only here the output has many bits. We have to output every edge in a Hamiltonian cycle.
We have to choose one Hamiltonian cycle and output the edges from the Hamiltonian cycle. So, we would like to distinguish between problems, where we require the output to be 1 bit and problems, where we require the output to be many bits. These we call decision problems and these we call search problems.
(Refer Slide Time: 56:58)
 
So, typically here for decision problem let me write decision and search. So here, output is 1 bit here is many bits. So, here you want to decide whether true or false here you want to search for a solution and output the solution. That is why this I guess that is why these are called decision and these are called search problems.
So example, does G have a Hamiltonian cycle. So, this is an example of a decision problem. Example of a search problem output a Hamiltonian cycle if one exists. The input is same to be both which is a graph G in one case you just want to know G has a Hamiltonian cycle or not. The other case you want Hamiltonian cycle output you want actually the edges we have.
Now, how do these two things, how do these two problems relate to it. Is one easier than the other is one harder than the other? Now it turns out people have just observed it that they are related to each other quite closely in the sense that one is easy the other one also turns out to be hard.
(Refer Slide Time: 59:08)
 
So, let us take this Hamiltonian cycle problem and see this. So, I have so the input is a graph G. Now there are two problems, one is does G have Hamiltonian cycle, this is 1. And the second thing output a Hamiltonian cycle if one exists this is problem 2 that is problem 1. Now supposing there is a algorithm for problem 2. So, which means given so if you feed in a input graph the sub routine outputs the Hamiltonian cycle.
Can we find a algorithm for problem 1, yes I hope all of you answered yes. So, the answer has to be yes. So, you just feed the graph in so this sub routine and look at the edges which have been output. So, if it forms a Hamiltonian cycle, then you just output saying that yes it does in fact have a Hamiltonian cycle. And if it says no it does not the original routine says no it does not have Hamiltonian cycle, I just then you just output things, so that is trivial.
Our job is to look at the other way which means I have an algorithm for problem 1. I have an algorithm for this. Now I want a algorithm for this so given. Let me add efficient just to remind you that we are talking of efficient algorithms given an efficient algorithm for problem 1. Construct one for problem 2. So, this is what we really want to do and how do we do this.
We have actually done something like this before which was with respect to matching?s when we wanted to find the edges in the matching of maximum weight. We used this sub routine which answered something else and we somehow got these edges out and we used absolutely the same trick here. So, what we do is this? We will look at the edges one by one let us say the edges e 1 e 2 e 3 e 4 and so on.
So, we look at the edges one by one I remove e 1. Now I ask if the graph has a Hamiltonian cycle or not. If the graph has a Hamiltonian cycle it does have a Hamiltonian cycle which does not use this edge e 1, so I just throw away e 1. Similarly, I look at e 2 I remove e 2 and ask if the graph has a Hamiltonian cycle. If it says if the sub routine says yes I just throw away this edge and I keep doing this.
Supposing I remove an edge and it says the graph does not have a Hamiltonian cycle then you also know that this Hamiltonian cycle uses this edge. So, these edges you put back in and you do not want to throw these edges. So, you just do this look at all edges one by one for those edges, where the algorithm says that it does not have a Hamiltonian cycle.
You throw away those edges, but edges for which the algorithm says that there is if you remove there is a Hamiltonian cycle. You throw away those edges for edges when you remove those edges and ask the question it says no there is no Hamiltonian cycle; that means, this edge must be present you put it back here.
So, you do this for all edges and at the end of it you throw away some of edges and some edges remaining. The claim is that the edges which remain must form a Hamiltonian cycle. Firstly, the graph must have a Hamiltonian cycle, because whatever remains whenever we ask whether it has a Hamiltonian cycle it says, yes.
So, the graph that remains must have a Hamiltonian cycle, why should not there be other spurious edges floating around, why should it not have apart from the Hamiltonian cycle other edge. Supposing it had some other edge when you remove this edge and ask whether the graph is Hamiltonian cycle. The answer should have been yes. So, the answer is yes you would have thrown this edge away. So, there are no spurious edges. The only edges which are present are Hamiltonian. 
(Refer Slide Time: 1:04:29)
 
So, let me write this. So, just to recap we are given a algorithm that says whether a graph is a Hamiltonian cycle or not. We are constructing using this as a sub routine, we are trying to find out edges in some Hamiltonian cycle. So, consider edges one by one for edge e i remove e i and ask HC if the resultant graph has a Hamiltonian cycle. If I says yes throw away e i else retain e i that is it you just do it for all edges.
Finally, what remains is a Hamiltonian cycle which means what remains are edges from a Hamiltonian cycle. So, you can see that both problems are quite related I just need to call the other algorithm n times able to actually extract the Hamiltonian cycle. So, it is been observed for most problems there are of course, problems which do not fall into this which do not fall into this property. But for most problems if I can sort of decide then I can also search for a solution. So, if the decision version of a problem is easy then even the search version is easy.
On the other hand, if the search version is hard then social decision word, so this is the reason they are going to focus on decision versions of problem from known. So, we are just going to look at the problems which have whose output is 1 bit and say yes or no.
(Refer Slide Time: 1:06:53)
 
We are now ready to define the class NP. So, before I formally define this. Let me, sort of give you a informal definition. Let us take our favorite Hamiltonian problem. So, HC the input is a graph G and the question does G have Hamiltonian cycle. So, let us look at this one just consider now we are going to do something slightly different from what we have been doing. We are not going to design algorithms for the time being what we are going to do is have a game between two players.
There are two players now one I will call a Prover and one I will call a Verifier. These are two people Prover and verifier, Prover is all powerful; he just knows everything. The Verifier has limited resources and it does not trust anybody sort of does not trust. Especially he does not trust the Prover he has limited resource.
Now, the Prover and Verifier sort of lets sitting in a room. And there is a huge graph in front of just imagine that somebody has drawn a huge graph in front of them. The Prover looks at the graph and he says this graph has a Hamiltonian cycle he is powerful he can figure these things out and he says this graph has a Hamiltonian cycle.
Now, the verifier is skeptical he does not believe the Prover. So, he says the why should the graph have Hamiltonian cycle and what is the deal, then what should the Prover do. The Prover wants to convince the Verifier that he is all powerful and he has made the right statement he said the graph is Hamiltonian cycle and the graph has a Hamiltonian cycle. So, he wants to convince the verifier that the graph has Hamiltonian cycle.
So, what could he do? Well if you think about this for a minute what the Prover could do was pick out the edges from the graph which form some edges which form some Hamiltonian cycle. So, pick out some Hamiltonian cycle from the graph and tell the Verifier that here are a set of edges which form a Hamiltonian cycle.
Now all the verifier needs to do is look at these edges make sure they form a make sure that they form a cycle. You just have to verify that these set of edges form a Hamiltonian cycle which is easy to verify even though he has limited resources and he is not too intelligent this much he can do. So, the Prover has somehow convinced the verifier that this graph has a Hamiltonian cycle.
So, the Prover says that there is a Hamiltonian cycle in this graph the Verifier asks why the Prover then picks out the edges. These are the edges look at these edges they form a Hamiltonian cycle in a graph. The Verifier then verifies that yes indeed the graph did have a Hamiltonian cycle. And we have just proved that Hamiltonian cycle is in the graph is in the class NP. I have not defined the class NP.
So, you really do not know what it is. But, this is all there is to proving things proving the problems are in the class NP. So, let us do this Prover Verifier game for some other problem just to get sort of just to get used to it. Let us say the input is an integer t. So, you want so the output factors let us say two non trivial integers t 1 and t 2 such that t equals t 1 times t 2. So, you want to find factors of these.
(Refer Slide Time: 1:12:08)
 
Now, this is a search question it is not a decision question. So, the corresponding decision question is the following. So, here is the question is t composite which means can t be written as t 1 times t 2 where t 1 is not 1 and t 2 is not 1. So, these two are non trivial factors of t. So, let us play the same Prover verifier game. So, here is the Prover whom I will say P here is the Verifier V.
So, P says t is composite, now V asks why, now t has to convince V that in fact, it is composite. So, Prover sends t 1 and t 2 across. So once, he sends t 1 and t 2 across Vmultiplies t 1 and t 2 and checks the result with t. If its equal to t of course, he is convinced that t is composite. And in fact, t 1 and t 2 are two factors go back and look at this Hamiltonian problem again.
So, let us look at this again, now if the answer was yes it does have a Hamiltonian cycle which means the Prover says it has a Hamiltonian cycle then the Prover could convince the Verifier that the graph does have a Hamiltonian cycle, what if the Prover says no it does not have a Hamiltonian cycle. So, usually he says yes this guy asks why, so he sends edges in HC and this guy Verifies.
Now supposing, he says no, it does not have a Hamiltonian cycle. Now the verifier asks why and well what does the Prover do could he convince, how could he convince the verifier the given graph does not have a Hamiltonian cycle. Now, think about this for a little, while you see that it is a difficult question to answer, how could he convince somebody that this graph does not have a Hamiltonian cycle.
Well the only way seems to be you just try all possible subsets of edges whether they form a Hamiltonian cycle or not. If known of them form then it does not have, but this is a brute force method. And the verifier does not have so much time he does not have. So, much time to check all possible subsets of edges. So, there is no easy way for the Prover to sort of convince the Verifier that G does not have a Hamiltonian cycle. So, it looks like the answer to the question, whether it is yes or no. If it is yes the Prover can convince the Verifier if it is not the Prover is not able to convince the Verifier. So, they behave sort of differently.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 28
NP- Completeness ? 111

We are ready to define the class NP. We will first do this using an example. So, the example we will pick a problem, the problem we pick is HC.
(Refer Slide Time: 00:57)
 
The input is a graph G and the question is does G have the Hamiltonian cycle. So, this is the question. Now, forget algorithms for the time being. So, we are not going to discuss algorithms, what we are going to do is discuss a game between two players. There are two players. We will call them prover and the verifier, we will see, why they are called prover and verifier very shortly.
This prover think of them as all powerful, he knows all answers that he supposed to and even answers is not supposed to. And the verifier has limited resources. In particular, he does not have too much time, limited resources or this includes time. Now, the prover and the verifier meet. So, they are sitting in a room and they see a graph in front of it. Now, the prover looks at the graph and says this graph is a Hamiltonian cycle. The verifier, who is bid trustful of the prover, asks him why, why does the graph have this.
Now, the prover can convince the verifier that the graph does have a Hamiltonian cycle very easy, how does he do it. He just gives the edges in the graph, in the form of a Hamiltonian cycle. So, he picks any Hamiltonian cycle in this cycle and he tells he tells the verifier, look at these edges, these edges in the graph form a Hamiltonian cycle. The verifier, even though is sort of limited in resources this much he can do, given a Hamiltonian cycle in a graph you can verify that these edges. In fact, do form a Hamiltonian cycle.
So, let me just write this conversation between the prover and the verifier. So, the prover says yes. So, this yes is an answer to this question above, yes here is an answer to the question. Does G have a Hamiltonian cycle the prover says yes, the graph has Hamiltonian cycle or there is an input graph that both of these are looking at both the prover and the verifier are looking at the prover says yes, the graph has a Hamiltonian cycle. The verifier then asks why. And the prover supplies edges in some HC in G. And the verifier verifies that these edges indeed, form Hamiltonian cycle. So, this is the conversation. Let us go over it again, the prover says yes to this question and to supplement this statement, he supplies the edges in some Hamiltonian cycle in G. And the verifier looks at these edges in the Hamiltonian cycle. And it is easy for him to verify, now that these edges indeed do form a cycle.
This is a game that we will sort of consider and in some ways, we have already proved. I have not defined what a NP is. But, we have actually proved that the Hamiltonian cycle problem is in the class NP. So, let us formally define the class NP. And then we will see, why this constitutes a proof that Hamiltonian cycle is in it. So, Hamiltonian cycle problem is of course, a decision problem. The question we ask is does there exist a Hamiltonian cycle or not it is to be answered yes or no. So, it is a decision problem.
(Refer Slide Time: 06:15)
 
So, a decision problem here is the definition, a decision problem is said to be in the class NP, if for all YES inputs. Let me write this down then I will explain what this means. If for all YES inputs, there is a proof, you can think of this as an advice or a string. So, this is, think of this as a string the prover gives to the verifier. So, this advice is what the prover will give to the verifier. So, there is a proof using which, it can be verified quickly where, the input is indeed a YES input. So, let us now, look at this. 
So, there are some terms that I need to describe. So, first thing is what is this YES input, what is quickly and then what does the statement mean, let us focus on the YES input. See this problem is a decision problem, since it is a decision problem for each input the answer is either yes or no right. So, YES input are those inputs, for which the answer is yes. So, we look at only those inputs, for which the answer is yes and we say that for these inputs, I do not worry about what happened to the no inputs.
For the YES inputs there is a proof, this is a proof that the prover gives the verifier, so, this is proof. So, YES input, YES inputs are inputs for which, the answer is YES, this is 1. The proof is what the prover sends to the verifier and quickly in time polynomial in the input size. So, I could have said efficiently, I guess that is what I have been using. But, quickly efficiently they all mean the same thing, which is that, it can be verified in time polynomial in the input side.
So, let me just, let go again this definition again, since it is sort of important. So, let us move over definition again. A decision problem is said to be in NP. If for all YES inputs, there is a proof, there is a proof using, which it can be verified that the input is indeed a yes. So, let us look at our previous example for Hamiltonian circuit. (Refer Time: 10:57) If for all YES inputs for a YES input, this means, if the graph does have a Hamiltonian cycle. 
There is a proof that the prover can give the verifier using, which this proof consists of edges in some Hamiltonian cycle. And using this verifier can easily verify, in polynomial time whether the graph has Hamiltonian circuit or not. So, this actually proves that Hamiltonian circuit the problem Hamiltonian circuit is in the class NP.
(Refer Slide Time: 11:41)
 
So, let me highlight the main sort of points in the definition. Then we will go over this again. So, first it has to be a decision problem. So, answer has to be yes or no. We, look at only YES inputs and on YES inputs supply a proof. This proof will help the verifier verify that it is a YES input, which helps verification. And this verification has to be in polynomial time. So, these are the initial things, it has to be a decision problem, we only look at YES inputs.
It is only for YES inputs that the prover has to convince the verifier and to convince he can give a proof, which can help the verification. So, here is let me sort of draw this diagrammatically. So, here is the YES input. So, you pick any one of the YES inputs. The prover is looking at this, shows the verifier. And the prover sends the proof or an advice, sometimes called an advice (Refer Time: 13:28) some string think of zeroes and ones, whichever way the verifier looks at the string and sort of figures out, what the prover means. So on YES inputs, the prover can send some proof to the verifier using, which the verifier in polynomial time can verify that. In fact, this is a YES input. So, let us do one more example.
(Refer Slide Time: 14:06)
 
This example is a composites, the input is a positive integer l, the question is l composite, it is the question. So, we want to know whether composites this problem composites is NP or not. First thing, it is a decision problem. So, we can it is a problem that we can look at, which could be. In the first thing it has to satisfy that the first property it has to satisfy is that it must be a decision problem which it is. The question asked is l is composite and the answer is either yes or no.
So, what is the next thing for all YES inputs, there must be some way for the prover to convince the verifier that. In fact, it is composite. So, let us just do this. So, we are looking at YES inputs which means, l is composite. So, what is the proof that the prover can give verifier. So, well I guess, it must be sort of obvious by now, that if you, now if you know that something is composite. And you want to convince somebody that it is composite, you can just give the two factors.
So, you give. So, the proof that the prover supplies to the verifier is a factorization. So, he gives let us say l 1 and l 2. So, this is the factorization of l. So, the verifier checks that indeed l equals l 1 times l 2 and neither of these is 1. Okay l 1 and l 2 cannot be 1. So, this proves that this is the proof that the prover can give to the verifier, to convince him at some number is composite.
So, one question I guess which most people sort of encounter when this definition is made. You know, you focus only on the YES inputs what about the no input. So, take composites for the time being. Input is a positive integer l, question is l composite let us say the answer is no, the prover says no it is not composite. Now, how can he convince the verifier that it is not composite, which means you have to somehow convince the verifier that it is prime, you know the only sort of numbers between 1 and the number that divides it are 1 and the number itself, nothing else in between.
Well, to think about it, this is not this does not seem to be as easy. Actually, in this case there is a way to do it. But, ways to do it is certainly not easy, there is a recent research, which says even without an advice, one can figure out whether a number is prime. But, that is way beyond the syllabus, for this course. So, let us again do this for the Hamiltonian cycle problem also, just to get the hang of this here, now (Refer Time: 18:26) when the answer was YES.
Yes the graph has a Hamiltonian circuit; this proof can be supplied right edges having Hamiltonian circuit in this. Supposing the prover had said no, if we look at the no inputs. How can a prover, how can you convince somebody that graph does not have a Hamiltonian circuit. So, if you think about it for a while, you see that this is actually a very difficult thing. Supposing, you say that this graph does not have a Hamiltonian circuit by some way or method whatever, you know this.
How, do you convince somebody else that it does not have a Hamiltonian circuit, seems to be extremely difficult thing. So, only way it looks like is the other person with or without your help, looks like all possible collections of edges. And sees whether these selection let us say, all edges of it take n edges where, n is the number of vertices. The graph takes all possible subsets of n edges and check whether they form a Hamiltonian circuit, now this could take a long time.
The time taken to go through all this exhaustive technique is certainly not may, certainly not be polynomial in the input side. So, and up to date nobody knows anything of better, so for at least HC and many of the problems that we look at. It looks like the YES input behaves differently from the no input. When the answer is yes, there is an easy proof by which you can convince the other person that it is the YES input. 
The no input seems to behave very differently. For the class NP, all that we require is that when the answer is yes, there should be an easy way to convince somebody else that. In fact, it is a YES input. So, how do you prove that something is in NP, we have seen two examples. But, let us just sort of note this down to prove something is in NP, you need to do two things.
(Refer Slide Time: 20:59)
 
You have to say the proof to prove that a problem is in NP, you have to do the following things. You have to say, the proof that the prover sends to the verifier and then how does the verifier check. So, these are the two things that you need to send, this proves by the way for only YES inputs. For YES inputs, you must say what is the proof given, any given YES input, what is the proof that the prover sends to the verifier. 
And once the verifier receives this and he looks at the input, the verifier should you should say how the verifier checks, how the verifier is convinced by looking at this proof and the input that the input is, in fact YES inputs. And for the Hamiltonian circuit case and for composites, for the problem composites we specified both. In the composites case for instance, the prover sends the factorization and the verifier just multiplies two of them and checks whether equal to. 
So, we have, in fact now defined a class NP we will see many other examples. In fact, I would encourage you to look at problems you have previously come across, at least the decision problems and try and prove whether, some of these fit into the class NP. So, one importance of this class NP is why I define this class NP at all. Well a large number of problems, which occur in real life fall into this class NP in the following sense.
I mean, if you look at problems in real life they, most of them you do not want problems with a yes no answer, the answer is usually many bits. But, these problems can be transformed into a problem, which is not much easier. In fact, as hard as the original problem where, the answer is one bit. And when one looks at these versions of real life, very often they cannot be that is why this class is very important because, a large number of problems that we will encounter.
And that people have been encountering in real life, they all fall into this class defined yet another problem. But, this is sort of central problem that will work around with. So, this will be a problem in NP and it will be the central problem, you would have seen something like, this in many of your previous courses in computer science. If you have done logic, you would have seen something like this; you would have seen this Boolean circuit etc, etc.
So, many of these concepts that we will deal with and will define should not be new to you anyway, I will define the problem. The problem is this I need to make few definitions, before I get on to the problem.
(Refer Slide Time: 24:40)
 
So, a Boolean variable is a variable, which takes value, which takes values in the set YES, NO. It takes two values, YES or NO. Now, if x is a Boolean variable Then x bar is a negation, is called the negation, is called its negation and if x is we would. In fact, may be YES or NO will also sort of alternate between true or false, instead of YES and NO. Essentially there are two values YES NO, true false or 1, 0. And if let us say x is true then x bar is false and if x is false then x bar is true and x bar is called the negation.
Some people also use, x is also used instead of x bar. A literal is either a Boolean variable or negation. So, both x and bar are literals. A clause is an OR that will define what an OR is of literals. So, OR is actually the usual logical OR for example, here is an example a clause x 1 OR x 2 bar OR x 3 bar OR x t. So, this symbol OR. So, clause is just an OR of literals and this clause evaluates to either true or false, depending on values that these variables here.
If any one of them evaluates to true, then the clause evaluates to true otherwise, it evaluates to false. For instance if x 1 is true the clause is true. If x 2 bar is true then it is true and. So, on any one of them, evaluates to true it is true. It is false if all of them are false, which means x 1 should be false x 2 bar should be false, which means x 2 should be true x 3 bar should be false x 3 should be true and x 3 should be false. All of them are false then it is false otherwise it is true. 
(Refer Slide Time: 28:39)
 
Let us let me write this. So, it is false if all literals are false otherwise true you could have said it is false if and only, if all literals are false. This is the usual OR, AND is coming up right now. 
(Refer Slide Time: 29:19)
 
So, a Boolean formula in CNF is an AND of clauses. So, I have c 1 AND c 2 AND c 3 AND c k and each of these is a clause, this symbol is AND this is again the usual symbol that we use in logic. So, this is another definition. So, Boolean formula in CNF is an AND of clauses, each of them is a clause c 1 c 2 and c k and c 1 AND c 2 AND c 3, so on up to c k, this I will call a Boolean formula in CNF, CNF stands for conjunctive normal form.
So, each of these clause remember is an OR of literals. So, this is an AND of OR?s and it is called the conjunctive normal form. So, I just have to tell you the logical behavior of AND, which is again the usual thing. This formula is true evaluates to true if, every clause evaluates to true otherwise it evaluates to false. So, maybe I can just write this formula, evaluates to true if and only if every clause evaluates to true.
If any one of those clauses is false and this formula evaluates to false. So, what do I mean by evaluates to false. It means, when you give values to these variables then if it is a variable then it evaluates to true. If it is the negation evaluates to false then you check each clause whether, it is value is true or not that you check. A clause evaluates to true if, any one of those literals evaluates to true. And then you look at the formula, you look at the collection of these clauses and this evaluates to true, if all clauses evaluate to true. 
Of course these are special Boolean formulae, which we call CNF, which is you have AND?s outside and OR?s inside. Of course you can form general Boolean formula, which AND?s OR?s and NOT?s any kind of way you like. But, these are the only Boolean formula that we are interested in and one can show, but we will not do it. One can show that every Boolean formula, take any Boolean formula that can be written equivalently in CNF. I need one more definition then we are ready to roll. 
So, Boolean formula, the Boolean formula in CNF is said to be satisfiable, if there exists an assignment to the variables, an assignment assigns a value either true or false to each variable. There is an assignment to the variables such that the formula evaluates to true. There is some way of assigning values to these variables. Each of these variables there is a way to assign values either true or false that when you evaluate the Boolean formula, it evaluates to true. 
In this case, you call the Boolean formula satisfied, if not you say that it is not satisfied. So, clearly there are Boolean formulae, which are in CNF, which are satisfiable and which are not satisfiable. For instance x and x bar if I take one clause to be just x. And another clause to be x bar and there is no way that you can satisfy this formula whatever, value you get to x the formula will always evaluates to false.
(Refer Slide Time: 35:30)
 
We are ready to define our next problem that we will study, which SAT not to confuse with exams, SAT stands for satisfiability. So, the input is a Boolean formula in CNF, the question is let me call this Boolean formula something f, is f satisfiable, this is the question. So, now let us observe that SAT is now empty. So, SAT is empty or is this, well it is a decision problem. And suppose the answer is yes, we have to only focus on YES input, so the prover says yes it is satisfied.
Then, how can the verifier be convinced that a given formula is indeed satisfied. So, what is the prover give the verifier, so that the verifier. So, the verifier can look at this advice that the prover has given him, look at the input formula and you know we completely convince that yes indeed the formula is yet satisfied. Well I hope most of you have answered this question, by now the proof that the prover gives is just the satisfying assignment. So, the proof is an assignment to the variables.
The verifier uses this assignment and checks that the formula evaluates to true. The verifier uses this assignment and he checks the formula evaluates to true. So, given an assignment you should be able to convince yourself that it is easy to check, what the value of your formula is. So, you just plug in the values into for each variables, now check whether each of these clauses are true.
All clauses are true then you are true the formula is satisfiable. So, if it is a YES input that is the proof that the prover can give the verifier using which the verifier can easily check that the formula is. In fact, true. So, what is the big deal yet another problem in NP that is yet another problem in NP, but this as we shall see in a minute is a very special problem in NP. So, thing that makes SAT very special is the following theorem of cook.
(Refer Slide Time: 39:01)
 
Cook is Steven Cook, it is a name of a person Steven Cook and there is no relationship to (Refer Time: 39:22). So, this theorem says if SAT has an efficient algorithm then so does every other problem in NP. Let us tire at this time for a minute. So, this is among the most important theorems in computer science. Let us make sure that we understand this. By efficient I mean polynomial time, efficient means polynomial time.
This says that if, there is a polynomial time algorithm for SAT then there is a polynomial time algorithm for every other problem in NP. So, to solve any other problem in NP, it suffices to solve just SAT I mean, remember that NP includes factor, factoring it also includes Hamiltonian cycle, matching etc, etc, etc. Large number of problems that we have seen, so far, in fact is in this class empty and this theorem says that if SAT has an efficient algorithm. So, does every other problem in NP.
We have proved statements of which are sort of similar. We have proved I mean this is the notion of reduction. So, we showed that if HP has an efficient algorithm. We showed this means that HC has an efficient algorithm, we have done this. Now, and this was not easy we were, we took a little bit of time and effort to do something, which is as sort of well specified at this. Both these problems are very well specified, we knew where we were going.
Look at this statement here this says that SAT as an efficient algorithm implies every problem in NP has an efficient algorithm. This is not just one problem somewhere, it is not Hamiltonian cycle, Hamiltonian path what have you which means every problem in NP has efficient algorithm. So, I hope you appreciate the power of the statement and this theorem has changed the shape of computer science. So, there are two ways of reading this statement, one is to say that if you can solve SAT, then you can solve everything else.
So, concentrate all your attention on SAT that is one way to do it. Lot of people have tried and you know, they have not been able to find an efficient solution for SAT. So, the other way of to do this look at the statement, SAT is among the hardest problems. Here is a class NP and SAT is amongst the hardest problems in the sense that you can solve SAT everything else follows. So, it is among the hardest problems.
So, SAT is one of the hardest problems are there other problems in NP, which are as hard of which I can make a statement, which is similar that. If this problem is as hard is the you know, I can solve this problem in NP and I can solve every other problem in NP and I make this statement of other problem. And how easy and difficult is it to prove these statements.
Fortunately with cook?s theorem in hand there are. In fact, let me further say there are such problems in NP. And with cook?s theorem in hand proving that these problems are among the hardest problems in NP becomes a bit simpler I mean. How does it go about doing, so here is one. So, I take a problem say some problem pi. So, what I do is this.
(Refer Slide Time: 44:23)
 
If there is an efficient algorithm for pi then there is an efficient algorithm for SAT. Supposing, I can prove something like, now this is something we have been doing and it looks like we are capable of doing such things. We have done it for instead of pi and SAT, we have done it for we have taken HP HC and well matching. We have seen some matching where, we have taken two algorithms, two problems like this. And we have shown if, there is one algorithm for a problem and there is another algorithm.
Essentially we use the sub routine for pi and we construct an algorithm for SAT something like this, we can do. But, now let us see what this gives us, now here is the other sort of implication that I would like to use. Cook?s theorem says that if there is an efficient algorithm for SAT. There is an efficient algorithm for every problem in it, there is an efficient for all of NP which means, all problems in NP. 
So, let us just put these two things together, if there is an efficient algorithm for pi there is one for SAT and there is one for SAT then there is a efficient algorithm for all of NP. This follows from cook and this result we will have to hook up. So, this is up to us to prove. So, to prove that pi is amongst the hardest problems in NP, all I need to do is prove that if there is a efficient algorithm for pi, there is an efficient algorithm for ((Refer Time: 46:36)) and once I do this.
Using this implication, I am down to prove that there is an efficient algorithm for all of NP. So, this would sort of identify pi among the problems in NP. So, let us identify our goal for the next few lectures, few hours will be to identify more and more problems in NP, which are among the hardest, in the sense that if you solve this then you can solve every other problem in it. So, here is the first one, I need to make a definition.
(Refer Slide Time: 47:36)
 
So, a clique in a graph G is a subset of V such that lets say U of V such that, when I look at any two vertices in U, there must be an edge between them. So, you look at a graph you look at a small subset of the vertex set and in this subset when, I look at the vertices. If between any pair of them there is an edge, in the graph then this I will call a clique. The subset U such that for every u 1, u 2, till U u 1, u 2 must be an edge in G, must be an edge in G which means, if I restrict myself to U, I must get the complete graph.
Every edge must be present or let us take an example. Let us take this example a b c d e f let me add this in this example, a, b, c, d is a clique. So, I look at these four vertices. Every possible edge is present I cannot replace c with e. a, b, d and e is not a clique because e is not adjacent to a and d, if I just take a, b, d that is a clique. If, I take just d, c and g that is also a clique. So, this is a clique of size 3, this is a clique of size 4. It is a subset of the vertex such that if I take any two vertices, it is in edge between any pair of them, problem is this.
(Refer Slide Time: 50:10)
 
So, this is a search version. So, the input is a graph G, output find a clique of maximum size, find a subset find a largest subset you can. So, that when I look at these set of vertices it forms a clique. So, in this graph for instance, the largest clique that you can find is of size 4, which is a, b, c that is the largest. So, the search version is this remembers we are trying to identify problems in NP this is not even a decision problem. So, you want the vertices in a largest clique.
So, we need to look at, what we will do is we will define a problem, which is very similar to this, which will be a decision problem and I will focus on that. So, the decision version is this clique, input is a graph G and a positive integer k, you see how k creeps in or y creeps in a minute. The question is does G have a clique of size k. So, this is the question that we look at. So, let us focus on these two problems. 
Now, the second problem which is a decision problem, in the sense the question the answer is either yes or no. So, that is fine, now if you can solve the search problem. If there is an algorithm to solve the search problem, clearly there is an algorithm to solve the decision problem. Just find the clique and find the size. If the size is k or larger then you are done the graph does have a clique of size k. On the other hand, suppose you have an algorithm for this.
How do you find, can you solve the search problem well the answer is yes and the construction is very similar to what we have done earlier. So, what you do is first find the size of the largest clique. You can do a binary search on k, feeding the graph with various values of k. And essentially want to find, the largest value I mean if the graph has an edge when you place two, the answer is yes.
So, find the largest k for which, this is true, this we can do by binary search. You can even go sequentially and do this k equals 1 2 3 4 5 6 stop as soon as the answer is no. The previous one was the largest clique size of the largest clique in the graph, once you found this size.
(Refer Slide Time: 53:57)
 
Let me write this, first the hint I am going to give you more hint, the hint is first design algorithm to find largest cliques size or size of the largest clique. Once you do this, now you use this to solve this. What you do is look at the vertices one by one, throw away vertex and ask does this graph, what is the size of the largest clique in this graph. If it is k, I mean if it is you know same as that of G. So, let me just backtrack a little bit.
So, first you feed this graph G and find the size of the largest clique, let us say k 0. So, k 0 is size of the largest clique in G. Now, I remove a vertex and ask whether, there is a clique of size k 0 in this graph. If yes, I throw the vertex away, if no I retain the vertex. I go through all vertices and at the end, I would have thrown away every vertex except, k 0 vertices and these will form a clique.
This is very similar to finding the edges in the Hamiltonian cycle, if you did earlier. Only here we throw away vertices, instead of edges. So, I would encourage you to, I strongly encourage you to follow through the steps, which I have said and make sure that you can do this. Given an algorithm that solves the decision version of the clique problem, describe an algorithm that solves the search version of the clique.
So, now, the decision version seems to be as hard as the search version of the clique. And from now on, we will focus on the decision version. Can we solve the decision version or not. In fact, our goal is to show that the decision version is among the hardest problems in NP, which means if there is an efficient algorithm to solve the decision version of this problem, there is an efficient algorithm to solve all problems in it.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 29
NP- Completeness ? IV

(Refer Slide Time: 00:55)
 
Let us do a quick recap, we have defined problem called SAT. The input was SAT was clauses C 1, C 2 so on up to C m. Each clause was an or of literals. So, it could be something like x i 1 or x i 2 or so on up to i k. So, we had clauses C 1 through C n, each clause was of this form. And the formula we consider is the think of the formula C 1 and C 2 and C 3 so on up to and C m. And the question we ask is, is this formula satisfiable. In other words, is there an assignment of values true or false to the variables such that the formula evaluates to true. So, this is the question.
So, let me just go over this again, you have clauses C 1, C 2 and C m. The formula we are interested in is C 1 and C 2 and C 3 and so on up to C m. Each of these C i?s looks like this. These are you have x i 1, x i 2 and so on up to x i k. You could also have negations also. For instance if you put x i 1 or x i 2 bar and so on. So, each clause is an or of literals. And the formula is an and of clauses. And the question you ask is, is this formula satisfiable.
In other words, is there a assignment of values true or false to each variable. Such that, when you evaluate this formula, the formula evaluates to true, which means, in each clause when I look at each clause there must be some literal, which must evaluate to true. Either there should be a x and x should be set to true or there should be a x bar, where x is said to be false. If x is set to false x bar is true. So, in each clause there must be at least one literal, which is, which evaluates to true.
Then we say the formula evaluates to true and such an assignment. An assignment to these variables that gives you the value true is called the satisfying assignment. So, the question we ask given a formula is does this formula have a satisfying assignment, the formula of which are of this kind, where you have and of clauses where each clause is or of literals. These formula?s are called formula?s in CNF conjunctive normal form. This all this means all are same. So, this is the problem SAT. And we also had a theorem.
(Refer Slide Time: 04:53)
 
So, this theorem is due to cook and it is called cook?s theorem. It states that, if there is a polynomial time algorithm for SAT. Then there is one for every problem in N P. So, if there exists a polynomial time algorithm for SAT there is one for all problems in N P. So, in other words satisfiability, the problem SAT is among the hardest problems in N P. You can solve this efficiently, which is a efficient algorithm to solve SAT. There is one for every other problem in N P.
So, this somehow identified one problem, which is among the hardest problems in SAT. In fact, there is a name for these problems. So, we call these problems N P hard, these are hard problems in N P. So, let me define this. So, a problem is called N P hard if it has the following property. If it has the above property which means, if what is the property, the property is this. If there exists a polynomial time algorithm for this problem there is one for every problem in it. So, the properties we are looking for is this, if it has a polynomial time algorithm. Then there is one for every problem in N P. To be ((Refer Time: 07:32)) more correct we say that, if there is a polynomial time algorithm that solves this problem. Then there is one for every problem in it. So, these are the hardest among the problems. And hence, they are called N P hard. There is one more definition I need, which is this a problem is called N P complete.
(Refer Slide Time: 07:56)
 
A problem is N P complete if it has to satisfy two things. Let us say, this problem is pi. Pi must belong to N P and pi is N P hard. Then it is called N P complete. So, apart from being among the hardest problems of N P. ((Refer Time: 08:28)) It should be see when you say a problem is N P hard. We do not require the problem to in N P. The problem could be outside N P. As long as it has the property that if you can solve this problem. You can solve everything else in N P it is called N P hard.
But, for N P complete we require the problem to be in it. The problem should be in N P and it should be also be N P. That is the only difference. So, now we can state cook?s theorem in a different way. The theorem due to cook, it states that SAT is N P complete. We saw that SAT was in N P. So, SAT belonged to N P and cook?s theorem essentially says it is also N P hard. So, it means, that putting these two things together, we know that SAT is N P complete.
Essentially, N P complete problems are just to state it again. They are among the hardest problems in N P. They have to be in N P and they are among the hardest problems. So, our goal is to add many problems to this set. This set of N P complete problems and will begin with clique, which also we defined last time. So, let me just define problem clique.
(Refer Slide Time: 10:11)
 
So, problem clique is as follows, the input is a graph G. And the positive integer k. And the question we ask is this. Does G have a clique of size k. So, this is clique, we saw last time that this problem is in N P. We gave there was a proof that, the Prover could give the verifier. So, that the verifier by looking at the proof and the input would check that a given graph, in fact, did have a clique of size k.
Our objective now is to prove the other part, which is the clique is N P hard, which means, if there is a polynomial time algorithm to solve clique, there is one to solve everything else in N P. Again, we saw I think I sort of entire how will do this and this is how we do it. So, what we do is this. We have to use cooks theorem to do this. So, we show the following, if there is a polynomial time algorithm for clique. We will show that this implies then there is one for let me remove this clique.
So, there is a polynomial time for clique will imply that there is a polynomial time algorithm for SAT. So, this is what we will prove. There is a polynomial given there is a polynomial time algorithm for clique will show that, we will construct a polynomial time algorithm for SAT. Now, cook?s theorem says that, if there is a polynomial time algorithm for SAT there is one for everything in N P and we will use this. So, just following these two implications we can see that, we prove what we want. So, all that remains now is I will assume that there is a polynomial time algorithm for clique. And I will construct one for SAT. So, here is how we do it. So, we have a algorithm for clique.
(Refer Slide Time: 13:01)
 
So, this is given, we are given an algorithm for clique. Now, what does this state input. It takes input as graph G, it takes k. And it says yes if the graph is a clique of size k. And it says no otherwise. Now, what we want is this. We want a black box, which will take. So, this is beyond a algorithm for SAT. So, the input to the SAT is a collection of clause, so C 1 and C 2, C m. Now, you want to know whether it is satisfiable or not. Now, what do these two answers, so somehow, we have this sub routine.
We have to use this as a sub routine to solve this problem. So, here is an example, where actually the two things actually look quite different. This is a problem on you know it is something to do with logic. You have all this variables and you have clause this is a problem on graph. So, this is available in a sub routine and we want to construct something. So, how do we sort of use something, sub routine which ((Refer Time: 14:27)) graphs to construct to determine whether a formula is satifiable or not.
So, here is a trick. So, what we do is this. So, we take these clause. So, we take C 1 through C n. Now, somehow I will tell you how, to construct a graph G and some k. Feed this to feed into, so it is called this algorithm clique into clique. And if this says yes, we will say that the formula is satisfiable. If this says no, we will say not satisfiable. So, let us look at this. We take these formulas?s C 1 through C m. We construct some graph G and some k. And we feed this into sub routine feed.
Now, this will say yes or no, if it says yes means we will we want to say that, the formula is satisfiable. It says no then we will say no. Now, how do we do this transformation. From these clauses we have to construct a graph. And from a clique, what we get as output is graph has a clique of size k. Somehow clique of size k should indicate that the formula is satisfiable. So, let us see how this is done. So, you take the formula C 1 through C m. And here is how you construct the graph.
(Refer Slide Time 16:32)
 
So, here are the clauses C 1, C 2 so on up to C m. Now, for each clause I will have a number of vertices in the graph. So, let us say, C 1 x 5. I will tell you what, this x 5 are x 7 bar, C 1, x t bar. If the clause C 1 were x 5, x 7 bar or, or, or up to or x t bar. Then I have a vertex for which each of them. In other words, I have a vertex for each literal clause literal pair. So, a literal appears in a clause I have a vertex for that. Similarly, I have listed them for C 2 so on. So, C m will have it is own set of vertices.
So, for each clause literal pair I have a vertex I have divided the vertex set into these end parts one for clause. Now, what do we want is that, from a clique of size k. We still have not specified what k is from a clique of size k I should be able to figure out that there is a satisfying assignment. So, what does the satisfying assignment look like, the satisfying assignment essentially says, for each clause which of them I can set to true, which literal I could set to true. May be I can set x 7 bar to true, which means x 7 to false.
So, may be here I have C 2 and x 30. May be x 30 I can set to true. Somewhere, down the line let us say, C 45 I have x 30 bar. I could have x 30 here and x 30 bar elsewhere and so on. Now, somehow each of these clauses I want to set one of them to be true. I want to make that one of them is true. And this I should be able to pick out with a clique. So, it stands to reason that I would have a clique of size m. And this clique I should pick by picking one vertex from each of these sets.
I should be able to pick one vertex from each of these sets, if I am picking only one vertex from each of these sets. Then each of these sets may be an independent set. I do not want edges between any of these. All the edges in the graph ((Refer Time: 19:21)) may only go between vertices ((Refer Time: 19:23)). Now, let us look at these two C 2 30 and C 45 x 30 bar. Now, clearly I cannot if I say this is true, I cannot set this to true, this will immediately become false, which means, I should not be able to pick these two in my clique.
Remember, the way we are going to get from a clique to a satisfying assignment is, when i pick the vertices in the clique they will decide, which literal in a clause is satisfied. This is roughly the intuition that I want. So, from each vertex the clique I will pick one vertex from a clause. And you can tell me which i can, which is the literal which is satisfied. So, if i pick x 30 in clique 2, I cannot pick x 30 bar in clique 30 or 37. So, these two I should not be able to pick, which means there should not be any edge between them. Otherwise, I can add all the vertices. So, this roughly actually is the concept. So, let me formally tell you what the construction is construction of the graph is.
(Refer Slide Time: 20:38)
  
So, the vertex set. So, vertex set V is consists of c and l, where literal. Let me, make this c i and l, literal l belongs to clause c i. So, there is one for each clause literal pair, if the literal belongs to that clause. So, what are the edges, this is the vertices edge set is c i, x and c j. y is an edge. If and only if first of all i is not equal to j, because edges flow only between vertices corresponding to different clauses and x is not y bar. Because, if x is equal to y bar. Then I do not I cannot pick both of them in my clique.
So, I can pick both of them in the clique, if they have different variables all together. One is x 15 another is x 30, it does not matter what value I set to it. But, I cannot pick x and x bar together. So, this is the graph and k is m. So, given my clauses, set of clauses I construct this instance of clique. I feed it into the sub routine for clique. If it says yes, it has a clique of size m. When I say yes, the formula is satisfiable, if say no I say the formula is not satisfiable. So, let us actually, formally prove this. So, if we actually, informally argued this, but let us quickly sort of do a recap. So, supposing the formula is satisfiable.
(Refer Slide Time: 23:00)
 
So, if formula is satisfiable, what does this mean in each clause. So, look at any satisfying assignments. If there is a satisfying assignment look at any satisfying assignment, in each clause some literal will be true. So, pick all this, so pick these vertices in the clique. So, we have picked m vertices. So, we have picked one vertex per clause though we have m vertices. And clearly, there will be a edge between any two of them. Because, Firstly they are in different clauses.
And if they are the same sort of they used the same variable. Then they cannot be negations of each other. So, these in fact, form a clique in the graph. And in fact, the other sort of other direction is also similar. So, take a clique of size m, which means there exists one vertex per clause. So, I am going to set these literals to true. So, these literals which corresponds to these vertices set them to true.
And you can check that this can be a satisfying assignment, in this clique with a variable x appears x bar will not appear. So, this will not be a satisfying assignment. ((Refer Time: 25:04)) We will just prove that clique is N P hard we saw last time that clique is empty. So, put these two things together, we have just shown that clique is N P complete. So, the thing to notice is that once we have cook?s theorem in hand. It is much easier to prove some other problem is N P complete.
All you need to do is prove that, if there is a polynomial time algorithm for this new problem. There is one for any of the old problem, which you have already proved N P complete for instance SAT. But, now you can use clique. So, our next problem is called independent set. So, let us define what a independent set is.
(Refer Slide Time 25:51)
 
So, given a graph G, this is the vertex set this is the edge set a subset U of V is called an independent set. If there are no edges between any pair of vertices in U. If all u 1, u 2 belonging to U. u 1, u 2 is not an edge. There are several subsets, so that if I take any two of them, there should not be an edge between them. This I mean for clique he wanted an edge to be in every pair. Here, you do not want a edge between any pair. So, our next objective is to prove the version of the independent set. Decision version of independent set problem to be N P complete, but the search was an independent set essentially states that given a graph find an independent set of maximum size. The decision so we convert this to a decision version in a fashion, which is very similar to clique. So, it looks like this. So, independent set problem independent set has input graph G, the positive integer k. The question you ask is does G, have an independent set of size k. This is the question we ask.
So, the first thing to see is this problem in N P. The well I hope most of you can now show that this problem is in N P. Anyway, let us do it. So, if the answer to the question is yes, we are looking at only yes inputs, inputs for which the answer is yes. And for these inputs, the prover should be able to convince the verifier that in fact, it is a yes input. So, we need to figure out, what is the hint or proof or advice that the prover gives the verifier.
In this case, again it is just the independent set. Then he just picks out k vertices from the graph. And tells the verifier here, these vertices form independent set. The verifier looks at these verifiers. He checks that there is no edge between any of them. So, that is what he verifies and that is very easy to verify. And we convince that, in fact, it is a yes input. The advice that the prover gives the verifier in all most all cases is supposing the question is yes, why is this question yes. You just answer the question.
So, here for instance if it is yes if G has an independent set of size k. So, focus on the independent set of size. If this is given to the verifier and the verifier can check that it is in fact, good. So, this problem is in n P, our job is my job for the time being is to prove that it is N P complete. So, let us do it. So, what we do is this.
(Refer Slide Time 30:01)
 
So, we will the strategies is we want to show, where independent set is N P hard. How do we do this we will this. So, assume there is a polynomial time algorithm for independent set. So, assuming this will construct a polynomial time algorithm or clique. What does this give us? We know that clique is N P complete, which means if there is a polynomial time algorithm for clique there is one for everything in use good. So, that is what we will use. And you can see that we have done, if we can do this step.
Assuming that, there is a polynomial time algorithm for independent set. We construct polynomial time algorithm for clique. And because, there is one for clique, there is one for everything in n p. So, to prove essentially that, if there is a polynomial time algorithm for dependent set, there is one for everything. This is not as difficult as the previous one. And independent set and clique they are both sort of problems in graphs. And we also look feel similar in one case we have an edge, in the other you have subset where there are no edges.
And in fact, this is exactly what we do. So, supposing I have a polynomial time algorithm for independent set. Now, how do I find for a graph as a clique of size k. So, I take this graph, where ever there is an edge, if there is an edge between two vertices I remove it. If there is no edge between two vertices I add an edge. So, I sort of complement the edge set. And in this new graph I feed into the independent sub routine. If it says there is a independent set of size k. Clearly, these k vertices must form a clique in the original value. Because, I complemented the graph, well this is all there is. So, let us just write this down.
(Refer Slide Time 32:38)
 
So, here is the transformation. So, take G and k. So, complement G to get G c. So, what is the complement essentially, the edge set is the same. I have complemented, the vertex set is the same the edge set I have complemented. So, the edge set of G c is, so u, v such that, u v is not E of G. If it is not there I put it in. If it is there I do not put it in. So, this is the complement. So, essentially the complement of E G. Now, keep this into so feed this into the sub routine or the independent set.
And if, this says yes then you output yes. If this says no then you output no. So, we recall that what we want is to find or decide to decide if G has a clique of size k. This is what we are doing, we are going to decide whether G has a clique of size k. We complement the graph feed this into the sub routine for independent set. And if, this independent set sub routine says the yes then we say yes, when we say no, the answer is no. And you can check that this box.
I mean in the sense that, if G has a clique of size k. G complement also will have a size of k. If G does not have a clique of size k, I am sorry if G has a clique of size k, then G complement will have a independent set of size k. If G does not have a clique of size k. And G complement will not have a independent set of size k. So, the answer in both case is correct. Our next problem is called vertex cover.
(Refer Slide Time 35:20)
 
Vertex cover, so what is a vertex cover. So, a vertex cover in a graph G is a set of vertices. Such that, every edge has at least one end point in the set. Is a set let us say, U of vertices, so that every edge has at least one end point in U. Now, let us take an example. This graph let us see, suppose I pick this, then I take this. This let us say that, now, I need to cover this edge. So, hopefully I have covered all edges. So, this edge, this end point is in, for this edge this end point is in, this edge this is in, this edge this is in.
For these both are in, but that is fine. Similarly, for this edge both are in and that is fine this is in the vertex ((Refer Time: 36:57)). So, if I pick these vertices, which are tick marked, that forms a vertex cover in the graph. It is a subset of the vertex set. So, that every edge has at least one end point. For instance, supposing you have a computer network. You have a network and you want to monitor each link. You want to monitor each link.
What do you want to do is pick a set of these nodes. Pick a set of these computers and to each computer you can assign a link. To each computer you can assign a link. So, that it monitors traffic on the links both ways from the computer and from going into the computer. And you want a subset of the computers ((Refer Time: 37:45)) every link is covered, which means for every link at least one of the end points must be chosen to monitor it.
So, this is the vertex cover ideally, we would like to pick very small number of computers to do this job. And in fact, we will look for vertex covers of minimum size. Given a graph find the vertex cover of minimum size is the optimization or the search problem that one looks for. And our objective is to show that this problem is hard, in the sense that you can solve this problem. Then you can solve every problem in N P in polynomial time. So, this is among the hardest problems in N P.
If you want to pick up minimum number of computers to, so that traffic on every link can be monitored this problem is N P hard. Unlike that you will come up with a algorithm. So, that is what we are going to do good. So, I just mentioned the search version of vertex cover, what is the decision version. Remember, if you want to prove something is N P complete, we look at the decision version they are easier to handle. So, here is how we do it the trip is the same. We introduce this extra positive integer k. And we ask if the graph has a vertex cover of size k. So, if it has a vertex cover of smaller size, clearly it has one of size k. So, here is the problem.
(Refer Slide Time 39:34)
 
So, vertex cover input is a graph G, the positive integer k, the question is does G have a vertex cover of size k. So, I will leave it as an exercise for you to do the following as a exercise for you to do, which is supposing there is a algorithm for this problem. And argue that there is an algorithm for the problem of finding minimum vertex cover. It is very similar to the things that we have done in the past.
The other direction is simple. If you can find vertex cover of small size I mean smaller size and clearly given this question you can determine whether, there is a vertex cover of size k. If that size is smaller than or equal to k the vertex cover you found. And there is a vertex cover of size k. So, adding vertices to a vertex cover, it still remains a vertex cover. So, you can form as large vertex cover that is not a problem. You put all the vertices in the vertex cover.
If you take all the vertices in the graph it will give you a vertex cover. So, here is the problem and we would like to prove that this is N P complete. The first thing we need to do is to prove that it is in N P. So, the first step is V C belongs to N P. Why is this true? So, what we do is, so what is we go to our usual game. So, here is the prover, here is the verifier and here is the graph G and K. Now, what does the prover give the verifier? The verifier is looking at this graph G and K.
What does the prover give the verifier? So, that the verifier can verify that there is a vertex cover of size k. Well, the prover just gives the subset. Subset of k vertices, so this is the prover gives the verifier. Now, the verifier looks at the subset. And when he looks at every edge. And checks that, this for every edge one of the end points is in the subset. So, that is what the verifier checks. He takes the subset of size k from the prover.
Now, he goes through the edges one by one and for each edge checks that at least one of these edge these two end points is in the subset and the prover is correct. So, this proves that vertex cover is in N P. Now we need to show that vertex cover is N P hard. So, given we want to show that, if there is a polynomial time algorithm for vertex cover, there is one for everything in N P.
(Refer Slide Time 43:04)
 
So, we will show we will assume that, there is a polynomial time algorithm for vertex cover. So, what we will do is we will construct a polynomial time algorithm for dependent set. Instead of independent set I could have chosen clique or I could have chosen satisfiability. Either of these one of these three problems I could have chosen. In fact, as we go along, we could choose any of these already proved N P complete problems here.
But, trick is to choose the right one. So that, we choose the right sort of problem, then this proving becomes easier. So, we assume that there is a polynomial time algorithm for vertex cover. We will construct the polynomial time algorithm for independent set. Now, how does one do this, what is the relationship between vertex cover and independent set. So, here is my graph G and K. I have a graph G and K and I want to find out if this graph G.
Does G have an independent set of size k? This is what we want to sort to prove that, we want to construct this is the algorithm we want to construct one for independent set. So, supposing this is my graph G. Now, supposing there is a independent set of size k. Let us say, this is the independent set. Supposing this portion of the graph is independent set. Let us, look at what can we say, we know that there are no edges here. There are no edges between any pair of vertices here, where are the edges in the graph.
Well, the edge can be on this side. Or the edges can be of this form. Edges are either completely in this side or one end point is here and one end point is here, which means that this portion is a vertex cover. If this portion is a independent set, this portion is a vertex cover. And in fact, this goes this implication goes the other way. If this portion is a vertex cover and the rest of the graph better be an independent set. Because, there cannot be a edge here. Every edge must have one end point in the vertex cover.
So, if a subset of the vertex set is a independent set. The complement of this set is subset will be a vertex cover. And if it is a vertex cover the other will be a independent set. So, we will use this to prove. So, essentially if you want to find independent set of size k. We will look for a vertex cover of size n minus k, where n is the number of vertices. So, if there is a vertex cover of size n minus k, then clearly there is a independent set of size k. And if there is no vertex cover of size n minus k, there is no independent set of size either. So, that is all there is. So, what we do is we just take the same graph, instead of k we take n minus k. And then we call the algorithm for vertex cover. So, given a algorithm for vertex cover, we are considering one for independent set.
(Refer Slide Time 47:08)
 
So, we take G and K. So, we want to find well we want to determine, if G has an independent set of size k. What we do is we take G prime and K, where the vertex set of G prime is the same as the vertex set of G. The edge set of G prime is also the same as edge set of G. This would be K prime, K prime is nothing but, n minus K, where n is the size of the vertex set. So, this is the same graph as the original. So, we just copy the graph down. And K prime is nothing but, n minus K.
And now, you ask is this pair. For this pair I feed this into sub routine for vertex cover. So, you ask does this is spread into sub routine V C. If it says yes, then you output yes. If it says no, then you output no, that is it. So, G and K are input. So, this is an algorithm for independent set. G and K are input, I take the same graph and I take N minus K as K prime. G prime is the same as G and K prime is N minus K. This I feed into sub routine for vertex cover, which we have assumed.
Remember, we have assumed that there is a polynomial time algorithm for vertex cover. And if this says yes, then our output is yes, if this says no our output is no. And it is easy to see we have argued that, if G has a vertex cover of size N minus K. It has a independent set of size K. And if it does not have a vertex cover of N minus K, it does not have a independent set of size K. So, we have just added new problem to our list of N P complete problems. We have just proved that vertex cover is N P complete.
So, we started out cook?s theorem gave us SAT. SAT was N P complete. And we then showed that clique is N P complete then we showed that independent set is N P complete and now you have showed that vertex cover is N P. So, our next problem is very similar to vertex cover. But, it is a restricted form of vertex cover.
(Refer Slide Time 50:28)
 
So, this is called less than equal to 3 V C. So, the input is a graph G, such that the maximum vertex degree is at most 3 and the positive integer k. The question is same does G have a vertex cover of size k. Without this, if I omit this just graph G and integer positive k, this is just vertex cover. I add this additional constraint to the input. The only inputs I can look for is graphs where the maximum vertex degree is 3. Delta of G is the maximum vertex degree in G. Delta G is max degree of a vertex in G.
So, look at the degrees of vertices in the graph. And delta is the maximum. So, the maximum degree I want is 3. So, this problem is like vertex cover, the only thing is that we focus on graphs on only some graphs, not all graphs. Only graphs which have maximum vertex degree at most 3. Now, is this as hard we know that vertex cover in general is the hard problem.
Is this problem as hard, is it as hard is it much easier to find vertex covers in graphs, where the maximum degree is just 3 and not more than 3. The answer is no. In fact, even in this case the problem turns out to be hard. So, even for graphs where the degree of a vertex does not exceed 3 vertex cover problem vertex cover is hard. We will show that this problem is N P hard or N P complete. To prove that, this is in N P is very similar to the vertex cover thing and I will not do it. So, just do it. I hope most of you can do it.
All of you can do it good. We will just prove that, less than equal to 3 V C is N P hard. Now, which of the problems, which have already been proved N P complete should be used for this, I guess vertex cover is a closest. So, once you sort of once you go for vertex cover I mean it is correct. So, vertex cover we will do. So, what we will do is given an efficient algorithm polynomial time algorithm or less than equal to 3 V C. We would like to construct efficient algorithm polynomial time algorithm for V C general V C. So, let me write this statement now.
(Refer Slide Time 53:59)
 
So, given polynomial time algorithm for less than equal to 3 V C. We want to prove that there is we want to construct a polynomial time algorithm for V C. This is what we want to do. And once we do this, we know that from the previous construction that there is one for everything in N P. So, let me just sort of say what this means, see going the other direction is easy. Supposing, you have polynomial time algorithm for V C.
Clearly, if you restrict the input in any way, it does not sort of I mean you can still feed the same input to the algorithm. But, here you cannot just do that. Your input is a graph and k. Now, this graph could have vertex degrees, which are much higher than 3. What do we do with these? That is the ((Refer Time: 55:05)). If all vertex degrees were 3 then there is no problem, I just feed this into algorithm for less than equal to 3 V C ((Refer Time: 55:13)).
So, what happens when that of vertices with degree greater than 3, 4, 5, 6 whatever up to N minus K largest N minus 1, when the trick is to somehow decrease that decreasing the vertex degrees by creating new. So, given this original graph i construct u graph, where the vertex degrees are smaller. And somehow, you know you must have some control over the vertex cover. The vertex cover in the new graph must be related very closely related to the vertex cover in the older. So, here is the trick. So, I will tell you what the trick is to reduce the degree of the vertex. So, the trick, main trick is degree reduction while keeping the size of the V C, while controlling. The size of V C if there is some controlled over how it changes, when we are here. So, let us take.
(Refer Slide Time: 56:46)
 
So, here is the vertex of degree 5. Somehow I need to that is here is rest of the graph. So, what do we do is this, if you change the degree I break it up into three parts, I break this vertex into three parts. So, here is the rest of the graph. ((Refer Time: 57:08)) three of them here. So, let us see, what I have done. See there are 5 of them I have just broken it up into two parts. Let us say, 1, 2, 3 this is a, b, c, d and e. a b c. So, this one vertex I have broken up into three vertices.
I have added two new vertices. And you can see that, vertex degrees are decreasing. Initially I had one of degree 5. And now, if I look at these vertices, there is one of degree 2, one of 3 and one of degree 4. So, these vertex degrees I have decreased, if this were 4 I would not have one of these edges. Let us say, that a was not there. Then I am done. So, I have three vertices one of degree 2 and two of degree 3. I am done. I have decreased vertex degree of degree 4 vertex.
If it is 5 I will have one with 4. If I have larger again I just split it into two parts. Of course, I have to do this repeatedly. I take this graph I split this vertex into two. So, I decrease the degree. And I keep doing this, till every vertex has degree 3 or less. That is the rough idea. Let us see, what happens to the vertex cover, when I do this split. So, just to recap. I look at this graph look at if there is a vertex of high degree I split it into I split this degree up. I split this degree into two parts. And add a new add another vertex.
So, instead of vertex and I have a three vertices. And the degree has been reduced. Initially, they were 4 ((Refer Time: 59:03)). They have two of them are degree 3 and 1 is degree 2 and so on. So, I keep doing this. Now, I have to somehow say, that the vertex cover does not change much. Even if it changes there must be I must say that it changes clearly by this much. Only, then can I make the connection between the vertex cover in the final graph. And the vertex cover in the original graph.
Final graph has degree of vertex at most three, that is fine. I should also be able to say that, if there is a vertex cover of this size in the final graph. If and only if there is a vertex cover of size k the original graph, and that we will see right now. So, supposing the claim is this. So, here is the graph G and here is G prime, which I get this way. Now, the claim is that G has vertex cover of size K, if and only if G prime has a vertex cover of size K plus 1, which means the size of the minimum vertex cover goes up by 1.
So, this is my each time I use this split the vertex cover size goes up by 1 ((Refer Time: 1:00:28)). So, why is this true? So, let us prove both ways of this inequality. So, let us prove this. Supposing G has a V C of size K. Why should G prime should have a vertex V C of K plus 1. So, take the V C, which G i. There are two cases. So, case 1 is let us call this vertex u. And here let me call these u 1, u 2, u 3. Case 1 is u belongs to the V C. Now, u is in the V C then put u 1 and u 2.
The rest of the vertices are in the vertex cover remain the same. There are some these vertices in the vertex cover in this portion of the graph. And I have used. I put u 1, u 2 and the same set of vertices in the vertex cover. And now, let us observe that this is also a vertex cover. So, the vertex cover size has gone up by 1. If there were l plus 1, l vertices here and 1 vertex here, then I have l vertices here and 2 vertices. So, it is l plus 1. Why is this the vertex cover. Now, if the edges completely in this portion it is covered.
Now, since both u 1 and u 2 are in these edges are covered. And these two edges are also covered. So, every edge is covered. So, this is a vertex cover. Now, case 2, u is not in V C. Then put u 3 in the V C. Now again let us, check this is the vertex cover. Now, all of these vertices all of this edges a u, b u and so on. Have to be covered, which means a b c d e must be in the vertex cover, u is not in the vertex cover. So, all these edges are covered by the vertex cover in this portion. Since, a b c d e is in the vertex cover.
So, the only edges we need to worry about is u 1, u 3 and u 2, u 3 and because we put u 3 in the vertex cover these four also covers. So, this is also ((Refer Time: 1:02:54)). Just to recap, if you is in the vertex cover remove u from the vertex cover and put u 1 and u 2 to get the new vertex cover in G prime. This is the vertex cover. If u is not in the vertex cover, take the old vertex cover and put u 3 additionally in this vertex cover. And this will be a vertex cover in the graph. This proves one direction.
So, we need to prove, the other direction in which that is also very, very simple. Now, just to observe let us look at u 1, u 2 and u 3. Now, there are two cases again. So, only look at the vertex cover here. I will show that the vertex cover of 1 size 1 less on this side. Whatever, vertex cover is there on this side I will show there is 1 of size 1 less. So, the case 1 is u 3 belongs to the vertex cover. We remember this vertex cover is in G prime. Then I just remove u 3.
And you can check the rest of the vertices form a vertex cover for this input. I must say that, only u 3 is in V C, but not u 1 and u 2. So, only u 3 among u 1, u 2, u 3; so that is case 1. u 3 is in the vertex cover, but u 1 and u 2 are not in the vertex cover. Now; that means, that a b c d e all of these edges are present in the vertex cover. So, these edges are taken care of and clearly if there is a edge only in this portion it is taken care of here also. Because, this portion of the vertex cover remains the same. I have just removed them. So, if u 3 is in the vertex cover, u 1 and u 2 are not in the vertex cover, then you are done you just remove u 3.
(Refer Slide Time: 1:05:27)
 
So, the next case case 2, if it is not this case so let us, look at u 3, u 1, u 2. Then I claim that at least two of u 1, u 2, u 3 must be? Two of these, why 2 of these well, I cannot have only u 1 in the vertex cover. I cannot have only u 1 in the vertex cover, because if both u 2 and u 3 are not there, then this edge is not covered. The case only u 3 is taken care of. So, we must have at least two of these. In this case, I remove this and put remove and put u in the V C instead. Then, I get vertex cover of 1 less.
Then, well what if all three are present. Well this cannot be a minimum vertex. If there is a vertex cover of size there is one of smaller size on the right hand side. So, that does not impede our progress. So, when I do this split, the vertex cover may rise by at most one. The vertex cover will rise by at most one by so then the procedure is clear. I start with G, then I do this sequence of these transformations G 1, G 2. Each time I split a vertex into 3. So, I keep doing this as long as there is a vertex of degree 4 or greater.
So, finally, I end up with G l. I have done this l times and I get a graph. So, delta of G l is less than or equal to 3. So, I want to find out, if G has a vertex cover of size k. What I do is I ask if this has a vertex cover of size k plus l. If it says yes, I say yes G has a vertex cover of size k, which says no I say G does not have a vertex cover of size k. And the proof that it works essentially is we have actually done it. Reason is each time we do a split vertex cover rises by 1.
So, this actually, proves that less than equal to 3 V C is N P complete. We just one small thing we need to worry about. Well, you start with a initial graph start splitting, what if there are too many splits. We have got the polynomial time algorithm for this. We started out with the graph. And supposing you construct a new graph you take lot of time. Then this will not work. We will show next time that this is not true and l is bounded. So, l will not be too large. If you show that l is not too large, then the final graph is not so big and. so our entire procedure will run in polynomial time.

Design and Analysis of Algorithms
Prof. Sunder Vishwanathan
Computer Science Engineering Department
Indian Institute of Technology, Bombay

Lecture - 30
NP-Completeness - 5

(Refer Slide Time: 01:00)
 
We were looking at the problem less than equal to 3 V C. So, this problem the input is a graph G such that delta G, which has the maximum vertex degree is at most 3 and the positive integer k. The question we ask is does G have a vertex cover of size k? Our objective was to prove that this is n p complete; it is easy to see that this problem is an n c; the proof is very similar as for the vertex cover. The fact that delta G is less than or equal to 3 does not change anything. So, now we need to prove that if there is a polynomial time algorithm for less than or equal to 3 V C. There is one for everything in it in n p; we have seen that it suffices to prove that if there is a polynomial time algorithm for less than equal to 3 V C. There is one for b c that is what we were doing. The trick was this, so we are given a algorithm for 3 V C. So, we are given an algorithm for less than equal to 3 V C. So we want to construct one for v c, so we take the input graph let us say G.
Now, this could have vertices of varying degrees especially vertices with degree greater than 3. If the vertex degrees are less than equal to 3 then we have no problem. Now, what do we do for large vertices with large degree, we use the split operation. So, supposing I have a vertex with large degree, what you do is you sort of split this vertex into 3 parts. Now, some of the neighbors are attached here and some of the neighbors are attached there. Essentially this is broken up into 2 parts some of them are attached here some of them are attached here. Now, we saw that if this has a vertex cover of size let us say l this has a vertex cover of size less than equal to l plus 1. And if this has a vertex cover of size p, this has a vertex cover of size less than or equal to sorry p minus 1. So, this is the effect of splitting a vertex into these 2 parts. So, the vertex cover goes up by only 1 while we have reduced the degree of this vertex.
(Refer Slide Time: 04:01)
 
So, the algorithm is as follows, so you take G and then construct keep splitting vertices as long as there are vertices with you know degree at least 4 G 1 G 2 so on. So, maybe I will do this l times so this is done l times. So, and in G l I know that there is no vertex with degree greater than 3. So, G l is applied on this algorithm for 3 V C. So, this is now, input to an algorithm for 3 V C and the k, so initially k had a k here. So, now I feed in k plus l and if it says yes then I output yes, if it says no then I output no, so this is the entire algorithm. So, I take G I want to find out whether it has a vertex cover of size k. I do this sort of transformation I get final graph G l I feed this into 3 V C with my new k as k plus l. If it says yes then I say yes if it says no I say no. Now, if this runs in polynomial time I want to show that this entire thing runs in polynomial time. The first thing is that there should not be too many of these steps right if there are large number of these steps the overall algorithm may not be polynomial may not run in polynomial time. So, let us first bound l, so how many times can a vertex get split into 2 parts. So, supposing I have a vertex with let us see this.
(Refer Slide Time: 05:55)
 
So, how many times can a vertex be split into this is the question we want to answer. So, supposing I have a vertex with degree d. So, I could split this once twice, I mean how many times we do this. Well, note that each time the maximum degree will keep decreasing by at least in fact, by 2. So, the maximum number of times I am going to do this is d by 2. You can do it in fact, better by splitting it in half etcetera. So, the maximum times you will do this is actually d by 2 I hope this argument is clear let me repeat this. So, each time I split right the maximum, so if I just look at these graphs the degree of this vertex now, is at least at most d minus 2 rights and the next time I split one of those vertices. Again I will decrease by 2 and so the maximum number of times I do this is d by 2.
The other way to see it is this. So, let us say I have 1 2 up to d first time I split it like this I have 1 2 and the rest 3 on up to d. And now, for this vertex I again split it into 2 parts. So, this portion remains as it is. So, this portion now I split as let us say this is 3 4 and then and so on. And you can see that this is done at most d by 2 times. So, the number of times the vertex of degrees d is split is d by 2. So, the total number of times, so the total number of times a split occurs is summation over all vertices d v by 2 and that we know that is nothing but number of edges in the graph right. So, this half comes out and sum of the degrees of the vertices is twice the number of edges right. So, the total number of times the split occurs is at most if the vertex has degree 3, of course it does not split.
(Refer Slide Time: 09:07)
 
So, here let us go back to this, I know that l is at most m and each time you split you add 2 new vertices to the graph. So, the size of G l v of G l is less than equal to size of v of G and each time you split you add 2 new vertices plus twice l. So, it is at most n plus twice m, the degree of each vertex in G l is 3. So, the number of edges is also some constant times n plus 2 m. So, the size of G l, so the number of edges in G l is polynomial in the size of G. it is not too big you do not do this too many times and final graph that results size of that graph is not too big. So, any running time which is polynomial in that size is also running time is polynomial in the input size. So, this thing runs in time, so we look at 3 V C this runs in time, which is polynomial in this input size of G l, but that is also polynomial in size of G.
So, the whole thing runs in time, which is polynomial in size of G. And now so we need to know show that this works. So, I hope that you are all convinced that this whole thing runs on polynomial time if 3 V C runs on polynomial time. Now, we need to convince ourselves that if this says yes then answer there is yes it says no then the answer is no. Now supposing G has a vertex cover of size k, so let us prove both ways supposing G has a vertex cover of size k. If G has a v c of size k, so this implies that G l has a v c of size less than equal to k plus l right. So, this has a vertex cover of size at most k plus l. So, it will say yes, so this answer will be yes, so this is fine. What we need to do is also the other way round that it does not have a vertex of size k it will answer no, but it is better to do the other way round. So, if this says yes, if 3 V C says yes, this means that G l.
(Refer Slide Time: 12:26)
 
So, 3 V C says yes, which means G 2 has vertex cover of size k plus l. And we know that this implies that G has a vertex cover of size at most k right. So, G has a size of at most k, so this is also same which means G has a vertex cover of size k. So, both ways we have proved that the algorithm worked out. If it has a vertex cover of size k well, here its k plus l G has a vertex cover of size k this implies that G l has a vertex cover of size k plus l. We have done, which means G has a vertex cover of size k then it answers yes and if it answers yes then G must have a vertex cover of size k. So, this proves that 3 V C is.
(Refer Slide Time: 13:55)
 
So, our next problem is exact cover. So, called exact cover XC exact cover here, the input is set S and the collection of subsets S A 1, A 2 up to A m there are m subsets of s. The question we ask is the following is there a sub collection of these sets. So, that the union is S and the sets in the sub collection are disjoined is there a sub collection. So, let us say A I A I 1 A I 2 up to A I p is that 2 things the union must be yes. The union for all j A I j is yes second thing is that A I j intersect A I x i s null set. 
So, you need to pick some sub sets of some of these sub sets in these collections. So, that each element is present in one of them and exactly one of them. So, each element is covered and covered exactly once. So, that is where you get the name exact cover is there a sub collection, which exactly covers the set S, which covers set S in the sense that union is S union of I j is S. And it is covered exactly in the sense that each element is covered exactly once. So, these must be disjoint. So, is this problem n p complete the answer is yes as you must have guessed, because those are the problems we have discussing.
So, let us prove that this problem exact cover is indeed n p complete. So, the first thing to note first thing is to prove that exact cover is in n p to prove that this is in n p. So, what is A S input A S input is just collection of I need a collection. So, that there is some sub collection. So, that a union is S and the sets in the sub collection are disjoined. So, I guess it must be clear what prove the provers must supply the verifier. So, that the verifier can verify that there exists such a sub collection. In fact, the prover just gives such a sub collection the prover tells the verifier what are the sets in the sub collection the verifier takes these sets. He verifies that they are disjoint and he also verifies that the union is S, which means every element of S does appear in one of the sets in the sub collection. So, this proves that this problem is in n p. The blame will prove it is n p hard is to show that if there is a polynomial time algorithm for exact cover. There should be a polynomial time algorithm for less than equal to 3 V C.
(Refer Slide Time: 18:26)
 
So, let us do this. So, given a polynomial time for x c I want to construct one for less than equal to 3 V C. This is our goal, so how do we do this. So, we want to construct for one less than equal to 3 V C. So, I guess the algorithm will take a graph as a input, there is also this k, we know that delta G is at most 3 and we want to know. We want to somehow figure out whether this graph has a vertex cover of size k or not. That is the problem we really want to solve and what we can use is the sub routine is this problem. So, somehow we need to use sub routine to X C to solve this problem on graphs. So, let us write down what a solution to x c and solution to vertex cover look like.
The input to X C is set s and A 1 through A m. This is the input this is input to V C now; here what I want to know is vertices to cover all edges. Here I want sets to cover all elements by sets I mean from this collection A 1 to A m right. So, somehow vertices there must be some relationship between the vertices and the sub sets, so let me say sub sets. There must be some correspondence here and the edges must correspond to elements of S. If there is such a correspondence then it looks like, you know that there is some similarity between these 2 problems. This is what we would like to we would like to exploit, so let us take to attempt at this problem.
(Refer Slide Time: 21:43)
 
So, we know that the elements of s are edges and blind thing and then we take G is the input. So, now, I look at G and I need to convert this input into A input for exact cover. So, I say the set s is nothing but the edge set of G and now, the subset must correspond to the vertices. Each subset here must correspond to a vertex in G and what is the natural sort of correspondence for vertex I, I add a set A I for vertex I. This is nothing but the edges incident on vertex I, if I take A I to be the edges incident on vertex I. Now, I have this correspondence right. So, I have the graph G here and I have s A 1 through A m picking vertices to cover edges corresponds to picking subsets, which cover elements of S.
Elements of edges are covering edges and each subset is like picking a vertex in G. Picking a sub set here is like picking a vertex in G. Now, there is a small problem first problem is I need to pick, you know there is this k floating here. So, I should somehow make sure that make sure that only k vertices are picked. We need this if I just do this there is no sort you know I could pick as many of them I want right. So, a vertex could be I could pick as many of these subsets. So, this fact that in the original problem, I should only be allowed to pick k vertices is the does not reflect in this transformation. So, how do we fix this problem? This problem is actually fixed easily. So, what we do is this, so we take k extra elements.
(Refer Slide Time: 24:26)
 
So, take k extra elements we need to what we are shooting for is pick only k subsets. This is what we want to do? So, take s now apart from G there will be some k extra elements x 1 x 2 so on till x k and supposing the sets k 1 through what I do is I take the, I change the collection of sets. I take A 1 union x 1 A 1 union x 2 so on A 1 union x k similarly A 2 union x 1 so on up to A 2 union x k and finally, A n union x 1 up to A n union x k. Well, n is the number of vertices in the graph that is why it is n here and not m, m is the number of vertices. Well, the number of collections has now, grown to a factor of k initially, we had n sets and now, we have k times n. But now, let us see supposing I pick up an exact cover here supposing I pick up an exact cover from this collection from these I can I can pick any one of them right.
But if I look across row, I can pick only one of them. From this I can pick only one from this, I can pick only one and so on. And from this I can pick only one, so totally I can pick only k sets. So, let me say this argument again now, when I look at this collection then in each set one of x 1 x 2 x 3 x k occurs. Each set one of these k elements must occur and if x 1 occurs in many states, I can only pick one of them, because x 1 has to be covered exactly once right. In fact, exactly one of these sets, so I have to pick a set, which contains S 1 as to pick a set, which contains s 2 I have to pick a set, which contains x 3 and so on up to x k. These are k sets and I cannot pick anymore, because any other set has to contain one of these elements x 1 x 2 up to x n. So, this trick force, you to pick when you pick an exact cover this forces, you to pick exactly k of these sets. So, that is taken care, so is there any other problem well unfortunately there is the problem is that in vertex cover.
(Refer Slide Time: 27:59)
 
So, let me sort of tell you what the problem is, so your problem is this, when you look at a graph I look at vertex cover. You know it may, so happen that there may be a edge between 2 vertices in the vertex cover. So, this is the vertex cover, it may be, it may happen that there is an edge between 2 things in a vertex cover. So, when I pick the corresponding sets A I 1 and so on. This is A I k, when I pick the corresponding sets, then you know this edge gets covered twice. It gets covered when I pick this vertex at this set; it also gets picked when I pick this set. So, this is the problem. So, what we would like to do is when you get to this vertex not pick this edge, but may be pick the other edges, which are not yet been covered. So, I would like to pick these sets 1 by 1 when I get to a vertex I would like to pick those sets those edges, which are not covered and the solution is actually simple.
So, supposing I had recalled that the degree of each vertex is 3 supposing I had a degree of vertex 3 right. So, there are 3 edges, which are adjacent let us say e f and g now, it could, so happen by the time I get to this vertex. I would like to pick this vertex, these edges sum of these edges are already been covered by other vertices. Then I would like to leap at G let us say if e and f are already covered I would like to leap at G for this vertex. If only e is covered I would like to pick only f and g and similarly if none of them are covered then I would like to you know have a set, which contains all of them. So, the trick is to just take all possible subsets. So, take all possible subsets. So, initially I had with the vertex, I had the set e f g. Now, I will have many of them instead of this, I will have now e f g and then all pairs e f so on and finally, e f g.
So, there are 7 of these with each vertex well actually this would have x 1 x 2, so on up to x k also. So, that also has to be put in. So, this set would have e f g in one of the exercise remember our previous construction that x i would also go into each of them. So, it us not 7 subsets, it is 7 k times it is 7 k with each vertex I had k subsets 1 for x 1 1 for x 2 and so on. Now, I have 7 k subsets per vertex I have 7 k subsets per vertex, if the vertex has degree 2. Then it would be less then it would be 3 k, because there are if I take a size 2 the number of distinct subsets, which are not empty is 3. If the vertex has degree one then I would just have one subset, which would translate k other subsets. So, when the vertex has degree k degree 3, I have 7 k subsets per vertex.
(Refer Slide Time: 32:23)
 
So, this is translation I take this graph G and for every vertex for vertex u. I must say that the set is nothing but E of G x 1 through x k for a vertex u. I would have either 7 k or 3 k or k say depending upon the degree of the vertex. So, what is the typical, what does the typical set look like? So, it is a subset of edges adjacent incident on u and one of the exercises. So, I take a subset and x 1 subset x 2 subset and x 3 and so on up to x k. So, this is how each element corresponding to a vertex look like. There could be either 7 k or 3 k depending on the degree of the vertex. And this is now, I take this instance of S and this collection and I feed this into sub routine for x c. If this says yes then I output yes if this says no then I output no. So, this is this is my transformation. Now, to see that this works clearly, if this has a vertex cover of size k I have a exact cover of I have exact cover of this, why is that true?
(Refer Slide Time: 34:35)
 
So, let the vertex cover consist of vertices v 1 v 2 so on up to v k now, for v 1. I put the set x 1 comma all edges incident on v 1 i pick the set with v i. I pick the set x i comma all edges incident on v i, but not on v i so on up to v i minus 1. This is my set which I picked and you can check that this is this is an exact cover x 1 is covered by this set this set corresponding to v 1. x i is covered by v i and so on. x k x 1 through x k is covered and every edge is also covered. Since this is a vertex cover if I take any edge, it must occur between there must be an end point in this there must be at least one end point. Choose the smaller end point supposing it is v I then this edge must have the other end point is either one of v i plus 1 through v k or something else completely different in either case that edge will be part of the set.
So, every edge is covered by by this collection edge is covered here. So, every element of S is covered by this collection. Similarly, if I have a collection which covers let us say b 1 through B k is the sub collection, which covers every element of S. Let us say that B i contains x i x 1 through x k must be covered. So, and they are covered exactly once, so they occur there must be k sets and each of them must contain one of x 1 through x k let us say b 1 contains x 1 and so on. And B I contains x i B k contains x k now, every edge must also be covered. And must occur in one of these sets, which means if it occurs in set B i, it must be adjacent to the vertex I right. So, what I do is the vertex cover just consists of the vertices, which corresponds to each of this each of these sets, which corresponds to B 1. The vertex, which corresponds to B i and so on, so those vertices will form a vertex cover, so this shoes that exact cover is n p complete, why did we pick 3 V C, why not I mean less than equal to 3 V C, why not vertex cover? We have done this with vertex cover well the answer is no at least this reduction does not work.
(Refer Slide Time: 38:06)
 
The reason is that supposing I have a vertex of large degree some degree d the number of subsets, I have corresponding to this is 2 to the d minus 1 times k. This is the number of subsets, remember how we took? How we got the subsets for the vertex? We took all possible subsets here non empty subsets. And we added for each subset I added x 1 x 2 x 3 so on up to x k. So, these many non empty subsets times k is the total number of subsets we get and this can be just very large right. If I have a graph with degree for instance the complete graph as degree n minus 1. This size can be 2 to the n minus 1, which is just too much, which is too much and by the time you write this down, it is much more than polynomial type. 
So, that will it will not work in polynomial time if you take just about extra cover. So, we want this degree to be bounded by a constant, if we use 3 V C and then this is 7 times k which is not 2 bit. So, the total number of subsets we have is actually 7 k times the number of vertices, this is the total number of subsets is at most this much if its instance if the degree is bounded by 3. That is the reason we needed less than equal to 3 V C we have shown that exact cover is n p complete. The main trick was to first show that 3 V C is n p complete less than equal to 3 V C is n p complete, which is a restriction of vertex cover. Then show that exact cover is n p complete one can actually do it in other ways without going through 3 V C and I will let you try this on your own. The reductions become slightly more involved, but you can still do them. After this we will look at problems where sizes come into play. So, that is the next installment.

Design & Analysis of Algorithms
Prof. Sunder Vishwanathan
Department of Computer Science Engineering
Indian Institute of Technology, Bombay

Lecture - 31
NP-Completeness ? VI

(Refer Slide Time: 01:07)
 
The next problem we consider is called subset sum. In this problem the input is a set S with m elements each element has a size. So, you have sizes s 1 s 2 and so on to s m each element has a size. Also you are given a positive integer B we will assume that these sizes are also positive integer. You have a set S with m elements each element has the size m and also part of the input is a positive integer B. The question that we want to answer is this is there a subset subset T of s whose size is B. So, this is a question that we want to answer. Now, what is the size of a subset? The size of a subset is just the sum of the sizes of the elements in the subset. 
Let me define this, the size of T is nothing but the sum of the sizes of the elements e measure in T. So, take all the elements in in the subset T and sum of their sizes this will give you the size of a subset. Now, if you look at size if you look at subsets in s each of them will have some size right. Now, we would like to pick up the subset of s whose size is exactly B this is the problem this is the algorithmic problem that you want to solve and this is the next problem we show is this NP complete. As before let us first prove that this problem is in NP this should not take as too much time what put the prover give that verifier.
So, that the verifier can look at an input to to the subset sum and conclude and convince himself that. In fact, it is a s input. Now, for a s input there there must exist a subset whose set whose size is exactly B right. So, the prover just gives this subset to the verifier the verifier takes this subset sums up the sizes verifies that this. In fact, is B and this verification happens very fast he just has to sum up the sizes of the subset. So, subsets are measured NP our next job is to show that subset sum is NP hard. And to do this we will use the exact cover, what does it mean that we going to use exact cover what we are going to do is this. So, we will assume that there is a polynomial time algorithm for subset sum if there is 1 such we will construct polynomial time algorithm for exact cover. So, this polynomial time algorithm for exact cover will take input for exact cover. Then if we convert it your input for subset sum somehow feed it into the subroutine for subset sum get an output back and then you know given answer for exact cover.
(Refer Slide Time: 04:55)
 
So, let us do this. So, we are given a polynomial time algorithm for subset sum what you want to do is construct a polynomial time algorithm construct 1 for exact cover frame model construct a polynomial time algorithm for exact cover. So, since we are constructing 1 for exact cover somehow we should convert input for exact cover into input for subset sum. So, what are the inputs for exact cover? So, let us see we have exact cover on this side the input is the set T and subsets A 1 A 2 so on up to Am. What a subset sum? Subset sum has a set S that is this positive integer B. And then we have sizes s 1, s 2 up to sm that these 2 m?s are the same is not a coincidence mean I have chosen this anyway let us just proceed.
Now, what do we want to do here in XC? We want to pick subsets to exactly cover elements of the set T you want to pick some of these. So, that this set P is covered exactly what do we want to do here? We want to pick elements of s of size of total size equal to B. Now, here we have to pick subsets it says picks up sets here it says pick elements. So, there must be a correspondence there must be a correspondence between subsets in the exact cover input and elements in the subset sum input. So, somehow subsets on 1 side and elements on the other side must correspond. So, that is the reason why m is I have chosen m to be the same because they will be the same. So, for each subset in the exact cover input there will be an element in the subsets sum input right. So, you want to pick subsets here and you want to pick elements here. So, subsets will correspond to elements.
So, let us write these subsets, it correspond to elements. What is the other thing? The other thing is here I want to exactly cover elements of T here I want to pick 1 of size B somehow exact cover on this side must translate to the subset of size B. This side I want to pick exact cover this side this side I want to pick a subset of size B the crucial thing of course, is what is B, what is B and what are the sizes s 1 s 2 s 3 up to sm? I know that with each element of s what it is a subset of exact cover that corresponds? How do I pick these sizes to make sure that an exact cover on 1 side corresponds to a subset of size B? So, this is what this is a problem that we would like to solve now. So, to solve this let us do a small switch. So, we will solve a different we look at a problem which is very related. And then we show at a solution to this problem can be transformed I mean is exactly what we are looking for a problem is this.
(Refer Slide Time: 09:36)
 
So, you have given a multi set M what is a multi set multi set is a is like set except that elements can be repeated you have multi set M the size k distinct elements let us say x 1 and so on up to xk each element occurs at most l times. Each element each of this can have at most l other copies of itself and we are given a positive integer l. Now, what do we do with this? What we want to do is assign. So, the problem is this. So, assign sizes to each distinct elements to x 1 up to xk such that the only way to pick a subset of size L from M is to pick 1 copy of each element. Let us see what this means. So, I you have a multi set there are k elements and there are l copies there could be l copies or perhaps less there is at least 1 copy and at most l copies of each element and there is a positive integer l this positive integer not given to you. So, let me clarify this is a positive integer you will have to fix also.
Now, what you want to do is give sizes to these elements x 1 to xk I want to give some sizes I also want to fix this positive integer l. So that when from this multi set if I pick any subset this subset can also be a multi set. So, if I remove any subset. So, that the size is exactly l then it must be the case that I have picked exactly 1 copy of each element 1 copy of x 1 1 copy of x 2 1 copy of x 3 and so on up to 1 copy of xk. Then we at least know that what l should be right l then should be size of x 1 plus size of x 2 dot dot dot up to size of xk this we know, because when I pick a subset of size l I must get 1 copy of each element nothing else. Now, these sizes are to be picked in such a way that any other subset of size l if I pick any other subset that must also contain 1 copy of each element if I put 2 copies of any element the size of such subset cannot be l. So, this is the goal. So, how do we do this?
(Refer Slide Time: 13:34)
 
So, let us see. So, we have elements x 1 x 2 let us say x 3 and so on now let us just focus on the first 2 elements let us look at x 1 and x 2. So, without loss of general generality I can say that x 1 has size 1. So, size 1 we have at most l copies of each let us say this is the size 1. In fact, what I want is the sizes to let us say increasing x 1 is the smallest size x 2 next 1 x three. So, I am going to assign them in increasing order x 1 is of size 1 what can be the size of x 2. So, let us look at x 2. So, size of x 1 is 1 what is size of x 2 can it be 1 really not because if its size is 1 I can always replace a copy of x 2 with the copy of x 1 can it be 2 well not really, because if I could replace x 2 with 2 copies of x 1 then I am again failing you know. So, somehow the size should be such that I should not be able to replace x 2 with some copies of x 1 right. So, I guess the smallest we can take is let us say l minus 1 or l if I take l the way to do it the way I could replace x 2 is with all of x 1. Let just look at the 2 element case supposing I just had 2 elements. So, I just had x 1 and x 2 supposing x 1 I said had size 1 x 2 has size l and my capital L has to be 1 plus l.
Now, what can we do? Well, if I pick a subset of this size I cannot do it only with copies of x 1 right there are l of them. So, the total size is is l. So, I cannot pick something of this this size this is my set S using only x one. So, I will have to pick at least 1 copy of x 2. So, if I r pick 1 copy of x 2 then this l goes out of the picture and now well, I have 1 and the only way to fill this is is to pick 1 copy of x 1. So, if my sizes are 1 and l and capital L is 1 plus small l then the only way to pick a subset from this set. So, that the size is 1 plus l is to pick 1 copy of x 1 and 1 copy of x 2 there is no other way we can do this. So, let us see if we can generalize this right. So, supposing we have 3 elements x 1 x 2 and x 3 this well we will still retain it as l what do we think? What do you think we should put here well, there are l elements here each of size l that gives l squared. So, may be l squared is a good, good, good bound here. Of course, anything larger than l also will work here you can check that again anything larger than l will also work similarly something larger than l squared may also work. So, let us check this.
So, l is nothing but 1 plus l plus l squared these are this is when the set has s 3 elements. Now, if I just took x x 1 and x 2 the total size I will get is l times l which is l squared plus 1 times l which is l right. I can only get l plus l squared which means there must exist at least 1 copy of x 3 in in capital L. So, there must be exit exist 1 copy of x 3. There cannot exist 2 copies of x 3 that I can make sure by stating that 1 plus l plus l squared would be less less than 2 l squared and for reasonable l?s this is this is satisfied. So, I can just pick I I can do this otherwise you pick l prime to be maximum of this l and some other constants. So, that this is satisfied we will just assume that this is satisfied. So, I have to pick 1 copy of x 3 now we are down to 2 elements 1 of size 1 the other of size l and it is the same argument. The only way I can I can do this is to pick 1 element of size l and the other element of size one. So, to pick an to pick a subset of size capital L the only the only way to do it is to pick 1 element of size l squared 1 of size l and 1 of size 1 which is 1 copy of each element and. In fact, a straightforward generalization of this actually works.
(Refer Slide Time: 19:36)
 
So, I have I have k elements I pick the size of xi to be l to the i and capital L to be 1 plus l and so on up to l to the k minus 1. Actually this is l to the i minus 1 x 1 was 1 x 2 was l and so on. So, size of xi is l to the i minus 1 and capital L is this. Then the only way to pick something of this size is to pick 1 element of each kind either you cannot do anyway. So, what is this got to do? What is this got to do with our set cover? So, before that it is just easier to prove that exactly 1 copy of the last element has to be for that we will assume that l is less than twice l to the k minus 1. So, for large enough l this is satisfied. So, this l we can take to be the maximum of the initial l that was given and an l which satisfies this. So, what is this got to do with our with exact cover? So, let us let us come back and do this. So, exact cover the input is a set T and I have subsets A 1 A 2 so on up to An these are the subsets. Now, I will choose l to be the maximum number of times an element occurs in this collection. 
So, is the maximum number of times that an element occurs in this collection the number of the number of ele the number of elements is nothing but size of size of T which is n which was k for us there were k distinct elements here there are n distinct elements right. And with each collection there are a few elements and l is just the number of maximum number of times an element occurs in this collection good. So, then how do we fix the sizes? So, the question is how do we fix sizes of each of these these each subsets. So, what we do is we fix the size of these elements first I mean fix the size of elements like this size of each element is l to the i minus 1. So, we think of each element of T and for each element of T for the ith element I pick the size as this what is the size of a subset this is what we really want which is going to be our input. The size of the subset is just the sum of the sizes of these elements right.
So, subset sum so we want to transform this to subset sum. So, we need a set S and an element ei or each subset ki for each of these subsets there is an element ki. Now, the size of ei is nothing but the size sum of the sizes. So, we will think of first fixing sizes of elements of T that is fixed like this. The size of the ith element is just l to the i minus 1. So, now, I can fix size of ei for each x in Ai remember that x is an element of T I just add the size of x. So, if it is the let us say jth element of T its size is l to the j minus 1 right if it is jth element in T in T it is nothing the size is l to the j minus 1 L. 
Then is the other thing or B is nothing but our l which is which is 1 plus l plus l squared and so on up to l to the n minus 1. Now, if this set p has an exact cover supposing this set T has an exact cover then if I look at those subsets corresponding to those subsets. I will have elements in the subset sum if I sum sum up these sizes it will be nothing but 1 plus l plus l squared plus so on up to l to the n minus 1. And for the reverse direction pick any subset of size 1 plus l plus and so on up to l to n minus 1 the only way to get something of this size is to pick 1 copy of each element this this ends a discussion of of sub set sum.
(Refer Slide Time: 26:24)
 
So, let me just do 1 small calculation just in case there was a confusion. So, what is 1 plus l and so on up to l to the k minus 1 this is nothing l to the k minus 1 up on l minus 1. So, instead of summing this up like this you can you can sort of you can calculate it fairly easily. So, this proves that. So, calculating the input is easy and so this proves that subset sum is NP complete I think I mention that we need to choose l. So, that. So, that this is is less than twice l to the k minus 1. So, that can also be done. In fact, you can check that for more most ones this will be true. So, just check that this is true because we just multiply it out you get that l to the k minus 1 is less than twice I just multiply these 2 right. So, twice l to the k minus twice l to the k minus 1 so I will need l thrice. So, I have 3 sorry I have 2 l to the k minus 1 minus 1 should be less than l to the k. So, you can check that for reasonable values of k this is true for l 3 or 4 it is true I can just take some l which is larger than that. So, that was 1 thing which was which I am not completely cover it whose that subset sum is is NP complete.
(Refer Slide Time: 28:55)
 
And the last problem we have in this series is called partition it somewhat similar to to subset sum. So, the input so it is called partition the input is a set T sizes T 1 T 2 up to tn. The question we would like to ask is is there a partition of T into subsets let us say s and T minus s. So, partition of T into 2 parts which is s and T minus s such that size of s equal size of a T minus s. So, this should be exactly half size of T. So, can you partition it in 2 parts both of which have exactly equal size again the size of the subset is nothing but sum of the sizes of the elements in this subset. So, can you partition these into 2 parts so that size of both are equal. Firstly, the total size must be even if let us say all sizes are integers positive integers then the sizes for instance it must be. So, let us assume that these things are true.
Now, this problem is also in NP clearly because if the answer is yes then the proof that the prover gives the verifier is just this partition for each element he tells which part it belongs right. Now, a verifier takes this partition he verifies that the sum of the sizes of elements in the first partition exactly equals sum of the sizes of the elements in the second partition. So, this problem is in NP we would like to prove that this is this problem is NP hard and for most problems where sizes are involved we would like to use subset sum. So, what we show is if there is a polynomial time algorithm for partition there is 1 for subset sum that is what we do. So, let us see. So, there is an algorithm for partition for subset sum the input is are some sizes s 1 to sn and I have B I want to know if there is a subset of size B. So, here is partition here I want to know if there is a partition into 2 equal parts what is this partition say this means that there must be one of size B and 1 of size.
Let us say w minus B where w is nothing but sum of the sizes. So, this asks if there is a partition of this form this asks if there is a partition where they both equal. So, how do we sort of use this to solve subset sum? Well, here is B what I am looking for is B and let us say w minus B what I do is I add say 2 extra elements 1 of size l 1 1 of size l 2. So, that l 1 plus B equals l 2 plus w minus B. Supposing I do this. So, I take this input, input for subset sum I add 2 more elements 1 of size l 1 and 1 of size l 2 right. Now, I ask is there a part and l 1 and l 2 satisfy this l 1 plus B exactly equals l 2 plus w minus B. Now, I ask is there a partition into 2 equal parts. So, I look at this supposing there is a partition into 2 equal parts supposing. So, supposing l 1 and l 2 land up in different partitions you partition into 2 eq 2 equal parts if l 1 and l 2 are in different partitions. 
Then there is a yes there is an answer to the subset sum problem right there is a yes answer to the subset sum problem. So, if there is a yes answer to the subset sum problem then there is clearly an answer to the partition right. I take an answer to the subset sum problem put l 1 along with B and w minus l 2 in the sorry an l 2 along with w minus B then I know there is an answer. Now, if I look at an answer for partition can I construct an answer for subset sum. Well I can if l 1 and l 2 land up in different partitions can I make can I force l 1 and l 2 to land up in different partitions the answer is yes just take l 1 and l 2 large enough. For instance if I take l 1 let us say equal to twice w if I take l 1 to be twice w then you can check that l 1 and l 2 will land up in different partitions. 
Because if l 1 and l 2 both of them land up in the same partition on the other side the maximum size that can happen is w right which is the sum of all sizes the sum of the sizes of all elements in the set set S. So, if I take l 1 to be twice w then l 1 and l 2 this is elements which have sizes l 1 and l 2 must land land up in 2 different 2 different partitions. And the rest of the elements when I look at the rest of the elements the ones with l 1 must sum to be B the ones in l 2 must sum to w minus B and this is the answer that we are looking for. So, this is then a reduction that shows that partition is is NP complete. So, this let me quickly revise what we have done. So, far I have not written a formal proof that partition is NP complete, but I hope that you can fill in the detail. So, let us revise what we have done so far.
(Refer Slide Time: 37:05)
 
So, Cook told us that SAT is NP complete and then we use this to prove that VC is NP complete then we showed that 3 VC is NP complete. Getting from SAT to VC we first showed that clique is NP complete and then independent set and then VC. So, it it is not a direct sort of reduction we first showed that clique was NP complete. Then independence set then vertex cover then 3 three VC then we showed that XC is NP complete then we show subset sum finally, partition. So, let us now look at a problem that we sorted out with. So, remember that you are joined a company and your boss had given a problem where you are given jobs with with time execution time for each job and he wanted to schedule them on let us say 2 processes. So, that the time that last job finishes was smallest. Supposing you could have supposing just was possible mean you actually had an efficient algorithm. Let us say polynomial time algorithm for this problem then you know observe that you can solve partition. The reason is you just take take the input for partition.
The same input you feed into your your algorithm which did the split in to 2 processors the sizes are just given as running time. Now, you look at the partition that your algorithm proof is gives you back if they are both of the same size then there exist a partition of the original set into 2 equal halves right where the sizes are equal if not there is not. So, the problem that you solve is more general than partition. So, if you could have solve that all that problem that your boss gave you then you could have solve partition and by this sequence of reductions that we that we did you could have you would actually have constructed an algorithm for all of NP that seems highly unlikely. So, hopefully when you give this the, this set of arguments or this argument to your boss will be both reasonable and intelligent with intelligent enough to understand and may be give you a raise instead of firing. 
Thank you. This finishes this module on NP complete.

Design and Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science & Engineering 
Indian Institute of Technology, Bombay

Lecture - 32
Approximation Algorithms for NP- Complete Problems ? I

Welcome to the course on Design and Analysis of Algorithms. The topic for today is Approximation Algorithms for NP Complete Problems. So, let me start with a question, suppose we have an NP complete problem, which we need to solve. So, you wanted to solve a certain problem, which arose in some real life situation. And it turned out, that it was NP complete. What you do? This is going to be the subject of the next two three lectures. How do we cope with the problem, which is known to be NP complete? Usually, NP complete problems arise, when we talk about optimization problems.
(Refer Slide Time: 01:36)
 
So, if finding an optimal solution is NP complete. One wonders whether, we can at least get nearly optimal solution in polynomial time. This approach is actually, quite promising. And it is the approach of finding fast approximation algorithms. It will not be interested in this two three lectures in finding the optimal solution. But, we will be interested in finding an approximately optimal solution. And therefore, we will be devising algorithms, which are called approximation algorithms.
And our hope is that for the real life application that we are worrying about, the approximately optimal solution that we find is also fairly useful. Another possibility is to examine, whether the real life problem that we want to solve has additional features that make it a special case of an NP complete problem. If this is true, then sometimes the special cases can have efficient algorithms, can have polynomial time algorithms.
So, for example, vertex cover is NP complete. But, if you are finding about X cover on a tree or on the bipartite graph. We do have fast algorithms, polynomial time algorithms for solving such problems. So, it is useful to think about, whether the real life problem that we are solving has any special features. Sometimes those special features can be exploited to get fast algorithms. Another possibility is to find what are called pseudo polynomial time algorithms. So, let me explain this a little bit more.
(Refer Slide Time: 03:31)
 
An algorithm is said to run in pseudo polynomial time. If it is run time is polynomial in the size of the input instance so far so good. So, far like the usual definition. Here is a difference. The run time is polynomial in the size of the input instance. When, the numbers in the input are represented in unary. In the normal definition or in the definition of polynomial time, we require that the numbers be represented in binary or in some radix, which is larger than 1. What happens, if you represent them in unary?
So, just to clarify, if we have a number 13 that we represent, that we want to represent. In the unary number in the unary representation system. It will be represented by a string of 13 ones. So, note for one thing, that this representation is going to be much longer, than this representation. If you look at binary, this is going to be the representation. But, this is still substantially smaller over here, over than over here. In fact, the number of bits needed over here is log of this.
So, there is going to be a bit difference between the length of your input instance, when measure it in unary or in binary. So, naturally if you are only interested in devising algorithms, which run in time polynomial in the length of the unary representation. You have got a lot more freedom to work with your algorithms can take somewhat longer. Then if they were to be running in time polynomial, in by in their binary representation, when the numbers are represented in binary.
We have in fact see in pseudo polynomial time algorithm in this course. So, this was the napes the dynamic programming algorithm, that we saw for the knapsack problem. Let me remind you, what the problem was you are given n items specified by their weights and values. And you are given an integer capacity for a knapsack. All these where integers, this description applies only 20 ((Refer Time: 05:51)). Now, it was assumed implicitly, that the weight value and capacity are in d bit numbers.
So, since there are 2 n weights and values and one capacity the input size is d times 2 n plus 1. And if you remember, we showed that the time taken for the knapsack problem was O of C times n, where C is the value of the capacity. So, this is crucial it is not necessarily the length of the bit string needed to represent the capacity. But, it is actually the value of the capacity. Now, if these numbers are represented in unary, then the time taken is O of d n. Because, then C would be d bits long.
So, this C itself would be d as would be smaller than d. And therefore, there is no problem, the time taken would be C times n. But C times n is also d times n is at most d times n as well. And this is certainly polynomial in the input size, because the input size is just this. In fact, it is linear in the input size. However, if the numbers are represented in binary what happens. Well, if the capacity is represented as a d bit binary number. Then C can be as large as 2 to the d.
So, then this time O of C times n really could be as large as O of 2 to the d times n. Now, notice that this expression 2 to the d times n is not polynomial in this expression. Whatever, power you take of this, whatever constant power you take of this you cannot beat this and therefore, this is not polynomial. So, clearly polynomial time, if you can get polynomial time it is better than pseudo polynomial time. So, pseudo polynomial time is not the best possible.
Or it is really different from our notion of good algorithms, which are polynomial time algorithms. However, pseudo polynomial time is better than exponential time. So, that is also worth noting, because the length is O of n d. And exponential would be something like 2 to the power n d. So, here we are getting d in the exponent, but we are getting n not in the exponent n just as a multiplier. So, this is certainly, still much better than this. So, as a compromise between polynomial time and exponential time. It is useful to think about whether, there are pseudo polynomial time algorithms, possible for the problem that you want to solve.
(Refer Slide Time: 08:52)
 
Then people do look at algorithms, which are difficult to analyze, but instead of analyzing them. They try out lots of instances and check whether the algorithms run fast enough. This is what I mean by saying we try to discover algorithms, which work well in practice, such algorithms are called heuristics. And they do tend to be useful, when solving problems which are known to be very hard. So, often it may turn out that you may have a good heuristic, which you really cannot analyze.
But, it seems to do the job, if it seems to do the job why not yourself. The last idea, which is also used is to use the exponential time algorithm. So, if a problem is NP complete, we know that it can be solved by an exponential time algorithm. So, we use that exponential time algorithm. If the problem size is small or small enough, then the time taken may be acceptable. Or, if the problem is just to be solved once, then even if the problem takes a day does not matter.
We will run a computer for a day and get a solution. So, this also works sometimes, sometimes for solving real life problems. The real life problems tend to be reasonably small and today computers are getting really fast. So, exponential time algorithms can work, it is not that they are entirely useless. Our focus at these lectures however, is going to be on approximation algorithms.
We would like to device algorithms, which are provably fast which are running in polynomial time, that is all that we mean in this, in these two three lectures when we say provably fast, that there are in polynomial time, and while they may not give the optimal solutions. We will prove that they will give somewhat close to optimal solutions.
(Refer Slide Time: 10:49)
 
So, here is the outline for today. So, I am going to define the notion of approximation algorithms. I will also define a term called the approximation ratio of an algorithm or an approximation factor of an algorithm. And then, I will describe approximation algorithms for two problems. One is the metric traveling salesman problem. And another is the precedence constrained scheduling problem. So, let us begin with the definition of approximation algorithms.
(Refer Slide Time: 11:22)
 
So, let us say P is denotes an optimization problem. P is an optimization problem and it look something like minimize this objective function subject to these constraints. Of course, it could be maximize, but for definiteness let us consider minimization first. Let A of i denote the cost of the solution found by an algorithm A on instance i. So, we are not worrying about the time right now. We are worrying about the objective function cost. So, we want this objective function cost to be as small as possible.
But, this algorithm A, when run on instance i produces this objective function value. Suppose, OPT of i denotes the cost of an optimal solution to this instance i. For technical reasons will assume that OPT of i is greater than 0, we will see why in a minute. Now, we define the approximation factor or the approximation ratio rho on instance i as A of i upon OPT i. What is the factor by which A is worse than OPT i. That is what this approximation ratio is all about. So, it is a natural definition.
Clearly, A of i the cost found by the algorithm can at best be as small as the optimal cost. In general, it could be larger and therefore, this rho sub i is going to be larger. And we would like it to be as close to 1 as possible. In general, the approximation factor of this algorithm is just the maximum value of rho sub i over all possible instances of size n. So, it is customary to use the worst case by enlarge. And so here, as well we are going to look at the worst case ratio.
And of course, it is going to be parameterized by the size n. So, we will write this as rho of n. So, for different n we will have a different ratio. So in fact, we are looking for, looking to evaluate this and we are looking to keep this small. The goal clearly, is to design approximation algorithms or algorithms such that, rho of n is small as close to 1 as possible for large n. And of course, the time for this algorithm is polynomial. The algorithm must run in polynomial time.
Sometimes, you want to maximize the objective function. In which case, we will define rho sub i, the approximation factor as the reciprocal. So, now we know, that A of i can at most be as large as this. And therefore, it will turn out. But, this is still going to be bigger than 1. So, again our goal is going to be similar. So, rho of n is going to be the same. And the goal is also going to be similar. We want algorithms, which keep rho of n as close to 1 as possible, which get rho of n as small as small as possible.
(Refer Slide Time: 15:09)
 
So, now we will use these ideas, to device an approximation algorithm for the metric TSP problem. So, let me define this problem first. The input to this problem is a graph G. And this is going to be specified as an n by n metrics D, in which D of i, j denotes the distance between vertex i and vertex j in this graph. Now, the metric in the title, says that D has to have certain additional structure. Specifically, D has to form a metric and by that we mean, first of all for all i the distance of a node to itself is has to be 0.
The distances have to be symmetric, the distance from i to j has to be the same as the distance from j to i. And the final thing is that for all i, j, k the distance of going from i to j directly has to be no larger than the distance of going from i to k first and from k to j next. This is often called a triangle inequality constraint. So, imagine that i, j, k are the vertices of a triangle. And this just says, that the straight distance going from i to j is smaller than the indirect distance.
So, let me first take an example of what a metric problem is going to be. So, I am not going to draw the metric, the metrics D, but I am just going to take the problem. And I am going to draw the graph corresponding to the problem. So, one way to use such a graph is to imagine that the vertices are embedded in the Euclidean plane.
(Refer Slide Time: 17:10)
 
So, for example, here is vertex 1, here is vertex 2, here says vertex 3, here says vertex 4. I could draw out all the edges, but even without drawing all the edges. Let me tell you, that the distance from i to j is simply the straight line distance in the plane. So, D of i to j is straight distance, straight Euclidean distance in the plane. Now, all of us know that the distance from here to here, plus the distance from here to here can never be smaller than the shorter distances from the straight distance.
And so clearly, our third constraint the triangle inequality constraint is obviously, applicable over here. So, for completeness I could write down this is the graph that here looking at. For example, and if you do the arithmetic you could say for example, that you could calculate the distances. So, this is the graph. And if you look at D i, j to be the Euclidean distance. Then clearly, it will satisfy all these metric property, the properties mentioned over here.
So, this would be a traveling salesman problem instance. What is suppose to be output ((Refer Time: 18:41)). What is suppose to be output is a cycle in the graph, passing through all vertices exactly once. Such that, the sum of the distances associated with the edges in the cycle is small as, is as small as possible. So, this is the same thing as in the TSP problem. You want to tour in the graph passing through every vertex. Such that, the tour length is as small as possible.
The first claim is that metric TSP is NP complete. Well, I think we have studied earlier, the TSP is NP complete. But, it turns out that even with these restrictions. So, this is the special instance of a TSP. But, even with these special restrictions TSP remains NP complete. Here is the claim, that we are interested in and which we are going to prove. So, the claim is there exist a two approximation algorithm for metric TSP. Here is a quick overview of the proof.
In fact, the proof is actually quite simple. The idea is actually quite interesting and but short. So, the general idea is this. And this scheme appears in other places as well. So, first we are going to find the lower bound L, on the length OPT of the optimal tour. So, whatever graph we are given, it has some optimal tour, will try to figure out a lower bound on it. Then, we will construct a tour of length C, which is at most twice this. So, notice that, it is very hard to figure out the length of the optimal tour.
So, we really want a tour, which has say twice the length of the optimal tour. But, rather than that we will find a lower bound, which will be easily computable. And we will show that we can construct a tour, which is at most twice the length. But since, this is a lower bound. We know that this is that C must also be less than twice OPT. Because, L is less than OPT. Therefore, C is less than twice OPT. So, this is going to be what we are going to do. So, we will look at each step in turn. So, this is the first step. So, we want to find a lower bound on the length of the optimal tour.
(Refer Slide Time: 21:00)
 
So, here is the main claim. The claim says that the weight of a minimum weight spanning tree of G, with D as the weight matrix is a lower bound L on the length of an optimal tour of G. So, this is the lower bound that we wanted. So, I just have to prove this. So, let us imagine that we are given any optimal tour. We take that optimal tour and we remove an edge in it, edge from it. What do we get? Well, we will get a path, which passes through all the vertices of the graph once.
It starts at some vertex and it passes through all the other vertex and returns to some other vertex. But, is there anything interesting that we can say about this path. Well, this path is also a special case of a spanning tree. This is a spanning path, it passes through every vertex. And therefore, this also is a spanning tree of G. So, it is length, it is total length is certainly no smaller than the weight of the minimum spanning tree. Because, the minimum spanning tree is by definition, that spanning tree whose weight is the least.
And therefore, the length of this, which is the weight of the corresponding the spanning tree, by the way, in this case weight and length are to be use synonymously. Weight is the terminology used in connection with minimum spanning trees. And length in connection with tours. So, I am sticking to those, stick into that, but really length and weight are the same. So, the length of the path has to be greater than or equal to the weight of the minimum spanning tree.
It will be equal, if the path itself happens to be the minimum spanning tree, a minimum spanning tree. But, the length of the tour is bigger than the length of the path. Because, the tour in fact, contained an extra edge. And therefore, the length of the tour is also bigger than the weight of the minimum spanning tree. But, this minimum spanning tree has beat L. And therefore, we are done. So, the length of the optimal tour is bigger than L.
So, ((Refer Time: 23:32)) we have proved this. We have established a lower bound L on the length OPT of the optimal tour. Now, we want to argue, we want to construct a tour and argue that it is length is at most 2 times L. And once we are done, we will have proved our result.
(Refer Slide Time: 23:56)
 
So, here is have you construct a tour with length less than 2 times L. So, I am going to give you the algorithm. So, first we find a minimum spanning tree, which allows us to determine the L. So, the weight of the tree is L. We can actually, write this down. We can find the minimum spanning tree. And we can find it is weight and that is going to be L, the lower bound. Next, we do a DFS, a DFS traversal depth first traversal of T or do a depth first search of T. And we look at, the sequence of vertices that get visited.
And that sequences return out as E. So, let us take our graph and let us look at that sequence. So, here is our graph ((Refer Time: 24:49)). Now, we start at 1 and if we are doing the depth first search well there could be many ways, in which we do the depth first search. So, first of all I have to identify, what this tree T is going to be. So, clearly this is going to be the tree T. So, this is going to be the minimum spanning tree in this graph the red edges.
Now, if I want to do a depth first traversal of this tree, say starting at vertex 1, what would I get? So, from here, let me just use red again. So, from here I would visit 2. Then from here I would visit 3. Then I would go back, then I would go forward. Then I would go back, then I would go forward. And so the E that I get is going to be something like this. So, I start with 1, then I go to 2, then I go to 3, then go to 2, then I go to 4, then I go to 2 and then I go to 1. So, this is going to be my sequence E, so 1, 2, 3, 2, 4, 2, 1.
So, this is how I have constructed E. Now, the idea is that if D appears, more than once in E. So, there are several vertices, which appear more than once. We are going to delete it is first appearance. So, 1 appears more than once, but this 1 is really the ((Refer Time: 26:28)) same as this 1, because the tour is just closing. So, we do not worry about this. So, the first that appears more than once is this 2. So, now we are going to delete it. And we are going to replace it by the direct edge.
So, we are going to delete 1 to 2 and 2 to 3. And we are going to replace it by a direct edge. Or maybe I will use black this fine, this will be perfectly understandable. And then, we are going to repeat the previous step while possible. So, if a vertex appears several times, we are going to short circuit it, we are going short cut it. So, our current E now is going to be this, we have just removed this. ((Refer Time: 27:14)) So, the next vertex that appears twice is 2.
It already appeared, but it is going to be 2. It could be another vertex, but in this case it just happens to be 2. So, we are again going to delete it is first occurrence. So, if we delete it is first occurrence what does it mean, instead of going from 3 to 2 and 2 to 4. We are going to remove these edges and we are going to replace it with this direct edge. So, we are going to keep on doing this step as many times as needed. And at the end we are going to return E. So, let me draw another picture to show you what this E, that was that is to be return is?
(Refer Slide Time: 28:02)
 
So, this is our vertex 1, vertex 2, vertex 3, vertex 4. So, in our so original tour was 1, 2, 3, 2, 4, 2, 1. So, we removed this 2 and then we removed this 2 and we were left with this. So, our E that remains at the end is going to be this So, going directly from 1 to 3, then going directly from 3 to 4 then 4 to 2 and 2 to 1. So, this is the E that we would be returning. This is the claimed final answer. So, let us go over each step. And we will very about how exactly it is done.
So, ((Refer Time: 29:10)) the first step is done as is finding the minimum spanning tree. So, how long does it take? Well, if we use prims algorithm it will take something like E plus V log V, so clearly polynomial time. How long does this take, this is just depth first search. So, it takes time linear. In fact, so this time is less than this time. So, now let us worry about. So, these steps are again going to be fairly straightforward. So, let us not worry about the time. But, let us worry about the correctness.
So, once we find this E, we eventually we modify it and eventually return E. So, let us try to figure out some properties of E. So, my first claim is that weight of E after the step 2 is going to be twice L. So, that actually, is obvious from this picture. But I will just draw it again.
(Refer Slide Time: 30:19)
 
So, our graph was this. And our tour was this. So, notice that our tour used every edge our tour E, the original value of tour E used every edge twice. This is going to work in general, yes. On every tree it will work, because no matter what do you have. When you do the tour starting from any edge, you go down an edge and then eventually, you come back up. And you have to do it exactly once. So, clearly every edge will appear twice ((Refer Time: 31:01)) in this E.
And therefore, the weight is going to be twice the length of the tree, twice the total weight of the tree. But, the weight of the tree itself is L. And so the weight of E after the step 2 is going to be twice L. The other property about E that is important is that E must contain all the vertices in T. E has to contain all the vertices in T. So, it is a tour the only problem is that it contains some vertices more than once. And that is why, it cannot be it is not a good tour for us.
So, if v appears more than once in E, we remove the first appearance. So, this is good. Because, we are going towards making sure that every vertex appears only once. But, what is this due to E in particular does it do anything bad to the weight of E. So, here is the important claim. The claim says, that after step 3 weight of E is at most twice L. It can only decrease. Now, this is the main part of the argument. And the proof is actually, quite simple.
So, what is the new weight? The new weight is the old weight. And suppose v was the edge we deleted. So, let me take a picture to explain this.
 (Refer Slide Time: 32:25)
 
So, this was a portion of E and this is the vertex V, which we are deleting. How do we deleted, we take the previous vertex, which we call u. Let us call it u, we take the next vertex, we call it w. And we removed these two edges. And we put down this edge. So, what happens to the total weight? Well, the total weight now becomes the old weight plus what we put in or minus what we removed. So, minus what we removed is so minus of D of u, v.
And we also removed this plus D of v, w. And we put in u, w. So, this is the new weight. That is what we have written down over here. But, notice that these are two sides of a triangle. And this is the third side essentially. So, this is the straight path and this is the cross path. So, which of these two is bigger? So, clearly this one is going to be bigger, if at all. And therefore, we know that this entire thing has to be less than or equal to 0 or therefore, the whole thing is at most old.
So, we have proved that the new weight is at most the ((Refer Time: 34:01)) old weight. The old weight was twice L. And so the new weight is also twice L. So, if we keep on repeating this step as many times as we can what happens, the weight keeps on reducing. So, it will always be bigger than, it will always be smaller than twice L. So, we have proved that the weight of E is always going to be at most twice L. So, the final claim is that just before we return of course, the weight is going to be at most twice L.
But, E is also going to have every vertex exactly once. Why is that? Well, we repeated until no vertex appeared more than once. So, clearly no vertex appears more than once. But, initially every vertex did appear at least once. And therefore, finally, every vertex appears exactly once. And so therefore, E is a tour every vertex appears once. And it is weight is twice L. L is lower bound and therefore, we are done. The final issue is there might be some question about the time required for this part.
So, here is a very nice simple observation, which says that this entire thing can be done in linear time. What does this loop do? So, it says, if v appears more than once, delete the first appearance. But, what if v appears several times, then we will delete all, but the last appearance. And this is going to be true for every vertex. So, we are going to keep only the last appearance of every vertex in this traversal E.
But, what is that, we know that. When you do graph search, we should do a post order traversal that is exactly what this is. And therefore, the time for this steps 3 and 4 together is at most the time for a breadth for a depth first search. And therefore, it is just O of the number of edges plus the number of vertices. So, the total time is just simply is dominated by the time for finding the minimum spanning tree. And therefore, it is E plus v log v, say using prims algorithm.
(Refer Slide Time: 36:41)
 
Let us now consider the next problem. The next problem is precedence constraint scheduling, which is also an NP complete problem. And we are going to find a polynomial time approximation algorithm for this. The input to this problem has two parts. The first part is a directed acyclic graph G. Vertices in this graph, represent unit time tasks. And there is an arc directed edge going from u to v corresponding to the restriction that vertex u must execute before vertex v.
So, there is a precedence constraint from u to v. And therefore the name of this problem. You are also given as a part of the input, and integer p, where p denotes the number of available processors. So, p is the number of tasks that you can perform at each step. You may not be able to find that many task. But certainly, you cannot perform more than p tasks at each step. For the output we require to specify an integer time of execution T of u for each vertex u.
Such that, first of all T of u is greater than 0. And at most p vertices has the same time of execution. Furthermore, if there is an arc from u to v. Then the time of u is must be strictly less than the time of v. Remember that, these are integers, so this really means less than or equal to so there is a difference of at least one. And finally, we want to minimize the length of the schedule. So, the maximum over all times is as small as possible. This problem is known to be NP complete for variable p.
So, if p, p changes p is allowed to change as a part of the input. Then this is known to be NP complete. Here is one lower bound. I claim that the length of the longest path in this graph is a lower bound. So, let us do that band. Let us see that and for that let us take an example as well. So, let us take a simple graph.
(Refer Slide Time: 39:32)
 
So, say the graph G looks something like this. So, here is vertex 1, which is one task. Here is vertex 2, which is another task. Then maybe there is vertex 3 over here. And there is an edge from 1 to 3, there is vertex 4, there is an edge from 1 to 4 as well, may be there is an edge from 2 to 4 also. May be there is a vertex 5. And there is an edge and say there is an vertex 6 with these edges. So, this for example is G. So, this is one part of the input. And let us say P is equal to 2. So we want to find a schedule.
So, I claim the first lower bound, which and that is claimed in the first lower bound. That no matter what you do, the length of the longest path is lower bound at the time required. ((Refer Time: 40:35)) So, the idea is actually fairly simple. So, let us identify a longest path over here. So, in this case, the longest path is quite simple. So, say for example, this is the longest path. There are several longest paths, but this is the longest path.
What is this length? Well, we are suppose to measure the length, in terms of the vertices, the vertex length. So, this has length 3. And the claim is that the length of the schedule must be at least 3. Why is that? Well. the precedence constraints says that, if this is executed at step 1. Whatever step it is executed, at this cannot be executed at the same step. So, it has to be executed one step later. This has to be executed one step further than that and so on.
So, whatever the length of the graph is that many steps are needed for this execution. So, that is the first lower bound. ((Refer Time: 41:36)) Second lower bound is based on how much load can be consumed at each step. So, if n is the number of vertices in G. How many time steps, how many how many vertices can be consumed can be worked on by the p processors at each step. Well, at most p and therefore, n over p steps are at least needed.
So, that lower bound in this case is 6 upon 2 which is also equal to 3. So, the first lower bound L is equal to 3 the second lower bound is also equal to 3. So, let us now consider, let us now examine whether in fact, the upper bound for this matches. So, is it match? Well, here is one possible schedule. So, we will schedule this at step 1. We will schedule this also at step 1. And in fact, that is our only choice. Next, we have these 3. So, we can pick say this we will schedule at step 2. This we will schedule at step 2.
This is ready to be scheduled, but we cannot schedule it, because we only have two processors. So, this has to be schedule at step 3. This we have a processor available. But, this cannot be scheduled at step 3, because this has to be scheduled only after this. So, this has to be scheduled at step 4. So, T of 1 and T of 2 are both 1?s. T of 3 and T of 4 are both 2?s. T of 5 is 3 and T of 6 is 4. So, in this case the upper bound in fact, is 4 and it is bigger than the lower bounds.
So, now I am going to describe the algorithm, which will get ((Refer Time: 43:36)) within twice the best possible schedule. And it will use these lower bounds. And it will also use the notion of a ready vertex. So, vertex is set to be ready or ready to be scheduled, if it has no predecessors. Or all it is predecessors have already be in scheduled. So, now I will describe the scheduling algorithm.
(Refer Slide Time: 44:04)
 
So, this is a procedure sched, which takes G and p and it is a 2 approximation algorithm. It produces a schedule, whose length is twice the optimal schedule as we will prove in a minute. So, here is the algorithm actually, it is quite simple, while the entire graph has not been scheduled. We select as many ready vertices as possible, but at most p. For each selected vertex u, we will set T of u equal to i. So, we will schedule it at step i. And then, we will increment the time and then we will repeat.
How long does this whole thing take? Well, the algorithm will take time the time required will be the time to identify this ready vertices. So, the ready vertices will be found by looking at by looking at vertices, which have already been scheduled. I will just say that this can be done very efficiently by doing a topological sort and in fact, you can do the whole thing in time linear in the size of the graph. So, this in fact, will run in polynomial time. So, it is easily shown that a topological sort will suffice.
Let us now, consider whether this is correct. So, is this correct well we are following the restriction about the number of processors, because we are only picking at most p vertices. We are following the restriction about precedence, we are because we are picking only ready vertices. So, this is going to produce a correct schedule a valid schedule. And it is going to run in polynomial time, the only thing that we need to prove that it is a two approximation algorithm.
So, let G sub i denote the graph induced by the unscheduled vertices after iteration i. L sub i is the length of the longest path in G sub i. So, remember that that is a lower bound on G sub i. Let n sub i denote the number of vertices in G sub i. The first claim is either n sub i upon p, which is the lower bound on the ith graph is equal to n sub i minus p, n sub i minus 1 upon p minus 1. So, either this lower bound decreases or L sub i is equal to L sub i minus 1 minus 1.
So, either this lower bound decreases or this lower bound decreases. So, after first iteration we have L sub 1 then we have L sub 2. So, L sub 2 will be either 1 less or this lower bound for the second iteration will be 1 less. And this will be enough to prove the 2 approximation. So, let us prove this.
(Refer Slide Time: 47:00)
 
The proof is actually quite simple. So, the basic step in the algorithm is to find p vertices in that step3 of iteration i. So, suppose it does find those p vertices in iteration i. So, what happens? So, if it finds p vertices, then the number of vertices that remain is going to be p less. So, n sub i is going to be equal to n sub i minus 1 upon p. But now, if you simply divide by p then we will get part a. So, this happens then part a will hold. The other case is suppose that the algorithm does not find p vertices. If the algorithm does not find p vertices, then there are at most p minus 1 ready vertices. So, what are the ready vertices, the ready vertices are the vertices in the graph.
(Refer Slide Time: 48:06)
 
Such that their predecessors have already being scheduled or they do not have any predecessors whatsoever. What do we know about such vertices? Well, what do we know about paths. Here is the key idea every longest path ((Refer Time: 48:22)) must originate on one of these ready vertices. Suppose, it does not, suppose here is a longest path. Well, we go back this is not ready vertex. So, there must be vertex behind it. If there is a vertex behind it, then we are getting a path even longer.
Therefore, by contradiction the longest path must originate over here. So, ((Refer Time: 48:47)) the algorithm on the other hand schedules all these ready vertices. But, if it does schedule all these ready vertices, then the lengths of all the paths starting at these ready vertices, including the longest path must decrease by 1. But, that is essentially saying that L sub i equal to L sub i ((Refer Time: 49:04)) minus 1 minus 1. Thus ((Refer Time: 49:07)) we have proved this, either this holds or this holds. The next claim is that this algorithm gives a 2 approximation. So, here is a proof.
(Refer Slide Time: 49:17)
 
So, remember L was a lower bound the length of the longest path in the entire graph. So, I am going to call it L sub 0. n was the number of vertices in the entire graph I am going to call it n sub 0. The initial lower bounds thus are L sub 0 and n sub 0 upon p. After iteration i the bounds are L sub i and n sub i upon p, and what else to be known. Claim 2, which we just proved says that either the first bound or the second bound drops by 1 in each iteration.
So, starting from L 0 and this n 0 upon p, we go to L 1 and n 1 upon p. L 2 and n 2 upon p and so on. Claim 2 says that either the first one drops or the second one drops. Eventually, until we get to the last iteration. No bound can drop below 0 .Because it does not make sense to say that the length of the path is negative. Or that the number of vertices is negative. So, which means that if more than L plus n over p steps are taken. Then one of these bounds must become negative starting from here.
Because, this L 0 can only drop by L, this can only drop by n over p. So, one of these has to go below 0. But, that is not possible. And therefore, it means that L plus n over p steps must suffice. Our schedule must have length L plus n over p at most. But, what do we know about L plus n over p. Well, this is certainly less than 2 times max of L and n over p. So, we will just replace the smaller of the 2 with the max. So, this is going to be less than 2 times max of L and n over p.
But, what is max of L and n over p. So, this is a lower bound, this is a lower bound. So, the larger of the 2 is also lower bound. So, but if it is a lower bound, then OPT is even bigger than this. So, this is less than twice OPT. So, this max is a lower bound. So, OPT the length of the optimal schedule cannot be smaller than the max. And therefore, we have that L plus n over p is less than 2 times OPT. But, this is the length of the schedule, which we produced. And this length is less than 2 times OPT, so L done. So, we have proved that this algorithm gives a 2 approximation.
(Refer Slide Time: 51:58)
 
So, now I am going to conclude. So, today we discussed various strategies for coping with NP complete problems. The strategy which we are going to study is the strategy of devising approximation algorithms. So, these are defined as giving nearly good solutions rather than the best possible solutions. But, the good thing about them is that the time is polynomial. There are various techniques for designing approximation algorithms. And the techniques that we studied today can be summarized as follows.
So, basically we try to find lower bounds on the objective function, which we want to minimize. And then we try to get close to this. Of course, if the objective function had to be maximize and we will try to find upper bounds and we will try to get close to those. So, device we will algorithms in this case, which will get close to this lower bounds. So, the lower bounds are easily should be easily identifiable. And therefore, we can actually compute them.
And then, we can may be try to target an algorithm, which tries to meet them. But, of course, it will not succeed in meeting them. But, it will try to it will succeed in hopefully getting close to them. We will see more such techniques in the next lectures.
Thank you.

Design & Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science & Engineering
Indian Institute of Technology, Bombay

Lecture - 33
Approximation Algorithms for NP - Complete Problems ? II

Welcome to the course on Design and Analysis of Algorithms. This is the second lecture on Approximation Algorithms. We already saw in the previous lecture, what are the approximation algorithms were? We will go over that very quickly today. And we will take two more examples. So, let me quickly remind you what an approximation algorithm is.
(Refer Slide Time: 01:20)
 
So, an approximation algorithm is a polynomial time algorithm, first of all. And it solves, an NP complete optimization problem. And it gives near optimal solutions. The solutions it gives are near optimal. Of course, if it gave optimal solutions in polynomial time then, that would prove p equals NP. And of course, we are not here to prove that. And most people in fact believe that, that is not correct.
So, the most people believe that p is not equal to NP. And therefore, polynomial time algorithms which give optimal solutions are unlikely to be there for NP complete problems. So, last time we looked at this and we define some notation for this. So, let me just quickly go over that notation.
(Refer Slide Time: 02:29)
 
So, let us say p is a problem. And A is an approximation algorithm for solving it. We said last time that, if i is an instance then A of i is the cost of the solution found by A on instance i. So, here we are going to assume that our problem is a cost minimization problem. So, there are some constraints which are specified as a part of the problem. And a cost function is given. And our goal is to minimize this cost.
As we said, this cost in general will not be the optimum A i, will not be the optimal for instance i. But in fact, there is some other cost, which I will call OPT of i which is presumably better than this. So, this would be the cost found by the optimal algorithm. So, clearly we know that A of i is at least as big as optimal as OPT of i. So, we said that rho are the approximation factor on instance i, is simply going to be A of i upon OPT of i.
And we also said that, rho of n is going to be defined was defined as Max over all instances of size n of rho sub i. So, this is the approximation factor of the algorithm. And this is what we want to keep small. So, this is the general framework in which we are going to work. So, we are going to devise algorithms for NP complete problems, which produce a small approximation factor.
In other words the cost that they return, the cost of the solution that they return A of i is going to be reason reasonably close to OPT of i. And of course, they are going to be running in polynomial time in path of i. So, today I am going to look at two problems.
(Refer Slide Time: 05:22)
 
So, one problem is going to be the Set Cover problem. And another is the, is going to be so called the k Center problem. I will describe these problems in a minute. And I will also give real life examples, corresponding to these problems. In addition to that, we are also going to look at some techniques or some themes. First of all, we are going to use some kind of a greedy idea for solving these problems.
See you have already seen greedy strategies earlier. So, a greedy strategy is, in greedy strategies procedure for solving is to be thought of as a sequence of decisions that you make. Every decision is going to change the cost a little bit. And the goal, the idea of the strategy is that the ith decision that you take, tries to make the ith change in cost the ith increase in cost, as small as possible.
Greedy is also short sighted in the following sense that, we will try to minimize the ith increase in cost using the ith decision. But in doing so, we will not worry about what happens subsequently. In general of course, greedy algorithms will not work. But, in many cases they seem to work. And today?s problems are somewhat in that same framework. In addition to that, we are also going to see an interesting proof strategy which I will call Compete with the Optimal.
So, we are going to imagine that, there is an optimal algorithm running along with the algorithm that we design. And we are going to try and do at least reasonably well, as compared to that. We cannot do better than or even as well as the optimal algorithm but, we will try to see that we do not do too badly. So, let me start off with the set cover problem. So, here is the set cover problem.
(Refer Slide Time: 08:00)
 
The input consists of sets, s 1 s 2 all the way till s sub n. Let me use, U to denote union of s 1 s 2 s n. So, let me call this collection this entire collection I am going to call C. So, this is the collection of sets. And our goal, the output that we want is a sub collection. Let me call it C prime. Such that the union of s sub i, where s sub i belongs to C prime has to be exactly equal to U or the union of all the sets.
So, we want a small collection C prime. A small set of sets such that, they contain the same elements as the original connection. So, what is to be there is something to be minimized over here. And we want to minimize the cardinality of C prime. So, this is the set cover problem.
(Refer Slide Time: 09:53)
 
In the decision version of the problem, so the decision version as usual we are given an additional target t. So, this is the number of sets allowed in C prime. And what happens over here is that this. And we are asking does there exist a C prime with cardinality t. Such that its union, the union of the sets in it is U. And this problem is NP complete. In fact, the decision there is a reasonably simple reduction from the vertex cover problem.
We are not going to prove this. But, we are just going to directly try and find an approximation algorithm for the set cover problem. Now, before going to the algorithm let me give you a brief application, a small application of this problem to tell you that, to tell you how it might arise in real life.
(Refer Slide Time: 11:07)
 
Suppose U denotes a set of villages. Suppose we are also given a set L of locations, here hospitals can be built. For each location say little l, we are given the set S L. And this is the set of villages that will be served, if a hospital is built at L. The natural problem now is to determine the smallest number of locations such that, all villages can be served. Let me write the term. We want to pick as few locations as possible. And build hospitals there, such that all villages will be served.
Do you see now, how this corresponds to our set cover problem? Well, the correspondence is actually exact our sets S L constitute the collection. These are the sets in the collection as defined in the problem, in the set cover problem. And what we want is to pick a sub collection of this collection such that, all villages will be served or in other words all villages will appear in at least 1 S L. So, the correspondence is exact. So, now we turn to the algorithm.
(Refer Slide Time: 13:45)
 
So, the basic idea of the algorithm is greedy. What do I mean by that? Well, we are going to think of picking sets when at a time. And every time we pick a set, we will try to cover or to collect together as many elements, which have not yet been collected. So, here is the idea. So, here is basically the algorithm. So, I am going to start by defining my c prime to be null.
So, c prime is going to be the collection our answer eventually. I am also going to say that, all elements of U which is the union of all the sets are initially uncovered. By covering, I mean they are inside a set which is included in this c prime. And now, here is the basic loop. So, while some uncovered elements exist, what do we do? We pick set S i that contains maximum uncovered elements.
So, we will have to maintain some data structure which says, which keeps track of which elements are covered and which elements are not covered. And as soon as we pick elements and we put that, pick a set and we put that set into c prime we will have to cover those elements. So, we pick the set. And then, we set c prime is equal to c prime union S i. And then, we uncover or recover elements in S i.
So, our universe or the ground set, has some elements has all elements initially uncovered. Then, we pick sets as a result of which some elements get covered. And we keep on covering elements. And eventually, we get to a point where all elements are covered. And that is a time, we stop. So, this is where we end the loop and we return c prime. So, we are making the decision at each stage which said to include into our sub collection and at each stage we are saying.
So, let us include a set which gives us which tries to cover maximum elements. So, the analysis of it is actually quite nice and simple. The analysis goes by iterations of this basic algorithm. So, I am going to start by defining some notation. So, let say N sub 0 is the number of elements in U, the total number of elements which I want to cover or this is also equal to N. There is a reason why I want to call it N sub 0 as well.
N sub i is going to be the number of elements, uncovered after iteration i. So, number of elements in U originally, these are all uncovered. So, this is uncovered before the first iteration. N 1 would be elements, which are uncovered after the first iteration after the second iteration, all the way till after the ith iteration. I am going to use OPT to denote, number of sets needed by optimal algorithm to cover all elements.
So, this is my notation. And now I want to state my main claim. So, the main claim is the following. The main claim says that, N sub i plus 1 is less than or equal to N sub i times 1 minus 1 over OPT, very simple claim. And the proof actually is also fairly simple. So, how does the proof work.
(Refer Slide Time: 19:30)
 
So, our algorithm has executed for i iterations. And this is our set U of which some elements have been covered. And these are the N sub i elements, which are not covered after the i, after the ith iteration. Now, what do we know about the optimal algorithm. As we said our strategy is going to be, we will try to do we try to compete with the optimal algorithm. So, what we know about the optimal algorithm?
Well, we know that this entire set U is covered by OPT sets, by a number of sets equal to OPT. So, it covers all these elements. So, therefore know that even this region is also covered by OPT sets. So, now if I look at how these sets are covering this region? I must claim that, there has to exist at least one set which covers N sub i upon OPT elements at least. So, at least one set covering at least. Because, if no set covered at least these many then this group of N i elements would not be, it would not be possible to cover this.
So, there has to exist at least one set which covers at least N sub i upon OPT elements. Here is where the greedy property now comes in. So, at this point we choose a subset, which covers the maximum number of elements from this. So, which? So, what do we know about the set? So, this subset that we choose is this. Then, we know that this region has to contain at least N sub i upon OPT elements.
(Refer Slide Time: 21:53)
 
So, what does that mean? That means that number of elements remaining after i plus 1 th iteration has to be less than the N sub I, which were present before the i plus 1 th iteration minus whatever got covered. And these are N sub i upon OPT. So, in other words N sub i times 1 minus 1 over OPT. This is precisely what we claimed a few minutes ago.
(Refer Slide Time: 22:36)
 
So, this claim has been proved. So, let me write down our second claim just here. Our second claim is that cardinality of c prime, number of sets returned by our algorithm is less than or equal to OPT times l n N. So, it is OPT by log N log of log N factor, log taken to the natural the base E. So, we will return c prime elements. Whereas, OPT will return the optimal algorithm, will return OPT. And we will not be returning too many, too much worse. We will not be doing too much worse. We will be doing only an l n and N factor worse. This is our second claim, this is what we want to prove next.
(Refer Slide Time: 23:41)
 
So, how many elements to be returned? Well, if the algorithm runs for t iterations then, we return cardinality of c prime is equal to t. So, what we need to evaluate is how many iterations, does the algorithm run for. So, suppose it runs t iterations then, what do we know about the number of elements that are uncovered after t iterations. Well, we know that N sub t is that number. And we know from our first claim that, N sub t is less than N sub t minus 1 times 1 minus 1 over OPT.
But, we can keep on repeating this. And so, therefore we get this is less than or equal to N sub 0 times 1 minus 1 over OPT whole to the power t because, this factor will keep on repeating. And note of course, that this is just N. Now, nice little inequality comes to our rescue. And that inequality is 1 minus x is less than e to the power minus x for x not equal to 0. So, what does that allow us to conclude?
So, therefore we can conclude N sub t is strictly less than now N times. So, this is 1 minus 1 over OPT. So, 1 over OPT is going to be my x. So, I am going to have e to the power minus t upon OPT. Now, let us check what we what happens if t is equal to OPT times l n N. What happens then? So, this is nothing but N times e to the power minus l n N times OPT upon OPT or this is N times e to the power minus l n N. And therefore, this is strictly less than 1.
So, we have proved that N t is strictly less than 1. But, if N t is strictly less than 1 what does it mean? That means, that N t is exactly equal to 0. What is that mean? That means, after t iterations all the elements of U have been covered. Nothing has been left uncovered. But, this happens for t equals OPT times l n N. So, in other words cardinality of c prime is OPT times l n N. And that is exactly what we claimed.
(Refer Slide Time: 26:40)
 
So, this finishes the analysis of the set cover algorithm. So, what we have proved over here is that, we can also find a set cover. The size of that set cover is going to be worse by a factor l n N. We now come to our second problem. The second problem is the so called k center problem. I do not want to use the letter k, because k comes in handy for indices and things like that. So, I am going to call it as the t center problem.
(Refer Slide Time: 27:19)
 
This time, I am going to describe the problem informally first. And then, I will formalize it. So, let us say let us take an example. The example is that, we are given some statistics say about students in a class. So, perhaps we are given a plot. So, on one axis we say have the height on the other axis may be we have say the weight. So, may be the class has some number of students and may be the plot looks something like this.
So, this is the plot of the heights of different students. Now, an important question in analyzing this data statistically, is to see if there are any clusters. So, say for example you could think, that this region is consists of students who are somehow similar in their build. This region consists of students, which are who are somehow similar in their build. So, it would be. So, it is interesting statistically.
It is see, if this entire distribution can be described just by using say two representatives. And the question then is, is there a good way of choosing those representatives. For example, we could say that this could be one of the representatives. And say, we could say that this is another representative. How do we choose a representative? Well, a representative is chosen, so that its distance from the rest of the elements in the cluster is as small as possible.
And furthermore, we decide to put an element. Say this one into this cluster because, it is closest to its representative. So, the two the thing to the important thing to note over here is the so called radius of each cluster. So, this cluster has this is the farthest element in this. And therefore, this is the radius of this cluster. In this say for example, this might be the farthest distance. And so, this is the radius of this cluster.
And so, the maximum of these two is the so called radius of this clustering. So, what is our goal? Well, we are given how many clusters are needed. And we want to pick the cluster centers and divide the data into clusters. So, in this case this is one cluster and this is going to be another cluster. Here the clusters are very obvious. But, in general they will be mixed and finding representatives and defining the boundary between the clusters, is going to be a little bit hard.
But in any case, we want to define the boundary and find a center the representatives and the radius, the radia of the clusters such that, the largest radius is as small as possible. So, that is how we measure the goodness of the clustering problem. So, we are going to do this but, before that I would like to give you the formal definition of this problem.
(Refer Slide Time: 30:48)
 
So, the input consists of a set P, which is a point set. Let us say, this point set is in some D dimensions. And it has say some n points. And we are also given an input parameter t. This is the number of clusters desired. Before I describe the output, I need to have good notion of distances and things like that. So, the distance between two points is going to be simply there, Euclidean distance. So, it is a d dimensional space.
It is going to be square root of the sum of the squares of the coordinate differences as usual. But here, we are also going to be talking about the distances between a set of points and a point. So, let me define that or a distance between one set of points and another set of points. So, let us say s is one set of points and s prime is another set of points. So, I am going to define the distance between two point sets. As the minimum, the minimum distance between p p prime, where p is in s and p prime is in s prime.
So, these could be in general arbitrary point sets. But, we want to see are there two points at least which are too close or very close. And that is the distance that if we are going to pay attention to. So, let me now tell you what the output is.
(Refer Slide Time: 32:49)
 
So, we had the set P of points. So, the output is a set c which is a subset of P such that, cardinality of c is equal to t remember t was number of clusters. And what we want is, that d of c to p should be as small as possible. So, I am going to take the maximum over little p belonging to capital P of this distance. So, I am going to ask which is the point which is farthest from these centers, that is the distance which I want to minimize?
So, max P in p is smallest possible. So, what is the intuition over here? So, the c is the c that we want to select are the cluster centers. Every point is at some distance. So, when we ask d of c comma p, we are asking which is the closest center for this point? So, of the closest centers I am going to take that distance. But, now I am going to ask, which is the point which is farthest from its closest distance? So, which is exactly, what we talked about earlier?
(Refer Slide Time: 35:00)
 
So, what has happened over here is that, this is our set of points. So, we pick some centers. And for each point we are asking, which is the closest center? And we are taking its distance and we are minimizing this maximum over all distances. So, let me also define the radius of each cluster. So, first of all let me define each cluster. So, if a point is closest to this center then, I am going to say that this point belongs to this cluster.
So, that is how clusters are defined. And of all the points which are belonging to a certain cluster, I just look at the maximum distance. And that is the radius of that cluster. Let us now describe, let me now describe an algorithm for this problem. So, this is the clustering algorithm.
(Refer Slide Time: 36:00)
 
So, what are we suppose to do? We are supposed to select the centers. We are supposed to select t centers such that, no point is too far away from any of the centers. And we are going to use a greedy strategy for solving this problem. So, how do I select the first center? Well, for that we do not really have really good idea right now. So, let us just say that it is picked arbitrary. But note that, if I pick some k clusters.
So, I have picked c 1 c 2 c k already picked. Then, if I want to pick c k plus 1 there is an interesting greedy idea that can come into play. So, what is the greedy idea? Well we ask, what is the farthest point from these first k clusters? We picked first k centers. And we want to know, which is the point which is farthest from this k centers? So, this point which is farthest is at some distance R.
Then that in fact, is the radius of this clustering that we have produced. So, now we want to reduce this radius of this clustering. How do we do that? Well, here is a simple idea. We pick c k plus 1 to be exactly that point. This is the point, farthest from c 1 all the way till c k. So, let me now have some. So, that is why that is basically the algorithm. So, I will pick c 1 arbitrarily. Then, I will pick c 2 to be the farthest point from c 1.
Then, I will pick c 3 to be the farthest point from c 1 as well as c 2. Then, given k points I will pick the k plus 1th point, to be the farthest from all these k points. Let me now define some notation. So, my notation is as follows. I am going to use R sub k to be the clustering radius, which is what we want to keep small after k points are picked. What do we know about this clustering radius? So, we know that R k plus 1 is less than or equal to R k.
Why? Because, that is what we exactly did. We pick the point, which is farthest and we made it into a cluster. So, very likely now the farthest point the distance of the farthest point has reduced. And therefore, we expect the k plus 1th clustering radius to be smaller than R k. Well, what do we know about R k itself? What is what do we know about the clustering radius R k? This in fact, is the distance from c 1 c 2 c k to c k plus 1. Because that is exactly, what we did.
We looked at which was the farthest point after picking, having picked the first k centers. We picked at we looked at the farthest point. So, this was the point p. And we said, what was that distance. So, this is exactly that distance. And therefore, that must have been the radius after having picked, the first k points. And what we have argued earlier is that, the distance is going to keep on decreasing. So, these two are the important facts about how the radius of clustering change is related to each other and to the distances and to the centers. So, we are going to use these facts in a minute.
(Refer Slide Time: 40:55)
 
So, let us now analyze this algorithm in more detail. As we said earlier, this algorithm the analysis of both these algorithms is going to be based on this compete with the optimal strategy. So, we are going to try and see what the algorithm, what the optimal algorithm would do. So, let me just remind you that, c 1 c 2 c t are centers selected by our algorithm A. Let us say, o 1 o 2 o t are centers selected by the optimal algorithm.
So, we would like to see how these centers are related to each other. The interesting thing is that, we do not know what o 1 o 2 o t are. But, nevertheless for the purposes of the proof we will assume that, we know them. So, we are not actually going to compute them any time. But, for the purposes of the proof we know them. And then, based on that we will be able to argue, what the relationship between the two clustering is.
So, we have those two we have the center selected by c 1 by the algorithm. And the center selected by the optimal by our algorithm and then by the optimal algorithm. So, let us focus on the centers selected by the optimal algorithm.
(Refer Slide Time: 42:23)
 
So, say this is o 1. This here is o 2. This here is o 3. This may be here is o 4. May be this is o 5. And let us also look at the corresponding clusters. So, say these are the corresponding clusters. I am not going to draw all the points inside but, I am just drawing the corresponding clusters. So, now let us ask how do the centers that we selected relate to these clusters. A natural question to ask is, is there exactly one center we select from each of the optimal clusters or do we perhaps select two centers from each cluster.
Natural guesses to say that perhaps, we just select our clusters our centers are also selected such that, that one comes from that each one comes from the other, from the optimal cluster. Of course, it could be this or it could be the case that we select two centers. Two of our centers are selected from the same cluster, as the algorithm selects as the optimal algorithm selects. So, basically there are two cases.
(Refer Slide Time: 43:46)
 
Case 1, each optimal cluster contains 1 c sub i. Or case 2, each optimal cluster or some optimal cluster contains say c sub i and c sub k. So, in fact let us call this optimal cluster o sub j. You might think that, we also need to consider a third case. Some optimal cluster contains no centers picked by our algorithm. But, note that in this case some other optimal cluster must contain at least two of our centers.
And so in fact, we will be having our case two in this possibility as well. So, let us now analyze case one in detail. In fact, each optimal cluster contains 1 c i. And we are free to give the optimal clusters whatever, names we want. So, we can say that each optimal cluster o sub i contains, exactly 1 c i. And in fact, we will name it correspondingly.
(Refer Slide Time: 45:36)
 
So, let us come to this picture over here. So, in this picture may be what we are looking at is, this is c 4 this is c 3 this is c 5 c 2 c 1. So, the algorithm happen to choose center such that, exactly one point is picked from the clusters that the optimal algorithm would have chosen. So, this is the first case that we are looking at. So, basically the idea now is that we will prove that in both cases. We will not do much worse than, what the optimal algorithm did.
So, now let us pick point p in one of these clusters. What do I know about the distance of this p from c 3. I know that, this distance is at most the sum of this distance plus the sum of this distance. But, what is this distance itself? This distance is at most, the radius of this cluster. This distance is also at most the radius of this cluster. So, that means that this distance is at most twice the radius of this cluster.
(Refer Slide Time: 47:17)
 
So, what is the important observation that we have made. We have established that, if P is a point in optimal cluster, c cluster O i capital O i. Then, the distance of P from c i is at most two times the radius of O i. But, this is nothing but, this is certainly at most 2 times the max radius of the optimal clustering. So, we are going to call this R sub OPT. So, this is 2 times R sub OPT. So, we have really proved what we wanted.
Surprising as it may seem, we have proved that this point is close to some center not necessarily the center with which it is associated, in the final clustering that our algorithm produces. It might be associated with some other center. But, its distance to this center c i is itself less than 2 times R OPT. And this is true for every point. And therefore, in this case we have in fact established the theorem which said that, which says that.
(Refer Slide Time: 48:49)
 
So, we have proved that the distance of the quality of the clustering, our quality of clustering produces clusters of radius at most twice the radius produced by the optimal cluster. So, now we come to case 2. So, case 2 says some optimal cluster O j contains c i and c k.
(Refer Slide Time: 49:16)
 
So, let us draw picture. So, here is my optimal cluster. Let me call it capital O j. Here is little o j, its center. And suppose that it contains cluster centers c i and c k which our algorithm picked. Well, what do we know about distance from c i to c k. So, I claim that this is greater than or equal to the distance of c 1 to c i. And I am going to assume here without loss of generality, that i is less than k. So, the distance of this set c 1 to c i or rather c 1 to c k minus 1 with center c k.
So, here I am only taking the distance from c i to c k. Here, I am allowing a large number of other points to come in. So, this distance can only be bigger than this distance. But, what is this distance. So, this distance from what we argued earlier is something quite is related to our clustering. So, we argued that R k plus 1 has to be less than R k, which is this. So, comparing these two we can argue that, this distance has to be bigger than.
So, this distance is nothing but, R k minus 1. But, this R k minus 1 is certainly bigger than or equal to the final radius, that the algorithm got. So, what we have argued is that the distance between these two clusters, if they happen to fall within the same optimal cluster. So, distance between any two clusters, any two cluster centers is going to be. The optimal is going to be bigger than the optimal radius produced by our algorithm.
Here we have not yet use the property that these two centers actually lie in this single optimal cluster. So, we will use that property now. So, what do we know about the two, these distances. So, we know that this distance, again by the triangle inequality is at most this distance plus this distance. So, what do we know? We know that, d c i c k is less than or equal to the distance from c i to o j and the distance from o j to c k.
But, what is this tell us. So, this lies inside this cluster. And therefore, this is at most the radius of this cluster. This is at most the radius of this cluster. So, this is just 2 times the radius of this cluster. So, what have we proved?
(Refer Slide Time: 52:57)
 
We have proved that, the distance from c i to c k is at most 2 times the radius of cluster o j. But, this is less than 2 times the optimal radius. So, together what have we argued? So, we have argued between these two things. That twice the optimal is greater than or equal to distance from c i to c k, which in turn is greater than or equal to the radius produced by the algorithm. So, even in this case we have argued that, the radius that is produced by the algorithm is at most twice the optimal radius. And that is what we argued in the first case as well.
(Refer Slide Time: 53:56)
 
So, in conclusion we can say that greedy algorithm produces clustering of radius at most 2 times radius produced by optimal algorithm. This is the main theorem of this entire exercise. So, let me now summarize and have some concluding remarks.
(Refer Slide Time: 54:56)
 
First, in the clustering example the distances I mentioned had to be Euclidean. But, we did not really use the Euclidean property. We just use the property that, the distances we just use the triangle inequality. So, essentially this means that any metric is enough. So, this clustering algorithm works not only for Euclidean distances but, anything which satisfies the metric properties. That is, it satisfies essentially the triangle inequality that will work.
So, instead of clustering the same t center problem or the k center, can be related not only to clustering but, it can also be related to some kind of a facility location problem. So, let you think about this. I would also like to observe that, set cover like problems also appear quite frequently, just as this clustering problem. Many scheduling problems and important problem amongst them is so called crew scheduling, is a variation on set cover.
And in general, the greedy algorithms and a style of analysis used over here which is which I would like to call, compete with the optimal also appears quite frequently. So, we have seen two styles of analysis, compete with the optimal and compete with and compare to the lower bounds. So, between these two styles we should be able to design a fair number of approximation algorithms. 
Thank you.

Design & Analysis of Algorithms
Prof. Abhiram Ranade
Department of Computer Science & Engineering
Indian Institute of Technology, Bombay

Lecture - 34
Approximation Algorithms for NP - Complete Problems ? III

This is the third lecture in the series on Approximation Algorithms. A short review of what we have seen so far.
(Refer Slide Time: 00:55)
 
So, one of the approximation algorithms we saw gave us, a two factor approximation for Metric TSP. We also saw a two approximation algorithm for Metric clustering. Having seen these algorithms, a natural question might be that instead of just getting a two approximation is it perhaps possible that, we can get a 1.5 approximation. Say either for metric TSP or for metric clustering. Here are some possible answers.
And these happen to be true, as a matter of fact. So, for example for metric clustering you cannot get better approximation than a factor two, unless P is equal to NP. So, this is the other interesting kind of a result. That, it is not only is it hard to find fast clustering algorithms, which accurately cluster. But, even getting approximate clustering algorithms seems to be difficult, because we believe that P is not equal to NP.
Most people believe that. However, here is some good news. If you look at the Euclidean TSP, so which is a special case of the metric TSP. It turns out that, you can get any approximation factor 1 plus epsilon. However, there is a catch and the catches that your running time will also depend upon epsilon. So, essentially there is going to be a trade of between the running time and the approximation factor.
Obviously, the closer you want the approximation factor to one, the higher is your running time going to be, so the smaller the epsilon the larger the running time. So, this dependence will be captured somehow. We can evaluate that dependence. And in fact, we can work with a wide range of epsilons. And that is, what this result says. Such results are called approximation schemes. And this is, what we are going to study today.
(Refer Slide Time: 03:13)
 
Here is a quick definition. An algorithm A for a problem P, is said to be a polynomial time approximation scheme abbreviated as PTAS, if the following conditions hold. First of all, A must now take two arguments. Of course, it has to take the problem instance. And it will return the problem answer. But, it will also take in addition a single number epsilon, which is greater than 0 which is going to tell the algorithm, how close an answer we want, how good an answer we want.
So, A is going to return a solution with approximation ratio one1 plus epsilon. The smaller we name this number, the better is our solution going to be. But, as mentioned earlier the time taken by A is going to be polynomial. And now, it is going to depend on epsilon. So, this polynomial we change with epsilon that is the new addition. So, the time is going to be polynomial. We are going to be able to do this for any epsilon but, the time will be a function of epsilon as well.
So, it is a scheme in the sense that, you can operate anywhere. It is not just one algorithm but in fact, you can think of it as a family of algorithms, defined by different values of epsilon. There is also a notion of fully polynomial approximation scheme, which is often abbreviated as FPTAS. And in this case, we require that the time should also be polynomial in 1 over epsilon. So, yes the time will increase as you reduce epsilon. But, it will only increase polynomial.
Here however, the time can increase and in fact, it can increase much faster than any polynomial. So, this is going to be something harder to do or this is going to be something, which is going to be faster for the same epsilon. Here is, what we are going to today. We will describe an FPTAS or a fully polynomial approximation scheme for the knapsack problem, which we studied earlier.
The time taken by the scheme is going to be O of n cube upon epsilon. So, notice that this is polynomial in the size of the instance n, as always we will use n to denote the instance, the size of the instance. It is also polynomial in 1 over epsilon. So, it is n cube. Read it as n cube times 1 over epsilon. So, it is dependence on 1 over epsilon is linear. So, it satisfies this condition as well. So, the time taken is polynomial in n as well as it is polynomial in 1 over epsilon.
And therefore, we will what we will get is going to be an FPTAS. And so we will be designing an algorithm which will take, which will be an approximation scheme. So, it will take an instance as one argument. And it will take the number epsilon. It will return a solution with approximation ratio 1 plus epsilon. And its time taken will be this.
(Refer Slide Time: 06:49)
 
Let me quickly remind you, what the knapsack problem is. We have actually, we have of course studied it earlier. The input consists of an array V 1 though n, V stands for value and V of i is going to be the value of the ith item. The second part of the argument is going to be an array W 1 through n. So, these are all going to be integers. V and W are both going to be integers. And W of i again is going to be the weight of the nth item.
That is going to be a third argument, which is C again an integer. And C is going to denote the capacity of a knapsack. The output, for the output we have to select a subset of these items 1 through n such that, the total weight is at most C. So, think of filling this knapsack. But, when we fill the knapsack we should not exceed its weight capacity. Otherwise, a knapsack will tear or something like that.
And we are only allowed to pick a subset of these items. And furthermore, we want to pick the most valuable subset. So, whatever a subset we pick, we add up the values of the elements which we pick and that is the value that we get. And we want that value to be as large as possible. So, it should be maximum over all possible subsets. You have of course, seen this problem earlier. We have devised an algorithm for this.
The algorithm was pseudo polynomial time, as we discuss some time ago. And in fact, knapsack is NP complete. What we are going to do today is, we are going to discuss a new algorithm. It is again going to be based on dynamic programming. So, we will have a new dynamic programming algorithm. It will also be pseudo polynomial time. So, that would not really work, for work as an approximation algorithm.
For one thing, it is not an approximation algorithm. It is going to be an exact algorithm. But, the reason it would not work is that, it will take pseudo polynomial time whereas, we wanted to take polynomial time. So, after that we will describe how we can modify the input instance to this algorithm. Such that, we get good approximate answers but, we get them fast. So, that is going to be the interesting idea.
The sort of new idea of today?s lecture is going to be this. That will take a pseudo polynomial time algorithm. And we will use that. But, instead of feeding to it, the exact instance that is given to us, we will feed it a different instance perhaps sort of an approximated instance. So that we will get the answers fast although, we will get approximate answers.
(Refer Slide Time: 10:05)
 
So, let me quickly go over the dynamic programming formulation that we have studied earlier, very quickly. And, then I will tell you however, new dynamic programming formulation is going to be different. Here is the old formulation. So, what we asked over there was, what is the best value we can get for each knapsack capacity c little c, where c is an integer somewhere between anywhere between 1 and C 1 and capital C.
So, capital C is the integer given to us as the part of the problem instance. So, if you remember the dynamic programming algorithm which we studied long ago, it was doing exactly this. For each value of c little c between 1 and capital C, it calculated what is the best value we can get. So, basically the question was for a fixed capacity and we take different capacities but, in each individual question the capacity is fixed.
What is the best possible value or what is the largest possible value, we can get? The new problem or the new way of looking at this problem, is to ask a different question. The question which we are going to ask is, what is the lightest knapsack? And by the time in, what is the smallest capacity knapsack, which we can use for getting some value v. And we will do this for all values of this little v, in the range 1 through V all.
What is V all? The V all is the sum of the values of all the items. We are looking at this V all, because at most we will be interested in filling all items. Beyond that, there is nothing of interest, because there are no more items to fill. If you figure out, what is the capacity needed to fill all the items, we should be we should really be happy. So, this is going to be the kind of question that, we are going to study today.
And as you can see, it is a sort of the complementary question. So, here we ask the largest value for fixed capacity. Here, we are going to ask the least capacity for the fixed value or the lightest knapsack for the fixed value. I want to point out that, if we answer these new questions we will still get a solution to the original problem, which we started off with. So, these new questions essentially compute S of V, where S of V is the lightest knapsack for value v.
That is exactly, what is being computed over here for different values of v. And notice that, S of V is an increasing sequence or a non decreasing sequence. Now, the question that we started off with was, to get the best value for capacity C. So, on the phase of it, it might seen that this is the natural question to ask. And indeed, it is somewhat more natural. But, the point is that even if we answer these questions, we will be able to find this why.
Because, we simply ask what is the largest value v such that, the size required for it is less than or equal to C. So, clearly this is going to be the value which we are going to get, had we started out with a knapsack of capacity C in this question. So, the idea over here is that, even if we solve all such questions we will be able to solve this question, which we originally started off with.
You might think that, here we are mobilized to solve many many questions, even though our single question over here is just, there is the single question over here. But, if you remember if you go back to the dynamic programming algorithm we looked at, even to answer the single question we really answered all this question. So, it is not that we are going to answer more questions, this time. Well, we are.
So, we may not necessarily answer more questions over here, we are just going to answer different kind of questions. And from those answers as mentioned over here, we will still be able to get the answer to the real question that, we are asking. So, from on for the rest of the lecture, I am going to think of these new questions. And once we have the answers to these new questions, we will use this idea to return the best value of capacity C, for capacity C.
So, basically my new instances for each of these questions are going to be characterized, by these three arguments. So, we are going to have an argument W the weight, the value V and the target value. The target total value that we need and we are going to have such questions, for all such different V?s. And our objective is going to be minimize, the knapsack size. So, it is going to be, this kind of questions where we want the lightest knapsack for this fixed value V.
(Refer Slide Time: 15:26)
 
So, how do we solve this problem? Basically, we see that the algorithm is actually the logic behind designing the algorithm is very similar to, what we did for the other for the more natural looking algorithm. Basically, we need to decide whether it will include item 1, whether it will include item 2, whether it will include item three and so on. So, we have a series of decisions to make. So, this is what we want.
We want the optimal solution to the instance W 1 through n V 1 through n and v. And let us look at it, as this search space idea. So, we consider the search space for this big instance. For this instance, which is W of 1 through n V of 1 through n and little v. So, this is our search space. So, what is contained in it?
(Refer Slide Time: 16:25)
 
It contains, knapsack capacities or it contains solutions to such problems. So, it contains feasible solutions to such problems. Now, the feasible solutions to such problems can be of two types. So, for example we can have solutions which have value V. So, everything over here must have value V. But, may be the solution contains item 1 or not. So, there could be one set of solutions, which contain item 1 and another set of solutions which do not contain item 1.
So, this consists of sets with value V, not containing item 1. And this must contain item 1, sets containing item 1. So, we really want the lightest capacity set, the lightest weight set from this. But, we set that this search space has been decomposed into two parts. So, we could ask what is the lightest capacity, the lightest set in this? What is the lightest set in this. And we could take the lighter of the two. This is precisely, what has been written down over here.
So, we wanted the optimal solution to the instance W 1 through n, V 1 through n and little v. And we can get that, by picking the lighter of the solutions or the solution with the smaller weight, of these two solutions. So, the first one is we look at, this set. And we pick the lightest solution in that. And that is over here. We will pick the lightest solution in that and that is over here. So, if you remember this is pretty much idea we used, in our original dynamic programming algorithm as well. What can we say about these two things? We can say something rather interesting.
(Refer Slide Time: 18:54)
 
So, this first term over here consist of all solutions to this instance, which do not have item 1. But, what does that mean. That just means, that we might as well we asking for solutions to W 2 through n and V 2 through n and v, because we are not using item 1 any way. So, this term is in fact, exactly the optimal solution to this instance. So, notice that this is interesting. This is useful, because we are heading towards a recursive solution.
So, a solution to this is being expressed in terms of a solution to a smaller problem. That is always good news. What about this? We want the lightest solution of value v, having item 1. So, we are looking at this set. We know that at this set of sets, this part of the solution space.
(Refer Slide Time: 19:57)
 
We know that, every set over here contains item 1. So, we take that away. And what is left. So, what is left are sets which do not contain item 1. But, we also know that there value had better be this, v minus the value of 1. Why? Because, every set in this part of the space, originally had value little v. If they contain an item 1 and if we remove that item 1, then the new value must be exactly v minus V 1. So, what remains in this part of the search space are sets, which do not contain 1. But, whose value is v minus V 1.
(Refer Slide Time: 20:46)
 
So, we take the best amongst those and act to that, the item1. But, what is the best amongst those. So, the best amongst those is again an optimal solution to some instance. In fact, it is an optimal solution to this instance. So, it is an optimal solution to W of 2 through n, V of 2 through n and v minus V 1. This is the knapsack capacity that, this is the value that we are seeking. Why we seeking this smaller value? Because, we know that we are, at the end we are going to add item 1 to it.
So, if we add item 1 into it, the total value will become v and which is the value that we want. So, therefore we take an optimal solution to this problem instance. We take an optimal solution to this problem instance. Add 1 to it and take. Add the item 1 to it and take the lighter of these two things. So, that is basically the algorithm. So, now all that we need to do is, express this as a recurrence. Well, there is a slight catch.
So, when we do this, this v minus V 1 could become negative. What is that mean. I want an optimal solution in which the value is negative, that does not mean anything. So, we had better be careful about that. So, we must generate this part. Only if v minus V 1 is greater than or equal to 0, because otherwise this problem instance is undefined. So, when we write down our occurrence, we will have to put an explicit.
Check whether v minus V 1 is greater than or equal to 0 or whatever, greater than or less than 0. So, here is our expression in terms of which we are going to define our recurrence. So, S i v I am going to define as the least capacity knapsack, which can give the value v using items i through n. So, this i defines this i. This is the same i. And this v defines, the value that we want. So, S i v is going to denote the least capacity knapsack, which gives me value v using items i through n.
So, now I am just going to take this expression that we derived. And express it in terms of this kind. So, what is this optimal solution to W V v. Well, this is simply this left hand side is simply S 1 v. Because, we are starting with i equal to 1, we are allowing all items. And we are asking for value v. Then, we are going to take the lighter of the solutions. So, correspondingly we are going to use the minimum over here, the minimum of two solutions.
What is the first solution? The first solution is to the instance W 2 n V 2 n little v. So, it is going to be the least capacity solution to this instance. So, that is as good as saying, it is S 2 v. So, that is what we have written down over here. The second part is this. However, when we write this down, we have to make sure that v minus V 1 is not less than 0. So, let us check that. So, I am going to write down a c style expression.
So, this says that, let us check whether v minus V 1 is greater than or equal to 0. If it is, then the value that we want here is W of 1 plus S of 2 v minus V 1. Why is that? Because, we are going to have item 1 always and therefore, we are going to have the weight corresponding to item, always present over here. And we are going to add this item 1, into the optimal solution to this problem.
But, the weight of that optimal solution, the capacity needed for that optimal solution is simply S of 2. We start with items 2 through n. And therefore, this is a 2 over here. And the value that we are expecting is v minus V 1, exactly this. So, if this expression is greater than or equal to 0, then this is the value that we want. If this expression is less than 0, this problem is undefined. But, what does. How do we represent that? So, that we represent by putting in an infinity.
So, we are taking the min, this infinity will never be taken. And if this second expression is in infinite, then that is as good as saying just give me the first expression. So, this is what we have distilled out of this. Well, it is actually the same thing but, it is now return out more compactly in terms of variables of this kind, subscripted expressions of this kind. You must of course, generalize it in order to use it to design an algorithm, we need to generalize it.
So, here is a generalization. So, here we were looking at all the items 1 through n. And you can note, you can see that internally we got we needed to have solutions to problems in which, we were having items 2 through n. If we did recursion on that, then we would get 3 through to n 4 to n and so on. So, therefore we now consider the more general case. So, we are going to ask what is the least capacity solution of value v using items i through n.
So, that is S of i v, that is denoted by S of i v. So, analogously we will write down the expression, the more general expression. So, here we skip the first item. And so we started off with 2. So, similarly here we are going to start off with i plus 1. So, that is the first solution corresponding to this. And instead of checking, whether v minus V 1 is greater than or equal to 0. We are simply going to be checking, whether v minus V of i is greater than or equal to 0.
If so here we took W 1 plus something. Here we will take W i plus something. Again, this something is going to contain items 2 through n over here. It will contain items i plus 1 through n over here. And so we will have i plus 1 over here. This was the value required, was v minus V 1. Here the value require is v minus V i, because we are adding an item i later on anyway. In case we want v minus V i that is the value of this part and then to it, we add item 1.
So, the weight solution value of this whole thing together will be W of i plus S of i plus 1 v of V minus i, as before. And of course, if v minus V i is less than 0, then we do not want this entire term to be taken into account at all. And therefore, otherwise we are going to put down infinity. So, this is a defining recurrence that we are going to use. So, let us see how this recurrence can be solved.
(Refer Slide Time: 28:41)
 
So, as usual we are going to be keeping a table. So, I have just written out that recurrence again. So, let us see what kind of table, we can use for this. So, here is the table. This is the i axis going down vertically. This is the v axis. So, earlier I said that v really needs to start from 1. But, it is useful to have a 0 here, as well. So, have started it off from 0 and as before it goes to V all. So, since we are going to have, since I want to show you what is recurrence means in this table?
I have put down some specific, I have marked out from specific entries. So, let us ask how this recurrence will work out in this table. So, I want to compute S of i v, which is this entry in this table ith column vth row, ith row vth column. Now, this entry depends upon which entries. Well, it depends upon this entry. And it depends upon this entry. It does not, we just do not have to look at this entry.
We have to do something to it. But, this other thing that we have to do, we know what the value of W i is. So, it really depends upon this entry and this entry. So, which are these two entries? So, this entry is this entry and this entry is this entry. Of course, if v minus V i was less than 0, then this entry would fall outside the table. And so we would only have a single entry over here. But, this is the more common more interesting case.
So, to fill this entry, it is sufficient if we have this entry and this entry filled. That is, all coming out of this recurrence which we have written down over here. Well, that suggests a way of filling in this table. So, we sort of fill in going bottom up or bottom right from. We start of at the bottom and go upwards, but we also need this. And so therefore, we also have to start from the left side. So, to do that we would need to have these entries filled.
How do we fill, this entries? Well, let us interpret what these entries are. So, this entry in general is going to be S i 0, for different values of i. And let me remind you, what S i 0 is. S i 0 is the capacity, the minimum capacity needed to get a value of 0 using items i through n. That does not seem to difficult does it, we just want to get a value of 0. So, what is the minimum capacity, we need. Well trivially, the answer is 0 capacity.
So, if we get capacity, if we get a knapsack of capacity 0 we can certainly get value of 0 by filling nothing into it. And clearly there is no smaller knapsack that, we can use, because this is the smallest possible knapsack. So, this entire yellow column just needs to be filled with 0s. And that something, we can do without any computation. So, that leaves open the question of, how do we fill this row. So, let us try to interpret what this row is.
So, this row in general is going to be S n v, which is denoted, which is denoting the capacity needed to get value v, the least capacity needed to get value v using item n alone. So, you are just allowed to pick item n nothing else. So, what kind of capacities can you get? What kind of values can you get? Well clearly, if this v happens to be V of n, then you can do that using just a single item. So, that would been that if v happens to be V of n, then the least capacity knapsack that you would need, would have to have capacity of W of n.
So, one of these entries can be filled using this. What about the rest of the entries? Suppose we want to do this for a different vale of v. So, let us say we want to get a value larger than v of n, can we do it. We are only use to, we are only allowed to use the item n. So, clearly we cannot do it. We cannot get a value either bigger or larger than V of n, by using only item n. So, then what we do. So, that can be represented quite nicely, by saying that by putting in infinity, in these entries.
So, v of V n, so if v is not equal to V n, then we cannot get that value and so we will say that a capacity knapsack of, capacity infinite is needed. Let me explain, why this works. Basically later on, we are going to take things like min of this or min of this or something like that. So, if there are infinities over here, then that value is essentially going to be ignored. Or if both of these values are infinites and if you take min of those infinities, then an infinity will crop up over here.
But, says that even this value is impossible to accomplish. So, that is sort of a nice thing. That is a kind of a nice coding that, infinity allows us to accomplish. So, basically now we have express the algorithm entirely. So, we have entries to fill. And there are sort of three kinds of entries to fill. We have these yellow entries to fill. We have these blue entries to fill. And, then there are the rest of the entries which we fill according to this recurrence.
(Refer Slide Time: 34:46)
 
So, we can write done our algorithm. So, I am going to call my algorithm KS as knapsack. It is going to take as argument. So, that brings us to our algorithm. We will call this KS for knapsack. It takes as arguments, the value, the weight of all the n item and the capacity. So, we are going to solve, we are going to find an answer to this. But, we are going to find an answer using our new formulation.
So, as mentioned earlier. So, there were those yellow entries to be filled. They were all going to be filled with 0. So, we will do that. So, these entries using this equation. Then, there were the blue entries to be filled. The blue entry is where that, for if v is equal to V of n, then the knapsack capacity needed was W of n. Otherwise, the knapsack capacity needed was infinity. So, this is how you fill the bottom row.
So, this is how we filled and this was the equation used. Finally, the rest of the entries were filled using the recurrence which we derived. So, this is that recurrence. At the end of it, to get the value for the capacity C, we needed to find as we described earlier the largest v such that, S v 1 is less than C. So, what is S v 1. So, we look at all possible v?s. So, this is column 1. And we know that, as we go as we go down this is going to be non decreasing.
So, we can easily find the largest v. Such that, S v 1 is less than C. And that is the value, which we will call v star. So, that is the value that we are going to get. If v in fact, had a capacity of C. So, if you are just interested in the value, then we would be done at this point. However, if we wanted the items to be returned as well, then we can do that also. You remember, how we did that for the older algorithm.
We just had to keep track of some additional pointer, some additional data structures. Basically, we can do that as well. So, corresponding to every entry of the table, we can determine what the corresponding set is going to be. So, we can also return the set of items. So, how long does it take just to compute everything from here, until v star? Well. These are all, this loop will take time O of n. This will take time O of V all. And here, we have nested loops.
And therefore, it will take time O of n times V all. In fact, if you compute the set itself, we will have to keep some additional data structures. But, as explained in the previous algorithm we can use exactly the same ideas to do the, to compute the set as well in exactly the same amount of time. Well, to within constant factors. So, in O of n times V all time, we can not only compute v star but, we can also compute the corresponding set.
So, this entire problem can be solved n time O of n times V all. So, that finishes the first task that we undertook. So, we now have a dynamic programming algorithm, which finishes in time n times V all. So, now we come to the approximate algorithm. Here is the important point.
(Refer Slide Time: 38:54)
 
So, here we are going to be allowed an error in the answer. We are only required to get within 1 plus epsilon. So, epsilon is sort of the error we are allowed. The point is that, if we are allowed an error, then it means that we can calculate using low precision. So, basically we are going to calculate using low precision. And that is going to allow, us to reduce the time. So, we are going to write a new procedure, a new algorithm which we are going to call an approximate case approximate knapsack or AKS.
It is going to take the same arguments as before. But, it is also going to take an argument called delta, which is somehow going to reflect the precision. And we will tight up to this epsilon later on. So, delta and epsilon will be related. How they will be related, we will specify pretty soon. Here is the algorithm. So, we are going to define a new array V prime. So, V prime is simply going to be V divided by delta corresponding elements.
So, every element here is going to be divided by delta. And we get the corresponding element of here. But, we want V to be integer as well. So, if we get a fraction which we will in general do. We will take the floor. We will take the largest integer less than or the floor. And, then so essentially we are scaling down the V values by a factor delta. We will call KS using those scale down values. But, the scale down values are not really going to be important for the final answer.
So, for the final answer we want to return delta times whatever, scale down values we got. So, this is expected to do roughly the same job. We scale the values down, we got a good solution. And, then we return. We scale the values up, the only catch is that here we took the floor. So, this will produce some error. At the same time sense, the answer the time taken for this case is proportional to the second argument. The time over here is going to be V prime rather than V.
So, that is where we are going to say on time, as well. So, we will say on time at a cost of some error. So, what remains now is to analyze all these. Let us say, that S denotes the set returned by our original KS call. So, where we were using the actual values of V as given? So, this is truly the optimal set which was return. And its value x is the actual optimal value. And I can think of this as the full precision problem or the full precision answer as well.
S prime is the set returned for this problem this by, this case without multiplication by delta for the minute. Let us say its value is X prime and this is a low precision answer. So, what we now need to do is to relate this X prime, that we so X prime and X and so on. So, let us just do that. So, I observe first, that X prime is the value of the sets over here, value of the set over here. And what is that.
So, it is the values of all the elements in the set. So, V prime of i, because this time we had passed V prime, where i belongs to this S. Now, this S prime, now here is the important point. This X prime, this value is bigger than this expression as well. Notice that the only difference between these two things is that, instead of choosing S prime I am choosing S. So, this was the optimal set which was returned. This is not the optimal set but, it is some difference set.
So, what happens if it is a difference set? So, its value need not be as big as the value of this. So, in other words this value could be has to be at least as big as this value. But, notice that this S was an acceptable solution to this. And in going from here to here, we have not changed the weights. So, this set is also an acceptable solution to this. So, therefore we can clearly say that this value had better be no smaller than this value. Because, this is also feasible solution to the V prime problem as well.
So, S is also a feasible solution. And therefore, we get this. Now finally, we observe that if I take the floor of any number, it is bigger than that number minus 1. And therefore, this is the floor of this. And therefore, this is bigger than this minus 1. So, V prime of i is the floor of this. And therefore, V prime of i is bigger than V of i upon delta minus 1 because of this. So, we have been able to relate X prime to this quantity over here.
So, the point is that we are getting towards the optimal set. Somehow we want to relate it to the value of the optimal solution. So, let us now ask what is the value that AKS will return? AKS returns, delta times this value. Correct, delta times this value. This is what we do over here. And what is that. So, delta times that value. Well, for X prime I am going to substitute this. So, I am going to get this, multiplied by delta.
So, if I multiply by delta, this delta is going to get cancelled out. So, I get a V minus i. And instead of this one, I am going to get a delta, because I multiplied by delta. So, that explains this part. Now, what is summation over S of V i. That simply X. So, I get an X over here. And there was a minus delta but, it was also in the sum. So, my delta is a constant. Delta does not vary depending upon, which element of the set I am considering.
So, I will get minus delta as many times as the cardinality of S. So, I am going to get X minus cardinality of S times delta. So, here is an important observation. We have proved, that the value returned approximately is at least the actual value minus cardinality of S times delta.
(Refer Slide Time: 45:55)
 
So, now we are ready to evaluate the approximation ratio. What is the approximation ratio? So, it is on the value of the returned value, upon the value of the value returned by the approximate solution. Remember, that this problem is a maximization problem. Therefore, the optimal solution has the largest possible value. And in that case, we define our approximation ratio as the optimal solution upon the approximate solution. So, that is exactly what we are doing over here. So, we want the approximation ratio of X upon Y.
(Refer Slide Time: 46:46)
 
So, let us collect the equality inequalities which we had over there. So, we had one inequality which is Y is greater than or equal to X minus cardinality S times delta. So, this is one inequality. And I can write this as X is less than or equal to Y plus cardinality of S times delta. If I divide the whole thing by Y, I am going to get X upon Y is less than or equal to 1 plus cardinality of S times delta upon Y. This is what we are going to get. So, this is what we finally have over here.
(Refer Slide Time: 47:34)
 
What is the time taken by AKS? So, it is n times V all V prime all or V prime all is essentially V all by delta. So, it is this time. So, notice that the time over the exact evaluation has reduced by a factor delta. So, all that remains now is to choose delta carefully. Now, here is the clever choice. We are going to choose delta equal to epsilon times V all upon n square. So, let us just do that.
(Refer Slide Time: 48:15)
 
So, we choose delta equal to epsilon times V all upon n squared. So, what does that do for us? So, first of all this was our approximation ratio. So, if we substitute into that, what do we get?
(Refer Slide Time: 48:29)
 
So, we substitute delta upon Y over here. So, we get epsilon V all. And, then this V of the Y remains as it is from here. And we get an n squared over here. So, this is the approximation ratio. So, what becomes to this? What becomes of this, I claim that this becomes at most 1 plus epsilon. Why? So, let us see that. The first observation is that, the cardinality of S which appears over here has to be less than n.
After all, what is S? It is a subset of n element. So, its cardinality has of course, to be less than n. So, that is one important observation. Then, second V all is the sum of all the elements. So, clearly it has to be less than V max times n. So, if V max denotes the maximum value if I multiplied by n, I certainly should get something which is bigger than just the mere some of values. So, this is what I get.
And I claim that, V max cannot be bigger than Y. So, the maximum item had better be accommodatable in my knapsack. Otherwise, I would not have considered it in my list in the first place. And therefore, the optimal solution had better include this largest item. And therefore, V max times n has to be at most Y times n. So, it follows from this, that S times V all is less than Y times n squared. So, this S times this V all is less than Y times n squared.
And therefore, our approximation ratio is 1 plus epsilon. I also claim that the time taken is n cube upon epsilon. And this is much easier to see. So, time taken is n times V all upon delta. So, I just substitute and I get instead of delta I substitute this. So, this V all cancels. And, then I get an n squared here. So, I get n cube upon epsilon. So, what has happened? We have shown that, our approximation ratio is 1 plus epsilon.
So, no matter what epsilon you give me. I will be able to get this time, If I choose delta equal to this in my procedure. And furthermore, my time taken is going to be n cube upon epsilon. So, which is exactly what we have promised? We had promised to get an approximation scheme, in which the approximation ratio is 1 plus epsilon for every epsilon. And that we would prove that, the time taken is polynomial in n the input instance length and 1 over epsilon. That is what we have done. So, that concludes the main part of the lecture. I just want to make a few remarks.
(Refer Slide Time: 51:18)
 
Our FPTAS had time of n cube upon epsilon. It is possible to device another FPTAS, with a different expression n log 1 over epsilon plus 1 over epsilon to the 4. So, it is very likely that in practice, this approximation. In many practical situations, this might be a better algorithm than this. But, this is of course, more complicated as well. And we are not going to look at it.
Now, here is an interesting fact that such as, that somehow that knapsack problem is actually among the easier NP complete problems, in the following sense. Knapsack problems, in which weights and values are drawn uniformly at random from the interval 0 through 1, can be shown to take polynomial time on the average. So, in some sense the FPTAS result says that, it is easier to approximate.
And this says that, it is also easier on the average in fact, its polynomial in the average. Finally, I just want to say that many problems having pseudo polynomial time algorithms. Even if there NP complete can be shown to have a PTAS or even an FPTAS. One example is the sub another example is the subset sub problem. 
Thank you.

