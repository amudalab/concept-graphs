
Decision tree
 /firstHeading 
 bodyContent 

 tagline 
From Wikipedia, the free encyclopedia
 /tagline 
 subtitle 

 /subtitle 
 jumpto 

					Jump to:					navigation, 					search

 /jumpto 
 bodycontent 
This article is about decision trees in decision analysis.  For the use of the term in machine learning, see Decision tree learning.




Traditionally, decision trees have been created manually.


A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm.
Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal.




Contents


1 Overview
2 Decision tree building blocks

2.1 Decision tree elements
2.2 Decision tree using flow chart symbols
2.3 Analysis example
2.4 Another example
2.5 Influence diagram


3 Advantages and disadvantages
4 See also
5 References
6 Further reading
7 External links




[edit] Overview
In decision analysis a decision tree and the closely related influence diagram is used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.
A decision tree consists of 3 types of nodes:

Decision nodes - commonly represented by squares
Chance nodes - represented by circles
End nodes - represented by triangles

Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal. If in practice decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities.
Decision trees, influence diagrams, utility functions, and other decision analysis tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of operations research or management science methods.
[edit] Decision tree building blocks
[edit] Decision tree elements



Drawn from left to right, a decision tree has only burst nodes (splitting paths) but no sink nodes (converging paths). Therefore, used manually, they can grow very big and are then often hard to draw fully by hand. Traditionally, decision trees have been created manually - as the aside example shows - although increasingly, specialized software is employed.
[edit] Decision tree using flow chart symbols
Commonly a decision tree is drawn using flow chart symbols as it is easier for many to read and understand.



[edit] Analysis example
Analysis can take into account the decision maker's (e.g., the company's) preference or utility function, for example:



The basic interpretation in this situation is that the company prefers B's risk and payoffs under realistic risk preference coefficients (greater than $400Kin that range of risk aversion, the company would need to model a third strategy, "Neither A nor B").
[edit] Another example



Decision trees can be used to optimize an investment portfolio. The following example shows a portfolio of 7 investment options (projects). The organization has $10,000,000 available for the total investment. Bold lines mark the best selection 1, 3, 5, 6, and 7, which will cost $9,750,000 and create a payoff of 16,175,000. All other combinations would either exceed the budget or yield a lower payoff.[1]
[edit] Influence diagram
A decision tree can be represented more compactly as an influence diagram, focusing attention on the issues and relationships between events.



The squares represent decisions, the ovals represent action, and the diamond represents results.
[edit] Advantages and disadvantages
Amongst decision support tools, decision trees (and influence diagrams) have several advantages. Decision trees:

Are simple to understand and interpret. People are able to understand decision tree models after a brief explanation.
Have value even with little hard data. Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.
Possible scenarios can be added
Worst, best and expected values can be determined for different scenarios
Use a white box model. If a given result is provided by a model.
Can be combined with other decision techniques. The following example uses Net Present Value calculations, PERT 3-point estimations (decision #1) and a linear distribution of expected outcomes (decision #2):

Disadvantages of decision trees:

For data including categorical variables with different number of levels, information gain in decision trees are biased in favor of those attributes with more levels.[2]
Calculations can get very complex particularly if many values are uncertain and/or if many outcomes are linked.

[edit] See also






Decision tables
Decision tree complexity
Decision tree model of computation
Expectiminimax tree






Influence diagram
Markov chain
Morphological analysis






Odds algorithm
Operations research
Topological combinatorics
Truth table






[edit] References


^ Y. Yuan and M.J. Shaw, Induction of fuzzy decision trees. Fuzzy Sets and Systems 69 (1995), pp. 125139
^ Deng,H.; Runger, G.; Tuv, E. (2011). "Bias of importance measures for multi-valued attributes and solutions". Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN).


[edit] Further reading

Cha, Sung-Hyuk; Tappert, Charles C (2009). "A Genetic Algorithm for Constructing Compact Binary Decision Trees". Journal of Pattern Recognition Research 4 (1): 113. http://www.jprr.org/index.php/jprr/article/view/44/25.

[edit] External links



Wikimedia Commons has media related to: decision diagrams



Decision Tree Analysis mindtools.com
Decision Analysis open course at George Mason University
Extensive Decision Tree tutorials and examples

 
NewPP limit report
Preprocessor visited node count: 1392/1000000
Preprocessor generated node count: 17042/1500000
Post-expand include size: 7145/2048000 bytes
Template argument size: 3400/2048000 bytes
Highest expansion depth: 13/40
Expensive parser function count: 1/500

 Saved in parser cache with key enwiki:pcache:idhash:232602-0!*!0!!en!4!* and timestamp 20130115063127 
  /bodycontent 
 printfooter 

				Retrieved from "http://en.wikipedia.org/w/index.php?title=Decision_tree&oldid=531799803"				
 /printfooter 
 catlinks 
Categories: Decision treesDecision theoryHidden categories: Use dmy dates from January 2011  /catlinks 

 debughtml 
 /debughtml 

 /bodyContent 

 